["```py\ntransform = transforms.Compose([    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.3337, 0.3064, 0.3171),\n        ( 0.2672, 0.2564, 0.2629))\n])\nbatch_size = 6\nn_class = 43\n# Loading train and test sets of\n# German Traffic Sign Recognition Benchmark (GTSRB) Dataset.\ntrainset = torchvision.datasets.GTSRB(\n    root='../../data',split = 'train',\n    download=True,transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset,\n    batch_size=batch_size,shuffle=True, num_workers=2)\ntestset = torchvision.datasets.GTSRB(\n    root='../../data',split = 'test',\n    download=True,transform=transform)\ntestloader = torch.utils.data.DataLoader(testset,\n    batch_size=batch_size,shuffle=False,num_workers=2)\n```", "```py\nimport torch.nn as nnimport torch.nn.functional as F\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, n_class)\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n```", "```py\nimport torch.optim as optimnet = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001,\n    momentum=0.9)\n```", "```py\nn_epoch = 3for epoch in range(n_epoch):\n    # running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the input data\n        inputs, labels = data\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # output identification\n        outputs = net(inputs)\n        # loss calculation and backward propagation for parameter update\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n```", "```py\nfrom transformers import AutoTokenizertokenizer_bertcased = AutoTokenizer.from_pretrained(\n    'bert-base-cased')\ntokenizer_gpt2 = AutoTokenizer.from_pretrained('gpt2')\n```", "```py\nbatch_sentences = [\"I know how to use machine learning in my projects\",\"I like reading books.\"]\n```", "```py\nencoded_input_gpt2 = tokenizer_gpt2(batch_sentences)\n```", "```py\n[[40, 760, 703, 284, 779, 4572, 4673, 287, 616, 4493], [40, 588, 3555, 3835, 13]]\n```", "```py\nencoded_input_bertcased = tokenizer_bertcased(    batch_sentences, padding=True, return_tensors=\"pt\")\n```", "```py\ntensor([[ 101,  146, 1221, 1293, 1106, 1329, 3395, 3776,    1107, 1139, 3203,  102],\n    [ 101,146, 1176, 3455, 2146, 119, 102, 0, 0, 0, 0, 0]])\n```", "```py\n[tokenizer_gpt2.decode(input_id_iter) for input_id_iter in encoded_input_gpt2[\"input_ids\"]]\n```", "```py\n['I know how to use machine learning in my projects', 'I like reading books.']\n```", "```py\n[tokenizer_bertcased.decode(input_id_iter) for input_id_iter in encoded_input_bertcased[\"input_ids\"]]\n```", "```py\n['[CLS] I know how to use machine learning in my projects [SEP]', '[CLS] I like reading books. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]']\n```", "```py\nimport torchfrom torch.utils.data import DataLoader\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n```", "```py\nfrom datasets import load_datasetdataset = load_dataset(\"imdb\")\n```", "```py\ntokenizer = DistilBertTokenizerFast.from_pretrained(    \"distilbert-base-uncased\")\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=True,\n        truncation=True, max_length=512)\n```", "```py\ntrain_dataset = dataset[\"train\"].train_test_split(    test_size=0.01)[\"test\"].map(tokenize, batched=True)\ntest_dataset = dataset[\"test\"].train_test_split(\n    test_size=0.01)[\"test\"].map(tokenize, batched=True)\n```", "```py\nmodel = DistilBertForSequenceClassification.from_pretrained(    \"distilbert-base-uncased\", num_labels=2)\n```", "```py\ntraining_args = TrainingArguments(output_dir=\"./results\",    num_train_epochs=3,per_device_train_batch_size=8,\n    per_device_eval_batch_size=8, logging_dir=\"./logs\")\ntrainer = Trainer(model=model, args=training_args,\n    train_dataset=train_dataset,eval_dataset=test_dataset)\ntrainer.train()\n```", "```py\neval_results = trainer.evaluate()\n```", "```py\nfrom torch_geometric.datasets import Planetoidfrom torch_geometric.transforms import NormalizeFeatures\ndataset = Planetoid(root='data/Planetoid', name='CiteSeer',\n    transform=NormalizeFeatures())\ndata = dataset[0]\n```", "```py\nimport torchfrom torch_geometric.nn import GCNConv\nimport torch.nn.functional as F\ntorch.manual_seed(123)\nclass GCNet(torch.nn.Module):\n    def __init__(self, hidden_channels):\n        super().__init__()\n        self.gcn_layer1 = GCNConv(dataset.num_features,\n            hidden_channels[0])\n        self.gcn_layer2 = GCNConv(hidden_channels[0],\n            hidden_channels[1])\n        self.gcn_layer3 = GCNConv(hidden_channels[1],\n            dataset.num_classes)\n    def forward(self, x, edge_index):\n        x = self.gcn_layer1(x, edge_index)\n        x = x.relu()\n        x = F.dropout(x, p=0.3, training=self.training)\n        x = self.gcn_layer2(x, edge_index)\n        x = x.relu()\n        x = self.gcn_layer3(x, edge_index)\n        return x\n```", "```py\nmodel = GCNet(hidden_channels=[128, 16])optimizer = torch.optim.Adam(model.parameters(), lr=0.01,\n    weight_decay=1e-4)\ncriterion = torch.nn.CrossEntropyLoss()\n```", "```py\ndef train():        model.train()\n        optimizer.zero_grad()\n        out = model(data.x, data.edge_index)\n        loss = criterion(out[data.train_mask],\n            data.y[data.train_mask])\n        loss.backward()\n        optimizer.step()\n        return loss\n```", "```py\nimport numpy as npepoch_list = []\nloss_list = []\nfor epoch in np.arange(1, 401):\n    loss = train()\n    if epoch%20 == 0:\n        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n        epoch_list.append(epoch)\n        loss_list.append(loss.detach().numpy())\n```", "```py\nmodel.eval()pred = model(data.x, data.edge_index).argmax(dim=1)\ntest_correct = pred[data.test_mask] ==\n    data.y[data.test_mask]\ntest_acc = int(test_correct.sum()) / int(\n    data.test_mask.sum())\n```", "```py\nfrom sklearn.metrics import confusion_matrixcf = confusion_matrix(y_true = data.y, y_pred = model(\n    data.x, data.edge_index).argmax(dim=1))\nimport seaborn as sns\nsns.set()\nsns.heatmap(cf, annot=True, fmt=\"d\")\n```"]