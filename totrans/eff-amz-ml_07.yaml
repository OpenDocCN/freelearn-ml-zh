- en: Command Line and SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the AWS web interface to manage and run your projects is time-consuming.
    In this chapter, we move away from the web interface and start running our projects
    via the command line with the **AWS Command Line Interface** (**AWS CLI**) and
    the Python SDK with the `Boto3` library.
  prefs: []
  type: TYPE_NORMAL
- en: The first step will be to drive a whole project via the AWS CLI, uploading files
    to S3, creating datasources, models, evaluations, and predictions. As you will
    see, scripting will greatly facilitate using Amazon ML. We will use these new
    abilities to expand our Data Science powers by carrying out cross-validation and
    feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far we have split our original dataset into three data chunks: training,
    validation, and testing. However, we have seen that the model selection can be
    strongly dependent on the data split. Shuffle the data — a different model might
    come as being the best one. Cross-validation is a technique that reduces this
    dependency by averaging the model performance on several data splits. Cross-validation
    involves creating many datasources for training, validation, and testing, and
    would be time-consuming using the web interface. The AWS CLI will allow us to
    quickly spin new datasources and models and carry out cross-validation effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Another important technique in data science is feature elimination. Having a
    large number of features in your dataset either as the results of intensive feature
    engineering or because they are present in the original dataset can impact the
    model's performance. It's possible to significantly improve the model prediction
    capabilities by selecting and retaining only the best and most meaningful features
    while rejecting less important ones. There are many feature selection methods.
    We will implement a simple and efficient one, called recursive feature selection.
    The AWS Python SDK accessible via the Boto3 library will allow us to build the
    code wrapping around Amazon ML required for recursive feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to handle a whole project workflow through the AWS command line and the
    AWS Python SDK:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing data uploads to S3
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and evaluating models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Making and exporting the predictions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement cross-validation with the AWS CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement Recursive Feature Selection with AWS the Python SDK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started and setting up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a performing predictive model from raw data requires many trials and
    errors, much back and forth. Creating new features, cleaning up data, and trying
    out new parameters for the model are needed to ensure the robustness of the model.
    There is a constant back and forth between the data, the models, and the evaluations.
    Scripting this workflow either via the AWS CLI or with the `Boto3` Python library,
    will give us the ability to speed up the create, test, select loop.
  prefs: []
  type: TYPE_NORMAL
- en: Using the CLI versus SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS offers several ways besides the UI to interact with its services, the CLI,
    APIs, and SDKs in several languages. Though the AWS CLI and SDKs do not include
    all AWS services. Athena SQL, for instance, being a new service, is not yet included
    in the AWS CLI module or in any of AWS SDK at the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: The AWS Command Line Interface or CLI is a command-line shell program that allows
    you to manage your AWS services from your shell terminal. Once installed and set
    up with proper permissions, you can write commands to manage your S3 files, AWS
    EC2 instances, Amazon ML models, and most AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, a software development kit*,* or SDK for short, is a set
    of tools that can be used to develop software applications targeting a specific
    platform. In short, the SDK is a wrapper around an API. Where an API holds the
    core interaction methods, the SDK includes debugging support, documentation, and
    higher-level functions and methods. The API can be seen as the lowest common denominator
    that AWS supports and the SDK as a higher-level implementation of the API.
  prefs: []
  type: TYPE_NORMAL
- en: AWS SDKs are available in 12 different languages including PHP, Java, Ruby,
    and .NET. In this chapter, we will use the Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Using the AWS CLI or SDK requires setting up our credentials, which we'll do
    in the following section
  prefs: []
  type: TYPE_NORMAL
- en: Installing AWS CLI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to set up your CLI credentials, you need your access key ID and your
    secret access key. You have most probably downloaded and saved them in a previous
    chapter. If that's not the case, you should simply create new ones from the **IAM**
    console ([https://console.aws.amazon.com/iam](https://console.aws.amazon.com/iam)).
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to Users, select your IAM user name and click on the Security credentials
    tab. Choose Create Access Key and download the CSV file. Store the keys in a secure
    location. We will need the key in a few minutes to set up AWS CLI. But first,
    we need to install AWS CLI.
  prefs: []
  type: TYPE_NORMAL
- en: '**Docker environment** – This tutorial will help you use the AWS CLI within
    a docker container: [https://blog.flowlog-stats.com/2016/05/03/aws-cli-in-a-docker-container/](https://blog.flowlog-stats.com/2016/05/03/aws-cli-in-a-docker-container/).
    A docker image for running the AWS CLI is available at [https://hub.docker.com/r/fstab/aws-cli/](https://hub.docker.com/r/fstab/aws-cli/).'
  prefs: []
  type: TYPE_NORMAL
- en: There is no need to rewrite the AWS documentation on how to install the AWS
    CLI. It is complete and up to date, and available at [http://docs.aws.amazon.com/cli/latest/userguide/installing.html](http://docs.aws.amazon.com/cli/latest/userguide/installing.html).
    In a nutshell, installing the CLI requires you to have Python and `pip` already
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Add AWS to your `$PATH`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Reload the bash configuration file (this is for OSX):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Check that everything works with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something similar to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installed, we need to configure the AWS CLI type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now input the access keys you just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Choose the region that is closest to you and the format you prefer (JSON, text,
    or table). JSON is the default format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AWS configure command creates two files: a `config` file and a credential
    file. On OSX, the files are `~/.aws/config` and `~/.aws/credentials`. You can
    directly edit these files to change your access or configuration. You will need
    to create different profiles if you need to access multiple AWS accounts. You
    can do so via the AWS configure command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also do so directly in the `config` and `credential` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can edit `Credential` file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the AWS CLI setup page for more in-depth information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Picking up CLI syntax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The overall format of any AWS CLI command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the terms are stated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<service>`: Is the name of the service you are managing: S3, machine learning,
    and EC2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[options]` : Allows you to set the region, the profile, and the output of
    the command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<command> <subcommand>`: Is the actual command you want to execute'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[parameters]` : Are the parameters for these commands'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A simple example will help you understand the syntax better. To list the content
    of an S3 bucket named `aml.packt`*,* the command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, `s3` is the service, `ls` is the command, and `aml.packt` is the parameter.
    The `aws help` command will output a list of all available services.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get help on a particular service and its commands, write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, `aws s3 help` will inform you that the available `s3` commands
    on single objects are ls, mv, and rm for list, move, and remove, and that the
    basic `aws s3` command follows the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `sourceURI` or `destinationURI` can be a file (or multiple files) on
    your local machine and a file on S3 or both files on S3\. Take the following,
    for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This will copy all (thanks to the parameter — recursive) JPG files (and only
    `*.jpg` files) in the `/tmp/foo` folder on your local machine to the S3 bucket
    named `The_Bucket`.
  prefs: []
  type: TYPE_NORMAL
- en: There are many more examples and explanations on the AWS documentation available
    at
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Passing parameters using JSON files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For some services and commands, the list of parameters can become long and difficult
    to check and maintain.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in order to create an Amazon ML model via the CLI, you need to
    specify at least seven different elements: the Model ID, name, type, the model''s
    parameters, the ID of the training data source, and the recipe name and URI (`aws
    machinelearning create-ml-model help` ).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When possible, we will use the CLI ability to read parameters from a JSON file
    instead of specifying them in the command line. AWS CLI also offers a way to generate
    a JSON template, which you can then use with the right parameters. To generate
    that JSON parameter file model (the JSON skeleton), simply add `--generate-cli-skeleton`
    after the command name. For instance, to generate the JSON skeleton for the create
    model command of the machine learning service, write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can then configure this to your liking.
  prefs: []
  type: TYPE_NORMAL
- en: 'To have the skeleton command generate a JSON file and not simply output the
    skeleton in the terminal, add `> filename.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a `filename.json` file with the JSON template. Once all the
    required parameters are specified, you create the model with the command (assuming
    the `filename.json` is in the current folder):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Before we dive further into the machine learning workflow via the CLI, we need
    to introduce the dataset we will be using in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Ames Housing dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the `Ames Housing` dataset that was compiled by
    *Dean De Cock* for use in data science education. It is a great alternative to
    the popular but older `Boston Housing` dataset. The `Ames Housing` dataset is
    used in the Advanced Regression Techniques challenge on the Kaggle website: [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/).
    The original version of the dataset is available: [http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls](http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls)
    and in the GitHub repository for this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: The `Ames Housing` dataset contains 79 explanatory variables describing (almost)
    every aspect of residential homes in Ames, Iowa with the goal of predicting the
    selling price of each home. The dataset has 2930 rows. The high number of variables
    makes this dataset a good candidate for Feature Selection.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the genesis of this dataset and an in-depth explanation
    of the different variables, read the paper by *Dean De Cock* available in PDF
    at [https://ww2.amstat.org/publications/jse/v19n3/decock.pdf](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we will start by splitting the dataset into a train and a validate
    set and build a model on the train set. Both train and validate sets are available
    in the GitHub repository as `ames_housing_training.csv` and `ames_housing_validate.csv`.
    The entire dataset is in the `ames_housing.csv` file.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the dataset with shell commands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The command line is an often forgotten but powerful ally to the data scientist.
    Many very powerful operations on the data can be achieved with the right shell
    commands and executed blazingly fast. To illustrate this, we will use shell commands
    to shuffle, split, and create training and validation subsets of the `Ames Housing`
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, extract the first line into a separate file, `ames_housing_header.csv`
    and remove it from the original file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We just tail all the lines after the first one into the same file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then randomly sort the rows into a temporary file. (`gshuf` is the OSX equivalent
    of the Linux **shuf shell** command. It can be installed via `brew install coreutils`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the first 2,050 rows as the training file and the last 880 rows as
    the validation file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, add back the header into both training and validation files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: A simple project using the CLI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to execute a simple Amazon ML workflow using the CLI. This
    includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Uploading files on S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a datasource and the recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction batch and real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by uploading the training and validation files to S3\. In the following
    lines, replace the bucket name `aml.packt` with your own bucket name.
  prefs: []
  type: TYPE_NORMAL
- en: 'To upload the files to the S3 location `s3://aml.packt/data/ch8/`, run the
    following command lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: An overview of Amazon ML CLI commands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That's it for the S3 part. Now let's explore the CLI for Amazon's machine learning
    service.
  prefs: []
  type: TYPE_NORMAL
- en: All Amazon ML CLI commands are available at [http://docs.aws.amazon.com/cli/latest/reference/machinelearning/](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/).
    There are 30 commands, which can be grouped by object and action.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`create` : creates the object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`describe`: searches objects given some parameters (location, dates, names,
    and so on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get`: given an object ID, returns information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update`: given an object ID, updates the object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delete`: deletes an object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These can be performed on the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: datasource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create-data-source-from-rds`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create-data-source-from-redshift`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create-data-source-from-s3`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`describe-data-sources`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delete-data-source`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get-data-source`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update-data-source`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ml-model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create-ml-model`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`describe-ml-models`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get-ml-model`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delete-ml-model`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update-ml-model`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create-evaluation`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`describe-evaluations`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get-evaluation`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delete-evaluation`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update-evaluation`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: batch prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create-batch-prediction`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`describe-batch-predictions`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get-batch-prediction`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delete-batch-prediction`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update-batch-prediction`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: real-time end point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create-realtime-endpoint`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delete-realtime-endpoint`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predict`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also handle tags and set waiting times.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the AWS CLI gives you the ability to create datasources from S3, Redshift,
    and RDS, while the web interface only allowed datasources from S3 and Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the datasource
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start by creating the datasource. Let''s first see what parameters
    are needed by generating the following skeleton:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following JSON object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The different parameters are mostly self-explanatory and further information
    can be found on the AWS documentation at [http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-s3.html](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-s3.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'A word on the schema: when creating a datasource from the web interface, you
    have the possibility to use a wizard, to be guided through the creation of the
    schema. As you may recall, you are guided through several screens where you can
    specify the type of all the columns, and the existence of a target variable and
    an index column. The wizard facilitates the process by guessing the type of the
    variables, thus making available a default schema that you can modify.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no default schema available via the AWS CLI. You have to define the
    entire schema yourself, either in a JSON format in the `DataSchema` field or by
    uploading a schema file to S3 and specifying its location, in the `DataSchemaLocationS3`
    field.
  prefs: []
  type: TYPE_NORMAL
- en: Since our dataset has many variables (79), we cheated and used the wizard to
    create a default schema that we uploaded to S3\. Throughout the rest of the chapter,
    we will specify the schema location not its JSON definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will create the following datasource parameter file, `dsrc_ames_housing_001.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'For the validation subset (save to `dsrc_ames_housing_002.json`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Since we have already split our data into a training and a validation set, there's
    no need to specify the data `DataRearrangement` field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we could also have avoided splitting our dataset and specified
    the following `DataRearrangement` on the original dataset, assuming it had been
    already shuffled: (save to `dsrc_ames_housing_003.json`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For the validation set (save to `dsrc_ames_housing_004.json`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `ames_housing.csv` file has previously been shuffled using the `gshuf`
    command line and uploaded to S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note that we don't need to create these four datasources; these are just examples
    of alternative ways to create datasources.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create these datasources by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check whether the datasource creation is pending:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In return, we get the datasoure ID we had specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then obtain information on that datasource with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have access to the operation log URI, which could be useful to
    analyze the model training later on.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating the model with the `create-ml-model` command follows the same steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate the skeleton with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the configuration file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note the parameters of the algorithm. Here, we used mild L2 regularization and
    100 passes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the model creation with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The model ID is returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This `get-ml-model` command gives you a status update on the operation as well
    as the URL to the log.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `watch` command allows you to repeat a shell command every *n* seconds.
    To get the status of the model creation every *10s*, just write the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The output of the `get-ml-model` will be refreshed every 10s until you kill
    it.
  prefs: []
  type: TYPE_NORMAL
- en: It is not possible to create the default recipe via the AWS CLI commands. You
    can always define a blank recipe that would not carry out any transformation on
    the data. However, the default recipe has been shown to be positively impacting
    the model performance. To obtain this default recipe, we created it via the web
    interface, copied it into a file that we uploaded to S3\. The resulting file `recipe_ames_housing_001.json`
    is available in our GitHub repository. Its content is quite long as the dataset
    has 79 variables and is not reproduced here for brevity purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating our model with create-evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our model is now trained and we would like to evaluate it on the evaluation
    subset. For that, we will use the `create-evaluation` CLI command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate the skeleton:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the parameter file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch the evaluation creation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the evaluation information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'From that output, we get the performance of the model in the form of the RMSE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The value may seem big, but it is relative to the range of the `salePrice` variable
    for the houses, which has a mean of 181300.0 and std of 79886.7\. So an RMSE of
    29853.2 is a decent score.
  prefs: []
  type: TYPE_NORMAL
- en: You don't have to wait for the datasource creation to be completed in order
    to launch the model training. Amazon ML will simply wait for the parent operation
    to conclude before launching the dependent one. This makes chaining operations
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: The next step would be to make batch predictions or create a real-time endpoint.
    These would follow the exact same steps of model creation and evaluation, and
    are not presented here.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have a trained and evaluated model. We chose a certain set
    of parameters and carried out a certain preprocessing of the data via the default
    recipe. We now would like to know whether we can improve on that model and feature
    set by trying new parameters for the algorithm and doing some creative feature
    engineering. We will then train our new models and evaluate them on the validation
    subset. As we've seen before, the problem with that approach is that our evaluation
    score can be highly dependent on the evaluation subset. Shuffling the data to
    generate new training and validation sets may result in different model performance
    and make us choose the wrong model. Even though we have shuffled the data to avoid
    sequential patterns, there is no way to be sure that our split is truly neutral
    and that both subsets show similar data distribution. One of the subsets could
    present anomalies such as outliers, or missing data that the other does not have.
    To solve this problem, we turn to cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: What is cross-validation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To lower the dependence on the data distribution in each split, the idea is
    to run many trials in parallel, each with a different data split, and average
    the results. This is called cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is simply to average the model performance across K trials, where each
    trial is built on a different split of the original dataset. There are many strategies
    to split the dataset. The most common one is called **k-fold cross-validation**
    and consists of splitting the dataset into **K chunks**, and for each trial using
    *K-1* chunks aggregated to train the model and the remaining chunk to evaluate
    it. Another strategy, called **leave-one-out** (**LOO**), comes from taking this
    idea to its extreme with *K* as the number of samples. You train your model on
    all the samples except one and estimate the error on the remaining sample. LOO
    is obviously more resource intensive.
  prefs: []
  type: TYPE_NORMAL
- en: The strategy we will implement is called **Monte Carlo cross-validation**, where
    the initial dataset is randomly split into a training and validation set in each
    trial. The advantage of that method over k-fold cross validation is that the proportion
    of the training/validation split is not dependent on the number of iterations
    (*K*). Its disadvantage is that some samples may never be selected in the validation
    subset, whereas others may be selected more than once. Validation subsets may
    overlap.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example with `k =5` trials. We will repeat these steps five
    times to evaluate one model (for instance, L2 mild regularization):'
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle the Ames Housing dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the dataset into training and validation subsets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model on the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model on the validation set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, we have five measures of the model performance; we average it
    to get a measure of overall model performance. We repeat the aforementioned five
    steps to evaluate another model (for instance, L1 medium regularization). Once
    we have tested all our models, we select the one that gives the best average performance
    on the trials.
  prefs: []
  type: TYPE_NORMAL
- en: This is why scripting becomes a necessity. To test one model setup, a cross-validation
    with *K trials* (K fold or Monte Carlo) requires `2*K` datasources, K models,
    and K evaluations. This will surely be too time-consuming when done via the web
    interface alone. This is where scripting the whole process becomes extremely useful
    and much more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to actually create the different subset files for cross-validation.
    The simplest way might be to use a spreadsheet editor with random sorting, and
    some cutting and pasting. R and Python libraries, such as the popular `scikit-learn`
    library or the Caret package, have rich methods that can be used out of the box.
    However, since this chapter is about the AWS command line interface, we will use
    shell commands to generate the files. We will also write shell scripts to generate
    the sequence of AWS CLI commands in order to avoid manually editing the same commands
    for the different data files and models.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Monte Carlo cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now implement a Monte Carlo cross-validation strategy with five trials
    using shell commands and AWS CLI. And we will use this evaluation method to compare
    two models, one with L2 mild regularization and the second with L1 heavy regularization
    on the Ames Housing dataset. Cross-validation will allow us to conclude with some
    level of confidence which model performs better.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the shuffled datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the datasource creation `DataRearrangement` field to split the data
    into a training and a validation subset. So, we only need to create five files
    of shuffled data in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shell script will create five shuffled versions of the `Ames
    Housing` dataset and upload the files to S3\. You can either save that code in
    a file with the `.sh` extension (`datasets_creation.sh`) or run it with `sh ./datasets_creation.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in this chapter, the code is organized around the following folder
    structure. All the command lines are run from the root folder, for instance, to
    run a Python script: `python py/the_script.py`, to list the data files `ls data/`
    and to run shell scripts: `sh ./shell/the_script.sh`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`├── data`'
  prefs: []
  type: TYPE_NORMAL
- en: '`├── images`'
  prefs: []
  type: TYPE_NORMAL
- en: '`├── py`'
  prefs: []
  type: TYPE_NORMAL
- en: '`└── shell` All the shell scripts and command are based on bash shell and should
    probably require adaptation to other shells such as zsh.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our datasets have been created and uploaded to S3\. The general strategy is
    now to create templates for each of the parameter JSON files required for the
    Amazon ML CLI commands: create datasources, models, and evaluations. We will create
    the template files for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Training datasource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation datasource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In all these template files, we will index the filenames with `{k}` and use
    the `sed` shell command to replace `{k}` with the proper index (1 to 5). Once
    we have the template files, we can use a simple shell script to generate the actual
    JSON parameter files for the datasources, models, and evaluations. We will end
    up with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 10 datasource configuration files (five for training and five for evaluation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 model configuration files (five for L2 and five for L1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 evaluation configuration files (one for each of the models)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the end, we will obtain five RMSE results for the L2 model and five RMSE
    results for the L1 model, whose average will tell us which model is the best,
    which type of regularization should be selected to make sales price predictions
    on the Ames Housing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by writing the configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the datasources template
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The template for the training files is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'And the template for for validation datasources is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The only different between the training and the validation templates are the
    names/IDs and the splitting ratio in the `DataRearrangement` field. We save these
    files to `dsrc_training_template.json` and `dsrc_validate_template.json` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the models template
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of a model with L2 regularization, the model template is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'And for a model with L1 regularization, the model template is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Note that the same recipe is used for both models. If we wanted to compare the
    performance of data preprocessing strategies, we could modify the recipes used
    in both models. The template files are very similar. The only difference is in
    the model name and ID and in the values for the `l1RegularizationAmount` and `l2RegularizationAmount`.
    We save these files to `mdl_l2_template.json` and `mdl_l1_template.json` respectively**.**
  prefs: []
  type: TYPE_NORMAL
- en: Generating the evaluations template
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of a model with L2 regularization, the evaluation template is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'And for a model with L1 regularization, the evaluation template is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Save these files to `eval_l2_template.json` and `eval_l1_template.json` espectively.
  prefs: []
  type: TYPE_NORMAL
- en: We will now use these template files to generate the different configuration
    files for the datasources, models, and evaluations. To keep things separate, all
    the generated files are in a subfolder `cfg/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shell script generates the actual configuration files that we
    will feed to the AWS CLI Machine Learning commands. It uses the `sed` command
    to find and replace the instances of `{k}` with the numbers 1 to 5\. The output
    is written to the configuration file. Since there will be many configuration files
    generated, the files are written in a `/cfg` subfolder under `/data`. The folder
    structure is now as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The last remaining step is to execute the AWS commands that will create the
    objects in Amazon ML. We also use a shell loop to execute the AWS CLI commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create datasources for training and evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Train models with L2 and L1 regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate trained models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check the status of the different jobs with the `get-data-source`,
    `get-ml-model` and `get-evaluation` CLI commands or on the Amazon ML dashboard.
    Once all the evaluation is finished, you capture the RMSE for each model by first
    creating a couple of files to receive the RMSE score and then running the following,
    final shell loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The `get-evaluation` command, given the ID of the evaluation, returns a JSON-formatted
    string that is fed to a grepping command and added to the `l1/l2_model_rmse.log`
    files.
  prefs: []
  type: TYPE_NORMAL
- en: The results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We end up with the following results for the two models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'On average, L1 gives an RMSE of 28075.2 (std: 1151), while L2 gives an RMSE
    of 29176.4 (std: 4246.7). Not only is the L1 model better performing, but it is
    also more robust when it comes to handling data variations since its std is lower.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-validation may be too time-consuming to implement via shell only. There
    are many files to create and coordinate. There are simpler ways to implement cross-validation
    with libraries such as `scikit-learn` for Python or Caret for R, where the whole
    model training and evaluation loop over several training and validation sets only
    requires a few lines of code. However, we showed that it was possible to implement
    cross-validation with Amazon ML. Cross-validation is a key component of the data-science
    workflow. Not being able to do cross validation with Amazon ML would have been
    a significant flaw in the service. In the end, the AWS CLI for machine learning
    is a very powerful and useful tool to conduct sequences of trials and compare
    results across different models, datasets, recipes, and features.
  prefs: []
  type: TYPE_NORMAL
- en: Boto3, the Python SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another tool to interact with the Amazon ML service outside of the web interface
    is an SDK. Simply put, an SDK is a wrapper around an API that makes working with
    the service much simpler and more efficient, as many details of the interactions
    are taken care of. AWS offers SDKs in the most widespread languages such as PHP,
    Java, Ruby, .Net, and of course, Python. In this chapter, we will focus on working
    with the Amazon ML service through the Python SDK. The Python SDK requires the
    Boto3 module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Installation of the Boto3 module is done via pip. Refer to the quickstart guide
    available at [http://boto3.readthedocs.io/en/latest/guide/quickstart.html](http://boto3.readthedocs.io/en/latest/guide/quickstart.html)
    if you need more information and troubleshooting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Boto3 is available for most AWS services. The complete list can be found at
    [http://boto3.readthedocs.io/en/latest/reference/services/index.html](http://boto3.readthedocs.io/en/latest/reference/services/index.html).
    We will focus on Boto3 for S3 and Amazon ML.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up permissions for SDK access can be done via the `aws configure` command
    that we followed at the beginning of this chapter, or directly by adding your
    access keys to the `~/.aws/credentials` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the `Boto3` logic is very similar to the AWS CLI logic and follows
    similar steps: declaring the service to be used and running commands with the
    appropriate set of parameters. Let''s start with a simple example around S3 with
    the following Python script, which will list all the buckets in your account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Uploading a local file to a bucket would be achieved by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The put command returns a JSON string, with an HTTPStatusCode field with a 200
    value, indicating that the upload was successful.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the Python SDK for Amazon Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The list of available methods can be found at [http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html](http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html)
    and closely follows the list of available commands for the AWS CLI for the Machine
    Learning service organized around the main objects: datasource, model, evaluation,
    batch prediction, and real-time endpoints. For each object, the methods are: create,
    update, describe, get, and delete.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now implement the standard Amazon ML workflow. But first, let''s define
    a naming method for the objects we will create. An important part of the workflow
    revolves around naming convention for object names and IDs. When working with
    the CLI, we created the names and IDs on the fly. This time we will use the following
    function to name our objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: This function takes in two strings and one integer as arguments, a prefix for
    the type of the object (datasource, model, and so on), a mode to specify training
    versus validation datasource, and a trial value to easily increment our experiments.
    The function returns a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now define a few variables that we will use later on in the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to import the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare that we want to interact with the Machine Learning service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now all set to create our training and validation datasources with the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: In both cases, we call on the naming function we defined earlier to generate
    the Name and ID of the datasource and use that dictionary when calling the `create_data_source_from_s3` Boto3
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We launch the training of the model with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'And create the evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now go to the Amazon ML dashboard and verify that you have two datasources,
    one model, and one evaluation in the In progress or Pending status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Waiting on operation completion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All these object, creation operations are, by default, chained by Amazon ML.
    This means that Amazon ML will wait on the datasources to be ready before launching
    the model training, and will also wait for the model training to be completed
    before trying to run the evaluation. However, at this point, we still need to
    wait for the evaluation to be complete before we can access its results. Similarly,
    we need to wait for the different objects to have been utilized by the next operation
    before deleting them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where the waiter methods become useful. Waiters are methods that simply
    wait for an AWS operation to be completed, to have status *Completed.* Waiters
    exists for all AWS operations and services. Amazon ML offers four waiters for
    models, datasources, evaluations, and batch predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MachineLearning.Waiter.BatchPredictionAvailable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MachineLearning.Waiter.DataSourceAvailable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MachineLearning.Waiter.EvaluationAvailable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MachineLearning.Waiter.MLModelAvailable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Machine Learning waiter follows the syntax – first, declare the object the
    waiter has to monitor, for instance an evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Then call the `wait` method on the waiter you just declared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the wait method is called, the Python scripts hangs until the operation
    reaches a status of `Completed`. The wait function takes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A filter value: `FilterVariable = CreatedAt`, `LastUpdatedAt`, `Status`, `Name`,
    `IAMUser`, `MLModelId`, `DataSourceId`, `DataURI`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An operator: EQ, GT, LT, GE, LE, NE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other parameters that depend on the nature of the object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With that parameter structure, you can make your script wait on a specific
    object completion, or wait on all the objects based on a datasource, a model,
    or even a user name. If we were to launch several evaluations for different models
    based on the same validation datasource, we would simply call a waiter for each
    model as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Wrapping up the Python-based workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how to wait for all our evaluation to finish, we still need
    to get the evaluation result and delete the models and datasources we have created.
    As in the case of the `get-evaluation` AWS CLI command, the Boto3 `get_evaluation`
    method returns a JSON string with the model performance measure, the RMSE in the
    case of regression. The following script wraps up our trial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting all the code blocks together returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Implementing recursive feature selection with Boto3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many real-world cases, the original dataset could have a very large number
    of variables. As dimensionality increases, so does the need for a larger sample
    set. This is called the *curse of dimensionality*, a classic predictive analytics
    problem. Simply put, if there is not enough diversity to infer a representative
    distribution for some variables, the algorithm will be unable to extract relevant
    information from the said variables. These low-signal variables drag down the
    algorithm's performance without adding any data fuel by adding useless complexity
    to the model. One strategy is to reduce the number of variables on which to train
    the model. However, that implies identifying which features can be dropped without
    significant loss of information.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many techniques to reduce the number of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Wrapper** techniques: These use rules and criteria to select the best and
    most impacting features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filter** techniques: These use statistical tests to measure the importance
    of each feature. Measuring the correlation with the target could be a simple way
    to remove non-significant variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedded** methods: For certain models, such as random forests, iteratively
    train the model on subsets of features, it is possible to evaluate the impact
    of the features that are left out during each iteration and thus infer the importance
    of each feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method most adapted to the Amazon Machine Learning context is the recursive
    evaluation of each feature's importance, filtering out the least important ones by
    measuring when performance drops significantly with the discarding of a certain
    feature. It is a brute force version of Recursive Feature Elimination.
  prefs: []
  type: TYPE_NORMAL
- en: 'It follows these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Build an initial model with all *N* features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then identify and remove the least important features by:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building N subsets, removing a different feature in each subset
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a model for each subset and evaluating its performance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the features for which the model's performance was least impacted
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You now have *N-1* features. Reiterate steps 1 to 3 to identify and remove the
    next least important feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop when you notice a significant drop in the model's performance compared
    to the initial N-feature model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The inverse version of this algorithm starts with *N* models, each built with
    just one feature, with a new feature added at each iteration. Choose the new feature
    as the new feature that generates the best increase in performance. Stop when
    adding new features does not lead to a significant increase in the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of the chapter, we will show how to implement this feature selection
    strategy in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Managing schema and recipe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Removing or adding features to a dataset directly impacts the schema and the
    recipe. The schema is used when creating the datasources, while the recipe is
    needed to train the model, as it specifies which data transformation will be performed
    prior to the model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modifying the schema to remove features from the dataset can be done by simply
    adding the names of the variable to the `excludedAttributeNames` field. We can
    take the initial schema, and each time we remove a feature from the initial feature
    list, we add it to the excludedAttributeNames list. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the JSON formatted schema into a schema dict
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append the feature name to schema [`'excludedAttributeNames'`]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the schema to a properly indented JSON file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the file to S3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When creating the datasource, we will point to the S3 location of the schema
    we just updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial recipe generated by default by Amazon ML for the Ames Housing dataset
    applies different quantile binning transformations on certain numerical features.
    The groups section of the recipe is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding or removing variable names from that structure requires a more complex
    script than just adding an element in a list. Since such a script would not add
    much educational value to the book, we decided to use a default simple recipe
    that does not perform any transformation on the dataset. As long as we have a
    baseline RMSE with all the features available, the recursive feature elimination
    strategy is still valid. The only difference is that the overall RMSE scores will
    probably be made higher by not applying any quantile binning to our numerical
    data. The recipe we use is defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: This is available in our examples at the S3 location `s3://aml.packt/data/ch8/recipe_ames_housing_default.json`.
    Using that recipe to evaluate our baseline model gives a baseline RMSE of *61507.35.*
    We will use that baseline RMSE to see whether removing a feature improved (lower)
    or degraded (higher) the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following script is broken into three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization and functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launching the Amazon ML workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the evaluation results and deleting the resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The script is available in our GitHub repo in its entirety. We use the same
    strategy to have a function to generate the names and IDs. We start with the following
    script to initialize the variable and declare the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: We now launch the datasource, model, and evaluation creation. The script only
    looks at the first 10 features, and not the entire set of 79 features, to save
    on resources.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that we added a prefix "*X"* to the numbering of the Amazon
    ML objects. We found that sometimes, Amazon ML cannot create an object if the
    IDs and names have been used on previous objects that have now been deleted. That
    problem may disappear after some time. In any case, making sure that all new datasources,
    models, evaluations, and batch predictions have names and IDs that have never
    been used previously removes any naming issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second part launches the creation of the datasources, models, and evaluations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the third part waits for the evaluation to be complete, records the
    RMSE for each removed feature, and deletes the datasources and models (we kept
    the evaluations to avoid having to rerun the whole script to get the results):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'In the end, we get the following RMSE variations  for the first 10 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`1stFlrSF 0.07%`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2ndFlrSF -18.28%`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BedroomAbvGr -0.02 %`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BsmtExposure -0.56 %`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BsmtFinSF2 -0.50 %`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BsmtCond 0.00 %`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BsmtFinSF1 -2.56 %`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Alley 0.00 %`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`3SsnPorch -4.60 %`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BldgType -0.00 %`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, removing the `2ndFlrSF` feature increased the RMSE by nearly 20%.
    This feature is definitely very important to predict salesPrice. Similarly, features
    `3SsnPorch` and `BsmtFinSF1` are also important to the model, since removing them
    increases the RMSE. On the other hand, removing `1stFlrSF`, `Alley`*,* `BedroomAbvGr`or`BldgType` only
    modified the RMSE by less than 0.10%. We can probably remove these feature without
    too much impact on the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have moved away from the Amazon ML web interface and learned
    how to work with the service through the AWS CLI and the Python SDK. The commands
    and methods for both types of interaction are very similar. The functions and
    commands perform a standard set of operations from creation to deletion of Amazon
    ML objects: datasources, models, evaluation, and batch predictions. The fact that
    Amazon ML chains the sequence of dependent object creation allows you to create
    all the objects at once without having to wait for one upstream to finish (datasource
    or model) before creating the downstream one (model or evaluation). The waiter
    methods make it possible to wait for all evaluations to be completed before retrieving
    the results and making the necessary object deletion.'
  prefs: []
  type: TYPE_NORMAL
- en: We showed how scripting Amazon ML allowed us to implement Machine Learning methods
    such as cross-validation and Recursive Feature Selection, both very useful methods
    in predictive analytics. Although we end up having to create many datasources
    and objects to conduct cross-validation and feature selection, the overall costs
    remain under control.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start using other AWS services to expand the capabilities
    of Amazon ML. We will look at other datasources beyond S3, such as Redshift and
    RDS, and how to use Amazon Lambda for machine learning.
  prefs: []
  type: TYPE_NORMAL
