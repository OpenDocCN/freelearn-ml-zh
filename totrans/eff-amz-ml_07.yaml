- en: Command Line and SDK
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命令行和 SDK
- en: Using the AWS web interface to manage and run your projects is time-consuming.
    In this chapter, we move away from the web interface and start running our projects
    via the command line with the **AWS Command Line Interface** (**AWS CLI**) and
    the Python SDK with the `Boto3` library.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS 网页界面来管理和运行项目是耗时的。在本章中，我们将远离网页界面，开始通过命令行使用 **AWS 命令行界面**（**AWS CLI**）和
    Python SDK 的 `Boto3` 库来运行我们的项目。
- en: The first step will be to drive a whole project via the AWS CLI, uploading files
    to S3, creating datasources, models, evaluations, and predictions. As you will
    see, scripting will greatly facilitate using Amazon ML. We will use these new
    abilities to expand our Data Science powers by carrying out cross-validation and
    feature selection.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步将是通过 AWS CLI 驱动整个项目，包括上传文件到 S3、创建数据源、模型、评估和预测。正如您将看到的，脚本将极大地简化使用 Amazon ML。我们将利用这些新能力通过执行交叉验证和特征选择来扩展我们的数据科学能力。
- en: 'So far we have split our original dataset into three data chunks: training,
    validation, and testing. However, we have seen that the model selection can be
    strongly dependent on the data split. Shuffle the data — a different model might
    come as being the best one. Cross-validation is a technique that reduces this
    dependency by averaging the model performance on several data splits. Cross-validation
    involves creating many datasources for training, validation, and testing, and
    would be time-consuming using the web interface. The AWS CLI will allow us to
    quickly spin new datasources and models and carry out cross-validation effectively.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将原始数据集分为三个数据块：训练、验证和测试。然而，我们已经看到模型选择可能强烈依赖于数据分割。打乱数据——可能不同的模型会被认为是最好的。交叉验证是一种通过平均模型在多个数据分割上的性能来减少这种依赖性的技术。交叉验证涉及创建许多用于训练、验证和测试的数据源，使用网页界面将会耗时。AWS
    CLI 将使我们能够快速创建新的数据源和模型，并有效地执行交叉验证。
- en: Another important technique in data science is feature elimination. Having a
    large number of features in your dataset either as the results of intensive feature
    engineering or because they are present in the original dataset can impact the
    model's performance. It's possible to significantly improve the model prediction
    capabilities by selecting and retaining only the best and most meaningful features
    while rejecting less important ones. There are many feature selection methods.
    We will implement a simple and efficient one, called recursive feature selection.
    The AWS Python SDK accessible via the Boto3 library will allow us to build the
    code wrapping around Amazon ML required for recursive feature selection.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中的另一个重要技术是特征消除。在数据集中拥有大量特征，无论是由于密集的特征工程结果还是因为它们存在于原始数据集中，都可能影响模型的表现。通过选择和保留最佳且最有意义的特征，同时拒绝不那么重要的特征，可以显著提高模型的预测能力。有许多特征选择方法。我们将实现一个简单而高效的方法，称为递归特征选择。通过
    Boto3 库可访问的 AWS Python SDK 将使我们能够构建围绕 Amazon ML 的代码，以实现递归特征选择。
- en: 'In this chapter, you will learn the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下内容：
- en: 'How to handle a whole project workflow through the AWS command line and the
    AWS Python SDK:'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过 AWS 命令行和 AWS Python SDK 处理整个项目工作流程：
- en: Managing data uploads to S3
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理数据上传到 S3
- en: Creating and evaluating models
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和评估模型
- en: Making and exporting the predictions
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制作和导出预测
- en: How to implement cross-validation with the AWS CLI
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 AWS CLI 实现交叉验证
- en: How to implement Recursive Feature Selection with AWS the Python SDK
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 AWS Python SDK 实现递归特征选择
- en: Getting started and setting up
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用和设置
- en: Creating a performing predictive model from raw data requires many trials and
    errors, much back and forth. Creating new features, cleaning up data, and trying
    out new parameters for the model are needed to ensure the robustness of the model.
    There is a constant back and forth between the data, the models, and the evaluations.
    Scripting this workflow either via the AWS CLI or with the `Boto3` Python library,
    will give us the ability to speed up the create, test, select loop.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始数据创建一个表现良好的预测模型需要许多尝试和错误，很多来回调整。创建新特征、清理数据以及尝试为模型设定新参数都是确保模型稳健性的必要步骤。在数据、模型和评估之间需要不断的来回调整。通过
    AWS CLI 或 `Boto3` Python 库脚本化这个工作流程，将使我们能够加快创建、测试、选择的循环。
- en: Using the CLI versus SDK
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CLI 与 SDK 的比较
- en: AWS offers several ways besides the UI to interact with its services, the CLI,
    APIs, and SDKs in several languages. Though the AWS CLI and SDKs do not include
    all AWS services. Athena SQL, for instance, being a new service, is not yet included
    in the AWS CLI module or in any of AWS SDK at the time of writing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了多种方式，除了 UI 之外，还可以与它的服务进行交互，包括 CLI、API 和多种语言的 SDK。尽管 AWS CLI 和 SDKs 并不包含所有
    AWS 服务。例如，Athena SQL 是一项新服务，在撰写本文时，它尚未包含在 AWS CLI 模块或任何 AWS SDK 中。
- en: The AWS Command Line Interface or CLI is a command-line shell program that allows
    you to manage your AWS services from your shell terminal. Once installed and set
    up with proper permissions, you can write commands to manage your S3 files, AWS
    EC2 instances, Amazon ML models, and most AWS services.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 命令行界面（CLI）是一个命令行外壳程序，允许您从您的 shell 终端管理您的 AWS 服务。一旦安装并设置了适当的权限，您就可以编写命令来管理您的
    S3 文件、AWS EC2 实例、Amazon ML 模型以及大多数 AWS 服务。
- en: Generally speaking, a software development kit*,* or SDK for short, is a set
    of tools that can be used to develop software applications targeting a specific
    platform. In short, the SDK is a wrapper around an API. Where an API holds the
    core interaction methods, the SDK includes debugging support, documentation, and
    higher-level functions and methods. The API can be seen as the lowest common denominator
    that AWS supports and the SDK as a higher-level implementation of the API.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，软件开发工具包（SDK），简称 SDK，是一组可用于开发针对特定平台的应用软件的工具。简而言之，SDK 是 API 的包装器。API 包含核心交互方法，而
    SDK 包括调试支持、文档以及高级函数和方法。API 可以被视为 AWS 支持的最低公共基数，而 SDK 则是 API 的高级实现。
- en: AWS SDKs are available in 12 different languages including PHP, Java, Ruby,
    and .NET. In this chapter, we will use the Python SDK.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SDKs 可用 12 种不同的语言，包括 PHP、Java、Ruby 和 .NET。在本章中，我们将使用 Python SDK。
- en: Using the AWS CLI or SDK requires setting up our credentials, which we'll do
    in the following section
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS CLI 或 SDK 需要设置我们的凭证，我们将在下一节中完成此操作。
- en: Installing AWS CLI
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 AWS CLI
- en: In order to set up your CLI credentials, you need your access key ID and your
    secret access key. You have most probably downloaded and saved them in a previous
    chapter. If that's not the case, you should simply create new ones from the **IAM**
    console ([https://console.aws.amazon.com/iam](https://console.aws.amazon.com/iam)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置您的 CLI 凭证，您需要您的访问密钥 ID 和您的秘密访问密钥。您很可能在之前的章节中下载并保存了它们。如果不是这样，您应该简单地从 **IAM**
    控制台（[https://console.aws.amazon.com/iam](https://console.aws.amazon.com/iam)）创建新的凭证。
- en: Navigate to Users, select your IAM user name and click on the Security credentials
    tab. Choose Create Access Key and download the CSV file. Store the keys in a secure
    location. We will need the key in a few minutes to set up AWS CLI. But first,
    we need to install AWS CLI.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到“用户”，选择您的 IAM 用户名，然后点击“安全凭证”选项卡。选择创建访问密钥并下载 CSV 文件。将密钥存储在安全位置。我们将在几分钟内需要该密钥来设置
    AWS CLI。但首先，我们需要安装 AWS CLI。
- en: '**Docker environment** – This tutorial will help you use the AWS CLI within
    a docker container: [https://blog.flowlog-stats.com/2016/05/03/aws-cli-in-a-docker-container/](https://blog.flowlog-stats.com/2016/05/03/aws-cli-in-a-docker-container/).
    A docker image for running the AWS CLI is available at [https://hub.docker.com/r/fstab/aws-cli/](https://hub.docker.com/r/fstab/aws-cli/).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker 环境** – 本教程将帮助您在 Docker 容器中使用 AWS CLI：[https://blog.flowlog-stats.com/2016/05/03/aws-cli-in-a-docker-container/](https://blog.flowlog-stats.com/2016/05/03/aws-cli-in-a-docker-container/)。运行
    AWS CLI 的 Docker 镜像可在 [https://hub.docker.com/r/fstab/aws-cli/](https://hub.docker.com/r/fstab/aws-cli/)
    找到。'
- en: There is no need to rewrite the AWS documentation on how to install the AWS
    CLI. It is complete and up to date, and available at [http://docs.aws.amazon.com/cli/latest/userguide/installing.html](http://docs.aws.amazon.com/cli/latest/userguide/installing.html).
    In a nutshell, installing the CLI requires you to have Python and `pip` already
    installed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 没有必要重写 AWS CLI 的安装文档。它是完整且最新的，可在 [http://docs.aws.amazon.com/cli/latest/userguide/installing.html](http://docs.aws.amazon.com/cli/latest/userguide/installing.html)
    找到。简而言之，安装 CLI 需要您已经安装了 Python 和 `pip`。
- en: 'Then, run the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行以下命令：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Add AWS to your `$PATH`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 将 AWS 添加到您的 `$PATH`：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Reload the bash configuration file (this is for OSX):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 重新加载 bash 配置文件（这是针对 OSX 的）：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Check that everything works with the following command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请使用以下命令检查一切是否正常工作：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should see something similar to the following output:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到以下类似的输出：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once installed, we need to configure the AWS CLI type:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，我们需要配置 AWS CLI 类型：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now input the access keys you just created:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在输入您刚刚创建的访问密钥：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Choose the region that is closest to you and the format you prefer (JSON, text,
    or table). JSON is the default format.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 选择离您最近且您喜欢的格式（JSON、文本或表格）。默认格式是JSON。
- en: 'The AWS configure command creates two files: a `config` file and a credential
    file. On OSX, the files are `~/.aws/config` and `~/.aws/credentials`. You can
    directly edit these files to change your access or configuration. You will need
    to create different profiles if you need to access multiple AWS accounts. You
    can do so via the AWS configure command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: AWS configure命令创建两个文件：一个`config`文件和一个凭证文件。在OSX上，这些文件是`~/.aws/config`和`~/.aws/credentials`。您可以直接编辑这些文件来更改您的访问或配置。如果您需要访问多个AWS账户，您需要创建不同的配置文件。您可以通过AWS
    configure命令这样做：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can also do so directly in the `config` and `credential` files:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以直接在`config`和`credential`文件中这样做：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can edit `Credential` file as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下方式编辑`Credential`文件：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Refer to the AWS CLI setup page for more in-depth information:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅AWS CLI设置页面以获取更深入的信息：
- en: '[http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html)'
- en: Picking up CLI syntax
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习CLI语法
- en: 'The overall format of any AWS CLI command is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 任何AWS CLI命令的整体格式如下：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here the terms are stated as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里术语如下所述：
- en: '`<service>`: Is the name of the service you are managing: S3, machine learning,
    and EC2'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<service>`：您正在管理的服务的名称：S3、机器学习和EC2'
- en: '`[options]` : Allows you to set the region, the profile, and the output of
    the command'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[options]`：允许您设置区域、配置文件和命令的输出'
- en: '`<command> <subcommand>`: Is the actual command you want to execute'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<command> <subcommand>`：这是您想要执行的真正命令'
- en: '`[parameters]` : Are the parameters for these commands'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[parameters]`：是这些命令的参数'
- en: 'A simple example will help you understand the syntax better. To list the content
    of an S3 bucket named `aml.packt`*,* the command is as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的例子将帮助您更好地理解语法。要列出名为`aml.packt`*的S3存储桶的内容，命令如下：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, `s3` is the service, `ls` is the command, and `aml.packt` is the parameter.
    The `aws help` command will output a list of all available services.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`s3`是服务，`ls`是命令，`aml.packt`是参数。`aws help`命令将输出所有可用服务的列表。
- en: 'To get help on a particular service and its commands, write the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取特定服务及其命令的帮助，请编写以下内容：
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For instance, `aws s3 help` will inform you that the available `s3` commands
    on single objects are ls, mv, and rm for list, move, and remove, and that the
    basic `aws s3` command follows the following format:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`aws s3 help`将通知您，在单个对象上可用的`s3`命令有ls、mv和rm，用于列出、移动和删除，并且基本的`aws s3`命令遵循以下格式：
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here, `sourceURI` or `destinationURI` can be a file (or multiple files) on
    your local machine and a file on S3 or both files on S3\. Take the following,
    for instance:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`sourceURI`或`destinationURI`可以是您本地机器上的文件（或多个文件）和S3上的文件，也可以是S3上的两个文件。以下是一个例子：
- en: '[PRE14]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This will copy all (thanks to the parameter — recursive) JPG files (and only
    `*.jpg` files) in the `/tmp/foo` folder on your local machine to the S3 bucket
    named `The_Bucket`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这将复制您本地机器上`/tmp/foo`文件夹中的所有（多亏了参数——递归）JPG文件（仅`*.jpg`文件）到名为`The_Bucket`的S3存储桶。
- en: There are many more examples and explanations on the AWS documentation available
    at
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: AWS文档中有更多示例和解释，可在以下链接找到：
- en: '[http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html)。'
- en: Passing parameters using JSON files
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用JSON文件传递参数
- en: For some services and commands, the list of parameters can become long and difficult
    to check and maintain.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些服务和命令，参数列表可能会变得很长，难以检查和维护。
- en: 'For instance, in order to create an Amazon ML model via the CLI, you need to
    specify at least seven different elements: the Model ID, name, type, the model''s
    parameters, the ID of the training data source, and the recipe name and URI (`aws
    machinelearning create-ml-model help` ).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要通过命令行界面（CLI）创建一个Amazon ML模型，您至少需要指定七个不同的元素：模型ID、名称、类型、模型的参数、训练数据源的ID以及食谱名称和URI（`aws
    machinelearning create-ml-model help`）。
- en: 'When possible, we will use the CLI ability to read parameters from a JSON file
    instead of specifying them in the command line. AWS CLI also offers a way to generate
    a JSON template, which you can then use with the right parameters. To generate
    that JSON parameter file model (the JSON skeleton), simply add `--generate-cli-skeleton`
    after the command name. For instance, to generate the JSON skeleton for the create
    model command of the machine learning service, write the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，我们将使用 CLI 功能从 JSON 文件中读取参数，而不是在命令行中指定它们。AWS CLI 还提供了一种生成 JSON 模板的方法，您可以使用正确的参数使用该模板。要生成该
    JSON 参数文件模型（JSON 框架），只需在命令名称后添加 `--generate-cli-skeleton`。例如，要生成机器学习服务创建模型命令的
    JSON 框架，请编写以下内容：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will give the following output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can then configure this to your liking.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据自己的喜好进行配置。
- en: 'To have the skeleton command generate a JSON file and not simply output the
    skeleton in the terminal, add `> filename.json`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要使骨架命令生成 JSON 文件而不是在终端中简单地输出骨架，请添加 `> filename.json`：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will create a `filename.json` file with the JSON template. Once all the
    required parameters are specified, you create the model with the command (assuming
    the `filename.json` is in the current folder):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个包含 JSON 模板的 `filename.json` 文件。一旦指定了所有必需的参数，您就可以使用以下命令创建模型（假设 `filename.json`
    在当前文件夹中）：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Before we dive further into the machine learning workflow via the CLI, we need
    to introduce the dataset we will be using in this chapter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步通过 CLI 深入探讨机器学习工作流程之前，我们需要介绍本章将使用的数据集。
- en: Introducing the Ames Housing dataset
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 `Ames Housing` 数据集
- en: 'In this chapter, we will use the `Ames Housing` dataset that was compiled by
    *Dean De Cock* for use in data science education. It is a great alternative to
    the popular but older `Boston Housing` dataset. The `Ames Housing` dataset is
    used in the Advanced Regression Techniques challenge on the Kaggle website: [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/).
    The original version of the dataset is available: [http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls](http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls)
    and in the GitHub repository for this chapter.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用由 *迪安·德·科克* 编制的 `Ames Housing` 数据集，用于数据科学教育。它是流行的但较旧的 `Boston Housing`
    数据集的一个很好的替代品。`Ames Housing` 数据集在 Kaggle 网站的“高级回归技术”挑战赛中被使用：[https://www.kaggle.com/c/house-prices-advanced-regression-techniques/](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/)。数据集的原始版本可在：[http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls](http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls)
    和本章的 GitHub 仓库中找到。
- en: The `Ames Housing` dataset contains 79 explanatory variables describing (almost)
    every aspect of residential homes in Ames, Iowa with the goal of predicting the
    selling price of each home. The dataset has 2930 rows. The high number of variables
    makes this dataset a good candidate for Feature Selection.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`Ames Housing` 数据集包含 79 个解释变量，描述了爱荷华州艾姆斯市住宅的各个方面（几乎涵盖所有方面），目的是预测每套住宅的售价。该数据集有
    2930 行。变量数量众多，使该数据集成为特征选择的良好候选。'
- en: For more information on the genesis of this dataset and an in-depth explanation
    of the different variables, read the paper by *Dean De Cock* available in PDF
    at [https://ww2.amstat.org/publications/jse/v19n3/decock.pdf](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 关于该数据集的起源以及不同变量的深入解释，请阅读由 *迪安·德·科克* 撰写的论文，该论文可在 PDF 格式下在 [https://ww2.amstat.org/publications/jse/v19n3/decock.pdf](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf)
    获取。
- en: As usual, we will start by splitting the dataset into a train and a validate
    set and build a model on the train set. Both train and validate sets are available
    in the GitHub repository as `ames_housing_training.csv` and `ames_housing_validate.csv`.
    The entire dataset is in the `ames_housing.csv` file.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们将首先将数据集分割成训练集和验证集，并在训练集上构建模型。训练集和验证集都可在 GitHub 仓库中找到，分别命名为 `ames_housing_training.csv`
    和 `ames_housing_validate.csv`。整个数据集位于 `ames_housing.csv` 文件中。
- en: Splitting the dataset with shell commands
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 shell 命令分割数据集
- en: 'The command line is an often forgotten but powerful ally to the data scientist.
    Many very powerful operations on the data can be achieved with the right shell
    commands and executed blazingly fast. To illustrate this, we will use shell commands
    to shuffle, split, and create training and validation subsets of the `Ames Housing`
    dataset:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行是数据科学家经常忘记但非常强大的盟友。许多非常强大的数据处理操作可以通过正确的shell命令实现，并且执行速度极快。为了说明这一点，我们将使用shell命令来打乱、分割，并创建`Ames
    Housing`数据集的训练和验证子集：
- en: 'First, extract the first line into a separate file, `ames_housing_header.csv`
    and remove it from the original file:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，将第一行提取到单独的文件`ames_housing_header.csv`中，并从原始文件中删除：
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We just tail all the lines after the first one into the same file:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需将第一行之后的全部行尾接到同一个文件中：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then randomly sort the rows into a temporary file. (`gshuf` is the OSX equivalent
    of the Linux **shuf shell** command. It can be installed via `brew install coreutils`):'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将行随机排序到一个临时文件中。（`gshuf`是OSX中Linux **shuf shell**命令的等价物。可以通过`brew install coreutils`安装。）
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Extract the first 2,050 rows as the training file and the last 880 rows as
    the validation file:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取前2,050行作为训练文件，最后880行作为验证文件：
- en: '[PRE22]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, add back the header into both training and validation files:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将标题添加回训练和验证文件中：
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: A simple project using the CLI
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CLI的一个简单项目
- en: 'We are now ready to execute a simple Amazon ML workflow using the CLI. This
    includes the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好使用CLI执行一个简单的Amazon ML工作流程。这包括以下内容：
- en: Uploading files on S3
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在S3上上传文件
- en: Creating a datasource and the recipe
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建数据源和配方
- en: Creating a model
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建模型
- en: Creating an evaluation
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建评估
- en: Prediction batch and real time
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测批量和实时
- en: Let's start by uploading the training and validation files to S3\. In the following
    lines, replace the bucket name `aml.packt` with your own bucket name.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从上传训练和验证文件到S3开始。在以下行中，将存储桶名称`aml.packt`替换为您自己的存储桶名称。
- en: 'To upload the files to the S3 location `s3://aml.packt/data/ch8/`, run the
    following command lines:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要将文件上传到S3位置`s3://aml.packt/data/ch8/`，运行以下命令行：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: An overview of Amazon ML CLI commands
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Amazon ML CLI命令概述
- en: That's it for the S3 part. Now let's explore the CLI for Amazon's machine learning
    service.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: S3部分到此结束。现在让我们探索Amazon机器学习服务的CLI。
- en: All Amazon ML CLI commands are available at [http://docs.aws.amazon.com/cli/latest/reference/machinelearning/](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/).
    There are 30 commands, which can be grouped by object and action.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Amazon ML CLI命令均可在[http://docs.aws.amazon.com/cli/latest/reference/machinelearning/](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/)找到。共有30个命令，可以根据对象和动作进行分组。
- en: 'You can perform the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以执行以下操作：
- en: '`create` : creates the object'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create` : 创建对象'
- en: '`describe`: searches objects given some parameters (location, dates, names,
    and so on)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`describe`: 根据某些参数（位置、日期、名称等）搜索对象'
- en: '`get`: given an object ID, returns information'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get`: 给定一个对象ID，返回信息'
- en: '`update`: given an object ID, updates the object'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update`: 给定一个对象ID，更新对象'
- en: '`delete`: deletes an object'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delete`: 删除一个对象'
- en: 'These can be performed on the following elements:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可以在以下元素上执行：
- en: datasource
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: datasource
- en: '`create-data-source-from-rds`'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create-data-source-from-rds`'
- en: '`create-data-source-from-redshift`'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create-data-source-from-redshift`'
- en: '`create-data-source-from-s3`'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create-data-source-from-s3`'
- en: '`describe-data-sources`'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`describe-data-sources`'
- en: '`delete-data-source`'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delete-data-source`'
- en: '`get-data-source`'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get-data-source`'
- en: '`update-data-source`'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update-data-source`'
- en: ml-model
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ml-model
- en: '`create-ml-model`'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create-ml-model`'
- en: '`describe-ml-models`'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`describe-ml-models`'
- en: '`get-ml-model`'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get-ml-model`'
- en: '`delete-ml-model`'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delete-ml-model`'
- en: '`update-ml-model`'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update-ml-model`'
- en: evaluation
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估
- en: '`create-evaluation`'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create-evaluation`'
- en: '`describe-evaluations`'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`describe-evaluations`'
- en: '`get-evaluation`'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get-evaluation`'
- en: '`delete-evaluation`'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delete-evaluation`'
- en: '`update-evaluation`'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update-evaluation`'
- en: batch prediction
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量预测
- en: '`create-batch-prediction`'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create-batch-prediction`'
- en: '`describe-batch-predictions`'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`describe-batch-predictions`'
- en: '`get-batch-prediction`'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get-batch-prediction`'
- en: '`delete-batch-prediction`'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delete-batch-prediction`'
- en: '`update-batch-prediction`'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update-batch-prediction`'
- en: real-time end point
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时端点
- en: '`create-realtime-endpoint`'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create-realtime-endpoint`'
- en: '`delete-realtime-endpoint`'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delete-realtime-endpoint`'
- en: '`predict`'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict`'
- en: You can also handle tags and set waiting times.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以处理标签并设置等待时间。
- en: Note that the AWS CLI gives you the ability to create datasources from S3, Redshift,
    and RDS, while the web interface only allowed datasources from S3 and Redshift.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，AWS CLI允许您从S3、Redshift和RDS创建数据源，而Web界面只允许从S3和Redshift创建数据源。
- en: Creating the datasource
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据源
- en: 'We will start by creating the datasource. Let''s first see what parameters
    are needed by generating the following skeleton:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先创建数据源。让我们首先通过生成以下骨架来查看需要哪些参数：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This generates the following JSON object:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下JSON对象：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The different parameters are mostly self-explanatory and further information
    can be found on the AWS documentation at [http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-s3.html](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-s3.html).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的参数大多一目了然，更多详细信息可以在AWS文档中找到，链接为[http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-s3.html](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-s3.html)。
- en: 'A word on the schema: when creating a datasource from the web interface, you
    have the possibility to use a wizard, to be guided through the creation of the
    schema. As you may recall, you are guided through several screens where you can
    specify the type of all the columns, and the existence of a target variable and
    an index column. The wizard facilitates the process by guessing the type of the
    variables, thus making available a default schema that you can modify.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模式的话：当从Web界面创建数据源时，您有使用向导的可能性，以指导您创建模式。如您所回忆的，您将通过几个屏幕进行指导，在这些屏幕上您可以指定所有列的类型，以及目标变量和索引列的存在。向导通过猜测变量的类型来简化过程，从而提供一个默认的模式，您可以对其进行修改。
- en: There is no default schema available via the AWS CLI. You have to define the
    entire schema yourself, either in a JSON format in the `DataSchema` field or by
    uploading a schema file to S3 and specifying its location, in the `DataSchemaLocationS3`
    field.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通过AWS CLI没有默认的模式可用。您必须自己定义整个模式，无论是在`DataSchema`字段中的JSON格式，还是在`DataSchemaLocationS3`字段中上传一个模式文件并指定其位置。
- en: Since our dataset has many variables (79), we cheated and used the wizard to
    create a default schema that we uploaded to S3\. Throughout the rest of the chapter,
    we will specify the schema location not its JSON definition.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集包含许多变量（79个），我们采取了捷径，使用向导创建了一个默认的模式，并将其上传到S3。在整个章节的其余部分，我们将指定模式位置而不是其JSON定义。
- en: 'In this example, we will create the following datasource parameter file, `dsrc_ames_housing_001.json`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将创建以下数据源参数文件，`dsrc_ames_housing_001.json`：
- en: '[PRE27]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'For the validation subset (save to `dsrc_ames_housing_002.json`):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证子集（保存为`dsrc_ames_housing_002.json`）：
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Since we have already split our data into a training and a validation set, there's
    no need to specify the data `DataRearrangement` field.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经将数据分为训练集和验证集，因此无需指定数据`DataRearrangement`字段。
- en: 'Alternatively, we could also have avoided splitting our dataset and specified
    the following `DataRearrangement` on the original dataset, assuming it had been
    already shuffled: (save to `dsrc_ames_housing_003.json`):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以避免分割数据集，并在原始数据集上指定以下`DataRearrangement`，假设它已经被洗牌了：（保存为`dsrc_ames_housing_003.json`）：
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For the validation set (save to `dsrc_ames_housing_004.json`):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证集（保存为`dsrc_ames_housing_004.json`）：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, the `ames_housing.csv` file has previously been shuffled using the `gshuf`
    command line and uploaded to S3:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`ames_housing.csv`文件之前已经使用`gshuf`命令行进行了洗牌，并上传到了S3：
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note that we don't need to create these four datasources; these are just examples
    of alternative ways to create datasources.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不需要创建这四个数据源；这些只是创建数据源的替代方法示例。
- en: 'We then create these datasources by running the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后通过运行以下命令创建这些数据源：
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can check whether the datasource creation is pending:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查数据源创建是否挂起：
- en: '![](img/B05028_08_01.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_08_01.png)'
- en: 'In return, we get the datasoure ID we had specified:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回报，我们得到了之前指定的数据源ID：
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can then obtain information on that datasource with the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下方式获取该数据源的信息：
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This returns the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回以下内容：
- en: '[PRE35]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note that we have access to the operation log URI, which could be useful to
    analyze the model training later on.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们有访问操作日志URI的权限，这可能在稍后分析模型训练时很有用。
- en: Creating the model
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建模型
- en: 'Creating the model with the `create-ml-model` command follows the same steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`create-ml-model`命令创建模型遵循相同的步骤：
- en: 'Generate the skeleton with the following:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下步骤生成骨架：
- en: '[PRE36]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Write the configuration file:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写配置文件：
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note the parameters of the algorithm. Here, we used mild L2 regularization and
    100 passes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意算法的参数。在这里，我们使用了温和的L2正则化和100次迭代。
- en: 'Launch the model creation with the following:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下步骤启动模型创建：
- en: '[PRE38]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The model ID is returned:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回模型ID：
- en: '[PRE39]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This `get-ml-model` command gives you a status update on the operation as well
    as the URL to the log.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个 `get-ml-model` 命令会给你操作的状态更新以及日志的URL。
- en: '[PRE40]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `watch` command allows you to repeat a shell command every *n* seconds.
    To get the status of the model creation every *10s*, just write the following:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`watch` 命令允许你每 *n* 秒重复一次shell命令。要每 *10s* 获取模型创建的状态，只需写下以下命令：'
- en: '[PRE41]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The output of the `get-ml-model` will be refreshed every 10s until you kill
    it.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`get-ml-model` 的输出将每10秒刷新一次，直到你将其终止。'
- en: It is not possible to create the default recipe via the AWS CLI commands. You
    can always define a blank recipe that would not carry out any transformation on
    the data. However, the default recipe has been shown to be positively impacting
    the model performance. To obtain this default recipe, we created it via the web
    interface, copied it into a file that we uploaded to S3\. The resulting file `recipe_ames_housing_001.json`
    is available in our GitHub repository. Its content is quite long as the dataset
    has 79 variables and is not reproduced here for brevity purposes.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 通过AWS CLI命令无法创建默认的食谱。你总是可以定义一个空白食谱，该食谱不会对数据进行任何转换。然而，默认食谱已被证明对模型性能有积极影响。为了获得此默认食谱，我们通过Web界面创建它，将其复制到一个我们上传到S3的文件中。生成的文件
    `recipe_ames_housing_001.json` 可在我们的GitHub仓库中找到。由于数据集有79个变量，其内容相当长，这里为了简洁起见没有展示。
- en: Evaluating our model with create-evaluation
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `create-evaluation` 评估我们的模型
- en: 'Our model is now trained and we would like to evaluate it on the evaluation
    subset. For that, we will use the `create-evaluation` CLI command:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的模型已经训练好了，我们想在评估子集上评估它。为此，我们将使用 `create-evaluation` CLI命令：
- en: 'Generate the skeleton:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成骨架：
- en: '[PRE42]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Configure the parameter file:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置参数文件：
- en: '[PRE43]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Launch the evaluation creation:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动评估创建：
- en: '[PRE44]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Get the evaluation information:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取评估信息：
- en: '[PRE45]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'From that output, we get the performance of the model in the form of the RMSE:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从那个输出中，我们以RMSE的形式获得了模型的性能：
- en: '[PRE46]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The value may seem big, but it is relative to the range of the `salePrice` variable
    for the houses, which has a mean of 181300.0 and std of 79886.7\. So an RMSE of
    29853.2 is a decent score.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值可能看起来很大，但它相对于房屋的 `salePrice` 变量的范围是相对的，该变量的均值为181300.0，标准差为79886.7。因此，RMSE为29853.2是一个不错的分数。
- en: You don't have to wait for the datasource creation to be completed in order
    to launch the model training. Amazon ML will simply wait for the parent operation
    to conclude before launching the dependent one. This makes chaining operations
    possible.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你不必等待数据源创建完成就可以启动模型训练。Amazon ML将简单地等待父操作完成后再启动依赖的操作。这使得操作链式执行成为可能。
- en: The next step would be to make batch predictions or create a real-time endpoint.
    These would follow the exact same steps of model creation and evaluation, and
    are not presented here.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步将是进行批量预测或创建实时端点。这些将遵循与模型创建和评估完全相同的步骤，这里没有展示。
- en: At this point, we have a trained and evaluated model. We chose a certain set
    of parameters and carried out a certain preprocessing of the data via the default
    recipe. We now would like to know whether we can improve on that model and feature
    set by trying new parameters for the algorithm and doing some creative feature
    engineering. We will then train our new models and evaluate them on the validation
    subset. As we've seen before, the problem with that approach is that our evaluation
    score can be highly dependent on the evaluation subset. Shuffling the data to
    generate new training and validation sets may result in different model performance
    and make us choose the wrong model. Even though we have shuffled the data to avoid
    sequential patterns, there is no way to be sure that our split is truly neutral
    and that both subsets show similar data distribution. One of the subsets could
    present anomalies such as outliers, or missing data that the other does not have.
    To solve this problem, we turn to cross-validation.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有一个训练和评估好的模型。我们选择了一组特定的参数，并通过默认的食谱对数据进行了一定的预处理。我们现在想知道是否可以通过尝试新的算法参数和进行一些创造性的特征工程来改进该模型和特征集。然后我们将训练新的模型并在验证子集上评估它们。正如我们之前看到的，这种方法的问题在于我们的评估分数可能高度依赖于评估子集。通过打乱数据来生成新的训练和验证集可能会导致不同的模型性能，并使我们选择错误的模型。尽管我们已经打乱了数据以避免顺序模式，但我们无法确保我们的分割是真正中立的，并且两个子集显示出相似的数据分布。其中一个子集可能呈现异常，如异常值或缺失数据，而另一个子集没有。为了解决这个问题，我们转向交叉验证。
- en: What is cross-validation?
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是交叉验证？
- en: To lower the dependence on the data distribution in each split, the idea is
    to run many trials in parallel, each with a different data split, and average
    the results. This is called cross-validation.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低对每个分割中数据分布的依赖，想法是并行运行许多试验，每个试验都有不同的数据分割，并平均结果。这被称为交叉验证。
- en: The idea is simply to average the model performance across K trials, where each
    trial is built on a different split of the original dataset. There are many strategies
    to split the dataset. The most common one is called **k-fold cross-validation**
    and consists of splitting the dataset into **K chunks**, and for each trial using
    *K-1* chunks aggregated to train the model and the remaining chunk to evaluate
    it. Another strategy, called **leave-one-out** (**LOO**), comes from taking this
    idea to its extreme with *K* as the number of samples. You train your model on
    all the samples except one and estimate the error on the remaining sample. LOO
    is obviously more resource intensive.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，就是将模型性能在 K 次试验中平均，其中每次试验都是基于原始数据集的不同分割。有许多分割数据集的策略。最常见的一种称为 **k 折交叉验证**，它包括将数据集分割成
    **K 个块**，并在每个试验中使用 *K-1* 个块来聚合训练模型，剩余的块来评估它。另一种策略，称为 **留一法**（**LOO**），是将这个想法推向极端，其中
    *K* 是样本数量。你将在所有样本中除了一个之外训练你的模型，并在剩余的样本上估计误差。LOO 显然更耗费资源。
- en: The strategy we will implement is called **Monte Carlo cross-validation**, where
    the initial dataset is randomly split into a training and validation set in each
    trial. The advantage of that method over k-fold cross validation is that the proportion
    of the training/validation split is not dependent on the number of iterations
    (*K*). Its disadvantage is that some samples may never be selected in the validation
    subset, whereas others may be selected more than once. Validation subsets may
    overlap.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实施的策略称为 **蒙特卡洛交叉验证**，其中在每次试验中，初始数据集被随机分割成训练集和验证集。这种方法相对于 k 折交叉验证的优势在于，训练/验证分割的比例不依赖于迭代次数（*K*）。它的缺点是，一些样本可能永远不会被选入验证子集，而另一些样本可能被选中多次。验证子集可能重叠。
- en: 'Let''s look at an example with `k =5` trials. We will repeat these steps five
    times to evaluate one model (for instance, L2 mild regularization):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个 `k =5` 次试验的例子。我们将重复这些步骤五次来评估一个模型（例如，L2 轻度正则化）：
- en: Shuffle the Ames Housing dataset.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打乱 Ames 住房数据集。
- en: Split the dataset into training and validation subsets.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分为训练集和验证集。
- en: Train the model on the training set.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上训练模型。
- en: Evaluate the model on the validation set.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在验证集上评估模型。
- en: At this point, we have five measures of the model performance; we average it
    to get a measure of overall model performance. We repeat the aforementioned five
    steps to evaluate another model (for instance, L1 medium regularization). Once
    we have tested all our models, we select the one that gives the best average performance
    on the trials.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有五个衡量模型性能的指标；我们将它们平均以获得整体模型性能的衡量标准。我们重复上述五个步骤来评估另一个模型（例如，L1 中等正则化）。一旦我们测试了所有模型，我们将选择在试验中给出最佳平均性能的模型。
- en: This is why scripting becomes a necessity. To test one model setup, a cross-validation
    with *K trials* (K fold or Monte Carlo) requires `2*K` datasources, K models,
    and K evaluations. This will surely be too time-consuming when done via the web
    interface alone. This is where scripting the whole process becomes extremely useful
    and much more efficient.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么脚本变得必要。为了测试一个模型设置，进行带有 *K trials*（K 折或蒙特卡洛）的交叉验证需要 `2*K` 数据源，K 个模型和 K
    个评估。如果仅通过网络界面完成，这肯定会非常耗时。这就是为什么整个过程的脚本化变得极其有用且效率更高。
- en: There are many ways to actually create the different subset files for cross-validation.
    The simplest way might be to use a spreadsheet editor with random sorting, and
    some cutting and pasting. R and Python libraries, such as the popular `scikit-learn`
    library or the Caret package, have rich methods that can be used out of the box.
    However, since this chapter is about the AWS command line interface, we will use
    shell commands to generate the files. We will also write shell scripts to generate
    the sequence of AWS CLI commands in order to avoid manually editing the same commands
    for the different data files and models.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Monte Carlo cross-validation
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now implement a Monte Carlo cross-validation strategy with five trials
    using shell commands and AWS CLI. And we will use this evaluation method to compare
    two models, one with L2 mild regularization and the second with L1 heavy regularization
    on the Ames Housing dataset. Cross-validation will allow us to conclude with some
    level of confidence which model performs better.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Generating the shuffled datasets
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the datasource creation `DataRearrangement` field to split the data
    into a training and a validation subset. So, we only need to create five files
    of shuffled data in the first place.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shell script will create five shuffled versions of the `Ames
    Housing` dataset and upload the files to S3\. You can either save that code in
    a file with the `.sh` extension (`datasets_creation.sh`) or run it with `sh ./datasets_creation.sh`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Note that in this chapter, the code is organized around the following folder
    structure. All the command lines are run from the root folder, for instance, to
    run a Python script: `python py/the_script.py`, to list the data files `ls data/`
    and to run shell scripts: `sh ./shell/the_script.sh`.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '`.`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '`├── data`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '`├── images`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '`├── py`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '`└── shell` All the shell scripts and command are based on bash shell and should
    probably require adaptation to other shells such as zsh.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Our datasets have been created and uploaded to S3\. The general strategy is
    now to create templates for each of the parameter JSON files required for the
    Amazon ML CLI commands: create datasources, models, and evaluations. We will create
    the template files for the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Training datasource
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation datasource
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 model
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 model
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 evaluation
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 evaluation
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In all these template files, we will index the filenames with `{k}` and use
    the `sed` shell command to replace `{k}` with the proper index (1 to 5). Once
    we have the template files, we can use a simple shell script to generate the actual
    JSON parameter files for the datasources, models, and evaluations. We will end
    up with the following:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 10 datasource configuration files (five for training and five for evaluation)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 model configuration files (five for L2 and five for L1)
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 evaluation configuration files (one for each of the models)
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the end, we will obtain five RMSE results for the L2 model and five RMSE
    results for the L1 model, whose average will tell us which model is the best,
    which type of regularization should be selected to make sales price predictions
    on the Ames Housing dataset.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将获得L2模型的五个RMSE结果和L1模型的五个RMSE结果，它们的平均值将告诉我们哪个模型是最好的，应该选择哪种正则化类型来在Ames住房数据集上预测销售价格。
- en: Let's start by writing the configuration files.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从编写配置文件开始。
- en: Generating the datasources template
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成数据源模板
- en: 'The template for the training files is as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 训练文件模板如下：
- en: '[PRE48]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'And the template for for validation datasources is as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据源模板如下：
- en: '[PRE49]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The only different between the training and the validation templates are the
    names/IDs and the splitting ratio in the `DataRearrangement` field. We save these
    files to `dsrc_training_template.json` and `dsrc_validate_template.json` respectively.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证模板之间的唯一区别是`DataRearrangement`字段中的名称/ID和分割比率。我们将这些文件分别保存到`dsrc_training_template.json`和`dsrc_validate_template.json`。
- en: Generating the models template
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型模板
- en: 'In the case of a model with L2 regularization, the model template is as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有L2正则化的模型，模型模板如下：
- en: '[PRE50]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'And for a model with L1 regularization, the model template is as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有L1正则化的模型，模型模板如下：
- en: '[PRE51]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Note that the same recipe is used for both models. If we wanted to compare the
    performance of data preprocessing strategies, we could modify the recipes used
    in both models. The template files are very similar. The only difference is in
    the model name and ID and in the values for the `l1RegularizationAmount` and `l2RegularizationAmount`.
    We save these files to `mdl_l2_template.json` and `mdl_l1_template.json` respectively**.**
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，相同的配方用于两个模型。如果我们想比较数据预处理策略的性能，我们可以修改两个模型中使用的配方。模板文件非常相似。唯一的不同在于模型名称和ID以及`l1RegularizationAmount`和`l2RegularizationAmount`的值。我们将这些文件分别保存到`mdl_l2_template.json`和`mdl_l1_template.json`****。**
- en: Generating the evaluations template
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成评估模板
- en: 'In the case of a model with L2 regularization, the evaluation template is as
    follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有L2正则化的模型，评估模板如下：
- en: '[PRE52]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'And for a model with L1 regularization, the evaluation template is as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有L1正则化的模型，评估模板如下：
- en: '[PRE53]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Save these files to `eval_l2_template.json` and `eval_l1_template.json` espectively.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些文件保存到`eval_l2_template.json`和`eval_l1_template.json`分别。
- en: We will now use these template files to generate the different configuration
    files for the datasources, models, and evaluations. To keep things separate, all
    the generated files are in a subfolder `cfg/`.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用这些模板文件来生成数据源、模型和评估的不同配置文件。为了保持独立，所有生成的文件都在子文件夹`cfg/`中。
- en: 'The following shell script generates the actual configuration files that we
    will feed to the AWS CLI Machine Learning commands. It uses the `sed` command
    to find and replace the instances of `{k}` with the numbers 1 to 5\. The output
    is written to the configuration file. Since there will be many configuration files
    generated, the files are written in a `/cfg` subfolder under `/data`. The folder
    structure is now as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的shell脚本生成了我们将要提供给AWS CLI机器学习命令的实际配置文件。它使用`sed`命令查找并替换`{k}`实例为1到5的数字。输出被写入配置文件。由于将生成许多配置文件，这些文件被写入`/data`下的`/cfg`子文件夹中。现在的文件夹结构如下：
- en: '[PRE54]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The last remaining step is to execute the AWS commands that will create the
    objects in Amazon ML. We also use a shell loop to execute the AWS CLI commands.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个剩余步骤是执行AWS命令，这些命令将在Amazon ML中创建对象。我们同样使用shell循环来执行AWS CLI命令。
- en: 'Create datasources for training and evaluation:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 创建用于训练和评估的数据源：
- en: '[PRE55]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Train models with L2 and L1 regularization:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 使用L2和L1正则化训练模型：
- en: '[PRE56]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Evaluate trained models:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 评估训练模型：
- en: '[PRE57]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'You can check the status of the different jobs with the `get-data-source`,
    `get-ml-model` and `get-evaluation` CLI commands or on the Amazon ML dashboard.
    Once all the evaluation is finished, you capture the RMSE for each model by first
    creating a couple of files to receive the RMSE score and then running the following,
    final shell loop:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`get-data-source`、`get-ml-model`和`get-evaluation`CLI命令或在美国机器学习仪表板上检查不同作业的状态。一旦所有评估完成，您首先创建几个文件来接收RMSE分数，然后运行以下最终shell循环：
- en: '[PRE58]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The `get-evaluation` command, given the ID of the evaluation, returns a JSON-formatted
    string that is fed to a grepping command and added to the `l1/l2_model_rmse.log`
    files.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: The results
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We end up with the following results for the two models:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'On average, L1 gives an RMSE of 28075.2 (std: 1151), while L2 gives an RMSE
    of 29176.4 (std: 4246.7). Not only is the L1 model better performing, but it is
    also more robust when it comes to handling data variations since its std is lower.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-validation may be too time-consuming to implement via shell only. There
    are many files to create and coordinate. There are simpler ways to implement cross-validation
    with libraries such as `scikit-learn` for Python or Caret for R, where the whole
    model training and evaluation loop over several training and validation sets only
    requires a few lines of code. However, we showed that it was possible to implement
    cross-validation with Amazon ML. Cross-validation is a key component of the data-science
    workflow. Not being able to do cross validation with Amazon ML would have been
    a significant flaw in the service. In the end, the AWS CLI for machine learning
    is a very powerful and useful tool to conduct sequences of trials and compare
    results across different models, datasets, recipes, and features.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Boto3, the Python SDK
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another tool to interact with the Amazon ML service outside of the web interface
    is an SDK. Simply put, an SDK is a wrapper around an API that makes working with
    the service much simpler and more efficient, as many details of the interactions
    are taken care of. AWS offers SDKs in the most widespread languages such as PHP,
    Java, Ruby, .Net, and of course, Python. In this chapter, we will focus on working
    with the Amazon ML service through the Python SDK. The Python SDK requires the
    Boto3 module.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Installation of the Boto3 module is done via pip. Refer to the quickstart guide
    available at [http://boto3.readthedocs.io/en/latest/guide/quickstart.html](http://boto3.readthedocs.io/en/latest/guide/quickstart.html)
    if you need more information and troubleshooting:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Boto3 is available for most AWS services. The complete list can be found at
    [http://boto3.readthedocs.io/en/latest/reference/services/index.html](http://boto3.readthedocs.io/en/latest/reference/services/index.html).
    We will focus on Boto3 for S3 and Amazon ML.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Setting up permissions for SDK access can be done via the `aws configure` command
    that we followed at the beginning of this chapter, or directly by adding your
    access keys to the `~/.aws/credentials` file.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the `Boto3` logic is very similar to the AWS CLI logic and follows
    similar steps: declaring the service to be used and running commands with the
    appropriate set of parameters. Let''s start with a simple example around S3 with
    the following Python script, which will list all the buckets in your account:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Uploading a local file to a bucket would be achieved by the following:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The put command returns a JSON string, with an HTTPStatusCode field with a 200
    value, indicating that the upload was successful.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`put` 命令返回一个 JSON 字符串，其中包含一个 HTTPStatusCode 字段，其值为 200，表示上传成功。'
- en: Working with the Python SDK for Amazon Machine Learning
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用亚马逊机器学习的 Python SDK
- en: 'The list of available methods can be found at [http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html](http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html)
    and closely follows the list of available commands for the AWS CLI for the Machine
    Learning service organized around the main objects: datasource, model, evaluation,
    batch prediction, and real-time endpoints. For each object, the methods are: create,
    update, describe, get, and delete.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 可用方法的列表可以在 [http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html](http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html)
    找到，并且紧密遵循围绕主要对象（数据源、模型、评估、批量预测和实时端点）组织的 AWS CLI 机器学习服务的可用命令列表。对于每个对象，方法有：创建、更新、描述、获取和删除。
- en: 'We will now implement the standard Amazon ML workflow. But first, let''s define
    a naming method for the objects we will create. An important part of the workflow
    revolves around naming convention for object names and IDs. When working with
    the CLI, we created the names and IDs on the fly. This time we will use the following
    function to name our objects:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将实现标准的亚马逊机器学习工作流程。但首先，让我们为我们将要创建的对象定义一个命名方法。工作流程的一个重要部分是围绕对象名称和 ID 的命名约定。当使用
    CLI 时，我们即时创建了名称和 ID。这次我们将使用以下函数来命名我们的对象：
- en: '[PRE63]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: This function takes in two strings and one integer as arguments, a prefix for
    the type of the object (datasource, model, and so on), a mode to specify training
    versus validation datasource, and a trial value to easily increment our experiments.
    The function returns a dictionary.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受两个字符串和一个整数作为参数，一个用于对象类型（数据源、模型等）的前缀，一个用于指定训练或验证数据源的模式，以及一个用于轻松增加实验的试验值。该函数返回一个字典。
- en: 'Let''s now define a few variables that we will use later on in the script:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在定义一些将在脚本中稍后使用的变量：
- en: '[PRE64]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We need to import the following libraries:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要导入以下库：
- en: '[PRE65]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Declare that we want to interact with the Machine Learning service:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 声明我们想要与机器学习服务交互：
- en: '[PRE66]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We are now all set to create our training and validation datasources with the
    following:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已准备好使用以下方式创建我们的训练和验证数据源：
- en: '[PRE67]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In both cases, we call on the naming function we defined earlier to generate
    the Name and ID of the datasource and use that dictionary when calling the `create_data_source_from_s3` Boto3
    method.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们调用之前定义的命名函数来生成数据源的名称和 ID，并在调用 `create_data_source_from_s3` Boto3 方法时使用该字典。
- en: 'We launch the training of the model with the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下方式启动模型的训练：
- en: '[PRE68]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'And create the evaluation:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 然后创建评估：
- en: '[PRE69]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'You can now go to the Amazon ML dashboard and verify that you have two datasources,
    one model, and one evaluation in the In progress or Pending status:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以访问亚马逊机器学习仪表板并验证您有两个数据源、一个模型和一个评估处于“进行中”或“待处理”状态：
- en: '![](img/B05028_08_02.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_08_02.png)'
- en: Waiting on operation completion
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等待操作完成
- en: All these object, creation operations are, by default, chained by Amazon ML.
    This means that Amazon ML will wait on the datasources to be ready before launching
    the model training, and will also wait for the model training to be completed
    before trying to run the evaluation. However, at this point, we still need to
    wait for the evaluation to be complete before we can access its results. Similarly,
    we need to wait for the different objects to have been utilized by the next operation
    before deleting them.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些对象创建操作默认情况下都是由亚马逊机器学习链式执行的。这意味着亚马逊机器学习将在启动模型训练之前等待数据源准备就绪，并在尝试运行评估之前等待模型训练完成。然而，在这个阶段，我们仍然需要等待评估完成才能访问其结果。同样，我们需要等待不同对象被下一个操作使用后才能删除它们。
- en: 'This is where the waiter methods become useful. Waiters are methods that simply
    wait for an AWS operation to be completed, to have status *Completed.* Waiters
    exists for all AWS operations and services. Amazon ML offers four waiters for
    models, datasources, evaluations, and batch predictions:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是等待方法变得有用的地方。等待器是简单地等待 AWS 操作完成、状态为 *完成* 的方法。所有 AWS 操作和服务都有等待器。亚马逊机器学习为模型、数据源、评估和批量预测提供了四个等待器：
- en: '`MachineLearning.Waiter.BatchPredictionAvailable`'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MachineLearning.Waiter.BatchPredictionAvailable`'
- en: '`MachineLearning.Waiter.DataSourceAvailable`'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MachineLearning.Waiter.DataSourceAvailable`'
- en: '`MachineLearning.Waiter.EvaluationAvailable`'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MachineLearning.Waiter.EvaluationAvailable`'
- en: '`MachineLearning.Waiter.MLModelAvailable`'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MachineLearning.Waiter.MLModelAvailable`'
- en: 'A Machine Learning waiter follows the syntax – first, declare the object the
    waiter has to monitor, for instance an evaluation:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习服务员遵循以下语法——首先，声明服务员需要监控的对象，例如一个评估：
- en: '[PRE70]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Then call the `wait` method on the waiter you just declared:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在您刚刚声明的服务员上调用`wait`方法：
- en: '[PRE71]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Once the wait method is called, the Python scripts hangs until the operation
    reaches a status of `Completed`. The wait function takes the following:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦调用等待方法，Python脚本就会挂起，直到操作达到`Completed`状态。等待函数需要以下内容：
- en: 'A filter value: `FilterVariable = CreatedAt`, `LastUpdatedAt`, `Status`, `Name`,
    `IAMUser`, `MLModelId`, `DataSourceId`, `DataURI`'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个过滤值：`FilterVariable = CreatedAt`, `LastUpdatedAt`, `Status`, `Name`, `IAMUser`,
    `MLModelId`, `DataSourceId`, `DataURI`
- en: An operator: EQ, GT, LT, GE, LE, NE
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作符：EQ, GT, LT, GE, LE, NE
- en: Other parameters that depend on the nature of the object
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他依赖于对象性质的参数
- en: 'With that parameter structure, you can make your script wait on a specific
    object completion, or wait on all the objects based on a datasource, a model,
    or even a user name. If we were to launch several evaluations for different models
    based on the same validation datasource, we would simply call a waiter for each
    model as such:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种参数结构，您可以让脚本等待特定对象的完成，或者根据数据源、模型或甚至用户名等待所有对象。如果我们要对基于相同验证数据源的多个模型进行评估，我们只需为每个模型调用一个服务员即可：
- en: '[PRE72]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Wrapping up the Python-based workflow
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结基于Python的工作流程
- en: 'Now that we know how to wait for all our evaluation to finish, we still need
    to get the evaluation result and delete the models and datasources we have created.
    As in the case of the `get-evaluation` AWS CLI command, the Boto3 `get_evaluation`
    method returns a JSON string with the model performance measure, the RMSE in the
    case of regression. The following script wraps up our trial:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何等待所有评估完成，我们仍然需要获取评估结果并删除我们创建的模型和数据源。正如`get-evaluation` AWS CLI命令的情况一样，Boto3的`get_evaluation`方法返回一个包含模型性能度量、回归情况下的RMSE的JSON字符串。以下脚本总结了我们的试验：
- en: '[PRE73]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Putting all the code blocks together returns the following output:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有代码块放在一起返回以下输出：
- en: '[PRE74]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Implementing recursive feature selection with Boto3
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Boto3实现递归特征选择
- en: In many real-world cases, the original dataset could have a very large number
    of variables. As dimensionality increases, so does the need for a larger sample
    set. This is called the *curse of dimensionality*, a classic predictive analytics
    problem. Simply put, if there is not enough diversity to infer a representative
    distribution for some variables, the algorithm will be unable to extract relevant
    information from the said variables. These low-signal variables drag down the
    algorithm's performance without adding any data fuel by adding useless complexity
    to the model. One strategy is to reduce the number of variables on which to train
    the model. However, that implies identifying which features can be dropped without
    significant loss of information.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际案例中，原始数据集可能包含大量变量。随着维度的增加，对更大样本集的需求也增加。这被称为*维度诅咒*，是一个经典的预测分析问题。简单来说，如果没有足够的多样性来推断某些变量的代表性分布，算法将无法从这些变量中提取相关信息。这些低信号变量会拖累算法的性能，而不会通过向模型添加无用的复杂性来增加任何数据燃料。一种策略是减少模型训练的变量数量。然而，这暗示了需要确定哪些特征可以被删除而不会导致信息损失。
- en: 'There are many techniques to reduce the number of features:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多技术可以减少特征数量：
- en: '**Wrapper** techniques: These use rules and criteria to select the best and
    most impacting features.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包装器**技术：这些技术使用规则和标准来选择最佳和最具影响力的特征。'
- en: '**Filter** techniques: These use statistical tests to measure the importance
    of each feature. Measuring the correlation with the target could be a simple way
    to remove non-significant variables.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤**技术：这些技术使用统计测试来衡量每个特征的重要性。与目标的相关性测量可能是一种简单的方法来删除非显著变量。'
- en: '**Embedded** methods: For certain models, such as random forests, iteratively
    train the model on subsets of features, it is possible to evaluate the impact
    of the features that are left out during each iteration and thus infer the importance
    of each feature.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入式**方法：对于某些模型，例如随机森林，可以在特征子集上迭代训练模型，因此可以评估每次迭代中省略的特征的影响，从而推断每个特征的重要性。'
- en: The method most adapted to the Amazon Machine Learning context is the recursive
    evaluation of each feature's importance, filtering out the least important ones by
    measuring when performance drops significantly with the discarding of a certain
    feature. It is a brute force version of Recursive Feature Elimination.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于Amazon机器学习环境的方法是递归评估每个特征的重要性，通过测量在丢弃某个特征时性能显著下降来过滤掉最不重要的特征。这是递归特征消除的暴力版本。
- en: 'It follows these steps:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 它遵循以下步骤：
- en: Build an initial model with all *N* features.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有*N*个特征构建一个初始模型。
- en: 'Then identify and remove the least important features by:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过以下方式识别并移除最不重要的特征：
- en: Building N subsets, removing a different feature in each subset
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建N个子集，每个子集中删除不同的特征
- en: Building a model for each subset and evaluating its performance
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个子集构建模型并评估其性能
- en: Identifying the features for which the model's performance was least impacted
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别对模型性能影响最小的特征
- en: You now have *N-1* features. Reiterate steps 1 to 3 to identify and remove the
    next least important feature.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您有*N-1*个特征。重复步骤1到3以识别并移除下一个最不重要的特征。
- en: Stop when you notice a significant drop in the model's performance compared
    to the initial N-feature model.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您注意到与初始N特征模型相比模型性能显著下降时停止。
- en: The inverse version of this algorithm starts with *N* models, each built with
    just one feature, with a new feature added at each iteration. Choose the new feature
    as the new feature that generates the best increase in performance. Stop when
    adding new features does not lead to a significant increase in the model's performance.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的逆版本从*N*个模型开始，每个模型仅使用一个特征构建，并在每次迭代中添加一个新特征。选择新特征为能带来最佳性能提升的新特征。当添加新特征不再导致模型性能显著提升时停止。
- en: In the rest of the chapter, we will show how to implement this feature selection
    strategy in Python.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将展示如何在Python中实现这种特征选择策略。
- en: Managing schema and recipe
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理模式和配方
- en: Removing or adding features to a dataset directly impacts the schema and the
    recipe. The schema is used when creating the datasources, while the recipe is
    needed to train the model, as it specifies which data transformation will be performed
    prior to the model training.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 直接删除或添加数据集的特征会直接影响模式和配方。模式用于创建数据源时使用，而配方需要用于训练模型，因为它指定了在模型训练之前将执行哪些数据转换。
- en: 'Modifying the schema to remove features from the dataset can be done by simply
    adding the names of the variable to the `excludedAttributeNames` field. We can
    take the initial schema, and each time we remove a feature from the initial feature
    list, we add it to the excludedAttributeNames list. The steps are as follows:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单地将变量的名称添加到`excludedAttributeNames`字段中，可以修改模式以从数据集中删除特征。我们可以从初始模式开始，每次从初始特征列表中删除一个特征时，就将其添加到`excludedAttributeNames`列表中。步骤如下：
- en: Open the JSON formatted schema into a schema dict
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将JSON格式的模式打开到模式字典中
- en: Append the feature name to schema [`'excludedAttributeNames'`]
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征名称追加到模式`['excludedAttributeNames'`]
- en: Save the schema to a properly indented JSON file
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模式保存到正确缩进的JSON文件中
- en: Upload the file to S3
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件上传到S3
- en: When creating the datasource, we will point to the S3 location of the schema
    we just updated.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建数据源时，我们将指向我们刚刚更新的模式的S3位置。
- en: 'The initial recipe generated by default by Amazon ML for the Ames Housing dataset
    applies different quantile binning transformations on certain numerical features.
    The groups section of the recipe is as follows:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 亚姆住房数据集默认由Amazon ML生成的初始配方对某些数值特征应用不同的分位数分箱转换。配方的分组部分如下：
- en: '[PRE75]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Adding or removing variable names from that structure requires a more complex
    script than just adding an element in a list. Since such a script would not add
    much educational value to the book, we decided to use a default simple recipe
    that does not perform any transformation on the dataset. As long as we have a
    baseline RMSE with all the features available, the recursive feature elimination
    strategy is still valid. The only difference is that the overall RMSE scores will
    probably be made higher by not applying any quantile binning to our numerical
    data. The recipe we use is defined by the following:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: This is available in our examples at the S3 location `s3://aml.packt/data/ch8/recipe_ames_housing_default.json`.
    Using that recipe to evaluate our baseline model gives a baseline RMSE of *61507.35.*
    We will use that baseline RMSE to see whether removing a feature improved (lower)
    or degraded (higher) the model performance.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'The following script is broken into three parts:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Initialization and functions
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launching the Amazon ML workflow
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the evaluation results and deleting the resources
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The script is available in our GitHub repo in its entirety. We use the same
    strategy to have a function to generate the names and IDs. We start with the following
    script to initialize the variable and declare the function:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: We now launch the datasource, model, and evaluation creation. The script only
    looks at the first 10 features, and not the entire set of 79 features, to save
    on resources.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that we added a prefix "*X"* to the numbering of the Amazon
    ML objects. We found that sometimes, Amazon ML cannot create an object if the
    IDs and names have been used on previous objects that have now been deleted. That
    problem may disappear after some time. In any case, making sure that all new datasources,
    models, evaluations, and batch predictions have names and IDs that have never
    been used previously removes any naming issue.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'The second part launches the creation of the datasources, models, and evaluations:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Finally, the third part waits for the evaluation to be complete, records the
    RMSE for each removed feature, and deletes the datasources and models (we kept
    the evaluations to avoid having to rerun the whole script to get the results):'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'In the end, we get the following RMSE variations  for the first 10 features:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '`1stFlrSF 0.07%`'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2ndFlrSF -18.28%`'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BedroomAbvGr -0.02 %`'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BsmtExposure -0.56 %`'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BsmtFinSF2 -0.50 %`'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BsmtCond 0.00 %`'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BsmtFinSF1 -2.56 %`'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Alley 0.00 %`'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`3SsnPorch -4.60 %`'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BldgType -0.00 %`'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, removing the `2ndFlrSF` feature increased the RMSE by nearly 20%.
    This feature is definitely very important to predict salesPrice. Similarly, features
    `3SsnPorch` and `BsmtFinSF1` are also important to the model, since removing them
    increases the RMSE. On the other hand, removing `1stFlrSF`, `Alley`*,* `BedroomAbvGr`or`BldgType` only
    modified the RMSE by less than 0.10%. We can probably remove these feature without
    too much impact on the model performance.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，移除`2ndFlrSF`特征将RMSE提高了近20%。这个特征对于预测销售价格无疑非常重要。同样，特征`3SsnPorch`和`BsmtFinSF1`对于模型也很重要，因为移除它们会增加RMSE。另一方面，移除`1stFlrSF`、`Alley`、`BedroomAbvGr`或`BldgType`仅使RMSE变化小于0.10%。我们可能可以移除这些特征，而对模型性能的影响不大。
- en: Summary
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we have moved away from the Amazon ML web interface and learned
    how to work with the service through the AWS CLI and the Python SDK. The commands
    and methods for both types of interaction are very similar. The functions and
    commands perform a standard set of operations from creation to deletion of Amazon
    ML objects: datasources, models, evaluation, and batch predictions. The fact that
    Amazon ML chains the sequence of dependent object creation allows you to create
    all the objects at once without having to wait for one upstream to finish (datasource
    or model) before creating the downstream one (model or evaluation). The waiter
    methods make it possible to wait for all evaluations to be completed before retrieving
    the results and making the necessary object deletion.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经从Amazon ML的Web界面转向，学习了如何通过AWS CLI和Python SDK与该服务交互。这两种交互类型的命令和方法非常相似。函数和命令执行从创建到删除Amazon
    ML对象的标准操作集：数据源、模型、评估和批量预测。Amazon ML将依赖对象创建的序列链式化，这使得您可以在创建下游对象（模型或评估）之前，无需等待上游对象（数据源或模型）完成，一次性创建所有对象。等待方法使得在检索结果和进行必要的对象删除之前，能够等待所有评估完成。
- en: We showed how scripting Amazon ML allowed us to implement Machine Learning methods
    such as cross-validation and Recursive Feature Selection, both very useful methods
    in predictive analytics. Although we end up having to create many datasources
    and objects to conduct cross-validation and feature selection, the overall costs
    remain under control.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何通过脚本化Amazon ML来实现机器学习方法，如交叉验证和递归特征选择，这两种方法在预测分析中都非常有用。尽管我们最终需要创建许多数据源和对象来进行交叉验证和特征选择，但总体成本仍然保持在可控范围内。
- en: In the next chapter, we will start using other AWS services to expand the capabilities
    of Amazon ML. We will look at other datasources beyond S3, such as Redshift and
    RDS, and how to use Amazon Lambda for machine learning.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始使用其他AWS服务来扩展Amazon ML的功能。我们将探讨S3之外的其它数据源，例如Redshift和RDS，以及如何使用Amazon
    Lambda进行机器学习。
