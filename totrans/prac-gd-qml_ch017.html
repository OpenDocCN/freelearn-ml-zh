<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="pandoc" name="generator"/>
<title>ch017.xhtml</title>
<link href="../styles/stylesheet1.css" rel="stylesheet" type="text/css"/>
<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body epub:type="bodymatter">
<section class="level1 chapterHead" data-number="16" id="chapter-8-what-is-quantum-machine-learning">
<h1 class="H1---Chapter chapterHead" data-number="16"><span class="titlemark"><span class="koboSpan" id="kobo.1.1" xmlns="http://www.w3.org/1999/xhtml">Chapter 8</span></span><br/>
<span id="x1-1390008"><span class="koboSpan" id="kobo.2.1" xmlns="http://www.w3.org/1999/xhtml">What Is Quantum Machine Learning?</span></span></h1>
<div class="flushright">
<p><em><span class="koboSpan" id="kobo.3.1" xmlns="http://www.w3.org/1999/xhtml">Tell me and I forget. </span><span class="koboSpan" id="kobo.3.2" xmlns="http://www.w3.org/1999/xhtml">Teach me and I remember. </span><span class="koboSpan" id="kobo.3.3" xmlns="http://www.w3.org/1999/xhtml">Involve me and I</span></em> <em><span class="koboSpan" id="kobo.4.1" xmlns="http://www.w3.org/1999/xhtml">learn.</span></em><br/><span class="koboSpan" id="kobo.5.1" xmlns="http://www.w3.org/1999/xhtml">
— Benjamin Franklin</span></p>
</div>
<p><span class="koboSpan" id="kobo.6.1" xmlns="http://www.w3.org/1999/xhtml">We now begin our journey through </span><strong><span class="koboSpan" id="kobo.7.1" xmlns="http://www.w3.org/1999/xhtml">Quantum Machine Learning</span></strong><span class="koboSpan" id="kobo.8.1" xmlns="http://www.w3.org/1999/xhtml"> (</span><strong><span class="koboSpan" id="kobo.9.1" xmlns="http://www.w3.org/1999/xhtml">QML</span></strong><span class="koboSpan" id="kobo.10.1" xmlns="http://www.w3.org/1999/xhtml">). </span><span class="koboSpan" id="kobo.10.2" xmlns="http://www.w3.org/1999/xhtml">In this chapter, we will set the foundation for the remainder of this part of the book. </span><span class="koboSpan" id="kobo.10.3" xmlns="http://www.w3.org/1999/xhtml">We will begin by reviewing some general notions from classical machine learning, and then we will introduce the basic ideas that underlie QML as a whole.</span></p>
<p><span class="koboSpan" id="kobo.11.1" xmlns="http://www.w3.org/1999/xhtml">We’ll cover the following topics in this chapter:</span></p>
<ul>
<li><p><span class="koboSpan" id="kobo.12.1" xmlns="http://www.w3.org/1999/xhtml">The basics of machine learning</span></p></li>
<li><p><span class="koboSpan" id="kobo.13.1" xmlns="http://www.w3.org/1999/xhtml">Do you wanna train a model?</span></p></li>
<li><p><span class="koboSpan" id="kobo.14.1" xmlns="http://www.w3.org/1999/xhtml">Quantum-classical models</span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.15.1" xmlns="http://www.w3.org/1999/xhtml">In this chapter, you will learn the basic principles behind general machine learning, and you will understand how to construct, train, and assess some simple classical models using industry-standard frameworks and tools. </span><span class="koboSpan" id="kobo.15.2" xmlns="http://www.w3.org/1999/xhtml">We will also present a general picture of the world of QML.</span></p>
<section class="level2 sectionHead" data-number="16.1" id="the-basics-of-machine-learning">
<h1 class="sectionHead" data-number="16.1"><span class="titlemark"><span class="koboSpan" id="kobo.16.1" xmlns="http://www.w3.org/1999/xhtml">8.1 </span></span> <span id="x1-1400008.1"><span class="koboSpan" id="kobo.17.1" xmlns="http://www.w3.org/1999/xhtml">The basics of machine learning</span></span></h1>
<p><span class="koboSpan" id="kobo.18.1" xmlns="http://www.w3.org/1999/xhtml">Before discussing quantum </span><span id="dx1-140001"/><span class="koboSpan" id="kobo.19.1" xmlns="http://www.w3.org/1999/xhtml">machine learning, it may be a good idea to review some basic notions of Machine Learning (ML), in general. </span><span class="koboSpan" id="kobo.19.2" xmlns="http://www.w3.org/1999/xhtml">If you are familiar with the subject, feel free to skip this section. </span><span class="koboSpan" id="kobo.19.3" xmlns="http://www.w3.org/1999/xhtml">Please, keep in mind that the world of machine learning is extraordinarily vast; so much so that sometimes it is difficult to make general statements that can do justice to the overwhelming diversity of this field. </span><span class="koboSpan" id="kobo.19.4" xmlns="http://www.w3.org/1999/xhtml">For this reason, we will highlight those elements that will be more relevant for our purposes, while other aspects of machine learning — which are also of significant importance on their own — will be barely covered.</span></p>
<p><span class="koboSpan" id="kobo.20.1" xmlns="http://www.w3.org/1999/xhtml">In addition to this, please keep in mind that this will be a very condensed and hands-on introduction to machine learning. </span><span class="koboSpan" id="kobo.20.2" xmlns="http://www.w3.org/1999/xhtml">If you’d like to dive deeper into this field, we can recommend some very good books, such as the one by Abu-Mostafa, Magdon-Ismail, and Lin </span><span class="cite"><span class="koboSpan" id="kobo.21.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xabu2012learning"><span class="koboSpan" id="kobo.22.1" xmlns="http://www.w3.org/1999/xhtml">1</span></a><span class="koboSpan" id="kobo.23.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.24.1" xmlns="http://www.w3.org/1999/xhtml">, or the one by Aurélien Géron </span><span class="cite"><span class="koboSpan" id="kobo.25.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xhandsonml"><span class="koboSpan" id="kobo.26.1" xmlns="http://www.w3.org/1999/xhtml">104</span></a><span class="koboSpan" id="kobo.27.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.28.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<p><span class="koboSpan" id="kobo.29.1" xmlns="http://www.w3.org/1999/xhtml">As mysterious as machine learning may seem, the ideas that underlie it are fairly simple. </span><span class="koboSpan" id="kobo.29.2" xmlns="http://www.w3.org/1999/xhtml">In broad terms, we could define the purpose of machine learning to be the design of algorithms that can make a ”computational system” aware of patterns in data. </span><span class="koboSpan" id="kobo.29.3" xmlns="http://www.w3.org/1999/xhtml">These patterns can be truly anything. </span><span class="koboSpan" id="kobo.29.4" xmlns="http://www.w3.org/1999/xhtml">Maybe you want to design a system that can distinguish pictures of cats from pictures of rabbits. </span><span class="koboSpan" id="kobo.29.5" xmlns="http://www.w3.org/1999/xhtml">Maybe you would like to come up with a computational model that can transcribe verbal speech in English spoken with an Irish accent. </span><span class="koboSpan" id="kobo.29.6" xmlns="http://www.w3.org/1999/xhtml">Maybe you want to create a model able to generate realistic pictures of faces of people who do not exist, but that are indistinguishable from the real deal. </span><span class="koboSpan" id="kobo.29.7" xmlns="http://www.w3.org/1999/xhtml">The possibilities, as you surely know, are endless! </span><span class="koboSpan" id="kobo.29.8" xmlns="http://www.w3.org/1999/xhtml">What will be common to all these algorithms is that they will not be explicitly programmed to solve those tasks; instead, they will ”learn” how to do it from data…hence the name ”machine learning!”</span></p>
<p><span class="koboSpan" id="kobo.30.1" xmlns="http://www.w3.org/1999/xhtml">The cats versus rabbits example is a particular case of an </span><span id="dx1-140002"/><span class="koboSpan" id="kobo.31.1" xmlns="http://www.w3.org/1999/xhtml">interesting type of model: a </span><strong><span class="koboSpan" id="kobo.32.1" xmlns="http://www.w3.org/1999/xhtml">classifier</span></strong><span class="koboSpan" id="kobo.33.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.33.2" xmlns="http://www.w3.org/1999/xhtml">As the name suggests, a classifier is any kind of system that assigns, to every input, one label out of a finite set of possibilities. </span><span class="koboSpan" id="kobo.33.3" xmlns="http://www.w3.org/1999/xhtml">In many cases, there are just two of these labels, and they are commonly represented by </span><span class="koboSpan" id="kobo.34.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.35.1" xmlns="http://www.w3.org/1999/xhtml"> (read as </span><strong><span class="koboSpan" id="kobo.36.1" xmlns="http://www.w3.org/1999/xhtml">positive</span></strong><span class="koboSpan" id="kobo.37.1" xmlns="http://www.w3.org/1999/xhtml">) and </span><span class="koboSpan" id="kobo.38.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.39.1" xmlns="http://www.w3.org/1999/xhtml"> (</span><strong><span class="koboSpan" id="kobo.40.1" xmlns="http://www.w3.org/1999/xhtml">negative</span></strong><span class="koboSpan" id="kobo.41.1" xmlns="http://www.w3.org/1999/xhtml">); in physics applications, for instance, these labels are often read, respectively, as </span><strong><span class="koboSpan" id="kobo.42.1" xmlns="http://www.w3.org/1999/xhtml">signal</span></strong><span class="koboSpan" id="kobo.43.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><strong><span class="koboSpan" id="kobo.44.1" xmlns="http://www.w3.org/1999/xhtml">background</span></strong><span class="koboSpan" id="kobo.45.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.45.2" xmlns="http://www.w3.org/1999/xhtml">In this situation, we say that the classifier is </span><strong><span class="koboSpan" id="kobo.46.1" xmlns="http://www.w3.org/1999/xhtml">binary</span></strong><span class="koboSpan" id="kobo.47.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.47.2" xmlns="http://www.w3.org/1999/xhtml">Keep this in mind, for we will use this terminology in a few of the coming examples!</span></p>
<p><span class="koboSpan" id="kobo.48.1" xmlns="http://www.w3.org/1999/xhtml">So now that we know where we are heading, we need to answer one basic question: how can we make machine learning a reality?</span></p>
<section class="level3 subsectionHead" data-number="16.1.1" id="the-ingredients-for-machine-learning">
<h2 class="subsectionHead" data-number="16.1.1"><span class="titlemark"><span class="koboSpan" id="kobo.49.1" xmlns="http://www.w3.org/1999/xhtml">8.1.1 </span></span> <span id="x1-1410008.1.1"><span class="koboSpan" id="kobo.50.1" xmlns="http://www.w3.org/1999/xhtml">The ingredients for machine learning</span></span></h2>
<p><span class="koboSpan" id="kobo.51.1" xmlns="http://www.w3.org/1999/xhtml">In most machine </span><span id="dx1-141001"/><span class="koboSpan" id="kobo.52.1" xmlns="http://www.w3.org/1999/xhtml">learning setups, there are three basic ingredients:</span></p>
<ul>
<li><p><span class="koboSpan" id="kobo.53.1" xmlns="http://www.w3.org/1999/xhtml">First of all, we need some sort of computational model that is ”powerful enough” to tackle our problem. </span><span class="koboSpan" id="kobo.53.2" xmlns="http://www.w3.org/1999/xhtml">By this, we will often mean an algorithm that can be configured to solve the task at hand — at least to some level of accuracy.</span></p></li>
<li><p><span class="koboSpan" id="kobo.54.1" xmlns="http://www.w3.org/1999/xhtml">Then, if we want our model to capture patterns, we need to feed it some data so that it can do that. </span><span class="koboSpan" id="kobo.54.2" xmlns="http://www.w3.org/1999/xhtml">We will thus need data, preferably lots of it. </span><span class="koboSpan" id="kobo.54.3" xmlns="http://www.w3.org/1999/xhtml">The nature of this data will depend on the approach that we take, but, in most cases, we will need to transform it into numerical form. </span><span class="koboSpan" id="kobo.54.4" xmlns="http://www.w3.org/1999/xhtml">Most models expect data to be represented as vectors of real numbers called </span><strong><span class="koboSpan" id="kobo.55.1" xmlns="http://www.w3.org/1999/xhtml">attributes</span></strong><span class="koboSpan" id="kobo.56.1" xmlns="http://www.w3.org/1999/xhtml">, so this is </span><span id="dx1-141002"/><span class="koboSpan" id="kobo.57.1" xmlns="http://www.w3.org/1999/xhtml">what we will usually assume that we have.</span></p></li>
<li><p><span class="koboSpan" id="kobo.58.1" xmlns="http://www.w3.org/1999/xhtml">And, lastly, we need a </span><strong><span class="koboSpan" id="kobo.59.1" xmlns="http://www.w3.org/1999/xhtml">training procedure</span></strong><span class="koboSpan" id="kobo.60.1" xmlns="http://www.w3.org/1999/xhtml"> that will </span><span id="dx1-141003"/><span class="koboSpan" id="kobo.61.1" xmlns="http://www.w3.org/1999/xhtml">allow us to optimize the configuration of our model to make it solve the task (or, at least, come close to solving it!). </span><span class="koboSpan" id="kobo.61.2" xmlns="http://www.w3.org/1999/xhtml">In ML jargon, we could say that we need to find a way to make our model </span><strong><span class="koboSpan" id="kobo.62.1" xmlns="http://www.w3.org/1999/xhtml">learn</span></strong><span class="koboSpan" id="kobo.63.1" xmlns="http://www.w3.org/1999/xhtml"> in order to identify the patterns that hide behind the data in our problem.</span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.64.1" xmlns="http://www.w3.org/1999/xhtml">That is a pretty solid — yet somewhat oversimplified — wish-list. </span><span class="koboSpan" id="kobo.64.2" xmlns="http://www.w3.org/1999/xhtml">Let’s see how we can make more sense out of this.</span></p>
<p><span class="paragraphHead"><span id="x1-1420008.1.1"/><strong><span class="koboSpan" id="kobo.65.1" xmlns="http://www.w3.org/1999/xhtml">The model</span></strong></span><span class="koboSpan" id="kobo.66.1" xmlns="http://www.w3.org/1999/xhtml"> Let us first analyze that </span><span id="dx1-142001"/><span class="koboSpan" id="kobo.67.1" xmlns="http://www.w3.org/1999/xhtml">computational model that we have talked about. </span><span class="koboSpan" id="kobo.67.2" xmlns="http://www.w3.org/1999/xhtml">We said that it had to be ”powerful enough,” and this </span><span id="dx1-142002"/><span class="koboSpan" id="kobo.68.1" xmlns="http://www.w3.org/1999/xhtml">means that there should be a way to configure the model in such a way that it behaves as we intend it to.</span></p>
<p><span class="koboSpan" id="kobo.69.1" xmlns="http://www.w3.org/1999/xhtml">At first sight, this requirement may seem suspicious: how can we possibly be sure that such a configuration exists? </span><span class="koboSpan" id="kobo.69.2" xmlns="http://www.w3.org/1999/xhtml">In most real-life problems, we can never be fully sure…but we can be certain to some degree! </span><span class="koboSpan" id="kobo.69.3" xmlns="http://www.w3.org/1999/xhtml">This certainty may come from experience or, desirably, also from some theoretical results that justify it. </span><span class="koboSpan" id="kobo.69.4" xmlns="http://www.w3.org/1999/xhtml">For instance, you </span><span id="dx1-142003"/><span class="koboSpan" id="kobo.70.1" xmlns="http://www.w3.org/1999/xhtml">may have heard of </span><strong><span class="koboSpan" id="kobo.71.1" xmlns="http://www.w3.org/1999/xhtml">neural networks</span></strong><span class="koboSpan" id="kobo.72.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.72.2" xmlns="http://www.w3.org/1999/xhtml">We will discuss them shortly, but, for now, you should know that they are models that have been </span><span id="dx1-142004"/><span class="koboSpan" id="kobo.73.1" xmlns="http://www.w3.org/1999/xhtml">proven to be </span><strong><span class="koboSpan" id="kobo.74.1" xmlns="http://www.w3.org/1999/xhtml">universal function approximators</span></strong><span class="koboSpan" id="kobo.75.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.75.2" xmlns="http://www.w3.org/1999/xhtml">That is, any function can be approximated up to any given accuracy, no matter its complexity, by a large-enough neural network. </span><span class="koboSpan" id="kobo.75.3" xmlns="http://www.w3.org/1999/xhtml">That makes neural networks natural good choices for many problems.</span></p>
<p><span class="koboSpan" id="kobo.76.1" xmlns="http://www.w3.org/1999/xhtml">We will later discuss neural networks — and many other interesting models — in detail, but, to start with, we can consider a simplified version that, in fact, could be considered the grandparent of neural networks: the </span><strong><span class="koboSpan" id="kobo.77.1" xmlns="http://www.w3.org/1999/xhtml">perceptron</span></strong><span class="koboSpan" id="kobo.78.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<p><span class="koboSpan" id="kobo.79.1" xmlns="http://www.w3.org/1999/xhtml">A </span><span id="dx1-142005"/><span class="koboSpan" id="kobo.80.1" xmlns="http://www.w3.org/1999/xhtml">perceptron is a computational model that takes </span><span class="koboSpan" id="kobo.81.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="N" class="math inline" src="../media/file784.png" style="vertical-align:middle" title="N"/></span><span class="koboSpan" id="kobo.82.1" xmlns="http://www.w3.org/1999/xhtml"> numerical inputs and returns a single bit as output. </span><span class="koboSpan" id="kobo.82.2" xmlns="http://www.w3.org/1999/xhtml">This model depends on a collection of weights </span><span class="koboSpan" id="kobo.83.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="w_{i}" class="math inline" src="../media/file1095.png" style="vertical-align:middle" title="w_{i}"/></span><span class="koboSpan" id="kobo.84.1" xmlns="http://www.w3.org/1999/xhtml"> for </span><span class="koboSpan" id="kobo.85.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="i = 1,\ldots,N" class="math inline" src="../media/file1096.png" style="vertical-align:middle" title="i = 1,\ldots,N"/></span><span class="koboSpan" id="kobo.86.1" xmlns="http://www.w3.org/1999/xhtml"> and on a bias </span><span class="koboSpan" id="kobo.87.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="b" class="math inline" src="../media/file17.png" style="vertical-align:middle" title="b"/></span><span class="koboSpan" id="kobo.88.1" xmlns="http://www.w3.org/1999/xhtml">, and it behaves as follows: for any input </span><span class="koboSpan" id="kobo.89.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x_{1},\ldots,x_{N}" class="math inline" src="../media/file1097.png" style="vertical-align:middle" title="x_{1},\ldots,x_{N}"/></span><span class="koboSpan" id="kobo.90.1" xmlns="http://www.w3.org/1999/xhtml">, if</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.91.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\sum\limits_{i = 1}^{N}x_{i}w_{i} + b \geq 0," class="math display" src="../media/file1098.png" style="vertical-align:middle" title="\sum\limits_{i = 1}^{N}x_{i}w_{i} + b \geq 0,"/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.92.1" xmlns="http://www.w3.org/1999/xhtml">then the model returns </span><span class="koboSpan" id="kobo.93.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.94.1" xmlns="http://www.w3.org/1999/xhtml">, and otherwise it returns </span><span class="koboSpan" id="kobo.95.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.96.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<p><span class="koboSpan" id="kobo.97.1" xmlns="http://www.w3.org/1999/xhtml">This is a very simple computational model, but we could use it to set up a basic binary classifier by looking for some appropriate values for the weights and bias. </span><span class="koboSpan" id="kobo.97.2" xmlns="http://www.w3.org/1999/xhtml">That is, given a set of points on which we want the output to be </span><span class="koboSpan" id="kobo.98.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.99.1" xmlns="http://www.w3.org/1999/xhtml"> and another set of points on which the output should be </span><span class="koboSpan" id="kobo.100.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.101.1" xmlns="http://www.w3.org/1999/xhtml">, we can try to search for some values for the </span><span class="koboSpan" id="kobo.102.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="w_{i}" class="math inline" src="../media/file1095.png" style="vertical-align:middle" title="w_{i}"/></span><span class="koboSpan" id="kobo.103.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.104.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="b" class="math inline" src="../media/file17.png" style="vertical-align:middle" title="b"/></span><span class="koboSpan" id="kobo.105.1" xmlns="http://www.w3.org/1999/xhtml"> coefficients that would make the perceptron return the desired outputs. </span><span class="koboSpan" id="kobo.105.2" xmlns="http://www.w3.org/1999/xhtml">In fact, in the dawn of the machine learning age, it was already proven that there is a simple learning algorithm that, under the condition that the problem data can be linearly separated, finds coefficients that can effectively classify the data.</span></p>
<p><span class="koboSpan" id="kobo.106.1" xmlns="http://www.w3.org/1999/xhtml">There you have it, that could be your first baby machine learning model! </span><span class="koboSpan" id="kobo.106.2" xmlns="http://www.w3.org/1999/xhtml">Needless to say, a perceptron — at least on its own — is not a particularly powerful model, but it is, at least, a promising beginning!</span></p>
<div class="tcolorbox questionx" id="tcolobox-148">
<span id="x1-142007x8.1.1"/>
<div class="tcolorbox-title">
<p><span class="koboSpan" id="kobo.107.1" xmlns="http://www.w3.org/1999/xhtml">Exercise 8.1</span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.108.1" xmlns="http://www.w3.org/1999/xhtml">We can all agree that perceptrons are cute models. </span><span class="koboSpan" id="kobo.108.2" xmlns="http://www.w3.org/1999/xhtml">But just to get an idea of their limitations, prove that they cannot implement an XOR gate. </span><span class="koboSpan" id="kobo.108.3" xmlns="http://www.w3.org/1999/xhtml">That is, if you are given inputs </span><span class="koboSpan" id="kobo.109.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\{(0,1),(1,0)\}" class="math inline" src="../media/file1099.png" style="vertical-align:middle" title="\{(0,1),(1,0)\}"/></span><span class="koboSpan" id="kobo.110.1" xmlns="http://www.w3.org/1999/xhtml"> with desired output </span><span class="koboSpan" id="kobo.111.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.112.1" xmlns="http://www.w3.org/1999/xhtml"> and inputs </span><span class="koboSpan" id="kobo.113.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\{(0,0),(1,1)\}" class="math inline" src="../media/file1100.png" style="vertical-align:middle" title="\{(0,0),(1,1)\}"/></span><span class="koboSpan" id="kobo.114.1" xmlns="http://www.w3.org/1999/xhtml"> with desired output </span><span class="koboSpan" id="kobo.115.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.116.1" xmlns="http://www.w3.org/1999/xhtml">, there is no choice of perceptron weights and bias that works in this case.</span></p>
</div>
</div>
<p><span class="paragraphHead"><span id="x1-1430008.1.1"/><strong><span class="koboSpan" id="kobo.117.1" xmlns="http://www.w3.org/1999/xhtml">The training procedure</span></strong></span><span class="koboSpan" id="kobo.118.1" xmlns="http://www.w3.org/1999/xhtml"> Alright, so let’s say that we have a </span><span id="dx1-143001"/><span class="koboSpan" id="kobo.119.1" xmlns="http://www.w3.org/1999/xhtml">model that we believe is just powerful enough to approach our problem (and not too powerful either…more on that later!). </span><span class="koboSpan" id="kobo.119.2" xmlns="http://www.w3.org/1999/xhtml">We will restrict ourselves to assuming that the configuration of our model depends on some numerical parameters </span><span class="koboSpan" id="kobo.120.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\theta" class="math inline" src="../media/file89.png" style="vertical-align:middle" title="\theta"/></span><span class="koboSpan" id="kobo.121.1" xmlns="http://www.w3.org/1999/xhtml">; this would mean that we would be looking for some choice </span><span class="koboSpan" id="kobo.122.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\theta_{0}" class="math inline" src="../media/file1045.png" style="vertical-align:middle" title="\theta_{0}"/></span><span class="koboSpan" id="kobo.123.1" xmlns="http://www.w3.org/1999/xhtml"> of those parameters that will make our model work as well as possible. </span><span class="koboSpan" id="kobo.123.2" xmlns="http://www.w3.org/1999/xhtml">So, how do we find those parameters?</span></p>
<div class="tcolorbox learnmore" id="tcolobox-149">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.124.1" xmlns="http://www.w3.org/1999/xhtml">To learn more</span></span><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.125.1" xmlns="http://www.w3.org/1999/xhtml">…</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.126.1" xmlns="http://www.w3.org/1999/xhtml">We will only discuss models whose behavior can be adjusted and defined solely by tweaking some numerical parameters, as in the case of the perceptron. </span><span class="koboSpan" id="kobo.126.2" xmlns="http://www.w3.org/1999/xhtml">Nevertheless, there also exist </span><strong><span class="koboSpan" id="kobo.127.1" xmlns="http://www.w3.org/1999/xhtml">non-parametric</span></strong> <span id="dx1-143002"/><span class="koboSpan" id="kobo.128.1" xmlns="http://www.w3.org/1999/xhtml">models that don’t behave in this manner. </span><span class="koboSpan" id="kobo.128.2" xmlns="http://www.w3.org/1999/xhtml">A popular example is the </span><span class="koboSpan" id="kobo.129.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="k" class="math inline" src="../media/file317.png" style="vertical-align:middle" title="k"/></span><span class="koboSpan" id="kobo.130.1" xmlns="http://www.w3.org/1999/xhtml">-nearest neighbours algorithm; you can find some information in the references </span><span class="cite"><span class="koboSpan" id="kobo.131.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xhandsonml"><span class="koboSpan" id="kobo.132.1" xmlns="http://www.w3.org/1999/xhtml">104</span></a><span class="koboSpan" id="kobo.133.1" xmlns="http://www.w3.org/1999/xhtml">, Chapter 1]</span></span><span class="koboSpan" id="kobo.134.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.135.1" xmlns="http://www.w3.org/1999/xhtml">To illustrate all of this, we will discuss how to train a parametric model to implement a binary classifier. </span><span class="koboSpan" id="kobo.135.2" xmlns="http://www.w3.org/1999/xhtml">That is, we aim to build a binary classifier on a certain domain </span><span class="koboSpan" id="kobo.136.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="D" class="math inline" src="../media/file1101.png" style="vertical-align:middle" title="D"/></span><span class="koboSpan" id="kobo.137.1" xmlns="http://www.w3.org/1999/xhtml"> with some elements </span><span class="koboSpan" id="kobo.138.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x" class="math inline" src="../media/file269.png" style="vertical-align:middle" title="x"/></span><span class="koboSpan" id="kobo.139.1" xmlns="http://www.w3.org/1999/xhtml"> that should be each classified as a certain </span><span class="koboSpan" id="kobo.140.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="y" class="math inline" src="../media/file270.png" style="vertical-align:middle" title="y"/></span><span class="koboSpan" id="kobo.141.1" xmlns="http://www.w3.org/1999/xhtml"> (where </span><span class="koboSpan" id="kobo.142.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="y" class="math inline" src="../media/file270.png" style="vertical-align:middle" title="y"/></span><span class="koboSpan" id="kobo.143.1" xmlns="http://www.w3.org/1999/xhtml"> can be either </span><span class="koboSpan" id="kobo.144.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.145.1" xmlns="http://www.w3.org/1999/xhtml"> or </span><span class="koboSpan" id="kobo.146.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.147.1" xmlns="http://www.w3.org/1999/xhtml">). </span><span class="koboSpan" id="kobo.147.2" xmlns="http://www.w3.org/1999/xhtml">For this, we will use a model </span><span class="koboSpan" id="kobo.148.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="M" class="math inline" src="../media/file704.png" style="vertical-align:middle" title="M"/></span><span class="koboSpan" id="kobo.149.1" xmlns="http://www.w3.org/1999/xhtml"> that depends on some parameters in a way that, for any choice </span><span class="koboSpan" id="kobo.150.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\theta" class="math inline" src="../media/file89.png" style="vertical-align:middle" title="\theta"/></span><span class="koboSpan" id="kobo.151.1" xmlns="http://www.w3.org/1999/xhtml"> of these parameters, it returns a label </span><span class="koboSpan" id="kobo.152.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="M_{\theta}(x)" class="math inline" src="../media/file1102.png" style="vertical-align:middle" title="M_{\theta}(x)"/></span><span class="koboSpan" id="kobo.153.1" xmlns="http://www.w3.org/1999/xhtml"> for every element </span><span class="koboSpan" id="kobo.154.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x \in D" class="math inline" src="../media/file1103.png" style="vertical-align:middle" title="x \in D"/></span><span class="koboSpan" id="kobo.155.1" xmlns="http://www.w3.org/1999/xhtml"> in the dataset.</span></p>
<p><span class="koboSpan" id="kobo.156.1" xmlns="http://www.w3.org/1999/xhtml">In this scenario, our goal is to look for a choice of parameters </span><span class="koboSpan" id="kobo.157.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\theta" class="math inline" src="../media/file89.png" style="vertical-align:middle" title="\theta"/></span><span class="koboSpan" id="kobo.158.1" xmlns="http://www.w3.org/1999/xhtml"> that can minimize the probability that any random input </span><span class="koboSpan" id="kobo.159.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x" class="math inline" src="../media/file269.png" style="vertical-align:middle" title="x"/></span><span class="koboSpan" id="kobo.160.1" xmlns="http://www.w3.org/1999/xhtml"> be misclassified. </span><span class="koboSpan" id="kobo.160.2" xmlns="http://www.w3.org/1999/xhtml">To put it in slightly more formal terms, if </span><span class="koboSpan" id="kobo.161.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="y" class="math inline" src="../media/file270.png" style="vertical-align:middle" title="y"/></span><span class="koboSpan" id="kobo.162.1" xmlns="http://www.w3.org/1999/xhtml"> is the correct label to be assigned to an input </span><span class="koboSpan" id="kobo.163.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x" class="math inline" src="../media/file269.png" style="vertical-align:middle" title="x"/></span><span class="koboSpan" id="kobo.164.1" xmlns="http://www.w3.org/1999/xhtml">, we want to minimize </span><span class="koboSpan" id="kobo.165.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="P(M_{\theta}(x) \neq y)" class="math inline" src="../media/file1104.png" style="vertical-align:middle" title="P(M_{\theta}(x) \neq y)"/></span><span class="koboSpan" id="kobo.166.1" xmlns="http://www.w3.org/1999/xhtml">, that is, the probability of assigning an incorrect label to </span><span class="koboSpan" id="kobo.167.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x" class="math inline" src="../media/file269.png" style="vertical-align:middle" title="x"/></span><span class="koboSpan" id="kobo.168.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.168.2" xmlns="http://www.w3.org/1999/xhtml">In this way, we have reduced the problem of training our model to the problem of finding some parameters </span><span class="koboSpan" id="kobo.169.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\theta" class="math inline" src="../media/file89.png" style="vertical-align:middle" title="\theta"/></span><span class="koboSpan" id="kobo.170.1" xmlns="http://www.w3.org/1999/xhtml"> that minimize </span><span class="koboSpan" id="kobo.171.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="P(M_{\theta}(x) \neq y)" class="math inline" src="../media/file1104.png" style="vertical-align:middle" title="P(M_{\theta}(x) \neq y)"/></span><span class="koboSpan" id="kobo.172.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.172.2" xmlns="http://www.w3.org/1999/xhtml">This probability is </span><span id="dx1-143003"/><span class="koboSpan" id="kobo.173.1" xmlns="http://www.w3.org/1999/xhtml">known as the </span><strong><span class="koboSpan" id="kobo.174.1" xmlns="http://www.w3.org/1999/xhtml">generalization error</span></strong><span class="koboSpan" id="kobo.175.1" xmlns="http://www.w3.org/1999/xhtml"> or </span><span id="dx1-143004"/><span class="koboSpan" id="kobo.176.1" xmlns="http://www.w3.org/1999/xhtml">the </span><strong><span class="koboSpan" id="kobo.177.1" xmlns="http://www.w3.org/1999/xhtml">true</span></strong> <strong><span class="koboSpan" id="kobo.178.1" xmlns="http://www.w3.org/1999/xhtml">error</span></strong><span class="koboSpan" id="kobo.179.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<p><span class="koboSpan" id="kobo.180.1" xmlns="http://www.w3.org/1999/xhtml">Now, if we had access to all the </span><span id="dx1-143005"/><span class="koboSpan" id="kobo.181.1" xmlns="http://www.w3.org/1999/xhtml">possible inputs in our domain </span><span class="koboSpan" id="kobo.182.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="D" class="math inline" src="../media/file1101.png" style="vertical-align:middle" title="D"/></span><span class="koboSpan" id="kobo.183.1" xmlns="http://www.w3.org/1999/xhtml"> and we knew all their expected outputs, we would simply have to minimize the true error…and we would be done! </span><span class="koboSpan" id="kobo.183.2" xmlns="http://www.w3.org/1999/xhtml">Nevertheless, this is neither an interesting situation nor a common one.</span></p>
<p><span class="koboSpan" id="kobo.184.1" xmlns="http://www.w3.org/1999/xhtml">If we had a problem in which we already knew all the possible inputs and their outputs…why should we bother with all this machine learning business? </span><span class="koboSpan" id="kobo.184.2" xmlns="http://www.w3.org/1999/xhtml">We could just implement an old-school algorithm! </span><span class="koboSpan" id="kobo.184.3" xmlns="http://www.w3.org/1999/xhtml">Indeed, the whole point of ”learning” is being able to predict correct outputs for unseen data. </span><span class="koboSpan" id="kobo.184.4" xmlns="http://www.w3.org/1999/xhtml">Thus, when we resort to machine learning, we do so because we do not have full access to all the possible inputs and outputs in our domain — either because it is unfeasible or because such a domain might be infinite!</span></p>
<p><span class="koboSpan" id="kobo.185.1" xmlns="http://www.w3.org/1999/xhtml">So now we are faced with a problem. </span><span class="koboSpan" id="kobo.185.2" xmlns="http://www.w3.org/1999/xhtml">We have a (potentially infinite) domain of data over which we have to minimize the true error, yet we only have access to a finite subset of it. </span><span class="koboSpan" id="kobo.185.3" xmlns="http://www.w3.org/1999/xhtml">But…how on earth can we compute the true error in order to minimize it? </span><span class="koboSpan" id="kobo.185.4" xmlns="http://www.w3.org/1999/xhtml">The answer is that, in general, we can’t, because we would need complete information on how all the data and the labels of our problem are distributed, something that we usually don’t have. </span><span class="koboSpan" id="kobo.185.5" xmlns="http://www.w3.org/1999/xhtml">Nevertheless, we still have access to a — presumably large — subset of data. </span><span class="koboSpan" id="kobo.185.6" xmlns="http://www.w3.org/1999/xhtml">Can we use it to our advantage? </span><span class="koboSpan" id="kobo.185.7" xmlns="http://www.w3.org/1999/xhtml">Yes, we surely can! </span><span class="koboSpan" id="kobo.185.8" xmlns="http://www.w3.org/1999/xhtml">The usual strategy is to divide the dataset that we have in two </span><span id="dx1-143006"/><span class="koboSpan" id="kobo.186.1" xmlns="http://www.w3.org/1999/xhtml">separate sets: a </span><strong><span class="koboSpan" id="kobo.187.1" xmlns="http://www.w3.org/1999/xhtml">training dataset</span></strong><span class="koboSpan" id="kobo.188.1" xmlns="http://www.w3.org/1999/xhtml"> and a </span><strong><span class="koboSpan" id="kobo.189.1" xmlns="http://www.w3.org/1999/xhtml">test</span></strong> <strong><span class="koboSpan" id="kobo.190.1" xmlns="http://www.w3.org/1999/xhtml">dataset</span></strong><span class="koboSpan" id="kobo.191.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.191.2" xmlns="http://www.w3.org/1999/xhtml">The training set, usually much bigger than the </span><span id="dx1-143007"/><span class="koboSpan" id="kobo.192.1" xmlns="http://www.w3.org/1999/xhtml">test set, will be used to adjust the parameters of our model in an attempt to minimize the true error, while the test set will be used to estimate the true error itself.</span></p>
<p><span class="koboSpan" id="kobo.193.1" xmlns="http://www.w3.org/1999/xhtml">Thus, what we can do is just take whichever </span><span id="dx1-143008"/><span class="koboSpan" id="kobo.194.1" xmlns="http://www.w3.org/1999/xhtml">training dataset </span><span class="koboSpan" id="kobo.195.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="T" class="math inline" src="../media/file74.png" style="vertical-align:middle" title="T"/></span><span class="koboSpan" id="kobo.196.1" xmlns="http://www.w3.org/1999/xhtml"> we are using, and — instead of minimizing the true error, to which we simply don’t have access — we can try to minimize the </span><strong><span class="koboSpan" id="kobo.197.1" xmlns="http://www.w3.org/1999/xhtml">empirical error</span></strong><span class="koboSpan" id="kobo.198.1" xmlns="http://www.w3.org/1999/xhtml">: the </span><span id="dx1-143009"/><span class="koboSpan" id="kobo.199.1" xmlns="http://www.w3.org/1999/xhtml">probability of misclassifying an element within the training dataset. </span><span class="koboSpan" id="kobo.199.2" xmlns="http://www.w3.org/1999/xhtml">This empirical error would be computed as the proportion of misclassified elements in </span><span class="koboSpan" id="kobo.200.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="T" class="math inline" src="../media/file74.png" style="vertical-align:middle" title="T"/></span><span class="koboSpan" id="kobo.201.1" xmlns="http://www.w3.org/1999/xhtml">:</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.202.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="R_{\text{emp}}(\theta) = \frac{1}{|T|}\sum\limits_{(x,y) \in T}1 - \delta(M_{\theta}(x),y)," class="math display" src="../media/file1105.png" style="vertical-align:middle" title="R_{\text{emp}}(\theta) = \frac{1}{|T|}\sum\limits_{(x,y) \in T}1 - \delta(M_{\theta}(x),y),"/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.203.1" xmlns="http://www.w3.org/1999/xhtml">where </span><span class="koboSpan" id="kobo.204.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="|T|" class="math inline" src="../media/file1106.png" style="vertical-align:middle" title="|T|"/></span><span class="koboSpan" id="kobo.205.1" xmlns="http://www.w3.org/1999/xhtml"> is the size of the training dataset and </span><span class="koboSpan" id="kobo.206.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\delta(a,b)" class="math inline" src="../media/file1107.png" style="vertical-align:middle" title="\delta(a,b)"/></span><span class="koboSpan" id="kobo.207.1" xmlns="http://www.w3.org/1999/xhtml"> is </span><span class="koboSpan" id="kobo.208.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.209.1" xmlns="http://www.w3.org/1999/xhtml"> if </span><span class="koboSpan" id="kobo.210.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="a = b" class="math inline" src="../media/file1108.png" style="vertical-align:middle" title="a = b"/></span><span class="koboSpan" id="kobo.211.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.212.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.213.1" xmlns="http://www.w3.org/1999/xhtml"> otherwise (this </span><span class="koboSpan" id="kobo.214.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\delta" class="math inline" src="../media/file1109.png" style="vertical-align:middle" title="\delta"/></span><span class="koboSpan" id="kobo.215.1" xmlns="http://www.w3.org/1999/xhtml"> function is known as the Kronecker delta). </span><span class="koboSpan" id="kobo.215.2" xmlns="http://www.w3.org/1999/xhtml">We would do all of this, of course, hoping that the real error would take similar values to the empirical error. </span><span class="koboSpan" id="kobo.215.3" xmlns="http://www.w3.org/1999/xhtml">Naturally, this hope will have to be justified and rest on some evidence, and we will soon see how the test dataset can help us with that. </span><span class="koboSpan" id="kobo.215.4" xmlns="http://www.w3.org/1999/xhtml">In any case, if we want all this setup to work, we will need to use a large enough dataset.</span></p>
<p><span class="koboSpan" id="kobo.216.1" xmlns="http://www.w3.org/1999/xhtml">Our goal then is to minimize the true error, and, so far, our only strategy is trying to achieve it by minimizing the empirical error. </span><span class="koboSpan" id="kobo.216.2" xmlns="http://www.w3.org/1999/xhtml">Nevertheless, in practice, we don’t often work with these magnitudes directly. </span><span class="koboSpan" id="kobo.216.3" xmlns="http://www.w3.org/1999/xhtml">Instead, we take a more ”general” approach: we seek to minimize the expected value of a </span><strong><span class="koboSpan" id="kobo.217.1" xmlns="http://www.w3.org/1999/xhtml">loss function</span></strong><span class="koboSpan" id="kobo.218.1" xmlns="http://www.w3.org/1999/xhtml">, which is </span><span id="dx1-143010"/><span class="koboSpan" id="kobo.219.1" xmlns="http://www.w3.org/1999/xhtml">defined for every choice of parameters </span><span class="koboSpan" id="kobo.220.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\theta" class="math inline" src="../media/file89.png" style="vertical-align:middle" title="\theta"/></span><span class="koboSpan" id="kobo.221.1" xmlns="http://www.w3.org/1999/xhtml"> and every possible input </span><span class="koboSpan" id="kobo.222.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x" class="math inline" src="../media/file269.png" style="vertical-align:middle" title="x"/></span><span class="koboSpan" id="kobo.223.1" xmlns="http://www.w3.org/1999/xhtml"> and its desired output </span><span class="koboSpan" id="kobo.224.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="y" class="math inline" src="../media/file270.png" style="vertical-align:middle" title="y"/></span><span class="koboSpan" id="kobo.225.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.225.2" xmlns="http://www.w3.org/1999/xhtml">For instance, we could define the 0-1 loss function as</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.226.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="L_{01}(\theta;x,y) = 1 - \delta(M_{\theta}(x),y)." class="math display" src="../media/file1110.png" style="vertical-align:middle" title="L_{01}(\theta;x,y) = 1 - \delta(M_{\theta}(x),y)."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.227.1" xmlns="http://www.w3.org/1999/xhtml">With this definition, it is trivial to see that the expected value, taken over the whole domain, of </span><span class="koboSpan" id="kobo.228.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="L_{01}" class="math inline" src="../media/file1111.png" style="vertical-align:middle" title="L_{01}"/></span><span class="koboSpan" id="kobo.229.1" xmlns="http://www.w3.org/1999/xhtml"> is exactly the true error; this expected value is </span><span id="dx1-143011"/><span class="koboSpan" id="kobo.230.1" xmlns="http://www.w3.org/1999/xhtml">known as the </span><strong><span class="koboSpan" id="kobo.231.1" xmlns="http://www.w3.org/1999/xhtml">true risk</span></strong><span class="koboSpan" id="kobo.232.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.232.2" xmlns="http://www.w3.org/1999/xhtml">In the same way, the expected value of this loss function over the training sample is the empirical error.</span></p>
<div class="tcolorbox important" id="tcolobox-150">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.233.1" xmlns="http://www.w3.org/1999/xhtml">Important note</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.234.1" xmlns="http://www.w3.org/1999/xhtml">Keep in mind that the expected value of a loss function over a finite dataset will just be its average value.</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.235.1" xmlns="http://www.w3.org/1999/xhtml">So, in practice, our strategy for minimizing the true error will be minimizing the expected value of a suitable loss function over the training dataset. </span><span class="koboSpan" id="kobo.235.2" xmlns="http://www.w3.org/1999/xhtml">We will refer to this expected value as the </span><strong><span class="koboSpan" id="kobo.236.1" xmlns="http://www.w3.org/1999/xhtml">empirical risk</span></strong><span class="koboSpan" id="kobo.237.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.237.2" xmlns="http://www.w3.org/1999/xhtml">For </span><span id="dx1-143012"/><span class="koboSpan" id="kobo.238.1" xmlns="http://www.w3.org/1999/xhtml">reasons that we will discuss later, we will usually consider loss functions different from </span><span class="koboSpan" id="kobo.239.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="L_{01}" class="math inline" src="../media/file1111.png" style="vertical-align:middle" title="L_{01}"/></span><span class="koboSpan" id="kobo.240.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<p><span class="paragraphHead"><span id="x1-1440008.1.1"/><strong><span class="koboSpan" id="kobo.241.1" xmlns="http://www.w3.org/1999/xhtml">Assessing a trained model</span></strong></span><span class="koboSpan" id="kobo.242.1" xmlns="http://www.w3.org/1999/xhtml"> We now have to address an </span><span id="dx1-144001"/><span class="koboSpan" id="kobo.243.1" xmlns="http://www.w3.org/1999/xhtml">important question. </span><span class="koboSpan" id="kobo.243.2" xmlns="http://www.w3.org/1999/xhtml">How can we be sure that — once we have trained a model — it will perform well on data outside the training dataset? </span><span class="koboSpan" id="kobo.243.3" xmlns="http://www.w3.org/1999/xhtml">For that, we cannot solely rely on </span><span class="koboSpan" id="kobo.244.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="R_{\text{emp}}(\theta)" class="math inline" src="../media/file1112.png" style="vertical-align:middle" title="R_{\text{emp}}(\theta)"/></span><span class="koboSpan" id="kobo.245.1" xmlns="http://www.w3.org/1999/xhtml"> because that average loss is computed on data that the classifier has already seen. </span><span class="koboSpan" id="kobo.245.2" xmlns="http://www.w3.org/1999/xhtml">That would be like testing a student only on problems that the teacher has already solved in class!</span></p>
<p><span class="koboSpan" id="kobo.246.1" xmlns="http://www.w3.org/1999/xhtml">Thankfully, there’s something that can save the day. </span><span class="koboSpan" id="kobo.246.2" xmlns="http://www.w3.org/1999/xhtml">Do you remember that test dataset we talked about before? </span><span class="koboSpan" id="kobo.246.3" xmlns="http://www.w3.org/1999/xhtml">This is its time to shine! </span><span class="koboSpan" id="kobo.246.4" xmlns="http://www.w3.org/1999/xhtml">In fact, we have kept this test set in a safe-deposit box to ensure that none of its examples were ever used in the training process. </span><span class="koboSpan" id="kobo.246.5" xmlns="http://www.w3.org/1999/xhtml">We can think of them as completely new problems that the student has never seen, so we can use them to assess their understanding of the subject. </span><span class="koboSpan" id="kobo.246.6" xmlns="http://www.w3.org/1999/xhtml">Thus, we can compute the average loss of </span><span class="koboSpan" id="kobo.247.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="M_{\theta}" class="math inline" src="../media/file1113.png" style="vertical-align:middle" title="M_{\theta}"/></span><span class="koboSpan" id="kobo.248.1" xmlns="http://www.w3.org/1999/xhtml"> on the examples of the test set — this is sometimes called the </span><strong><span class="koboSpan" id="kobo.249.1" xmlns="http://www.w3.org/1999/xhtml">test error</span></strong><span class="koboSpan" id="kobo.250.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.250.2" xmlns="http://www.w3.org/1999/xhtml">Provided that they are representative of the classification problem as a whole and that the number of examples is big enough, we can be quite confident that the test error will be close to the true error of the model. </span><span class="koboSpan" id="kobo.250.3" xmlns="http://www.w3.org/1999/xhtml">This is just an application of some </span><em><span class="koboSpan" id="kobo.251.1" xmlns="http://www.w3.org/1999/xhtml">central</span></em><span class="koboSpan" id="kobo.252.1" xmlns="http://www.w3.org/1999/xhtml"> theorems in statistics!</span></p>
<p><span class="koboSpan" id="kobo.253.1" xmlns="http://www.w3.org/1999/xhtml">Now, if the test error is similar to the empirical risk (and if they are low enough), we are done. </span><span class="koboSpan" id="kobo.253.2" xmlns="http://www.w3.org/1999/xhtml">That’s it! </span><span class="koboSpan" id="kobo.253.3" xmlns="http://www.w3.org/1999/xhtml">We have successfully trained a model. </span><span class="koboSpan" id="kobo.253.4" xmlns="http://www.w3.org/1999/xhtml">Nonetheless, as you can imagine, things can also go wrong. </span><span class="koboSpan" id="kobo.253.5" xmlns="http://www.w3.org/1999/xhtml">Terribly wrong.</span></p>
<p><span class="koboSpan" id="kobo.254.1" xmlns="http://www.w3.org/1999/xhtml">What if the test error is much bigger than the empirical error, the one computed on the training set? </span><span class="koboSpan" id="kobo.254.2" xmlns="http://www.w3.org/1999/xhtml">This would be similar to having a student who knows how to repeat the solution to problems already solved by the teacher but who is unable to solve new problems. </span><span class="koboSpan" id="kobo.254.3" xmlns="http://www.w3.org/1999/xhtml">In our case, this would mean having a classifier that works beautifully on the training dataset but makes a lot of errors on any inputs outside of it.</span></p>
<p><span class="koboSpan" id="kobo.255.1" xmlns="http://www.w3.org/1999/xhtml">This </span><span id="dx1-144002"/><span class="koboSpan" id="kobo.256.1" xmlns="http://www.w3.org/1999/xhtml">situation is called </span><strong><span class="koboSpan" id="kobo.257.1" xmlns="http://www.w3.org/1999/xhtml">overfitting</span></strong><span class="koboSpan" id="kobo.258.1" xmlns="http://www.w3.org/1999/xhtml">, and it is one of the </span><span id="dx1-144003"/><span class="koboSpan" id="kobo.259.1" xmlns="http://www.w3.org/1999/xhtml">biggest risks (no pun intended) in machine learning. </span><span class="koboSpan" id="kobo.259.2" xmlns="http://www.w3.org/1999/xhtml">It occurs whenever, somehow, our model has learned the particularities of the data it has seen but not the general patterns; that is, it has fitted the training data too well, hence the name ”overfitting.” </span><span class="koboSpan" id="kobo.259.3" xmlns="http://www.w3.org/1999/xhtml">This problem usually occurs when the training dataset is too small or when the model is too powerful. </span><span class="koboSpan" id="kobo.259.4" xmlns="http://www.w3.org/1999/xhtml">In the first case, there is simply not enough information to extract general patterns. </span><span class="koboSpan" id="kobo.259.5" xmlns="http://www.w3.org/1999/xhtml">That is why, in this chapter, we have insisted that the more data we have, the better. </span><span class="koboSpan" id="kobo.259.6" xmlns="http://www.w3.org/1999/xhtml">But what about the second case? </span><span class="koboSpan" id="kobo.259.7" xmlns="http://www.w3.org/1999/xhtml">Why can having a very powerful model end up being something bad?</span></p>
<p><span class="koboSpan" id="kobo.260.1" xmlns="http://www.w3.org/1999/xhtml">An example can be very illustrative here. </span><span class="koboSpan" id="kobo.260.2" xmlns="http://www.w3.org/1999/xhtml">Let’s say that we want to use machine learning to approximate some unknown real function. </span><span class="koboSpan" id="kobo.260.3" xmlns="http://www.w3.org/1999/xhtml">We haven’t discussed how this setup would work, but the core ideas would be analogous to the ones we have seen (we would seek to minimize the expected value of a loss function, and so on). </span><span class="koboSpan" id="kobo.260.4" xmlns="http://www.w3.org/1999/xhtml">If we have a sample of </span><span class="koboSpan" id="kobo.261.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1000" class="math inline" src="../media/file790.png" style="vertical-align:middle" title="1000"/></span><span class="koboSpan" id="kobo.262.1" xmlns="http://www.w3.org/1999/xhtml"> points in the plane, we can always find a polynomial of degree </span><span class="koboSpan" id="kobo.263.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="999" class="math inline" src="../media/file1114.png" style="vertical-align:middle" title="999"/></span><span class="koboSpan" id="kobo.264.1" xmlns="http://www.w3.org/1999/xhtml"> that fits the data perfectly, in the same way that we can always fit a line to just two points. </span><span class="koboSpan" id="kobo.264.2" xmlns="http://www.w3.org/1999/xhtml">However, if the points are just samples of </span><span class="koboSpan" id="kobo.265.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="f(x) = x" class="math inline" src="../media/file1115.png" style="vertical-align:middle" title="f(x) = x"/></span><span class="koboSpan" id="kobo.266.1" xmlns="http://www.w3.org/1999/xhtml"> with some noise (which could result from some empirical sampling errors or some other reason), our polynomial will go out of its way to fit those points perfectly and will quickly deviate from the linear shape that it should have learned. </span><span class="koboSpan" id="kobo.266.2" xmlns="http://www.w3.org/1999/xhtml">In this way, being able to fit too much information can sometimes go against the goal of learning the general patterns of data. </span><span class="koboSpan" id="kobo.266.3" xmlns="http://www.w3.org/1999/xhtml">This is illustrated in </span><em><span class="koboSpan" id="kobo.267.1" xmlns="http://www.w3.org/1999/xhtml">Figure</span></em> <a href="#Figure8.1"><em><span class="koboSpan" id="kobo.268.1" xmlns="http://www.w3.org/1999/xhtml">8.1</span></em></a><span class="koboSpan" id="kobo.269.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.269.2" xmlns="http://www.w3.org/1999/xhtml">In it, the degree of the ”fitting” polynomial is so big that it can fit the training data perfectly, including its noise, but it misses the implicit linear pattern and performs very badly on test data.</span></p>
<figure>
<span class="koboSpan" id="kobo.270.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="Figure 8.1: A simple example of overfitting that results from using too powerful a model." src="../media/file1116.png"/></span>
<figcaption aria-hidden="true"><span class="id" id="Figure8.1"><strong><span class="koboSpan" id="kobo.271.1" xmlns="http://www.w3.org/1999/xhtml">Figure 8.1</span></strong><span class="koboSpan" id="kobo.272.1" xmlns="http://www.w3.org/1999/xhtml">: </span></span><span class="koboSpan" id="kobo.273.1" xmlns="http://www.w3.org/1999/xhtml">A simple example of overfitting that results from using too powerful a model.</span></figcaption>
</figure>
<div class="tcolorbox important" id="tcolobox-151">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.274.1" xmlns="http://www.w3.org/1999/xhtml">Important note</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.275.1" xmlns="http://www.w3.org/1999/xhtml">Sometimes a machine learning model may only work properly on its training dataset. </span><span class="koboSpan" id="kobo.275.2" xmlns="http://www.w3.org/1999/xhtml">This phenomenon is </span><span id="dx1-144006"/><span class="koboSpan" id="kobo.276.1" xmlns="http://www.w3.org/1999/xhtml">known as </span><strong><span class="koboSpan" id="kobo.277.1" xmlns="http://www.w3.org/1999/xhtml">overfitting</span></strong><span class="koboSpan" id="kobo.278.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.278.2" xmlns="http://www.w3.org/1999/xhtml">It usually occurs when the training dataset is too small or when the model is too powerful.</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.279.1" xmlns="http://www.w3.org/1999/xhtml">If you find</span><span id="dx1-144007"/><span class="koboSpan" id="kobo.280.1" xmlns="http://www.w3.org/1999/xhtml"> yourself in a </span><span id="dx1-144008"/><span class="koboSpan" id="kobo.281.1" xmlns="http://www.w3.org/1999/xhtml">situation in which your model has overfitted the data, you can try obtaining more data — something that is not always possible — or somehow reducing the power of your model. </span><span class="koboSpan" id="kobo.281.2" xmlns="http://www.w3.org/1999/xhtml">For instance, with the neural networks that we will be studying later in the chapter, you can try reducing their size.</span></p>
<div class="tcolorbox learnmore" id="tcolobox-152">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.282.1" xmlns="http://www.w3.org/1999/xhtml">To learn more</span></span><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.283.1" xmlns="http://www.w3.org/1999/xhtml">…</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.284.1" xmlns="http://www.w3.org/1999/xhtml">Another popular technique for avoiding </span><span id="dx1-144009"/><span class="koboSpan" id="kobo.285.1" xmlns="http://www.w3.org/1999/xhtml">overfitting is using </span><strong><span class="koboSpan" id="kobo.286.1" xmlns="http://www.w3.org/1999/xhtml">regularization</span></strong><span class="koboSpan" id="kobo.287.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.287.2" xmlns="http://www.w3.org/1999/xhtml">Roughly speaking, regularization restricts the values that some of the parameters of your model can take, effectively making it less powerful and less prone to fit every single detail of the training data.</span></p>
<p><span class="koboSpan" id="kobo.288.1" xmlns="http://www.w3.org/1999/xhtml">To learn more about regularization techniques and their use in machine learning, we highly recommend checking the book by Aurélien Géron </span><span class="cite"><span class="koboSpan" id="kobo.289.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xhandsonml"><span class="koboSpan" id="kobo.290.1" xmlns="http://www.w3.org/1999/xhtml">104</span></a><span class="koboSpan" id="kobo.291.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.292.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.293.1" xmlns="http://www.w3.org/1999/xhtml">You may also want to know that your models can exhibit a type of problem that is the opposite of </span><span id="dx1-144010"/><span class="koboSpan" id="kobo.294.1" xmlns="http://www.w3.org/1999/xhtml">overfitting and has been aptly named </span><strong><span class="koboSpan" id="kobo.295.1" xmlns="http://www.w3.org/1999/xhtml">underfitting</span></strong><span class="koboSpan" id="kobo.296.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.296.2" xmlns="http://www.w3.org/1999/xhtml">If your model is not expressive enough, you can find yourself with both a high error rate on the training set and a high error rate on the test set. </span><span class="koboSpan" id="kobo.296.3" xmlns="http://www.w3.org/1999/xhtml">For instance, if you are using a linear function to try to fit points that come from a quadratic polynomial and, thus, follow a parabolic shape, you will surely experience some form of </span><span id="dx1-144011"/><span class="koboSpan" id="kobo.297.1" xmlns="http://www.w3.org/1999/xhtml">underfitting. </span><span class="koboSpan" id="kobo.297.2" xmlns="http://www.w3.org/1999/xhtml">To fix this problem, use a more powerful model — or reduce regularization if you happen to be using it.</span></p>
<p><span class="koboSpan" id="kobo.298.1" xmlns="http://www.w3.org/1999/xhtml">To summarize what we have discussed so far, remember that we want to obtain a model that has a low generalization error; that is, a model that works well even on data it has not been trained with. </span><span class="koboSpan" id="kobo.298.2" xmlns="http://www.w3.org/1999/xhtml">In order to achieve this, we consider a parametric model and look for those model parameters that minimize the error on the training set, because we cannot easily compute the true error. </span><span class="koboSpan" id="kobo.298.3" xmlns="http://www.w3.org/1999/xhtml">And to be sure that the model will behave well when confronted with new data, we compute the error on the test dataset as a way of assessing how representative the empirical risk is of the error on unseen data.</span></p>
<p><span class="koboSpan" id="kobo.299.1" xmlns="http://www.w3.org/1999/xhtml">With this strategy, however, we may still be vulnerable to an additional problem. </span><span class="koboSpan" id="kobo.299.2" xmlns="http://www.w3.org/1999/xhtml">If we train a lot of different models, there is a risk that — just by pure chance! </span><span class="koboSpan" id="kobo.299.3" xmlns="http://www.w3.org/1999/xhtml">— one of them has great performance on the test dataset but not on the rest of the domain. </span><span class="koboSpan" id="kobo.299.4" xmlns="http://www.w3.org/1999/xhtml">In fact, this risk is higher the more models you train. </span><span class="koboSpan" id="kobo.299.5" xmlns="http://www.w3.org/1999/xhtml">Imagine that a thousand students take a test of 10 questions with 2 possible answers each. </span><span class="koboSpan" id="kobo.299.6" xmlns="http://www.w3.org/1999/xhtml">Even if they have not studied for the test and they answer completely at random, there is a very high probability that at least one of them will nail it. </span><span class="koboSpan" id="kobo.299.7" xmlns="http://www.w3.org/1999/xhtml">For this reason, you should never use the test dataset to select among your models, only to assess if their behavior is similar to the behavior they show during training.</span></p>
<p><span class="koboSpan" id="kobo.300.1" xmlns="http://www.w3.org/1999/xhtml">This is definitely a problem because we usually want to train many different models and select the one we believe to be the best. </span><span class="koboSpan" id="kobo.300.2" xmlns="http://www.w3.org/1999/xhtml">What is more, many models have what are called </span><strong><span class="koboSpan" id="kobo.301.1" xmlns="http://www.w3.org/1999/xhtml">hyperparameters</span></strong><span class="koboSpan" id="kobo.302.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.302.2" xmlns="http://www.w3.org/1999/xhtml">These are </span><span id="dx1-144012"/><span class="koboSpan" id="kobo.303.1" xmlns="http://www.w3.org/1999/xhtml">parameters that fix some property of the model, such as the size and number of layers in a neural network (more on that later), that cannot be optimized during training. </span><span class="koboSpan" id="kobo.303.2" xmlns="http://www.w3.org/1999/xhtml">Usually, we train many different models with different values of these hyperparameters, and then we select the best model from them.</span></p>
<p><span class="koboSpan" id="kobo.304.1" xmlns="http://www.w3.org/1999/xhtml">This is where a third type of dataset </span><span id="dx1-144013"/><span class="koboSpan" id="kobo.305.1" xmlns="http://www.w3.org/1999/xhtml">comes into the equation: the </span><strong><span class="koboSpan" id="kobo.306.1" xmlns="http://www.w3.org/1999/xhtml">validation</span></strong> <strong><span class="koboSpan" id="kobo.307.1" xmlns="http://www.w3.org/1999/xhtml">dataset</span></strong><span class="koboSpan" id="kobo.308.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.308.2" xmlns="http://www.w3.org/1999/xhtml">This is an additional </span><span id="dx1-144014"/><span class="koboSpan" id="kobo.309.1" xmlns="http://www.w3.org/1999/xhtml">dataset that we could construct when splitting our global dataset; it should, of course, be fully independent of the training and test datasets.</span></p>
<p><span class="koboSpan" id="kobo.310.1" xmlns="http://www.w3.org/1999/xhtml">What do we want the validation set for? </span><span class="koboSpan" id="kobo.310.2" xmlns="http://www.w3.org/1999/xhtml">Once we have trained our models with different choices of hyperparameters and configurations, we can compute the empirical risk on the validation set, and we may select the best one or maybe a handful of the best ones. </span><span class="koboSpan" id="kobo.310.3" xmlns="http://www.w3.org/1999/xhtml">Then, we could train those models again on the union of the training set and the validation set — to better extract all the information from our data — and then compute the error of the models on the test set, which we have held back until this very moment so that it remains a good estimator of the generalization error. </span><span class="koboSpan" id="kobo.310.4" xmlns="http://www.w3.org/1999/xhtml">In this way, we can select the best choice of hyperparameters or models while keeping the test dataset in pristine condition to be used in a final assessment process.</span></p>
<div class="tcolorbox learnmore" id="tcolobox-153">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.311.1" xmlns="http://www.w3.org/1999/xhtml">To learn more</span></span><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.312.1" xmlns="http://www.w3.org/1999/xhtml">…</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.313.1" xmlns="http://www.w3.org/1999/xhtml">You may also want to know that, instead of using a fixed validation set, a popular way of selecting </span><span id="dx1-144015"/><span class="koboSpan" id="kobo.314.1" xmlns="http://www.w3.org/1999/xhtml">hyperparameters is to use </span><span class="koboSpan" id="kobo.315.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="k" class="math inline" src="../media/file317.png" style="vertical-align:middle" title="k"/></span><strong><span class="koboSpan" id="kobo.316.1" xmlns="http://www.w3.org/1999/xhtml">-fold</span></strong> <strong><span class="koboSpan" id="kobo.317.1" xmlns="http://www.w3.org/1999/xhtml">cross-validation</span></strong><span class="koboSpan" id="kobo.318.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.318.2" xmlns="http://www.w3.org/1999/xhtml">With this technique, the training dataset is divided into </span><span class="koboSpan" id="kobo.319.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="k" class="math inline" src="../media/file317.png" style="vertical-align:middle" title="k"/></span><span class="koboSpan" id="kobo.320.1" xmlns="http://www.w3.org/1999/xhtml"> subsets or </span><strong><span class="koboSpan" id="kobo.321.1" xmlns="http://www.w3.org/1999/xhtml">folds</span></strong><span class="koboSpan" id="kobo.322.1" xmlns="http://www.w3.org/1999/xhtml"> of equal size. </span><span class="koboSpan" id="kobo.322.2" xmlns="http://www.w3.org/1999/xhtml">The training of the model is repeated </span><span class="koboSpan" id="kobo.323.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="k" class="math inline" src="../media/file317.png" style="vertical-align:middle" title="k"/></span><span class="koboSpan" id="kobo.324.1" xmlns="http://www.w3.org/1999/xhtml"> times, each one with a different subset acting as a validation dataset and the rest used as the training dataset. </span><span class="koboSpan" id="kobo.324.2" xmlns="http://www.w3.org/1999/xhtml">The performance is computed over each validation set and averaged over the </span><span class="koboSpan" id="kobo.325.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="k" class="math inline" src="../media/file317.png" style="vertical-align:middle" title="k"/></span><span class="koboSpan" id="kobo.326.1" xmlns="http://www.w3.org/1999/xhtml"> repetitions. </span><span class="koboSpan" id="kobo.326.2" xmlns="http://www.w3.org/1999/xhtml">Of course, the estimation obtained with cross-validation is better than when using a fixed validation set, but the computational cost is much higher — </span><span class="koboSpan" id="kobo.327.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="k" class="math inline" src="../media/file317.png" style="vertical-align:middle" title="k"/></span><span class="koboSpan" id="kobo.328.1" xmlns="http://www.w3.org/1999/xhtml"> times higher, in fact! </span><span class="koboSpan" id="kobo.328.2" xmlns="http://www.w3.org/1999/xhtml">Software libraries such as scikit-learn — which we will be using in the next section of this chapter — provide implementations of cross-validation for hyperparameter selection. </span><span class="koboSpan" id="kobo.328.3" xmlns="http://www.w3.org/1999/xhtml">Take a look at </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"><span class="koboSpan" id="kobo.329.1" xmlns="http://www.w3.org/1999/xhtml">the documentation of </span><span class="lstinline"><span style="color:#FF00FF"><code><span class="koboSpan" id="kobo.330.1" xmlns="http://www.w3.org/1999/xhtml">GridSearchCV</span></code></span></span></a><span class="koboSpan" id="kobo.331.1" xmlns="http://www.w3.org/1999/xhtml"> — where CV stands for cross validation — if you want to see a </span><span id="dx1-144016"/><span class="koboSpan" id="kobo.332.1" xmlns="http://www.w3.org/1999/xhtml">concrete implementation of this technique.</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.333.1" xmlns="http://www.w3.org/1999/xhtml">Furthermore, sometimes </span><span id="dx1-144017"/><span class="koboSpan" id="kobo.334.1" xmlns="http://www.w3.org/1999/xhtml">training processes are iterative. </span><span class="koboSpan" id="kobo.334.2" xmlns="http://www.w3.org/1999/xhtml">In these cases, the </span><strong><span class="koboSpan" id="kobo.335.1" xmlns="http://www.w3.org/1999/xhtml">validation loss</span></strong><span class="koboSpan" id="kobo.336.1" xmlns="http://www.w3.org/1999/xhtml"> (the average loss over the validation dataset) can be </span><span id="dx1-144018"/><span class="koboSpan" id="kobo.337.1" xmlns="http://www.w3.org/1999/xhtml">computed at the end of each iteration and compared against the training loss, just to know how the training is going — and to be able to stop it early should the model begin to overfit! </span><span class="koboSpan" id="kobo.337.2" xmlns="http://www.w3.org/1999/xhtml">It wouldn’t be a good practice to use the test set for this purpose: the test set should only be used once all the training is complete and we just want some reassurance on the validity of our results.</span></p>
<div class="tcolorbox learnmore" id="tcolobox-154">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.338.1" xmlns="http://www.w3.org/1999/xhtml">To learn more</span></span><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.339.1" xmlns="http://www.w3.org/1999/xhtml">…</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.340.1" xmlns="http://www.w3.org/1999/xhtml">All the informal notions that we are considering here can be formulated precisely using the language of probability theory. </span><span class="koboSpan" id="kobo.340.2" xmlns="http://www.w3.org/1999/xhtml">If you want to learn about the formal machinery behind machine learning, you should have a look at the book </span><em><span class="koboSpan" id="kobo.341.1" xmlns="http://www.w3.org/1999/xhtml">Learning from data</span></em> <span class="cite"><span class="koboSpan" id="kobo.342.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xabu2012learning"><span class="koboSpan" id="kobo.343.1" xmlns="http://www.w3.org/1999/xhtml">1</span></a><span class="koboSpan" id="kobo.344.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.345.1" xmlns="http://www.w3.org/1999/xhtml"> or at </span><em><span class="koboSpan" id="kobo.346.1" xmlns="http://www.w3.org/1999/xhtml">Understanding machine learning</span></em> <span class="cite"><span class="koboSpan" id="kobo.347.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xunderml"><span class="koboSpan" id="kobo.348.1" xmlns="http://www.w3.org/1999/xhtml">105</span></a><span class="koboSpan" id="kobo.349.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.350.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.351.1" xmlns="http://www.w3.org/1999/xhtml">With all of this, we now have a good understanding of all the elements needed for machine learning to come alive. </span><span class="koboSpan" id="kobo.351.2" xmlns="http://www.w3.org/1999/xhtml">In the following section, we will try to make all of this more precise by studying some of the most common approaches that are taken when training ML models.</span></p>
</section>
<section class="level3 subsectionHead" data-number="16.1.2" id="types-of-machine-learning">
<h2 class="subsectionHead" data-number="16.1.2"><span class="titlemark"><span class="koboSpan" id="kobo.352.1" xmlns="http://www.w3.org/1999/xhtml">8.1.2 </span></span> <span id="x1-1450008.1.2"><span class="koboSpan" id="kobo.353.1" xmlns="http://www.w3.org/1999/xhtml">Types of machine learning</span></span></h2>
<p><span class="koboSpan" id="kobo.354.1" xmlns="http://www.w3.org/1999/xhtml">There are three big categories in which most, if not all, machine </span><span id="dx1-145001"/><span class="koboSpan" id="kobo.355.1" xmlns="http://www.w3.org/1999/xhtml">learning techniques can fit: </span><strong><span class="koboSpan" id="kobo.356.1" xmlns="http://www.w3.org/1999/xhtml">supervised learning</span></strong><span class="koboSpan" id="kobo.357.1" xmlns="http://www.w3.org/1999/xhtml">, </span><strong><span class="koboSpan" id="kobo.358.1" xmlns="http://www.w3.org/1999/xhtml">unsupervised learning</span></strong><span class="koboSpan" id="kobo.359.1" xmlns="http://www.w3.org/1999/xhtml">, and </span><strong><span class="koboSpan" id="kobo.360.1" xmlns="http://www.w3.org/1999/xhtml">reinforcement learning</span></strong><span class="koboSpan" id="kobo.361.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.361.2" xmlns="http://www.w3.org/1999/xhtml">In this book, we will work mostly with supervised learning, but we will also consider some unsupervised learning techniques. </span><span class="koboSpan" id="kobo.361.3" xmlns="http://www.w3.org/1999/xhtml">Let’s explain in a little bit more detail what each of these machine learning branches is about.</span></p>
<p><span class="paragraphHead"><span id="x1-1460008.1.2"/><strong><span class="koboSpan" id="kobo.362.1" xmlns="http://www.w3.org/1999/xhtml">Supervised learning</span></strong></span><span class="koboSpan" id="kobo.363.1" xmlns="http://www.w3.org/1999/xhtml"> The main </span><span id="dx1-146001"/><span class="koboSpan" id="kobo.364.1" xmlns="http://www.w3.org/1999/xhtml">goal of supervised </span><span id="dx1-146002"/><span class="koboSpan" id="kobo.365.1" xmlns="http://www.w3.org/1999/xhtml">learning is to learn to predict the values of a function on input data. </span><span class="koboSpan" id="kobo.365.2" xmlns="http://www.w3.org/1999/xhtml">These values can either be chosen from a finite set (the classification problems we have been talking about for the most part of this chapter) or be continuous values, such as the weight of a person or the value of some bonds a month from now. </span><span class="koboSpan" id="kobo.365.3" xmlns="http://www.w3.org/1999/xhtml">When the values we want to predict are continuous, we say that we are </span><span id="dx1-146003"/><span class="koboSpan" id="kobo.366.1" xmlns="http://www.w3.org/1999/xhtml">tackling a </span><strong><span class="koboSpan" id="kobo.367.1" xmlns="http://www.w3.org/1999/xhtml">regression</span></strong><span class="koboSpan" id="kobo.368.1" xmlns="http://www.w3.org/1999/xhtml"> problem.</span></p>
<p><span class="koboSpan" id="kobo.369.1" xmlns="http://www.w3.org/1999/xhtml">When we train a model using supervised learning, we need to work with a dataset that has both a large-enough collection of valid inputs and all the expected outputs that our model should return for these inputs. </span><span class="koboSpan" id="kobo.369.2" xmlns="http://www.w3.org/1999/xhtml">This is </span><span id="dx1-146004"/><span class="koboSpan" id="kobo.370.1" xmlns="http://www.w3.org/1999/xhtml">known as having a </span><strong><span class="koboSpan" id="kobo.371.1" xmlns="http://www.w3.org/1999/xhtml">labeled</span></strong><span class="koboSpan" id="kobo.372.1" xmlns="http://www.w3.org/1999/xhtml"> dataset.</span></p>
<p><span class="koboSpan" id="kobo.373.1" xmlns="http://www.w3.org/1999/xhtml">For example, if we were to train a cat-rabbit picture classifier using supervised learning, we would need to have a (large-enough) dataset with pictures of rabbits and cats, and we would also need to know, for each of those pictures, whether they are pictures of rabbits or pictures of cats.</span></p>
<p><span class="koboSpan" id="kobo.374.1" xmlns="http://www.w3.org/1999/xhtml">With our labeled dataset, we would define a loss function that would depend on the inputs and the parameters of the model — so that we can compute the corresponding outputs — and the expected (correct) outputs. </span><span class="koboSpan" id="kobo.374.2" xmlns="http://www.w3.org/1999/xhtml">And, as we discussed before, then we would just apply an optimization algorithm to find a configuration of the model that could minimize the loss function on the training dataset — while ensuring that there is no overfitting.</span></p>
<p><span class="koboSpan" id="kobo.375.1" xmlns="http://www.w3.org/1999/xhtml">We still have to discuss how that optimization algorithm is going to work, but that is for later!</span></p>
<p><span class="paragraphHead"><span id="x1-1470008.1.2"/><strong><span class="koboSpan" id="kobo.376.1" xmlns="http://www.w3.org/1999/xhtml">Unsupervised learning</span></strong></span><span class="koboSpan" id="kobo.377.1" xmlns="http://www.w3.org/1999/xhtml"> When we </span><span id="dx1-147001"/><span class="koboSpan" id="kobo.378.1" xmlns="http://www.w3.org/1999/xhtml">work with </span><span id="dx1-147002"/><span class="koboSpan" id="kobo.379.1" xmlns="http://www.w3.org/1999/xhtml">unsupervised learning, we have </span><span id="dx1-147003"/><span class="koboSpan" id="kobo.380.1" xmlns="http://www.w3.org/1999/xhtml">access to </span><strong><span class="koboSpan" id="kobo.381.1" xmlns="http://www.w3.org/1999/xhtml">unlabeled</span></strong><span class="koboSpan" id="kobo.382.1" xmlns="http://www.w3.org/1999/xhtml"> datasets, in which there are no expected outputs. </span><span class="koboSpan" id="kobo.382.2" xmlns="http://www.w3.org/1999/xhtml">We let the algorithm learn on its own by trying to identify certain patterns. </span><span class="koboSpan" id="kobo.382.3" xmlns="http://www.w3.org/1999/xhtml">For instance, we may want to group similar data </span><span id="dx1-147004"/><span class="koboSpan" id="kobo.383.1" xmlns="http://www.w3.org/1999/xhtml">points together (this is known as </span><strong><span class="koboSpan" id="kobo.384.1" xmlns="http://www.w3.org/1999/xhtml">clustering</span></strong><span class="koboSpan" id="kobo.385.1" xmlns="http://www.w3.org/1999/xhtml">) or we may want to learn something about how the data is distributed.</span></p>
<p><span class="koboSpan" id="kobo.386.1" xmlns="http://www.w3.org/1999/xhtml">In this latter case, our goal would be to </span><span id="dx1-147005"/><span class="koboSpan" id="kobo.387.1" xmlns="http://www.w3.org/1999/xhtml">train a </span><strong><span class="koboSpan" id="kobo.388.1" xmlns="http://www.w3.org/1999/xhtml">generative model</span></strong><span class="koboSpan" id="kobo.389.1" xmlns="http://www.w3.org/1999/xhtml"> that we can use to create new data samples. </span><span class="koboSpan" id="kobo.389.2" xmlns="http://www.w3.org/1999/xhtml">An impressive </span><span id="dx1-147006"/><span class="koboSpan" id="kobo.390.1" xmlns="http://www.w3.org/1999/xhtml">example is the use of </span><strong><span class="koboSpan" id="kobo.391.1" xmlns="http://www.w3.org/1999/xhtml">Generative Adversarial Networks</span></strong><span class="koboSpan" id="kobo.392.1" xmlns="http://www.w3.org/1999/xhtml">, introduced by Ian Goodfellow and his </span><span id="dx1-147007"/><span class="koboSpan" id="kobo.393.1" xmlns="http://www.w3.org/1999/xhtml">collaborators in a highly influential paper </span><span class="cite"><span class="koboSpan" id="kobo.394.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xgoodfellow2014generative"><span class="koboSpan" id="kobo.395.1" xmlns="http://www.w3.org/1999/xhtml">46</span></a><span class="koboSpan" id="kobo.396.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.397.1" xmlns="http://www.w3.org/1999/xhtml"> to create </span><span id="dx1-147008"/><span class="koboSpan" id="kobo.398.1" xmlns="http://www.w3.org/1999/xhtml">images that are similar — but completely different — to the ones used in the training phase. </span><span class="koboSpan" id="kobo.398.2" xmlns="http://www.w3.org/1999/xhtml">This is the kind of model that we will be working with in </span><em><span class="koboSpan" id="kobo.399.1" xmlns="http://www.w3.org/1999/xhtml">Chapter</span></em> <em/> <a href="ch021.xhtml#x1-21200012"><em><span class="koboSpan" id="kobo.400.1" xmlns="http://www.w3.org/1999/xhtml">12</span></em></a><span class="koboSpan" id="kobo.401.1" xmlns="http://www.w3.org/1999/xhtml">, </span><em><span class="koboSpan" id="kobo.402.1" xmlns="http://www.w3.org/1999/xhtml">Quantum Generative Adversarial Networks</span></em><span class="koboSpan" id="kobo.403.1" xmlns="http://www.w3.org/1999/xhtml">,…in a quantum form, of course!</span></p>
<p><span class="paragraphHead"><span id="x1-1480008.1.2"/><strong><span class="koboSpan" id="kobo.404.1" xmlns="http://www.w3.org/1999/xhtml">Reinforcement learning</span></strong></span><span class="koboSpan" id="kobo.405.1" xmlns="http://www.w3.org/1999/xhtml"> In </span><span id="dx1-148001"/><span class="koboSpan" id="kobo.406.1" xmlns="http://www.w3.org/1999/xhtml">reinforcement learning, the model — usually </span><span id="dx1-148002"/><span class="koboSpan" id="kobo.407.1" xmlns="http://www.w3.org/1999/xhtml">called the </span><strong><span class="koboSpan" id="kobo.408.1" xmlns="http://www.w3.org/1999/xhtml">agent</span></strong><span class="koboSpan" id="kobo.409.1" xmlns="http://www.w3.org/1999/xhtml"> in this setting — interacts with an </span><strong><span class="koboSpan" id="kobo.410.1" xmlns="http://www.w3.org/1999/xhtml">environment</span></strong><span class="koboSpan" id="kobo.411.1" xmlns="http://www.w3.org/1999/xhtml">, trying to </span><span id="dx1-148003"/><span class="koboSpan" id="kobo.412.1" xmlns="http://www.w3.org/1999/xhtml">complete some task. </span><span class="koboSpan" id="kobo.412.2" xmlns="http://www.w3.org/1999/xhtml">This agent observes the </span><strong><span class="koboSpan" id="kobo.413.1" xmlns="http://www.w3.org/1999/xhtml">state</span></strong><span class="koboSpan" id="kobo.414.1" xmlns="http://www.w3.org/1999/xhtml"> of the </span><span id="dx1-148004"/><span class="koboSpan" id="kobo.415.1" xmlns="http://www.w3.org/1999/xhtml">environment and takes some </span><strong><span class="koboSpan" id="kobo.416.1" xmlns="http://www.w3.org/1999/xhtml">actions</span></strong><span class="koboSpan" id="kobo.417.1" xmlns="http://www.w3.org/1999/xhtml"> that in turn influence the </span><span id="dx1-148005"/><span class="koboSpan" id="kobo.418.1" xmlns="http://www.w3.org/1999/xhtml">state it observes. </span><span class="koboSpan" id="kobo.418.2" xmlns="http://www.w3.org/1999/xhtml">Depending on its performance, it receives ”rewards” and ”punishments”…and, of course, it wants to maximize the rewards while minimizing the punishments. </span><span class="koboSpan" id="kobo.418.3" xmlns="http://www.w3.org/1999/xhtml">To do that, it tries to learn a </span><strong><span class="koboSpan" id="kobo.419.1" xmlns="http://www.w3.org/1999/xhtml">policy</span></strong><span class="koboSpan" id="kobo.420.1" xmlns="http://www.w3.org/1999/xhtml"> that </span><span id="dx1-148006"/><span class="koboSpan" id="kobo.421.1" xmlns="http://www.w3.org/1999/xhtml">determines what action to take for a given state of the environment.</span></p>
<p><span class="koboSpan" id="kobo.422.1" xmlns="http://www.w3.org/1999/xhtml">For instance, the agent may be a robot and the environment a maze it needs to navigate. </span><span class="koboSpan" id="kobo.422.2" xmlns="http://www.w3.org/1999/xhtml">The state can consist of its position in the maze and the open paths it can follow, and its actions can be rotating in some direction and moving forward. </span><span class="koboSpan" id="kobo.422.3" xmlns="http://www.w3.org/1999/xhtml">The goal may be finding the exit to the maze in some predefined time, for which the robot will get a positive reward.</span></p>
<p><span class="koboSpan" id="kobo.423.1" xmlns="http://www.w3.org/1999/xhtml">This kind of learning has been used extensively to train models designed to play games — AlphaGo, the computer program that in 2016 beat Go (human) grandmaster Lee Sedol in a five-games match, is a prominent example! </span><span class="koboSpan" id="kobo.423.2" xmlns="http://www.w3.org/1999/xhtml">To learn more about reinforcement learning, a good source is the book by Sutton and Barto </span><span class="cite"><span class="koboSpan" id="kobo.424.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xsutton2018reinforcement"><span class="koboSpan" id="kobo.425.1" xmlns="http://www.w3.org/1999/xhtml">93</span></a><span class="koboSpan" id="kobo.426.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.427.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<p><span class="koboSpan" id="kobo.428.1" xmlns="http://www.w3.org/1999/xhtml">Although there has been some interest in using quantum techniques in reinforcement learning (see, for instance </span><span class="cite"><span class="koboSpan" id="kobo.429.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xskolik2022quantum"><span class="koboSpan" id="kobo.430.1" xmlns="http://www.w3.org/1999/xhtml">91</span></a><span class="koboSpan" id="kobo.431.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.432.1" xmlns="http://www.w3.org/1999/xhtml">), this may very well be the machine learning branch in which quantum algorithms are less developed at the moment. </span><span class="koboSpan" id="kobo.432.2" xmlns="http://www.w3.org/1999/xhtml">For this reason, we will not cover this kind of learning in this book. </span><span class="koboSpan" id="kobo.432.3" xmlns="http://www.w3.org/1999/xhtml">Hopefully, in a few years there will be much more to tell about quantum reinforcement learning! </span><span class="koboSpan" id="kobo.432.4" xmlns="http://www.w3.org/1999/xhtml">Let’s now try to make everything concrete by using supervised learning to implement a very simple classifier. </span><span class="koboSpan" id="kobo.432.5" xmlns="http://www.w3.org/1999/xhtml">For this, we will use TensorFlow.</span></p>
</section>
</section>
<section class="level2 sectionHead" data-number="16.2" id="do-you-wanna-train-a-model">
<h1 class="sectionHead" data-number="16.2"><span class="titlemark"><span class="koboSpan" id="kobo.433.1" xmlns="http://www.w3.org/1999/xhtml">8.2 </span></span> <span id="x1-1490008.2"><span class="koboSpan" id="kobo.434.1" xmlns="http://www.w3.org/1999/xhtml">Do you wanna train a model?</span></span></h1>
<p><span class="koboSpan" id="kobo.435.1" xmlns="http://www.w3.org/1999/xhtml">TensorFlow is a </span><span id="dx1-149001"/><span class="koboSpan" id="kobo.436.1" xmlns="http://www.w3.org/1999/xhtml">machine learning framework developed at Google, and it is very widely used. </span><span class="koboSpan" id="kobo.436.2" xmlns="http://www.w3.org/1999/xhtml">You should refer to </span><em><span class="koboSpan" id="kobo.437.1" xmlns="http://www.w3.org/1999/xhtml">Appendix</span></em> <a href="ch027.xhtml#x1-240000D"><em><span class="koboSpan" id="kobo.438.1" xmlns="http://www.w3.org/1999/xhtml">D</span></em></a><span class="koboSpan" id="kobo.439.1" xmlns="http://www.w3.org/1999/xhtml">, </span><em><span class="koboSpan" id="kobo.440.1" xmlns="http://www.w3.org/1999/xhtml">Installing the</span></em> <em><span class="koboSpan" id="kobo.441.1" xmlns="http://www.w3.org/1999/xhtml">Tools</span></em><span class="koboSpan" id="kobo.442.1" xmlns="http://www.w3.org/1999/xhtml">, for installation instructions. </span><span class="koboSpan" id="kobo.442.2" xmlns="http://www.w3.org/1999/xhtml">Keep in mind that we will be using version 2.9.1. </span><span class="koboSpan" id="kobo.442.3" xmlns="http://www.w3.org/1999/xhtml">We will use TensorFlow in some of our quantum machine learning models, so it is a good idea to become familiar with it early on.</span></p>
<p><span class="koboSpan" id="kobo.443.1" xmlns="http://www.w3.org/1999/xhtml">To keep things simple, we will </span><span id="dx1-149002"/><span class="koboSpan" id="kobo.444.1" xmlns="http://www.w3.org/1999/xhtml">tackle an artificial problem. </span><span class="koboSpan" id="kobo.444.2" xmlns="http://www.w3.org/1999/xhtml">We are going to prepare a dataset of elements belonging to one of two possible categories, and we will try to use machine learning to construct a classifier that can distinguish to which category any given input belongs.</span></p>
<p><span class="koboSpan" id="kobo.445.1" xmlns="http://www.w3.org/1999/xhtml">Before we do anything, let us quickly import NumPy and set a seed:</span></p>
<p><span id="x1-149003"/></p>
<pre class="lstlisting" id="listing-193"><span class="koboSpan" id="kobo.446.1" xmlns="http://www.w3.org/1999/xhtml">

import numpy as np 
 
seed = 128 
 
np.random.seed(seed)
</span></pre>
<p><span class="koboSpan" id="kobo.447.1" xmlns="http://www.w3.org/1999/xhtml">We will later use this same seed with TensorFlow. </span><span class="koboSpan" id="kobo.447.2" xmlns="http://www.w3.org/1999/xhtml">And now, let’s generate the data!</span></p>
<p><span class="koboSpan" id="kobo.448.1" xmlns="http://www.w3.org/1999/xhtml">Instead of generating a dataset by hand, we will use a function provided by the Python </span><strong><span class="koboSpan" id="kobo.449.1" xmlns="http://www.w3.org/1999/xhtml">scikit-learn</span></strong><span class="koboSpan" id="kobo.450.1" xmlns="http://www.w3.org/1999/xhtml"> package (</span><code><span class="koboSpan" id="kobo.451.1" xmlns="http://www.w3.org/1999/xhtml">sklearn</span></code><span class="koboSpan" id="kobo.452.1" xmlns="http://www.w3.org/1999/xhtml">). </span><span class="koboSpan" id="kobo.452.2" xmlns="http://www.w3.org/1999/xhtml">This </span><span id="dx1-149007"/><span class="koboSpan" id="kobo.453.1" xmlns="http://www.w3.org/1999/xhtml">package is a very valuable resource for machine learning: not only does it include plenty of useful tools for everyday machine-learning-related tasks, but it also allows you to train and execute a wide collection of interesting models! </span><span class="koboSpan" id="kobo.453.2" xmlns="http://www.w3.org/1999/xhtml">We will use version 1.0.2 of </span><code><span class="koboSpan" id="kobo.454.1" xmlns="http://www.w3.org/1999/xhtml">sklearn</span></code><span class="koboSpan" id="kobo.455.1" xmlns="http://www.w3.org/1999/xhtml"> and, as always, you should refer to </span><em><span class="koboSpan" id="kobo.456.1" xmlns="http://www.w3.org/1999/xhtml">Appendix</span></em> <em/> <a href="ch027.xhtml#x1-240000D"><em><span class="koboSpan" id="kobo.457.1" xmlns="http://www.w3.org/1999/xhtml">D</span></em></a><span class="koboSpan" id="kobo.458.1" xmlns="http://www.w3.org/1999/xhtml">, </span><em><span class="koboSpan" id="kobo.459.1" xmlns="http://www.w3.org/1999/xhtml">Installing the Tools</span></em><span class="koboSpan" id="kobo.460.1" xmlns="http://www.w3.org/1999/xhtml">, for installation instructions.</span></p>
<p><span class="koboSpan" id="kobo.461.1" xmlns="http://www.w3.org/1999/xhtml">In order to generate our dataset, we will use the </span><code><span class="koboSpan" id="kobo.462.1" xmlns="http://www.w3.org/1999/xhtml">make_classification</span></code><span class="koboSpan" id="kobo.463.1" xmlns="http://www.w3.org/1999/xhtml"> function from </span><code><span class="koboSpan" id="kobo.464.1" xmlns="http://www.w3.org/1999/xhtml">sklearn</span></code><span style="color:#000000"><code><span class="koboSpan" id="kobo.465.1" xmlns="http://www.w3.org/1999/xhtml">.</span></code></span><code><span class="koboSpan" id="kobo.466.1" xmlns="http://www.w3.org/1999/xhtml">datasets</span></code><span class="koboSpan" id="kobo.467.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.467.2" xmlns="http://www.w3.org/1999/xhtml">We will ask it to generate </span><span class="koboSpan" id="kobo.468.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="2500" class="math inline" src="../media/file1117.png" style="vertical-align:middle" title="2500"/></span><span class="koboSpan" id="kobo.469.1" xmlns="http://www.w3.org/1999/xhtml"> samples of a dataset with two features (variables). </span><span class="koboSpan" id="kobo.469.2" xmlns="http://www.w3.org/1999/xhtml">We will also ask for both features to be </span><strong><span class="koboSpan" id="kobo.470.1" xmlns="http://www.w3.org/1999/xhtml">informative</span></strong><span class="koboSpan" id="kobo.471.1" xmlns="http://www.w3.org/1999/xhtml"> and not redundant; the variables would be redundant, for example, if one of them were just a multiple of the other. </span><span class="koboSpan" id="kobo.471.2" xmlns="http://www.w3.org/1999/xhtml">Lastly, we will ask for the proportions of the two categories in the dataset to be </span><span class="koboSpan" id="kobo.472.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="20\,\%" class="math inline" src="../media/file1118.png" style="vertical-align:middle" title="20\,\%"/></span><span class="koboSpan" id="kobo.473.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><span class="koboSpan" id="kobo.474.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="80\,\%" class="math inline" src="../media/file1119.png" style="vertical-align:middle" title="80\,\%"/></span><span class="koboSpan" id="kobo.475.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.475.2" xmlns="http://www.w3.org/1999/xhtml">We can do this as follows:</span></p>
<pre class="lstlisting" id="listing-194"><span class="koboSpan" id="kobo.476.1" xmlns="http://www.w3.org/1999/xhtml">

from sklearn.datasets import make_classification 
 
 
 
data, labels = make_classification(n_samples = 2500, 
 
    n_features = 2, n_informative = 2, n_redundant = 0, 
 
    weights = (0.2,0.8), class_sep = 0.5, random_state = seed)
</span></pre>
<p><span class="koboSpan" id="kobo.477.1" xmlns="http://www.w3.org/1999/xhtml">The </span><code><span class="koboSpan" id="kobo.478.1" xmlns="http://www.w3.org/1999/xhtml">class_sep</span></code><span class="koboSpan" id="kobo.479.1" xmlns="http://www.w3.org/1999/xhtml"> argument specifies how </span><span id="dx1-149013"/><span class="koboSpan" id="kobo.480.1" xmlns="http://www.w3.org/1999/xhtml">separable we want the two categories to be: the higher the value of this argument, the easier it is to distinguish them. </span><span class="koboSpan" id="kobo.480.2" xmlns="http://www.w3.org/1999/xhtml">Notice, also, that we have used the seed that we set earlier in order for the results to be repeatable.</span></p>
<p><span class="koboSpan" id="kobo.481.1" xmlns="http://www.w3.org/1999/xhtml">You may now be wondering why we have specified that we want the two categories in the dataset to be in a proportion </span><span class="koboSpan" id="kobo.482.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="20\,\%" class="math inline" src="../media/file1118.png" style="vertical-align:middle" title="20\,\%"/></span><span class="koboSpan" id="kobo.483.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><span class="koboSpan" id="kobo.484.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="80\,\%" class="math inline" src="../media/file1119.png" style="vertical-align:middle" title="80\,\%"/></span><span class="koboSpan" id="kobo.485.1" xmlns="http://www.w3.org/1999/xhtml">, when it would be much more natural for the two categories to be balanced. </span><span class="koboSpan" id="kobo.485.2" xmlns="http://www.w3.org/1999/xhtml">Indeed, it is desirable for both categories to have the same number of representatives in a dataset…but life is difficult, and in many practical scenarios, that is not a possibility! </span><span class="koboSpan" id="kobo.485.3" xmlns="http://www.w3.org/1999/xhtml">So just think of this choice of ours as our own little way of feeling closer to real life.</span></p>
<p><span class="koboSpan" id="kobo.486.1" xmlns="http://www.w3.org/1999/xhtml">Essentially, the </span><code><span class="koboSpan" id="kobo.487.1" xmlns="http://www.w3.org/1999/xhtml">make_classification</span></code><span class="koboSpan" id="kobo.488.1" xmlns="http://www.w3.org/1999/xhtml"> function has returned an array </span><code><span class="koboSpan" id="kobo.489.1" xmlns="http://www.w3.org/1999/xhtml">data</span></code><span class="koboSpan" id="kobo.490.1" xmlns="http://www.w3.org/1999/xhtml"> with the whole dataset (including all the elements from both categories, positive and negative), and an array </span><code><span class="koboSpan" id="kobo.491.1" xmlns="http://www.w3.org/1999/xhtml">labels</span></code><span class="koboSpan" id="kobo.492.1" xmlns="http://www.w3.org/1999/xhtml"> such that the label of </span><code><span class="koboSpan" id="kobo.493.1" xmlns="http://www.w3.org/1999/xhtml">data</span></code><span style="color:#000000"><code><span class="koboSpan" id="kobo.494.1" xmlns="http://www.w3.org/1999/xhtml">[</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.495.1" xmlns="http://www.w3.org/1999/xhtml">i</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.496.1" xmlns="http://www.w3.org/1999/xhtml">]</span></code><span class="koboSpan" id="kobo.497.1" xmlns="http://www.w3.org/1999/xhtml"> will be </span><code><span class="koboSpan" id="kobo.498.1" xmlns="http://www.w3.org/1999/xhtml">labels</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.499.1" xmlns="http://www.w3.org/1999/xhtml">[</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.500.1" xmlns="http://www.w3.org/1999/xhtml">i</span></code></span><code><span class="koboSpan" id="kobo.501.1" xmlns="http://www.w3.org/1999/xhtml">]</span></code><span class="koboSpan" id="kobo.502.1" xmlns="http://www.w3.org/1999/xhtml"> (where </span><span class="koboSpan" id="kobo.503.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.504.1" xmlns="http://www.w3.org/1999/xhtml"> corresponds to positive and </span><span class="koboSpan" id="kobo.505.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.506.1" xmlns="http://www.w3.org/1999/xhtml"> to negative).</span></p>
<p><span class="koboSpan" id="kobo.507.1" xmlns="http://www.w3.org/1999/xhtml">Just to get a feeling of what this dataset that we have created looks like, we can plot a simple histogram showing the distributions of the two features of our dataset:</span></p>
<pre class="lstlisting" id="listing-195"><span class="koboSpan" id="kobo.508.1" xmlns="http://www.w3.org/1999/xhtml">

import matplotlib.pyplot as plt 
 
 
 
for i in range(2): 
 
    plt.hist(data[:,i][labels == 1], bins=100, alpha=0.8, label = "Negative") 
 
    plt.hist(data[:,i][labels == 0], bins=100, alpha=0.8, label = "Positive") 
 
    plt.legend() 
 
    plt.show()
</span></pre>
<p><span class="koboSpan" id="kobo.509.1" xmlns="http://www.w3.org/1999/xhtml">Upon running this, we got the plots shown in </span><em><span class="koboSpan" id="kobo.510.1" xmlns="http://www.w3.org/1999/xhtml">Figure</span></em> <a href="#Figure8.2"><em><span class="koboSpan" id="kobo.511.1" xmlns="http://www.w3.org/1999/xhtml">8.2</span></em></a><span class="koboSpan" id="kobo.512.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<figure>
<span class="koboSpan" id="kobo.513.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="Figure 8.2: Histograms representing the distributions of the two features of our dataset." src="../media/file1120.png"/></span>
<figcaption aria-hidden="true"><span class="id" id="Figure8.2"><strong><span class="koboSpan" id="kobo.514.1" xmlns="http://www.w3.org/1999/xhtml">Figure 8.2</span></strong><span class="koboSpan" id="kobo.515.1" xmlns="http://www.w3.org/1999/xhtml">: </span></span><span class="koboSpan" id="kobo.516.1" xmlns="http://www.w3.org/1999/xhtml">Histograms representing the distributions of the two features of our dataset.</span></figcaption>
</figure>
<div class="tcolorbox questionx" id="tcolobox-155">
<span id="x1-149024x8.2"/>
<div class="tcolorbox-title">
<p><span class="koboSpan" id="kobo.517.1" xmlns="http://www.w3.org/1999/xhtml">Exercise 8.2</span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.518.1" xmlns="http://www.w3.org/1999/xhtml">Visualizing the data you are working with through graphs can help you gain insights into how to approach the problem you have at hand. </span><span class="koboSpan" id="kobo.518.2" xmlns="http://www.w3.org/1999/xhtml">We have plotted our data using a histogram, which is usually a good choice. </span><span class="koboSpan" id="kobo.518.3" xmlns="http://www.w3.org/1999/xhtml">What other representations could we have used?</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.519.1" xmlns="http://www.w3.org/1999/xhtml">Our goal now is to use </span><span id="dx1-149025"/><span class="koboSpan" id="kobo.520.1" xmlns="http://www.w3.org/1999/xhtml">machine learning to come up with a system that can solve the classification problem that we have created. </span><span class="koboSpan" id="kobo.520.2" xmlns="http://www.w3.org/1999/xhtml">And the first step in doing so will be to pick a good model to tackle our problem!</span></p>
<section class="level3 subsectionHead" data-number="16.2.1" id="picking-a-model">
<h2 class="subsectionHead" data-number="16.2.1"><span class="titlemark"><span class="koboSpan" id="kobo.521.1" xmlns="http://www.w3.org/1999/xhtml">8.2.1 </span></span> <span id="x1-1500008.2.1"><span class="koboSpan" id="kobo.522.1" xmlns="http://www.w3.org/1999/xhtml">Picking a model</span></span></h2>
<p><span class="koboSpan" id="kobo.523.1" xmlns="http://www.w3.org/1999/xhtml">Not long ago, we </span><span id="dx1-150001"/><span class="koboSpan" id="kobo.524.1" xmlns="http://www.w3.org/1999/xhtml">introduced the perceptron and we showed how, </span><em><span class="koboSpan" id="kobo.525.1" xmlns="http://www.w3.org/1999/xhtml">on its own</span></em><span class="koboSpan" id="kobo.526.1" xmlns="http://www.w3.org/1999/xhtml">, it wasn’t the most powerful of models out there. </span><span class="koboSpan" id="kobo.526.2" xmlns="http://www.w3.org/1999/xhtml">We will now shed some light on why we emphasized ”on its own,” for we are about to introduce a very interesting model that can be thought of as being built by joining perceptrons together. </span><span class="koboSpan" id="kobo.526.3" xmlns="http://www.w3.org/1999/xhtml">Let’s dive into </span><strong><span class="koboSpan" id="kobo.527.1" xmlns="http://www.w3.org/1999/xhtml">neural networks</span></strong><span class="koboSpan" id="kobo.528.1" xmlns="http://www.w3.org/1999/xhtml">!</span></p>
<p><span class="koboSpan" id="kobo.529.1" xmlns="http://www.w3.org/1999/xhtml">You may remember how a </span><span id="dx1-150002"/><span class="koboSpan" id="kobo.530.1" xmlns="http://www.w3.org/1999/xhtml">perceptron took </span><span class="koboSpan" id="kobo.531.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="N" class="math inline" src="../media/file784.png" style="vertical-align:middle" title="N"/></span><span class="koboSpan" id="kobo.532.1" xmlns="http://www.w3.org/1999/xhtml"> numerical inputs </span><span class="koboSpan" id="kobo.533.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x_{i}" class="math inline" src="../media/file714.png" style="vertical-align:middle" title="x_{i}"/></span><span class="koboSpan" id="kobo.534.1" xmlns="http://www.w3.org/1999/xhtml">, used on a collection of </span><span class="koboSpan" id="kobo.535.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="N" class="math inline" src="../media/file784.png" style="vertical-align:middle" title="N"/></span><span class="koboSpan" id="kobo.536.1" xmlns="http://www.w3.org/1999/xhtml"> weights </span><span class="koboSpan" id="kobo.537.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="w_{i}" class="math inline" src="../media/file1095.png" style="vertical-align:middle" title="w_{i}"/></span><span class="koboSpan" id="kobo.538.1" xmlns="http://www.w3.org/1999/xhtml"> and a bias </span><span class="koboSpan" id="kobo.539.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="b" class="math inline" src="../media/file17.png" style="vertical-align:middle" title="b"/></span><span class="koboSpan" id="kobo.540.1" xmlns="http://www.w3.org/1999/xhtml">, and returned an output that depended on the value of</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.541.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\sum\limits_{i = 1}^{N}w_{i}x_{i} + b." class="math display" src="../media/file1121.png" style="vertical-align:middle" title="\sum\limits_{i = 1}^{N}w_{i}x_{i} + b."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.542.1" xmlns="http://www.w3.org/1999/xhtml">Well, in this way, we can think of a neural network as being a collection of perceptrons — which we will, from now on, call </span><strong><span class="koboSpan" id="kobo.543.1" xmlns="http://www.w3.org/1999/xhtml">neurons</span></strong><span class="koboSpan" id="kobo.544.1" xmlns="http://www.w3.org/1999/xhtml"> — organized in the following way:</span></p>
<ul>
<li><p><span class="koboSpan" id="kobo.545.1" xmlns="http://www.w3.org/1999/xhtml">All the neurons are </span><span id="dx1-150003"/><span class="koboSpan" id="kobo.546.1" xmlns="http://www.w3.org/1999/xhtml">arranged into layers, and the output of the neurons in one layer is the input of the neurons in the next layer</span></p></li>
<li><p><span class="koboSpan" id="kobo.547.1" xmlns="http://www.w3.org/1999/xhtml">In addition to this, the ”raw” linear </span><span id="dx1-150004"/><span class="koboSpan" id="kobo.548.1" xmlns="http://www.w3.org/1999/xhtml">output of every neuron will go through a (very possibly non-linear) </span><strong><span class="koboSpan" id="kobo.549.1" xmlns="http://www.w3.org/1999/xhtml">activation function</span></strong><span class="koboSpan" id="kobo.550.1" xmlns="http://www.w3.org/1999/xhtml"> of our choice</span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.551.1" xmlns="http://www.w3.org/1999/xhtml">That is the </span><span id="dx1-150005"/><span class="koboSpan" id="kobo.552.1" xmlns="http://www.w3.org/1999/xhtml">general idea, but let’s now make it precise.</span></p>
<p><span class="koboSpan" id="kobo.553.1" xmlns="http://www.w3.org/1999/xhtml">A neural network with </span><span class="koboSpan" id="kobo.554.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="N_{0}" class="math inline" src="../media/file1122.png" style="vertical-align:middle" title="N_{0}"/></span><span class="koboSpan" id="kobo.555.1" xmlns="http://www.w3.org/1999/xhtml"> inputs is defined from the following elements:</span></p>
<ul>
<li><p><span class="koboSpan" id="kobo.556.1" xmlns="http://www.w3.org/1999/xhtml">An ordered sequence of </span><strong><span class="koboSpan" id="kobo.557.1" xmlns="http://www.w3.org/1999/xhtml">layers</span></strong><span class="koboSpan" id="kobo.558.1" xmlns="http://www.w3.org/1999/xhtml"> (</span><span class="koboSpan" id="kobo.559.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="l = 1,\ldots,L" class="math inline" src="../media/file1123.png" style="vertical-align:middle" title="l = 1,\ldots,L"/></span><span class="koboSpan" id="kobo.560.1" xmlns="http://www.w3.org/1999/xhtml">), each with a fixed amount of </span><strong><span class="koboSpan" id="kobo.561.1" xmlns="http://www.w3.org/1999/xhtml">neurons</span></strong> <span class="koboSpan" id="kobo.562.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="N_{l}" class="math inline" src="../media/file1124.png" style="vertical-align:middle" title="N_{l}"/></span><span class="koboSpan" id="kobo.563.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p></li>
<li><p><span class="koboSpan" id="kobo.564.1" xmlns="http://www.w3.org/1999/xhtml">A bunch of </span><strong><span class="koboSpan" id="kobo.565.1" xmlns="http://www.w3.org/1999/xhtml">activation functions</span></strong> <span class="koboSpan" id="kobo.566.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="h_{ln}" class="math inline" src="../media/file1125.png" style="vertical-align:middle" title="h_{ln}"/></span><span class="koboSpan" id="kobo.567.1" xmlns="http://www.w3.org/1999/xhtml"> for each neuron </span><span class="koboSpan" id="kobo.568.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="n" class="math inline" src="../media/file244.png" style="vertical-align:middle" title="n"/></span><span class="koboSpan" id="kobo.569.1" xmlns="http://www.w3.org/1999/xhtml"> in a layer </span><span class="koboSpan" id="kobo.570.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="l" class="math inline" src="../media/file514.png" style="vertical-align:middle" title="l"/></span><span class="koboSpan" id="kobo.571.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p></li>
<li><p><span class="koboSpan" id="kobo.572.1" xmlns="http://www.w3.org/1999/xhtml">A set of </span><strong><span class="koboSpan" id="kobo.573.1" xmlns="http://www.w3.org/1999/xhtml">biases</span></strong> <span class="koboSpan" id="kobo.574.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="b_{ln}" class="math inline" src="../media/file1126.png" style="vertical-align:middle" title="b_{ln}"/></span><span class="koboSpan" id="kobo.575.1" xmlns="http://www.w3.org/1999/xhtml"> for </span><span id="dx1-150006"/><span class="koboSpan" id="kobo.576.1" xmlns="http://www.w3.org/1999/xhtml">every neuron, and, for every neuron </span><span class="koboSpan" id="kobo.577.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="n" class="math inline" src="../media/file244.png" style="vertical-align:middle" title="n"/></span><span class="koboSpan" id="kobo.578.1" xmlns="http://www.w3.org/1999/xhtml"> in a layer </span><span class="koboSpan" id="kobo.579.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="l" class="math inline" src="../media/file514.png" style="vertical-align:middle" title="l"/></span><span class="koboSpan" id="kobo.580.1" xmlns="http://www.w3.org/1999/xhtml">, a set of </span><span class="koboSpan" id="kobo.581.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="N_{l - 1}" class="math inline" src="../media/file1127.png" style="vertical-align:middle" title="N_{l - 1}"/></span> <strong><span class="koboSpan" id="kobo.582.1" xmlns="http://www.w3.org/1999/xhtml">weights</span></strong> <span class="koboSpan" id="kobo.583.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="w_{kln}" class="math inline" src="../media/file1128.png" style="vertical-align:middle" title="w_{kln}"/></span><span class="koboSpan" id="kobo.584.1" xmlns="http://www.w3.org/1999/xhtml"> with </span><span class="koboSpan" id="kobo.585.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="k = 1,\ldots,N_{l - 1}" class="math inline" src="../media/file1129.png" style="vertical-align:middle" title="k = 1,\ldots,N_{l - 1}"/></span><span class="koboSpan" id="kobo.586.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.586.2" xmlns="http://www.w3.org/1999/xhtml">These biases and weights are the adjustable parameters that we would need to tweak in order to</span><span id="dx1-150007"/><span class="koboSpan" id="kobo.587.1" xmlns="http://www.w3.org/1999/xhtml"> get the model to behave as we want it to.</span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.588.1" xmlns="http://www.w3.org/1999/xhtml">In </span><em><span class="koboSpan" id="kobo.589.1" xmlns="http://www.w3.org/1999/xhtml">Figure</span></em> <em/> <a href="#Figure8.3"><em><span class="koboSpan" id="kobo.590.1" xmlns="http://www.w3.org/1999/xhtml">8.3</span></em></a><span class="koboSpan" id="kobo.591.1" xmlns="http://www.w3.org/1999/xhtml">, we can see a graphical </span><span id="dx1-150008"/><span class="koboSpan" id="kobo.592.1" xmlns="http://www.w3.org/1999/xhtml">representation of a simple neural network.</span></p>
<figure>
<span class="koboSpan" id="kobo.593.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="Figure 8.3: A simple neural network with two layers taking three inputs (a_{0n}). We have labeled some of the weights, but none of the biases or the activation functions " src="../media/file1131.jpg"/></span>
<figcaption aria-hidden="true"><span class="id" id="Figure8.3"><strong><span class="koboSpan" id="kobo.594.1" xmlns="http://www.w3.org/1999/xhtml">Figure 8.3</span></strong><span class="koboSpan" id="kobo.595.1" xmlns="http://www.w3.org/1999/xhtml">: </span></span><span class="content"><span class="koboSpan" id="kobo.596.1" xmlns="http://www.w3.org/1999/xhtml">A simple neural network with two layers taking three inputs (</span><span class="koboSpan" id="kobo.597.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="a_{0n})" class="math inline" src="../media/file1130.png" style="vertical-align:middle" title="a_{0n})"/></span><span class="koboSpan" id="kobo.598.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.598.2" xmlns="http://www.w3.org/1999/xhtml">We have labeled some of the weights, but none of the biases or the activation functions </span></span></figcaption>
</figure>
<p><span class="koboSpan" id="kobo.599.1" xmlns="http://www.w3.org/1999/xhtml">These are the ingredients that we need to set up a </span><span id="dx1-150011"/><span class="koboSpan" id="kobo.600.1" xmlns="http://www.w3.org/1999/xhtml">neural network. </span><span class="koboSpan" id="kobo.600.2" xmlns="http://www.w3.org/1999/xhtml">So, how does it work, then? </span><span class="koboSpan" id="kobo.600.3" xmlns="http://www.w3.org/1999/xhtml">Easy! </span><span class="koboSpan" id="kobo.600.4" xmlns="http://www.w3.org/1999/xhtml">For any choice of activation functions </span><span class="koboSpan" id="kobo.601.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="h_{ln}" class="math inline" src="../media/file1125.png" style="vertical-align:middle" title="h_{ln}"/></span><span class="koboSpan" id="kobo.602.1" xmlns="http://www.w3.org/1999/xhtml">, biases </span><span class="koboSpan" id="kobo.603.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="b_{ln}" class="math inline" src="../media/file1126.png" style="vertical-align:middle" title="b_{ln}"/></span><span class="koboSpan" id="kobo.604.1" xmlns="http://www.w3.org/1999/xhtml"> and weights </span><span class="koboSpan" id="kobo.605.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="w_{kln}" class="math inline" src="../media/file1128.png" style="vertical-align:middle" title="w_{kln}"/></span><span class="koboSpan" id="kobo.606.1" xmlns="http://www.w3.org/1999/xhtml">, the neural network takes some numerical inputs </span><span class="koboSpan" id="kobo.607.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="a_{0n}" class="math inline" src="../media/file1132.png" style="vertical-align:middle" title="a_{0n}"/></span><span class="koboSpan" id="kobo.608.1" xmlns="http://www.w3.org/1999/xhtml"> and, from there on, these inputs are propagated through the layers of the </span><span id="dx1-150012"/><span class="koboSpan" id="kobo.609.1" xmlns="http://www.w3.org/1999/xhtml">neural network in the following way: the values </span><span class="koboSpan" id="kobo.610.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="a_{ln}" class="math inline" src="../media/file1133.png" style="vertical-align:middle" title="a_{ln}"/></span><span class="koboSpan" id="kobo.611.1" xmlns="http://www.w3.org/1999/xhtml"> of the neurons </span><span class="koboSpan" id="kobo.612.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="n" class="math inline" src="../media/file244.png" style="vertical-align:middle" title="n"/></span><span class="koboSpan" id="kobo.613.1" xmlns="http://www.w3.org/1999/xhtml"> in all layers </span><span class="koboSpan" id="kobo.614.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="l" class="math inline" src="../media/file514.png" style="vertical-align:middle" title="l"/></span><span class="koboSpan" id="kobo.615.1" xmlns="http://www.w3.org/1999/xhtml"> are determined according to the inductive formula</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.616.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="a_{ln}: - h_{ln}\left( {b_{ln} + \sum\limits_{k = 1}^{N_{l - 1}}w_{kln}a_{l - 1,k}} \right)." class="math display" src="../media/file1134.png" style="vertical-align:middle" title="a_{ln}: - h_{ln}\left( {b_{ln} + \sum\limits_{k = 1}^{N_{l - 1}}w_{kln}a_{l - 1,k}} \right)."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.617.1" xmlns="http://www.w3.org/1999/xhtml">With this procedure, we can assign a value to each neuron in the network. </span><span class="koboSpan" id="kobo.617.2" xmlns="http://www.w3.org/1999/xhtml">The values of the neurons in the last layer are the output of the model.</span></p>
<p><span class="koboSpan" id="kobo.618.1" xmlns="http://www.w3.org/1999/xhtml">To be precise, what we have just described is </span><span id="dx1-150013"/><span class="koboSpan" id="kobo.619.1" xmlns="http://www.w3.org/1999/xhtml">known as an </span><strong><span class="koboSpan" id="kobo.620.1" xmlns="http://www.w3.org/1999/xhtml">artificial</span></strong> <strong><span class="koboSpan" id="kobo.621.1" xmlns="http://www.w3.org/1999/xhtml">feed-forward dense neural network</span></strong><span class="koboSpan" id="kobo.622.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.622.2" xmlns="http://www.w3.org/1999/xhtml">There are other possible architectures for neural networks, but this is the one that will be using for the most part of the rest of the book.</span></p>
<p><span class="koboSpan" id="kobo.623.1" xmlns="http://www.w3.org/1999/xhtml">That is how you can </span><span id="dx1-150014"/><span class="koboSpan" id="kobo.624.1" xmlns="http://www.w3.org/1999/xhtml">define a </span><span id="dx1-150015"/><span class="koboSpan" id="kobo.625.1" xmlns="http://www.w3.org/1999/xhtml">neural network, but there is one element in the definition to which we have not paid much attention: the activation function. </span><span class="koboSpan" id="kobo.625.2" xmlns="http://www.w3.org/1999/xhtml">We have mentioned before that this can be any function of our choice, and we have seen what role it plays in the behavior of a neural network, but what are some reasonable choices for this function? </span><span class="koboSpan" id="kobo.625.3" xmlns="http://www.w3.org/1999/xhtml">Let’s explore the most common ones:</span></p>
<ul>
<li><p><span class="koboSpan" id="kobo.626.1" xmlns="http://www.w3.org/1999/xhtml">We may start off with a simple activation function, actually, the same one that we implicitly considered when we defined the </span><span id="dx1-150016"/><span class="koboSpan" id="kobo.627.1" xmlns="http://www.w3.org/1999/xhtml">perceptron. </span><span class="koboSpan" id="kobo.627.2" xmlns="http://www.w3.org/1999/xhtml">This is a </span><strong><span class="koboSpan" id="kobo.628.1" xmlns="http://www.w3.org/1999/xhtml">step function</span></strong><span class="koboSpan" id="kobo.629.1" xmlns="http://www.w3.org/1999/xhtml"> given by</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.630.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="h(x) = \left\{ \begin{array}{ll} {1,\quad} &amp; {x \geq 0} \\ {0,\quad} &amp; {x &lt; 0.} \\ \end{array} \right." class="math display" src="../media/file1135.png" style="vertical-align:middle" title="h(x) = \left\{ \begin{array}{ll} {1,\quad} &amp; {x \geq 0} \\ {0,\quad} &amp; {x &lt; 0.} \\ \end{array} \right."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.631.1" xmlns="http://www.w3.org/1999/xhtml">We could technically use this in a neural network, but…in truth…it would not be a very wise choice. </span><span class="koboSpan" id="kobo.631.2" xmlns="http://www.w3.org/1999/xhtml">It is not differentiable, not even continuous. </span><span class="koboSpan" id="kobo.631.3" xmlns="http://www.w3.org/1999/xhtml">And, as we will soon see, that usually makes any function a terrible candidate to be an activation function inside a neural network. </span><span class="koboSpan" id="kobo.631.4" xmlns="http://www.w3.org/1999/xhtml">In any case, it is an example of historical importance.</span></p></li>
<li><p><span class="koboSpan" id="kobo.632.1" xmlns="http://www.w3.org/1999/xhtml">Let’s now consider a somewhat more sophisticated and interesting example: the </span><strong><span class="koboSpan" id="kobo.633.1" xmlns="http://www.w3.org/1999/xhtml">sigmoid</span></strong><span class="koboSpan" id="kobo.634.1" xmlns="http://www.w3.org/1999/xhtml"> activation function. </span><span class="koboSpan" id="kobo.634.2" xmlns="http://www.w3.org/1999/xhtml">This </span><span id="dx1-150017"/><span class="koboSpan" id="kobo.635.1" xmlns="http://www.w3.org/1999/xhtml">function is smooth and continuous, and it outputs values between </span><span class="koboSpan" id="kobo.636.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.637.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.638.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.639.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.639.2" xmlns="http://www.w3.org/1999/xhtml">This makes it an ideal candidate for the activation function in the final layer of, for example, a classifier. </span><span class="koboSpan" id="kobo.639.3" xmlns="http://www.w3.org/1999/xhtml">It is defined by</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.640.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="S(x) = \frac{e^{x}}{e^{x} + 1}." class="math display" src="../media/file1136.png" style="vertical-align:middle" title="S(x) = \frac{e^{x}}{e^{x} + 1}."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.641.1" xmlns="http://www.w3.org/1999/xhtml">We have plotted it in </span><em><span class="koboSpan" id="kobo.642.1" xmlns="http://www.w3.org/1999/xhtml">Figure</span></em> <a href="#Figure8.4a"><em><span class="koboSpan" id="kobo.643.1" xmlns="http://www.w3.org/1999/xhtml">8.4a</span></em></a><span class="koboSpan" id="kobo.644.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p></li>
<li><p><span class="koboSpan" id="kobo.645.1" xmlns="http://www.w3.org/1999/xhtml">As beautiful as it may seem, when used in inner layers, the sigmoid function can easily lead to problems in the training process (see Aurelien’s book for more on this </span><span class="cite"><span class="koboSpan" id="kobo.646.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xhandsonml"><span class="koboSpan" id="kobo.647.1" xmlns="http://www.w3.org/1999/xhtml">104</span></a><span class="koboSpan" id="kobo.648.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.649.1" xmlns="http://www.w3.org/1999/xhtml">). </span><span class="koboSpan" id="kobo.649.2" xmlns="http://www.w3.org/1999/xhtml">In general, a better choice for inner layers is the </span><strong><span class="koboSpan" id="kobo.650.1" xmlns="http://www.w3.org/1999/xhtml">exponential linear unit</span></strong><span class="koboSpan" id="kobo.651.1" xmlns="http://www.w3.org/1999/xhtml"> or </span><strong><span class="koboSpan" id="kobo.652.1" xmlns="http://www.w3.org/1999/xhtml">ELU</span></strong><span class="koboSpan" id="kobo.653.1" xmlns="http://www.w3.org/1999/xhtml"> activation </span><span id="dx1-150018"/><span class="koboSpan" id="kobo.654.1" xmlns="http://www.w3.org/1999/xhtml">function, defined as</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.655.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="E(x) = \left\{ \begin{array}{ll} {x,\quad} &amp; {x \geq 0} \\ {e^{x} - 1,\quad} &amp; {x &lt; 0.} \\ \end{array} \right." class="math display" src="../media/file1137.png" style="vertical-align:middle" title="E(x) = \left\{ \begin{array}{ll} {x,\quad} &amp; {x \geq 0} \\ {e^{x} - 1,\quad} &amp; {x &lt; 0.} \\ \end{array} \right."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.656.1" xmlns="http://www.w3.org/1999/xhtml">You can find its plot in </span><em><span class="koboSpan" id="kobo.657.1" xmlns="http://www.w3.org/1999/xhtml">Figure</span></em> <a href="#Figure8.4b"><em><span class="koboSpan" id="kobo.658.1" xmlns="http://www.w3.org/1999/xhtml">8.4b</span></em></a><span class="koboSpan" id="kobo.659.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p></li>
<li><p><span class="koboSpan" id="kobo.660.1" xmlns="http://www.w3.org/1999/xhtml">We will also </span><span id="dx1-150019"/><span class="koboSpan" id="kobo.661.1" xmlns="http://www.w3.org/1999/xhtml">discuss one last activation function: the </span><strong><span class="koboSpan" id="kobo.662.1" xmlns="http://www.w3.org/1999/xhtml">rectified linear unit</span></strong><span class="koboSpan" id="kobo.663.1" xmlns="http://www.w3.org/1999/xhtml"> or </span><strong><span class="koboSpan" id="kobo.664.1" xmlns="http://www.w3.org/1999/xhtml">ReLU</span></strong><span class="koboSpan" id="kobo.665.1" xmlns="http://www.w3.org/1999/xhtml"> function. </span><span class="koboSpan" id="kobo.665.2" xmlns="http://www.w3.org/1999/xhtml">In general, it yields worse </span><span id="dx1-150020"/><span class="koboSpan" id="kobo.666.1" xmlns="http://www.w3.org/1999/xhtml">results than the ELU function, but it is easier to compute and thus its use can speed up the training. </span><span class="koboSpan" id="kobo.666.2" xmlns="http://www.w3.org/1999/xhtml">It is defined as</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.667.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="R(x) = \max\{ 0,x\}." class="math display" src="../media/file1138.png" style="vertical-align:middle" title="R(x) = \max\{ 0,x\}."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.668.1" xmlns="http://www.w3.org/1999/xhtml">The plot can be found in </span><em><span class="koboSpan" id="kobo.669.1" xmlns="http://www.w3.org/1999/xhtml">Figure</span></em> <a href="#Figure8.4c"><em><span class="koboSpan" id="kobo.670.1" xmlns="http://www.w3.org/1999/xhtml">8.4c</span></em></a><span class="koboSpan" id="kobo.671.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p></li>
</ul>
<div class="tcolorbox questionx" id="tcolobox-156">
<span id="x1-150022x8.2.1"/>
<div class="tcolorbox-title">
<p><span class="koboSpan" id="kobo.672.1" xmlns="http://www.w3.org/1999/xhtml">Exercise 8.3</span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.673.1" xmlns="http://www.w3.org/1999/xhtml">Check that the image of the sigmoid function </span><span class="koboSpan" id="kobo.674.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="S" class="math inline" src="../media/file73.png" style="vertical-align:middle" title="S"/></span><span class="koboSpan" id="kobo.675.1" xmlns="http://www.w3.org/1999/xhtml"> is </span><span class="koboSpan" id="kobo.676.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(0,1)" class="math inline" src="../media/file305.png" style="vertical-align:middle" title="(0,1)"/></span><span class="koboSpan" id="kobo.677.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.677.2" xmlns="http://www.w3.org/1999/xhtml">Prove that the ELU function </span><span class="koboSpan" id="kobo.678.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="E" class="math inline" src="../media/file327.png" style="vertical-align:middle" title="E"/></span><span class="koboSpan" id="kobo.679.1" xmlns="http://www.w3.org/1999/xhtml"> is smooth and that its image is </span><span class="koboSpan" id="kobo.680.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="( - 1,\infty)" class="math inline" src="../media/file1139.png" style="vertical-align:middle" title="( - 1,\infty)"/></span><span class="koboSpan" id="kobo.681.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.681.2" xmlns="http://www.w3.org/1999/xhtml">What is the image of the ReLU function? </span><span class="koboSpan" id="kobo.681.3" xmlns="http://www.w3.org/1999/xhtml">Is it smooth?</span></p>
</div>
</div>
<figure>
<span class="koboSpan" id="kobo.682.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(a) Sigmoid function" src="../media/file1140.jpg"/></span>
<figcaption aria-hidden="true"><span id="Figure8.4a"><strong><span class="koboSpan" id="kobo.683.1" xmlns="http://www.w3.org/1999/xhtml">(a)</span></strong></span><span class="koboSpan" id="kobo.684.1" xmlns="http://www.w3.org/1999/xhtml"> Sigmoid function</span></figcaption>
</figure>
<figure>
<span class="koboSpan" id="kobo.685.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(b) ELU function" src="../media/file1141.jpg"/></span>
<figcaption aria-hidden="true"><span id="Figure8.4b"><strong><span class="koboSpan" id="kobo.686.1" xmlns="http://www.w3.org/1999/xhtml">(b)</span></strong></span><span class="koboSpan" id="kobo.687.1" xmlns="http://www.w3.org/1999/xhtml"> ELU function</span></figcaption>
</figure>
<figure>
<span class="koboSpan" id="kobo.688.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(c) ReLU function" src="../media/file1142.jpg"/></span>
<figcaption aria-hidden="true"><span id="Figure8.4c"><strong><span class="koboSpan" id="kobo.689.1" xmlns="http://www.w3.org/1999/xhtml">(c)</span></strong></span><span class="koboSpan" id="kobo.690.1" xmlns="http://www.w3.org/1999/xhtml"> ReLU function</span></figcaption>
</figure>
<p><span id="Figure8.4"><strong><span class="koboSpan" id="kobo.691.1" xmlns="http://www.w3.org/1999/xhtml">Figure 8.4</span></strong></span><span class="koboSpan" id="kobo.692.1" xmlns="http://www.w3.org/1999/xhtml">: Some common activation functions in neural network</span></p>
<p><span class="koboSpan" id="kobo.693.1" xmlns="http://www.w3.org/1999/xhtml">As we </span><span id="dx1-150028"/><span class="koboSpan" id="kobo.694.1" xmlns="http://www.w3.org/1999/xhtml">mentioned at the </span><span id="dx1-150029"/><span class="koboSpan" id="kobo.695.1" xmlns="http://www.w3.org/1999/xhtml">beginning of the chapter, it has been proven that neural networks are universal function approximators </span><span class="cite"><span class="koboSpan" id="kobo.696.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xnn-universal"><span class="koboSpan" id="kobo.697.1" xmlns="http://www.w3.org/1999/xhtml">107</span></a><span class="koboSpan" id="kobo.698.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.699.1" xmlns="http://www.w3.org/1999/xhtml">, so they are interesting models to consider in any problem involving supervised machine learning. </span><span class="koboSpan" id="kobo.699.2" xmlns="http://www.w3.org/1999/xhtml">And thus, they will be the model we will use to build our classifier. </span><span class="koboSpan" id="kobo.699.3" xmlns="http://www.w3.org/1999/xhtml">We will consider a neural network with two inputs and some layers — we will later decide how many of them and how many neurons each will have. </span><span class="koboSpan" id="kobo.699.4" xmlns="http://www.w3.org/1999/xhtml">The final layer, of course, will have a single neuron, which will be the output. </span><span class="koboSpan" id="kobo.699.5" xmlns="http://www.w3.org/1999/xhtml">We will use ELU activation functions all throughout the network, except for the last layer; there, we will use a sigmoid activation function in order to get a normalized result. </span><span class="koboSpan" id="kobo.699.6" xmlns="http://www.w3.org/1999/xhtml">That way, we will get a continuous value between </span><span class="koboSpan" id="kobo.700.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.701.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.702.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.703.1" xmlns="http://www.w3.org/1999/xhtml">, and, as it is customary, we will define a threshold at </span><span class="koboSpan" id="kobo.704.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\left. 1\slash 2 \right." class="math inline" src="../media/file136.png" style="vertical-align:middle" title="\left. 1\slash 2 \right."/></span><span class="koboSpan" id="kobo.705.1" xmlns="http://www.w3.org/1999/xhtml"> to assign positive (</span><span class="koboSpan" id="kobo.706.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\left. \geq 1\slash 2 \right." class="math inline" src="../media/file1143.png" style="vertical-align:middle" title="\left. \geq 1\slash 2 \right."/></span><span class="koboSpan" id="kobo.707.1" xmlns="http://www.w3.org/1999/xhtml">) or negative (</span><span class="koboSpan" id="kobo.708.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\left. &lt; 1\slash 2 \right." class="math inline" src="../media/file1144.png" style="vertical-align:middle" title="\left. &lt; 1\slash 2 \right."/></span><span class="koboSpan" id="kobo.709.1" xmlns="http://www.w3.org/1999/xhtml">) to any given output.</span></p>
<p><span class="koboSpan" id="kobo.710.1" xmlns="http://www.w3.org/1999/xhtml">Now that our model is ready, the next challenge that is waiting for us is finding a suitable loss function.</span></p>
</section>
<section class="level3 subsectionHead" data-number="16.2.2" id="understanding-loss-functions">
<h2 class="subsectionHead" data-number="16.2.2"><span class="titlemark"><span class="koboSpan" id="kobo.711.1" xmlns="http://www.w3.org/1999/xhtml">8.2.2 </span></span> <span id="x1-1510008.2.2"><span class="koboSpan" id="kobo.712.1" xmlns="http://www.w3.org/1999/xhtml">Understanding loss functions</span></span></h2>
<p><span class="koboSpan" id="kobo.713.1" xmlns="http://www.w3.org/1999/xhtml">When it comes to defining a </span><span id="dx1-151001"/><span class="koboSpan" id="kobo.714.1" xmlns="http://www.w3.org/1999/xhtml">loss function for supervised machine learning, with models that depend on some continuous parameters, we want to look for loss functions that are continuous and differentiable with respect to the trainable parameters of the model. </span><span class="koboSpan" id="kobo.714.2" xmlns="http://www.w3.org/1999/xhtml">The reason for this is the same reason why we want our activation functions to be differentiable, and it will become clear later on.</span></p>
<p><span class="koboSpan" id="kobo.715.1" xmlns="http://www.w3.org/1999/xhtml">As we discussed earlier, the most natural loss function — and the one whose expected value we truly want to minimize — would be the 0-1 loss function, but this function would not have a continuous dependence on the parameters of the model: it would take ”discrete jumps” as the classifier changes its behavior. </span><span class="koboSpan" id="kobo.715.2" xmlns="http://www.w3.org/1999/xhtml">Therefore, we need to look for alternative loss functions that are indeed continuous and differentiable while still measuring the loss in a manner that is reasonable and natural enough for classification problems.</span></p>
<p><span class="koboSpan" id="kobo.716.1" xmlns="http://www.w3.org/1999/xhtml">Another somewhat naive yet much better choice would be to take the </span><strong><span class="koboSpan" id="kobo.717.1" xmlns="http://www.w3.org/1999/xhtml">mean</span></strong> <strong><span class="koboSpan" id="kobo.718.1" xmlns="http://www.w3.org/1999/xhtml">squared error</span></strong><span class="koboSpan" id="kobo.719.1" xmlns="http://www.w3.org/1999/xhtml"> as our loss function. </span><span class="koboSpan" id="kobo.719.2" xmlns="http://www.w3.org/1999/xhtml">For the purposes of our problem, we know that the neural network returns a continuous value between </span><span class="koboSpan" id="kobo.720.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.721.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.722.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.723.1" xmlns="http://www.w3.org/1999/xhtml">, and we know that — ideally — the closer this value is to </span><span class="koboSpan" id="kobo.724.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.725.1" xmlns="http://www.w3.org/1999/xhtml"> or </span><span class="koboSpan" id="kobo.726.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.727.1" xmlns="http://www.w3.org/1999/xhtml">, the more likely it corresponds to a negative or positive input respectively. </span><span class="koboSpan" id="kobo.727.2" xmlns="http://www.w3.org/1999/xhtml">In order to do the classification, we set a threshold at </span><span class="koboSpan" id="kobo.728.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\left. 1\slash 2 \right." class="math inline" src="../media/file136.png" style="vertical-align:middle" title="\left. 1\slash 2 \right."/></span><span class="koboSpan" id="kobo.729.1" xmlns="http://www.w3.org/1999/xhtml"> and get a discrete label, but, in order to compute the loss function, we should actually look at that continuous output! </span><span class="koboSpan" id="kobo.729.2" xmlns="http://www.w3.org/1999/xhtml">In this way, if we let </span><span class="koboSpan" id="kobo.730.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="M_{\theta}(x)" class="math inline" src="../media/file1102.png" style="vertical-align:middle" title="M_{\theta}(x)"/></span><span class="koboSpan" id="kobo.731.1" xmlns="http://www.w3.org/1999/xhtml"> be the continuous value in </span><span class="koboSpan" id="kobo.732.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\lbrack 0,1\rbrack" class="math inline" src="../media/file1145.png" style="vertical-align:middle" title="\lbrack 0,1\rbrack"/></span><span class="koboSpan" id="kobo.733.1" xmlns="http://www.w3.org/1999/xhtml"> returned by the model for a given input </span><span class="koboSpan" id="kobo.734.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x" class="math inline" src="../media/file269.png" style="vertical-align:middle" title="x"/></span><span class="koboSpan" id="kobo.735.1" xmlns="http://www.w3.org/1999/xhtml">, and we let </span><span class="koboSpan" id="kobo.736.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="y \in \{ 0,1\}" class="math inline" src="../media/file1146.png" style="vertical-align:middle" title="y \in \{ 0,1\}"/></span><span class="koboSpan" id="kobo.737.1" xmlns="http://www.w3.org/1999/xhtml"> be its corresponding label, we could take our loss function to be</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.738.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="L(\theta;x,y) = \left( {M_{\theta}(x) - y} \right)^{2}," class="math display" src="../media/file1147.png" style="vertical-align:middle" title="L(\theta;x,y) = \left( {M_{\theta}(x) - y} \right)^{2},"/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.739.1" xmlns="http://www.w3.org/1999/xhtml">where we have grouped in </span><span class="koboSpan" id="kobo.740.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\theta" class="math inline" src="../media/file89.png" style="vertical-align:middle" title="\theta"/></span><span class="koboSpan" id="kobo.741.1" xmlns="http://www.w3.org/1999/xhtml"> all the parameters (weights and biases) on which our neural network </span><span class="koboSpan" id="kobo.742.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="M" class="math inline" src="../media/file704.png" style="vertical-align:middle" title="M"/></span><span class="koboSpan" id="kobo.743.1" xmlns="http://www.w3.org/1999/xhtml"> depends.</span></p>
<p><span class="koboSpan" id="kobo.744.1" xmlns="http://www.w3.org/1999/xhtml">Of course, in order to compute the training loss (the expected value over the training dataset), we would just take the average value over the training dataset, and analogously for the validation loss. </span><span class="koboSpan" id="kobo.744.2" xmlns="http://www.w3.org/1999/xhtml">This is usually </span><span id="dx1-151002"/><span class="koboSpan" id="kobo.745.1" xmlns="http://www.w3.org/1999/xhtml">called the </span><strong><span class="koboSpan" id="kobo.746.1" xmlns="http://www.w3.org/1999/xhtml">mean squared error</span></strong><span class="koboSpan" id="kobo.747.1" xmlns="http://www.w3.org/1999/xhtml"> (</span><strong><span class="koboSpan" id="kobo.748.1" xmlns="http://www.w3.org/1999/xhtml">MSE</span></strong><span class="koboSpan" id="kobo.749.1" xmlns="http://www.w3.org/1999/xhtml">) because, well, it is the average of the error squared.</span></p>
<p><span class="koboSpan" id="kobo.750.1" xmlns="http://www.w3.org/1999/xhtml">The MSE is a good loss function, but when it comes to binary classifiers, there is actually an even better candidate: the </span><strong><span class="koboSpan" id="kobo.751.1" xmlns="http://www.w3.org/1999/xhtml">binary cross-entropy</span></strong><span class="koboSpan" id="kobo.752.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.752.2" xmlns="http://www.w3.org/1999/xhtml">It is </span><span id="dx1-151003"/><span class="koboSpan" id="kobo.753.1" xmlns="http://www.w3.org/1999/xhtml">computed as</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.754.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="H(\theta;x,y) = - y\log\left( {M_{\theta}(x)} \right) - (1 - y)\log\left( {1 - M_{\theta}(x)} \right)." class="math display" src="../media/file1148.png" style="vertical-align:middle" title="H(\theta;x,y) = - y\log\left( {M_{\theta}(x)} \right) - (1 - y)\log\left( {1 - M_{\theta}(x)} \right)."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.755.1" xmlns="http://www.w3.org/1999/xhtml">Now, this may seem like a very complicated expression, but it is actually a very elegant and powerful loss function! </span><span class="koboSpan" id="kobo.755.2" xmlns="http://www.w3.org/1999/xhtml">For starters, if the output of the model is differentiable and continuous with respect to its trainable parameters, so is the loss (that is easy to check, just go back to Calculus 101). </span><span class="koboSpan" id="kobo.755.3" xmlns="http://www.w3.org/1999/xhtml">And that’s not all. </span><span class="koboSpan" id="kobo.755.4" xmlns="http://www.w3.org/1999/xhtml">The following exercise may help you realize why the binary cross-entropy function is a great choice function for binary classifiers.</span></p>
<div class="tcolorbox questionx" id="tcolobox-157">
<span id="x1-151005x8.2.2"/>
<div class="tcolorbox-title">
<p><span class="koboSpan" id="kobo.756.1" xmlns="http://www.w3.org/1999/xhtml">Exercise 8.4</span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.757.1" xmlns="http://www.w3.org/1999/xhtml">Show that the output of the binary cross-entropy loss function </span><span class="koboSpan" id="kobo.758.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="H(\theta;x,y)" class="math inline" src="../media/file1149.png" style="vertical-align:middle" title="H(\theta;x,y)"/></span><span class="koboSpan" id="kobo.759.1" xmlns="http://www.w3.org/1999/xhtml"> is </span><span class="koboSpan" id="kobo.760.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.761.1" xmlns="http://www.w3.org/1999/xhtml"> if </span><span class="koboSpan" id="kobo.762.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="M_{\theta}(x) = y" class="math inline" src="../media/file1150.png" style="vertical-align:middle" title="M_{\theta}(x) = y"/></span><span class="koboSpan" id="kobo.763.1" xmlns="http://www.w3.org/1999/xhtml"> and that it diverges to </span><span class="koboSpan" id="kobo.764.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\infty" class="math inline" src="../media/file1151.png" style="vertical-align:middle" title="\infty"/></span><span class="koboSpan" id="kobo.765.1" xmlns="http://www.w3.org/1999/xhtml"> as </span><span class="koboSpan" id="kobo.766.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="M_{\theta}(x)" class="math inline" src="../media/file1102.png" style="vertical-align:middle" title="M_{\theta}(x)"/></span><span class="koboSpan" id="kobo.767.1" xmlns="http://www.w3.org/1999/xhtml"> approaches the opposite label to </span><span class="koboSpan" id="kobo.768.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="y" class="math inline" src="../media/file270.png" style="vertical-align:middle" title="y"/></span><span class="koboSpan" id="kobo.769.1" xmlns="http://www.w3.org/1999/xhtml"> (this is, as </span><span class="koboSpan" id="kobo.770.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\left. M_{\theta}(x)\rightarrow 1 \right." class="math inline" src="../media/file1152.png" style="vertical-align:middle" title="\left. M_{\theta}(x)\rightarrow 1 \right."/></span><span class="koboSpan" id="kobo.771.1" xmlns="http://www.w3.org/1999/xhtml"> if </span><span class="koboSpan" id="kobo.772.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="y_{i} = 0" class="math inline" src="../media/file1153.png" style="vertical-align:middle" title="y_{i} = 0"/></span><span class="koboSpan" id="kobo.773.1" xmlns="http://www.w3.org/1999/xhtml"> and as </span><span class="koboSpan" id="kobo.774.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\left. M(x)\rightarrow 0 \right." class="math inline" src="../media/file1154.png" style="vertical-align:middle" title="\left. M(x)\rightarrow 0 \right."/></span><span class="koboSpan" id="kobo.775.1" xmlns="http://www.w3.org/1999/xhtml"> if </span><span class="koboSpan" id="kobo.776.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="y = 1" class="math inline" src="../media/file769.png" style="vertical-align:middle" title="y = 1"/></span><span class="koboSpan" id="kobo.777.1" xmlns="http://www.w3.org/1999/xhtml">).</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.778.1" xmlns="http://www.w3.org/1999/xhtml">And, with this, our shiny new loss </span><span id="dx1-151006"/><span class="koboSpan" id="kobo.779.1" xmlns="http://www.w3.org/1999/xhtml">function is ready to be used. </span><span class="koboSpan" id="kobo.779.2" xmlns="http://www.w3.org/1999/xhtml">However, there is one last element we still have to take care of, one that we have so far neglected and ignored. </span><span class="koboSpan" id="kobo.779.3" xmlns="http://www.w3.org/1999/xhtml">Yes, in the following section, we shall give optimization algorithms the attention and care that they deserve!</span></p>
</section>
<section class="level3 subsectionHead" data-number="16.2.3" id="gradient-descent">
<h2 class="subsectionHead" data-number="16.2.3"><span class="titlemark"><span class="koboSpan" id="kobo.780.1" xmlns="http://www.w3.org/1999/xhtml">8.2.3 </span></span> <span id="x1-1520008.2.3"><span class="koboSpan" id="kobo.781.1" xmlns="http://www.w3.org/1999/xhtml">Gradient descent</span></span></h2>
<p><span class="koboSpan" id="kobo.782.1" xmlns="http://www.w3.org/1999/xhtml">You are now reading this book, probably in the comfort of your home, college library, or office. </span><span class="koboSpan" id="kobo.782.2" xmlns="http://www.w3.org/1999/xhtml">But life </span><span id="dx1-152001"/><span class="koboSpan" id="kobo.783.1" xmlns="http://www.w3.org/1999/xhtml">changes in the most unexpected ways, and, maybe, in a couple of weeks, you will find yourself at the top of a mountain, blindfolded (don’t ask us why) and tasked with the mission of reaching the bottom of a nearby valley. </span><span class="koboSpan" id="kobo.783.2" xmlns="http://www.w3.org/1999/xhtml">If this happened, what would you do?</span></p>
<p><span class="koboSpan" id="kobo.784.1" xmlns="http://www.w3.org/1999/xhtml">You don’t have to be a survival expert to accomplish this task. </span><span class="koboSpan" id="kobo.784.2" xmlns="http://www.w3.org/1999/xhtml">It’s true that — for undisclosed reasons — you are blindfolded, so you can’t see where the valley is, but, hey, you can still move around, can’t you? </span><span class="koboSpan" id="kobo.784.3" xmlns="http://www.w3.org/1999/xhtml">So, you could take some small steps in whichever direction you feel is leading you downwards with the highest slope. </span><span class="koboSpan" id="kobo.784.4" xmlns="http://www.w3.org/1999/xhtml">And you could just repeat that process several times and, eventually, you would reach the bottom of a valley.</span></p>
<p><span class="koboSpan" id="kobo.785.1" xmlns="http://www.w3.org/1999/xhtml">Of course, as you descend, you will have to be careful with how big your steps are. </span><span class="koboSpan" id="kobo.785.2" xmlns="http://www.w3.org/1999/xhtml">Take steps that are too big, and you may go from the top of a </span><span id="dx1-152002"/><span class="koboSpan" id="kobo.786.1" xmlns="http://www.w3.org/1999/xhtml">mountain to the top of another one, skipping all the valleys in between (some medical doctors have suggested this might not be anatomically possible, but, well, you get what we mean). </span><span class="koboSpan" id="kobo.786.2" xmlns="http://www.w3.org/1999/xhtml">On the other hand, make your steps too small and it is going to take you forever to reach the valley. </span><span class="koboSpan" id="kobo.786.3" xmlns="http://www.w3.org/1999/xhtml">So, you will have to find a sweet spot!</span></p>
<p><span class="koboSpan" id="kobo.787.1" xmlns="http://www.w3.org/1999/xhtml">Anyhow, how does this seemingly crazy thought experiment relate to machine learning? </span><span class="koboSpan" id="kobo.787.2" xmlns="http://www.w3.org/1999/xhtml">Let’s see.</span></p>
<p><span class="paragraphHead"><span id="x1-1530008.2.3"/><strong><span class="koboSpan" id="kobo.788.1" xmlns="http://www.w3.org/1999/xhtml">Gradient descent algorithms</span></strong></span><span class="koboSpan" id="kobo.789.1" xmlns="http://www.w3.org/1999/xhtml"> We now have a powerful-enough model that </span><span id="dx1-153001"/><span class="koboSpan" id="kobo.790.1" xmlns="http://www.w3.org/1999/xhtml">depends on some parameters. </span><span class="koboSpan" id="kobo.790.2" xmlns="http://www.w3.org/1999/xhtml">Moreover, since we have made wise life choices, we also have a loss function </span><span class="koboSpan" id="kobo.791.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="L" class="math inline" src="../media/file1012.png" style="vertical-align:middle" title="L"/></span><span class="koboSpan" id="kobo.792.1" xmlns="http://www.w3.org/1999/xhtml"> that depends continuously on and is differentiable with respect to these parameters (that is because we picked some smooth activation functions and the binary cross-entropy).</span></p>
<p><span class="koboSpan" id="kobo.793.1" xmlns="http://www.w3.org/1999/xhtml">By doing this, we have effectively reduced our machine learning problem to the problem of minimizing a loss function, which is a differentiable function on some variables (the trainable parameters). </span><span class="koboSpan" id="kobo.793.2" xmlns="http://www.w3.org/1999/xhtml">And how do we do this? </span><span class="koboSpan" id="kobo.793.3" xmlns="http://www.w3.org/1999/xhtml">Using the Force…Sorry, we got carried away. </span><span class="koboSpan" id="kobo.793.4" xmlns="http://www.w3.org/1999/xhtml">We meant: using calculus!</span></p>
<p><span class="koboSpan" id="kobo.794.1" xmlns="http://www.w3.org/1999/xhtml">The ”getting to the valley” problem that we discussed before is — as you may have very well guessed by now — a simple analogy that will help us illustrate the </span><strong><span class="koboSpan" id="kobo.795.1" xmlns="http://www.w3.org/1999/xhtml">gradient descent method</span></strong><span class="koboSpan" id="kobo.796.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.796.2" xmlns="http://www.w3.org/1999/xhtml">This </span><span id="dx1-153002"/><span class="koboSpan" id="kobo.797.1" xmlns="http://www.w3.org/1999/xhtml">method is just an algorithm that will allow us to minimize a differentiable function, and we can think of it as the mathematical equivalent of taking small steps in the steepest downward direction on a mountain. </span><span class="koboSpan" id="kobo.797.2" xmlns="http://www.w3.org/1999/xhtml">We should warn you that the remaining content of this subsection might be somewhat dense. </span><span class="koboSpan" id="kobo.797.3" xmlns="http://www.w3.org/1999/xhtml">Please, don’t let technicalities overwhelm you. </span><span class="koboSpan" id="kobo.797.4" xmlns="http://www.w3.org/1999/xhtml">If this were a song, it’d be perfectly fine not to know its lyrics; all that would matter is for you to be familiar with its rhythm!</span></p>
<p><span class="koboSpan" id="kobo.798.1" xmlns="http://www.w3.org/1999/xhtml">As you may remember from the sweet old days of undergraduate calculus, whenever you have a differentiable function </span><span class="koboSpan" id="kobo.799.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\left. f:R^{N}\rightarrow R \right." class="math inline" src="../media/file1155.png" style="vertical-align:middle" title="\left. f:R^{N}\rightarrow R \right."/></span><span class="koboSpan" id="kobo.800.1" xmlns="http://www.w3.org/1999/xhtml"> (for those of you less familiar with mathematical notation, this is a fancy way of saying that </span><span class="koboSpan" id="kobo.801.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="f" class="math inline" src="../media/file778.png" style="vertical-align:middle" title="f"/></span><span class="koboSpan" id="kobo.802.1" xmlns="http://www.w3.org/1999/xhtml"> has </span><span class="koboSpan" id="kobo.803.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="N" class="math inline" src="../media/file784.png" style="vertical-align:middle" title="N"/></span><span class="koboSpan" id="kobo.804.1" xmlns="http://www.w3.org/1999/xhtml"> real-number inputs and returns a single real-number output), the direction in which it decreases more steeply at a point </span><span class="koboSpan" id="kobo.805.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x" class="math inline" src="../media/file269.png" style="vertical-align:middle" title="x"/></span><span class="koboSpan" id="kobo.806.1" xmlns="http://www.w3.org/1999/xhtml"> is given by </span><span class="koboSpan" id="kobo.807.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="- \nabla f(x)" class="math inline" src="../media/file1156.png" style="vertical-align:middle" title="- \nabla f(x)"/></span><span class="koboSpan" id="kobo.808.1" xmlns="http://www.w3.org/1999/xhtml">, where </span><span class="koboSpan" id="kobo.809.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\nabla f(x)" class="math inline" src="../media/file1157.png" style="vertical-align:middle" title="\nabla f(x)"/></span><span class="koboSpan" id="kobo.810.1" xmlns="http://www.w3.org/1999/xhtml"> is the </span><strong><span class="koboSpan" id="kobo.811.1" xmlns="http://www.w3.org/1999/xhtml">gradient</span></strong> <strong><span class="koboSpan" id="kobo.812.1" xmlns="http://www.w3.org/1999/xhtml">vector</span></strong><span class="koboSpan" id="kobo.813.1" xmlns="http://www.w3.org/1999/xhtml"> at </span><span class="koboSpan" id="kobo.814.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x" class="math inline" src="../media/file269.png" style="vertical-align:middle" title="x"/></span><span class="koboSpan" id="kobo.815.1" xmlns="http://www.w3.org/1999/xhtml">, and is </span><span id="dx1-153003"/><span class="koboSpan" id="kobo.816.1" xmlns="http://www.w3.org/1999/xhtml">computed as</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.817.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\nabla f(x) = \left( {\left. \frac{\partial f}{\partial x_{1}} \right|_{x},\ldots,\left. \frac{\partial f}{\partial x_{n}} \right|_{x}} \right)," class="math display" src="../media/file1158.png" style="vertical-align:middle" title="\nabla f(x) = \left( {\left. \frac{\partial f}{\partial x_{1}} \right|_{x},\ldots,\left. \frac{\partial f}{\partial x_{n}} \right|_{x}} \right),"/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.818.1" xmlns="http://www.w3.org/1999/xhtml">where </span><span class="koboSpan" id="kobo.819.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\left. \partial\slash\partial x_{i} \right." class="math inline" src="../media/file1159.png" style="vertical-align:middle" title="\left. \partial\slash\partial x_{i} \right."/></span><span class="koboSpan" id="kobo.820.1" xmlns="http://www.w3.org/1999/xhtml"> denotes the partial derivative operator with respect to a variable </span><span class="koboSpan" id="kobo.821.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="x_{i}" class="math inline" src="../media/file714.png" style="vertical-align:middle" title="x_{i}"/></span><span class="koboSpan" id="kobo.822.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.822.2" xmlns="http://www.w3.org/1999/xhtml">So, if we want to move towards a minimum at a given point, we will have to move in the direction of </span><span class="koboSpan" id="kobo.823.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="- \nabla f(x)" class="math inline" src="../media/file1156.png" style="vertical-align:middle" title="- \nabla f(x)"/></span><span class="koboSpan" id="kobo.824.1" xmlns="http://www.w3.org/1999/xhtml">, but by what amount?</span></p>
<p><span class="koboSpan" id="kobo.825.1" xmlns="http://www.w3.org/1999/xhtml">The mathematical </span><span id="dx1-153004"/><span class="koboSpan" id="kobo.826.1" xmlns="http://www.w3.org/1999/xhtml">equivalent of the size of a step is going to be a parameter </span><span class="koboSpan" id="kobo.827.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\tau" class="math inline" src="../media/file1160.png" style="vertical-align:middle" title="\tau"/></span><span class="koboSpan" id="kobo.828.1" xmlns="http://www.w3.org/1999/xhtml"> known as the </span><strong><span class="koboSpan" id="kobo.829.1" xmlns="http://www.w3.org/1999/xhtml">learning rate</span></strong><span class="koboSpan" id="kobo.830.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.830.2" xmlns="http://www.w3.org/1999/xhtml">And, in this way, given a learning rate </span><span class="koboSpan" id="kobo.831.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\tau" class="math inline" src="../media/file1160.png" style="vertical-align:middle" title="\tau"/></span><span class="koboSpan" id="kobo.832.1" xmlns="http://www.w3.org/1999/xhtml"> and an initial configuration </span><span class="koboSpan" id="kobo.833.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\theta_{0}" class="math inline" src="../media/file1045.png" style="vertical-align:middle" title="\theta_{0}"/></span><span class="koboSpan" id="kobo.834.1" xmlns="http://www.w3.org/1999/xhtml"> of the parameters of our model, we can try to find the parameters that minimize the loss function </span><span class="koboSpan" id="kobo.835.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="L" class="math inline" src="../media/file1012.png" style="vertical-align:middle" title="L"/></span><span class="koboSpan" id="kobo.836.1" xmlns="http://www.w3.org/1999/xhtml"> by computing, iteratively, new parameters according to the rule</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.837.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\theta_{k + 1} = \theta_{k} - \tau\nabla L(\theta_{k})." class="math display" src="../media/file1161.png" style="vertical-align:middle" title="\theta_{k + 1} = \theta_{k} - \tau\nabla L(\theta_{k})."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.838.1" xmlns="http://www.w3.org/1999/xhtml">There are some algorithms that dynamically adjust this step size from an initial learning rate as the optimization progresses. </span><span class="koboSpan" id="kobo.838.2" xmlns="http://www.w3.org/1999/xhtml">One such </span><span id="dx1-153005"/><span class="koboSpan" id="kobo.839.1" xmlns="http://www.w3.org/1999/xhtml">algorithm is </span><strong><span class="koboSpan" id="kobo.840.1" xmlns="http://www.w3.org/1999/xhtml">Adam</span></strong><span class="koboSpan" id="kobo.841.1" xmlns="http://www.w3.org/1999/xhtml"> (short for </span><strong><span class="koboSpan" id="kobo.842.1" xmlns="http://www.w3.org/1999/xhtml">Adaptive Moment Estimator</span></strong><span class="koboSpan" id="kobo.843.1" xmlns="http://www.w3.org/1999/xhtml">), which is one of the best gradient descents algorithms out there; it will actually be our go-to choice.</span></p>
<div class="tcolorbox important" id="tcolobox-158">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.844.1" xmlns="http://www.w3.org/1999/xhtml">Important note</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.845.1" xmlns="http://www.w3.org/1999/xhtml">It is important to pick the learning rate wisely. </span><span class="koboSpan" id="kobo.845.2" xmlns="http://www.w3.org/1999/xhtml">If it is too small, the training will be very slow. </span><span class="koboSpan" id="kobo.845.3" xmlns="http://www.w3.org/1999/xhtml">If it is too large, you may find yourself taking huge strides that jump whole valleys, and the training may never be successful.</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.846.1" xmlns="http://www.w3.org/1999/xhtml">Of course, in order for </span><span id="dx1-153006"/><span class="koboSpan" id="kobo.847.1" xmlns="http://www.w3.org/1999/xhtml">gradient descent algorithms to work, you need to be able to compute the gradient of the loss function. </span><span class="koboSpan" id="kobo.847.2" xmlns="http://www.w3.org/1999/xhtml">There are several ways to do this; for example, you could always estimate gradients numerically. </span><span class="koboSpan" id="kobo.847.3" xmlns="http://www.w3.org/1999/xhtml">But, when working with certain models such as neural networks, you can employ a </span><span id="dx1-153007"/><span class="koboSpan" id="kobo.848.1" xmlns="http://www.w3.org/1999/xhtml">technique known as </span><strong><span class="koboSpan" id="kobo.849.1" xmlns="http://www.w3.org/1999/xhtml">backpropagation</span></strong><span class="koboSpan" id="kobo.850.1" xmlns="http://www.w3.org/1999/xhtml">, which enables the efficient </span><span id="dx1-153008"/><span class="koboSpan" id="kobo.851.1" xmlns="http://www.w3.org/1999/xhtml">computation of exact gradients. </span><span class="koboSpan" id="kobo.851.2" xmlns="http://www.w3.org/1999/xhtml">You may learn more about the technical details in Geron’s exceptional book </span><span class="cite"><span class="koboSpan" id="kobo.852.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xhandsonml"><span class="koboSpan" id="kobo.853.1" xmlns="http://www.w3.org/1999/xhtml">104</span></a><span class="koboSpan" id="kobo.854.1" xmlns="http://www.w3.org/1999/xhtml">, Chapter 10]</span></span><span class="koboSpan" id="kobo.855.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<div class="tcolorbox learnmore" id="tcolobox-159">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.856.1" xmlns="http://www.w3.org/1999/xhtml">To learn more</span></span><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.857.1" xmlns="http://www.w3.org/1999/xhtml">…</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.858.1" xmlns="http://www.w3.org/1999/xhtml">The method of backpropagation has been one of the key developments leading to the great success of deep learning that we are experiencing today. </span><span class="koboSpan" id="kobo.858.2" xmlns="http://www.w3.org/1999/xhtml">Although this technique was already known in the 1960s, it was popularized for training neural networks by the work of Geoffrey Hinton and his collaborators. </span><span class="koboSpan" id="kobo.858.3" xmlns="http://www.w3.org/1999/xhtml">Hinton, together with Yoshua Bengio, Demis Hassabis, and Yann LeCun, received the 2022 Princess of Asturias Award for Technical and Scientific Research for outstanding work in the field of neural networks. </span><span class="koboSpan" id="kobo.858.4" xmlns="http://www.w3.org/1999/xhtml">You can learn a lot about the inception of backpropagation and about the history of neural networks research by reading the excellent </span><em><span class="koboSpan" id="kobo.859.1" xmlns="http://www.w3.org/1999/xhtml">Architects of Intelligence</span></em><span class="koboSpan" id="kobo.860.1" xmlns="http://www.w3.org/1999/xhtml">, in which Martin Ford interviews Bengio, Hassabis, Hinton, LeCun, and many other prominent figures in artificial intelligence </span><span class="cite"><span class="koboSpan" id="kobo.861.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xford2018architects"><span class="koboSpan" id="kobo.862.1" xmlns="http://www.w3.org/1999/xhtml">40</span></a><span class="koboSpan" id="kobo.863.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.864.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.864.2" xmlns="http://www.w3.org/1999/xhtml">By the way, Demis Hassabis is, in great part, responsible for the success of AlphaGo, one of the examples of reinforcement learning that we mentioned earlier in this chapter.</span></p>
</div>
</div>
<p><span class="paragraphHead"><span id="x1-1540008.2.3"/><strong><span class="koboSpan" id="kobo.865.1" xmlns="http://www.w3.org/1999/xhtml">Mini-batch gradient descent</span></strong></span><span class="koboSpan" id="kobo.866.1" xmlns="http://www.w3.org/1999/xhtml"> When the </span><span id="dx1-154001"/><span class="koboSpan" id="kobo.867.1" xmlns="http://www.w3.org/1999/xhtml">training dataset is large, computing the </span><span id="dx1-154002"/><span class="koboSpan" id="kobo.868.1" xmlns="http://www.w3.org/1999/xhtml">gradient of the loss function — as a function of the optimizable parameters of the model — can slow down the training significantly. </span><span class="koboSpan" id="kobo.868.2" xmlns="http://www.w3.org/1999/xhtml">In order to speed up the training, you can resort to the technique of </span><strong><span class="koboSpan" id="kobo.869.1" xmlns="http://www.w3.org/1999/xhtml">mini-batch gradient descent</span></strong><span class="koboSpan" id="kobo.870.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.870.2" xmlns="http://www.w3.org/1999/xhtml">With this optimization method, the training dataset is split into batches of a fixed </span><strong><span class="koboSpan" id="kobo.871.1" xmlns="http://www.w3.org/1999/xhtml">batch size</span></strong><span class="koboSpan" id="kobo.872.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.872.2" xmlns="http://www.w3.org/1999/xhtml">The </span><span id="dx1-154003"/><span class="koboSpan" id="kobo.873.1" xmlns="http://www.w3.org/1999/xhtml">gradient of the loss function is then computed on each of these batches, and the results are used to approximate the gradient of the global loss function: this is, the loss function on the whole training dataset. </span><span class="koboSpan" id="kobo.873.2" xmlns="http://www.w3.org/1999/xhtml">When we use this technique, we need to be careful with the batch size that we use: make it too small, the training will be very unstable; make it too large, the training will be too slow. </span><span class="koboSpan" id="kobo.873.3" xmlns="http://www.w3.org/1999/xhtml">As with the learning rate, it’s all a matter of finding an equilibrium! </span><span class="koboSpan" id="kobo.873.4" xmlns="http://www.w3.org/1999/xhtml">However, in some cases, speed is of the essence, and we go to the extreme, using </span><span id="dx1-154004"/><span class="koboSpan" id="kobo.874.1" xmlns="http://www.w3.org/1999/xhtml">batches of just one input. </span><span class="koboSpan" id="kobo.874.2" xmlns="http://www.w3.org/1999/xhtml">This is called </span><strong><span class="koboSpan" id="kobo.875.1" xmlns="http://www.w3.org/1999/xhtml">stochastic gradient descent</span></strong><span class="koboSpan" id="kobo.876.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.876.2" xmlns="http://www.w3.org/1999/xhtml">On the other hand, when the batch includes all the elements in the dataset, we say that we </span><span id="dx1-154005"/><span class="koboSpan" id="kobo.877.1" xmlns="http://www.w3.org/1999/xhtml">are using </span><strong><span class="koboSpan" id="kobo.878.1" xmlns="http://www.w3.org/1999/xhtml">batch gradient</span></strong> <strong><span class="koboSpan" id="kobo.879.1" xmlns="http://www.w3.org/1999/xhtml">descent</span></strong><span class="koboSpan" id="kobo.880.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<p><span class="koboSpan" id="kobo.881.1" xmlns="http://www.w3.org/1999/xhtml">Now we do have all that we need to train our first model. </span><span class="koboSpan" id="kobo.881.2" xmlns="http://www.w3.org/1999/xhtml">We have a dataset, we know what our model should look like, we have picked a loss function and we know how to optimize it. </span><span class="koboSpan" id="kobo.881.3" xmlns="http://www.w3.org/1999/xhtml">So let’s make this work! </span><span class="koboSpan" id="kobo.881.4" xmlns="http://www.w3.org/1999/xhtml">For this, we will use TensorFlow and scikit-learn.</span></p>
</section>
<section class="level3 subsectionHead" data-number="16.2.4" id="getting-in-the-tensorflow">
<h2 class="subsectionHead" data-number="16.2.4"><span class="titlemark"><span class="koboSpan" id="kobo.882.1" xmlns="http://www.w3.org/1999/xhtml">8.2.4 </span></span> <span id="x1-1550008.2.4"><span class="koboSpan" id="kobo.883.1" xmlns="http://www.w3.org/1999/xhtml">Getting in the (Tensor)Flow</span></span></h2>
<p><span class="koboSpan" id="kobo.884.1" xmlns="http://www.w3.org/1999/xhtml">We already have our </span><span id="dx1-155001"/><span class="koboSpan" id="kobo.885.1" xmlns="http://www.w3.org/1999/xhtml">dataset ready, and we could split it manually into training, validation, and test datasets, but there are already some good-quality machine learning packages with functions that help you do that. </span><span class="koboSpan" id="kobo.885.2" xmlns="http://www.w3.org/1999/xhtml">One of these packages is </span><code><span class="koboSpan" id="kobo.886.1" xmlns="http://www.w3.org/1999/xhtml">sklearn</span></code><span class="koboSpan" id="kobo.887.1" xmlns="http://www.w3.org/1999/xhtml">, which implements a </span><code><span class="koboSpan" id="kobo.888.1" xmlns="http://www.w3.org/1999/xhtml">train_test_split</span></code><span class="koboSpan" id="kobo.889.1" xmlns="http://www.w3.org/1999/xhtml"> function. </span><span class="koboSpan" id="kobo.889.2" xmlns="http://www.w3.org/1999/xhtml">It splits a dataset into a training and test dataset (it doesn’t return a validation dataset, but we can work our way around that). </span><span class="koboSpan" id="kobo.889.3" xmlns="http://www.w3.org/1999/xhtml">It does so by taking as arguments the dataset and the labels array; in addition, it has some optional arguments to specify whether the dataset should be shuffled and the proportions in which the dataset should be split. </span><span class="koboSpan" id="kobo.889.4" xmlns="http://www.w3.org/1999/xhtml">In order to get a training, validation, and test dataset with proportions </span><span class="koboSpan" id="kobo.890.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.8" class="math inline" src="../media/file1162.png" style="vertical-align:middle" title="0.8"/></span><span class="koboSpan" id="kobo.891.1" xmlns="http://www.w3.org/1999/xhtml">, </span><span class="koboSpan" id="kobo.892.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.1" class="math inline" src="../media/file1163.png" style="vertical-align:middle" title="0.1"/></span><span class="koboSpan" id="kobo.893.1" xmlns="http://www.w3.org/1999/xhtml">, and </span><span class="koboSpan" id="kobo.894.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.1" class="math inline" src="../media/file1163.png" style="vertical-align:middle" title="0.1"/></span><span class="koboSpan" id="kobo.895.1" xmlns="http://www.w3.org/1999/xhtml"> respectively, we just need to use this function twice: once to get a training dataset (size </span><span class="koboSpan" id="kobo.896.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.8" class="math inline" src="../media/file1162.png" style="vertical-align:middle" title="0.8"/></span><span class="koboSpan" id="kobo.897.1" xmlns="http://www.w3.org/1999/xhtml">) and a test dataset (size </span><span class="koboSpan" id="kobo.898.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.2" class="math inline" src="../media/file1091.png" style="vertical-align:middle" title="0.2"/></span><span class="koboSpan" id="kobo.899.1" xmlns="http://www.w3.org/1999/xhtml">), and once more to split the test dataset in half, yielding a validation dataset and a test dataset of relative size </span><span class="koboSpan" id="kobo.900.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.1" class="math inline" src="../media/file1163.png" style="vertical-align:middle" title="0.1"/></span><span class="koboSpan" id="kobo.901.1" xmlns="http://www.w3.org/1999/xhtml"> each.</span></p>
<p><span class="koboSpan" id="kobo.902.1" xmlns="http://www.w3.org/1999/xhtml">Following convention, we will denote the datasets as variables </span><code><span class="koboSpan" id="kobo.903.1" xmlns="http://www.w3.org/1999/xhtml">x</span></code><span class="koboSpan" id="kobo.904.1" xmlns="http://www.w3.org/1999/xhtml"> and the labels as variables </span><code><span class="koboSpan" id="kobo.905.1" xmlns="http://www.w3.org/1999/xhtml">y</span></code><span class="koboSpan" id="kobo.906.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.906.2" xmlns="http://www.w3.org/1999/xhtml">In this way, we can run the following:</span></p>
<pre class="lstlisting" id="listing-196"><span class="koboSpan" id="kobo.907.1" xmlns="http://www.w3.org/1999/xhtml">

from sklearn.model_selection import train_test_split 
 
# Split into a training and a test dataset. 
 
</span><span class="koboSpan" id="kobo.907.2" xmlns="http://www.w3.org/1999/xhtml">x_tr, x_test, y_tr, y_test = train_test_split( 
 
    data, labels, shuffle = True, train_size = 0.8) 
 
# Split the test dataset to get a validation one. 
 
</span><span class="koboSpan" id="kobo.907.3" xmlns="http://www.w3.org/1999/xhtml">x_val, x_test, y_val, y_test = train_test_split( 
 
    x_test, y_test, shuffle = True, train_size = 0.5)
</span></pre>
<p><span class="koboSpan" id="kobo.908.1" xmlns="http://www.w3.org/1999/xhtml">Notice how the function returns four arrays in the following order: the data for the training dataset, the data for the test dataset, the labels for the training dataset, and the labels for the test dataset. </span><span class="koboSpan" id="kobo.908.2" xmlns="http://www.w3.org/1999/xhtml">One important thing about the </span><code><span class="koboSpan" id="kobo.909.1" xmlns="http://www.w3.org/1999/xhtml">train_test_split</span></code><span class="koboSpan" id="kobo.910.1" xmlns="http://www.w3.org/1999/xhtml"> function is that it can use </span><strong><span class="koboSpan" id="kobo.911.1" xmlns="http://www.w3.org/1999/xhtml">stratification</span></strong><span class="koboSpan" id="kobo.912.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.912.2" xmlns="http://www.w3.org/1999/xhtml">If we had also </span><span id="dx1-155009"/><span class="koboSpan" id="kobo.913.1" xmlns="http://www.w3.org/1999/xhtml">provided the arguments </span><span class="lstinline"><span style="color:#000000"><code><span class="koboSpan" id="kobo.914.1" xmlns="http://www.w3.org/1999/xhtml">stratify</span></code></span><span style="color:#000000"> </span><span style="color:#000000"><code><span class="koboSpan" id="kobo.915.1" xmlns="http://www.w3.org/1999/xhtml">=</span></code></span><span style="color:#000000"> </span><span style="color:#000000"><code><span class="koboSpan" id="kobo.916.1" xmlns="http://www.w3.org/1999/xhtml">labels</span></code></span></span><span class="koboSpan" id="kobo.917.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="lstinline"><span style="color:#000000"><code><span class="koboSpan" id="kobo.918.1" xmlns="http://www.w3.org/1999/xhtml">stratify</span></code></span><span style="color:#000000"> </span><span style="color:#000000"><code><span class="koboSpan" id="kobo.919.1" xmlns="http://www.w3.org/1999/xhtml">=</span></code></span><span style="color:#000000"> </span><span style="color:#000000"><code><span class="koboSpan" id="kobo.920.1" xmlns="http://www.w3.org/1999/xhtml">y_test</span></code></span></span><span class="koboSpan" id="kobo.921.1" xmlns="http://www.w3.org/1999/xhtml">, this would have meant that, when splitting the data into training and test examples, it would have kept the exact proportion of positive and negative classes from the original data (or at least as close to exact as possible). </span><span class="koboSpan" id="kobo.921.2" xmlns="http://www.w3.org/1999/xhtml">This can be important, especially if we are working with unbalanced datasets in which one class is much more abundant than the other. </span><span class="koboSpan" id="kobo.921.3" xmlns="http://www.w3.org/1999/xhtml">If we are not careful, we could end up with a dataset in which the minority class is non-existent.</span></p>
<p><span class="koboSpan" id="kobo.922.1" xmlns="http://www.w3.org/1999/xhtml">Now that the data is perfectly prepared, it is time for us to focus on the model. </span><span class="koboSpan" id="kobo.922.2" xmlns="http://www.w3.org/1999/xhtml">For our problem, we are going to use a neural network with the following components:</span></p>
<ul>
<li><p><span class="koboSpan" id="kobo.923.1" xmlns="http://www.w3.org/1999/xhtml">An input layer with two inputs</span></p></li>
<li><p><span class="koboSpan" id="kobo.924.1" xmlns="http://www.w3.org/1999/xhtml">Three intermediate (also known as </span><strong><span class="koboSpan" id="kobo.925.1" xmlns="http://www.w3.org/1999/xhtml">hidden</span></strong><span class="koboSpan" id="kobo.926.1" xmlns="http://www.w3.org/1999/xhtml">) layers with ELU activation functions and with </span><span class="koboSpan" id="kobo.927.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="8" class="math inline" src="../media/file506.png" style="vertical-align:middle" title="8"/></span><span class="koboSpan" id="kobo.928.1" xmlns="http://www.w3.org/1999/xhtml">, </span><span class="koboSpan" id="kobo.929.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="16" class="math inline" src="../media/file619.png" style="vertical-align:middle" title="16"/></span><span class="koboSpan" id="kobo.930.1" xmlns="http://www.w3.org/1999/xhtml">, and </span><span class="koboSpan" id="kobo.931.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="8" class="math inline" src="../media/file506.png" style="vertical-align:middle" title="8"/></span><span class="koboSpan" id="kobo.932.1" xmlns="http://www.w3.org/1999/xhtml"> neurons respectively</span></p></li>
<li><p><span class="koboSpan" id="kobo.933.1" xmlns="http://www.w3.org/1999/xhtml">An output layer with a single neuron that uses the sigmoid activation function</span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.934.1" xmlns="http://www.w3.org/1999/xhtml">Let’s now try to digest this specification a little bit. </span><span class="koboSpan" id="kobo.934.2" xmlns="http://www.w3.org/1999/xhtml">Because of the nature of the problem, we know that our model needs two inputs and one output, hence the sizes of the input and output layers. </span><span class="koboSpan" id="kobo.934.3" xmlns="http://www.w3.org/1999/xhtml">What is more, we want to get an output normalized between </span><span class="koboSpan" id="kobo.935.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.936.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.937.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.938.1" xmlns="http://www.w3.org/1999/xhtml">, so it makes sense to use the sigmoid activation function in the output layer. </span><span class="koboSpan" id="kobo.938.2" xmlns="http://www.w3.org/1999/xhtml">Now, we need to find a way to get from </span><span class="koboSpan" id="kobo.939.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="2" class="math inline" src="../media/file302.png" style="vertical-align:middle" title="2"/></span><span class="koboSpan" id="kobo.940.1" xmlns="http://www.w3.org/1999/xhtml"> neurons in the first layer to </span><span class="koboSpan" id="kobo.941.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.942.1" xmlns="http://www.w3.org/1999/xhtml"> neuron in the output layer. </span><span class="koboSpan" id="kobo.942.2" xmlns="http://www.w3.org/1999/xhtml">We could use hidden layers with </span><span class="koboSpan" id="kobo.943.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="2" class="math inline" src="../media/file302.png" style="vertical-align:middle" title="2"/></span><span class="koboSpan" id="kobo.944.1" xmlns="http://www.w3.org/1999/xhtml"> or </span><span class="koboSpan" id="kobo.945.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.946.1" xmlns="http://www.w3.org/1999/xhtml"> layers…but that wouldn’t yield a very powerful neural network. </span><span class="koboSpan" id="kobo.946.2" xmlns="http://www.w3.org/1999/xhtml">Thus, we have progressively scaled the size of the neural network: first going from </span><span class="koboSpan" id="kobo.947.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="2" class="math inline" src="../media/file302.png" style="vertical-align:middle" title="2"/></span><span class="koboSpan" id="kobo.948.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><span class="koboSpan" id="kobo.949.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="8" class="math inline" src="../media/file506.png" style="vertical-align:middle" title="8"/></span><span class="koboSpan" id="kobo.950.1" xmlns="http://www.w3.org/1999/xhtml">, then from </span><span class="koboSpan" id="kobo.951.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="8" class="math inline" src="../media/file506.png" style="vertical-align:middle" title="8"/></span><span class="koboSpan" id="kobo.952.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><span class="koboSpan" id="kobo.953.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="16" class="math inline" src="../media/file619.png" style="vertical-align:middle" title="16"/></span><span class="koboSpan" id="kobo.954.1" xmlns="http://www.w3.org/1999/xhtml">, then down from </span><span class="koboSpan" id="kobo.955.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="16" class="math inline" src="../media/file619.png" style="vertical-align:middle" title="16"/></span><span class="koboSpan" id="kobo.956.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><span class="koboSpan" id="kobo.957.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="8" class="math inline" src="../media/file506.png" style="vertical-align:middle" title="8"/></span><span class="koboSpan" id="kobo.958.1" xmlns="http://www.w3.org/1999/xhtml">, to finally reach the output layer with 1 neuron.</span></p>
<p><span class="koboSpan" id="kobo.959.1" xmlns="http://www.w3.org/1999/xhtml">How do we define such a model in TensorFlow? </span><span class="koboSpan" id="kobo.959.2" xmlns="http://www.w3.org/1999/xhtml">Well, after doing the necessary imports and setting a seed (remember that it is an important part if we want this to be reproducible!), all it takes is to </span><span id="dx1-155010"/><span class="koboSpan" id="kobo.960.1" xmlns="http://www.w3.org/1999/xhtml">define what is </span><span id="dx1-155011"/><span class="koboSpan" id="kobo.961.1" xmlns="http://www.w3.org/1999/xhtml">known as a </span><strong><span class="koboSpan" id="kobo.962.1" xmlns="http://www.w3.org/1999/xhtml">Keras</span></strong> <strong><span class="koboSpan" id="kobo.963.1" xmlns="http://www.w3.org/1999/xhtml">sequential model</span></strong><span class="koboSpan" id="kobo.964.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<p><span class="koboSpan" id="kobo.965.1" xmlns="http://www.w3.org/1999/xhtml">The code is pretty self-explanatory:</span></p>
<pre class="lstlisting" id="listing-197"><span class="koboSpan" id="kobo.966.1" xmlns="http://www.w3.org/1999/xhtml">

import tensorflow as tf 
 
tf.random.set_seed(seed) 
 
 
 
model = tf.keras.Sequential([ 
 
    tf.keras.layers.Input(2), 
 
    tf.keras.layers.Dense(8, activation = "elu"), 
 
    tf.keras.layers.Dense(16, activation = "elu"), 
 
    tf.keras.layers.Dense(8, activation = "elu"), 
 
    tf.keras.layers.Dense(1, activation = "sigmoid"), 
 
])
</span></pre>
<p><span class="koboSpan" id="kobo.967.1" xmlns="http://www.w3.org/1999/xhtml">And that is how we can create our model, storing it as an object of the </span><code><span class="koboSpan" id="kobo.968.1" xmlns="http://www.w3.org/1999/xhtml">Sequential</span></code><span class="koboSpan" id="kobo.969.1" xmlns="http://www.w3.org/1999/xhtml"> class.</span></p>
<div class="tcolorbox learnmore" id="tcolobox-160">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.970.1" xmlns="http://www.w3.org/1999/xhtml">To learn more</span></span><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.971.1" xmlns="http://www.w3.org/1999/xhtml">…</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.972.1" xmlns="http://www.w3.org/1999/xhtml">Once you have defined a Keras </span><code><span class="koboSpan" id="kobo.973.1" xmlns="http://www.w3.org/1999/xhtml">model</span></code><span class="koboSpan" id="kobo.974.1" xmlns="http://www.w3.org/1999/xhtml">, like the sequential model that we have just considered, you can print a visual summary of it by running the instruction </span><span class="lstinline"><span style="color:#A71D5D"><code><span class="koboSpan" id="kobo.975.1" xmlns="http://www.w3.org/1999/xhtml">print</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.976.1" xmlns="http://www.w3.org/1999/xhtml">(</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.977.1" xmlns="http://www.w3.org/1999/xhtml">model</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.978.1" xmlns="http://www.w3.org/1999/xhtml">.</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.979.1" xmlns="http://www.w3.org/1999/xhtml">summary</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.980.1" xmlns="http://www.w3.org/1999/xhtml">())</span></code></span></span><span class="koboSpan" id="kobo.981.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.981.2" xmlns="http://www.w3.org/1999/xhtml">This summary lists all the layers of the model together with their shape, and also displays a count of all the model parameters.</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.982.1" xmlns="http://www.w3.org/1999/xhtml">Before we can train this model, we will need to </span><strong><span class="koboSpan" id="kobo.983.1" xmlns="http://www.w3.org/1999/xhtml">compile it</span></strong><span class="koboSpan" id="kobo.984.1" xmlns="http://www.w3.org/1999/xhtml">, associating it with an optimization algorithm and a loss function. </span><span class="koboSpan" id="kobo.984.2" xmlns="http://www.w3.org/1999/xhtml">This is done by calling the </span><span class="lstinline"><span style="color:#A71D5D"><code><span class="koboSpan" id="kobo.985.1" xmlns="http://www.w3.org/1999/xhtml">compile</span></code></span></span><span class="koboSpan" id="kobo.986.1" xmlns="http://www.w3.org/1999/xhtml"> method and giving it the arguments </span><code><span class="koboSpan" id="kobo.987.1" xmlns="http://www.w3.org/1999/xhtml">optimizer</span></code><span class="koboSpan" id="kobo.988.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><code><span class="koboSpan" id="kobo.989.1" xmlns="http://www.w3.org/1999/xhtml">loss</span></code><span class="koboSpan" id="kobo.990.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.990.2" xmlns="http://www.w3.org/1999/xhtml">In our case, we seek to use the Adam optimizer (just with its default parameters) and the binary cross entropy loss function. </span><span class="koboSpan" id="kobo.990.3" xmlns="http://www.w3.org/1999/xhtml">We can thus compile our model as follows:</span></p>
<pre class="lstlisting" id="listing-198"><span class="koboSpan" id="kobo.991.1" xmlns="http://www.w3.org/1999/xhtml">

opt = tf.keras.optimizers.Adam() 
 
lossf = tf.keras.losses.BinaryCrossentropy() 
 
model.compile(optimizer = opt, loss = lossf)
</span></pre>
<p><span class="koboSpan" id="kobo.992.1" xmlns="http://www.w3.org/1999/xhtml">When we instantiate the Adam optimizer without providing any arguments, the learning rate is set, by default, to </span><span class="koboSpan" id="kobo.993.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="10^{- 3}" class="math inline" src="../media/file1164.png" style="vertical-align:middle" title="10^{- 3}"/></span><span class="koboSpan" id="kobo.994.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.994.2" xmlns="http://www.w3.org/1999/xhtml">We may change this value — and we will very often do! </span><span class="koboSpan" id="kobo.994.3" xmlns="http://www.w3.org/1999/xhtml">— by setting a value for the optional argument </span><code><span class="koboSpan" id="kobo.995.1" xmlns="http://www.w3.org/1999/xhtml">learning_rate</span></code><span class="koboSpan" id="kobo.996.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
</section>
<section class="level3 subsectionHead" data-number="16.2.5" id="training-the-model">
<h2 class="subsectionHead" data-number="16.2.5"><span class="titlemark"><span class="koboSpan" id="kobo.997.1" xmlns="http://www.w3.org/1999/xhtml">8.2.5 </span></span> <span id="x1-1560008.2.5"><span class="koboSpan" id="kobo.998.1" xmlns="http://www.w3.org/1999/xhtml">Training the model</span></span></h2>
<p><span class="koboSpan" id="kobo.999.1" xmlns="http://www.w3.org/1999/xhtml">Now we are ready to </span><span id="dx1-156001"/><span class="koboSpan" id="kobo.1000.1" xmlns="http://www.w3.org/1999/xhtml">train our model. </span><span class="koboSpan" id="kobo.1000.2" xmlns="http://www.w3.org/1999/xhtml">This will be done by calling the </span><code><span class="koboSpan" id="kobo.1001.1" xmlns="http://www.w3.org/1999/xhtml">fit</span></code><span class="koboSpan" id="kobo.1002.1" xmlns="http://www.w3.org/1999/xhtml"> method. </span><span class="koboSpan" id="kobo.1002.2" xmlns="http://www.w3.org/1999/xhtml">But before we do that, let’s explore in some detail the most important arguments that we have to and can pass to this method:</span></p>
<ul>
<li><p><span class="koboSpan" id="kobo.1003.1" xmlns="http://www.w3.org/1999/xhtml">The first argument that </span><code><span class="koboSpan" id="kobo.1004.1" xmlns="http://www.w3.org/1999/xhtml">fit</span></code><span class="koboSpan" id="kobo.1005.1" xmlns="http://www.w3.org/1999/xhtml"> admits is the dataset </span><code><span class="koboSpan" id="kobo.1006.1" xmlns="http://www.w3.org/1999/xhtml">x</span></code><span class="koboSpan" id="kobo.1007.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1007.2" xmlns="http://www.w3.org/1999/xhtml">It should be an array containing the inputs that need to be passed to the model in order to train it. </span><span class="koboSpan" id="kobo.1007.3" xmlns="http://www.w3.org/1999/xhtml">In our case, that would be </span><code><span class="koboSpan" id="kobo.1008.1" xmlns="http://www.w3.org/1999/xhtml">x_tr</span></code><span class="koboSpan" id="kobo.1009.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p></li>
<li><p><span class="koboSpan" id="kobo.1010.1" xmlns="http://www.w3.org/1999/xhtml">The second argument that we can send is the array of labels </span><code><span class="koboSpan" id="kobo.1011.1" xmlns="http://www.w3.org/1999/xhtml">y</span></code><span class="koboSpan" id="kobo.1012.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1012.2" xmlns="http://www.w3.org/1999/xhtml">Of course, the dimensions of </span><code><span class="koboSpan" id="kobo.1013.1" xmlns="http://www.w3.org/1999/xhtml">x</span></code><span class="koboSpan" id="kobo.1014.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><code><span class="koboSpan" id="kobo.1015.1" xmlns="http://www.w3.org/1999/xhtml">y</span></code><span class="koboSpan" id="kobo.1016.1" xmlns="http://www.w3.org/1999/xhtml"> need to match. </span><span class="koboSpan" id="kobo.1016.2" xmlns="http://www.w3.org/1999/xhtml">In our case, we will set </span><code><span class="koboSpan" id="kobo.1017.1" xmlns="http://www.w3.org/1999/xhtml">y</span></code><span class="koboSpan" id="kobo.1018.1" xmlns="http://www.w3.org/1999/xhtml"> to be </span><code><span class="koboSpan" id="kobo.1019.1" xmlns="http://www.w3.org/1999/xhtml">y_tr</span></code><span class="koboSpan" id="kobo.1020.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p></li>
<li><p><span class="koboSpan" id="kobo.1021.1" xmlns="http://www.w3.org/1999/xhtml">If you are using an optimizer that relies on gradient descent, you may want to resort to mini-batch gradient descent. </span><span class="koboSpan" id="kobo.1021.2" xmlns="http://www.w3.org/1999/xhtml">For this purpose, you can give an integer value to the </span><code><span class="koboSpan" id="kobo.1022.1" xmlns="http://www.w3.org/1999/xhtml">batch_size</span></code><span class="koboSpan" id="kobo.1023.1" xmlns="http://www.w3.org/1999/xhtml"> argument, which defaults to </span><span class="koboSpan" id="kobo.1024.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="32" class="math inline" src="../media/file771.png" style="vertical-align:middle" title="32"/></span><span class="koboSpan" id="kobo.1025.1" xmlns="http://www.w3.org/1999/xhtml"> (thus, by default, mini-batch gradient descent is used). </span><span class="koboSpan" id="kobo.1025.2" xmlns="http://www.w3.org/1999/xhtml">If you do not want to use mini-batch gradient descent, you should set </span><code><span class="koboSpan" id="kobo.1026.1" xmlns="http://www.w3.org/1999/xhtml">batch_size</span></code><span class="koboSpan" id="kobo.1027.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><code><span class="koboSpan" id="kobo.1028.1" xmlns="http://www.w3.org/1999/xhtml">None</span></code><span class="koboSpan" id="kobo.1029.1" xmlns="http://www.w3.org/1999/xhtml">; that is what we will do.</span></p></li>
<li><p><span class="koboSpan" id="kobo.1030.1" xmlns="http://www.w3.org/1999/xhtml">When we discussed gradient descent, we saw how these gradient descent algorithms are </span><strong><span class="koboSpan" id="kobo.1031.1" xmlns="http://www.w3.org/1999/xhtml">iterative</span></strong><span class="koboSpan" id="kobo.1032.1" xmlns="http://www.w3.org/1999/xhtml">: they work by computing a sequence of points that, in principle, should converge to a (local) minimum. </span><span class="koboSpan" id="kobo.1032.2" xmlns="http://www.w3.org/1999/xhtml">But this raises the question of how many optimization cycles the algorithm should make — how many such points in the sequence it should compute. </span><span class="koboSpan" id="kobo.1032.3" xmlns="http://www.w3.org/1999/xhtml">You may fix how many steps, also </span><span id="dx1-156002"/><span class="koboSpan" id="kobo.1033.1" xmlns="http://www.w3.org/1999/xhtml">known as </span><strong><span class="koboSpan" id="kobo.1034.1" xmlns="http://www.w3.org/1999/xhtml">epochs</span></strong><span class="koboSpan" id="kobo.1035.1" xmlns="http://www.w3.org/1999/xhtml">, you want the optimization algorithm to take. </span><span class="koboSpan" id="kobo.1035.2" xmlns="http://www.w3.org/1999/xhtml">This is done by setting a value for the </span><code><span class="koboSpan" id="kobo.1036.1" xmlns="http://www.w3.org/1999/xhtml">epochs</span></code><span class="koboSpan" id="kobo.1037.1" xmlns="http://www.w3.org/1999/xhtml"> argument, which defaults to </span><span class="koboSpan" id="kobo.1038.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.1039.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1039.2" xmlns="http://www.w3.org/1999/xhtml">In our case, we will use </span><span class="koboSpan" id="kobo.1040.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="8" class="math inline" src="../media/file506.png" style="vertical-align:middle" title="8"/></span><span class="koboSpan" id="kobo.1041.1" xmlns="http://www.w3.org/1999/xhtml"> epochs.</span></p></li>
<li><span class="koboSpan" id="kobo.1042.1" xmlns="http://www.w3.org/1999/xhtml">If we want to use some validation data, as it is our case, we can pass it through the </span><code><span class="koboSpan" id="kobo.1043.1" xmlns="http://www.w3.org/1999/xhtml">validation_data</span></code><span class="koboSpan" id="kobo.1044.1" xmlns="http://www.w3.org/1999/xhtml"> argument. </span><span class="koboSpan" id="kobo.1044.2" xmlns="http://www.w3.org/1999/xhtml">The value of this argument should be a tuple with the validation dataset in the first entry and the corresponding labels in the second one. </span><span class="koboSpan" id="kobo.1044.3" xmlns="http://www.w3.org/1999/xhtml">Thus, in our case, we would set </span><code><span class="koboSpan" id="kobo.1045.1" xmlns="http://www.w3.org/1999/xhtml">validation_data</span></code><span class="koboSpan" id="kobo.1046.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><code><span class="koboSpan" id="kobo.1047.1" xmlns="http://www.w3.org/1999/xhtml">(</span></code><span style="color:#000000"><code><span class="koboSpan" id="kobo.1048.1" xmlns="http://www.w3.org/1999/xhtml">x_val</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.1049.1" xmlns="http://www.w3.org/1999/xhtml">,</span></code></span><span style="color:#000000"> </span><span style="color:#000000"><code><span class="koboSpan" id="kobo.1050.1" xmlns="http://www.w3.org/1999/xhtml">y_val</span></code></span>
<code><span class="koboSpan" id="kobo.1051.1" xmlns="http://www.w3.org/1999/xhtml">)</span></code><span class="koboSpan" id="kobo.1052.1" xmlns="http://www.w3.org/1999/xhtml">.</span></li>
<li><p><span class="koboSpan" id="kobo.1053.1" xmlns="http://www.w3.org/1999/xhtml">You may have </span><span id="dx1-156003"/><span class="koboSpan" id="kobo.1054.1" xmlns="http://www.w3.org/1999/xhtml">noticed that the whole process of extracting a training, validation, and test dataset can be somewhat tiresome. </span><span class="koboSpan" id="kobo.1054.2" xmlns="http://www.w3.org/1999/xhtml">Well, it turns out that TensorFlow can help out here. </span><span class="koboSpan" id="kobo.1054.3" xmlns="http://www.w3.org/1999/xhtml">In principle, we could just have given TensorFlow a dataset with both the training and validation data and told it in which proportions they should be split by setting a value in the </span><code><span class="koboSpan" id="kobo.1055.1" xmlns="http://www.w3.org/1999/xhtml">validation_split</span></code><span class="koboSpan" id="kobo.1056.1" xmlns="http://www.w3.org/1999/xhtml"> argument. </span><span class="koboSpan" id="kobo.1056.2" xmlns="http://www.w3.org/1999/xhtml">This value must be a float between </span><span class="koboSpan" id="kobo.1057.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.1058.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.1059.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.1060.1" xmlns="http://www.w3.org/1999/xhtml"> representing the proportion of the training dataset that should be used for validation.</span></p>
<p><span class="koboSpan" id="kobo.1061.1" xmlns="http://www.w3.org/1999/xhtml">By doing this, we would save ourselves a ”split”, but we would still have to extract a test dataset on our own.</span></p></li>
</ul>
<div class="tcolorbox learnmore" id="tcolobox-161">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.1062.1" xmlns="http://www.w3.org/1999/xhtml">To learn more</span></span><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.1063.1" xmlns="http://www.w3.org/1999/xhtml">…</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.1064.1" xmlns="http://www.w3.org/1999/xhtml">We have only covered some of the possibilities offered by TensorFlow — the ones that we will use most often. </span><span class="koboSpan" id="kobo.1064.2" xmlns="http://www.w3.org/1999/xhtml">If you feel comfortable enough with the material that we have seen so far and want to explore TensorFlow in depth, you should check out the documentation (</span><a class="url" href="https://www.tensorflow.org/api_docs/python/tf"><span class="koboSpan" id="kobo.1065.1" xmlns="http://www.w3.org/1999/xhtml">https://www.tensorflow.org/api_docs/python/tf</span></a><span class="koboSpan" id="kobo.1066.1" xmlns="http://www.w3.org/1999/xhtml">).</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.1067.1" xmlns="http://www.w3.org/1999/xhtml">The way we will then train our model will be the following:</span></p>
<pre class="lstlisting" id="listing-199"><span class="koboSpan" id="kobo.1068.1" xmlns="http://www.w3.org/1999/xhtml">

history = model.fit(x_tr, y_tr, 
 
    validation_data = (x_val, y_val), epochs = 8, 
 
    batch_size = None)
</span></pre>
<p><span class="koboSpan" id="kobo.1069.1" xmlns="http://www.w3.org/1999/xhtml">And, upon executing this instruction on an interactive shell, we will get the following output:</span></p>
<pre class="listings"><span class="koboSpan" id="kobo.1070.1" xmlns="http://www.w3.org/1999/xhtml">

 
Epoch 1/8 
63/63 [====================] - 1s 3ms/step - loss: 0.6748 
- val_loss: 0.4859 
Epoch 2/8 
63/63 [====================] - 0s 1ms/step - loss: 0.4144 
- val_loss: 0.3095 
Epoch 3/8 
63/63 [====================] - 0s 1ms/step - loss: 0.3173 
- val_loss: 0.2502 
Epoch 4/8 
63/63 [====================] - 0s 1ms/step - loss: 0.2908 
- val_loss: 0.2315 
Epoch 5/8 
63/63 [====================] - 0s 1ms/step - loss: 0.2830 
- val_loss: 0.2262 
Epoch 6/8 
63/63 [====================] - 0s 1ms/step - loss: 0.2793 
- val_loss: 0.2221 
Epoch 7/8 
63/63 [====================] - 0s 1ms/step - loss: 0.2765 
- val_loss: 0.2187 
Epoch 8/8 
63/63 [====================] - 0s 1ms/step - loss: 0.2744 
- val_loss: 0.2185
    
</span></pre>
<p><span class="koboSpan" id="kobo.1071.1" xmlns="http://www.w3.org/1999/xhtml">When seeing this, the first thing we </span><span id="dx1-156032"/><span class="koboSpan" id="kobo.1072.1" xmlns="http://www.w3.org/1999/xhtml">should do is comparing the training loss with the validation loss — just to stay away from overfitting! </span><span class="koboSpan" id="kobo.1072.2" xmlns="http://www.w3.org/1999/xhtml">In our case, we see that these two are close enough and have evolved following similar decreasing trends during the training. </span><span class="koboSpan" id="kobo.1072.3" xmlns="http://www.w3.org/1999/xhtml">That is indeed a good sign!</span></p>
<p><span class="koboSpan" id="kobo.1073.1" xmlns="http://www.w3.org/1999/xhtml">You may have noticed how we have saved the output of the </span><code><span class="koboSpan" id="kobo.1074.1" xmlns="http://www.w3.org/1999/xhtml">fit</span></code><span class="koboSpan" id="kobo.1075.1" xmlns="http://www.w3.org/1999/xhtml"> method in an object that we have called </span><code><span class="koboSpan" id="kobo.1076.1" xmlns="http://www.w3.org/1999/xhtml">history</span></code><span class="koboSpan" id="kobo.1077.1" xmlns="http://www.w3.org/1999/xhtml"> in which TensorFlow will store information about the training. </span><span class="koboSpan" id="kobo.1077.2" xmlns="http://www.w3.org/1999/xhtml">For example, the training and validation losses at the end of each epoch is recorded in a dictionary that we could access as </span><code><span class="koboSpan" id="kobo.1078.1" xmlns="http://www.w3.org/1999/xhtml">history</span></code><span style="color:#000000"><code><span class="koboSpan" id="kobo.1079.1" xmlns="http://www.w3.org/1999/xhtml">.</span></code></span><code><span class="koboSpan" id="kobo.1080.1" xmlns="http://www.w3.org/1999/xhtml">history</span></code><span class="koboSpan" id="kobo.1081.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<div class="tcolorbox questionx" id="tcolobox-162">
<span id="x1-156034x8.2.5"/>
<div class="tcolorbox-title">
<p><span class="koboSpan" id="kobo.1082.1" xmlns="http://www.w3.org/1999/xhtml">Exercise 8.5</span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.1083.1" xmlns="http://www.w3.org/1999/xhtml">Plot on a single graph the evolution of the training and validation losses through the epochs, relying on the information contained in the </span><code><span class="koboSpan" id="kobo.1084.1" xmlns="http://www.w3.org/1999/xhtml">history</span></code><span class="koboSpan" id="kobo.1085.1" xmlns="http://www.w3.org/1999/xhtml"> object.</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.1086.1" xmlns="http://www.w3.org/1999/xhtml">In this case, we have manually set the number of epochs to </span><span class="koboSpan" id="kobo.1087.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="8" class="math inline" src="../media/file506.png" style="vertical-align:middle" title="8"/></span><span class="koboSpan" id="kobo.1088.1" xmlns="http://www.w3.org/1999/xhtml">, but this is not always the best strategy. </span><span class="koboSpan" id="kobo.1088.2" xmlns="http://www.w3.org/1999/xhtml">Ideally, we would like to fix a maximum number of epochs that is reasonably large, but we would want the training to stop as soon as the loss is not improving. </span><span class="koboSpan" id="kobo.1088.3" xmlns="http://www.w3.org/1999/xhtml">This is known as </span><strong><span class="koboSpan" id="kobo.1089.1" xmlns="http://www.w3.org/1999/xhtml">early stopping</span></strong><span class="koboSpan" id="kobo.1090.1" xmlns="http://www.w3.org/1999/xhtml">, and it can be easily used in TensorFlow.</span></p>
<p><span class="koboSpan" id="kobo.1091.1" xmlns="http://www.w3.org/1999/xhtml">In order to </span><span id="dx1-156035"/><span class="koboSpan" id="kobo.1092.1" xmlns="http://www.w3.org/1999/xhtml">use early </span><span id="dx1-156036"/><span class="koboSpan" id="kobo.1093.1" xmlns="http://www.w3.org/1999/xhtml">stopping in TensorFlow, we first need to create an </span><code><span class="koboSpan" id="kobo.1094.1" xmlns="http://www.w3.org/1999/xhtml">EarlyStopping</span></code><span class="koboSpan" id="kobo.1095.1" xmlns="http://www.w3.org/1999/xhtml"> object in which we specify how we want early stopping to behave. </span><span class="koboSpan" id="kobo.1095.2" xmlns="http://www.w3.org/1999/xhtml">Let’s say that we want to train our model until, for three consecutive epochs, the validation loss doesn’t decrease more than </span><span class="koboSpan" id="kobo.1096.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.001" class="math inline" src="../media/file1165.png" style="vertical-align:middle" title="0.001"/></span><span class="koboSpan" id="kobo.1097.1" xmlns="http://www.w3.org/1999/xhtml"> after each epoch. </span><span class="koboSpan" id="kobo.1097.2" xmlns="http://www.w3.org/1999/xhtml">To do this, we would have to invoke the following object:</span></p>
<pre class="lstlisting" id="listing-200"><span class="koboSpan" id="kobo.1098.1" xmlns="http://www.w3.org/1999/xhtml">

early_stp = tf.keras.callbacks.EarlyStopping( 
 
    monitor = "val_loss", patience = 3, min_delta = 0.001)
</span></pre>
<p><span class="koboSpan" id="kobo.1099.1" xmlns="http://www.w3.org/1999/xhtml">And then, when calling the </span><code><span class="koboSpan" id="kobo.1100.1" xmlns="http://www.w3.org/1999/xhtml">fit</span></code><span class="koboSpan" id="kobo.1101.1" xmlns="http://www.w3.org/1999/xhtml"> method, we would just have to pass the optional argument </span><span class="lstinline"><span style="color:#000000"><code><span class="koboSpan" id="kobo.1102.1" xmlns="http://www.w3.org/1999/xhtml">callbacks</span></code></span><span style="color:#000000"> </span><span style="color:#000000"><code><span class="koboSpan" id="kobo.1103.1" xmlns="http://www.w3.org/1999/xhtml">=</span></code></span><span style="color:#000000"> </span><span style="color:#000000"><code><span class="koboSpan" id="kobo.1104.1" xmlns="http://www.w3.org/1999/xhtml">[</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.1105.1" xmlns="http://www.w3.org/1999/xhtml">early_stp</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.1106.1" xmlns="http://www.w3.org/1999/xhtml">]</span></code></span></span><span class="koboSpan" id="kobo.1107.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1107.2" xmlns="http://www.w3.org/1999/xhtml">It’s as easy as that!</span></p>
<p><span class="koboSpan" id="kobo.1108.1" xmlns="http://www.w3.org/1999/xhtml">In any case, now we have trained our model. </span><span class="koboSpan" id="kobo.1108.2" xmlns="http://www.w3.org/1999/xhtml">If we want our model to process any inputs, we can use the </span><code><span class="koboSpan" id="kobo.1109.1" xmlns="http://www.w3.org/1999/xhtml">predict</span></code><span class="koboSpan" id="kobo.1110.1" xmlns="http://www.w3.org/1999/xhtml"> method, passing an array with any number of valid inputs. </span><span class="koboSpan" id="kobo.1110.2" xmlns="http://www.w3.org/1999/xhtml">For example, in our case, if we wanted to get the output of the model on the test dataset, we could retrieve </span><code><span class="koboSpan" id="kobo.1111.1" xmlns="http://www.w3.org/1999/xhtml">model</span></code><span style="color:#000000"><code><span class="koboSpan" id="kobo.1112.1" xmlns="http://www.w3.org/1999/xhtml">.</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.1113.1" xmlns="http://www.w3.org/1999/xhtml">predict</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.1114.1" xmlns="http://www.w3.org/1999/xhtml">(</span></code></span><span style="color:#000000"><code><span class="koboSpan" id="kobo.1115.1" xmlns="http://www.w3.org/1999/xhtml">x_test</span></code></span><code><span class="koboSpan" id="kobo.1116.1" xmlns="http://www.w3.org/1999/xhtml">)</span></code><span class="koboSpan" id="kobo.1117.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1117.2" xmlns="http://www.w3.org/1999/xhtml">However, this will give us the continuous values returned by the model (which will range from </span><span class="koboSpan" id="kobo.1118.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.1119.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><span class="koboSpan" id="kobo.1120.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.1121.1" xmlns="http://www.w3.org/1999/xhtml">), not a label! </span><span class="koboSpan" id="kobo.1121.2" xmlns="http://www.w3.org/1999/xhtml">In order to get a discrete label (</span><span class="koboSpan" id="kobo.1122.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.1123.1" xmlns="http://www.w3.org/1999/xhtml"> or </span><span class="koboSpan" id="kobo.1124.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.1125.1" xmlns="http://www.w3.org/1999/xhtml">), we need to set a threshold. </span><span class="koboSpan" id="kobo.1125.2" xmlns="http://www.w3.org/1999/xhtml">Naturally, we will set it to </span><span class="koboSpan" id="kobo.1126.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.5" class="math inline" src="../media/file1166.png" style="vertical-align:middle" title="0.5"/></span><span class="koboSpan" id="kobo.1127.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1127.2" xmlns="http://www.w3.org/1999/xhtml">Thus, if we want to get the labels that our model would predict, we would have to run the following:</span></p>
<pre class="lstlisting" id="listing-201"><span class="koboSpan" id="kobo.1128.1" xmlns="http://www.w3.org/1999/xhtml">

output = model.predict(x_test) 
 
result = (output &gt; 0.5).astype(float)
</span></pre>
<p><span class="koboSpan" id="kobo.1129.1" xmlns="http://www.w3.org/1999/xhtml">Of course, now we have to decide whether or not this training has been successful, so we should assess the performance of our model on the test dataset. </span><span class="koboSpan" id="kobo.1129.2" xmlns="http://www.w3.org/1999/xhtml">In order to do this, we may simply compute the </span><strong><span class="koboSpan" id="kobo.1130.1" xmlns="http://www.w3.org/1999/xhtml">accuracy</span></strong><span class="koboSpan" id="kobo.1131.1" xmlns="http://www.w3.org/1999/xhtml"> of our model on the test dataset, that is, we may compute the proportion of inputs in the test dataset that are correctly classified by our model.</span></p>
<p><span class="koboSpan" id="kobo.1132.1" xmlns="http://www.w3.org/1999/xhtml">In order to do this, we can use the </span><code><span class="koboSpan" id="kobo.1133.1" xmlns="http://www.w3.org/1999/xhtml">accuracy_score</span></code><span class="koboSpan" id="kobo.1134.1" xmlns="http://www.w3.org/1999/xhtml"> function from </span><code><span class="koboSpan" id="kobo.1135.1" xmlns="http://www.w3.org/1999/xhtml">sklearn</span></code><span style="color:#000000"><code><span class="koboSpan" id="kobo.1136.1" xmlns="http://www.w3.org/1999/xhtml">.</span></code></span><code><span class="koboSpan" id="kobo.1137.1" xmlns="http://www.w3.org/1999/xhtml">metrics</span></code><span class="koboSpan" id="kobo.1138.1" xmlns="http://www.w3.org/1999/xhtml">:</span></p>
<pre class="lstlisting" id="listing-202"><span class="koboSpan" id="kobo.1139.1" xmlns="http://www.w3.org/1999/xhtml">

from sklearn.metrics import accuracy_score 
 
print(accuracy_score(result, y_test))
</span></pre>
<p><span class="koboSpan" id="kobo.1140.1" xmlns="http://www.w3.org/1999/xhtml">In our case, we got </span><span class="koboSpan" id="kobo.1141.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="89.2\%" class="math inline" src="../media/file1167.png" style="vertical-align:middle" title="89.2\%"/></span><span class="koboSpan" id="kobo.1142.1" xmlns="http://www.w3.org/1999/xhtml"> accuracy. </span><span class="koboSpan" id="kobo.1142.2" xmlns="http://www.w3.org/1999/xhtml">This </span><span id="dx1-156043"/><span class="koboSpan" id="kobo.1143.1" xmlns="http://www.w3.org/1999/xhtml">seems like a pretty decent value, but we should always consider accuracy values in the context of each problem. </span><span class="koboSpan" id="kobo.1143.2" xmlns="http://www.w3.org/1999/xhtml">For some tasks, </span><span class="koboSpan" id="kobo.1144.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="89.2\%" class="math inline" src="../media/file1167.png" style="vertical-align:middle" title="89.2\%"/></span><span class="koboSpan" id="kobo.1145.1" xmlns="http://www.w3.org/1999/xhtml"> can indeed be marvelous, but for others it can be simply disappointing. </span><span class="koboSpan" id="kobo.1145.2" xmlns="http://www.w3.org/1999/xhtml">Imagine, for instance, that you have a problem in which </span><span class="koboSpan" id="kobo.1146.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="99\%" class="math inline" src="../media/file1168.png" style="vertical-align:middle" title="99\%"/></span><span class="koboSpan" id="kobo.1147.1" xmlns="http://www.w3.org/1999/xhtml"> of the examples belong to one class. </span><span class="koboSpan" id="kobo.1147.2" xmlns="http://www.w3.org/1999/xhtml">Then, it is trivial to obtain at least </span><span class="koboSpan" id="kobo.1148.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="99\%" class="math inline" src="../media/file1168.png" style="vertical-align:middle" title="99\%"/></span><span class="koboSpan" id="kobo.1149.1" xmlns="http://www.w3.org/1999/xhtml"> accuracy! </span><span class="koboSpan" id="kobo.1149.2" xmlns="http://www.w3.org/1999/xhtml">You just need to classify all the inputs as belonging to the majority class. </span><span class="koboSpan" id="kobo.1149.3" xmlns="http://www.w3.org/1999/xhtml">In the next few pages, we will introduce tools to take this kind of situation into account and better quantify classification performance.</span></p>
<div class="tcolorbox questionx" id="tcolobox-163">
<span id="x1-156045x8.2.5"/>
<div class="tcolorbox-title">
<p><span class="koboSpan" id="kobo.1150.1" xmlns="http://www.w3.org/1999/xhtml">Exercise 8.6</span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.1151.1" xmlns="http://www.w3.org/1999/xhtml">Re-train the model under the following conditions and compute the accuracy of the resulting model:</span></p>
<ul>
<li><p><span class="koboSpan" id="kobo.1152.1" xmlns="http://www.w3.org/1999/xhtml">Reducing the learning rate to </span><span class="koboSpan" id="kobo.1153.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="10^{- 6}" class="math inline" src="../media/file1169.png" style="vertical-align:middle" title="10^{- 6}"/></span></p></li>
<li><p><span class="koboSpan" id="kobo.1154.1" xmlns="http://www.w3.org/1999/xhtml">Reducing the learning rate to </span><span class="koboSpan" id="kobo.1155.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="10^{- 6}" class="math inline" src="../media/file1169.png" style="vertical-align:middle" title="10^{- 6}"/></span><span class="koboSpan" id="kobo.1156.1" xmlns="http://www.w3.org/1999/xhtml"> and increasing the number of epochs to </span><span class="koboSpan" id="kobo.1157.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1,000" class="math inline" src="../media/file1170.png" style="vertical-align:middle" title="1,000"/></span></p></li>
<li><p><span class="koboSpan" id="kobo.1158.1" xmlns="http://www.w3.org/1999/xhtml">Reducing the size of the training dataset to </span><span class="koboSpan" id="kobo.1159.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="20" class="math inline" src="../media/file588.png" style="vertical-align:middle" title="20"/></span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.1160.1" xmlns="http://www.w3.org/1999/xhtml">In which cases is the resulting model less accurate? </span><span class="koboSpan" id="kobo.1160.2" xmlns="http://www.w3.org/1999/xhtml">Why?</span></p>
<p><span class="koboSpan" id="kobo.1161.1" xmlns="http://www.w3.org/1999/xhtml">Does overfitting occur in any of these scenarios? </span><span class="koboSpan" id="kobo.1161.2" xmlns="http://www.w3.org/1999/xhtml">How could you identify it?</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.1162.1" xmlns="http://www.w3.org/1999/xhtml">So far, we have assessed the accuracy of our model just by measuring the proportion of elements that it would correctly classify by setting a threshold of </span><span class="koboSpan" id="kobo.1163.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.5" class="math inline" src="../media/file1166.png" style="vertical-align:middle" title="0.5"/></span><span class="koboSpan" id="kobo.1164.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1164.2" xmlns="http://www.w3.org/1999/xhtml">There are nevertheless other metrics of the performance of a binary classifier. </span><span class="koboSpan" id="kobo.1164.3" xmlns="http://www.w3.org/1999/xhtml">We will study them in the next subsection!</span></p>
</section>
<section class="level3 subsectionHead" data-number="16.2.6" id="binary-classifier-performance">
<h2 class="subsectionHead" data-number="16.2.6"><span class="titlemark"><span class="koboSpan" id="kobo.1165.1" xmlns="http://www.w3.org/1999/xhtml">8.2.6 </span></span> <span id="x1-1570008.2.6"><span class="koboSpan" id="kobo.1166.1" xmlns="http://www.w3.org/1999/xhtml">Binary classifier performance</span></span></h2>
<p><span class="koboSpan" id="kobo.1167.1" xmlns="http://www.w3.org/1999/xhtml">Whenever you have a </span><span id="dx1-157001"/><span class="koboSpan" id="kobo.1168.1" xmlns="http://www.w3.org/1999/xhtml">binary classifier, any output can belong to one of the four categories depicted in the following table:</span></p>
<div>
<div>
<table style="border: 1px solid black;">
<colgroup>
<col style="width: 33%; border: 1px solid black;"/>
<col style="width: 33%; border: 1px solid black;"/>
<col style="width: 33%"/>
</colgroup>
<tbody>
<tr style="border: 1px solid black;">
<th style="text-align: center;"/>
<th style="text-align: center;"><span class="koboSpan" id="kobo.1169.1" xmlns="http://www.w3.org/1999/xhtml">Classified as positive</span></th>
<th style="text-align: center;"><span class="koboSpan" id="kobo.1170.1" xmlns="http://www.w3.org/1999/xhtml">Classified as negative</span></th>
</tr>
<tr>
<th style="text-align: center;"><span class="koboSpan" id="kobo.1171.1" xmlns="http://www.w3.org/1999/xhtml">Actual positive</span></th>
<td style="text-align: center;"><span class="koboSpan" id="kobo.1172.1" xmlns="http://www.w3.org/1999/xhtml">True positive</span></td>
<td style="text-align: center;"><span class="koboSpan" id="kobo.1173.1" xmlns="http://www.w3.org/1999/xhtml">False negative</span></td>
</tr>
<tr>
<th style="text-align: center;"><span class="koboSpan" id="kobo.1174.1" xmlns="http://www.w3.org/1999/xhtml">Actual negative</span></th>
<td style="text-align: center;"><span class="koboSpan" id="kobo.1175.1" xmlns="http://www.w3.org/1999/xhtml">False positive</span></td>
<td style="text-align: center;"><span class="koboSpan" id="kobo.1176.1" xmlns="http://www.w3.org/1999/xhtml">True negative</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<p><span class="koboSpan" id="kobo.1177.1" xmlns="http://www.w3.org/1999/xhtml">The abbreviations TP, FN, FP, and TN are also used to denote the number of true positives, false negatives, false positives, and true negatives (respectively) produced by a classifier over a given dataset. </span><span class="koboSpan" id="kobo.1177.2" xmlns="http://www.w3.org/1999/xhtml">These quantities are used very often. </span><span class="koboSpan" id="kobo.1177.3" xmlns="http://www.w3.org/1999/xhtml">In fact, a common way of assessing the performance of a classifier is by </span><span id="dx1-157002"/><span class="koboSpan" id="kobo.1178.1" xmlns="http://www.w3.org/1999/xhtml">looking at its </span><strong><span class="koboSpan" id="kobo.1179.1" xmlns="http://www.w3.org/1999/xhtml">confusion matrix</span></strong><span class="koboSpan" id="kobo.1180.1" xmlns="http://www.w3.org/1999/xhtml"> (usually over the test dataset), which is nothing more than the matrix</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.1181.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\begin{pmatrix} \text{TP} &amp; \text{FN} \\ \text{FP} &amp; \text{TN} \\ \end{pmatrix}." class="math display" src="../media/file1171.png" style="vertical-align:middle" title="\begin{pmatrix} \text{TP} &amp; \text{FN} \\ \text{FP} &amp; \text{TN} \\ \end{pmatrix}."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.1182.1" xmlns="http://www.w3.org/1999/xhtml">To get started, we can now compute the confusion matrix for the binary classifier that we have just trained over the test dataset. </span><span class="koboSpan" id="kobo.1182.2" xmlns="http://www.w3.org/1999/xhtml">For this, we can use the </span><code><span class="koboSpan" id="kobo.1183.1" xmlns="http://www.w3.org/1999/xhtml">confusion_matrix</span></code><span class="koboSpan" id="kobo.1184.1" xmlns="http://www.w3.org/1999/xhtml"> function from </span><code><span class="koboSpan" id="kobo.1185.1" xmlns="http://www.w3.org/1999/xhtml">sklearn</span></code><span style="color:#000000"><code><span class="koboSpan" id="kobo.1186.1" xmlns="http://www.w3.org/1999/xhtml">.</span></code></span><code><span class="koboSpan" id="kobo.1187.1" xmlns="http://www.w3.org/1999/xhtml">metrics</span></code><span class="koboSpan" id="kobo.1188.1" xmlns="http://www.w3.org/1999/xhtml">, which requires two arguments: an array of predicted labels and an array of true labels:</span></p>
<pre class="lstlisting" id="listing-203"><span class="koboSpan" id="kobo.1189.1" xmlns="http://www.w3.org/1999/xhtml">

from sklearn.metrics import confusion_matrix 
 
confusion_matrix(y_true = y_test, y_pred = result)
</span></pre>
<p><span class="koboSpan" id="kobo.1190.1" xmlns="http://www.w3.org/1999/xhtml">Upon executing this piece of code, we get the following confusion matrix for our classifier:</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.1191.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\begin{pmatrix} {24} &amp; {20} \\ 7 &amp; {199} \\ \end{pmatrix}." class="math display" src="../media/file1172.png" style="vertical-align:middle" title="\begin{pmatrix} {24} &amp; {20} \\ 7 &amp; {199} \\ \end{pmatrix}."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.1192.1" xmlns="http://www.w3.org/1999/xhtml">This matrix shows that there are very few false positives compared to the number of true negatives, but almost as many false negatives as true positives. </span><span class="koboSpan" id="kobo.1192.2" xmlns="http://www.w3.org/1999/xhtml">This means that our classifier does a very good job of picking up the negative class but it is not so good at identifying the positive one. </span><span class="koboSpan" id="kobo.1192.3" xmlns="http://www.w3.org/1999/xhtml">In a moment, we will discuss how to quantify this more precisely.</span></p>
<div class="tcolorbox learnmore" id="tcolobox-164">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.1193.1" xmlns="http://www.w3.org/1999/xhtml">To learn more</span></span><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.1194.1" xmlns="http://www.w3.org/1999/xhtml">…</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.1195.1" xmlns="http://www.w3.org/1999/xhtml">Although we have focused just on binary classifiers, confusion matrices can also be defined for classification problems in which there are </span><span class="koboSpan" id="kobo.1196.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="n" class="math inline" src="../media/file244.png" style="vertical-align:middle" title="n"/></span><span class="koboSpan" id="kobo.1197.1" xmlns="http://www.w3.org/1999/xhtml"> classes. </span><span class="koboSpan" id="kobo.1197.2" xmlns="http://www.w3.org/1999/xhtml">They have </span><span class="koboSpan" id="kobo.1198.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="n" class="math inline" src="../media/file244.png" style="vertical-align:middle" title="n"/></span><span class="koboSpan" id="kobo.1199.1" xmlns="http://www.w3.org/1999/xhtml"> rows and </span><span class="koboSpan" id="kobo.1200.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="n" class="math inline" src="../media/file244.png" style="vertical-align:middle" title="n"/></span><span class="koboSpan" id="kobo.1201.1" xmlns="http://www.w3.org/1999/xhtml"> columns, and the entry in row </span><span class="koboSpan" id="kobo.1202.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="k" class="math inline" src="../media/file317.png" style="vertical-align:middle" title="k"/></span><span class="koboSpan" id="kobo.1203.1" xmlns="http://www.w3.org/1999/xhtml"> column </span><span class="koboSpan" id="kobo.1204.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="l" class="math inline" src="../media/file514.png" style="vertical-align:middle" title="l"/></span><span class="koboSpan" id="kobo.1205.1" xmlns="http://www.w3.org/1999/xhtml"> represents the number of elements that actually belong to class </span><span class="koboSpan" id="kobo.1206.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="k" class="math inline" src="../media/file317.png" style="vertical-align:middle" title="k"/></span><span class="koboSpan" id="kobo.1207.1" xmlns="http://www.w3.org/1999/xhtml"> but that are labeled as class </span><span class="koboSpan" id="kobo.1208.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="l" class="math inline" src="../media/file514.png" style="vertical-align:middle" title="l"/></span><span class="koboSpan" id="kobo.1209.1" xmlns="http://www.w3.org/1999/xhtml"> by the system.</span></p>
<p><span class="koboSpan" id="kobo.1210.1" xmlns="http://www.w3.org/1999/xhtml">Additionally, if you fix one of the </span><span class="koboSpan" id="kobo.1211.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="n" class="math inline" src="../media/file244.png" style="vertical-align:middle" title="n"/></span><span class="koboSpan" id="kobo.1212.1" xmlns="http://www.w3.org/1999/xhtml"> classes as the positive one and consider the rest as negative, you can obtain TP, FP, TN, and FN for that particular class.</span></p>
</div>
</div>
<p><span class="koboSpan" id="kobo.1213.1" xmlns="http://www.w3.org/1999/xhtml">Confusion </span><span id="dx1-157005"/><span class="koboSpan" id="kobo.1214.1" xmlns="http://www.w3.org/1999/xhtml">matrices are very informative, and the quantities in them can help us define several metrics of the performance of a binary classifier. </span><span class="koboSpan" id="kobo.1214.2" xmlns="http://www.w3.org/1999/xhtml">For instance, the usual accuracy metric can be defined by</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.1215.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\text{Acc} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}." class="math display" src="../media/file1173.png" style="vertical-align:middle" title="\text{Acc} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.1216.1" xmlns="http://www.w3.org/1999/xhtml">Other interesting </span><span id="dx1-157006"/><span class="koboSpan" id="kobo.1217.1" xmlns="http://www.w3.org/1999/xhtml">metrics are the </span><strong><span class="koboSpan" id="kobo.1218.1" xmlns="http://www.w3.org/1999/xhtml">positive predictive value</span></strong><span class="koboSpan" id="kobo.1219.1" xmlns="http://www.w3.org/1999/xhtml"> and the </span><strong><span class="koboSpan" id="kobo.1220.1" xmlns="http://www.w3.org/1999/xhtml">sensitivity</span></strong><span class="koboSpan" id="kobo.1221.1" xmlns="http://www.w3.org/1999/xhtml">, which are defined respectively as</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.1222.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="P = \frac{\text{TP}}{\text{TP} + \text{FP}},\qquad S = \frac{\text{TP}}{\text{TP} + \text{FN}}." class="math display" src="../media/file1174.png" style="vertical-align:middle" title="P = \frac{\text{TP}}{\text{TP} + \text{FP}},\qquad S = \frac{\text{TP}}{\text{TP} + \text{FN}}."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.1223.1" xmlns="http://www.w3.org/1999/xhtml">The positive predictive value is also </span><span id="dx1-157007"/><span class="koboSpan" id="kobo.1224.1" xmlns="http://www.w3.org/1999/xhtml">known as the </span><strong><span class="koboSpan" id="kobo.1225.1" xmlns="http://www.w3.org/1999/xhtml">precision</span></strong><span class="koboSpan" id="kobo.1226.1" xmlns="http://www.w3.org/1999/xhtml"> and the sensitivity is also known as the </span><strong><span class="koboSpan" id="kobo.1227.1" xmlns="http://www.w3.org/1999/xhtml">recall</span></strong><span class="koboSpan" id="kobo.1228.1" xmlns="http://www.w3.org/1999/xhtml"> of the </span><span id="dx1-157008"/><span class="koboSpan" id="kobo.1229.1" xmlns="http://www.w3.org/1999/xhtml">classifier.</span></p>
<p><span class="koboSpan" id="kobo.1230.1" xmlns="http://www.w3.org/1999/xhtml">There is a trade-off between </span><span class="koboSpan" id="kobo.1231.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="P" class="math inline" src="../media/file1.png" style="vertical-align:middle" title="P"/></span><span class="koboSpan" id="kobo.1232.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.1233.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="S" class="math inline" src="../media/file73.png" style="vertical-align:middle" title="S"/></span><span class="koboSpan" id="kobo.1234.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1234.2" xmlns="http://www.w3.org/1999/xhtml">Obtaining a perfect recall is trivial: you just need to classify every input as positive. </span><span class="koboSpan" id="kobo.1234.3" xmlns="http://www.w3.org/1999/xhtml">But then, you will have a low precision. </span><span class="koboSpan" id="kobo.1234.4" xmlns="http://www.w3.org/1999/xhtml">Similarly, it is easy to obtain very good values of precision: only classify an example as positive if you are extremely sure that it is positive. </span><span class="koboSpan" id="kobo.1234.5" xmlns="http://www.w3.org/1999/xhtml">But then the recall will be very low.</span></p>
<p><span class="koboSpan" id="kobo.1235.1" xmlns="http://www.w3.org/1999/xhtml">For this reason, an interesting metric is the </span><span class="koboSpan" id="kobo.1236.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="F_{1}" class="math inline" src="../media/file1175.png" style="vertical-align:middle" title="F_{1}"/></span><span class="koboSpan" id="kobo.1237.1" xmlns="http://www.w3.org/1999/xhtml"> score, defined as the harmonic mean of </span><span class="koboSpan" id="kobo.1238.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="P" class="math inline" src="../media/file1.png" style="vertical-align:middle" title="P"/></span><span class="koboSpan" id="kobo.1239.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.1240.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="S" class="math inline" src="../media/file73.png" style="vertical-align:middle" title="S"/></span><span class="koboSpan" id="kobo.1241.1" xmlns="http://www.w3.org/1999/xhtml">:</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.1242.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="F_{1} = \frac{2}{\frac{1}{P} + \frac{1}{S}} = \frac{2PS}{P + S}." class="math display" src="../media/file1176.png" style="vertical-align:middle" title="F_{1} = \frac{2}{\frac{1}{P} + \frac{1}{S}} = \frac{2PS}{P + S}."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.1243.1" xmlns="http://www.w3.org/1999/xhtml">It is easy to see how this score can range from </span><span class="koboSpan" id="kobo.1244.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.1245.1" xmlns="http://www.w3.org/1999/xhtml"> (the score of the worst possible classifier) to </span><span class="koboSpan" id="kobo.1246.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.1247.1" xmlns="http://www.w3.org/1999/xhtml"> (the score of a perfect classifier). </span><span class="koboSpan" id="kobo.1247.2" xmlns="http://www.w3.org/1999/xhtml">Moreover, a high </span><span class="koboSpan" id="kobo.1248.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="F_{1}" class="math inline" src="../media/file1175.png" style="vertical-align:middle" title="F_{1}"/></span><span class="koboSpan" id="kobo.1249.1" xmlns="http://www.w3.org/1999/xhtml"> score means that we are not favoring recall over precision or precision over recall.</span></p>
<p><span class="koboSpan" id="kobo.1250.1" xmlns="http://www.w3.org/1999/xhtml">If you are </span><span id="dx1-157009"/><span class="koboSpan" id="kobo.1251.1" xmlns="http://www.w3.org/1999/xhtml">mathematically oriented, you may have realized that our expression for </span><span class="koboSpan" id="kobo.1252.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="F_{1}" class="math inline" src="../media/file1175.png" style="vertical-align:middle" title="F_{1}"/></span><span class="koboSpan" id="kobo.1253.1" xmlns="http://www.w3.org/1999/xhtml"> is actually undefined for </span><span class="koboSpan" id="kobo.1254.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="P = S = 0" class="math inline" src="../media/file1177.png" style="vertical-align:middle" title="P = S = 0"/></span><span class="koboSpan" id="kobo.1255.1" xmlns="http://www.w3.org/1999/xhtml">, but we can trivially extend it by continuity to take the value </span><span class="koboSpan" id="kobo.1256.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="F_{1} = 0" class="math inline" src="../media/file1178.png" style="vertical-align:middle" title="F_{1} = 0"/></span><span class="koboSpan" id="kobo.1257.1" xmlns="http://www.w3.org/1999/xhtml"> there.</span></p>
<p><span class="koboSpan" id="kobo.1258.1" xmlns="http://www.w3.org/1999/xhtml">In order to compute these metrics, we may use the </span><code><span class="koboSpan" id="kobo.1259.1" xmlns="http://www.w3.org/1999/xhtml">classification_report</span></code><span class="koboSpan" id="kobo.1260.1" xmlns="http://www.w3.org/1999/xhtml"> function from </span><code><span class="koboSpan" id="kobo.1261.1" xmlns="http://www.w3.org/1999/xhtml">sklearn</span></code><span style="color:#000000"><code><span class="koboSpan" id="kobo.1262.1" xmlns="http://www.w3.org/1999/xhtml">.</span></code></span><code><span class="koboSpan" id="kobo.1263.1" xmlns="http://www.w3.org/1999/xhtml">metrics</span></code><span class="koboSpan" id="kobo.1264.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1264.2" xmlns="http://www.w3.org/1999/xhtml">In our case, we may run the following:</span></p>
<pre class="lstlisting" id="listing-204"><span class="koboSpan" id="kobo.1265.1" xmlns="http://www.w3.org/1999/xhtml">

from sklearn.metrics import classification_report 
 
print(classification_report(y_true = y_test, y_pred = result))
</span></pre>
<p><span class="koboSpan" id="kobo.1266.1" xmlns="http://www.w3.org/1999/xhtml">This yields the following output:</span></p>
<pre class="listings"><span class="koboSpan" id="kobo.1267.1" xmlns="http://www.w3.org/1999/xhtml">

 
              precision    recall  f1-score   support 
 
           0       0.77      0.55      0.64        44 
           1       0.91      0.97      0.94       206 
 
    accuracy                           0.89       250 
   macro avg       0.84      0.76      0.79       250 
weighted avg       0.89      0.89      0.88       250
    
</span></pre>
<p><span class="koboSpan" id="kobo.1268.1" xmlns="http://www.w3.org/1999/xhtml">And in this table, we can see all the metrics that we have mentioned. </span><span class="koboSpan" id="kobo.1268.2" xmlns="http://www.w3.org/1999/xhtml">You can see that the scores are returned for both the case in which </span><span class="koboSpan" id="kobo.1269.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.1270.1" xmlns="http://www.w3.org/1999/xhtml"> is the positive class and for the case when </span><span class="koboSpan" id="kobo.1271.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.1272.1" xmlns="http://www.w3.org/1999/xhtml"> is positive instead (in our case, we have considered </span><span class="koboSpan" id="kobo.1273.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.1274.1" xmlns="http://www.w3.org/1999/xhtml"> to be positive, so we would look at the first row). </span><span class="koboSpan" id="kobo.1274.2" xmlns="http://www.w3.org/1999/xhtml">By the way, the </span><strong><span class="koboSpan" id="kobo.1275.1" xmlns="http://www.w3.org/1999/xhtml">support</span></strong><span class="koboSpan" id="kobo.1276.1" xmlns="http://www.w3.org/1999/xhtml"> of a class is meant to represent the number of elements in the class that can be found in the dataset. </span><span class="koboSpan" id="kobo.1276.2" xmlns="http://www.w3.org/1999/xhtml">Also, the </span><strong><span class="koboSpan" id="kobo.1277.1" xmlns="http://www.w3.org/1999/xhtml">macro average</span></strong><span class="koboSpan" id="kobo.1278.1" xmlns="http://www.w3.org/1999/xhtml"> of each metric is just the plain </span><span id="dx1-157021"/><span class="koboSpan" id="kobo.1279.1" xmlns="http://www.w3.org/1999/xhtml">average of the values of the metric obtained by taking each class as positive. </span><span class="koboSpan" id="kobo.1279.2" xmlns="http://www.w3.org/1999/xhtml">The weighted average is like the macro average, but weighted by the proportion of elements of each class in the dataset.</span></p>
<p><span class="koboSpan" id="kobo.1280.1" xmlns="http://www.w3.org/1999/xhtml">Let’s say that we have a binary classifier that returns a continuous output between </span><span class="koboSpan" id="kobo.1281.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.1282.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.1283.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.1284.1" xmlns="http://www.w3.org/1999/xhtml"> before cutting through a threshold in order to assign a label. </span><span class="koboSpan" id="kobo.1284.2" xmlns="http://www.w3.org/1999/xhtml">As we saw earlier, we could just measure the </span><span id="dx1-157022"/><span class="koboSpan" id="kobo.1285.1" xmlns="http://www.w3.org/1999/xhtml">performance of our classifier by using a bunch of metrics. </span><span class="koboSpan" id="kobo.1285.2" xmlns="http://www.w3.org/1999/xhtml">But if we want to get a broader perspective of how our classifier could work for any threshold, we can take another approach.</span></p>
<p><span class="koboSpan" id="kobo.1286.1" xmlns="http://www.w3.org/1999/xhtml">Using the entries of the confusion matrix over a dataset, we may </span><span id="dx1-157023"/><span class="koboSpan" id="kobo.1287.1" xmlns="http://www.w3.org/1999/xhtml">define the </span><strong><span class="koboSpan" id="kobo.1288.1" xmlns="http://www.w3.org/1999/xhtml">true positive rate</span></strong><span class="koboSpan" id="kobo.1289.1" xmlns="http://www.w3.org/1999/xhtml"> as the proportion</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.1290.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}," class="math display" src="../media/file1179.png" style="vertical-align:middle" title="\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}},"/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.1291.1" xmlns="http://www.w3.org/1999/xhtml">that is, the proportion of examples from the positive class that are actually classified as positive. </span><span class="koboSpan" id="kobo.1291.2" xmlns="http://www.w3.org/1999/xhtml">On the other hand, we can analogously </span><span id="dx1-157024"/><span class="koboSpan" id="kobo.1292.1" xmlns="http://www.w3.org/1999/xhtml">define the </span><strong><span class="koboSpan" id="kobo.1293.1" xmlns="http://www.w3.org/1999/xhtml">false</span></strong> <strong><span class="koboSpan" id="kobo.1294.1" xmlns="http://www.w3.org/1999/xhtml">positive rate</span></strong><span class="koboSpan" id="kobo.1295.1" xmlns="http://www.w3.org/1999/xhtml"> as the quotient</span></p>
<table class="equation-star">
<tbody>
<tr class="odd odd">
<td><span class="koboSpan" id="kobo.1296.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}." class="math display" src="../media/file1180.png" style="vertical-align:middle" title="\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}."/></span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.1297.1" xmlns="http://www.w3.org/1999/xhtml">The </span><strong><span class="koboSpan" id="kobo.1298.1" xmlns="http://www.w3.org/1999/xhtml">Receiver Operating Characteristic curve</span></strong><span class="koboSpan" id="kobo.1299.1" xmlns="http://www.w3.org/1999/xhtml"> or </span><strong><span class="koboSpan" id="kobo.1300.1" xmlns="http://www.w3.org/1999/xhtml">ROC curve</span></strong><span class="koboSpan" id="kobo.1301.1" xmlns="http://www.w3.org/1999/xhtml"> of a </span><span id="dx1-157025"/><span class="koboSpan" id="kobo.1302.1" xmlns="http://www.w3.org/1999/xhtml">classifier that returns continuous values is computed over a given dataset by plotting, for every possible choice of threshold, a point with a </span><span class="koboSpan" id="kobo.1303.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="Y" class="math inline" src="../media/file11.png" style="vertical-align:middle" title="Y"/></span><span class="koboSpan" id="kobo.1304.1" xmlns="http://www.w3.org/1999/xhtml"> coordinate given by the corresponding TPR and an </span><span class="koboSpan" id="kobo.1305.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="X" class="math inline" src="../media/file9.png" style="vertical-align:middle" title="X"/></span><span class="koboSpan" id="kobo.1306.1" xmlns="http://www.w3.org/1999/xhtml"> coordinate with the FPR for that threshold. </span><span class="koboSpan" id="kobo.1306.2" xmlns="http://www.w3.org/1999/xhtml">As the threshold increases from </span><span class="koboSpan" id="kobo.1307.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.1308.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><span class="koboSpan" id="kobo.1309.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.1310.1" xmlns="http://www.w3.org/1999/xhtml">, this will give rise to a finite sequence of points. </span><span class="koboSpan" id="kobo.1310.2" xmlns="http://www.w3.org/1999/xhtml">The curve is obtained by joining these through straight lines. </span><span class="koboSpan" id="kobo.1310.3" xmlns="http://www.w3.org/1999/xhtml">Notice that we evaluate the performance of the classifier with different levels of ”demand” for classifying an input as positive. </span><span class="koboSpan" id="kobo.1310.4" xmlns="http://www.w3.org/1999/xhtml">When the threshold is high, it will be harder to classify something as positive; the FPR will be low — great! </span><span class="koboSpan" id="kobo.1310.5" xmlns="http://www.w3.org/1999/xhtml">— but the TPR will probably be also low. </span><span class="koboSpan" id="kobo.1310.6" xmlns="http://www.w3.org/1999/xhtml">On the other hand, for low values of the threshold, it will be easier for an input to be classified as positive: the TPR will be high — yay! </span><span class="koboSpan" id="kobo.1310.7" xmlns="http://www.w3.org/1999/xhtml">— but that can also cause the false positives to go up.</span></p>
<p><span class="koboSpan" id="kobo.1311.1" xmlns="http://www.w3.org/1999/xhtml">Sounds familiar? </span><span class="koboSpan" id="kobo.1311.2" xmlns="http://www.w3.org/1999/xhtml">This is the same kind of trade-off that we discussed when we defined precision and recall. </span><span class="koboSpan" id="kobo.1311.3" xmlns="http://www.w3.org/1999/xhtml">The difference is that, in this case, we are taking into account the behavior of the classifier for every possible choice of threshold, giving us a global assessment. </span><span class="koboSpan" id="kobo.1311.4" xmlns="http://www.w3.org/1999/xhtml">Plotting the ROC curve can be very informative because it can also help in selecting classification thresholds that are more suitable for our problem. </span><span class="koboSpan" id="kobo.1311.5" xmlns="http://www.w3.org/1999/xhtml">For instance, if you are trying to detect whether a given patient has a certain serious illness, it may pay off to have some false positives — people that may need to </span><span id="dx1-157026"/><span class="koboSpan" id="kobo.1312.1" xmlns="http://www.w3.org/1999/xhtml">undergo additional medical tests — at the cost of having very low false negatives. </span><span class="koboSpan" id="kobo.1312.2" xmlns="http://www.w3.org/1999/xhtml">The ROC curve can help you there by identifying points at which the TPR is high and the FPR is acceptable.</span></p>
<p><span class="koboSpan" id="kobo.1313.1" xmlns="http://www.w3.org/1999/xhtml">In order to plot a ROC curve, we can use the </span><code><span class="koboSpan" id="kobo.1314.1" xmlns="http://www.w3.org/1999/xhtml">roc_curve</span></code><span class="koboSpan" id="kobo.1315.1" xmlns="http://www.w3.org/1999/xhtml"> function from </span><code><span class="koboSpan" id="kobo.1316.1" xmlns="http://www.w3.org/1999/xhtml">sklearn</span></code><span style="color:#000000"><code><span class="koboSpan" id="kobo.1317.1" xmlns="http://www.w3.org/1999/xhtml">.</span></code></span><code><span class="koboSpan" id="kobo.1318.1" xmlns="http://www.w3.org/1999/xhtml">metrics</span></code><span class="koboSpan" id="kobo.1319.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1319.2" xmlns="http://www.w3.org/1999/xhtml">It will return the </span><span class="koboSpan" id="kobo.1320.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="X" class="math inline" src="../media/file9.png" style="vertical-align:middle" title="X"/></span><span class="koboSpan" id="kobo.1321.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.1322.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="Y" class="math inline" src="../media/file11.png" style="vertical-align:middle" title="Y"/></span><span class="koboSpan" id="kobo.1323.1" xmlns="http://www.w3.org/1999/xhtml"> coordinates of the points of the curve. </span><span class="koboSpan" id="kobo.1323.2" xmlns="http://www.w3.org/1999/xhtml">In our particular case, we may run the following piece of code:</span></p>
<pre class="lstlisting" id="listing-205"><span class="koboSpan" id="kobo.1324.1" xmlns="http://www.w3.org/1999/xhtml">

from sklearn.metrics import roc_curve 
 
fpr, tpr, _ = roc_curve(y_test, output) 
 
plt.plot(fpr, tpr) 
 
plt.plot([0,1],[0,1],linestyle="--",color="black") 
 
plt.xlabel("FPR"); plt.ylabel("TPR") 
 
plt.show()
</span></pre>
<p><span class="koboSpan" id="kobo.1325.1" xmlns="http://www.w3.org/1999/xhtml">Notice how we have dropped part of the output of the </span><code><span class="koboSpan" id="kobo.1326.1" xmlns="http://www.w3.org/1999/xhtml">roc_curve</span></code><span class="koboSpan" id="kobo.1327.1" xmlns="http://www.w3.org/1999/xhtml"> function; in particular, the return object that we ignore yields an array that includes the thresholds at which the classifier accuracy changes (you can refer to the documentation at </span><a class="url" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html"><span class="koboSpan" id="kobo.1328.1" xmlns="http://www.w3.org/1999/xhtml">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html</span></a><span class="koboSpan" id="kobo.1329.1" xmlns="http://www.w3.org/1999/xhtml"> for more information). </span><span class="koboSpan" id="kobo.1329.2" xmlns="http://www.w3.org/1999/xhtml">The output that we got can be found in </span><em><span class="koboSpan" id="kobo.1330.1" xmlns="http://www.w3.org/1999/xhtml">Figure</span></em> <a href="#Figure8.5"><em><span class="koboSpan" id="kobo.1331.1" xmlns="http://www.w3.org/1999/xhtml">8.5</span></em></a><span class="koboSpan" id="kobo.1332.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1332.2" xmlns="http://www.w3.org/1999/xhtml">Notice that we have manually drawn a dashed line between </span><span class="koboSpan" id="kobo.1333.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(0,0)" class="math inline" src="../media/file613.png" style="vertical-align:middle" title="(0,0)"/></span><span class="koboSpan" id="kobo.1334.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.1335.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(1,1)" class="math inline" src="../media/file1181.png" style="vertical-align:middle" title="(1,1)"/></span><span class="koboSpan" id="kobo.1336.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1336.2" xmlns="http://www.w3.org/1999/xhtml">That is meant to represent the ROC curve that could be generated by a random classifier, one that assigns an input to a class with probability proportional to the size of that class, and it is an important visual aid. </span><span class="koboSpan" id="kobo.1336.3" xmlns="http://www.w3.org/1999/xhtml">That’s because any curves above that dashed line are ROC curves of classifiers that have some real classification power.</span></p>
<figure>
<span class="koboSpan" id="kobo.1337.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="Figure 8.5: ROC curve (solid line) for the classifier that we have trained." src="../media/file1182.png"/></span>
<figcaption aria-hidden="true"><span class="id" id="Figure8.5"><strong><span class="koboSpan" id="kobo.1338.1" xmlns="http://www.w3.org/1999/xhtml">Figure 8.5</span></strong><span class="koboSpan" id="kobo.1339.1" xmlns="http://www.w3.org/1999/xhtml">: </span></span><span class="koboSpan" id="kobo.1340.1" xmlns="http://www.w3.org/1999/xhtml">ROC curve (solid line) for the classifier that we have trained.</span></figcaption>
</figure>
<p><span class="koboSpan" id="kobo.1341.1" xmlns="http://www.w3.org/1999/xhtml">There are some interesting features in this ROC curve, so let’s discuss it a little bit. </span><span class="koboSpan" id="kobo.1341.2" xmlns="http://www.w3.org/1999/xhtml">To start with, notice that the points </span><span class="koboSpan" id="kobo.1342.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(0,0)" class="math inline" src="../media/file613.png" style="vertical-align:middle" title="(0,0)"/></span><span class="koboSpan" id="kobo.1343.1" xmlns="http://www.w3.org/1999/xhtml"> and </span><span class="koboSpan" id="kobo.1344.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(1,1)" class="math inline" src="../media/file1181.png" style="vertical-align:middle" title="(1,1)"/></span><span class="koboSpan" id="kobo.1345.1" xmlns="http://www.w3.org/1999/xhtml"> always belong to the ROC curve of any classifier because they are </span><span id="dx1-157035"/><span class="koboSpan" id="kobo.1346.1" xmlns="http://www.w3.org/1999/xhtml">achieved with the highest and lowest thresholds, respectively. </span><span class="koboSpan" id="kobo.1346.2" xmlns="http://www.w3.org/1999/xhtml">In the first case, no input is assigned to the positive class, so we have neither TPs nor FPs. </span><span class="koboSpan" id="kobo.1346.3" xmlns="http://www.w3.org/1999/xhtml">In the second one, all inputs are assigned to the positive class, so we have neither FNs nor TNs. </span><span id="dx1-157036"/></p>
<p><span class="koboSpan" id="kobo.1347.1" xmlns="http://www.w3.org/1999/xhtml">In addition to this, we can observe in our graph that, from </span><span class="koboSpan" id="kobo.1348.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(0,0)" class="math inline" src="../media/file613.png" style="vertical-align:middle" title="(0,0)"/></span><span class="koboSpan" id="kobo.1349.1" xmlns="http://www.w3.org/1999/xhtml">, the ROC curve starts moving horizontally, increasing the FPR without increasing the TPR. </span><span class="koboSpan" id="kobo.1349.2" xmlns="http://www.w3.org/1999/xhtml">This means that there are some examples in the test dataset that the model very confidently classifies as belonging to the positive class but that, in fact, are negative. </span><span class="koboSpan" id="kobo.1349.3" xmlns="http://www.w3.org/1999/xhtml">This is undesirable, of course. </span><span class="koboSpan" id="kobo.1349.4" xmlns="http://www.w3.org/1999/xhtml">We would like our ROC curve to go up — increasing the TPR — without moving to the right. </span><span class="koboSpan" id="kobo.1349.5" xmlns="http://www.w3.org/1999/xhtml">And that is exactly what happens after that first hiccup. </span><span class="koboSpan" id="kobo.1349.6" xmlns="http://www.w3.org/1999/xhtml">We observe a long segment in which the TPR goes up without any increase in the FPR. </span><span class="koboSpan" id="kobo.1349.7" xmlns="http://www.w3.org/1999/xhtml">If we need our classifier to have high precision, we could select the threshold that achieves TPR of about </span><span class="koboSpan" id="kobo.1350.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.71" class="math inline" src="../media/file1183.png" style="vertical-align:middle" title="0.71"/></span><span class="koboSpan" id="kobo.1351.1" xmlns="http://www.w3.org/1999/xhtml"> with FPR of only about </span><span class="koboSpan" id="kobo.1352.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.02" class="math inline" src="../media/file1184.png" style="vertical-align:middle" title="0.02"/></span><span class="koboSpan" id="kobo.1353.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1353.2" xmlns="http://www.w3.org/1999/xhtml">On the other hand, if we need high recall, we can select the point in the curve where the TPR is already </span><span class="koboSpan" id="kobo.1354.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.1355.1" xmlns="http://www.w3.org/1999/xhtml"> with a FPR of about </span><span class="koboSpan" id="kobo.1356.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.5" class="math inline" src="../media/file1166.png" style="vertical-align:middle" title="0.5"/></span><span class="koboSpan" id="kobo.1357.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1357.2" xmlns="http://www.w3.org/1999/xhtml">For a more balanced classifier, notice that there is a point in the ROC curve with TPR around </span><span class="koboSpan" id="kobo.1358.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.91" class="math inline" src="../media/file1185.png" style="vertical-align:middle" title="0.91"/></span><span class="koboSpan" id="kobo.1359.1" xmlns="http://www.w3.org/1999/xhtml"> and FPR below </span><span class="koboSpan" id="kobo.1360.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.21" class="math inline" src="../media/file1186.png" style="vertical-align:middle" title="0.21"/></span><span class="koboSpan" id="kobo.1361.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<p><span class="koboSpan" id="kobo.1362.1" xmlns="http://www.w3.org/1999/xhtml">Of course, the ideal classifier would have a ROC curve that goes all the way from </span><span class="koboSpan" id="kobo.1363.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(0,0)" class="math inline" src="../media/file613.png" style="vertical-align:middle" title="(0,0)"/></span><span class="koboSpan" id="kobo.1364.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><span class="koboSpan" id="kobo.1365.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(1,0)" class="math inline" src="../media/file1187.png" style="vertical-align:middle" title="(1,0)"/></span><span class="koboSpan" id="kobo.1366.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1366.2" xmlns="http://www.w3.org/1999/xhtml">That would mean that there is a threshold for which all the positive examples are classified as positive, while no negative example is assigned to the positive class. </span><span class="koboSpan" id="kobo.1366.3" xmlns="http://www.w3.org/1999/xhtml">That’s just perfection! </span><span class="koboSpan" id="kobo.1366.4" xmlns="http://www.w3.org/1999/xhtml">From there, the ROC curve would go straight to </span><span class="koboSpan" id="kobo.1367.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(1,1)" class="math inline" src="../media/file1181.png" style="vertical-align:middle" title="(1,1)"/></span><span class="koboSpan" id="kobo.1368.1" xmlns="http://www.w3.org/1999/xhtml">: we have already found all the positive examples so the TPR cannot increase, but by decreasing the threshold we will eventually increase the FPR from </span><span class="koboSpan" id="kobo.1369.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.1370.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><span class="koboSpan" id="kobo.1371.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.1372.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<p><span class="koboSpan" id="kobo.1373.1" xmlns="http://www.w3.org/1999/xhtml">Obviously, that kind of perfect ROC curve is only achievable for extremely simple classification problems. </span><span class="koboSpan" id="kobo.1373.2" xmlns="http://www.w3.org/1999/xhtml">However, we can still </span><span id="dx1-157037"/><span class="koboSpan" id="kobo.1374.1" xmlns="http://www.w3.org/1999/xhtml">compare our actual model to that ideal classifier by computing the </span><strong><span class="koboSpan" id="kobo.1375.1" xmlns="http://www.w3.org/1999/xhtml">area under the ROC curve</span></strong><span class="koboSpan" id="kobo.1376.1" xmlns="http://www.w3.org/1999/xhtml">, often </span><span id="dx1-157038"/><span class="koboSpan" id="kobo.1377.1" xmlns="http://www.w3.org/1999/xhtml">abbreviated as </span><strong><span class="koboSpan" id="kobo.1378.1" xmlns="http://www.w3.org/1999/xhtml">AUC</span></strong><span class="koboSpan" id="kobo.1379.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1379.2" xmlns="http://www.w3.org/1999/xhtml">Since the ROC curve of the perfect classifier would have area equal to 1, we can consider that the closer the AUC of a classifier is to 1, the better its global performance is. </span><span class="koboSpan" id="kobo.1379.3" xmlns="http://www.w3.org/1999/xhtml">In the same way, a random classifier would have an ROC curve that is a straight line from </span><span class="koboSpan" id="kobo.1380.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(0,0)" class="math inline" src="../media/file613.png" style="vertical-align:middle" title="(0,0)"/></span><span class="koboSpan" id="kobo.1381.1" xmlns="http://www.w3.org/1999/xhtml"> to </span><span class="koboSpan" id="kobo.1382.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="(1,1)" class="math inline" src="../media/file1181.png" style="vertical-align:middle" title="(1,1)"/></span><span class="koboSpan" id="kobo.1383.1" xmlns="http://www.w3.org/1999/xhtml">, so its AUC would be </span><span class="koboSpan" id="kobo.1384.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.5" class="math inline" src="../media/file1166.png" style="vertical-align:middle" title="0.5"/></span><span class="koboSpan" id="kobo.1385.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1385.2" xmlns="http://www.w3.org/1999/xhtml">Hence, classifiers whose AUC is higher than </span><span class="koboSpan" id="kobo.1386.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.5" class="math inline" src="../media/file1166.png" style="vertical-align:middle" title="0.5"/></span><span class="koboSpan" id="kobo.1387.1" xmlns="http://www.w3.org/1999/xhtml"> have some actual classification power beyond just random guessing.</span></p>
<p><span class="koboSpan" id="kobo.1388.1" xmlns="http://www.w3.org/1999/xhtml">Having the coordinates of the points that define the ROC curve, we can easily get the AUC score using the </span><code><span class="koboSpan" id="kobo.1389.1" xmlns="http://www.w3.org/1999/xhtml">auc</span></code><span class="koboSpan" id="kobo.1390.1" xmlns="http://www.w3.org/1999/xhtml"> function from </span><code><span class="koboSpan" id="kobo.1391.1" xmlns="http://www.w3.org/1999/xhtml">sklearn</span></code><span style="color:#000000"><code><span class="koboSpan" id="kobo.1392.1" xmlns="http://www.w3.org/1999/xhtml">.</span></code></span><code><span class="koboSpan" id="kobo.1393.1" xmlns="http://www.w3.org/1999/xhtml">metrics</span></code><span class="koboSpan" id="kobo.1394.1" xmlns="http://www.w3.org/1999/xhtml">:</span></p>
<pre class="lstlisting" id="listing-206"><span class="koboSpan" id="kobo.1395.1" xmlns="http://www.w3.org/1999/xhtml">

from sklearn.metrics import auc 
 
print(auc(fpr,tpr))
</span></pre>
<p><span class="koboSpan" id="kobo.1396.1" xmlns="http://www.w3.org/1999/xhtml">In our case, we get an AUC score of approximately </span><span class="koboSpan" id="kobo.1397.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.9271" class="math inline" src="../media/file1188.png" style="vertical-align:middle" title="0.9271"/></span><span class="koboSpan" id="kobo.1398.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1398.2" xmlns="http://www.w3.org/1999/xhtml">Again, this seems like a great value, but let us stress once again that it all depended on the difficulty of the problem — and the one we have been considering is not particularly hard. </span><span class="koboSpan" id="kobo.1398.3" xmlns="http://www.w3.org/1999/xhtml">Also, remember that the AUC is a global performance metric that takes into account every possible threshold of your classifier. </span><span class="koboSpan" id="kobo.1398.4" xmlns="http://www.w3.org/1999/xhtml">At the end of the day, you need to commit to just one threshold value, and a high AUC might not mean much if, for your particular threshold choice, the accuracy, precision, and recall are not that great.</span></p>
<p><span class="koboSpan" id="kobo.1399.1" xmlns="http://www.w3.org/1999/xhtml">That was a lot of information! </span><span class="koboSpan" id="kobo.1399.2" xmlns="http://www.w3.org/1999/xhtml">In any case, for most practical purposes, all that you will need to know is summarized in the following note.</span></p>
<div class="tcolorbox important" id="tcolobox-165">
<div class="tcolorbox-title">
<p><span class="cmss-10x-x-109"><span class="koboSpan" id="kobo.1400.1" xmlns="http://www.w3.org/1999/xhtml">Important note</span></span></p>
</div>
<div class="tcolorbox-content">
<p><span class="koboSpan" id="kobo.1401.1" xmlns="http://www.w3.org/1999/xhtml">Given a binary classifier with continuous output, we may compute its receiver operating characteristic curve (also known as the ROC curve) over a dataset. </span><span class="koboSpan" id="kobo.1401.2" xmlns="http://www.w3.org/1999/xhtml">The higher the area under that curve, the higher the classifying power of the classifier.</span></p>
<p><span class="koboSpan" id="kobo.1402.1" xmlns="http://www.w3.org/1999/xhtml">We refer to the area under the ROC curve of a classifier as its AUC (short for ”area under the curve”):</span></p>
<ul>
<li><p><span class="koboSpan" id="kobo.1403.1" xmlns="http://www.w3.org/1999/xhtml">An AUC of </span><span class="koboSpan" id="kobo.1404.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="1" class="math inline" src="../media/file13.png" style="vertical-align:middle" title="1"/></span><span class="koboSpan" id="kobo.1405.1" xmlns="http://www.w3.org/1999/xhtml"> corresponds to a perfect classifier</span></p></li>
<li><p><span class="koboSpan" id="kobo.1406.1" xmlns="http://www.w3.org/1999/xhtml">An AUC of </span><span class="koboSpan" id="kobo.1407.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0.5" class="math inline" src="../media/file1166.png" style="vertical-align:middle" title="0.5"/></span><span class="koboSpan" id="kobo.1408.1" xmlns="http://www.w3.org/1999/xhtml"> would match that of a random classifier</span></p></li>
<li><p><span class="koboSpan" id="kobo.1409.1" xmlns="http://www.w3.org/1999/xhtml">An AUC of </span><span class="koboSpan" id="kobo.1410.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="0" class="math inline" src="../media/file12.png" style="vertical-align:middle" title="0"/></span><span class="koboSpan" id="kobo.1411.1" xmlns="http://www.w3.org/1999/xhtml"> corresponds to a classifier that always returns the wrong output</span></p></li>
</ul>
</div>
</div>
<p><span class="koboSpan" id="kobo.1412.1" xmlns="http://www.w3.org/1999/xhtml">By now, we </span><span id="dx1-157041"/><span class="koboSpan" id="kobo.1413.1" xmlns="http://www.w3.org/1999/xhtml">should have a decent understanding of (classical) machine learning, and you may be wondering where does the ”quantum” part begin? </span><span class="koboSpan" id="kobo.1413.2" xmlns="http://www.w3.org/1999/xhtml">It begins now.</span></p>
</section>
</section>
<section class="level2 sectionHead" data-number="16.3" id="quantum-classical-models">
<h1 class="sectionHead" data-number="16.3"><span class="titlemark"><span class="koboSpan" id="kobo.1414.1" xmlns="http://www.w3.org/1999/xhtml">8.3 </span></span> <span id="x1-1580008.3"><span class="koboSpan" id="kobo.1415.1" xmlns="http://www.w3.org/1999/xhtml">Quantum-classical models</span></span></h1>
<p><span class="koboSpan" id="kobo.1416.1" xmlns="http://www.w3.org/1999/xhtml">In general terms, quantum </span><span id="dx1-158001"/><span class="koboSpan" id="kobo.1417.1" xmlns="http://www.w3.org/1999/xhtml">machine learning refers to the application of machine learning techniques — only that quantum computing is involved at same stage of the process. </span><span class="koboSpan" id="kobo.1417.2" xmlns="http://www.w3.org/1999/xhtml">Maybe you use a quantum computer in some part a model that you wish to train. </span><span class="koboSpan" id="kobo.1417.3" xmlns="http://www.w3.org/1999/xhtml">Maybe you wish to use data generated by some quantum process. </span><span class="koboSpan" id="kobo.1417.4" xmlns="http://www.w3.org/1999/xhtml">Maybe you use a quantum computer to process quantum-generated data. </span><span class="koboSpan" id="kobo.1417.5" xmlns="http://www.w3.org/1999/xhtml">As you can imagine, the subject of quantum machine learning, as a whole, is broad enough to accommodate for a wide range of ideas and applications.</span></p>
<p><span class="koboSpan" id="kobo.1418.1" xmlns="http://www.w3.org/1999/xhtml">In an attempt to categorize it all a little bit, we can follow the useful classification shown in Schuld’s and Petruccione’s book </span><span class="cite"><span class="koboSpan" id="kobo.1419.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xschuld"><span class="koboSpan" id="kobo.1420.1" xmlns="http://www.w3.org/1999/xhtml">106</span></a><span class="koboSpan" id="kobo.1421.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.1422.1" xmlns="http://www.w3.org/1999/xhtml"> and divide quantum machine learning into four different flavors, which are depicted in </span><em><span class="koboSpan" id="kobo.1423.1" xmlns="http://www.w3.org/1999/xhtml">Figure</span></em> <a href="#Figure8.6"><em><span class="koboSpan" id="kobo.1424.1" xmlns="http://www.w3.org/1999/xhtml">8.6</span></em></a><span class="koboSpan" id="kobo.1425.1" xmlns="http://www.w3.org/1999/xhtml">, according to the classical or quantum nature of the data and processing devices that are used:</span></p>
<ul>
<li><p><span class="koboSpan" id="kobo.1426.1" xmlns="http://www.w3.org/1999/xhtml">We could consider part of quantum machine learning all the quantum-inspired classical machine learning techniques; that is, all the classical machine learning methods that draw ideas from quantum computing. </span><span class="koboSpan" id="kobo.1426.2" xmlns="http://www.w3.org/1999/xhtml">In this case, both the data and the computers are classical, but there is some quantum flavor involved in the process. </span><span class="koboSpan" id="kobo.1426.3" xmlns="http://www.w3.org/1999/xhtml">This is represented as CC in the chart. </span><span class="koboSpan" id="kobo.1426.4" xmlns="http://www.w3.org/1999/xhtml">Since there are no actual quantum computers involved in this approach, we will not study this kind of method.</span></p></li>
<li><p><span class="koboSpan" id="kobo.1427.1" xmlns="http://www.w3.org/1999/xhtml">In addition, we can also </span><span id="dx1-158002"/><span class="koboSpan" id="kobo.1428.1" xmlns="http://www.w3.org/1999/xhtml">consider part of quantum machine learning any classical machine learning algorithms that rely on quantum data; for our purposes, we can just think of it as data generated by quantum processes, or as the application of classical machine learning to quantum computing. </span><span class="koboSpan" id="kobo.1428.2" xmlns="http://www.w3.org/1999/xhtml">This is the QC block in the chart. </span><span class="koboSpan" id="kobo.1428.3" xmlns="http://www.w3.org/1999/xhtml">In this approach, machine learning is a tool rather than an end, so we will not be covering these techniques.</span></p></li>
<li><p><span class="koboSpan" id="kobo.1429.1" xmlns="http://www.w3.org/1999/xhtml">The kind of machine learning that we will focus on in this book is the one represented by the CQ label in the chart: machine learning that relies on classical data and uses quantum computing in the model or the training.</span></p></li>
<li><p><span class="koboSpan" id="kobo.1430.1" xmlns="http://www.w3.org/1999/xhtml">Lastly, there is also a very interesting QQ category. </span><span class="koboSpan" id="kobo.1430.2" xmlns="http://www.w3.org/1999/xhtml">These techniques work on quantum data using quantum computing in the models themselves or in the training processes. </span><span class="koboSpan" id="kobo.1430.3" xmlns="http://www.w3.org/1999/xhtml">Notice that — as opposed to CQ quantum machine learning — in this scenario, the quantum data need not be obtained from measurements: quantum states could be directly fed into a quantum model, for instance. </span><span class="koboSpan" id="kobo.1430.4" xmlns="http://www.w3.org/1999/xhtml">This is an area of great promise (see, for instance, the recent paper by Huang et al. </span><span class="cite"><span class="koboSpan" id="kobo.1431.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xhuang2022quantum"><span class="koboSpan" id="kobo.1432.1" xmlns="http://www.w3.org/1999/xhtml">54</span></a><span class="koboSpan" id="kobo.1433.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.1434.1" xmlns="http://www.w3.org/1999/xhtml">), but the required technologies are still immature, so we will not be talking about this approach in much detail.</span></p></li>
</ul>
<figure>
<span class="koboSpan" id="kobo.1435.1" xmlns="http://www.w3.org/1999/xhtml"><img alt="Figure 8.6: The four big families of quantum machine learning, categorized according to the nature of the models and data that they use " src="../media/file1189.jpg"/></span>
<figcaption aria-hidden="true"><span class="id" id="Figure8.6"><strong><span class="koboSpan" id="kobo.1436.1" xmlns="http://www.w3.org/1999/xhtml">Figure 8.6</span></strong><span class="koboSpan" id="kobo.1437.1" xmlns="http://www.w3.org/1999/xhtml">: </span></span><span class="content"><span class="koboSpan" id="kobo.1438.1" xmlns="http://www.w3.org/1999/xhtml">The four big families of quantum machine learning, categorized according to the nature of the models and data that they use </span></span></figcaption>
</figure>
<p><span class="koboSpan" id="kobo.1439.1" xmlns="http://www.w3.org/1999/xhtml">Our plan, then, is to focus on CQ quantum machine learning: machine learning on classical data that relies on quantum computing. </span><span class="koboSpan" id="kobo.1439.2" xmlns="http://www.w3.org/1999/xhtml">Now, within this category, there is still a fairly broad range of possibilities. </span><span class="koboSpan" id="kobo.1439.3" xmlns="http://www.w3.org/1999/xhtml">We could use quantum computing on the model and also in the optimization process. </span><span class="koboSpan" id="kobo.1439.4" xmlns="http://www.w3.org/1999/xhtml">There are already many interesting proposals for how quantum computing could speed up traditional machine learning models, but these approaches cannot, in general, be used on our current quantum hardware. </span><span class="koboSpan" id="kobo.1439.5" xmlns="http://www.w3.org/1999/xhtml">For this reason, we will not discuss them in this book — but if you are interested in learning more about them, we can recommend the excellent paper by Biamonte et al. </span><span class="cite"><span class="koboSpan" id="kobo.1440.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xbiamonte-qml"><span class="koboSpan" id="kobo.1441.1" xmlns="http://www.w3.org/1999/xhtml">108</span></a><span class="koboSpan" id="kobo.1442.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.1443.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
<p><span class="koboSpan" id="kobo.1444.1" xmlns="http://www.w3.org/1999/xhtml">Instead, we will devote </span><span id="dx1-158005"/><span class="koboSpan" id="kobo.1445.1" xmlns="http://www.w3.org/1999/xhtml">ourselves, heart and soul, to the study of fully quantum-oriented models that can be run on NISQ devices. </span><span class="koboSpan" id="kobo.1445.2" xmlns="http://www.w3.org/1999/xhtml">These models will be trained on classical data and, in general, we will use purely classical optimization techniques.</span></p>
<p><span class="koboSpan" id="kobo.1446.1" xmlns="http://www.w3.org/1999/xhtml">In the following chapters, we will study the following models:</span></p>
<ul>
<li><p><strong><span class="koboSpan" id="kobo.1447.1" xmlns="http://www.w3.org/1999/xhtml">Quantum support vector machines</span></strong><span class="koboSpan" id="kobo.1448.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1448.2" xmlns="http://www.w3.org/1999/xhtml">We will soon explore what support vector machines are and how they can be trained using </span><span id="dx1-158006"/><span class="koboSpan" id="kobo.1449.1" xmlns="http://www.w3.org/1999/xhtml">classical machine learning. </span><span class="koboSpan" id="kobo.1449.2" xmlns="http://www.w3.org/1999/xhtml">We will also see how their quantum version is just a particular case of a general support vector machine in which we use quantum computers to map data into a space of quantum states.</span></p></li>
<li><p><strong><span class="koboSpan" id="kobo.1450.1" xmlns="http://www.w3.org/1999/xhtml">Quantum neural networks</span></strong><span class="koboSpan" id="kobo.1451.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1451.2" xmlns="http://www.w3.org/1999/xhtml">We will then </span><span id="dx1-158007"/><span class="koboSpan" id="kobo.1452.1" xmlns="http://www.w3.org/1999/xhtml">explore a purely quantum model: quantum neural networks. </span><span class="koboSpan" id="kobo.1452.2" xmlns="http://www.w3.org/1999/xhtml">This model runs fully on a quantum computer, and its behavior is inspired by classical neural networks.</span></p></li>
<li><p><strong><span class="koboSpan" id="kobo.1453.1" xmlns="http://www.w3.org/1999/xhtml">Hybrid networks</span></strong><span class="koboSpan" id="kobo.1454.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1454.2" xmlns="http://www.w3.org/1999/xhtml">In the </span><span id="dx1-158008"/><span class="koboSpan" id="kobo.1455.1" xmlns="http://www.w3.org/1999/xhtml">subsequent chapter, we will learn how to combine quantum neural networks with other classical models (most commonly, neural networks). </span><span class="koboSpan" id="kobo.1455.2" xmlns="http://www.w3.org/1999/xhtml">We will refer to these models as hybrid networks.</span></p></li>
<li><p><strong><span class="koboSpan" id="kobo.1456.1" xmlns="http://www.w3.org/1999/xhtml">Quantum generative adversarial networks</span></strong><span class="koboSpan" id="kobo.1457.1" xmlns="http://www.w3.org/1999/xhtml">. </span><span class="koboSpan" id="kobo.1457.2" xmlns="http://www.w3.org/1999/xhtml">Lastly, we </span><span id="dx1-158009"/><span class="koboSpan" id="kobo.1458.1" xmlns="http://www.w3.org/1999/xhtml">will study generative adversarial networks and cover how the components of these models can be replaced by quantum circuits.</span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.1459.1" xmlns="http://www.w3.org/1999/xhtml">As in the rest of this book, our approach will be very hands-on and practical. </span><span class="koboSpan" id="kobo.1459.2" xmlns="http://www.w3.org/1999/xhtml">If you wish to broaden your theoretical background on quantum machine learning, you can also have a look at the book by Maria Schuld and Francesco Petruccione </span><span class="cite"><span class="koboSpan" id="kobo.1460.1" xmlns="http://www.w3.org/1999/xhtml">[</span><a href="ch030.xhtml#Xschuld"><span class="koboSpan" id="kobo.1461.1" xmlns="http://www.w3.org/1999/xhtml">106</span></a><span class="koboSpan" id="kobo.1462.1" xmlns="http://www.w3.org/1999/xhtml">]</span></span><span class="koboSpan" id="kobo.1463.1" xmlns="http://www.w3.org/1999/xhtml">.</span></p>
</section>
<section class="level2 likesectionHead" data-number="16.4" id="summary-7">
<h1 class="likesectionHead" data-number="16.4"><span id="x1-1590008.3"><span class="koboSpan" id="kobo.1464.1" xmlns="http://www.w3.org/1999/xhtml">Summary</span></span></h1>
<p><span id="Q1-1-218"/></p>
<p><span class="koboSpan" id="kobo.1465.1" xmlns="http://www.w3.org/1999/xhtml">In this chapter, we have explored some basic concepts and ideas that lie at the foundation of machine learning. </span><span class="koboSpan" id="kobo.1465.2" xmlns="http://www.w3.org/1999/xhtml">And we haven’t just explored them from a theoretical point of view: we have also seen them come to life.</span></p>
<p><span class="koboSpan" id="kobo.1466.1" xmlns="http://www.w3.org/1999/xhtml">We have learned what machine learning is all about, and we have discussed some of the most common approaches used to make it a reality. </span><span class="koboSpan" id="kobo.1466.2" xmlns="http://www.w3.org/1999/xhtml">In particular, we have learned that many machine learning problems can be reduced to the minimization of a loss function through some optimization algorithm on a suitable model.</span></p>
<p><span class="koboSpan" id="kobo.1467.1" xmlns="http://www.w3.org/1999/xhtml">We have also studied in some depth classical neural networks, and we have used an industry-standard machine learning framework (TensorFlow) to train one.</span></p>
<p><span class="koboSpan" id="kobo.1468.1" xmlns="http://www.w3.org/1999/xhtml">Lastly, we have wrapped up this chapter by introducing what quantum machine learning is all about and having a sneak peek into the rest of the chapters of this part of the book.</span></p>
</section>
</section>
</body>
</html>
