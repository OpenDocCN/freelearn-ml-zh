["```py\n> pip install spark-nlp==1.7.0\n```", "```py\nconf = SparkConf().set(\"spark.jars\", '/opt/anaconda3/lib/python3.6/sitepackages/sparknlp/lib/sparknlp.jar')\n   .setAppName(\"Natural Language Processing - Sentiment Analysis\")\nsc = SparkContext(conf=conf)\n```", "```py\nimport findspark\nfindspark.init()\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import LongType, DoubleType, IntegerType, StringType, BooleanType\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import Tokenizer\nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.ml.feature import HashingTF, IDF\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n\nfrom sparknlp.base import *\nfrom sparknlp.annotator import Tokenizer as NLPTokenizer\nfrom sparknlp.annotator import Stemmer, Normalizer\n```", "```py\nconf = SparkConf().set(\"spark.jars\", '/opt/anaconda3/lib/python3.6/site-packages/sparknlp/lib/sparknlp.jar')\n   .setAppName(\"Natural Language Processing - Sentiment Analysis\")\nsc = SparkContext(conf=conf)\nsqlContext = SQLContext(sc)\nsc.getConf().getAll()\n```", "```py\nairline_tweets_with_labels_df = airline_tweets_df\n   .withColumn(\"negative_sentiment_label\",\n      when(col(\"airline_sentiment\") == \"negative\", lit(\"true\"))\n      .otherwise(lit(\"false\")))\n   .select(\"unit_id\", \"text\", \"negative_sentiment_label\")\n```", "```py\nfiltered_df = airline_tweets_with_labels_df\n   .filter(\"text is not null\")\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens_1\")\ntokenized_df = tokenizer.transform(filtered_df)\nremover = StopWordsRemover(inputCol=\"tokens_1\", \n   outputCol=\"filtered_tokens\")\npreprocessed_part_1_df = remover.transform(tokenized_df)\npreprocessed_part_1_df = preprocessed_part_1_df\n   .withColumn(\"concatenated_filtered_tokens\",\n      concat_ws(\" \", col(\"filtered_tokens\")))\n```", "```py\ndocument_assembler = DocumentAssembler()\n   .setInputCol(\"concatenated_filtered_tokens\")\ntokenizer = NLPTokenizer()\n   .setInputCols([\"document\"]).setOutputCol(\"tokens_2\")\nstemmer = Stemmer().setInputCols([\"tokens_2\"])\n   .setOutputCol(\"stems\")\nnormalizer = Normalizer()\n   .setInputCols([\"stems\"]).setOutputCol(\"normalised_stems\")\npipeline = Pipeline(stages=[document_assembler, tokenizer, stemmer,\n   normalizer])\npipeline_model = pipeline.fit(preprocessed_part_1_df)\npreprocessed_df = pipeline_model.transform(preprocessed_part_1_df)\npreprocessed_df.select(\"unit_id\", \"text\", \n   \"negative_sentiment_label\", \"normalised_stems\")\n```", "```py\nexploded_df = preprocessed_df\n   .withColumn(\"stems\", explode(\"normalised_stems\"))\n   .withColumn(\"stems\", col(\"stems\").getItem(\"result\"))\n   .select(\"unit_id\", \"negative_sentiment_label\", \"text\", \"stems\")\n\naggregated_df = exploded_df.groupBy(\"unit_id\")\n   .agg(concat_ws(\" \", collect_list(col(\"stems\"))), \n      first(\"text\"), first(\"negative_sentiment_label\"))\n   .toDF(\"unit_id\", \"tokens\", \"text\", \"negative_sentiment_label\")\n   .withColumn(\"tokens\", split(col(\"tokens\"), \" \")\n      .cast(\"array<string>\"))\n```", "```py\nhashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\",\n   numFeatures=280)\nfeatures_df = hashingTF.transform(aggregated_df)\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\nidf_model = idf.fit(features_df)\nscaled_features_df = idf_model.transform(features_df)\n```", "```py\nindexer = StringIndexer(inputCol = \"negative_sentiment_label\", \n   outputCol = \"label\").fit(scaled_features_df)\nscaled_features_indexed_label_df = indexer.transform(scaled_features_df)\n```", "```py\ntrain_df, test_df = scaled_features_indexed_label_df\n   .randomSplit([0.9, 0.1], seed=12345)\n```", "```py\ndecision_tree = DecisionTreeClassifier(featuresCol = 'features',\n   labelCol = 'label')\ndecision_tree_model = decision_tree.fit(train_df)\n```", "```py\ntest_decision_tree_predictions_df = decision_tree_model\n   .transform(test_df)\nprint(\"TEST DATASET PREDICTIONS AGAINST ACTUAL LABEL: \")\ntest_decision_tree_predictions_df.select(\"prediction\", \"label\",\n   \"text\").show(10, False)\n```", "```py\npredictions_and_label = test_decision_tree_predictions_df\n   .select(\"prediction\", \"label\").rdd\nmetrics = MulticlassMetrics(predictions_and_label)\nprint(\"N = %g\" % test_decision_tree_predictions_df.count())\nprint(metrics.confusionMatrix())\n```", "```py\n# Create Training and Test DataFrames based on the Bag of Words Feature Vectors\nbow_indexer = StringIndexer(inputCol = \"negative_sentiment_label\",\n   outputCol = \"label\").fit(features_df)\nbow_features_indexed_label_df = bow_indexer.transform(features_df)\n   .withColumnRenamed(\"raw_features\", \"features\")\nbow_train_df, bow_test_df = bow_features_indexed_label_df\n   .randomSplit([0.9, 0.1], seed=12345)\n\n# Train a Decision Tree Classifier using the Bag of Words Feature Vectors\nbow_decision_tree = DecisionTreeClassifier(featuresCol = \n   'features', labelCol = 'label')\nbow_decision_tree_model = bow_decision_tree.fit(bow_train_df)\n\n# Apply the Bag of Words Decision Tree Classifier to the Test DataFrame and generate the Confusion Matrix\nbow_test_decision_tree_predictions_df = bow_decision_tree_model\n   .transform(bow_test_df)\nbow_predictions_and_label = bow_test_decision_tree_predictions_df\n   .select(\"prediction\", \"label\").rdd\nbow_metrics = MulticlassMetrics(bow_predictions_and_label)\nprint(\"N = %g\" % bow_test_decision_tree_predictions_df.count())\nprint(bow_metrics.confusionMatrix())\n```", "```py\nbow_decision_tree_model.save('<Target filesystem path to save MLlib Model>')\n```"]