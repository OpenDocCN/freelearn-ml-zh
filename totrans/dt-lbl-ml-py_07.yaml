- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labeling Text Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore techniques for labeling text data for classification
    in cases where an insufficient amount of labeled data is available. We are going
    to use Generative AI to label the text data, in addition to Snorkel and k-means
    clustering. The chapter focuses on the essential process of annotating textual
    data for NLP and text analysis. It aims to provide readers with practical knowledge
    and insights into various labeling techniques. The chapter will specifically cover
    automatic labeling using OpenAI, rule-based labeling using Snorkel labeling functions,
    and unsupervised learning using k-means clustering. By understanding these techniques,
    readers will be equipped to effectively label text data and extract meaningful
    insights from unstructured textual information.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following sections in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Real-world applications of text data labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools and frameworks for text data labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploratory data analysis of text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI and OpenAI for labeling text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labeling text data using Snorkel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labeling text data using logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labeling text data using K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labeling customer reviews (sentiment analysis) using neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code files used in this chapter are located at [https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch07](https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch07).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gutenberg Corpus and movie review dataset can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pypi.org/project/Gutenberg/](https://pypi.org/project/Gutenberg/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.nltk.org/api/nltk.sentiment.util.html?highlight=movie#nltk.sentiment.util.demo_movie_reviews](https://www.nltk.org/api/nltk.sentiment.util.html?highlight=movie#nltk.sentiment.util.demo_movie_reviews)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You also need to create an Azure account and add the OpenAI resource for working
    with Generative AI. To sign up for a free Azure subscription, visit https://azure.microsoft.com/free.
    To request access to the Azure OpenAI service, visit [https://aka.ms/oaiapply](https://aka.ms/oaiapply).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have provisioned the Azure OpenAI service, set up the following environment
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Your endpoint should look like [https://YOUR_RESOURCE_NAME.openai.azure.com/](https://YOUR_RESOURCE_NAME.openai.azure.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Real-world applications of text data labeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Text data labeling or classification is widely used across various industries
    and applications to extract valuable information, automate processes, and improve
    decision-making. Here are some real-world examples across different use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Customer support ticket classification:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case: Companies receive a large volume of customer support tickets.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: Automated classification of support tickets into categories such
    as Billing, Technical Support, and Product Inquiry. This helps prioritize and
    route tickets to the right teams.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spam email filtering:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case: Sorting emails into spam and non-spam categories.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: Email providers use text classification to identify and filter
    out unwanted emails, providing users with a cleaner inbox and reducing the risk
    of phishing attacks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentiment analysis in social media:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case: Analyzing social media comments and posts.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: Brands use sentiment analysis to gauge public opinion, track brand
    sentiment, and respond to customer feedback. It helps with reputation management
    and understanding customer preferences.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'News categorization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case: Sorting news articles into categories.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: News websites use text classification to automatically categorize
    articles into sections such as Politics, Technology, and Entertainment, making
    it easier for readers to find relevant content.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resume screening:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case: Sorting job applications.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: Human resources departments use text classification to quickly
    identify resumes that match specific job requirements. This accelerates the hiring
    process and ensures a more efficient candidate screening.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Medical document classification:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case: Sorting medical records and documents.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: Healthcare organizations use text classification to categorize
    and organize medical records, lab reports, and patient notes. This aids in efficient
    data retrieval and analysis.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Legal document classification:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case: Sorting legal documents.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: Law firms use text classification to categorize and manage legal
    documents, contracts, and case-related information, streamlining legal research
    and case management.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fraud detection in financial transactions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case: Identifying fraudulent activity.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: Financial institutions use text classification to analyze transaction
    descriptions and identify potential cases of fraud or suspicious activities, enhancing
    security measures.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Product review analysis:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case: Analyzing customer reviews.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: E-commerce platforms use sentiment analysis to categorize and
    understand product reviews. This helps in improving products, addressing customer
    concerns, and enhancing overall customer satisfaction.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language identification:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case: Determining the language of a given text.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: Social media platforms and translation services use text classification
    to automatically identify the language of a user’s post or content, enabling accurate
    language-specific interactions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These examples highlight the versatility of text classification across different
    domains, showcasing its significance in automating tasks, improving efficiency,
    and gaining valuable insights from textual data.
  prefs: []
  type: TYPE_NORMAL
- en: Tools and frameworks for text data labeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several open source tools and frameworks available for text data
    analysis and labeling. Here are some popular ones, along with their pros and cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tools** **and frameworks** | **Pros** | **Cons** |'
  prefs: []
  type: TYPE_TB
- en: '| **Natural Language** **Toolkit** (**NLTK**) | Comprehensive library for NLP
    tasks.Rich set of tools for tokenization, stemming, tagging, parsing, and more.Active
    community support.Suitable for educational purposes and research projects. | Some
    components may not be as efficient for large-scale industrial applications.Steep
    learning curve for beginners. |'
  prefs: []
  type: TYPE_TB
- en: '| spaCy | Fast and efficient, designed for production use.Pre-trained models
    for various languages.Provides robust support for tokenization, named entity recognition,
    and dependency parsing.Easy-to-use API. | Less emphasis on educational resources
    compared to NLTK.Limited support for some languages. |'
  prefs: []
  type: TYPE_TB
- en: '| scikit-learn | General-purpose machine learning library with excellent text
    processing capabilities.Easy integration with other scikit-learn modules for feature
    extraction and model training.Well-documented and widely used in the machine learning
    community. | May not have specialized tools for certain NLP tasks.Limited support
    for deep learning-based models. |'
  prefs: []
  type: TYPE_TB
- en: '| TextBlob | Simple API for common NLP tasks such as part-of-speech tagging,
    noun phrase extraction, and sentiment analysis.Built on NLTK and provides an easy
    entry point for beginners.Useful for quick prototyping and small projects. | Limited
    customization options compared to lower-level libraries.May not be as performant
    for large-scale applications. |'
  prefs: []
  type: TYPE_TB
- en: '| Gensim | Focus on topic modeling, document similarity, and vector space modeling.Efficient
    implementation of algorithms such as Word2Vec.Suitable for large text corpora
    and document similarity tasks. | Less versatile for general-purpose NLP tasks.Limited
    support for some advanced NLP functionalities. |'
  prefs: []
  type: TYPE_TB
- en: '| Transformers (Hugging Face) | Provides pre-trained models for a wide range
    of NLP tasks (BERT, GPT, etc.).Easy-to-use interfaces for integrating state-of-the-art
    models.Excellent community support. | Heavy computational requirements for fine-tuning
    large models.May not be as straightforward for beginners. |'
  prefs: []
  type: TYPE_TB
- en: '| Stanford NLP | Comprehensive suite of NLP tools, including tokenization,
    part-of-speech tagging, and named entity recognition.Java-based, making it suitable
    for Java projects. | Heavier resource usage compared to Python-based libraries.May
    have a steeper learning curve for certain tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| Flair | Focus on state-of-the-art NLP models and embeddings.Provides embeddings
    for a variety of languages.Easy-to-use API. | May not have as many pre-built models
    as other libraries.May not be as established as some older frameworks. |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Popular tools with their pros and cons
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this list is OpenAI’s **Generative Pre-trained Transformer**
    (**GPT**), which is a state-of-the-art language model that utilizes transformer
    architecture. It’s pre-trained on a massive amount of diverse data and can be
    fine-tuned for specific tasks. GPT is known for its ability to generate coherent
    and contextually relevant text, making it a powerful tool for various **natural
    language processing** (**NLP**) applications.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer architecture, introduced by Vaswani et al. in the paper *Attention
    is All You Need*, revolutionized NLP. It relies on self-attention mechanisms to
    capture contextual relationships between words in a sequence, enabling parallelization
    and scalability. Transformers have become the foundation of numerous advanced
    language models, including GPT and BERT, due to their ability to capture long-range
    dependencies in sequential data efficiently. Its pros include versatility and
    the ability to understand context in text which is why it is used for various
    natural language understanding tasks. Its cons are that it is resource-intensive,
    requiring substantial computing power, and fine-tuning requires access to significant
    computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these tools has strengths and weaknesses, and the choice depends on
    project requirements, available resources, and the desired level of customization.
    It’s common to see a combination of these tools being used together in more complex
    NLP pipelines. When selecting a tool, it’s important to consider factors such
    as ease of use, community support, and compatibility with the specific tasks at
    hand.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis of text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Exploratory Data Analysis** (**EDA**) is a crucial step in any data science
    project. When it comes to text data, EDA can help us understand the structure
    and characteristics of the data, identify potential issues or inconsistencies,
    and inform our choice of data preprocessing and modeling techniques. In this section,
    we will walk through the steps involved in performing EDA on text data.'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in EDA is to load the text data into our environment. Text data
    can come in many formats, including plain text files, CSV files, or database tables.
    Once we have the data loaded, we can begin to explore its structure and content.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step in EDA is to gain an understanding of the data. For text data,
    this may involve examining the size of the dataset, the number of documents or
    samples, and the overall structure of the text (e.g., whether it is structured
    or unstructured). We can use descriptive statistics to gain insights into the
    data, such as the distribution of text lengths or the frequency of certain words
    or phrases.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and preprocessing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After understanding the data, the next step in EDA is to clean and preprocess
    the text data. This can involve a number of steps, such as removing punctuation
    and stop words, stemming or lemmatizing words, and converting text to lowercase.
    Cleaning and preprocessing the data is important for preparing the data for modeling
    and ensuring that we are working with high-quality data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the text’s content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have cleaned and preprocessed the data, we can begin to explore the
    content of the text itself. This can involve examining the most frequent words
    or phrases, identifying patterns or themes in the text, and visualizing the data
    using techniques such as word clouds or frequency histograms. We can also use
    NLP techniques to extract features from the text, such as named entities, part-of-speech
    tags, or sentiment scores.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing relationships between text and other variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, we may want to explore the relationships between the text data
    and other variables, such as demographic or behavioral data. For example, we may
    want to examine whether the sentiment of movie reviews varies by genre, or whether
    the topics discussed in social media posts differ by user age or location. This
    type of analysis can help us gain deeper insights into the text data and inform
    our modeling approach.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we can visualize the results of our EDA using a variety of techniques,
    such as word clouds, bar charts, scatterplots, or heat maps. Visualization is
    an important tool for communicating insights and findings to stakeholders, and
    can help us identify patterns and relationships in the data that might not be
    immediately apparent from the raw text.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, exploratory data analysis is a critical step in any text data
    project. By understanding the structure and content of the data, cleaning and
    preprocessing it, exploring the text’s content, analyzing relationships between
    text and other variables, and visualizing the results, we can gain deep insights
    into the textual data and inform our modeling approach. With the right tools and
    techniques, EDA can help us uncover hidden patterns and insights in text data
    that can be used to drive business decisions and improve outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis of sample text data set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here’s an example Python code for performing EDA on a text dataset. We will
    be using the Gutenberg corpus ([https://pypi.org/project/Gutenberg/](https://pypi.org/project/Gutenberg/)),
    which is a publicly available collection of over 60,000 electronic books.
  prefs: []
  type: TYPE_NORMAL
- en: The NLTK corpus is a collection of publicly available datasets for NLP research
    and development. The Gutenberg corpus ([https://www.nltk.org/book/ch02.html](https://www.nltk.org/book/ch02.html)),
    which is one of the datasets included in NLTK, specifically contains a selection
    of public domain texts from Project Gutenberg. Project Gutenberg is a digital
    library that offers free access to books and other texts that are no longer protected
    by copyright.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the Gutenberg corpus within the NLTK is based on public domain texts,
    making it a publicly available dataset. It can be used for various NLP tasks,
    such as text classification, language modeling, and information retrieval, without
    any commercial restrictions or licensing requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s download the Gutenberg corpus using the NLTK library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s load the text data into a Pandas DataFrame by iterating the fields from
    Gutenberg and appending documents to the list data. Then we’ll convert the list
    data to dataframe, `df`, with a single column, `text`, to store the document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the dataframe’s size by calling the `shape` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – The first few rows of data](img/B18944_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – The first few rows of data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check the length of each document by calling the `apply` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Distribution of document length](img/B18944_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Distribution of document length
  prefs: []
  type: TYPE_NORMAL
- en: 'In text analysis, removing stopwords and punctuation is one of the most common
    tasks because stopwords do not tell us anything about the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the stopwords list from the NLTK corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s count the frequency of words in the clean text using the `value_counts`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, plot a bar chart to visualize the most frequent words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Most frequent words](img/B18944_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Most frequent words
  prefs: []
  type: TYPE_NORMAL
- en: In this code, we first downloaded the Gutenberg corpus using the NLTK library.
    We then loaded the text data into a Pandas DataFrame and performed some initial
    checks on the size and structure of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we calculated the length of each document and visualized the distribution
    of document lengths using a histogram. We then removed punctuation and stop words
    from the text data and calculated the frequency of each word. We visualized the
    most frequent words using a bar chart.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this code is just a basic example of EDA on text data, and you may
    need to modify it to suit your specific dataset and research question. Now we
    have clean text data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how to use Generative AI to label text data in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Generative AI and OpenAI for labeling text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI refers to a category of artificial intelligence that involves
    training models to generate new content or data based on patterns and information
    present in the training data. OpenAI is a prominent organization that has developed
    and released powerful generative models for various NLP tasks. One of the notable
    models is GPT, such as GPT-3, GPT-3.5, and GPT-4\. These models have been influential
    in the fields of text data labeling and classification.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI focuses on training models to generate new data instances that
    resemble existing examples. It is often used for tasks such as text generation,
    image synthesis, and more. Generative models are trained on large datasets to
    learn underlying patterns, allowing them to generate coherent and contextually
    relevant content. In text-related tasks, generative AI can be applied to text
    completion, summarization, question answering, and even creative writing. Let’s
    take a look at some key concepts that will help us with labeling text data.
  prefs: []
  type: TYPE_NORMAL
- en: GPT models by OpenAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI has developed a series of sophisticated language models, with GPT-4 being
    among the most advanced. These models undergo pre-training on diverse datasets,
    enabling them to excel in various natural language understanding and generation
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot learning capabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT models are renowned for their zero-shot learning capabilities, enabling
    them to make predictions or generate content for tasks they were not explicitly
    trained on. This versatility enhances their applicability across diverse domains.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification with OpenAI models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Leveraging the language understanding and generation capabilities of OpenAI
    models, they can be effectively utilized for text classification tasks. This includes
    sentiment analysis, topic categorization, and other classification-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: Data labeling assistance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although GPT models are not specifically designed for traditional data labeling
    tasks, they can offer assistance in generating labeled data. This can be achieved
    through natural language instructions or by providing context that aids in making
    labeling decisions.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI API overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The OpenAI API is a service provided by OpenAI that allows users to access their
    advanced language models through an API. It serves as a gateway for integrating
    OpenAI’s language capabilities into various applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the pros and cons of OpenAI’s GPT models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Versatility: OpenAI’s GPT models are versatile and can be adapted for various
    text-related tasks, including data labeling and classification'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Large scale: These models are trained on massive amounts of data, enabling
    them to capture intricate patterns and nuances present in natural language'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpretability: The generated content might lack interpretability, making
    it challenging to understand the model’s decision-making process'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resource intensive: Training and using large generative models such as GPT-4
    can be computationally expensive'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, OpenAI’s generative models, particularly GPT-3 , GPT-3.5, and GPT-4,
    have made significant contributions to the field of text data processing, and
    they can be used creatively for tasks such as data labeling and classification
    by utilizing their language-understanding capabilities. However, careful consideration
    and evaluation are needed, especially regarding ethical concerns and potential
    bias in generated content.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of language processing, text classification serves to categorize
    documents based on their content. Traditionally, this task relied on labeled training
    data; however, advanced models such as OpenAI’s GPT have revolutionized the process
    by autonomously generating labels with the assistance of explicit instructions
    or **prompts**.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring text data labeling with **Azure OpenAI**, a collaborative initiative
    within Microsoft Azure’s cloud, unlocks the potential of powerful language models.
    This section acts as a guide, facilitating efficient text data labeling by harnessing
    the capabilities of Generative AI and OpenAI models, and providing users with
    custom tools for typical tasks in text data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at some use cases with Python and Azure OpenAI for text data
    labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Use case 1 – summarizing the text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Summarization is a crucial NLP task that involves condensing a piece of text
    while retaining its essential information and main ideas. In the context of Azure
    OpenAI, the following code exemplifies the application of summarization using
    the GPT-3.5-turbo model deployed on the Azure platform.
  prefs: []
  type: TYPE_NORMAL
- en: The following code example begins by setting the necessary environment variables
    for the Azure OpenAI API, including the API key and endpoint. The OpenAI API is
    then configured with the deployment name of the model, allowing the code to interact
    with the specific GPT-3.5-turbo instance.
  prefs: []
  type: TYPE_NORMAL
- en: The input text, which is a detailed description of Dachepalli, a town in Andhra
    Pradesh, India, is provided for summarization. The code utilizes the Azure OpenAI
    Completion API to generate a summary, employing parameters such as temperature,
    max tokens, and penalties for frequency and presence.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the code includes the generated summary, showcasing the main ideas
    extracted from the input text. The summarized content emphasizes key aspects such
    as the author’s connection to Dachepalli, the town’s features, and notable historical
    events. This example demonstrates how Azure OpenAI can effectively summarize information,
    providing concise and informative outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the required libraries and getting the configuration
    values (the Azure OpenAI key and endpoint, API version, and the GPT model deployment
    name) that we have set already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let’s understand the parameters used in this OpenAI completion API.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI’s parameters control the behavior of the language model during text
    generation. Here’s a brief description of the provided parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Temperature (`temperature=0`): It determines the randomness of the model’s
    output. A high value (e.g., `0.8`) makes the output more diverse, while a low
    value (e.g., `0.2`) makes it more deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Max tokens (`max_tokens=118`): This specifies the maximum number of tokens
    (words or characters) to generate in the output. It’s useful for limiting response
    length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Top P (`top_p=1`): Also known as nucleus sampling, it controls the diversity
    of the generated output. Setting it to `1` ensures that only the top probability
    tokens are considered during sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frequency penalty (`frequency_penalty=0`): This discourages the repetition
    of specific tokens in the output. A non-zero value penalizes the model for choosing
    frequently occurring tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Presence Penalty (`presence_penalty=0`): Similar to frequency penalty, presence
    penalty discourages the repetition of entire phrases or concepts, promoting more
    diverse responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stop (`stop=None`): This allows users to specify a custom stopping criterion
    for generation. When the model encounters the specified token, it stops generating
    further content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These parameters provide users with fine-grained control over the generation
    process, allowing customization of the model’s output based on factors such as
    randomness, length, diversity, and repetition. Adjusting these parameters enables
    users to tailor the language model’s behavior to meet specific requirements in
    various applications, such as chatbots, content generation, and more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code will output the following summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We have seen how to generate a summary using the OpenAI GPT-3.5 model. Now let’s
    see how to generate the topic for news articles using OpenAI’s GPT model.
  prefs: []
  type: TYPE_NORMAL
- en: Use case 2 – topic generation for news articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s explore generating topic names for news articles using a generative model,
    specifically, Azure OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Topic generation is a powerful application of NLP that involves creating relevant
    and coherent content based on a given prompt. In the context of Azure OpenAI prompts,
    the ability to generate topics is demonstrated using a news headline classification
    example.
  prefs: []
  type: TYPE_NORMAL
- en: In this code snippet, the task is to categorize a news headline into one of
    the predefined categories, which are Business, Tech, Politics, Sport, and Entertainment.
    The news headline, provided as input, is *“Trump is ready to contest in Nov 2024
    elections.”* The code uses the Azure OpenAI API to generate a response that predicts
    the most appropriate category for the given headline.
  prefs: []
  type: TYPE_NORMAL
- en: The completion engine is configured with specific parameters, such as temperature,
    max tokens, and penalties for frequency and presence. After generating the response,
    the code extracts and prints the predicted category from the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example showcases how Azure OpenAI prompts can be utilized for the automatic
    categorization of news headlines, demonstrating the versatility and effectiveness
    of NLP in topic-generation tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Use case 3 – classification of customer queries using the user-defined categories
    and sub-categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s see how to classify the customer queries into user-defined categories
    and sub-categories using **Azure OpenAI**.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification is a fundamental NLP task that involves assigning predefined
    categories to textual input. In the provided code, a customer support system utilizes
    text classification to categorize customer queries related to their orders. The
    system employs user-defined primary and secondary categories, each with specific
    sub-categories.
  prefs: []
  type: TYPE_NORMAL
- en: The system message serves as a guide for the classification task, outlining
    the primary categories (Order Status, Product Inquiries, Shipping and Delivery,
    and Payment Assistance) and their corresponding secondary categories. The primary
    and secondary categories are structured to capture various aspects of customer
    queries, such as tracking information, product availability, and payment confirmation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when a user submits a query to cancel an order, the code uses the
    OpenAI ChatCompletion API to generate a response. The output includes a JSON-formatted
    response indicating the primary and secondary categories assigned to the user’s
    query. In this case, the primary category is Order Status, and the secondary category
    is Order Modification or Cancellation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example demonstrates how text classification can be applied in a customer
    support context, allowing for the efficient handling and categorization of customer
    queries based on predefined categories. The system provides a structured approach
    to address diverse aspects of order-related inquiries, enhancing the overall customer
    support experience:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Use case 4 – information retrieval using entity extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us see how to extract the entity names from the text data using Azure OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Entity extraction is a vital aspect of NLP, involving the identification and
    extraction of specific entities, such as names, organizations, locations, and
    contact numbers, from a given text. In the presented code snippet, the task is
    to identify and extract people’s names, organization names, geographical locations,
    and contact numbers from various text passages.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt provides clear instructions for the entity extraction task, specifying
    the entities of interest and their corresponding categories. It includes examples
    that illustrate how to extract information from different texts, showcasing the
    versatility of the entity extraction process.
  prefs: []
  type: TYPE_NORMAL
- en: The code utilizes the OpenAI API to generate responses that include extracted
    entities, such as people’s names, organization names, locations, and contact numbers,
    from the given text passages. The output is structured in a JSON format, making
    it easy to parse and integrate the extracted entities into further processing
    or analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example demonstrates the practical application of entity extraction for
    extracting relevant information from diverse textual data, showcasing its potential
    in various domains, such as customer relationship management, information retrieval,
    and data analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s extract the required information name, organization, location, and
    contact information from the output JSON, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Use case 5 – aspect-based sentiment analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sentiment aspect analysis is a sophisticated NLP task that involves evaluating
    the sentiment expressed towards specific aspects or features within a given text.
    In the provided code snippet, aspect-based sentiment analysis is conducted on
    product reviews, aiming to assess both the overall sentiment of the reviews and
    the sentiment polarity associated with individual aspects mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt outlines the objectives of the sentiment analysis task, which include
    providing an overall sentiment score for each review on a scale from 0 to 5, assigning
    sentiment polarity scores between 0 and 5 for each aspect, and identifying the
    top positive and negative aspects.
  prefs: []
  type: TYPE_NORMAL
- en: The code processes multiple product reviews, extracting sentiments associated
    with aspects such as camera quality, battery life, design, speaker quality, performance,
    keyboard, display, trackpad responsiveness, sound quality, touch controls, graphics,
    load times, online community, subscription fee, and controller.
  prefs: []
  type: TYPE_NORMAL
- en: The output includes comprehensive sentiment scores, polarity scores, and the
    identification of the most positively and negatively rated aspects in each review.
    This example illustrates how aspect-based sentiment analysis can provide detailed
    insights into the nuanced opinions expressed in diverse reviews, assisting businesses
    in understanding customer sentiments towards specific product features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the code example for aspect-based sentiment analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s use the Snorkel API to classify this text data and generate labels
    by creating rule-based labeling functions.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on labeling of text data using the Snorkel API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to learn how to label text data using the Snorkel
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Snorkel provides an API for programmatically labeling text data using a small
    set of ground truth labels that are created by domain experts. Snorkel, an open
    source data labeling and training platform, is used by various companies and organizations
    across different industries, such as Google, Apple, Facebook, IBM, and SAP.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has unique features that differentiate it from other competitors, especially
    in the context of weak supervision and programmatically generating labeled data.
    Here’s a comparison with some of the other tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weak supervision**: Snorkel excels in scenarios where labeled data is scarce,
    and manual labeling is expensive. It allows users to programmatically label large
    amounts of data using heuristics, patterns, and external resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible labeling functions**: Snorkel enables the creation of labeling functions,
    which are essentially heuristic functions that assign labels to data. This provides
    a flexible and scalable way to generate labeled data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probabilistic labeling**: Snorkel generates probabilistic labels, acknowledging
    that labeling functions may have varying levels of accuracy. This probabilistic
    framework is useful in downstream tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be a learning curve with Snorkel, especially for users who are new
    to weak supervision concepts. Other tools, such as Prodigy and Labelbox, are commercial
    tools and may involve licensing costs.
  prefs: []
  type: TYPE_NORMAL
- en: When choosing between these tools, the specific requirements of the project,
    the available budget, and the expertise of the users play crucial roles. Snorkel
    stands out when weak supervision and programmatically generated labels are essential
    for the task at hand. It’s particularly well suited for scenarios where manual
    labeling is impractical or cost-prohibitive. Other tools may be more appropriate
    based on different use cases, interface preferences, and integration requirements.
  prefs: []
  type: TYPE_NORMAL
- en: We will create rule-based labeling functions using Snorkel and then apply these
    labeling functions to classify and label text.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen what a labeling function is and how to create labeling functions
    in [*Chapter 2*](B18944_02.xhtml#_idTextAnchor043). Let’s recap. In Snorkel, a
    labeling function is a Python function that heuristically generates labels for
    a dataset. These functions are used in the process of weak supervision, where
    instead of relying solely on manually labeled data, a machine learning model is
    trained using noisy, imperfect, or weakly labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example Python code that uses the Snorkel API to label text data
    using rule-based labeling functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s install Snorkel using pip and import the required Python libraries for
    labeling as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Let’s break down the code into four steps and explain each one.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1*: Data preparation and labeling function definition. This step prepares
    the data and defines the labeling functions. It first imports the Pandas library
    and defines some constants for the labels. It then creates a DataFrame with movie
    reviews and splits it into a training set and a test set. The true labels for
    the test set are defined and converted to a NumPy array. Finally, it defines three
    labeling functions that label a review as positive, negative, or abstain based
    on the presence of certain words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s define the labeling functions, one for positive reviews, one for
    negative reviews, and one for neutral reviews, using regular expressions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 2*: Applying labeling functions and majority voting. This chunk of code
    applies the labeling functions to the training and test sets, and then uses a
    majority vote model to predict the labels. It first creates a list of the labeling
    functions and applies them to the training and test sets using `PandasLFApplier`.
    It then prints the resulting label matrices and their shapes. It imports the `MajorityLabelVoter`
    and `LabelModel` classes from Snorkel, creates a majority vote mode, and uses
    it to predict the labels for the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.\uFEFF4 – Label matrices](img/B18944_07_04.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Label matrices
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the accuracy of the model using `MajorityLabelVoter` model
    on the test set and print it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it predicts the labels for the training set and prints them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 3*: Training a label model and predicting labels. This chunk of code
    trains a label model and uses it to predict the labels. It creates a `LabelModel`
    with a `cardinality` of `2` (for the two labels, positive and negative), fits
    it to the training set, and calculates its accuracy on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.\uFEFF5 – Training a LabelModel](img/B18944_07_05.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Training a LabelModel
  prefs: []
  type: TYPE_NORMAL
- en: 'It then predicts the labels for the training set and prints them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 4*: Analyzing labeling functions and creating a DataFrame with predicted
    labels. We can use the `LFAnalysis` class to analyze the labeling functions by
    passing the labels (`L`) and the list of labeling functions (`lfs`). The `lf_summary()`
    method provides an overview of the labeling functions and their coverage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.\uFEFF6 – LFAnalysis summary](img/B18944_07_06.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – LFAnalysis summary
  prefs: []
  type: TYPE_NORMAL
- en: 'The table is a summary of the results from LFAnalysis, specifically for three
    labeling functions: `lf_positive_review`, `lf_negative_review`, and `if_neutral_review`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`j`: The index of the labeling function in the list of labeling functions.
    Here, `j=0` corresponds to `lf_positive_review`, and `j=1` corresponds to `lf_negative_review`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Polarity`: The polarity assigned to the labeling function, representing the
    label value assigned by the function. In this case, `lf_positive_review` has a
    polarity of `[0, 1]`, meaning it assigns both label `0` and label `1`. On the
    other hand, `lf_negative_review` has a polarity of `[0]`, indicating it only assigns
    label `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Coverage`: The set of labels predicted by the labeling function. For `lf_positive_review`,
    it predicts both label `0` and label `1` (`[0, 1]`), indicating it provides a
    non-abstain output for all examples. However, `lf_negative_review` predicts only
    label `0` (`[0]`), meaning it provides a non-abstain output for only 55.25% of
    the examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Overlaps`: The percentage of examples for which the labeling function provides
    a non-abstain output. It represents the extent to which the labeling function
    is applicable. In this case, both `lf_positive_review` and `lf_negative_review`
    have a coverage of 0.5525, indicating that they provide a non-abstain label for
    55.25% of the examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Conflicts`: The percentage of examples for which the labeling function disagrees
    with at least one other labeling function. It measures the level of conflict between
    the labeling function and other functions. Both `lf_positive_review` and `lf_negative_review`
    have a conflict value of 0.2105, indicating they have conflicts with other labeling
    functions in approximately 21.05% of the examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This summary provides insights into the performance, coverage, and conflicts
    of the labeling functions, allowing you to assess their effectiveness and identify
    areas of improvement in your labeling process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, the following chunk of code analyzes the labeling functions and creates
    a DataFrame with the predicted labels. It uses the `LFAnalysis` class from Snorkel
    to analyze the labeling functions and print a summary. It then creates a DataFrame
    with the predicted labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.\uFEFF7 – Predicted labels](img/B18944_07_07.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Predicted labels
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we first created the `Movie Reviews` DataFrame. We then defined
    three rule-based labeling functions using regular expressions to label reviews
    as positive, negative, or neutral based on the presence of certain keywords. We
    applied these labeling functions to the text data using the `PandasLFApplier`
    provided by the Snorkel API. Finally, we analyzed the labeled data using `LFAnalysis`
    and printed a summary of the results.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is a simple example and you may need to adjust the code depending
    on the specific requirements of your use case. Also, you can add more labeling
    functions depending on your task, and these functions should be carefully designed
    and tested to ensure high-quality labels.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look into labeling the data using logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on text labeling using Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text labeling is a crucial task in NLP, enabling the categorization of textual
    data into predefined classes or sentiments. Logistic Regression, a popular machine
    learning algorithm, proves effective in text classification scenarios. In the
    following code, we walk through the process of using Logistic Regression to classify
    movie reviews into positive or negative sentiments. Here’s a breakdown of the
    code.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1*. Import necessary libraries and modules.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code begins by importing the necessary libraries and modules. These include
    NLTK for NLP, scikit-learn for machine learning, and specific modules for sentiment
    analysis, text preprocessing, and classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 2*. Download the necessary NLTK data. The code downloads the movie reviews
    dataset and other necessary NLTK data, such as the WordNet lemmatizer and the
    Punkt tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 3*. Initialize the sentiment analyzer and get movie review IDs. The code
    initializes a sentiment analyzer and gets the IDs of the movie reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 4*. Preprocessing setup. The code sets up the preprocessing tools, including
    a lemmatizer and a list of English stopwords. It also defines a preprocessing
    function that tokenizes the text, removes stop words, and lemmatizes the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 5*. Feature extraction. The code sets up a TF-IDF vectorizer with the
    preprocessing function and uses it to transform the movie reviews into a feature
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 6*. Create a target vector. The code creates a target vector with the
    categories of the movie reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 7*. Split the data. The code splits the data into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 8*. Model training. The code initializes a Logistic Regression classifier
    and trains it on the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 9*. Model evaluation. The code evaluates the model on the test data and
    prints the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.\uFEFF8 – Accuracy of  logistic regression](img/B18944_07_08.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Accuracy of logistic regression
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 10*. Testing with custom sentences. The code tests the model with custom
    sentences. It preprocesses the sentences, transforms them into features, predicts
    their sentiment, and prints the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.\uFEFF9 – Predicted labels](img/B18944_07_09.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Predicted labels
  prefs: []
  type: TYPE_NORMAL
- en: This code serves as a comprehensive guide to text labeling using logistic regression,
    encompassing data preprocessing, model training, evaluation, and application to
    custom sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look into the second method, K-means clustering, to label the text
    data by grouping similar text together and creating labels for that group or cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on label prediction using K-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-means clustering is a powerful unsupervised machine learning technique used
    for grouping similar data points into clusters. In the context of text data, K-means
    clustering can be employed to predict labels or categories for the given text
    based on their similarity. The provided code showcases how to utilize K-Means
    clustering to predict labels for movie reviews, breaking down the process into
    several key steps.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1*: Importing libraries and downloading data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code begins by importing essential libraries such as scikit-learn
    and NLTK. It then downloads the necessary NLTK data, including the movie reviews
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 2*: Retrieving and preprocessing movie reviews.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrieve movie reviews from the NLTK dataset and preprocess them. This involves
    lemmatization, removal of stop words, and converting text to lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 3*: Creating the TF-IDF vectorizer and transforming data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a TF-IDF vectorizer to convert the preprocessed reviews into numerical
    features. This step is crucial for preparing the data for clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 4*: Applying K-means clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply K-means clustering to the TF-IDF features, specifying the number of clusters.
    In this case, the code sets `n_clusters=3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 5*: Labeling and testing with custom sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define labels for the clusters and test the K-means classifier with custom
    sentences. The code preprocesses the sentences, transforms them into TF-IDF features,
    predicts the cluster, and assigns a label based on the predefined cluster labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.\uFEFF10 – K-means clustering for text](img/B18944_07_10.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – K-means clustering for text
  prefs: []
  type: TYPE_NORMAL
- en: This code demonstrates a comprehensive process of utilizing K-means clustering
    for text label prediction, covering data preprocessing, feature extraction, clustering,
    and testing with custom sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Generating labels for customer reviews (sentiment analysis)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Customer reviews are a goldmine of information for businesses. Analyzing sentiment
    in customer reviews helps in understanding customer satisfaction, identifying
    areas for improvement, and making data-driven business decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we delve into sentiment analysis using a neural network
    model. The code utilizes TensorFlow and Keras to create a simple neural network
    architecture with an embedding layer, a flatten layer, and a dense layer. The
    model is trained on a small labeled dataset for sentiment classification, distinguishing
    between positive and negative sentiments. Following training, the model is employed
    to classify new sentences. The provided Python code demonstrates each step, from
    tokenizing and padding sequences to compiling, training, and making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following dataset is used for training on sentiment analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We then use a tokenizer to convert the text into sequences of numbers, and then
    pad the sequences so that they have the same length. We then define a generative
    AI model with an embedding layer, a flatten layer, and a dense layer. Then, we
    compile and train the model on the training data. Finally, we use the trained
    model to classify a new sentence as either positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a complete Python code example with a dataset of four sentences labeled
    as positive or negative. We begin by importing libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The NumPy library is imported as `np` for numerical computations. The necessary
    modules from the TensorFlow library are imported for text preprocessing and model
    creation. Then we define the labeled dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sentences` list contains textual sentences. The `labels` list contains
    corresponding labels where 1 represents a positive sentiment and 0 represents
    a negative sentiment. Next, we tokenize the text and convert it to sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'A `Tokenizer` object is created to tokenize the text. The `fit_on_texts` method
    is used to fit the tokenizer on the provided sentences. The `texts_to_sequences`
    method is used to convert the sentences into sequences of tokens. Now we need
    to pad the sequences so they are the same length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The maximum sequence length is determined by finding the length of the longest
    sequence. The `pad_sequences` function is used to pad the sequences to the maximum
    length. Next, we define the model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'A sequential model is created using the `Sequential` class from Keras. The
    model consists of an embedding layer, a flatten layer, and a dense layer. The
    embedding layer converts the tokens into dense vectors. The flatten layer flattens
    the input for the subsequent dense layer. The dense layer is used for binary classification
    with sigmoid activation. Now, we need to compile the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is compiled with the Adam optimizer, binary cross-entropy loss, and
    accuracy as the metric. Now, we train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is trained on the padded sequences and corresponding labels for a
    specified number of epochs. Next, we classify a new sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'A new sentence is provided for classification. The sentence is converted to
    a sequence of tokens using the tokenizer. The sequence is padded to match the
    maximum sequence length used during training. The model predicts the sentiment
    class for the new sentence. Finally, we print the predicted label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.1\uFEFF1 – Prediction with a neural network model](img/B18944_07_11.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Prediction with a neural network model
  prefs: []
  type: TYPE_NORMAL
- en: The predicted label is printed based on the prediction output. If the predicted
    label is `1`, it is considered a positive sentiment, and if it is `0`, it is considered
    a negative sentiment. In summary, the provided code demonstrates a sentiment analysis
    task using a neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into the realm of text data exploration using Python,
    gaining a comprehensive understanding of harnessing Generative AI and OpenAI models
    for effective text data labeling. Through code examples, we explored diverse text
    data labeling tasks, including classification, summarization, and sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We then extended our knowledge by exploring Snorkel labeling functions, allowing
    us to label text data with enhanced flexibility. Additionally, we delved into
    the application of K-means clustering for labeling text data and concluded by
    discovering how to label customer reviews using neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: With these acquired skills, you now possess the tools to unlock the full potential
    of your text data, extracting valuable insights for various applications. The
    next chapter awaits, where we will shift our focus to video data exploration,
    exploring different methods to gain insights from this dynamic data type.
  prefs: []
  type: TYPE_NORMAL
