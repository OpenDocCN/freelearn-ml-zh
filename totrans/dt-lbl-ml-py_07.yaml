- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Labeling Text Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标注文本数据
- en: In this chapter, we will explore techniques for labeling text data for classification
    in cases where an insufficient amount of labeled data is available. We are going
    to use Generative AI to label the text data, in addition to Snorkel and k-means
    clustering. The chapter focuses on the essential process of annotating textual
    data for NLP and text analysis. It aims to provide readers with practical knowledge
    and insights into various labeling techniques. The chapter will specifically cover
    automatic labeling using OpenAI, rule-based labeling using Snorkel labeling functions,
    and unsupervised learning using k-means clustering. By understanding these techniques,
    readers will be equipped to effectively label text data and extract meaningful
    insights from unstructured textual information.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨在标注数据不足的情况下对文本数据进行分类的技术。我们将使用生成式AI来标注文本数据，除了Snorkel和k-means聚类。本章重点介绍了为NLP和文本分析标注文本数据的必要过程。它旨在为读者提供关于各种标注技术的实用知识和见解。本章将具体涵盖使用OpenAI的自动标注、基于Snorkel标注函数的规则标注以及使用k-means聚类的无监督学习。通过理解这些技术，读者将能够有效地标注文本数据并从非结构化文本信息中提取有意义的见解。
- en: 'We will cover the following sections in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下部分：
- en: Real-world applications of text data labeling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据标注的实战应用
- en: Tools and frameworks for text data labeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据标注的工具和框架
- en: Exploratory data analysis of text
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据的探索性数据分析
- en: Generative AI and OpenAI for labeling text data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于标注文本数据的生成式AI和OpenAI
- en: Labeling text data using Snorkel
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Snorkel标注文本数据
- en: Labeling text data using logistic regression
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑回归标注文本数据
- en: Labeling text data using K-means clustering
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K-means聚类标注文本数据
- en: Labeling customer reviews (sentiment analysis) using neural networks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络标注客户评论（情感分析）
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code files used in this chapter are located at [https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch07](https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch07).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码文件位于[https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch07](https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/code/Ch07)。
- en: 'The Gutenberg Corpus and movie review dataset can be found here:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Gutenberg语料库和电影评论数据集可在此找到：
- en: '[https://pypi.org/project/Gutenberg/](https://pypi.org/project/Gutenberg/)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pypi.org/project/Gutenberg/](https://pypi.org/project/Gutenberg/)'
- en: '[https://www.nltk.org/api/nltk.sentiment.util.html?highlight=movie#nltk.sentiment.util.demo_movie_reviews](https://www.nltk.org/api/nltk.sentiment.util.html?highlight=movie#nltk.sentiment.util.demo_movie_reviews)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.nltk.org/api/nltk.sentiment.util.html?highlight=movie#nltk.sentiment.util.demo_movie_reviews](https://www.nltk.org/api/nltk.sentiment.util.html?highlight=movie#nltk.sentiment.util.demo_movie_reviews)'
- en: You also need to create an Azure account and add the OpenAI resource for working
    with Generative AI. To sign up for a free Azure subscription, visit https://azure.microsoft.com/free.
    To request access to the Azure OpenAI service, visit [https://aka.ms/oaiapply](https://aka.ms/oaiapply).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要创建一个Azure账户，并添加用于与生成式AI工作的OpenAI资源。要注册免费的Azure订阅，请访问https://azure.microsoft.com/free。要申请访问Azure
    OpenAI服务，请访问[https://aka.ms/oaiapply](https://aka.ms/oaiapply)。
- en: 'Once you have provisioned the Azure OpenAI service, set up the following environment
    variables:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您配置了Azure OpenAI服务，请设置以下环境变量：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Your endpoint should look like [https://YOUR_RESOURCE_NAME.openai.azure.com/](https://YOUR_RESOURCE_NAME.openai.azure.com/).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您的端点应类似于[https://YOUR_RESOURCE_NAME.openai.azure.com/](https://YOUR_RESOURCE_NAME.openai.azure.com/)。
- en: Real-world applications of text data labeling
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本数据标注的实战应用
- en: 'Text data labeling or classification is widely used across various industries
    and applications to extract valuable information, automate processes, and improve
    decision-making. Here are some real-world examples across different use cases:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据标注或分类在各个行业和应用中得到了广泛应用，用于提取有价值的信息、自动化流程和改善决策。以下是不同用例中的真实世界示例：
- en: 'Customer support ticket classification:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户支持工单分类：
- en: 'Use case: Companies receive a large volume of customer support tickets.'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例：公司收到大量客户支持工单。
- en: 'Application: Automated classification of support tickets into categories such
    as Billing, Technical Support, and Product Inquiry. This helps prioritize and
    route tickets to the right teams.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：将支持工单自动分类到账单、技术支持和产品咨询等类别。这有助于优先排序并将工单路由到正确的团队。
- en: 'Spam email filtering:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤：
- en: 'Use case: Sorting emails into spam and non-spam categories.'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例：将电子邮件分类为垃圾邮件和非垃圾邮件。
- en: 'Application: Email providers use text classification to identify and filter
    out unwanted emails, providing users with a cleaner inbox and reducing the risk
    of phishing attacks.'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：电子邮件服务提供商使用文本分类来识别和过滤掉不想要的电子邮件，为用户提供更干净的收件箱，并降低钓鱼攻击的风险。
- en: 'Sentiment analysis in social media:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交媒体情感分析：
- en: 'Use case: Analyzing social media comments and posts.'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例：分析社交媒体评论和帖子。
- en: 'Application: Brands use sentiment analysis to gauge public opinion, track brand
    sentiment, and respond to customer feedback. It helps with reputation management
    and understanding customer preferences.'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：品牌使用情感分析来衡量公众舆论、跟踪品牌情绪并回应客户反馈。这有助于声誉管理和理解客户偏好。
- en: 'News categorization:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新闻分类：
- en: 'Use case: Sorting news articles into categories.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例：将新闻文章分类。
- en: 'Application: News websites use text classification to automatically categorize
    articles into sections such as Politics, Technology, and Entertainment, making
    it easier for readers to find relevant content.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：新闻网站使用文本分类自动将文章分类到政治、科技和娱乐等版块，使读者更容易找到相关内容。
- en: 'Resume screening:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简历筛选：
- en: 'Use case: Sorting job applications.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例：对求职申请进行分类。
- en: 'Application: Human resources departments use text classification to quickly
    identify resumes that match specific job requirements. This accelerates the hiring
    process and ensures a more efficient candidate screening.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：人力资源部门使用文本分类快速识别符合特定工作要求的简历。这加速了招聘流程并确保了更高效的候选人筛选。
- en: 'Medical document classification:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗文档分类：
- en: 'Use case: Sorting medical records and documents.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例：对医疗记录和文件进行分类。
- en: 'Application: Healthcare organizations use text classification to categorize
    and organize medical records, lab reports, and patient notes. This aids in efficient
    data retrieval and analysis.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：医疗保健组织使用文本分类对医疗记录、实验室报告和患者笔记进行分类和组织。这有助于高效的数据检索和分析。
- en: 'Legal document classification:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 法律文档分类：
- en: 'Use case: Sorting legal documents.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例：对法律文件进行分类。
- en: 'Application: Law firms use text classification to categorize and manage legal
    documents, contracts, and case-related information, streamlining legal research
    and case management.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：律师事务所使用文本分类对法律文件、合同和与案件相关的信息进行分类和管理，简化法律研究和案件管理。
- en: 'Fraud detection in financial transactions:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金融交易欺诈检测：
- en: 'Use case: Identifying fraudulent activity.'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例：识别欺诈活动。
- en: 'Application: Financial institutions use text classification to analyze transaction
    descriptions and identify potential cases of fraud or suspicious activities, enhancing
    security measures.'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：金融机构使用文本分类分析交易描述并识别潜在的欺诈或可疑活动，增强安全措施。
- en: 'Product review analysis:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品评论分析：
- en: 'Use case: Analyzing customer reviews.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例：分析客户评论。
- en: 'Application: E-commerce platforms use sentiment analysis to categorize and
    understand product reviews. This helps in improving products, addressing customer
    concerns, and enhancing overall customer satisfaction.'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：电子商务平台使用情感分析对产品评论进行分类和理解。这有助于改进产品、解决客户关注的问题并提高整体客户满意度。
- en: 'Language identification:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言识别：
- en: 'Use case: Determining the language of a given text.'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例：确定给定文本的语言。
- en: 'Application: Social media platforms and translation services use text classification
    to automatically identify the language of a user’s post or content, enabling accurate
    language-specific interactions.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：社交媒体平台和翻译服务使用文本分类自动识别用户的帖子或内容所使用的语言，实现准确的语言特定交互。
- en: These examples highlight the versatility of text classification across different
    domains, showcasing its significance in automating tasks, improving efficiency,
    and gaining valuable insights from textual data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子突出了文本分类在不同领域的多功能性，展示了它在自动化任务、提高效率和从文本数据中获得有价值见解中的重要性。
- en: Tools and frameworks for text data labeling
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本数据标注的工具和框架
- en: 'There are several open source tools and frameworks available for text data
    analysis and labeling. Here are some popular ones, along with their pros and cons:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个开源工具和框架可用于文本数据分析与标注。以下是一些流行的工具，以及它们的优缺点：
- en: '| **Tools** **and frameworks** | **Pros** | **Cons** |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **工具** **和框架** | **优点** | **缺点** |'
- en: '| **Natural Language** **Toolkit** (**NLTK**) | Comprehensive library for NLP
    tasks.Rich set of tools for tokenization, stemming, tagging, parsing, and more.Active
    community support.Suitable for educational purposes and research projects. | Some
    components may not be as efficient for large-scale industrial applications.Steep
    learning curve for beginners. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| **自然语言** **工具包** (**NLTK**) | 用于NLP任务的综合性库。提供丰富的分词、词干提取、标注、解析等工具。拥有活跃的社区支持。适合教育目的和科研项目。
    | 对于大规模工业应用，某些组件可能效率不高。对于初学者，学习曲线可能较陡。 |'
- en: '| spaCy | Fast and efficient, designed for production use.Pre-trained models
    for various languages.Provides robust support for tokenization, named entity recognition,
    and dependency parsing.Easy-to-use API. | Less emphasis on educational resources
    compared to NLTK.Limited support for some languages. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| spaCy | 快速高效，专为生产使用设计。提供多种语言的预训练模型。提供强大的分词、命名实体识别和依存句法分析支持。易于使用的API。 | 相比NLTK，对教育资源重视度较低。对某些语言的支持有限。
    |'
- en: '| scikit-learn | General-purpose machine learning library with excellent text
    processing capabilities.Easy integration with other scikit-learn modules for feature
    extraction and model training.Well-documented and widely used in the machine learning
    community. | May not have specialized tools for certain NLP tasks.Limited support
    for deep learning-based models. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| scikit-learn | 具有出色文本处理能力的通用机器学习库。易于与其他scikit-learn模块集成进行特征提取和模型训练。文档完善，在机器学习社区中广泛使用。
    | 对于某些NLP任务可能没有专门的工具。对基于深度学习的模型支持有限。 |'
- en: '| TextBlob | Simple API for common NLP tasks such as part-of-speech tagging,
    noun phrase extraction, and sentiment analysis.Built on NLTK and provides an easy
    entry point for beginners.Useful for quick prototyping and small projects. | Limited
    customization options compared to lower-level libraries.May not be as performant
    for large-scale applications. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| TextBlob | 提供常见NLP任务的简单API，如词性标注、名词短语提取和情感分析。基于NLTK构建，为初学者提供便捷的入门途径。适用于快速原型设计和小型项目。
    | 相比底层库，定制选项有限。对于大规模应用，性能可能不如。 |'
- en: '| Gensim | Focus on topic modeling, document similarity, and vector space modeling.Efficient
    implementation of algorithms such as Word2Vec.Suitable for large text corpora
    and document similarity tasks. | Less versatile for general-purpose NLP tasks.Limited
    support for some advanced NLP functionalities. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Gensim | 专注于主题建模、文档相似性和向量空间建模。Word2Vec等算法的高效实现。适合大型文本语料库和文档相似性任务。 | 对于通用NLP任务来说，功能可能不够全面。对某些高级NLP功能支持有限。
    |'
- en: '| Transformers (Hugging Face) | Provides pre-trained models for a wide range
    of NLP tasks (BERT, GPT, etc.).Easy-to-use interfaces for integrating state-of-the-art
    models.Excellent community support. | Heavy computational requirements for fine-tuning
    large models.May not be as straightforward for beginners. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Transformers (Hugging Face) | 提供广泛NLP任务的预训练模型（BERT、GPT等）。提供易于使用的接口以集成最先进的模型。拥有卓越的社区支持。
    | 调整大型模型时计算需求量大。对于初学者来说可能不够直观。 |'
- en: '| Stanford NLP | Comprehensive suite of NLP tools, including tokenization,
    part-of-speech tagging, and named entity recognition.Java-based, making it suitable
    for Java projects. | Heavier resource usage compared to Python-based libraries.May
    have a steeper learning curve for certain tasks. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 斯坦福NLP | 综合的NLP工具套件，包括分词、词性标注和命名实体识别。基于Java，适合Java项目使用。 | 相比基于Python的库，资源使用量更大。对于某些任务，学习曲线可能更陡峭。
    |'
- en: '| Flair | Focus on state-of-the-art NLP models and embeddings.Provides embeddings
    for a variety of languages.Easy-to-use API. | May not have as many pre-built models
    as other libraries.May not be as established as some older frameworks. |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Flair | 专注于最先进的NLP模型和嵌入。提供多种语言的嵌入。易于使用的API。 | 相比其他库，预建模型可能较少。可能不如一些较老框架那样成熟。
    |'
- en: Table 7.1 – Popular tools with their pros and cons
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 – 流行工具及其优缺点
- en: In addition to this list is OpenAI’s **Generative Pre-trained Transformer**
    (**GPT**), which is a state-of-the-art language model that utilizes transformer
    architecture. It’s pre-trained on a massive amount of diverse data and can be
    fine-tuned for specific tasks. GPT is known for its ability to generate coherent
    and contextually relevant text, making it a powerful tool for various **natural
    language processing** (**NLP**) applications.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个列表之外，还有OpenAI的**生成预训练转换器**（**GPT**），这是一个最先进的语言模型，它利用了转换器架构。它在大量多样化的数据上进行预训练，并且可以针对特定任务进行微调。GPT以其生成连贯且上下文相关的文本的能力而闻名，使其成为各种**自然语言处理**（**NLP**）应用的强大工具。
- en: The transformer architecture, introduced by Vaswani et al. in the paper *Attention
    is All You Need*, revolutionized NLP. It relies on self-attention mechanisms to
    capture contextual relationships between words in a sequence, enabling parallelization
    and scalability. Transformers have become the foundation of numerous advanced
    language models, including GPT and BERT, due to their ability to capture long-range
    dependencies in sequential data efficiently. Its pros include versatility and
    the ability to understand context in text which is why it is used for various
    natural language understanding tasks. Its cons are that it is resource-intensive,
    requiring substantial computing power, and fine-tuning requires access to significant
    computational resources.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Vaswani等人提出的论文《Attention is All You Need》中引入的转换器架构，彻底改变了NLP。它依赖于自注意力机制来捕捉序列中单词之间的上下文关系，从而实现并行化和可扩展性。由于它们能够有效地捕捉序列数据中的长距离依赖关系，转换器已成为包括GPT和BERT在内的许多高级语言模型的基础。其优点包括多功能性和理解文本上下文的能力，这就是为什么它被用于各种自然语言理解任务。其缺点是资源密集，需要大量的计算能力，并且微调需要访问大量的计算资源。
- en: Each of these tools has strengths and weaknesses, and the choice depends on
    project requirements, available resources, and the desired level of customization.
    It’s common to see a combination of these tools being used together in more complex
    NLP pipelines. When selecting a tool, it’s important to consider factors such
    as ease of use, community support, and compatibility with the specific tasks at
    hand.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具各有优缺点，选择取决于项目需求、可用资源和所需的定制程度。在更复杂的NLP管道中，常见的是将这些工具组合使用。在选择工具时，重要的是要考虑使用简便性、社区支持和与特定任务的兼容性等因素。
- en: Exploratory data analysis of text
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本探索性数据分析
- en: '**Exploratory Data Analysis** (**EDA**) is a crucial step in any data science
    project. When it comes to text data, EDA can help us understand the structure
    and characteristics of the data, identify potential issues or inconsistencies,
    and inform our choice of data preprocessing and modeling techniques. In this section,
    we will walk through the steps involved in performing EDA on text data.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索性数据分析**（**EDA**）是任何数据科学项目中的关键步骤。当涉及到文本数据时，EDA可以帮助我们了解数据的结构和特征，识别潜在的问题或不一致性，并指导我们选择数据预处理和建模技术。在本节中，我们将介绍在文本数据上执行EDA的步骤。'
- en: Loading the data
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: The first step in EDA is to load the text data into our environment. Text data
    can come in many formats, including plain text files, CSV files, or database tables.
    Once we have the data loaded, we can begin to explore its structure and content.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: EDA（探索性数据分析）的第一步是将文本数据加载到我们的环境中。文本数据可以以多种格式存在，包括纯文本文件、CSV文件或数据库表。一旦数据被加载，我们就可以开始探索其结构和内容。
- en: Understanding the data
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据
- en: The next step in EDA is to gain an understanding of the data. For text data,
    this may involve examining the size of the dataset, the number of documents or
    samples, and the overall structure of the text (e.g., whether it is structured
    or unstructured). We can use descriptive statistics to gain insights into the
    data, such as the distribution of text lengths or the frequency of certain words
    or phrases.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: EDA的下一步是了解数据。对于文本数据，这可能包括检查数据集的大小、文档或样本的数量，以及文本的整体结构（例如，是否为结构化或非结构化）。我们可以使用描述性统计来深入了解数据，例如文本长度的分布或某些单词或短语的频率。
- en: Cleaning and preprocessing the data
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清洗和预处理
- en: After understanding the data, the next step in EDA is to clean and preprocess
    the text data. This can involve a number of steps, such as removing punctuation
    and stop words, stemming or lemmatizing words, and converting text to lowercase.
    Cleaning and preprocessing the data is important for preparing the data for modeling
    and ensuring that we are working with high-quality data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解数据之后，探索性数据分析的下一步是对文本数据进行清理和预处理。这可能涉及多个步骤，例如删除标点符号和停用词，对单词进行词干提取或词形还原，以及将文本转换为小写。清理和预处理数据对于为建模准备数据以及确保我们使用高质量数据至关重要。
- en: Exploring the text’s content
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索文本的内容
- en: Once we have cleaned and preprocessed the data, we can begin to explore the
    content of the text itself. This can involve examining the most frequent words
    or phrases, identifying patterns or themes in the text, and visualizing the data
    using techniques such as word clouds or frequency histograms. We can also use
    NLP techniques to extract features from the text, such as named entities, part-of-speech
    tags, or sentiment scores.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在清理和预处理数据之后，我们可以开始探索文本本身的内容。这可能包括检查最频繁出现的单词或短语，识别文本中的模式或主题，并使用如词云或频率直方图等技术来可视化数据。我们还可以使用自然语言处理技术从文本中提取特征，例如命名实体、词性标签或情感分数。
- en: Analyzing relationships between text and other variables
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析文本与其他变量之间的关系
- en: In some cases, we may want to explore the relationships between the text data
    and other variables, such as demographic or behavioral data. For example, we may
    want to examine whether the sentiment of movie reviews varies by genre, or whether
    the topics discussed in social media posts differ by user age or location. This
    type of analysis can help us gain deeper insights into the text data and inform
    our modeling approach.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能想要探索文本数据与其他变量之间的关系，例如人口统计或行为数据。例如，我们可能想要检查电影评论的情感是否因类型而异，或者社交媒体帖子中讨论的主题是否因用户年龄或地理位置而异。这种分析可以帮助我们更深入地了解文本数据，并指导我们的建模方法。
- en: Visualizing the results
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化结果
- en: Finally, we can visualize the results of our EDA using a variety of techniques,
    such as word clouds, bar charts, scatterplots, or heat maps. Visualization is
    an important tool for communicating insights and findings to stakeholders, and
    can help us identify patterns and relationships in the data that might not be
    immediately apparent from the raw text.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用各种技术来可视化我们的探索性数据分析结果，例如词云、条形图、散点图或热图。可视化是向利益相关者传达见解和发现的重要工具，可以帮助我们识别数据中的模式和关系，这些模式和关系可能从原始文本中并不立即明显。
- en: In conclusion, exploratory data analysis is a critical step in any text data
    project. By understanding the structure and content of the data, cleaning and
    preprocessing it, exploring the text’s content, analyzing relationships between
    text and other variables, and visualizing the results, we can gain deep insights
    into the textual data and inform our modeling approach. With the right tools and
    techniques, EDA can help us uncover hidden patterns and insights in text data
    that can be used to drive business decisions and improve outcomes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，探索性数据分析是任何文本数据项目中的关键步骤。通过理解数据的结构和内容，对其进行清理和预处理，探索文本的内容，分析文本与其他变量之间的关系，以及可视化结果，我们可以深入了解文本数据，并指导我们的建模方法。有了合适的工具和技术，探索性数据分析可以帮助我们发现文本数据中的隐藏模式和见解，这些模式和见解可以用于驱动业务决策并改善结果。
- en: Exploratory data analysis of sample text data set
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本文本数据集的探索性数据分析
- en: Here’s an example Python code for performing EDA on a text dataset. We will
    be using the Gutenberg corpus ([https://pypi.org/project/Gutenberg/](https://pypi.org/project/Gutenberg/)),
    which is a publicly available collection of over 60,000 electronic books.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个用于在文本数据集上执行探索性数据分析的Python代码示例。我们将使用古腾堡语料库（[https://pypi.org/project/Gutenberg/](https://pypi.org/project/Gutenberg/)），这是一个包含60,000多本电子书的公开可用集合。
- en: The NLTK corpus is a collection of publicly available datasets for NLP research
    and development. The Gutenberg corpus ([https://www.nltk.org/book/ch02.html](https://www.nltk.org/book/ch02.html)),
    which is one of the datasets included in NLTK, specifically contains a selection
    of public domain texts from Project Gutenberg. Project Gutenberg is a digital
    library that offers free access to books and other texts that are no longer protected
    by copyright.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK语料库是用于NLP研究和开发的公开可用数据集的集合。Gutenberg语料库（[https://www.nltk.org/book/ch02.html](https://www.nltk.org/book/ch02.html)），NLTK包含的数据集之一，特别包含来自Project
    Gutenberg的公共领域文本的选择。Project Gutenberg是一个提供免费访问不再受版权保护的书本和其他文本的数字图书馆。
- en: 'Therefore, the Gutenberg corpus within the NLTK is based on public domain texts,
    making it a publicly available dataset. It can be used for various NLP tasks,
    such as text classification, language modeling, and information retrieval, without
    any commercial restrictions or licensing requirements:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，NLTK中的Gutenberg语料库基于公共领域文本，使其成为一个公开可用的数据集。它可以用于各种NLP任务，如文本分类、语言建模和信息检索，没有任何商业限制或许可要求：
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s download the Gutenberg corpus using the NLTK library:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用NLTK库下载Gutenberg语料库：
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s load the text data into a Pandas DataFrame by iterating the fields from
    Gutenberg and appending documents to the list data. Then we’ll convert the list
    data to dataframe, `df`, with a single column, `text`, to store the document:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过迭代Gutenberg的字段并将文档添加到列表数据中来将文本数据加载到Pandas DataFrame中。然后我们将列表数据转换为包含单个列`text`的DataFrame，用于存储文档：
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s check the dataframe’s size by calling the `shape` function:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过调用`shape`函数来检查DataFrame的大小：
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here’s the output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '![Figure 7.1 – The first few rows of data](img/B18944_07_01.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 数据的前几行](img/B18944_07_01.jpg)'
- en: Figure 7.1 – The first few rows of data
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 数据的前几行
- en: 'Let’s check the length of each document by calling the `apply` function:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过调用`apply`函数来检查每个文档的长度：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here’s the output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '![Figure 7.2 – Distribution of document length](img/B18944_07_02.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 文档长度的分布](img/B18944_07_02.jpg)'
- en: Figure 7.2 – Distribution of document length
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 文档长度的分布
- en: 'In text analysis, removing stopwords and punctuation is one of the most common
    tasks because stopwords do not tell us anything about the text:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本分析中，移除停用词和标点是其中最常见的任务之一，因为停用词不会告诉我们关于文本的任何信息：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will use the stopwords list from the NLTK corpus:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用NLTK语料库中的停用词列表：
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now let’s count the frequency of words in the clean text using the `value_counts`
    function:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用`value_counts`函数来计算清洗文本中单词的频率：
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, plot a bar chart to visualize the most frequent words:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，绘制一个条形图来可视化最频繁的单词：
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here’s the output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '![Figure 7.3 – Most frequent words](img/B18944_07_03.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 最频繁的单词](img/B18944_07_03.jpg)'
- en: Figure 7.3 – Most frequent words
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 最频繁的单词
- en: In this code, we first downloaded the Gutenberg corpus using the NLTK library.
    We then loaded the text data into a Pandas DataFrame and performed some initial
    checks on the size and structure of the dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们首先使用NLTK库下载了Gutenberg语料库。然后我们将文本数据加载到Pandas DataFrame中，并对数据集的大小和结构进行了初步检查。
- en: Next, we calculated the length of each document and visualized the distribution
    of document lengths using a histogram. We then removed punctuation and stop words
    from the text data and calculated the frequency of each word. We visualized the
    most frequent words using a bar chart.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算了每个文档的长度，并使用直方图可视化文档长度的分布。然后我们从文本数据中移除了标点符号和停用词，并计算了每个单词的频率。我们使用条形图可视化了最频繁的单词。
- en: Note that this code is just a basic example of EDA on text data, and you may
    need to modify it to suit your specific dataset and research question. Now we
    have clean text data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这段代码只是文本数据探索性数据分析（EDA）的一个基本示例，你可能需要根据你的特定数据集和研究问题对其进行修改。现在我们有了清洗后的文本数据。
- en: Let’s see how to use Generative AI to label text data in the following section.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在下一节中使用生成式AI对文本数据进行标注。
- en: Exploring Generative AI and OpenAI for labeling text data
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索使用生成式AI和OpenAI进行文本数据标注
- en: Generative AI refers to a category of artificial intelligence that involves
    training models to generate new content or data based on patterns and information
    present in the training data. OpenAI is a prominent organization that has developed
    and released powerful generative models for various NLP tasks. One of the notable
    models is GPT, such as GPT-3, GPT-3.5, and GPT-4\. These models have been influential
    in the fields of text data labeling and classification.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI是指一类人工智能，它涉及根据训练数据中的模式和信息训练模型以生成新的内容或数据。OpenAI是一个杰出的组织，它为各种NLP任务开发和发布了强大的生成模型。其中一些引人注目的模型是GPT，例如GPT-3、GPT-3.5和GPT-4。这些模型在文本数据标注和分类领域产生了重大影响。
- en: Generative AI focuses on training models to generate new data instances that
    resemble existing examples. It is often used for tasks such as text generation,
    image synthesis, and more. Generative models are trained on large datasets to
    learn underlying patterns, allowing them to generate coherent and contextually
    relevant content. In text-related tasks, generative AI can be applied to text
    completion, summarization, question answering, and even creative writing. Let’s
    take a look at some key concepts that will help us with labeling text data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI专注于训练模型生成与现有示例相似的新数据实例。它通常用于文本生成、图像合成等任务。生成模型在大型数据集上训练，以学习潜在的模式，从而生成连贯且与上下文相关的内
    容。在文本相关任务中，生成式AI可以应用于文本补全、摘要、问答甚至创意写作。让我们看看一些关键概念，这些概念将帮助我们进行文本数据标注。
- en: GPT models by OpenAI
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI的GPT模型
- en: OpenAI has developed a series of sophisticated language models, with GPT-4 being
    among the most advanced. These models undergo pre-training on diverse datasets,
    enabling them to excel in various natural language understanding and generation
    tasks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI开发了一系列复杂的语言模型，其中GPT-4是最先进的之一。这些模型在多样化的数据集上进行预训练，使它们在自然语言理解和生成任务上表现出色。
- en: Zero-shot learning capabilities
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本学习能力
- en: GPT models are renowned for their zero-shot learning capabilities, enabling
    them to make predictions or generate content for tasks they were not explicitly
    trained on. This versatility enhances their applicability across diverse domains.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型以其零样本学习能力而闻名，这使得它们能够在没有明确训练的情况下对任务进行预测或生成内容。这种多功能性增强了它们在各个领域的适用性。
- en: Text classification with OpenAI models
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用OpenAI模型进行文本分类
- en: Leveraging the language understanding and generation capabilities of OpenAI
    models, they can be effectively utilized for text classification tasks. This includes
    sentiment analysis, topic categorization, and other classification-based applications.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 利用OpenAI模型的自然语言理解和生成能力，它们可以有效地用于文本分类任务。这包括情感分析、主题分类以及其他基于分类的应用。
- en: Data labeling assistance
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标注辅助
- en: Although GPT models are not specifically designed for traditional data labeling
    tasks, they can offer assistance in generating labeled data. This can be achieved
    through natural language instructions or by providing context that aids in making
    labeling decisions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPT模型并非专门为传统的数据标注任务设计，但它们可以在生成标注数据方面提供帮助。这可以通过自然语言指令或提供有助于做出标注决策的上下文来实现。
- en: OpenAI API overview
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI API概述
- en: The OpenAI API is a service provided by OpenAI that allows users to access their
    advanced language models through an API. It serves as a gateway for integrating
    OpenAI’s language capabilities into various applications.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI API是OpenAI提供的一项服务，允许用户通过API访问其高级语言模型。它作为将OpenAI的语言能力集成到各种应用中的门户。
- en: 'Let’s see the pros and cons of OpenAI’s GPT models:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看OpenAI的GPT模型的优缺点：
- en: 'Pros:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优点：
- en: 'Versatility: OpenAI’s GPT models are versatile and can be adapted for various
    text-related tasks, including data labeling and classification'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多功能性：OpenAI的GPT模型具有多功能性，可以适应各种与文本相关的任务，包括数据标注和分类
- en: 'Large scale: These models are trained on massive amounts of data, enabling
    them to capture intricate patterns and nuances present in natural language'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模：这些模型在大量数据上训练，使它们能够捕捉自然语言中存在的复杂模式和细微差别
- en: 'Cons:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺点：
- en: 'Interpretability: The generated content might lack interpretability, making
    it challenging to understand the model’s decision-making process'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性：生成的内 容可能缺乏可解释性，这使得理解模型的决策过程变得具有挑战性
- en: 'Resource intensive: Training and using large generative models such as GPT-4
    can be computationally expensive'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源密集型：训练和使用像GPT-4这样的大型生成模型在计算上可能非常昂贵
- en: In summary, OpenAI’s generative models, particularly GPT-3 , GPT-3.5, and GPT-4,
    have made significant contributions to the field of text data processing, and
    they can be used creatively for tasks such as data labeling and classification
    by utilizing their language-understanding capabilities. However, careful consideration
    and evaluation are needed, especially regarding ethical concerns and potential
    bias in generated content.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，OpenAI 的生成模型，特别是 GPT-3、GPT-3.5 和 GPT-4，在文本数据处理领域做出了重大贡献，并且可以利用它们的语言理解能力创造性地用于数据标注和分类等任务。然而，在考虑道德问题和生成内容中可能存在的偏见时，需要谨慎考虑和评估。
- en: In the realm of language processing, text classification serves to categorize
    documents based on their content. Traditionally, this task relied on labeled training
    data; however, advanced models such as OpenAI’s GPT have revolutionized the process
    by autonomously generating labels with the assistance of explicit instructions
    or **prompts**.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言处理领域，文本分类用于根据内容对文档进行分类。传统上，这项任务依赖于标注的训练数据；然而，像 OpenAI 的 GPT 这样的先进模型通过在明确指令或**提示**的帮助下自主生成标签，彻底改变了这一过程。
- en: Exploring text data labeling with **Azure OpenAI**, a collaborative initiative
    within Microsoft Azure’s cloud, unlocks the potential of powerful language models.
    This section acts as a guide, facilitating efficient text data labeling by harnessing
    the capabilities of Generative AI and OpenAI models, and providing users with
    custom tools for typical tasks in text data analysis.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 探索使用 **Azure OpenAI** 进行文本数据标注，这是微软 Azure 云中的协作倡议，释放了强大语言模型的能力。本节作为指南，通过利用生成
    AI 和 OpenAI 模型的能力，以及为用户提供文本数据分析中典型任务的定制工具，促进高效文本数据标注。
- en: Let’s take a look at some use cases with Python and Azure OpenAI for text data
    labeling.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些使用 Python 和 Azure OpenAI 进行文本数据标注的用例。
- en: Use case 1 – summarizing the text
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例 1 – 文本摘要
- en: Summarization is a crucial NLP task that involves condensing a piece of text
    while retaining its essential information and main ideas. In the context of Azure
    OpenAI, the following code exemplifies the application of summarization using
    the GPT-3.5-turbo model deployed on the Azure platform.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要是涉及在保留文本的必要信息和主要思想的同时压缩文本的关键 NLP 任务。在 Azure OpenAI 的背景下，以下代码示例展示了在 Azure 平台上部署的
    GPT-3.5-turbo 模型应用摘要的示例。
- en: The following code example begins by setting the necessary environment variables
    for the Azure OpenAI API, including the API key and endpoint. The OpenAI API is
    then configured with the deployment name of the model, allowing the code to interact
    with the specific GPT-3.5-turbo instance.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例首先设置了 Azure OpenAI API 所需的环境变量，包括 API 密钥和端点。然后，使用模型的部署名称配置 OpenAI API，使代码能够与特定的
    GPT-3.5-turbo 实例交互。
- en: The input text, which is a detailed description of Dachepalli, a town in Andhra
    Pradesh, India, is provided for summarization. The code utilizes the Azure OpenAI
    Completion API to generate a summary, employing parameters such as temperature,
    max tokens, and penalties for frequency and presence.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的输入文本是对印度安得拉邦达切帕利镇的详细描述，用于摘要。代码利用 Azure OpenAI Completion API 生成摘要，使用温度、最大标记数以及频率和存在性惩罚等参数。
- en: The output of the code includes the generated summary, showcasing the main ideas
    extracted from the input text. The summarized content emphasizes key aspects such
    as the author’s connection to Dachepalli, the town’s features, and notable historical
    events. This example demonstrates how Azure OpenAI can effectively summarize information,
    providing concise and informative outputs.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的输出包括生成的摘要，展示了从输入文本中提取的主要思想。摘要内容强调了作者与达切帕利的联系、城镇的特点以及显著的历史事件。这个例子展示了 Azure
    OpenAI 如何有效地总结信息，提供简洁且信息丰富的输出。
- en: 'Let’s start by importing the required libraries and getting the configuration
    values (the Azure OpenAI key and endpoint, API version, and the GPT model deployment
    name) that we have set already:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入所需的库和获取配置值（Azure OpenAI 密钥和端点、API 版本以及已设置的 GPT 模型部署名称）开始：
- en: '[PRE10]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let’s understand the parameters used in this OpenAI completion API.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解这个 OpenAI 完成API中使用的参数。
- en: 'OpenAI’s parameters control the behavior of the language model during text
    generation. Here’s a brief description of the provided parameters:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的参数控制语言模型在文本生成过程中的行为。以下是提供的参数的简要描述：
- en: 'Temperature (`temperature=0`): It determines the randomness of the model’s
    output. A high value (e.g., `0.8`) makes the output more diverse, while a low
    value (e.g., `0.2`) makes it more deterministic.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '温度 (`temperature=0`): 它决定了模型输出的随机性。高值（例如，`0.8`）会使输出更加多样化，而低值（例如，`0.2`）会使输出更加确定。'
- en: 'Max tokens (`max_tokens=118`): This specifies the maximum number of tokens
    (words or characters) to generate in the output. It’s useful for limiting response
    length.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '最大标记数 (`max_tokens=118`): 这指定了在输出中生成最大标记数（单词或字符）。它对于限制响应长度很有用。'
- en: 'Top P (`top_p=1`): Also known as nucleus sampling, it controls the diversity
    of the generated output. Setting it to `1` ensures that only the top probability
    tokens are considered during sampling.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Top P (`top_p=1`): 也称为核采样，它控制着生成输出的多样性。将其设置为 `1` 确保在采样过程中只考虑概率最高的标记。'
- en: 'Frequency penalty (`frequency_penalty=0`): This discourages the repetition
    of specific tokens in the output. A non-zero value penalizes the model for choosing
    frequently occurring tokens.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '频率惩罚 (`frequency_penalty=0`): 这会阻止在输出中重复特定的标记。非零值会惩罚模型选择频繁出现的标记。'
- en: 'Presence Penalty (`presence_penalty=0`): Similar to frequency penalty, presence
    penalty discourages the repetition of entire phrases or concepts, promoting more
    diverse responses.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '存在惩罚 (`presence_penalty=0`): 与频率惩罚类似，存在惩罚会阻止整个短语或概念的重复，从而促进更多样化的响应。'
- en: 'Stop (`stop=None`): This allows users to specify a custom stopping criterion
    for generation. When the model encounters the specified token, it stops generating
    further content.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '停止 (`stop=None`): 这允许用户指定生成过程的自定义停止标准。当模型遇到指定的标记时，它将停止生成进一步的内容。'
- en: 'These parameters provide users with fine-grained control over the generation
    process, allowing customization of the model’s output based on factors such as
    randomness, length, diversity, and repetition. Adjusting these parameters enables
    users to tailor the language model’s behavior to meet specific requirements in
    various applications, such as chatbots, content generation, and more:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数为用户提供了对生成过程的精细控制，允许根据随机性、长度、多样性和重复等因素自定义模型的输出。调整这些参数使用户能够根据各种应用的具体要求定制语言模型的行为，例如聊天机器人、内容生成等：
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Running this code will output the following summary:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将输出以下摘要：
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We have seen how to generate a summary using the OpenAI GPT-3.5 model. Now let’s
    see how to generate the topic for news articles using OpenAI’s GPT model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何使用 OpenAI GPT-3.5 模型生成摘要。现在让我们看看如何使用 OpenAI 的 GPT 模型生成新闻文章的主题。
- en: Use case 2 – topic generation for news articles
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例 2 – 新闻文章的主题生成
- en: Let’s explore generating topic names for news articles using a generative model,
    specifically, Azure OpenAI.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索使用生成模型生成新闻文章的主题名称，具体来说，是使用 Azure OpenAI。
- en: Topic generation is a powerful application of NLP that involves creating relevant
    and coherent content based on a given prompt. In the context of Azure OpenAI prompts,
    the ability to generate topics is demonstrated using a news headline classification
    example.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 主题生成是 NLP 的强大应用，它涉及根据给定的提示创建相关且连贯的内容。在 Azure OpenAI 提示的背景下，使用新闻标题分类示例展示了生成主题的能力。
- en: In this code snippet, the task is to categorize a news headline into one of
    the predefined categories, which are Business, Tech, Politics, Sport, and Entertainment.
    The news headline, provided as input, is *“Trump is ready to contest in Nov 2024
    elections.”* The code uses the Azure OpenAI API to generate a response that predicts
    the most appropriate category for the given headline.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码片段中，任务是将新闻标题分类到预定义的类别之一，这些类别是商业、科技、政治、体育和娱乐。提供的输入新闻标题是 *“特朗普准备在 2024 年 11
    月选举中竞选。”* 代码使用 Azure OpenAI API 生成一个响应，预测给定标题最合适的类别。
- en: The completion engine is configured with specific parameters, such as temperature,
    max tokens, and penalties for frequency and presence. After generating the response,
    the code extracts and prints the predicted category from the output.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 完成引擎配置了特定的参数，如温度、最大标记数以及频率和存在惩罚。在生成响应后，代码从输出中提取并打印预测的类别。
- en: 'This example showcases how Azure OpenAI prompts can be utilized for the automatic
    categorization of news headlines, demonstrating the versatility and effectiveness
    of NLP in topic-generation tasks:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例展示了如何利用 Azure OpenAI 提示自动分类新闻标题，展示了 NLP 在主题生成任务中的灵活性和有效性：
- en: '[PRE13]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here’s the output:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE14]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Use case 3 – classification of customer queries using the user-defined categories
    and sub-categories
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例 3 – 使用用户定义的类别和子类别对客户查询进行分类
- en: Let’s see how to classify the customer queries into user-defined categories
    and sub-categories using **Azure OpenAI**.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用**Azure OpenAI**将客户查询分类到用户定义的类别和子类别。
- en: Text classification is a fundamental NLP task that involves assigning predefined
    categories to textual input. In the provided code, a customer support system utilizes
    text classification to categorize customer queries related to their orders. The
    system employs user-defined primary and secondary categories, each with specific
    sub-categories.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是自然语言处理的一个基本任务，涉及将预定义的类别分配给文本输入。在提供的代码中，客户支持系统利用文本分类对与订单相关的客户查询进行分类。系统使用用户定义的主要和次要类别，每个类别都有特定的子类别。
- en: The system message serves as a guide for the classification task, outlining
    the primary categories (Order Status, Product Inquiries, Shipping and Delivery,
    and Payment Assistance) and their corresponding secondary categories. The primary
    and secondary categories are structured to capture various aspects of customer
    queries, such as tracking information, product availability, and payment confirmation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 系统消息作为分类任务的指南，概述了主要类别（订单状态、产品咨询、运输和配送以及支付帮助）及其对应的次要类别。主要和次要类别被组织起来以捕捉客户查询的各个方面，例如跟踪信息、产品可用性和支付确认。
- en: For example, when a user submits a query to cancel an order, the code uses the
    OpenAI ChatCompletion API to generate a response. The output includes a JSON-formatted
    response indicating the primary and secondary categories assigned to the user’s
    query. In this case, the primary category is Order Status, and the secondary category
    is Order Modification or Cancellation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当用户提交取消订单的查询时，代码使用 OpenAI ChatCompletion API 生成响应。输出包括一个 JSON 格式的响应，指示分配给用户查询的主要和次要类别。在这种情况下，主要类别是订单状态，次要类别是订单修改或取消。
- en: 'This example demonstrates how text classification can be applied in a customer
    support context, allowing for the efficient handling and categorization of customer
    queries based on predefined categories. The system provides a structured approach
    to address diverse aspects of order-related inquiries, enhancing the overall customer
    support experience:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了如何在客户支持环境中应用文本分类，允许根据预定义的类别高效地处理和分类客户查询。系统提供了一种结构化的方法来处理与订单相关的各种查询，从而提高整体客户支持体验：
- en: '[PRE15]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here’s the output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果：
- en: '[PRE16]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Use case 4 – information retrieval using entity extraction
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例 4 – 使用实体提取进行信息检索
- en: Let us see how to extract the entity names from the text data using Azure OpenAI.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用 Azure OpenAI 从文本数据中提取实体名称。
- en: Entity extraction is a vital aspect of NLP, involving the identification and
    extraction of specific entities, such as names, organizations, locations, and
    contact numbers, from a given text. In the presented code snippet, the task is
    to identify and extract people’s names, organization names, geographical locations,
    and contact numbers from various text passages.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 实体提取是自然语言处理的一个重要方面，涉及从给定文本中识别和提取特定实体，如姓名、组织、地点和联系电话。在提供的代码片段中，任务是识别和提取来自各种文本段落的人名、组织名称、地理位置和联系电话。
- en: The prompt provides clear instructions for the entity extraction task, specifying
    the entities of interest and their corresponding categories. It includes examples
    that illustrate how to extract information from different texts, showcasing the
    versatility of the entity extraction process.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 提示为实体提取任务提供了清晰的指令，指明了感兴趣的实体及其对应的类别。它包括示例，说明了如何从不同的文本中提取信息，展示了实体提取过程的灵活性。
- en: The code utilizes the OpenAI API to generate responses that include extracted
    entities, such as people’s names, organization names, locations, and contact numbers,
    from the given text passages. The output is structured in a JSON format, making
    it easy to parse and integrate the extracted entities into further processing
    or analysis.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 代码使用 OpenAI API 生成响应，包括从给定文本段落中提取的实体，如人名、组织名称、地点和联系电话。输出以 JSON 格式结构化，便于解析和将提取的实体集成到进一步的处理或分析中。
- en: 'This example demonstrates the practical application of entity extraction for
    extracting relevant information from diverse textual data, showcasing its potential
    in various domains, such as customer relationship management, information retrieval,
    and data analysis:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了实体提取在从多样化的文本数据中提取相关信息方面的实际应用，展示了其在客户关系管理、信息检索和数据分析等各个领域的潜力：
- en: '[PRE17]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here’s the output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now let’s extract the required information name, organization, location, and
    contact information from the output JSON, as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从输出 JSON 中提取所需的信息：名称、组织、地点和联系方式，如下所示：
- en: '[PRE19]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here’s the output:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE20]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Use case 5 – aspect-based sentiment analysis
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例 5 – 基于方面的情感分析
- en: Sentiment aspect analysis is a sophisticated NLP task that involves evaluating
    the sentiment expressed towards specific aspects or features within a given text.
    In the provided code snippet, aspect-based sentiment analysis is conducted on
    product reviews, aiming to assess both the overall sentiment of the reviews and
    the sentiment polarity associated with individual aspects mentioned.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 情感方面分析是一个复杂的自然语言处理任务，涉及评估给定文本中特定方面或特征的所表达的情感。在提供的代码片段中，对产品评论进行了基于方面的情感分析，旨在评估评论的整体情感以及与提到的各个方面的情感极性。
- en: The prompt outlines the objectives of the sentiment analysis task, which include
    providing an overall sentiment score for each review on a scale from 0 to 5, assigning
    sentiment polarity scores between 0 and 5 for each aspect, and identifying the
    top positive and negative aspects.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 提示概述了情感分析任务的目标，包括为每个评论提供一个从 0 到 5 的整体情感分数，为每个方面分配 0 到 5 之间的情感极性分数，并识别最正面和最负面的方面。
- en: The code processes multiple product reviews, extracting sentiments associated
    with aspects such as camera quality, battery life, design, speaker quality, performance,
    keyboard, display, trackpad responsiveness, sound quality, touch controls, graphics,
    load times, online community, subscription fee, and controller.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 代码处理了多个产品评论，提取了与相机质量、电池寿命、设计、扬声器质量、性能、键盘、显示、触摸板响应速度、音质、触控、图形、加载时间、在线社区、订阅费和控制器等方面相关的情感。
- en: The output includes comprehensive sentiment scores, polarity scores, and the
    identification of the most positively and negatively rated aspects in each review.
    This example illustrates how aspect-based sentiment analysis can provide detailed
    insights into the nuanced opinions expressed in diverse reviews, assisting businesses
    in understanding customer sentiments towards specific product features.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 输出包括全面的情感分数、极性分数，以及识别每个评论中最正面和最负面的方面。这个例子说明了基于方面的情感分析如何提供对多样化评论中细微观点的详细见解，帮助企业在理解客户对特定产品功能的情感态度。
- en: 'Let’s see the code example for aspect-based sentiment analysis:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看基于方面的情感分析的代码示例：
- en: '[PRE21]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here’s the output:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Next, let’s use the Snorkel API to classify this text data and generate labels
    by creating rule-based labeling functions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用 Snorkel API 来对这段文本数据进行分类，并通过创建基于规则的标注函数来生成标签。
- en: Hands-on labeling of text data using the Snorkel API
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Snorkel API 进行文本数据的实际标注
- en: In this section, we are going to learn how to label text data using the Snorkel
    API.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 Snorkel API 对文本数据进行标注。
- en: Snorkel provides an API for programmatically labeling text data using a small
    set of ground truth labels that are created by domain experts. Snorkel, an open
    source data labeling and training platform, is used by various companies and organizations
    across different industries, such as Google, Apple, Facebook, IBM, and SAP.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Snorkel 提供了一个 API，用于使用一组由领域专家创建的少量真实标签来编程式地标注文本数据。Snorkel 是一个开源的数据标注和训练平台，被不同行业的各种公司和组织使用，例如
    Google、Apple、Facebook、IBM 和 SAP。
- en: 'It has unique features that differentiate it from other competitors, especially
    in the context of weak supervision and programmatically generating labeled data.
    Here’s a comparison with some of the other tools:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有独特的功能，使其与其他竞争对手区分开来，尤其是在弱监督和编程式生成标注数据的情况下。以下是与一些其他工具的比较：
- en: '**Weak supervision**: Snorkel excels in scenarios where labeled data is scarce,
    and manual labeling is expensive. It allows users to programmatically label large
    amounts of data using heuristics, patterns, and external resources.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弱监督**：Snorkel 在标注数据稀缺且人工标注成本高昂的场景中表现出色。它允许用户通过启发式方法、模式和外部资源编程式地标注大量数据。'
- en: '**Flexible labeling functions**: Snorkel enables the creation of labeling functions,
    which are essentially heuristic functions that assign labels to data. This provides
    a flexible and scalable way to generate labeled data.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的标签函数**：Snorkel允许创建标签函数，这些函数本质上是一种启发式函数，用于为数据分配标签。这提供了一种灵活且可扩展的方式来生成标记数据。'
- en: '**Probabilistic labeling**: Snorkel generates probabilistic labels, acknowledging
    that labeling functions may have varying levels of accuracy. This probabilistic
    framework is useful in downstream tasks.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率标签**：Snorkel生成概率标签，承认标签函数可能具有不同级别的准确性。这种概率框架在下游任务中非常有用。'
- en: There can be a learning curve with Snorkel, especially for users who are new
    to weak supervision concepts. Other tools, such as Prodigy and Labelbox, are commercial
    tools and may involve licensing costs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Snorkel可能会存在学习曲线，尤其是对于新接触弱监督概念的初学者。其他工具，如Prodigy和Labelbox，是商业工具，可能涉及许可费用。
- en: When choosing between these tools, the specific requirements of the project,
    the available budget, and the expertise of the users play crucial roles. Snorkel
    stands out when weak supervision and programmatically generated labels are essential
    for the task at hand. It’s particularly well suited for scenarios where manual
    labeling is impractical or cost-prohibitive. Other tools may be more appropriate
    based on different use cases, interface preferences, and integration requirements.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择这些工具时，项目的具体要求、可用的预算和用户的专长起着至关重要的作用。当弱监督和程序化生成的标签对于手头的任务至关重要时，Snorkel脱颖而出。它特别适合于手动标记不切实际或成本高昂的场景。其他工具可能更适合不同的用例、界面偏好和集成要求。
- en: We will create rule-based labeling functions using Snorkel and then apply these
    labeling functions to classify and label text.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Snorkel创建基于规则的标签函数，然后将这些标签函数应用于文本的分类和标签。
- en: We have seen what a labeling function is and how to create labeling functions
    in [*Chapter 2*](B18944_02.xhtml#_idTextAnchor043). Let’s recap. In Snorkel, a
    labeling function is a Python function that heuristically generates labels for
    a dataset. These functions are used in the process of weak supervision, where
    instead of relying solely on manually labeled data, a machine learning model is
    trained using noisy, imperfect, or weakly labeled data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了标签函数是什么以及如何创建标签函数，请参阅[*第2章*](B18944_02.xhtml#_idTextAnchor043)。让我们回顾一下。在Snorkel中，标签函数是一个Python函数，它启发式地为数据集生成标签。这些函数用于弱监督的过程，在这个过程中，不是完全依赖于手动标记的数据，而是使用噪声的、不完美的或弱标记的数据来训练机器学习模型。
- en: Here is an example Python code that uses the Snorkel API to label text data
    using rule-based labeling functions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用Snorkel API通过基于规则的标签函数标记文本数据的Python代码示例。
- en: 'Let’s install Snorkel using pip and import the required Python libraries for
    labeling as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用pip安装Snorkel，并导入用于标签的必需Python库，如下所示：
- en: '[PRE23]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Let’s break down the code into four steps and explain each one.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将代码分解为四个步骤，并解释每个步骤。
- en: '*Step 1*: Data preparation and labeling function definition. This step prepares
    the data and defines the labeling functions. It first imports the Pandas library
    and defines some constants for the labels. It then creates a DataFrame with movie
    reviews and splits it into a training set and a test set. The true labels for
    the test set are defined and converted to a NumPy array. Finally, it defines three
    labeling functions that label a review as positive, negative, or abstain based
    on the presence of certain words:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 1*：数据准备和标签函数定义。此步骤准备数据并定义标签函数。首先导入Pandas库并定义一些标签的常量。然后创建一个包含电影评论的DataFrame，并将其分为训练集和测试集。测试集的真实标签被定义并转换为NumPy数组。最后，定义了三个标签函数，根据某些词的存在将评论标记为正面、负面或弃权：'
- en: '[PRE24]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now let’s define the labeling functions, one for positive reviews, one for
    negative reviews, and one for neutral reviews, using regular expressions as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义标签函数，一个用于正面评论，一个用于负面评论，一个用于中性评论，如下使用正则表达式：
- en: '[PRE25]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*Step 2*: Applying labeling functions and majority voting. This chunk of code
    applies the labeling functions to the training and test sets, and then uses a
    majority vote model to predict the labels. It first creates a list of the labeling
    functions and applies them to the training and test sets using `PandasLFApplier`.
    It then prints the resulting label matrices and their shapes. It imports the `MajorityLabelVoter`
    and `LabelModel` classes from Snorkel, creates a majority vote mode, and uses
    it to predict the labels for the training set:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2*：应用标签函数和多数投票。这段代码将标签函数应用于训练集和测试集，然后使用多数投票模型来预测标签。它首先创建一个标签函数列表，并使用`PandasLFApplier`将它们应用于训练集和测试集。然后，它打印出结果标签矩阵及其形状。它从Snorkel导入`MajorityLabelVoter`和`LabelModel`类，创建一个多数投票模型，并使用它来预测训练集的标签：'
- en: '[PRE26]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here’s the output:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: "![Figure 7.\uFEFF4 – Label matrices](img/B18944_07_04.jpg)"
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 标签矩阵](img/B18944_07_04.jpg)'
- en: Figure 7.4 – Label matrices
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 标签矩阵
- en: 'Let’s calculate the accuracy of the model using `MajorityLabelVoter` model
    on the test set and print it:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`MajorityLabelVoter`模型在测试集上计算模型的准确率并打印出来：
- en: '[PRE27]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here’s the output:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE28]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, it predicts the labels for the training set and prints them:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它为训练集预测标签并打印出来：
- en: '[PRE29]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here’s the output:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE30]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '*Step 3*: Training a label model and predicting labels. This chunk of code
    trains a label model and uses it to predict the labels. It creates a `LabelModel`
    with a `cardinality` of `2` (for the two labels, positive and negative), fits
    it to the training set, and calculates its accuracy on the test set:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤3*：训练标签模型并预测标签。这段代码训练一个标签模型并使用它来预测标签。它创建一个`LabelModel`，其`cardinality`为`2`（对于两个标签，正面和负面），将其拟合到训练集，并在测试集上计算其准确率：'
- en: '[PRE31]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here’s the output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: "![Figure 7.\uFEFF5 – Training a LabelModel](img/B18944_07_05.jpg)"
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 训练标签模型](img/B18944_07_05.jpg)'
- en: Figure 7.5 – Training a LabelModel
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 训练标签模型
- en: 'It then predicts the labels for the training set and prints them:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它为训练集预测标签并打印出来：
- en: '[PRE32]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here’s the output:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE33]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '*Step 4*: Analyzing labeling functions and creating a DataFrame with predicted
    labels. We can use the `LFAnalysis` class to analyze the labeling functions by
    passing the labels (`L`) and the list of labeling functions (`lfs`). The `lf_summary()`
    method provides an overview of the labeling functions and their coverage:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤4*：分析标签函数并创建包含预测标签的DataFrame。我们可以使用`LFAnalysis`类通过传递标签（`L`）和标签函数列表（`lfs`）来分析标签函数。`lf_summary()`方法提供了标签函数及其覆盖范围的概述：'
- en: '[PRE34]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here’s the output:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: "![Figure 7.\uFEFF6 – LFAnalysis summary](img/B18944_07_06.jpg)"
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – LFAnalysis摘要](img/B18944_07_06.jpg)'
- en: Figure 7.6 – LFAnalysis summary
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – LFAnalysis摘要
- en: 'The table is a summary of the results from LFAnalysis, specifically for three
    labeling functions: `lf_positive_review`, `lf_negative_review`, and `if_neutral_review`.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 该表是LFAnalysis的结果摘要，具体针对三个标签函数：`lf_positive_review`、`lf_negative_review`和`if_neutral_review`。
- en: 'Let’s break down the columns:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解列：
- en: '`j`: The index of the labeling function in the list of labeling functions.
    Here, `j=0` corresponds to `lf_positive_review`, and `j=1` corresponds to `lf_negative_review`.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`j`：标签函数在标签函数列表中的索引。在这里，`j=0`对应于`lf_positive_review`，而`j=1`对应于`lf_negative_review`。'
- en: '`Polarity`: The polarity assigned to the labeling function, representing the
    label value assigned by the function. In this case, `lf_positive_review` has a
    polarity of `[0, 1]`, meaning it assigns both label `0` and label `1`. On the
    other hand, `lf_negative_review` has a polarity of `[0]`, indicating it only assigns
    label `0`.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`极性`：分配给标签函数的极性，表示函数分配的标签值。在这种情况下，`lf_positive_review`的极性为`[0, 1]`，意味着它分配了标签`0`和标签`1`。另一方面，`lf_negative_review`的极性为`[0]`，表示它只分配标签`0`。'
- en: '`Coverage`: The set of labels predicted by the labeling function. For `lf_positive_review`,
    it predicts both label `0` and label `1` (`[0, 1]`), indicating it provides a
    non-abstain output for all examples. However, `lf_negative_review` predicts only
    label `0` (`[0]`), meaning it provides a non-abstain output for only 55.25% of
    the examples.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`覆盖范围`：标签函数预测的标签集合。对于`lf_positive_review`，它预测了标签`0`和标签`1`（`[0, 1]`），表示它为所有示例提供了非弃权输出。然而，`lf_negative_review`只预测标签`0`（`[0]`），意味着它只为55.25%的示例提供了非弃权输出。'
- en: '`Overlaps`: The percentage of examples for which the labeling function provides
    a non-abstain output. It represents the extent to which the labeling function
    is applicable. In this case, both `lf_positive_review` and `lf_negative_review`
    have a coverage of 0.5525, indicating that they provide a non-abstain label for
    55.25% of the examples.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Overlaps`：对于提供非弃权输出的标签函数的示例百分比。它表示标签函数适用的程度。在这种情况下，`lf_positive_review` 和
    `lf_negative_review` 的覆盖率为 0.5525，表明它们为 55.25% 的示例提供了非弃权标签。'
- en: '`Conflicts`: The percentage of examples for which the labeling function disagrees
    with at least one other labeling function. It measures the level of conflict between
    the labeling function and other functions. Both `lf_positive_review` and `lf_negative_review`
    have a conflict value of 0.2105, indicating they have conflicts with other labeling
    functions in approximately 21.05% of the examples.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Conflicts`：对于至少与其他一个标签函数存在分歧的示例的百分比。它衡量标签函数与其他函数之间的冲突水平。`lf_positive_review`
    和 `lf_negative_review` 的冲突值为 0.2105，表明它们在大约 21.05% 的示例中与其他标签函数存在冲突。'
- en: This summary provides insights into the performance, coverage, and conflicts
    of the labeling functions, allowing you to assess their effectiveness and identify
    areas of improvement in your labeling process.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这个摘要提供了对标签函数的性能、覆盖率和冲突的见解，使你能够评估它们的有效性，并在你的标签过程中识别改进的领域。
- en: 'Lastly, the following chunk of code analyzes the labeling functions and creates
    a DataFrame with the predicted labels. It uses the `LFAnalysis` class from Snorkel
    to analyze the labeling functions and print a summary. It then creates a DataFrame
    with the predicted labels:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以下代码块分析标签函数并创建一个包含预测标签的 DataFrame。它使用 Snorkel 的 `LFAnalysis` 类来分析标签函数并打印摘要。然后，它创建一个包含预测标签的
    DataFrame：
- en: '[PRE35]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here’s the output:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: "![Figure 7.\uFEFF7 – Predicted labels](img/B18944_07_07.jpg)"
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 预测标签](img/B18944_07_07.jpg)'
- en: Figure 7.7 – Predicted labels
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 预测标签
- en: In this example, we first created the `Movie Reviews` DataFrame. We then defined
    three rule-based labeling functions using regular expressions to label reviews
    as positive, negative, or neutral based on the presence of certain keywords. We
    applied these labeling functions to the text data using the `PandasLFApplier`
    provided by the Snorkel API. Finally, we analyzed the labeled data using `LFAnalysis`
    and printed a summary of the results.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们首先创建了 `Movie Reviews` DataFrame。然后，我们定义了三个基于规则的标签函数，使用正则表达式根据某些关键词的存在将评论标注为正面、负面或中性。我们使用
    Snorkel API 提供的 `PandasLFApplier` 将这些标签函数应用于文本数据。最后，我们使用 `LFAnalysis` 分析了标注数据，并打印了结果摘要。
- en: Note that this is a simple example and you may need to adjust the code depending
    on the specific requirements of your use case. Also, you can add more labeling
    functions depending on your task, and these functions should be carefully designed
    and tested to ensure high-quality labels.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是一个简单的示例，你可能需要根据你用例的具体要求调整代码。此外，你可以根据你的任务添加更多的标签函数，并且这些函数应该被精心设计和测试，以确保高质量的标签。
- en: Now, let’s look into labeling the data using logistic regression.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用逻辑回归进行数据标注。
- en: Hands-on text labeling using Logistic Regression
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行实战文本标注
- en: Text labeling is a crucial task in NLP, enabling the categorization of textual
    data into predefined classes or sentiments. Logistic Regression, a popular machine
    learning algorithm, proves effective in text classification scenarios. In the
    following code, we walk through the process of using Logistic Regression to classify
    movie reviews into positive or negative sentiments. Here’s a breakdown of the
    code.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 文本标注是自然语言处理中的一个关键任务，它使文本数据能够被分类到预定义的类别或情感中。逻辑回归，一种流行的机器学习算法，在文本分类场景中证明是有效的。在下面的代码中，我们将通过使用逻辑回归将电影评论分类为正面或负面情感的过程进行说明。以下是代码的分解。
- en: '*Step 1*. Import necessary libraries and modules.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 1*. 导入必要的库和模块。'
- en: 'The code begins by importing the necessary libraries and modules. These include
    NLTK for NLP, scikit-learn for machine learning, and specific modules for sentiment
    analysis, text preprocessing, and classification:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先导入必要的库和模块。这些包括用于自然语言处理的 NLTK、用于机器学习的 scikit-learn，以及用于情感分析、文本预处理和分类的特定模块：
- en: '[PRE36]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '*Step 2*. Download the necessary NLTK data. The code downloads the movie reviews
    dataset and other necessary NLTK data, such as the WordNet lemmatizer and the
    Punkt tokenizer:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2*。下载必要的NLTK数据。代码下载电影评论数据集和其他必要的NLTK数据，如WordNet词元化和Punkt分词器：'
- en: '[PRE37]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '*Step 3*. Initialize the sentiment analyzer and get movie review IDs. The code
    initializes a sentiment analyzer and gets the IDs of the movie reviews:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤3*。初始化情感分析器和获取电影评论ID。代码初始化一个情感分析器并获取电影评论的ID：'
- en: '[PRE38]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '*Step 4*. Preprocessing setup. The code sets up the preprocessing tools, including
    a lemmatizer and a list of English stopwords. It also defines a preprocessing
    function that tokenizes the text, removes stop words, and lemmatizes the words:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤4*。预处理设置。代码设置了预处理工具，包括词元化和英语停用词列表。它还定义了一个预处理函数，该函数对文本进行分词、去除停用词并进行词元化：'
- en: '[PRE39]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '*Step 5*. Feature extraction. The code sets up a TF-IDF vectorizer with the
    preprocessing function and uses it to transform the movie reviews into a feature
    matrix:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤5*。特征提取。代码设置了一个带有预处理函数的TF-IDF向量器，并使用它将电影评论转换为特征矩阵：'
- en: '[PRE40]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '*Step 6*. Create a target vector. The code creates a target vector with the
    categories of the movie reviews:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤6*。创建目标向量。代码创建了一个包含电影评论类别的目标向量：'
- en: '[PRE41]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '*Step 7*. Split the data. The code splits the data into training and test sets:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤7*。分割数据。代码将数据分割为训练集和测试集：'
- en: '[PRE42]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '*Step 8*. Model training. The code initializes a Logistic Regression classifier
    and trains it on the training data:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤8*。模型训练。代码初始化一个逻辑回归分类器，并在训练数据上对其进行训练：'
- en: '[PRE43]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '*Step 9*. Model evaluation. The code evaluates the model on the test data and
    prints the accuracy:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤9*。模型评估。代码在测试数据上评估模型并打印准确率：'
- en: '[PRE44]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Here’s the output:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: "![Figure 7.\uFEFF8 – Accuracy of  logistic regression](img/B18944_07_08.jpg)"
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 逻辑回归的准确率](img/B18944_07_08.jpg)'
- en: Figure 7.8 – Accuracy of logistic regression
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 逻辑回归的准确率
- en: '*Step 10*. Testing with custom sentences. The code tests the model with custom
    sentences. It preprocesses the sentences, transforms them into features, predicts
    their sentiment, and prints the results:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤10*。使用自定义句子进行测试。代码使用自定义句子测试模型。它预处理句子，将它们转换为特征，预测它们的情感，并打印结果：'
- en: '[PRE45]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here’s the output:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: "![Figure 7.\uFEFF9 – Predicted labels](img/B18944_07_09.jpg)"
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 预测标签](img/B18944_07_09.jpg)'
- en: Figure 7.9 – Predicted labels
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 预测标签
- en: This code serves as a comprehensive guide to text labeling using logistic regression,
    encompassing data preprocessing, model training, evaluation, and application to
    custom sentences.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码作为使用逻辑回归进行文本标记的全面指南，涵盖了数据预处理、模型训练、评估以及应用于自定义句子。
- en: Now, let’s look into the second method, K-means clustering, to label the text
    data by grouping similar text together and creating labels for that group or cluster.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看第二种方法，K-means聚类，通过将相似文本分组并为此组或簇创建标签来对文本数据进行标记。
- en: Hands-on label prediction using K-means clustering
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用K-means聚类进行手动的标签预测
- en: K-means clustering is a powerful unsupervised machine learning technique used
    for grouping similar data points into clusters. In the context of text data, K-means
    clustering can be employed to predict labels or categories for the given text
    based on their similarity. The provided code showcases how to utilize K-Means
    clustering to predict labels for movie reviews, breaking down the process into
    several key steps.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类是一种强大的无监督机器学习技术，用于将相似的数据点分组到簇中。在文本数据的上下文中，K-means聚类可以用来根据文本的相似性预测给定的文本的标签或类别。提供的代码展示了如何利用K-Means聚类来预测电影评论的标签，将整个过程分解为几个关键步骤。
- en: '*Step 1*: Importing libraries and downloading data.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤1*：导入库和下载数据。'
- en: 'The following code begins by importing essential libraries such as scikit-learn
    and NLTK. It then downloads the necessary NLTK data, including the movie reviews
    dataset:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码首先导入必要的库，如scikit-learn和NLTK。然后下载必要的NLTK数据，包括电影评论数据集：
- en: '[PRE46]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '*Step 2*: Retrieving and preprocessing movie reviews.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2*：检索和预处理电影评论。'
- en: 'Retrieve movie reviews from the NLTK dataset and preprocess them. This involves
    lemmatization, removal of stop words, and converting text to lowercase:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 从NLTK数据集中检索电影评论并进行预处理。这包括词元化、去除停用词并将文本转换为小写：
- en: '[PRE47]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '*Step 3*: Creating the TF-IDF vectorizer and transforming data.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤3*：创建TF-IDF向量器和转换数据。'
- en: 'Create a TF-IDF vectorizer to convert the preprocessed reviews into numerical
    features. This step is crucial for preparing the data for clustering:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 TF-IDF 向量化器，将预处理后的评论转换为数值特征。这一步对于为聚类准备数据至关重要：
- en: '[PRE48]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '*Step 4*: Applying K-means clustering.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 4*：应用 K-means 聚类。'
- en: 'Apply K-means clustering to the TF-IDF features, specifying the number of clusters.
    In this case, the code sets `n_clusters=3`:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 将 TF-IDF 特征应用于 K-means 聚类，指定聚类数量。在本例中，代码设置 `n_clusters=3`：
- en: '[PRE49]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '*Step 5*: Labeling and testing with custom sentences.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 5*：使用自定义句子进行标签化和测试。'
- en: 'Define labels for the clusters and test the K-means classifier with custom
    sentences. The code preprocesses the sentences, transforms them into TF-IDF features,
    predicts the cluster, and assigns a label based on the predefined cluster labels:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 定义聚类标签并使用自定义句子测试 K-means 分类器。代码对句子进行预处理，将它们转换为 TF-IDF 特征，预测聚类，并根据预定义的聚类标签分配标签：
- en: '[PRE50]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here’s the output:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果：
- en: "![Figure 7.\uFEFF10 – K-means clustering for text](img/B18944_07_10.jpg)"
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 文本 K-means 聚类](img/B18944_07_10.jpg)'
- en: Figure 7.10 – K-means clustering for text
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 文本 K-means 聚类
- en: This code demonstrates a comprehensive process of utilizing K-means clustering
    for text label prediction, covering data preprocessing, feature extraction, clustering,
    and testing with custom sentences.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码演示了使用 K-means 聚类进行文本标签预测的全面过程，包括数据预处理、特征提取、聚类以及使用自定义句子进行测试。
- en: Generating labels for customer reviews (sentiment analysis)
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成客户评论标签（情感分析）
- en: Customer reviews are a goldmine of information for businesses. Analyzing sentiment
    in customer reviews helps in understanding customer satisfaction, identifying
    areas for improvement, and making data-driven business decisions.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 客户评论是企业信息宝库。分析客户评论中的情感有助于了解客户满意度，确定改进领域，并做出数据驱动的商业决策。
- en: In the following example, we delve into sentiment analysis using a neural network
    model. The code utilizes TensorFlow and Keras to create a simple neural network
    architecture with an embedding layer, a flatten layer, and a dense layer. The
    model is trained on a small labeled dataset for sentiment classification, distinguishing
    between positive and negative sentiments. Following training, the model is employed
    to classify new sentences. The provided Python code demonstrates each step, from
    tokenizing and padding sequences to compiling, training, and making predictions.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们深入探讨使用神经网络模型进行情感分析。代码利用 TensorFlow 和 Keras 创建了一个简单的神经网络架构，包括嵌入层、展平层和密集层。该模型在用于情感分类的小型标记数据集上训练，区分积极和消极情绪。训练后，该模型用于分类新句子。提供的
    Python 代码演示了从分词和填充序列到编译、训练和预测的每个步骤。
- en: 'The following dataset is used for training on sentiment analysis:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 以下数据集用于情感分析训练：
- en: '[PRE51]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We then use a tokenizer to convert the text into sequences of numbers, and then
    pad the sequences so that they have the same length. We then define a generative
    AI model with an embedding layer, a flatten layer, and a dense layer. Then, we
    compile and train the model on the training data. Finally, we use the trained
    model to classify a new sentence as either positive or negative.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用分词器将文本转换为数字序列，然后填充序列以确保它们具有相同的长度。然后我们定义一个具有嵌入层、展平层和密集层的生成式 AI 模型。然后，我们在训练数据上编译和训练模型。最后，我们使用训练好的模型将新句子分类为积极或消极。
- en: 'Here is a complete Python code example with a dataset of four sentences labeled
    as positive or negative. We begin by importing libraries:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个包含四个句子数据集的完整 Python 代码示例，这些句子被标记为积极或消极。我们首先导入库：
- en: '[PRE52]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The NumPy library is imported as `np` for numerical computations. The necessary
    modules from the TensorFlow library are imported for text preprocessing and model
    creation. Then we define the labeled dataset:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 将 NumPy 库导入为 `np` 以进行数值计算。从 TensorFlow 库中导入必要的模块用于文本预处理和模型创建。然后我们定义标记的数据集：
- en: '[PRE53]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The `sentences` list contains textual sentences. The `labels` list contains
    corresponding labels where 1 represents a positive sentiment and 0 represents
    a negative sentiment. Next, we tokenize the text and convert it to sequences:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '`sentences` 列表包含文本句子。`labels` 列表包含相应的标签，其中 1 代表积极情绪，0 代表消极情绪。接下来，我们对文本进行分词并将其转换为序列：'
- en: '[PRE54]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'A `Tokenizer` object is created to tokenize the text. The `fit_on_texts` method
    is used to fit the tokenizer on the provided sentences. The `texts_to_sequences`
    method is used to convert the sentences into sequences of tokens. Now we need
    to pad the sequences so they are the same length:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了一个`Tokenizer`对象来分词文本。使用`fit_on_texts`方法将分词器拟合到提供的句子上。使用`texts_to_sequences`方法将句子转换为标记序列。现在我们需要填充序列，使它们的长度相同：
- en: '[PRE55]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The maximum sequence length is determined by finding the length of the longest
    sequence. The `pad_sequences` function is used to pad the sequences to the maximum
    length. Next, we define the model architecture:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 最大序列长度是通过找到最长序列的长度来确定的。使用`pad_sequences`函数将序列填充到最大长度。接下来，我们定义模型架构：
- en: '[PRE56]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'A sequential model is created using the `Sequential` class from Keras. The
    model consists of an embedding layer, a flatten layer, and a dense layer. The
    embedding layer converts the tokens into dense vectors. The flatten layer flattens
    the input for the subsequent dense layer. The dense layer is used for binary classification
    with sigmoid activation. Now, we need to compile the model:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras的`Sequential`类创建了一个顺序模型。该模型由一个嵌入层、一个展平层和一个密集层组成。嵌入层将标记转换为密集向量。展平层将输入展平以供后续的密集层使用。密集层用于具有sigmoid激活的二元分类。现在，我们需要编译模型：
- en: '[PRE57]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The model is compiled with the Adam optimizer, binary cross-entropy loss, and
    accuracy as the metric. Now, we train the model:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用Adam优化器、二元交叉熵损失和准确率作为指标进行编译。现在，我们开始训练模型：
- en: '[PRE58]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The model is trained on the padded sequences and corresponding labels for a
    specified number of epochs. Next, we classify a new sentence:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在填充的序列和相应的标签上训练了指定数量的轮次。接下来，我们分类一个新句子：
- en: '[PRE59]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'A new sentence is provided for classification. The sentence is converted to
    a sequence of tokens using the tokenizer. The sequence is padded to match the
    maximum sequence length used during training. The model predicts the sentiment
    class for the new sentence. Finally, we print the predicted label:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了一个新的句子用于分类。该句子通过分词器转换为一系列标记。该序列被填充以匹配训练期间使用的最大序列长度。模型预测新句子的情感类别。最后，我们打印出预测的标签：
- en: '[PRE60]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Here’s the output:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: "![Figure 7.1\uFEFF1 – Prediction with a neural network model](img/B18944_07_11.jpg)"
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: "![图7.1\uFEFF1 – 使用神经网络模型的预测](img/B18944_07_11.jpg)"
- en: Figure 7.11 – Prediction with a neural network model
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 使用神经网络模型的预测
- en: The predicted label is printed based on the prediction output. If the predicted
    label is `1`, it is considered a positive sentiment, and if it is `0`, it is considered
    a negative sentiment. In summary, the provided code demonstrates a sentiment analysis
    task using a neural network model.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 根据预测输出打印出预测的标签。如果预测标签是`1`，则被认为是积极情绪，如果是`0`，则被认为是消极情绪。总之，提供的代码演示了使用神经网络模型进行情感分析的任务。
- en: Summary
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we delved into the realm of text data exploration using Python,
    gaining a comprehensive understanding of harnessing Generative AI and OpenAI models
    for effective text data labeling. Through code examples, we explored diverse text
    data labeling tasks, including classification, summarization, and sentiment analysis.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了使用Python进行文本数据探索的领域，全面了解了利用生成式AI和OpenAI模型进行有效的文本数据标记。通过代码示例，我们探讨了包括分类、摘要和情感分析在内的各种文本数据标记任务。
- en: We then extended our knowledge by exploring Snorkel labeling functions, allowing
    us to label text data with enhanced flexibility. Additionally, we delved into
    the application of K-means clustering for labeling text data and concluded by
    discovering how to label customer reviews using neural networks.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过探索Snorkel标记函数扩展了我们的知识，这使得我们能够以增强的灵活性标记文本数据。此外，我们深入研究了K-means聚类在标记文本数据中的应用，并通过发现如何使用神经网络标记客户评论来结束研究。
- en: With these acquired skills, you now possess the tools to unlock the full potential
    of your text data, extracting valuable insights for various applications. The
    next chapter awaits, where we will shift our focus to video data exploration,
    exploring different methods to gain insights from this dynamic data type.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 通过获得这些技能，你现在拥有了解锁文本数据全部潜力的工具，为各种应用提取有价值的见解。下一章等待着，我们将把重点转向视频数据探索，探索从这种动态数据类型中获取洞察力的不同方法。
