<html><head></head><body>
		<div id="_idContainer477">
			<h1 id="_idParaDest-93"><a id="_idTextAnchor100"/>Chapter 6: Social Network Graphs</h1>
			<p>The growth of social networking sites has been one of the most active trends in digital media over the years. Since the late 1990s, when the first social applications were published, they have attracted billions of active users worldwide, many of whom have integrated digital social interactions into their daily lives. New ways of communication are being driven by social networks such as Facebook, Twitter, and Instagram, among others. Users can share ideas, post updates and feedback, or engage in activities and events while sharing their broader interests on social networking sites.</p>
			<p>Besides, social networks constitute a huge source of information for studying user behaviors, interpreting interaction among people, and predicting their interests. Structuring them as graphs, where a vertex corresponds to a person and an edge represents the connection between them, enables a powerful tool to extract useful knowledge.</p>
			<p>However, understanding the dynamics that drive the evolution of a social network is a complex problem due to a large number of variable parameters.</p>
			<p>In this chapter, we will talk about how we can analyze the Facebook social network using graph theory and how we can solve useful problems such as link prediction and community detection using machine learning.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Overview of the dataset</li>
				<li>Network topology and community detection </li>
				<li>Embedding for supervised and unsupervised tasks</li>
			</ul>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor101"/>Technical requirements</h1>
			<p>We will be using <em class="italic">Jupyter</em> notebooks with <em class="italic">Python</em> 3.8 for all of our exercises. The following is a list of the Python libraries that need to be installed for this chapter using <strong class="source-inline">pip</strong>. For example, run <strong class="source-inline">pip install networkx==2.5</strong> on the command line:</p>
			<p class="source-code">Jupyter==1.0.0</p>
			<p class="source-code">networkx==2.5</p>
			<p class="source-code">scikit-learn==0.24.0 </p>
			<p class="source-code">numpy==1.19.2 </p>
			<p class="source-code">node2vec==0.3.3 </p>
			<p class="source-code">tensorflow==2.4.1 </p>
			<p class="source-code">stellargraph==1.2.1</p>
			<p class="source-code">communities==2.2.0 </p>
			<p class="source-code">git+https://github.com/palash1992/GEM.git</p>
			<p>In the rest of this chapter, if not clearly stated, we will refer to <strong class="source-inline">nx</strong>, <strong class="source-inline">pd</strong>, and <strong class="source-inline">np</strong> as results of the following Python commands: <strong class="source-inline">import networkx</strong> as <strong class="source-inline">nx</strong>, <strong class="source-inline">import pandas</strong> as <strong class="source-inline">pd</strong>, and <strong class="source-inline">import numpy</strong> as <strong class="source-inline">np</strong>.</p>
			<p>All code files relevant to this chapter are available at <a href="https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter06">https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter06</a>.</p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor102"/>Overview of the dataset</h1>
			<p>We will be using the <strong class="bold">Social circles SNAP Facebook public dataset</strong>, from Stanford University (<a href="https://snap.stanford.edu/data/ego-Facebook.html">https://snap.stanford.edu/data/ego-Facebook.html</a>).</p>
			<p>The dataset<a id="_idIndexMarker629"/> was created by collecting Facebook user information from survey participants. Ego networks were created from 10 users. Each user was <a id="_idIndexMarker630"/>asked to identify all the <strong class="bold">circles</strong> (list of friends) to which their friends belong. On average, each user identified 19 circles in their <strong class="bold">ego networks</strong>, where each circle has on average 22 friends.</p>
			<p>For each user, the following information was collected:</p>
			<ul>
				<li><strong class="bold">Edges</strong>: An edge exists if two users are friends on Facebook.</li>
				<li><strong class="bold">Node features</strong>: Features we labeled <strong class="source-inline">1</strong> if the user has this property in their profile and <strong class="source-inline">0</strong> otherwise. Features have been anonymized since the names of the features would reveal private data.</li>
			</ul>
			<p>The 10 ego networks were then <a id="_idIndexMarker631"/>unified in a single graph that we are going to study.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor103"/>Dataset download</h2>
			<p>The dataset can be retrieved using the following URL: <a href="https://snap.stanford.edu/data/ego-Facebook.html">https://snap.stanford.edu/data/ego-Facebook.html</a>. In particular, three files can be downloaded: <strong class="source-inline">facebook.tar.gz</strong>, <strong class="source-inline">facebook_combined.txt.gz</strong>, and <strong class="source-inline">readme-Ego.txt</strong>. Let's inspect each<a id="_idIndexMarker632"/> file separately:</p>
			<ul>
				<li><strong class="source-inline">facebook.tar.gz</strong>: This is an archive containing four files for each <strong class="bold">ego user </strong>(40 files in total). Each file is named <strong class="source-inline">nodeId.extension</strong> where <strong class="source-inline">nodeId</strong> is the node ID of the ego user and <strong class="source-inline">extension</strong> is either <strong class="source-inline">edges</strong>, <strong class="source-inline">circles</strong>, <strong class="source-inline">feat</strong>, <strong class="source-inline">egofeat</strong>, or <strong class="source-inline">featnames</strong>. The following provides more details:<p>a. <strong class="source-inline">nodeId.edges</strong>: This contains a list of edges for the network of the <strong class="source-inline">nodeId</strong> node.</p><p>b. <strong class="source-inline">nodeId.circles</strong>: This contains several lines (one for each circle). Each line consists of a name (the circle name) followed by a series of node IDs.</p><p>c. <strong class="source-inline">nodeId.feat</strong>: This contains the features (<strong class="source-inline">0</strong> if <strong class="source-inline">nodeId</strong> has the feature, <strong class="source-inline">1</strong> otherwise) for each node in the ego network.</p><p>d. <strong class="source-inline">nodeId.egofeat</strong>: This contains the features for the ego user.</p><p>e. <strong class="source-inline">nodeId.featname</strong>: This contains the names of the features.</p></li>
				<li><strong class="source-inline">facebook_combined.txt.gz</strong>: This is an archive containing a single file, <strong class="source-inline">facebook_combined.txt</strong>, which is a list of edges from all the ego networks combined.</li>
				<li><strong class="source-inline">readme-Ego.txt</strong>: This contains a description for the previously mentioned files.</li>
			</ul>
			<p>Take a look at those files <a id="_idIndexMarker633"/>by yourself. It is strongly suggested to explore and become as comfortable as possible with the dataset before starting any machine learning task.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor104"/>Loading the dataset using networkx</h2>
			<p>The first step of our <a id="_idIndexMarker634"/>analysis will be loading the aggregated ego networks using <strong class="source-inline">networkx</strong>. As we have seen in previous chapters, <strong class="source-inline">networkx</strong> is powerful for graph analysis and, given the size of the datasets, will be the perfect tool for the<a id="_idIndexMarker635"/> analysis that we will be doing in this chapter. However, for larger social network graphs with billions of nodes and edges, more specific tools might be required for loading and processing them. We will cover the tools and technologies used for scaling out the analysis in<em class="italic"> </em><a href="B16069_09_Final_JM_ePub.xhtml#_idTextAnchor141"><em class="italic">Chapter 9</em></a>, <em class="italic">Building a Data-Driven Graph-Powered Application</em>.</p>
			<p>As we have seen, the combined ego network is represented as a list of edges. We can create an undirected graph from a list of edges using <strong class="source-inline">networkx</strong> as follows:</p>
			<p class="source-code">G = nx.read_edgelist("facebook_combined.txt", create_using=nx.Graph(), nodetype=int)</p>
			<p>Let's print some basic information about the graph:</p>
			<p class="source-code">print(nx.info(G))</p>
			<p>The output should be as follows:</p>
			<p class="source-code">Name: </p>
			<p class="source-code">Type: Graph</p>
			<p class="source-code">Number of nodes: 4039</p>
			<p class="source-code">Number of edges: 88234</p>
			<p class="source-code">Average degree:  43.6910</p>
			<p>As we can see, the aggregated network contains <strong class="source-inline">4039</strong> nodes and <strong class="source-inline">88234</strong> edges. This is a fairly connected network with a number of edges more than 20 times the number of nodes. Indeed, several clusters should be present in the aggregated networks (likely the small worlds of each ego user).</p>
			<p>Drawing the network <a id="_idIndexMarker636"/>will also help in better understanding what <a id="_idIndexMarker637"/>we are going to analyze. We can draw the graph using <strong class="source-inline">networkx</strong> as follows:</p>
			<p class="source-code">nx.draw_networkx(G, pos=spring_pos, with_labels=False, node_size=35)</p>
			<p>The output should be as follows:</p>
			<div>
				<div id="_idContainer467" class="IMG---Figure">
					<img src="image/B16069_06_01.jpg" alt="Figure 6.1 – The aggregated Facebook ego network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – The aggregated Facebook ego network</p>
			<p>We can observe the presence of highly interconnected hubs. This is interesting from a social network analysis point of view since they might be the result of underlying social mechanisms that can be further investigated for better understanding the structure of an individual's relationships with respect to their world.</p>
			<p>Before continuing our analysis, let's save the IDs of the ego user nodes inside the network. We can retrieve them from the files contained in the <strong class="source-inline">facebook.tar.gz</strong> archive.</p>
			<p>First, unpack the archive. The extracted folder will be named <strong class="source-inline">facebook</strong>. Let's run the following Python code for retrieving the IDs by taking the first part of each filename:</p>
			<p class="source-code">ego_nodes = set([int(name.split('.')[0]) for name in os.listdir("facebook/")])</p>
			<p>We are now ready for analyzing the graph. In particular, in the next section, we will better understand the structure of the graph by inspecting its properties. This will help us to have a clearer idea of its topology and its relevant characteristics.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor105"/>Network topology and community detection</h1>
			<p>Understanding the <a id="_idIndexMarker638"/>topology of the network as well as the role of its nodes is a crucial step in the analysis of a social network. It is important to keep in mind that, in this context, nodes<a id="_idIndexMarker639"/> are actually users, each with their own interests, habits, and behaviors. Such knowledge will be extremely useful when performing predictions and/or finding insights.</p>
			<p>We will be using <strong class="source-inline">networkx</strong> to compute most of the useful metrics we have seen in <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs</em>. We will try to give them an interpretation to collect insight into the graph. Let's begin as usual, by importing the required libraries and defining some variables that we will use throughout the code:</p>
			<p class="source-code">import os</p>
			<p class="source-code">import math</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">default_edge_color = 'gray'</p>
			<p class="source-code">default_node_color = '#407cc9'</p>
			<p class="source-code">enhanced_node_color = '#f5b042'</p>
			<p class="source-code">enhanced_edge_color = '#cc2f04'</p>
			<p>We can now proceed to the analysis.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor106"/>Topology overview</h2>
			<p>As we have<a id="_idIndexMarker640"/> already seen before, our combined network has 4,039 nodes and more than 80,000 edges. The next metric we will compute is assortativity. It will reveal information about the tendency of users to be connected with users with a similar degree. We can do that as follows:</p>
			<p class="source-code">assortativity = nx.degree_pearson_correlation_coefficient(G)</p>
			<p>The output should be as follows:</p>
			<p class="source-code">0.06357722918564912</p>
			<p>Here we can observe a positive assortativity, likely showing that well-connected individuals associate with other well-connected individuals (as we have seen in <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs</em>). This is expected since inside each circle users might tend to be highly connected to each other.</p>
			<p>Transitivity could also help at better understanding how individuals are connected. Recall transitivity indicates the mean probability that two people with a common friend are themselves friends:</p>
			<p class="source-code">t = nx.transitivity(G)</p>
			<p>The output should be as follows:</p>
			<p class="source-code">0.5191742775433075</p>
			<p>Here we have<a id="_idIndexMarker641"/> the probability of around 50% that two friends can or cannot have common friends.</p>
			<p>The observation is also confirmed by computing the average clustering coefficient. Indeed, it can be considered as an alternative definition of transitivity:</p>
			<p class="source-code">aC = nx.average_clustering(G)</p>
			<p>The output should be as follows:</p>
			<p class="source-code">0.6055467186200876</p>
			<p>Notice that the clustering coefficient tends to be higher than transitivity. Indeed, by definition, it puts more weight on vertices with a low degree, since they have a limited number of possible pairs of neighbors (the denominator of the local clustering coefficient).</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor107"/>Node centrality</h2>
			<p>Once we have a clearer<a id="_idIndexMarker642"/> idea of what the overall topology looks like, we can proceed by investigating the importance of each individual inside the network. As we have seen in <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs</em>, the first definition of importance can be given by means of the betweenness centrality metric. It measures how many shortest paths pass through a given node, giving an idea of how <em class="italic">central</em> that node is for the spreading of information inside the network. We can compute it using the following:</p>
			<p class="source-code">bC = nx.betweenness_centrality(G)</p>
			<p class="source-code"> np.mean(list(bC.values()))</p>
			<p>The output should be as follows:</p>
			<p class="source-code">0.0006669573568730229</p>
			<p>The average betweenness centrality is pretty low, which is understandable given the large amount of non-bridging nodes inside the network. However, we could collect better insight by <a id="_idIndexMarker643"/>visual inspection of the graph. In particular, we will draw the combined ego network by enhancing nodes with the highest betweenness centrality. Let's define a proper function for this:</p>
			<p class="source-code">def draw_metric(G, dct, spring_pos):</p>
			<p class="source-code">  top = 10</p>
			<p class="source-code">  max_nodes =  sorted(dct.items(), key=lambda v: -v[1])[:top]</p>
			<p class="source-code">  max_keys = [key for key,_ in max_nodes]</p>
			<p class="source-code">  max_vals = [val*300 for _, val in max_nodes]</p>
			<p class="source-code">  plt.axis("off")</p>
			<p class="source-code">  nx.draw_networkx(G,</p>
			<p class="source-code">                   pos=spring_pos,</p>
			<p class="source-code">                   cmap='Blues',</p>
			<p class="source-code">                   edge_color=default_edge_color,</p>
			<p class="source-code">                   node_color=default_node_color,</p>
			<p class="source-code">                   node_size=3,</p>
			<p class="source-code">                   alpha=0.4,</p>
			<p class="source-code">                   with_labels=False)</p>
			<p class="source-code">  nx.draw_networkx_nodes(G,</p>
			<p class="source-code">                         pos=spring_pos,</p>
			<p class="source-code">                         nodelist=max_keys,</p>
			<p class="source-code">                         node_color=enhanced_edge_color,</p>
			<p class="source-code">                         node_size=max_vals)</p>
			<p>Now let's invoke it as follows:</p>
			<p class="source-code">draw_metric(G,bC,spring_pos)</p>
			<p>The output should be as follows:</p>
			<div>
				<div id="_idContainer468" class="IMG---Figure">
					<img src="image/B16069_06_02.jpg" alt="Figure 6.2 – Betweenness centrality&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Betweenness centrality</p>
			<p>Let's also inspect the<a id="_idIndexMarker644"/> degree centrality of each node. Since this metric is related to the number of neighbors of a node, we will have a clearer idea of how well the nodes are connected to each other:</p>
			<p class="source-code">deg_C = nx.degree_centrality(G)</p>
			<p class="source-code"> np.mean(list(deg_C.values()))</p>
			<p class="source-code">draw_metric(G,deg_C,spring_pos)</p>
			<p>The output should be as follows:</p>
			<p class="source-code">0.010819963503439287</p>
			<p>Here is a representation of the degree centrality:</p>
			<div>
				<div id="_idContainer469" class="IMG---Figure">
					<img src="image/B16069_06_03.jpg" alt="Figure 6.3 – Degree centrality&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Degree centrality</p>
			<p>Finally, let's also have a look at the closeness centrality. This will help us understand how close nodes are to each other in terms of the shortest path:</p>
			<p class="source-code">clos_C = nx.closeness_centrality(G)</p>
			<p class="source-code"> np.mean(list(clos_C.values()))</p>
			<p class="source-code">draw_metric(G,clos_C,spring_pos)</p>
			<p>The output should be as follows:</p>
			<p class="source-code">0.2761677635668376</p>
			<p>Here is a<a id="_idIndexMarker645"/> representation of the closeness centrality:</p>
			<div>
				<div id="_idContainer470" class="IMG---Figure">
					<img src="image/B16069_06_04.jpg" alt="Figure 6.4 – Closeness centrality&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Closeness centrality</p>
			<p>From the centrality analysis, it is interesting to observe that each central node seems to be part of a sort of community (this is reasonable, since the central nodes might correspond to the ego nodes of the network). It is also interesting to notice the presence of a bunch of highly interconnected<a id="_idIndexMarker646"/> nodes (especially from the closeness centrality analysis). Let's thus identify these communities in the next part of our analysis.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor108"/>Community detection</h2>
			<p>Since we are<a id="_idIndexMarker647"/> performing social network analysis, it is worth exploring one of the most interesting graph structures for social networks: communities. If you use Facebook, it is very likely that your friends reflect different aspects of your life: friends from an educational environment (high school, college, and so on), friends from your weekly football match, friends you have met at parties, and so on.</p>
			<p>An interesting aspect of social network analysis is to automatically identify such groups. This can be done automatically, inferring them from topological properties, or semi-automatically, exploiting some prior insight.</p>
			<p>One good criterion is to try to minimize intra-community edges (edges connecting members of different communities) while maximizing inter-community edges (connecting members within the same community).</p>
			<p>We can do that in <strong class="source-inline">networkx</strong> as follows:</p>
			<p class="source-code">import community</p>
			<p class="source-code">parts = community.best_partition(G)</p>
			<p class="source-code"> values = [parts.get(node) for node in G.nodes()]</p>
			<p class="source-code">n_sizes = [5]*len(G.nodes())</p>
			<p class="source-code">plt.axis("off")</p>
			<p class="source-code">nx.draw_networkx(G, pos=spring_pos, cmap=plt.get_cmap("Blues"), edge_color=default_edge_color, node_color=values, node_size=n_sizes, with_labels=False)</p>
			<p>The output should be as follows:</p>
			<div>
				<div id="_idContainer471" class="IMG---Figure">
					<img src="image/B16069_06_05.jpg" alt="Figure 6.5 – Detected communities using networkx&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Detected communities using networkx</p>
			<p>In this context, it is also interesting to investigate whether the ego users occupy some roles inside the <a id="_idIndexMarker648"/>detected communities. Let's enhance the size and color of the ego user nodes as follows:</p>
			<p class="source-code">for node in ego_nodes:</p>
			<p class="source-code">   n_sizes[node] = 250</p>
			<p class="source-code">nodes = nx.draw_networkx_nodes(G,spring_pos,ego_nodes,node_color=[parts.get(node) for node in ego_nodes])</p>
			<p class="source-code"> nodes.set_edgecolor(enhanced_node_color)</p>
			<p>The output should be as follows:</p>
			<div>
				<div id="_idContainer472" class="IMG---Figure">
					<img src="image/B16069_06_06.jpg" alt="Figure 6.6 – Detected communities using networkx with the ego users node size enhanced&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Detected communities using networkx with the ego users node size enhanced</p>
			<p>It is interesting to notice that some ego users belong to the same community. It is possible that ego users are actual friends on Facebook, and therefore their ego networks are partially shared.</p>
			<p>We have now <a id="_idIndexMarker649"/>completed our basic understanding of the graph structure. We now know that some important nodes can be identified inside the network. We have also seen the presence of well-defined communities to which those nodes belong. Keep in mind these observations while performing the next part of the analysis, which is applying machine learning methods for supervised and unsupervised tasks.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor109"/>Embedding for supervised and unsupervised tasks</h1>
			<p>Social media<a id="_idIndexMarker650"/> represents, nowadays, one of the most interesting and rich sources of information. Every day, thousands of new connections arise, new users join communities, and billions of posts are shared. Graphs mathematically represent all those interactions, helping to make order of all such spontaneous and unstructured traffic.</p>
			<p>When dealing with social graphs, there are many interesting problems that can be addressed using machine learning. Under the correct settings, it is possible to extract useful insights from this huge amount of data, for improving your marketing strategy, identifying users with dangerous behaviors (for example, terrorist networks), and predicting the likelihood that a user will read your new post.</p>
			<p>Specifically, link prediction is one of the most interesting and important research topics in this field. Depending on what a <em class="italic">connection</em> in your social graph represents, by predicting future edges, you will be able to predict your next suggested friend, the next suggested movie, and which product you are likely to buy.</p>
			<p>As we have already seen in <a href="B16069_05_Final_JM_ePub.xhtml#_idTextAnchor079"><em class="italic">Chapter 5</em></a>, <em class="italic">Problems with Machine Learning on Graphs</em>, the link prediction task aims at forecasting the likelihood of a future connection between two nodes and it can be solved using several machine learning algorithms.</p>
			<p>In the next examples, we <a id="_idIndexMarker651"/>will be applying supervised and unsupervised machine learning graph embedding algorithms for predicting future connections on the SNAP Facebook social graph. Furthermore, we will evaluate the contribution of node features in the prediction task.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor110"/>Task preparation</h2>
			<p>In order to perform the link<a id="_idIndexMarker652"/> prediction task, it is necessary to prepare our dataset. The problem will be treated as a supervised task. Pairs of nodes will be provided to each algorithm as input, while the target will be binary, that is, <em class="italic">connected</em> if the two nodes are actually connected in the network, and <em class="italic">not connected</em> otherwise.</p>
			<p>Since we aim to cast this problem as a supervised learning task, we need to create a training and testing dataset. We will therefore create two new subgraphs with the same numbers of nodes but different numbers of edges (as some edges will be removed and treated as positive samples for training/testing the algorithm).</p>
			<p>The <strong class="source-inline">stellargraph</strong> library provides a useful tool for splitting the data and creating training and test reduced subgraphs. This process is similar to the one we have already seen in <a href="B16069_05_Final_JM_ePub.xhtml#_idTextAnchor079"><em class="italic">Chapter 5</em></a>, <em class="italic">Problems with Machine Learning on Graphs</em>:</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">from stellargraph.data import EdgeSplitter</p>
			<p class="source-code">from stellargraph import StellarGraph</p>
			<p class="source-code">edgeSplitter = EdgeSplitter(G) </p>
			<p class="source-code">graph_test, samples_test, labels_test = edgeSplitter.train_test_split(p=0.1, method="global", seed=24)</p>
			<p class="source-code">edgeSplitter = EdgeSplitter(graph_test, G)</p>
			<p class="source-code"> graph_train, samples_train, labels_train = edgeSplitter.train_test_split(p=0.1, method="global", seed=24)</p>
			<p>We are using the <strong class="source-inline">EdgeSplitter</strong> class to extract a fraction (<strong class="source-inline">p</strong>=10%) of all the edges in <strong class="source-inline">G</strong>, as well as the same number of negative edges, in order to obtain a reduced graph, <strong class="source-inline">graph_test</strong>. The <strong class="source-inline">train_test_split</strong> method also returns a list of node pairs, <strong class="source-inline">samples_test</strong> (where each pair corresponds to an existing or not existing edge in the graph), and a list of binary targets (<strong class="source-inline">labels_test</strong>) of the same length of the <strong class="source-inline">samples_test</strong> list. Then, from such a reduced graph, we are repeating the operation to obtain another reduced graph, <strong class="source-inline">graph_train</strong>, as well as the corresponding <strong class="source-inline">samples_train</strong> and <strong class="source-inline">labels_train</strong> lists.</p>
			<p>We will be comparing three different methods <a id="_idIndexMarker653"/>for predicting missing edges:</p>
			<ul>
				<li><strong class="bold">Method 1</strong>: node2vec will be used to learn a node embedding without supervision. The learned <a id="_idIndexMarker654"/>embedding will be used as input for a supervised classification algorithm to determine whether the input pair is actually connected.</li>
				<li><strong class="bold">Method 2</strong>: The graph neural network-based algorithm GraphSAGE will be used to jointly learn the embedding and perform the classification task.</li>
				<li><strong class="bold">Method 3</strong>: Hand-crafted features will be extracted from the graph and used as inputs for a supervised classifier, together with the nodes' IDs.</li>
			</ul>
			<p>Let's analyze them in more detail.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor111"/>node2vec-based link prediction</h2>
			<p>The herein proposed method is <a id="_idIndexMarker655"/>carried out in several steps:</p>
			<ol>
				<li>We use node2vec to generate node embeddings without supervision from the training graph. This can be done using the <strong class="source-inline">node2vec</strong> Python implementation, as we have already seen in <a href="B16069_05_Final_JM_ePub.xhtml#_idTextAnchor079"><em class="italic">Chapter 5</em></a><em class="italic">,</em> <em class="italic">Problems with Machine Learning on Graphs</em>:<p class="source-code">from node2vec import Node2Vec</p><p class="source-code">node2vec = Node2Vec(graph_train) </p><p class="source-code">model = node2vec.fit()</p></li>
				<li>Then, we use <strong class="source-inline">HadamardEmbedder</strong> for generating an embedding for each pair of embedded nodes. Such feature vectors will be used as input to train the classifier:<p class="source-code">from node2vec.edges import HadamardEmbedder</p><p class="source-code">edges_embs = HadamardEmbedder(keyed_vectors=model.wv)</p><p class="source-code"> train_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_train]</p></li>
				<li>It's time for<a id="_idIndexMarker656"/> training our supervised classifier. We will be using the RandomForest classifier, a powerful decision tree-based ensemble algorithm:<p class="source-code">from sklearn.ensemble import RandomForestClassifier </p><p class="source-code">from sklearn import metrics </p><p class="source-code">rf = RandomForestClassifier(n_estimators=10)</p><p class="source-code"> rf.fit(train_embeddings, labels_train);</p></li>
				<li>Finally, let's apply the trained model for creating the embedding of the test set:<p class="source-code">edges_embs = HadamardEmbedder(keyed_vectors=model.wv) test_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_test]</p></li>
				<li>Now we are ready to perform the prediction on the test set using our trained model:<p class="source-code">y_pred = rf.predict(test_embeddings) </p><p class="source-code">print('Precision:', metrics.precision_score(labels_test, y_pred)) </p><p class="source-code">print('Recall:', metrics.recall_score(labels_test, y_pred)) </p><p class="source-code">print('F1-Score:', metrics.f1_score(labels_test, y_pred)) </p></li>
				<li>The output should be as follows:<p class="source-code">Precision: 0.9701333333333333</p><p class="source-code">Recall: 0.9162573983125551</p><p class="source-code">F1-Score: 0.9424260086781945</p></li>
			</ol>
			<p>Not bad at all! We can <a id="_idIndexMarker657"/>observe that the node2vec-based embedding already provides a powerful representation for actually predicting links on the combined Facebook ego network.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor112"/>GraphSAGE-based link prediction</h2>
			<p>Next, we will use<a id="_idIndexMarker658"/> GraphSAGE for learning node embeddings and classifying edges. We will build a two-layer GraphSAGE architecture that, given labeled pairs of nodes, outputs a pair of node<a id="_idIndexMarker659"/> embeddings. Then, a fully connected neural network will be used to process these embeddings and produce link predictions. Notice that the GraphSAGE model and the fully connected network will be concatenated and trained end to end so that the embeddings learning stage is influenced by the predictions.</p>
			<h3>Featureless approach</h3>
			<p>Before starting, we<a id="_idIndexMarker660"/> may recall from <em class="italic">Chapters 4, Supervised Graph Learning</em>, and <a href="B16069_05_Final_JM_ePub.xhtml#_idTextAnchor079"><em class="italic">Chapter 5</em></a>, <em class="italic">Problems with Machine Learning on Graphs</em>, that GraphSAGE needs node descriptors (features). Such features may or may not be available in your dataset. Let's begin our analysis by not considering available node features. In this case, a common approach is to assign to each node a one-hot feature vector of length |<em class="italic">V</em>| (the number of nodes in the graph), where only the cell corresponding to the given node is 1, while the remaining cells are 0.</p>
			<p>This can be done in Python and <strong class="source-inline">networkx</strong> as follows:</p>
			<p class="source-code">eye = np.eye(graph_train.number_of_nodes())</p>
			<p class="source-code">fake_features = {n:eye[n] for n in G.nodes()}</p>
			<p class="source-code">nx.set_node_attributes(graph_train, fake_features, "fake")</p>
			<p class="source-code">eye = np.eye(graph_test.number_of_nodes())</p>
			<p class="source-code">fake_features = {n:eye[n] for n in G.nodes()}</p>
			<p class="source-code">nx.set_node_attributes(graph_test, fake_features, "fake")</p>
			<p>In the preceding code snippet, we did the following:</p>
			<ol>
				<li value="1">We created an identity matrix of size |<em class="italic">V</em>|. Each row of the matrix is the one-hot vector we need for each node in the graph.</li>
				<li>Then, we created a Python dictionary where, for each <strong class="source-inline">nodeID</strong> (used as the key), we assign the corresponding row of the previously created identity matrix.</li>
				<li>Finally, the<a id="_idIndexMarker661"/> dictionary was passed to the <strong class="source-inline">networkx</strong> <strong class="source-inline">set_node_attributes</strong> function to assign the "fake" features to each node in the <strong class="source-inline">networkx</strong> graph.</li>
			</ol>
			<p>Notice that the process is repeated for both the training and test graph.</p>
			<p>The next step will be defining the generator that will be used to feed the model. We will be using the <strong class="source-inline">stellargraph</strong> <strong class="source-inline">GraphSAGELinkGenerator</strong> for this, which essentially provides the model with pairs of nodes as input:</p>
			<p class="source-code">from stellargraph.mapper import GraphSAGELinkGenerator</p>
			<p class="source-code">batch_size = 64</p>
			<p class="source-code">num_samples = [4, 4]</p>
			<p class="source-code"># convert graph_train and graph_test for stellargraph</p>
			<p class="source-code">sg_graph_train = StellarGraph.from_networkx(graph_train, node_features="fake")</p>
			<p class="source-code">sg_graph_test = StellarGraph.from_networkx(graph_test, node_features="fake")</p>
			<p class="source-code">train_gen = GraphSAGELinkGenerator(sg_graph_train, batch_size, num_samples)</p>
			<p class="source-code"> train_flow = train_gen.flow(samples_train, labels_train, shuffle=True, seed=24)</p>
			<p class="source-code">test_gen = GraphSAGELinkGenerator(sg_graph_test, batch_size, num_samples)</p>
			<p class="source-code"> test_flow = test_gen.flow(samples_test, labels_test, seed=24)</p>
			<p>Note that we also need to define <strong class="source-inline">batch_size</strong> (number of inputs per minibatch) and the number <a id="_idIndexMarker662"/>of first- and second-hop neighbor samples that GraphSAGE should consider.</p>
			<p>Finally, we are ready to create the model:</p>
			<p class="source-code">from stellargraph.layer import GraphSAGE, link_classification</p>
			<p class="source-code">from tensorflow import keras</p>
			<p class="source-code">layer_sizes = [20, 20]</p>
			<p class="source-code">graphsage = GraphSAGE(layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3)</p>
			<p class="source-code">x_inp, x_out = graphsage.in_out_tensors()</p>
			<p class="source-code"># define the link classifier</p>
			<p class="source-code">prediction = link_classification(output_dim=1, output_act="sigmoid", edge_embedding_method="ip")(x_out)</p>
			<p class="source-code">model = keras.Model(inputs=x_inp, outputs=prediction)</p>
			<p class="source-code">model.compile(</p>
			<p class="source-code">    optimizer=keras.optimizers.Adam(lr=1e-3),</p>
			<p class="source-code">    loss=keras.losses.mse,</p>
			<p class="source-code">    metrics=["acc"],</p>
			<p class="source-code">)</p>
			<p>In the preceding snippet, we are creating a GraphSAGE model with two hidden layers of size 20, each with a bias term and a dropout layer for reducing overfitting. Then, the output of the GraphSAGE part of the module is concatenated with a <strong class="source-inline">link_classification</strong> layer that takes pairs of node embeddings (output of GraphSAGE), uses binary operators (inner product; <strong class="source-inline">ip</strong> in our case) to produce edge embeddings, and finally passes them through a fully connected neural network for classification.</p>
			<p>The model is<a id="_idIndexMarker663"/> optimized via the Adam optimizer (learning rate = <strong class="source-inline">1e-3</strong>) using the mean squared error as a loss function.</p>
			<p>Let's train the model for 10 epochs:</p>
			<p class="source-code">epochs = 10</p>
			<p class="source-code">history = model.fit(train_flow, epochs=epochs, validation_data=test_flow)</p>
			<p>The output should be as follows:</p>
			<p class="source-code">Epoch 18/20</p>
			<p class="source-code">loss: 0.4921 - acc: 0.8476 - val_loss: 0.5251 - val_acc: 0.7884</p>
			<p class="source-code">Epoch 19/20</p>
			<p class="source-code">loss: 0.4935 - acc: 0.8446 - val_loss: 0.5247 - val_acc: 0.7922</p>
			<p class="source-code">Epoch 20/20</p>
			<p class="source-code">loss: 0.4922 - acc: 0.8476 - val_loss: 0.5242 - val_acc: 0.7913</p>
			<p>Once trained, let's compute the performance metrics over the test set:</p>
			<p class="source-code">from sklearn import metrics </p>
			<p class="source-code">y_pred = np.round(model.predict(train_flow)).flatten()</p>
			<p class="source-code">print('Precision:', metrics.precision_score(labels_train, y_pred)) </p>
			<p class="source-code">print('Recall:', metrics.recall_score(labels_train, y_pred))  print('F1-Score:', metrics.f1_score(labels_train, y_pred)) </p>
			<p>The output should be as follows:</p>
			<p class="source-code">Precision: 0.7156476303969199</p>
			<p class="source-code">Recall: 0.983125550938169</p>
			<p class="source-code">F1-Score: 0.8283289124668435</p>
			<p>As we can observe, performances <a id="_idIndexMarker664"/>are lower than the ones obtained in the node2vec-based approach. However, we are not considering real node features yet, which could represent a great source of information. Let's do that in the following test.</p>
			<h3>Introducing node features</h3>
			<p>The process of extracting<a id="_idIndexMarker665"/> node features for the combined ego network is quite verbose. This is because, as we have explained in the first part of the chapter, each ego network is described using several files, as well as all the feature names and values. We have written useful functions for parsing all the ego network in order to extract the node features. You can find their implementation in the Python notebook provided in the GitHub repository. Here, let's just briefly summarize how they work:</p>
			<ul>
				<li>The <strong class="source-inline">load_features</strong> function parses each ego network and creates two dictionaries:<p>a. <strong class="source-inline">feature_index</strong>, which maps numeric indices to feature names</p><p>b. <strong class="source-inline">inverted_feature_indexes</strong>, which maps names to numeric indices</p></li>
				<li>The <strong class="source-inline">parse_nodes</strong> function receives the combined ego network <strong class="source-inline">G</strong> and the ego nodes' IDs. Then, each ego node in the network is assigned with the corresponding features previously loaded using the <strong class="source-inline">load_features</strong> function.</li>
			</ul>
			<p>Let's invoke them in order to load a feature vector for each node in the combined ego network:</p>
			<p class="source-code">load_features()</p>
			<p class="source-code">parse_nodes(G, ego_nodes)</p>
			<p>We can easily check the result by printing the information of one node in the network (for example, the node with ID <strong class="source-inline">0</strong>):</p>
			<p class="source-code">print(G.nodes[0])</p>
			<p>The output should be as follows:</p>
			<p class="source-code">{'features': array([1., 1., 1., ..., 0., 0., 0.])}</p>
			<p>As we can observe, the node has a dictionary containing a key named <strong class="source-inline">features</strong>. The corresponding <a id="_idIndexMarker666"/>value is the feature vector assigned to this node.</p>
			<p>We are now ready to repeat the same steps used before for training the GraphSAGE model, this time using <strong class="source-inline">features</strong> as the key when converting the <strong class="source-inline">networkx</strong> graph to the <strong class="source-inline">StellarGraph</strong> format:</p>
			<p class="source-code">sg_graph_train = StellarGraph.from_networkx(graph_train, node_features="features")</p>
			<p class="source-code">sg_graph_test = StellarGraph.from_networkx(graph_test, node_features="features")</p>
			<p>Finally, as we have done before, we create the generators, compile the model, and train it for 10 epochs:</p>
			<p class="source-code">train_gen = GraphSAGELinkGenerator(sg_graph_train, batch_size, num_samples)</p>
			<p class="source-code">train_flow = train_gen.flow(samples_train, labels_train, shuffle=True, seed=24)</p>
			<p class="source-code">test_gen = GraphSAGELinkGenerator(sg_graph_test, batch_size, num_samples)</p>
			<p class="source-code">test_flow = test_gen.flow(samples_test, labels_test, seed=24)</p>
			<p class="source-code">layer_sizes = [20, 20]</p>
			<p class="source-code">graphsage = GraphSAGE(layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3)</p>
			<p class="source-code">x_inp, x_out = graphsage.in_out_tensors()</p>
			<p class="source-code">prediction = link_classification(output_dim=1, output_act="sigmoid", edge_embedding_method="ip")(x_out)</p>
			<p class="source-code">model = keras.Model(inputs=x_inp, outputs=prediction)</p>
			<p class="source-code">model.compile(</p>
			<p class="source-code">    optimizer=keras.optimizers.Adam(lr=1e-3),</p>
			<p class="source-code">    loss=keras.losses.mse,</p>
			<p class="source-code">    metrics=["acc"],</p>
			<p class="source-code">)</p>
			<p class="source-code">epochs = 10</p>
			<p class="source-code">history = model.fit(train_flow, epochs=epochs, validation_data=test_flow)</p>
			<p>Notice that <a id="_idIndexMarker667"/>we are using the same hyperparameters (including the number of layers, batch size, and learning rate) as well as the random seed, to ensure a fair comparison between the models.</p>
			<p>The output should be as follows:</p>
			<p class="source-code">Epoch 18/20</p>
			<p class="source-code">loss: 0.1337 - acc: 0.9564 - val_loss: 0.1872 - val_acc: 0.9387</p>
			<p class="source-code">Epoch 19/20</p>
			<p class="source-code">loss: 0.1324 - acc: 0.9560 - val_loss: 0.1880 - val_acc: 0.9340</p>
			<p class="source-code">Epoch 20/20</p>
			<p class="source-code">loss: 0.1310 - acc: 0.9585 - val_loss: 0.1869 - val_acc: 0.9365</p>
			<p>Let's evaluate the model performance:</p>
			<p class="source-code">from sklearn import metrics </p>
			<p class="source-code">y_pred = np.round(model.predict(train_flow)).flatten()</p>
			<p class="source-code">print('Precision:', metrics.precision_score(labels_train, y_pred)) </p>
			<p class="source-code">print('Recall:', metrics.recall_score(labels_train, y_pred)) </p>
			<p class="source-code">print('F1-Score:', metrics.f1_score(labels_train, y_pred))</p>
			<p>We can check the output:</p>
			<p class="source-code">Precision: 0.7895418326693228</p>
			<p class="source-code">Recall: 0.9982369978592117</p>
			<p class="source-code">F1-Score: 0.8817084700517213</p>
			<p>As we can see, the<a id="_idIndexMarker668"/> introduction of real node features has brought a good improvement, even if the best performances are still the ones achieved using the node2vec approach.</p>
			<p>Finally, we will evaluate a shallow embedding approach where hand-crafted features will be used for training a supervised classifier.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor113"/>Hand-crafted features for link prediction</h2>
			<p>As we have already seen in <a href="B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064"><em class="italic">Chapter 4</em></a>, <em class="italic">Supervised Graph Learning</em>, shallow embedding methods<a id="_idIndexMarker669"/> represent a simple yet powerful approach for dealing with supervised tasks. Basically, for each input edge, we will compute a set of metrics that will be given as input to a classifier.</p>
			<p>In this example, for each input edge represented as a pair of nodes (<em class="italic">u</em>,<em class="italic">v</em>), four metrics will be considered, namely the following:</p>
			<ul>
				<li><strong class="bold">Shortest path</strong>: The length of <a id="_idIndexMarker670"/>the shortest path between <em class="italic">u</em> and <em class="italic">v</em>. If <em class="italic">u</em> and <em class="italic">v</em> are directly connected through an edge, this edge will be removed before computing the shortest path. The value <strong class="source-inline">0</strong> will be used if <em class="italic">u</em> is not reachable from <em class="italic">v</em>.</li>
				<li><strong class="bold">The Jaccard coefficient</strong>: Given a pair of nodes (<em class="italic">u</em>,<em class="italic">v</em>), it is defined as the intersection over <a id="_idIndexMarker671"/>a union of the set of neighbors of <em class="italic">u</em> and <em class="italic">v</em>. Formally, let <img src="image/B16069_06_001.png" alt=""/> be the set of neighbors of the node <em class="italic">u</em> and <img src="image/B16069_06_002.png" alt=""/> be the set of neighbors of the node <em class="italic">v</em>:</li>
			</ul>
			<div>
				<div id="_idContainer475" class="IMG---Figure">
					<img src="image/B16069_06_003.jpg" alt=""/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">The u centrality</strong>: The degree<a id="_idIndexMarker672"/> centrality computed for node <em class="italic">v</em>.</li>
				<li><strong class="bold">The v centrality</strong>: The<a id="_idIndexMarker673"/> degree centrality computed for node <em class="italic">u</em>.</li>
				<li><strong class="bold">The u community</strong>: The <a id="_idIndexMarker674"/>community ID assigned to node <em class="italic">u</em> using the Louvain heuristic.</li>
				<li><strong class="bold">The v community</strong>: The <a id="_idIndexMarker675"/>community ID assigned to node <em class="italic">v</em> using the Louvain heuristic.</li>
			</ul>
			<p>We have written a useful function for computing these metrics using Python and <strong class="source-inline">networkx</strong>. You can<a id="_idIndexMarker676"/> find the implementation in the Python notebook provided in the GitHub repository.</p>
			<p>Let's compute the features for each edge in the training and the test set:</p>
			<p class="source-code">feat_train = get_hc_features(graph_train, samples_train, labels_train)</p>
			<p class="source-code">feat_test = get_hc_features(graph_test, samples_test, labels_test)</p>
			<p>In the proposed shallow approach, these features will be directly used as input for a <strong class="source-inline">Random Forest</strong> classifier. We will use its <strong class="source-inline">scikit-learn</strong> implementation as follows:</p>
			<p class="source-code">from sklearn.ensemble import RandomForestClassifier </p>
			<p class="source-code">from sklearn import metrics </p>
			<p class="source-code">rf = RandomForestClassifier(n_estimators=10) </p>
			<p class="source-code">rf.fit(feat_train, labels_train); </p>
			<p>The preceding lines automatically instantiate and train a RandomForest classifier using the edge features we have computed before. We are now ready to compute the performance as follows:</p>
			<p class="source-code">y_pred = rf.predict(feat_test)</p>
			<p class="source-code">print('Precision:', metrics.precision_score(labels_test, y_pred))</p>
			<p class="source-code"> print('Recall:', metrics.recall_score(labels_test, y_pred)) print('F1-Score:', metrics.f1_score(labels_test, y_pred)) </p>
			<p>The output will be as follows:</p>
			<p class="source-code">Precision: 0.9636952636282395</p>
			<p class="source-code">Recall: 0.9777853337866939</p>
			<p class="source-code">F1-Score: 0.9706891701828411</p>
			<p>Surprisingly, the<a id="_idIndexMarker677"/> shallow method based on hand-crafted features performs better than the others.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor114"/>Summary of results</h2>
			<p>In the preceding examples, we<a id="_idIndexMarker678"/> have trained three algorithms on learning, with and without supervision, useful embeddings for link prediction. In the following table, we summarize the results:</p>
			<div>
				<div id="_idContainer476" class="IMG---Figure">
					<img src="image/B16069_06_011.jpg" alt="Table 6.1 – Summary of the results achieved for the link prediction task&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 6.1 – Summary of the results achieved for the link prediction task</p>
			<p>As shown in <em class="italic">Table 6.1</em>, the node2vec-based method is already able to achieve a high level of performance without supervision and per-node information. Such high results might be related to the particular structure of the combined ego network. Due to the high sub-modularity of the network (since it is composed of several ego networks), predicting whether two users will be connected or not might be highly related to the way the two candidate nodes are connected inside the network. For example, there might be a systematic situation in which two users, both connected to several users in the same ego network, have a high chance of being connected as well. On the other hand, two users belonging to different ego networks, or <em class="italic">very far</em> from each other, are likely to not be connected, making the prediction task easier. This is also confirmed by the high results achieved using the shallow method.</p>
			<p>Such a situation might be confusing, instead, for more complicated algorithms like GraphSAGE, especially when node features are involved. For example, two users might share similar interests, making them very similar. However, they might belong to different ego networks, where the corresponding ego users live in two very different parts of the world. So, similar users, which in principle should be connected, are not. However, it is also possible that such algorithms are predicting something further in the future. Recall that the combined ego network is a timestamp of a particular situation in a given period of time. Who knows how it might have evolved right now!</p>
			<p>Interpreting <a id="_idIndexMarker679"/>machine learning algorithms is probably the most interesting challenge of machine learning itself. For this reason, we should always interpret results with care. Our suggestion is always to dig into the dataset and try to give an explanation of your results.</p>
			<p>Finally, it is important to remark that each of the algorithms was not tuned for the purpose of this demonstration. Different results can be obtained by properly tuning each hyperparameter and we highly suggest you try to do this.</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor115"/>Summary</h1>
			<p>In this chapter, we have seen how machine learning can be useful for solving practical machine learning tasks on social network graphs. Furthermore, we have seen how future connections can be predicted on the SNAP Facebook combined ego network.</p>
			<p>We reviewed graph analysis concepts and used graph-derived metrics to collect insight on the social graph. Then, we benchmarked several machine learning algorithms on the link prediction task, evaluating their performance and trying to give them interpretations.</p>
			<p>In the next chapter, we will focus on how similar approaches can be used to analyze a corpus of documents using text analytics and natural language processing.</p>
		</div>
	</body></html>