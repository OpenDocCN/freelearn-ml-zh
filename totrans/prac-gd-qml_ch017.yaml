- en: Chapter 8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章
- en: What Is Quantum Machine Learning?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是量子机器学习？
- en: '*Tell me and I forget. Teach me and I remember. Involve me and I* *learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*告诉我，我会忘记。教我，我会记住。参与其中，我会学习。*'
- en: — Benjamin Franklin
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: —— 本杰明·富兰克林
- en: We now begin our journey through **Quantum Machine Learning** (**QML**). In
    this chapter, we will set the foundation for the remainder of this part of the
    book. We will begin by reviewing some general notions from classical machine learning,
    and then we will introduce the basic ideas that underlie QML as a whole.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始我们的量子机器学习（**QML**）之旅。在本章中，我们将为本部分书籍的其余部分奠定基础。我们将首先回顾一些经典机器学习的一般概念，然后我们将介绍支撑整个QML的基本思想。
- en: 'We’ll cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The basics of machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的基本原理
- en: Do you wanna train a model?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你想训练一个模型吗？
- en: Quantum-classical models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量子-经典模型
- en: In this chapter, you will learn the basic principles behind general machine
    learning, and you will understand how to construct, train, and assess some simple
    classical models using industry-standard frameworks and tools. We will also present
    a general picture of the world of QML.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习通用机器学习背后的基本原理，并了解如何使用行业标准框架和工具构建、训练和评估一些简单的经典模型。我们还将展示量子机器学习（QML）世界的总体图景。
- en: 8.1 The basics of machine learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.1 机器学习的基本原理
- en: Before discussing quantum machine learning, it may be a good idea to review
    some basic notions of Machine Learning (ML), in general. If you are familiar with
    the subject, feel free to skip this section. Please, keep in mind that the world
    of machine learning is extraordinarily vast; so much so that sometimes it is difficult
    to make general statements that can do justice to the overwhelming diversity of
    this field. For this reason, we will highlight those elements that will be more
    relevant for our purposes, while other aspects of machine learning — which are
    also of significant importance on their own — will be barely covered.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论量子机器学习之前，回顾一些机器学习（ML）的基本概念可能是个好主意。如果你熟悉这个主题，请随意跳过这一部分。请记住，机器学习的世界非常广阔；如此之大，有时很难做出公正的概括，以体现这个领域的压倒性多样性。因此，我们将强调对我们更有相关性的元素，而其他机器学习的方面——虽然它们本身也很重要——将只做简要介绍。
- en: In addition to this, please keep in mind that this will be a very condensed
    and hands-on introduction to machine learning. If you’d like to dive deeper into
    this field, we can recommend some very good books, such as the one by Abu-Mostafa,
    Magdon-Ismail, and Lin [[1](ch030.xhtml#Xabu2012learning)], or the one by Aurélien
    Géron [[104](ch030.xhtml#Xhandsonml)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个，请记住，这将是一个非常浓缩且实用的机器学习入门介绍。如果您想更深入地了解这个领域，我们可以推荐一些非常好的书籍，例如阿布-莫斯塔法、马格东-伊斯拉姆和林所著的书籍[[1](ch030.xhtml#Xabu2012learning)]，或者奥雷利安·热隆所著的书籍[[104](ch030.xhtml#Xhandsonml)]。
- en: As mysterious as machine learning may seem, the ideas that underlie it are fairly
    simple. In broad terms, we could define the purpose of machine learning to be
    the design of algorithms that can make a ”computational system” aware of patterns
    in data. These patterns can be truly anything. Maybe you want to design a system
    that can distinguish pictures of cats from pictures of rabbits. Maybe you would
    like to come up with a computational model that can transcribe verbal speech in
    English spoken with an Irish accent. Maybe you want to create a model able to
    generate realistic pictures of faces of people who do not exist, but that are
    indistinguishable from the real deal. The possibilities, as you surely know, are
    endless! What will be common to all these algorithms is that they will not be
    explicitly programmed to solve those tasks; instead, they will ”learn” how to
    do it from data…hence the name ”machine learning!”
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习可能看起来很神秘，但其背后的思想相当简单。从广义上讲，我们可以将机器学习的目的定义为设计算法，使“计算系统”能够意识到数据中的模式。这些模式可以是真正的一切。也许你想设计一个能够区分猫和兔子的图片的系统。也许你希望提出一个能够转录带有爱尔兰口音的英语口语的计算模型。也许你想创建一个能够生成不存在但与真实情况难以区分的人脸图片的模型。正如你肯定知道的，可能性是无限的！所有这些算法的共同之处在于，它们不会明确编程来解决这些任务；相反，它们将从数据中“学习”如何去做……这就是“机器学习”这个名字的由来！
- en: 'The cats versus rabbits example is a particular case of an interesting type
    of model: a **classifier**. As the name suggests, a classifier is any kind of
    system that assigns, to every input, one label out of a finite set of possibilities.
    In many cases, there are just two of these labels, and they are commonly represented
    by ![0](img/file12.png "0") (read as **positive**) and ![1](img/file13.png "1")
    (**negative**); in physics applications, for instance, these labels are often
    read, respectively, as **signal** and **background**. In this situation, we say
    that the classifier is **binary**. Keep this in mind, for we will use this terminology
    in a few of the coming examples!'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 猫与兔子的例子是某种有趣类型模型的一个特例：**分类器**。正如其名所示，分类器是任何将标签分配给每个输入的系统，这些标签来自有限的可能集合。在许多情况下，只有两个这样的标签，它们通常表示为![0](img/file12.png
    "0" "正")（读作**正**）和![1](img/file13.png "1" "负"）；例如，在物理学应用中，这些标签通常分别读作**信号**和**背景**。在这种情况下，我们说分类器是**二元的**。请记住这一点，因为我们将在接下来的几个例子中使用这个术语！
- en: 'So now that we know where we are heading, we need to answer one basic question:
    how can we make machine learning a reality?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，既然我们已经知道了我们的目标，我们需要回答一个基本问题：我们如何使机器学习成为现实？
- en: 8.1.1 The ingredients for machine learning
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1.1 机器学习的要素
- en: 'In most machine learning setups, there are three basic ingredients:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数机器学习设置中，有三个基本要素：
- en: First of all, we need some sort of computational model that is ”powerful enough”
    to tackle our problem. By this, we will often mean an algorithm that can be configured
    to solve the task at hand — at least to some level of accuracy.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们需要某种“足够强大”的计算模型来处理我们的问题。通过这种方式，我们通常意味着一个可以配置来解决当前任务——至少达到一定准确度的算法。
- en: Then, if we want our model to capture patterns, we need to feed it some data
    so that it can do that. We will thus need data, preferably lots of it. The nature
    of this data will depend on the approach that we take, but, in most cases, we
    will need to transform it into numerical form. Most models expect data to be represented
    as vectors of real numbers called **attributes**, so this is what we will usually
    assume that we have.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，如果我们想让我们的模型捕捉到模式，我们需要给它一些数据，这样它才能做到这一点。因此，我们需要数据，最好是大量的数据。这种数据的性质将取决于我们采取的方法，但在大多数情况下，我们需要将其转换为数值形式。大多数模型期望数据以实数向量的形式表示，称为**属性**，所以我们通常假设我们拥有这些。
- en: And, lastly, we need a **training procedure** that will allow us to optimize
    the configuration of our model to make it solve the task (or, at least, come close
    to solving it!). In ML jargon, we could say that we need to find a way to make
    our model **learn** in order to identify the patterns that hide behind the data
    in our problem.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们需要一个**训练过程**，这将允许我们优化模型的配置，使其解决问题（或者至少接近解决问题！）在机器学习术语中，我们可以说我们需要找到一种方法让我们的模型**学习**，以便识别隐藏在我们问题数据背后的模式。
- en: That is a pretty solid — yet somewhat oversimplified — wish-list. Let’s see
    how we can make more sense out of this.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当稳固——但有些过于简化的——愿望清单。让我们看看我们如何使这个清单更有意义。
- en: '**The model** Let us first analyze that computational model that we have talked
    about. We said that it had to be ”powerful enough,” and this means that there
    should be a way to configure the model in such a way that it behaves as we intend
    it to.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型** 让我们先分析一下我们之前提到的计算模型。我们说它必须“足够强大”，这意味着应该有一种方式来配置模型，使其按我们的意图行事。'
- en: 'At first sight, this requirement may seem suspicious: how can we possibly be
    sure that such a configuration exists? In most real-life problems, we can never
    be fully sure…but we can be certain to some degree! This certainty may come from
    experience or, desirably, also from some theoretical results that justify it.
    For instance, you may have heard of **neural networks**. We will discuss them
    shortly, but, for now, you should know that they are models that have been proven
    to be **universal function approximators**. That is, any function can be approximated
    up to any given accuracy, no matter its complexity, by a large-enough neural network.
    That makes neural networks natural good choices for many problems.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这个要求可能看起来有些可疑：我们怎么可能确信存在这样的配置呢？在大多数现实生活中的问题中，我们永远不能完全确定……但我们可以在一定程度上确信！这种确定性可能来自经验，或者更理想的是，也来自一些证明它的理论结果。例如，你可能听说过**神经网络**。我们将在稍后讨论它们，但就目前而言，你应该知道它们是已被证明为**通用函数逼近器**的模型。也就是说，任何函数都可以通过足够大的神经网络逼近到任何给定的精度，无论其复杂性如何。这使得神经网络成为许多问题的自然选择。
- en: 'We will later discuss neural networks — and many other interesting models —
    in detail, but, to start with, we can consider a simplified version that, in fact,
    could be considered the grandparent of neural networks: the **perceptron**.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面详细讨论神经网络——以及许多其他有趣的模型，但首先，我们可以考虑一个简化的版本，实际上这个版本可以被认为是神经网络的老祖宗：**感知器**。
- en: 'A perceptron is a computational model that takes ![N](img/file784.png "N")
    numerical inputs and returns a single bit as output. This model depends on a collection
    of weights ![w_{i}](img/file1095.png "w_{i}") for ![i = 1,\ldots,N](img/file1096.png
    "i = 1,\ldots,N") and on a bias ![b](img/file17.png "b"), and it behaves as follows:
    for any input ![x_{1},\ldots,x_{N}](img/file1097.png "x_{1},\ldots,x_{N}"), if'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是一个计算模型，它接受![N](img/file784.png "N")个数值输入，并返回一个单比特输出。这个模型依赖于一组权重![w_{i}](img/file1095.png
    "w_{i}")，其中![i = 1,\ldots,N](img/file1096.png "i = 1,\ldots,N")，以及一个偏置![b](img/file17.png
    "b")，并且它的工作方式如下：对于任何输入![x_{1},\ldots,x_{N}](img/file1097.png "x_{1},\ldots,x_{N}"),如果
- en: '| ![\sum\limits_{i = 1}^{N}x_{i}w_{i} + b \geq 0,](img/file1098.png "\sum\limits_{i
    = 1}^{N}x_{i}w_{i} + b \geq 0,") |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| ![sum_{i=1}^{N}x_{i}w_{i} + b \geq 0,](img/file1098.png "sum_{i=1}^{N}x_{i}w_{i}
    + b \geq 0,") |'
- en: then the model returns ![1](img/file13.png "1"), and otherwise it returns ![0](img/file12.png
    "0").
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，模型返回![1](img/file13.png "1")，否则返回![0](img/file12.png "0")。
- en: This is a very simple computational model, but we could use it to set up a basic
    binary classifier by looking for some appropriate values for the weights and bias.
    That is, given a set of points on which we want the output to be ![1](img/file13.png
    "1") and another set of points on which the output should be ![0](img/file12.png
    "0"), we can try to search for some values for the ![w_{i}](img/file1095.png "w_{i}")
    and ![b](img/file17.png "b") coefficients that would make the perceptron return
    the desired outputs. In fact, in the dawn of the machine learning age, it was
    already proven that there is a simple learning algorithm that, under the condition
    that the problem data can be linearly separated, finds coefficients that can effectively
    classify the data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的计算模型，但我们可以通过寻找适当的权重和偏置值来设置一个基本的二元分类器。也就是说，给定一组我们希望输出为![1](img/file13.png
    "1")的点，以及另一组我们希望输出为![0](img/file12.png "0")的点，我们可以尝试寻找![w_{i}](img/file1095.png
    "w_{i}")和![b](img/file17.png "b")系数的某些值，使得感知器返回期望的输出。实际上，在机器学习时代的黎明时期，已经证明存在一个简单的学习算法，在问题数据可以线性分离的条件下，可以找到能够有效分类数据的系数。
- en: There you have it, that could be your first baby machine learning model! Needless
    to say, a perceptron — at least on its own — is not a particularly powerful model,
    but it is, at least, a promising beginning!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你的第一个婴儿机器学习模型！不用说，感知器——至少就其本身而言——不是一个特别强大的模型，但至少是一个有希望的起点！
- en: Exercise 8.1
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.1
- en: We can all agree that perceptrons are cute models. But just to get an idea of
    their limitations, prove that they cannot implement an XOR gate. That is, if you
    are given inputs ![\{(0,1),(1,0)\}](img/file1099.png "\{(0,1),(1,0)\}") with desired
    output ![1](img/file13.png "1") and inputs ![\{(0,0),(1,1)\}](img/file1100.png
    "\{(0,0),(1,1)\}") with desired output ![0](img/file12.png "0"), there is no choice
    of perceptron weights and bias that works in this case.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都可以同意，感知器是可爱的模型。但为了了解它们的局限性，证明它们不能实现XOR门。也就是说，如果你给出输入 ![\{(0,1),(1,0)\}](img/file1099.png
    "\{(0,1),(1,0)\}")，期望输出![1](img/file13.png "1")，以及输入 ![\{(0,0),(1,1)\}](img/file1100.png
    "\{(0,0),(1,1)\}")，期望输出![0](img/file12.png "0")，那么在这种情况下没有感知器权重和偏置的组合能够工作。
- en: '**The training procedure** Alright, so let’s say that we have a model that
    we believe is just powerful enough to approach our problem (and not too powerful
    either…more on that later!). We will restrict ourselves to assuming that the configuration
    of our model depends on some numerical parameters ![\theta](img/file89.png "\theta");
    this would mean that we would be looking for some choice ![\theta_{0}](img/file1045.png
    "\theta_{0}") of those parameters that will make our model work as well as possible.
    So, how do we find those parameters?'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练过程** 好吧，假设我们有一个模型，我们相信它足够强大来接近我们的问题（但也不太强大……关于这一点稍后还会讨论！）。我们将限制自己假设我们的模型配置依赖于一些数值参数
    ![\theta](img/file89.png "\theta")；这意味着我们将会寻找那些参数的某个选择 ![\theta_{0}](img/file1045.png
    "\theta_{0}")，这将使我们的模型尽可能好地工作。那么，我们如何找到这些参数呢？'
- en: To learn more…
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多…
- en: We will only discuss models whose behavior can be adjusted and defined solely
    by tweaking some numerical parameters, as in the case of the perceptron. Nevertheless,
    there also exist **non-parametric** models that don’t behave in this manner. A
    popular example is the ![k](img/file317.png "k")-nearest neighbours algorithm;
    you can find some information in the references [[104](ch030.xhtml#Xhandsonml),
    Chapter 1].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只讨论那些行为可以通过调整和定义一些数值参数来调整和定义的模型，就像感知器的情况一样。然而，也存在**非参数**模型，它们的行为不是这样。一个流行的例子是
    ![k](img/file317.png "k")-最近邻算法；你可以在参考文献 [[104](ch030.xhtml#Xhandsonml), 第1章]
    中找到一些信息。
- en: To illustrate all of this, we will discuss how to train a parametric model to
    implement a binary classifier. That is, we aim to build a binary classifier on
    a certain domain ![D](img/file1101.png "D") with some elements ![x](img/file269.png
    "x") that should be each classified as a certain ![y](img/file270.png "y") (where
    ![y](img/file270.png "y") can be either ![0](img/file12.png "0") or ![1](img/file13.png
    "1")). For this, we will use a model ![M](img/file704.png "M") that depends on
    some parameters in a way that, for any choice ![\theta](img/file89.png "\theta")
    of these parameters, it returns a label ![M_{\theta}(x)](img/file1102.png "M_{\theta}(x)")
    for every element ![x \in D](img/file1103.png "x \in D") in the dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明所有这些，我们将讨论如何训练一个参数模型来实现二元分类器。也就是说，我们旨在在某个领域 ![D](img/file1101.png "D") 上构建一个二元分类器，该领域有一些元素
    ![x](img/file269.png "x")，它们应该被分类为特定的 ![y](img/file270.png "y")（其中 ![y](img/file270.png
    "y") 可以是 ![0](img/file12.png "0") 或 ![1](img/file13.png "1")）。为此，我们将使用一个模型 ![M](img/file704.png
    "M")，它依赖于一些参数，对于这些参数的任何选择 ![\theta](img/file89.png "\theta")，它为数据集中的每个元素 ![x \in
    D](img/file1103.png "x \in D") 返回一个标签 ![M_{\theta}(x)](img/file1102.png "M_{\theta}(x)")。
- en: In this scenario, our goal is to look for a choice of parameters ![\theta](img/file89.png
    "\theta") that can minimize the probability that any random input ![x](img/file269.png
    "x") be misclassified. To put it in slightly more formal terms, if ![y](img/file270.png
    "y") is the correct label to be assigned to an input ![x](img/file269.png "x"),
    we want to minimize ![P(M_{\theta}(x) \neq y)](img/file1104.png "P(M_{\theta}(x)
    \neq y)"), that is, the probability of assigning an incorrect label to ![x](img/file269.png
    "x"). In this way, we have reduced the problem of training our model to the problem
    of finding some parameters ![\theta](img/file89.png "\theta") that minimize ![P(M_{\theta}(x)
    \neq y)](img/file1104.png "P(M_{\theta}(x) \neq y)"). This probability is known
    as the **generalization error** or the **true** **error**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，我们的目标是寻找一组参数 ![\theta](img/file89.png "\theta")，它可以最小化任何随机输入 ![x](img/file269.png
    "x") 被错误分类的概率。用稍微正式一点的话来说，如果 ![y](img/file270.png "y") 是应该分配给输入 ![x](img/file269.png
    "x") 的正确标签，我们希望最小化 ![P(M_{\theta}(x) \neq y)](img/file1104.png "P(M_{\theta}(x)
    \neq y)"), 即将错误标签分配给 ![x](img/file269.png "x") 的概率。这样，我们就将训练模型的问题简化为寻找一些参数 ![\theta](img/file89.png
    "\theta")，以最小化 ![P(M_{\theta}(x) \neq y)](img/file1104.png "P(M_{\theta}(x) \neq
    y)"), 这被称为**泛化误差**或**真实**误差。
- en: Now, if we had access to all the possible inputs in our domain ![D](img/file1101.png
    "D") and we knew all their expected outputs, we would simply have to minimize
    the true error…and we would be done! Nevertheless, this is neither an interesting
    situation nor a common one.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们能够访问我们领域 ![D](img/file1101.png "D") 中所有可能的输入，并且我们知道它们的预期输出，我们只需最小化真实误差……我们就完成了！然而，这既不是一个有趣的情况，也不是一个常见的情况。
- en: If we had a problem in which we already knew all the possible inputs and their
    outputs…why should we bother with all this machine learning business? We could
    just implement an old-school algorithm! Indeed, the whole point of ”learning”
    is being able to predict correct outputs for unseen data. Thus, when we resort
    to machine learning, we do so because we do not have full access to all the possible
    inputs and outputs in our domain — either because it is unfeasible or because
    such a domain might be infinite!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个问题，我们已经知道所有可能的输入和它们的输出……我们为什么要费心去做所有这些机器学习的事情？我们可以直接实现一个老式的算法！确实，"学习"的全部意义在于能够预测未见数据的正确输出。因此，当我们求助于机器学习时，我们之所以这样做，是因为我们没有完全访问我们领域中的所有可能的输入和输出——要么因为这是不可行的，要么因为这样的领域可能是无限的！
- en: 'So now we are faced with a problem. We have a (potentially infinite) domain
    of data over which we have to minimize the true error, yet we only have access
    to a finite subset of it. But…how on earth can we compute the true error in order
    to minimize it? The answer is that, in general, we can’t, because we would need
    complete information on how all the data and the labels of our problem are distributed,
    something that we usually don’t have. Nevertheless, we still have access to a
    — presumably large — subset of data. Can we use it to our advantage? Yes, we surely
    can! The usual strategy is to divide the dataset that we have in two separate
    sets: a **training dataset** and a **test** **dataset**. The training set, usually
    much bigger than the test set, will be used to adjust the parameters of our model
    in an attempt to minimize the true error, while the test set will be used to estimate
    the true error itself.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们面临一个问题。我们有一个（可能是无限的）数据域，我们必须最小化真实误差，但我们只能访问它的一小部分。但是……我们究竟如何计算真实误差以使其最小化呢？答案是，在一般情况下，我们无法做到，因为我们需要有关所有数据和问题标签分布的完整信息，而这通常是我们没有的。尽管如此，我们仍然可以访问一个——可能很大的——数据子集。我们能利用它吗？是的，我们当然可以！通常的策略是将我们拥有的数据集分成两个独立的集合：一个**训练数据集**和一个**测试数据集**。训练集通常比测试集大得多，将用于调整我们模型的参数，以尝试最小化真实误差，而测试集将用于估计真实误差本身。
- en: 'Thus, what we can do is just take whichever training dataset ![T](img/file74.png
    "T") we are using, and — instead of minimizing the true error, to which we simply
    don’t have access — we can try to minimize the **empirical error**: the probability
    of misclassifying an element within the training dataset. This empirical error
    would be computed as the proportion of misclassified elements in ![T](img/file74.png
    "T"):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们能做的就是取我们正在使用的任意训练数据集![T](img/file74.png "T")，而不是最小化我们根本无法访问的真实误差，我们可以尝试最小化**经验误差**：在训练数据集中错误分类一个元素的概率。这个经验误差将被计算为![T](img/file74.png
    "T")中错误分类元素的比例：
- en: '| ![R_{\text{emp}}(\theta) = \frac{1}{&#124;T&#124;}\sum\limits_{(x,y) \in
    T}1 - \delta(M_{\theta}(x),y),](img/file1105.png "R_{\text{emp}}(\theta) = \frac{1}{&#124;T&#124;}\sum\limits_{(x,y)
    \in T}1 - \delta(M_{\theta}(x),y),") |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| ![R_{\text{emp}}(\theta) = \frac{1}{&#124;T&#124;}\sum\limits_{(x,y) \in
    T}1 - \delta(M_{\theta}(x),y),](img/file1105.png "R_{\text{emp}}(\theta) = \frac{1}{&#124;T&#124;}\sum\limits_{(x,y)
    \in T}1 - \delta(M_{\theta}(x),y),") |'
- en: where ![|T|](img/file1106.png "|T|") is the size of the training dataset and
    ![\delta(a,b)](img/file1107.png "\delta(a,b)") is ![1](img/file13.png "1") if
    ![a = b](img/file1108.png "a = b") and ![0](img/file12.png "0") otherwise (this
    ![\delta](img/file1109.png "\delta") function is known as the Kronecker delta).
    We would do all of this, of course, hoping that the real error would take similar
    values to the empirical error. Naturally, this hope will have to be justified
    and rest on some evidence, and we will soon see how the test dataset can help
    us with that. In any case, if we want all this setup to work, we will need to
    use a large enough dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![|T|](img/file1106.png "|T|") 是训练数据集的大小，![\delta(a,b)](img/file1107.png
    "\delta(a,b)") 如果 ![a = b](img/file1108.png "a = b") 则为 ![1](img/file13.png "1")，否则为
    ![0](img/file12.png "0")（这个![\delta](img/file1109.png "\delta")函数被称为克罗内克δ）。我们当然会做所有这些，希望真实误差会与经验误差相似。自然地，这种希望将需要得到证实，并基于一些证据，我们很快就会看到测试数据集如何帮助我们做到这一点。无论如何，如果我们想让所有这些设置都起作用，我们需要使用足够大的数据集。
- en: 'Our goal then is to minimize the true error, and, so far, our only strategy
    is trying to achieve it by minimizing the empirical error. Nevertheless, in practice,
    we don’t often work with these magnitudes directly. Instead, we take a more ”general”
    approach: we seek to minimize the expected value of a **loss function**, which
    is defined for every choice of parameters ![\theta](img/file89.png "\theta") and
    every possible input ![x](img/file269.png "x") and its desired output ![y](img/file270.png
    "y"). For instance, we could define the 0-1 loss function as'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的目标是最小化真实误差，到目前为止，我们唯一的策略是通过最小化经验误差来实现它。然而，在实践中，我们并不经常直接处理这些量。相反，我们采取一种更“通用”的方法：我们寻求最小化**损失函数**的期望值，该函数为每个参数选择![\theta](img/file89.png
    "\theta")和每个可能的输入![x](img/file269.png "x")及其期望输出![y](img/file270.png "y")定义。例如，我们可以定义0-1损失函数为
- en: '| ![L_{01}(\theta;x,y) = 1 - \delta(M_{\theta}(x),y).](img/file1110.png "L_{01}(\theta;x,y)
    = 1 - \delta(M_{\theta}(x),y).") |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| ![L_{01}(\theta;x,y) = 1 - \delta(M_{\theta}(x),y).](img/file1110.png "L_{01}(\theta;x,y)
    = 1 - \delta(M_{\theta}(x),y).") |'
- en: With this definition, it is trivial to see that the expected value, taken over
    the whole domain, of ![L_{01}](img/file1111.png "L_{01}") is exactly the true
    error; this expected value is known as the **true risk**. In the same way, the
    expected value of this loss function over the training sample is the empirical
    error.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个定义，可以很容易地看出，在整个域上取![L_{01}](img/file1111.png "L_{01}")的期望值正好是真实误差；这个期望值被称为**真实风险**。同样，这个损失函数在训练样本上的期望值是实证误差。
- en: Important note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Keep in mind that the expected value of a loss function over a finite dataset
    will just be its average value.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在有限数据集上的损失函数的期望值只是它的平均值。
- en: So, in practice, our strategy for minimizing the true error will be minimizing
    the expected value of a suitable loss function over the training dataset. We will
    refer to this expected value as the **empirical risk**. For reasons that we will
    discuss later, we will usually consider loss functions different from ![L_{01}](img/file1111.png
    "L_{01}").
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在实践中，我们最小化真实误差的策略将是最小化在训练数据集上适合的损失函数的期望值。我们将把这个期望值称为**经验风险**。由于我们将在后面讨论的原因，我们通常会考虑与![L_{01}](img/file1111.png
    "L_{01}")不同的损失函数。
- en: '**Assessing a trained model** We now have to address an important question.
    How can we be sure that — once we have trained a model — it will perform well
    on data outside the training dataset? For that, we cannot solely rely on ![R_{\text{emp}}(\theta)](img/file1112.png
    "R_{\text{emp}}(\theta)") because that average loss is computed on data that the
    classifier has already seen. That would be like testing a student only on problems
    that the teacher has already solved in class!'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**评估训练好的模型**我们现在必须解决一个重要问题。我们如何确保——一旦我们训练了一个模型——它将在训练数据集之外的数据上表现良好？为此，我们不能仅仅依赖于![R_{\text{emp}}(\theta)](img/file1112.png
    "R_{\text{emp}}(\theta)")，因为那个平均损失是在分类器已经看到的数据上计算的。这就像只对一个学生进行老师已经在课堂上解决的问题的测试一样！'
- en: Thankfully, there’s something that can save the day. Do you remember that test
    dataset we talked about before? This is its time to shine! In fact, we have kept
    this test set in a safe-deposit box to ensure that none of its examples were ever
    used in the training process. We can think of them as completely new problems
    that the student has never seen, so we can use them to assess their understanding
    of the subject. Thus, we can compute the average loss of ![M_{\theta}](img/file1113.png
    "M_{\theta}") on the examples of the test set — this is sometimes called the **test
    error**. Provided that they are representative of the classification problem as
    a whole and that the number of examples is big enough, we can be quite confident
    that the test error will be close to the true error of the model. This is just
    an application of some *central* theorems in statistics!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一种东西可以拯救这一天。你还记得我们之前提到的测试数据集吗？现在是它大放异彩的时候了！事实上，我们已经把这个测试集保存在保险箱里，以确保其示例从未在训练过程中使用过。我们可以把它们看作是完全新的问题，学生从未见过，因此我们可以用它们来评估他们对主题的理解。因此，我们可以计算![M_{\theta}](img/file1113.png
    "M_{\theta}")在测试集示例上的平均损失——这有时被称为**测试误差**。只要它们代表整个分类问题，并且示例数量足够多，我们就可以相当有信心地说，测试误差将接近模型的真实误差。这仅仅是应用一些统计学中的**中心**定理的例子！
- en: Now, if the test error is similar to the empirical risk (and if they are low
    enough), we are done. That’s it! We have successfully trained a model. Nonetheless,
    as you can imagine, things can also go wrong. Terribly wrong.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果测试误差与经验风险相似（并且如果它们足够低），我们就完成了！这就是全部！我们已经成功训练了一个模型。然而，正如你可以想象的那样，事情也可能出错。非常糟糕的错误。
- en: What if the test error is much bigger than the empirical error, the one computed
    on the training set? This would be similar to having a student who knows how to
    repeat the solution to problems already solved by the teacher but who is unable
    to solve new problems. In our case, this would mean having a classifier that works
    beautifully on the training dataset but makes a lot of errors on any inputs outside
    of it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果测试误差远大于在训练集上计算的实证误差，这会类似于有一个学生知道如何重复老师已经解决的问题的解决方案，但却无法解决新问题。在我们的情况下，这意味着有一个在训练数据集上工作得很好但在其之外的任何输入上犯很多错误的分类器。
- en: This situation is called **overfitting**, and it is one of the biggest risks
    (no pun intended) in machine learning. It occurs whenever, somehow, our model
    has learned the particularities of the data it has seen but not the general patterns;
    that is, it has fitted the training data too well, hence the name ”overfitting.”
    This problem usually occurs when the training dataset is too small or when the
    model is too powerful. In the first case, there is simply not enough information
    to extract general patterns. That is why, in this chapter, we have insisted that
    the more data we have, the better. But what about the second case? Why can having
    a very powerful model end up being something bad?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况被称为**过拟合**，它是机器学习中最大的风险之一（无意中开玩笑）。它发生在我们的模型以某种方式学习了它所看到的数据的特定细节，但没有学习到一般模式；也就是说，它对训练数据拟合得太好了，因此得名“过拟合”。这个问题通常发生在训练数据集太小或模型太强大时。在前一种情况下，没有足够的信息来提取一般模式。这就是为什么在本章中，我们一直强调数据越多越好。但第二种情况呢？为什么一个非常强大的模型最终会变成一件坏事？
- en: An example can be very illustrative here. Let’s say that we want to use machine
    learning to approximate some unknown real function. We haven’t discussed how this
    setup would work, but the core ideas would be analogous to the ones we have seen
    (we would seek to minimize the expected value of a loss function, and so on).
    If we have a sample of ![1000](img/file790.png "1000") points in the plane, we
    can always find a polynomial of degree ![999](img/file1114.png "999") that fits
    the data perfectly, in the same way that we can always fit a line to just two
    points. However, if the points are just samples of ![f(x) = x](img/file1115.png
    "f(x) = x") with some noise (which could result from some empirical sampling errors
    or some other reason), our polynomial will go out of its way to fit those points
    perfectly and will quickly deviate from the linear shape that it should have learned.
    In this way, being able to fit too much information can sometimes go against the
    goal of learning the general patterns of data. This is illustrated in *Figure*
    [*8.1*](#Figure8.1). In it, the degree of the ”fitting” polynomial is so big that
    it can fit the training data perfectly, including its noise, but it misses the
    implicit linear pattern and performs very badly on test data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子可以非常说明问题。假设我们想使用机器学习来近似一些未知的真实函数。我们还没有讨论这种设置将如何工作，但核心思想将与我们所看到的类似（我们会寻求最小化损失函数的期望值，等等）。如果我们有一个平面上的![1000](img/file790.png
    "1000")个点的样本，我们总能找到一个![999](img/file1114.png "999")次的多项式来完美地拟合数据，就像我们总能将一条线拟合到两个点一样。然而，如果这些点只是![f(x)
    = x](img/file1115.png "f(x) = x")加上一些噪声（这可能是由于一些经验采样错误或其他原因），我们的多项式会不遗余力地拟合这些点，并迅速偏离它应该学习的线性形状。以这种方式，能够拟合过多信息有时会与学习数据的一般模式的目标相悖。这如图*8.1*所示。在这张图中，“拟合”多项式的次数非常大，可以完美地拟合训练数据，包括其噪声，但它错过了隐含的线性模式，在测试数据上的表现非常糟糕。
- en: '![Figure 8.1: A simple example of overfitting that results from using too powerful
    a model.](img/file1116.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1：使用过于强大的模型导致的过拟合简单示例](img/file1116.png)'
- en: '**Figure 8.1**: A simple example of overfitting that results from using too
    powerful a model.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**图8.1**：使用过于强大的模型导致的过拟合简单示例。'
- en: Important note
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Sometimes a machine learning model may only work properly on its training dataset.
    This phenomenon is known as **overfitting**. It usually occurs when the training
    dataset is too small or when the model is too powerful.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，机器学习模型可能仅在训练数据集上正常工作。这种现象被称为**过拟合**。它通常发生在训练数据集太小或模型太强大时。
- en: If you find yourself in a situation in which your model has overfitted the data,
    you can try obtaining more data — something that is not always possible — or somehow
    reducing the power of your model. For instance, with the neural networks that
    we will be studying later in the chapter, you can try reducing their size.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现自己处于模型过度拟合数据的情境中，你可以尝试获取更多数据——这并不总是可能的——或者以某种方式降低你模型的力量。例如，在我们将在本章后面研究的神经网络中，你可以尝试减小它们的大小。
- en: To learn more…
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多…
- en: Another popular technique for avoiding overfitting is using **regularization**.
    Roughly speaking, regularization restricts the values that some of the parameters
    of your model can take, effectively making it less powerful and less prone to
    fit every single detail of the training data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种避免过拟合的流行技术是使用**正则化**。简而言之，正则化限制了模型某些参数可以取的值，有效地使其变得更弱，并且不太可能拟合训练数据的每一个细节。
- en: To learn more about regularization techniques and their use in machine learning,
    we highly recommend checking the book by Aurélien Géron [[104](ch030.xhtml#Xhandsonml)].
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于正则化技术及其在机器学习中的应用，我们强烈推荐阅读Aurélien Géron所著的书籍[[104](ch030.xhtml#Xhandsonml)]。
- en: You may also want to know that your models can exhibit a type of problem that
    is the opposite of overfitting and has been aptly named **underfitting**. If your
    model is not expressive enough, you can find yourself with both a high error rate
    on the training set and a high error rate on the test set. For instance, if you
    are using a linear function to try to fit points that come from a quadratic polynomial
    and, thus, follow a parabolic shape, you will surely experience some form of underfitting.
    To fix this problem, use a more powerful model — or reduce regularization if you
    happen to be using it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还想知道，你的模型可能会表现出一种与过拟合相反的问题，这被恰当地命名为**欠拟合**。如果你的模型表达能力不足，你可能会在训练集和测试集上发现高误差率。例如，如果你使用线性函数来尝试拟合来自二次多项式的点，这些点遵循抛物线形状，你肯定会遇到某种形式的欠拟合。为了解决这个问题，使用更强大的模型——或者如果你恰好在使用正则化，减少正则化的强度。
- en: To summarize what we have discussed so far, remember that we want to obtain
    a model that has a low generalization error; that is, a model that works well
    even on data it has not been trained with. In order to achieve this, we consider
    a parametric model and look for those model parameters that minimize the error
    on the training set, because we cannot easily compute the true error. And to be
    sure that the model will behave well when confronted with new data, we compute
    the error on the test dataset as a way of assessing how representative the empirical
    risk is of the error on unseen data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总结到目前为止我们所讨论的内容，请记住，我们希望获得一个具有低泛化误差的模型；也就是说，一个即使在未训练过的数据上也能表现良好的模型。为了实现这一点，我们考虑一个参数化模型，并寻找那些在训练集上最小化误差的模型参数，因为我们无法轻易计算真实误差。并且为了确保模型在面对新数据时表现良好，我们计算测试数据集上的误差，作为评估经验风险对未见过数据的误差代表性的方法。
- en: With this strategy, however, we may still be vulnerable to an additional problem.
    If we train a lot of different models, there is a risk that — just by pure chance!
    — one of them has great performance on the test dataset but not on the rest of
    the domain. In fact, this risk is higher the more models you train. Imagine that
    a thousand students take a test of 10 questions with 2 possible answers each.
    Even if they have not studied for the test and they answer completely at random,
    there is a very high probability that at least one of them will nail it. For this
    reason, you should never use the test dataset to select among your models, only
    to assess if their behavior is similar to the behavior they show during training.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用这种策略，我们仍然可能面临另一个问题。如果我们训练了大量的不同模型，那么仅仅通过纯粹的偶然性，其中一个模型可能在测试数据集上有很好的表现，但在其他领域则不行。事实上，训练的模型越多，这种风险就越高。想象一下，一千名学生参加一个有10个问题、每个问题有两个可能答案的考试。即使他们没有为考试学习，并且完全随机回答，至少有一个人答对的可能性非常高。因此，你不应该使用测试数据集来选择模型，而应该用它来评估模型的行为是否与训练期间的行为相似。
- en: This is definitely a problem because we usually want to train many different
    models and select the one we believe to be the best. What is more, many models
    have what are called **hyperparameters**. These are parameters that fix some property
    of the model, such as the size and number of layers in a neural network (more
    on that later), that cannot be optimized during training. Usually, we train many
    different models with different values of these hyperparameters, and then we select
    the best model from them.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是一个问题，因为我们通常希望训练许多不同的模型，并选择我们认为最好的一个。更重要的是，许多模型都有所谓的**超参数**。这些参数固定了模型的某些属性，例如神经网络的大小和层数（稍后会有更多介绍），这些参数在训练过程中无法优化。通常，我们会用不同超参数值训练许多不同的模型，然后从这些模型中选出最佳模型。
- en: 'This is where a third type of dataset comes into the equation: the **validation**
    **dataset**. This is an additional dataset that we could construct when splitting
    our global dataset; it should, of course, be fully independent of the training
    and test datasets.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是第三种数据集进入等式的地方：**验证数据集**。当我们分割全局数据集时，我们可以构建这样一个额外的数据集；当然，它应该完全独立于训练集和测试集。
- en: What do we want the validation set for? Once we have trained our models with
    different choices of hyperparameters and configurations, we can compute the empirical
    risk on the validation set, and we may select the best one or maybe a handful
    of the best ones. Then, we could train those models again on the union of the
    training set and the validation set — to better extract all the information from
    our data — and then compute the error of the models on the test set, which we
    have held back until this very moment so that it remains a good estimator of the
    generalization error. In this way, we can select the best choice of hyperparameters
    or models while keeping the test dataset in pristine condition to be used in a
    final assessment process.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要验证集做什么呢？一旦我们用不同的超参数和配置训练了我们的模型，我们就可以在验证集上计算经验风险，并可能选择最好的一个或几个。然后，我们可以在训练集和验证集的并集上再次训练这些模型——以便更好地从我们的数据中提取所有信息——然后计算模型在测试集上的误差，我们一直保留到这个时刻，以便它仍然是一个很好的泛化误差估计器。这样，我们可以在保持测试集原始状态以用于最终评估过程中选择最佳的超参数或模型。
- en: To learn more…
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多...
- en: You may also want to know that, instead of using a fixed validation set, a popular
    way of selecting hyperparameters is to use ![k](img/file317.png "k")**-fold**
    **cross-validation**. With this technique, the training dataset is divided into
    ![k](img/file317.png "k") subsets or **folds** of equal size. The training of
    the model is repeated ![k](img/file317.png "k") times, each one with a different
    subset acting as a validation dataset and the rest used as the training dataset.
    The performance is computed over each validation set and averaged over the ![k](img/file317.png
    "k") repetitions. Of course, the estimation obtained with cross-validation is
    better than when using a fixed validation set, but the computational cost is much
    higher — ![k](img/file317.png "k") times higher, in fact! Software libraries such
    as scikit-learn — which we will be using in the next section of this chapter —
    provide implementations of cross-validation for hyperparameter selection. Take
    a look at [the documentation of `GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
    — where CV stands for cross validation — if you want to see a concrete implementation
    of this technique.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还想知道，使用固定的验证集，选择超参数的一种流行方式是使用![k](img/file317.png "k")**-折交叉验证**。使用这种技术，训练数据集被分成![k](img/file317.png
    "k")个子集或**折**，大小相等。模型的训练重复![k](img/file317.png "k")次，每次使用不同的子集作为验证数据集，其余的作为训练数据集。性能在每一个验证集上计算，并在![k](img/file317.png
    "k")次重复中平均。当然，使用交叉验证获得的结果估计比使用固定的验证集更好，但计算成本要高得多——实际上高![k](img/file317.png "k")倍！像scikit-learn这样的软件库——我们将在本章下一节中使用——提供了用于超参数选择的交叉验证实现。如果你想看到这种技术的具体实现，请查看[GridSearchCV的文档](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)——其中CV代表交叉验证。
- en: 'Furthermore, sometimes training processes are iterative. In these cases, the
    **validation loss** (the average loss over the validation dataset) can be computed
    at the end of each iteration and compared against the training loss, just to know
    how the training is going — and to be able to stop it early should the model begin
    to overfit! It wouldn’t be a good practice to use the test set for this purpose:
    the test set should only be used once all the training is complete and we just
    want some reassurance on the validity of our results.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有时训练过程是迭代的。在这些情况下，**验证损失**（验证数据集上的平均损失）可以在每个迭代的末尾计算，并与训练损失进行比较，以便了解训练的进展情况——并且如果模型开始过拟合，可以提前停止！使用测试集来达到这个目的不是好的做法：测试集应该只在所有训练完成后使用，我们只是想确保我们的结果的有效性。
- en: To learn more…
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多...
- en: All the informal notions that we are considering here can be formulated precisely
    using the language of probability theory. If you want to learn about the formal
    machinery behind machine learning, you should have a look at the book *Learning
    from data* [[1](ch030.xhtml#Xabu2012learning)] or at *Understanding machine learning*
    [[105](ch030.xhtml#Xunderml)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里考虑的所有非正式概念都可以使用概率理论的语言精确地表达。如果你想了解机器学习背后的正式机制，你应该看看《从数据中学习》[[1](ch030.xhtml#Xabu2012learning)]或《理解机器学习》[[105](ch030.xhtml#Xunderml)]这本书。
- en: With all of this, we now have a good understanding of all the elements needed
    for machine learning to come alive. In the following section, we will try to make
    all of this more precise by studying some of the most common approaches that are
    taken when training ML models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过所有这些，我们现在对机器学习所需的所有元素都有了很好的理解。在下一节中，我们将通过研究在训练机器学习模型时采取的一些最常见方法，来尝试使所有这些内容更加精确。
- en: 8.1.2 Types of machine learning
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1.2 机器学习的类型
- en: 'There are three big categories in which most, if not all, machine learning
    techniques can fit: **supervised learning**, **unsupervised learning**, and **reinforcement
    learning**. In this book, we will work mostly with supervised learning, but we
    will also consider some unsupervised learning techniques. Let’s explain in a little
    bit more detail what each of these machine learning branches is about.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数，如果不是所有，机器学习技术都可以归入三个主要类别：**监督学习**、**无监督学习**和**强化学习**。在这本书中，我们将主要使用监督学习，但也会考虑一些无监督学习技术。让我们更详细地解释一下每个机器学习分支的内容。
- en: '**Supervised learning** The main goal of supervised learning is to learn to
    predict the values of a function on input data. These values can either be chosen
    from a finite set (the classification problems we have been talking about for
    the most part of this chapter) or be continuous values, such as the weight of
    a person or the value of some bonds a month from now. When the values we want
    to predict are continuous, we say that we are tackling a **regression** problem.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习** 监督学习的主要目标是学习预测输入数据上函数的值。这些值可以是来自有限集（我们在这章的大部分时间里一直在讨论的分类问题）的选择，或者可以是连续值，例如一个人的体重或一个月后某些债券的价值。当我们想要预测的值是连续的，我们说我们在解决一个**回归**问题。'
- en: When we train a model using supervised learning, we need to work with a dataset
    that has both a large-enough collection of valid inputs and all the expected outputs
    that our model should return for these inputs. This is known as having a **labeled**
    dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用监督学习来训练模型时，我们需要处理一个数据集，这个数据集既有足够多的有效输入，也有模型对这些输入应该返回的所有预期输出。这被称为拥有一个**标记**的数据集。
- en: For example, if we were to train a cat-rabbit picture classifier using supervised
    learning, we would need to have a (large-enough) dataset with pictures of rabbits
    and cats, and we would also need to know, for each of those pictures, whether
    they are pictures of rabbits or pictures of cats.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想要使用监督学习训练一个猫-兔图片分类器，我们需要有一个包含兔子和大猫图片的（足够大的）数据集，并且我们还需要知道，对于这些图片中的每一张，它们是兔子的图片还是猫的图片。
- en: With our labeled dataset, we would define a loss function that would depend
    on the inputs and the parameters of the model — so that we can compute the corresponding
    outputs — and the expected (correct) outputs. And, as we discussed before, then
    we would just apply an optimization algorithm to find a configuration of the model
    that could minimize the loss function on the training dataset — while ensuring
    that there is no overfitting.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的标记数据集，我们会定义一个依赖于输入和模型参数的损失函数——这样我们就可以计算相应的输出——以及预期的（正确）输出。而且，正如我们之前讨论的那样，然后我们只需应用一个优化算法来找到一个模型配置，该配置可以在训练数据集上最小化损失函数——同时确保没有过拟合。
- en: We still have to discuss how that optimization algorithm is going to work, but
    that is for later!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要讨论那个优化算法将如何工作，但这留待以后再说！
- en: '**Unsupervised learning** When we work with unsupervised learning, we have
    access to **unlabeled** datasets, in which there are no expected outputs. We let
    the algorithm learn on its own by trying to identify certain patterns. For instance,
    we may want to group similar data points together (this is known as **clustering**)
    or we may want to learn something about how the data is distributed.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习** 当我们使用无监督学习时，我们可以访问**未标记**的数据集，其中没有预期的输出。我们让算法通过尝试识别某些模式来自行学习。例如，我们可能想要将相似的数据点分组在一起（这被称为**聚类**），或者我们可能想要了解数据是如何分布的。'
- en: In this latter case, our goal would be to train a **generative model** that
    we can use to create new data samples. An impressive example is the use of **Generative
    Adversarial Networks**, introduced by Ian Goodfellow and his collaborators in
    a highly influential paper [[46](ch030.xhtml#Xgoodfellow2014generative)] to create
    images that are similar — but completely different — to the ones used in the training
    phase. This is the kind of model that we will be working with in *Chapter* *[*12*](ch021.xhtml#x1-21200012),
    *Quantum Generative Adversarial Networks*,…in a quantum form, of course!*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在后一种情况下，我们的目标将是训练一个**生成模型**，我们可以用它来创建新的数据样本。一个令人印象深刻的应用实例是使用**生成对抗网络**，由Ian
    Goodfellow及其合作者在具有高度影响力的论文[[46](ch030.xhtml#Xgoodfellow2014generative)]中提出，以创建与训练阶段使用的图像相似——但完全不同的图像。这正是我们将在*第*
    *[*12*](ch021.xhtml#x1-21200012)，* *量子生成对抗网络* *中工作的模型……当然是以量子形式！*
- en: '***Reinforcement learning** In reinforcement learning, the model — usually
    called the **agent** in this setting — interacts with an **environment**, trying
    to complete some task. This agent observes the **state** of the environment and
    takes some **actions** that in turn influence the state it observes. Depending
    on its performance, it receives ”rewards” and ”punishments”…and, of course, it
    wants to maximize the rewards while minimizing the punishments. To do that, it
    tries to learn a **policy** that determines what action to take for a given state
    of the environment.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习** 在强化学习中，模型——在这个设置中通常被称为**智能体**——与**环境**交互，试图完成某些任务。这个智能体观察环境的**状态**并采取一些**行动**，这些行动反过来又影响它所观察到的状态。根据其表现，它会收到“奖励”和“惩罚”……当然，它希望最大化奖励同时最小化惩罚。为了做到这一点，它试图学习一个**策略**，该策略决定了在给定环境状态时采取什么行动。'
- en: For instance, the agent may be a robot and the environment a maze it needs to
    navigate. The state can consist of its position in the maze and the open paths
    it can follow, and its actions can be rotating in some direction and moving forward.
    The goal may be finding the exit to the maze in some predefined time, for which
    the robot will get a positive reward.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，智能体可能是一个机器人，环境是一个它需要导航的迷宫。状态可以包括它在迷宫中的位置和它可以跟随的开放路径，它的行动可以是旋转到某个方向并向前移动。目标可能是找到迷宫的出口，在预定的某个时间内完成，这样机器人将获得正面的奖励。
- en: This kind of learning has been used extensively to train models designed to
    play games — AlphaGo, the computer program that in 2016 beat Go (human) grandmaster
    Lee Sedol in a five-games match, is a prominent example! To learn more about reinforcement
    learning, a good source is the book by Sutton and Barto [[93](ch030.xhtml#Xsutton2018reinforcement)].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习已被广泛用于训练旨在玩游戏的设计模型——AlphaGo，这个计算机程序在2016年以五场比赛击败了围棋（人类）大师李世石，是一个突出的例子！要了解更多关于强化学习的信息，一个很好的来源是Sutton和Barto合著的书籍[[93](ch030.xhtml#Xsutton2018reinforcement)]。
- en: Although there has been some interest in using quantum techniques in reinforcement
    learning (see, for instance [[91](ch030.xhtml#Xskolik2022quantum)]), this may
    very well be the machine learning branch in which quantum algorithms are less
    developed at the moment. For this reason, we will not cover this kind of learning
    in this book. Hopefully, in a few years there will be much more to tell about
    quantum reinforcement learning! Let’s now try to make everything concrete by using
    supervised learning to implement a very simple classifier. For this, we will use
    TensorFlow.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人们对在强化学习中使用量子技术表现出一些兴趣（例如，参见[[91](ch030.xhtml#Xskolik2022quantum)]），但这可能是目前量子算法发展较慢的机器学习分支。因此，我们不会在本书中涵盖这种学习。希望几年后会有更多关于量子强化学习的内容！现在，让我们通过使用监督学习来实现一个非常简单的分类器来使一切具体化。为此，我们将使用TensorFlow。
- en: 8.2 Do you wanna train a model?
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.2 你想训练一个模型吗？
- en: TensorFlow is a machine learning framework developed at Google, and it is very
    widely used. You should refer to *Appendix* [*D*](ch027.xhtml#x1-240000D), *Installing
    the* *Tools*, for installation instructions. Keep in mind that we will be using
    version 2.9.1\. We will use TensorFlow in some of our quantum machine learning
    models, so it is a good idea to become familiar with it early on.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是Google开发的一个机器学习框架，它被广泛使用。你应该参考*附录* [*D*](ch027.xhtml#x1-240000D)，*安装工具*，以获取安装说明。请记住，我们将使用2.9.1版本。我们将在我们的量子机器学习模型中使用TensorFlow，因此尽早熟悉它是明智的。
- en: To keep things simple, we will tackle an artificial problem. We are going to
    prepare a dataset of elements belonging to one of two possible categories, and
    we will try to use machine learning to construct a classifier that can distinguish
    to which category any given input belongs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持事情简单，我们将解决一个人工问题。我们将准备一个包含属于两种可能类别之一的元素的数据库，并尝试使用机器学习来构建一个分类器，以区分任何给定输入属于哪个类别。
- en: 'Before we do anything, let us quickly import NumPy and set a seed:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们做任何事情之前，让我们快速导入NumPy并设置一个种子：
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will later use this same seed with TensorFlow. And now, let’s generate the
    data!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后将与TensorFlow使用相同的种子。现在，让我们生成数据！
- en: 'Instead of generating a dataset by hand, we will use a function provided by
    the Python **scikit-learn** package (`sklearn`). This package is a very valuable
    resource for machine learning: not only does it include plenty of useful tools
    for everyday machine-learning-related tasks, but it also allows you to train and
    execute a wide collection of interesting models! We will use version 1.0.2 of
    `sklearn` and, as always, you should refer to *Appendix* *[*D*](ch027.xhtml#x1-240000D),
    *Installing the Tools*, for installation instructions.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会手动生成数据库，而是将使用Python **scikit-learn** 包（`sklearn`）提供的函数。这个包是机器学习的一个非常有价值的资源：它不仅包括大量用于日常机器学习相关任务的有用工具，而且还允许你训练和执行大量有趣的模型！我们将使用`sklearn`的1.0.2版本，并且，像往常一样，你应该参考*附录*
    *[*D*](ch027.xhtml#x1-240000D)，*安装工具*，以获取安装说明。
- en: '*In order to generate our dataset, we will use the `make_classification` function
    from `sklearn``.``datasets`. We will ask it to generate ![2500](img/file1117.png
    "2500") samples of a dataset with two features (variables). We will also ask for
    both features to be **informative** and not redundant; the variables would be
    redundant, for example, if one of them were just a multiple of the other. Lastly,
    we will ask for the proportions of the two categories in the dataset to be ![20\,\%](img/file1118.png
    "20\,\%") to ![80\,\%](img/file1119.png "80\,\%"). We can do this as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*为了生成我们的数据库，我们将使用`sklearn.datasets`中的`make_classification`函数。我们将要求它生成![2500](img/file1117.png
    "2500")个具有两个特征（变量）的数据库样本。我们还将要求两个特征都是**信息性**的，而不是冗余的；例如，如果其中一个变量只是另一个变量的倍数，则这些变量将是冗余的。最后，我们还将要求数据库中两个类别的比例是![20\,\%](img/file1118.png
    "20\,\%")到![80\,\%](img/file1119.png "80\,\%")。我们可以这样做：'
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `class_sep` argument specifies how separable we want the two categories
    to be: the higher the value of this argument, the easier it is to distinguish
    them. Notice, also, that we have used the seed that we set earlier in order for
    the results to be repeatable.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`class_sep`参数指定了我们希望两个类别有多大的可区分性：此参数的值越高，区分它们就越容易。请注意，我们还使用了之前设置的种子，以确保结果可重复。'
- en: You may now be wondering why we have specified that we want the two categories
    in the dataset to be in a proportion ![20\,\%](img/file1118.png "20\,\%") to ![80\,\%](img/file1119.png
    "80\,\%"), when it would be much more natural for the two categories to be balanced.
    Indeed, it is desirable for both categories to have the same number of representatives
    in a dataset…but life is difficult, and in many practical scenarios, that is not
    a possibility! So just think of this choice of ours as our own little way of feeling
    closer to real life.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可能想知道，为什么我们指定了数据库中的两个类别比例应该是![20\,\%](img/file1118.png "20\,\%")到![80\,\%](img/file1119.png
    "80\,\%")，当两个类别平衡会更为自然。确实，在数据库中两个类别拥有相同数量的代表是理想的……但是生活是艰难的，在许多实际场景中，这种情况并不总是可能的！所以，就让我们把我们的这个选择看作是我们自己接近现实生活的小小方式吧。
- en: Essentially, the `make_classification` function has returned an array `data`
    with the whole dataset (including all the elements from both categories, positive
    and negative), and an array `labels` such that the label of `data``[``i``]` will
    be `labels``[``i``]` (where ![0](img/file12.png "0") corresponds to positive and
    ![1](img/file13.png "1") to negative).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`make_classification`函数返回了一个包含整个数据集（包括两个类别中所有正负元素）的数组`data`，以及一个数组`labels`，使得`data[i]`的标签将是`labels[i]`（其中![0](img/file12.png
    "0")对应正类，![1](img/file13.png "1")对应负类）。
- en: 'Just to get a feeling of what this dataset that we have created looks like,
    we can plot a simple histogram showing the distributions of the two features of
    our dataset:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们对所创建的数据集有一个直观的感觉，我们可以绘制一个简单的直方图，显示我们数据集中两个特征的分布：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Upon running this, we got the plots shown in *Figure* [*8.2*](#Figure8.2).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此操作后，我们得到了*图*[*8.2*](#Figure8.2)中所示的图表。
- en: '![Figure 8.2: Histograms representing the distributions of the two features
    of our dataset.](img/file1120.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2：表示我们数据集两个特征的分布的直方图。](img/file1120.png)'
- en: '**Figure 8.2**: Histograms representing the distributions of the two features
    of our dataset.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**图8.2**：表示我们数据集两个特征的分布的直方图。'
- en: Exercise 8.2
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.2
- en: Visualizing the data you are working with through graphs can help you gain insights
    into how to approach the problem you have at hand. We have plotted our data using
    a histogram, which is usually a good choice. What other representations could
    we have used?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过图表可视化你所处理的数据可以帮助你深入了解如何解决你手头的难题。我们已经使用直方图绘制了我们的数据，这通常是一个不错的选择。我们还可以使用哪些其他表示方法？
- en: Our goal now is to use machine learning to come up with a system that can solve
    the classification problem that we have created. And the first step in doing so
    will be to pick a good model to tackle our problem!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的目标是使用机器学习来构建一个可以解决我们创建的分类问题的系统。这样做的第一步将是选择一个好的模型来处理我们的问题！
- en: 8.2.1 Picking a model
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.1 选择一个模型
- en: Not long ago, we introduced the perceptron and we showed how, *on its own*,
    it wasn’t the most powerful of models out there. We will now shed some light on
    why we emphasized ”on its own,” for we are about to introduce a very interesting
    model that can be thought of as being built by joining perceptrons together. Let’s
    dive into **neural networks**!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 不久前，我们介绍了感知器，并展示了它**单独使用**时并不是最强大的模型。现在，我们将解释为什么我们强调了“单独使用”，因为我们即将介绍一个非常有趣的模型，这个模型可以被认为是通过将感知器连接在一起来构建的。让我们深入探讨**神经网络**吧！
- en: You may remember how a perceptron took ![N](img/file784.png "N") numerical inputs
    ![x_{i}](img/file714.png "x_{i}"), used on a collection of ![N](img/file784.png
    "N") weights ![w_{i}](img/file1095.png "w_{i}") and a bias ![b](img/file17.png
    "b"), and returned an output that depended on the value of
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，感知器如何接受![N](img/file784.png "N")个数值输入![x_{i}](img/file714.png "x_{i}")，使用一组![N](img/file784.png
    "N")个权重![w_{i}](img/file1095.png "w_{i}")和一个偏置![b](img/file17.png "b")，并返回一个依赖于
- en: '| ![\sum\limits_{i = 1}^{N}w_{i}x_{i} + b.](img/file1121.png "\sum\limits_{i
    = 1}^{N}w_{i}x_{i} + b.") |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ![求和符号\sum\limits_{i = 1}^{N}w_{i}x_{i} + b.](img/file1121.png "\sum\limits_{i
    = 1}^{N}w_{i}x_{i} + b.") |'
- en: 'Well, in this way, we can think of a neural network as being a collection of
    perceptrons — which we will, from now on, call **neurons** — organized in the
    following way:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这样我们就可以将神经网络看作是一组感知器——从现在起，我们将它们称为**神经元**——按照以下方式组织：
- en: All the neurons are arranged into layers, and the output of the neurons in one
    layer is the input of the neurons in the next layer
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有神经元都排列成层，一个层的神经元的输出是下一层神经元的输入。
- en: In addition to this, the ”raw” linear output of every neuron will go through
    a (very possibly non-linear) **activation function** of our choice
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，每个神经元的“原始”线性输出将通过我们选择的（很可能非线性的）**激活函数**。
- en: That is the general idea, but let’s now make it precise.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一般思路，但现在让我们使其更精确。
- en: 'A neural network with ![N_{0}](img/file1122.png "N_{0}") inputs is defined
    from the following elements:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有![N_{0}](img/file1122.png "N_{0}")个输入的神经网络由以下元素定义：
- en: An ordered sequence of **layers** (![l = 1,\ldots,L](img/file1123.png "l = 1,\ldots,L")),
    each with a fixed amount of **neurons** ![N_{l}](img/file1124.png "N_{l}").
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有序的**层**序列([l = 1,\ldots,L](img/file1123.png "l = 1,\ldots,L"))，每层有固定数量的**神经元**![N_{l}](img/file1124.png
    "N_{l}")。
- en: A bunch of **activation functions** ![h_{ln}](img/file1125.png "h_{ln}") for
    each neuron ![n](img/file244.png "n") in a layer ![l](img/file514.png "l").
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个层的每个神经元![n](img/file244.png "n")都有一个**激活函数**![h_{ln}](img/file1125.png "h_{ln}")。
- en: A set of **biases** ![b_{ln}](img/file1126.png "b_{ln}") for every neuron, and,
    for every neuron ![n](img/file244.png "n") in a layer ![l](img/file514.png "l"),
    a set of ![N_{l - 1}](img/file1127.png "N_{l - 1}") **weights** ![w_{kln}](img/file1128.png
    "w_{kln}") with ![k = 1,\ldots,N_{l - 1}](img/file1129.png "k = 1,\ldots,N_{l
    - 1}"). These biases and weights are the adjustable parameters that we would need
    to tweak in order to get the model to behave as we want it to.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个神经元都有一个**偏差** ![b_{ln}](img/file1126.png "b_{ln}") 的集合，对于每一层 ![l](img/file514.png
    "l") 中的神经元 ![n](img/file244.png "n")，都有一个 ![N_{l - 1}](img/file1127.png "N_{l
    - 1}") **权重** ![w_{kln}](img/file1128.png "w_{kln}") 的集合，其中 ![k = 1,\ldots,N_{l
    - 1}](img/file1129.png "k = 1,\ldots,N_{l - 1}")。这些偏差和权重是我们需要调整以使模型按我们希望的方式运行的调整参数。
- en: In *Figure* *[*8.3*](#Figure8.3), we can see a graphical representation of a
    simple neural network.*
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图* *[*8.3*](#Figure8.3) 中，我们可以看到一个简单神经网络的图形表示。
- en: '*![Figure 8.3: A simple neural network with two layers taking three inputs
    (a_{0n}). We have labeled some of the weights, but none of the biases or the activation
    functions ](img/file1131.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*![图8.3：一个简单的具有两层且接收三个输入 (a_{0n}) 的神经网络。我们标记了一些权重，但没有标记任何偏差或激活函数 ](img/file1131.jpg)'
- en: '**Figure 8.3**: A simple neural network with two layers taking three inputs
    (![a_{0n})](img/file1130.png "a_{0n})"). We have labeled some of the weights,
    but none of the biases or the activation functions'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**图8.3**：一个简单的具有两层且接收三个输入 ![a_{0n})](img/file1130.png "a_{0n})") 的神经网络。我们标记了一些权重，但没有标记任何偏差或激活函数'
- en: 'These are the ingredients that we need to set up a neural network. So, how
    does it work, then? Easy! For any choice of activation functions ![h_{ln}](img/file1125.png
    "h_{ln}"), biases ![b_{ln}](img/file1126.png "b_{ln}") and weights ![w_{kln}](img/file1128.png
    "w_{kln}"), the neural network takes some numerical inputs ![a_{0n}](img/file1132.png
    "a_{0n}") and, from there on, these inputs are propagated through the layers of
    the neural network in the following way: the values ![a_{ln}](img/file1133.png
    "a_{ln}") of the neurons ![n](img/file244.png "n") in all layers ![l](img/file514.png
    "l") are determined according to the inductive formula'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是我们设置神经网络所需的成分。那么，它是如何工作的呢？很简单！对于任何选择的激活函数 ![h_{ln}](img/file1125.png "h_{ln}")、偏差
    ![b_{ln}](img/file1126.png "b_{ln}") 和权重 ![w_{kln}](img/file1128.png "w_{kln}")，神经网络接收一些数值输入
    ![a_{0n}](img/file1132.png "a_{0n}")，然后，从那里开始，这些输入以以下方式通过神经网络的层传播：所有层 ![l](img/file514.png
    "l") 中神经元 ![n](img/file244.png "n") 的值 ![a_{ln}](img/file1133.png "a_{ln}") 根据归纳公式确定
- en: '| ![a_{ln}: - h_{ln}\left( {b_{ln} + \sum\limits_{k = 1}^{N_{l - 1}}w_{kln}a_{l
    - 1,k}} \right).](img/file1134.png "a_{ln}: - h_{ln}\left( {b_{ln} + \sum\limits_{k
    = 1}^{N_{l - 1}}w_{kln}a_{l - 1,k}} \right).") |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ![a_{ln}: - h_{ln}\left( {b_{ln} + \sum\limits_{k = 1}^{N_{l - 1}}w_{kln}a_{l
    - 1,k}} \right).](img/file1134.png "a_{ln}: - h_{ln}\left( {b_{ln} + \sum\limits_{k
    = 1}^{N_{l - 1}}w_{kln}a_{l - 1,k}} \right).") |'
- en: With this procedure, we can assign a value to each neuron in the network. The
    values of the neurons in the last layer are the output of the model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个程序，我们可以为网络中的每个神经元分配一个值。最后一层中神经元的值是模型的输出。
- en: To be precise, what we have just described is known as an **artificial** **feed-forward
    dense neural network**. There are other possible architectures for neural networks,
    but this is the one that will be using for the most part of the rest of the book.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，我们刚才描述的是所谓的**人工** **前馈密集神经网络**。神经网络还有其他可能的架构，但这是本书大部分内容将使用的架构。
- en: 'That is how you can define a neural network, but there is one element in the
    definition to which we have not paid much attention: the activation function.
    We have mentioned before that this can be any function of our choice, and we have
    seen what role it plays in the behavior of a neural network, but what are some
    reasonable choices for this function? Let’s explore the most common ones:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是如何定义一个神经网络，但在定义中有一个元素我们没有给予太多关注：激活函数。我们之前提到过，这可以是我们选择的任何函数，我们也看到了它在神经网络行为中扮演的角色，但这个函数有哪些合理的选项呢？让我们来探索最常见的几种：
- en: We may start off with a simple activation function, actually, the same one that
    we implicitly considered when we defined the perceptron. This is a **step function**
    given by
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能从一个简单的激活函数开始，实际上，这就是我们在定义感知器时隐含考虑的同一个函数。这是一个**阶梯函数**，由以下给出
- en: '| ![h(x) = \left\{ \begin{array}{ll} {1,\quad} & {x \geq 0} \\ {0,\quad} &
    {x < 0.} \\ \end{array} \right.](img/file1135.png "h(x) = \left\{ \begin{array}{ll}
    {1,\quad} & {x \geq 0} \\ {0,\quad} & {x < 0.} \\ \end{array} \right.") |'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: We could technically use this in a neural network, but…in truth…it would not
    be a very wise choice. It is not differentiable, not even continuous. And, as
    we will soon see, that usually makes any function a terrible candidate to be an
    activation function inside a neural network. In any case, it is an example of
    historical importance.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s now consider a somewhat more sophisticated and interesting example: the
    **sigmoid** activation function. This function is smooth and continuous, and it
    outputs values between ![0](img/file12.png "0") and ![1](img/file13.png "1").
    This makes it an ideal candidate for the activation function in the final layer
    of, for example, a classifier. It is defined by'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![S(x) = \frac{e^{x}}{e^{x} + 1}.](img/file1136.png "S(x) = \frac{e^{x}}{e^{x}
    + 1}.") |'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: We have plotted it in *Figure* [*8.4a*](#Figure8.4a).
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As beautiful as it may seem, when used in inner layers, the sigmoid function
    can easily lead to problems in the training process (see Aurelien’s book for more
    on this [[104](ch030.xhtml#Xhandsonml)]). In general, a better choice for inner
    layers is the **exponential linear unit** or **ELU** activation function, defined
    as
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![E(x) = \left\{ \begin{array}{ll} {x,\quad} & {x \geq 0} \\ {e^{x} - 1,\quad}
    & {x < 0.} \\ \end{array} \right.](img/file1137.png "E(x) = \left\{ \begin{array}{ll}
    {x,\quad} & {x \geq 0} \\ {e^{x} - 1,\quad} & {x < 0.} \\ \end{array} \right.")
    |'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: You can find its plot in *Figure* [*8.4b*](#Figure8.4b).
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will also discuss one last activation function: the **rectified linear unit**
    or **ReLU** function. In general, it yields worse results than the ELU function,
    but it is easier to compute and thus its use can speed up the training. It is
    defined as'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![R(x) = \max\{ 0,x\}.](img/file1138.png "R(x) = \max\{ 0,x\}.") |'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: The plot can be found in *Figure* [*8.4c*](#Figure8.4c).
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Exercise 8.3
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Check that the image of the sigmoid function ![S](img/file73.png "S") is ![(0,1)](img/file305.png
    "(0,1)"). Prove that the ELU function ![E](img/file327.png "E") is smooth and
    that its image is ![( - 1,\infty)](img/file1139.png "( - 1,\infty)"). What is
    the image of the ReLU function? Is it smooth?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![(a) Sigmoid function](img/file1140.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: '**(a)** Sigmoid function'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![(b) ELU function](img/file1141.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: '**(b)** ELU function'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![(c) ReLU function](img/file1142.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: '**(c)** ReLU function'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 8.4**: Some common activation functions in neural network'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned at the beginning of the chapter, it has been proven that neural
    networks are universal function approximators [[107](ch030.xhtml#Xnn-universal)],
    so they are interesting models to consider in any problem involving supervised
    machine learning. And thus, they will be the model we will use to build our classifier.
    We will consider a neural network with two inputs and some layers — we will later
    decide how many of them and how many neurons each will have. The final layer,
    of course, will have a single neuron, which will be the output. We will use ELU
    activation functions all throughout the network, except for the last layer; there,
    we will use a sigmoid activation function in order to get a normalized result.
    That way, we will get a continuous value between ![0](img/file12.png "0") and
    ![1](img/file13.png "1"), and, as it is customary, we will define a threshold
    at ![\left. 1\slash 2 \right.](img/file136.png "\left. 1\slash 2 \right.") to
    assign positive (![\left. \geq 1\slash 2 \right.](img/file1143.png "\left. \geq
    1\slash 2 \right.")) or negative (![\left. < 1\slash 2 \right.](img/file1144.png
    "\left. < 1\slash 2 \right.")) to any given output.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在本章开头提到的，已经证明神经网络是通用的函数逼近器 [[107](ch030.xhtml#Xnn-universal)]，因此它们是任何涉及监督机器学习问题的有趣模型。因此，它们将成为我们构建分类器所使用的模型。我们将考虑一个具有两个输入和一些层的神经网络——我们稍后将会决定它们的具体数量以及每个层将有多少个神经元。最后一层当然将只有一个神经元，它将是输出。在整个网络中，我们将使用ELU激活函数，除了最后一层；在那里，我们将使用sigmoid激活函数以获得归一化的结果。这样，我们将得到一个介于
    ![0](img/file12.png "0") 和 ![1](img/file13.png "1") 之间的连续值，并且按照惯例，我们将定义一个阈值 ![\left.
    1\slash 2 \right.](img/file136.png "\left. 1\slash 2 \right.") 来分配正 (![\left.
    \geq 1\slash 2 \right.](img/file1143.png "\left. \geq 1\slash 2 \right.")) 或负
    (![\left. < 1\slash 2 \right.](img/file1144.png "\left. < 1\slash 2 \right."))
    给任何给定的输出。
- en: Now that our model is ready, the next challenge that is waiting for us is finding
    a suitable loss function.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了模型，接下来等待我们的挑战是找到一个合适的损失函数。
- en: 8.2.2 Understanding loss functions
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.2 理解损失函数
- en: When it comes to defining a loss function for supervised machine learning, with
    models that depend on some continuous parameters, we want to look for loss functions
    that are continuous and differentiable with respect to the trainable parameters
    of the model. The reason for this is the same reason why we want our activation
    functions to be differentiable, and it will become clear later on.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到为依赖于某些连续参数的监督机器学习模型定义损失函数时，我们希望寻找对模型的可训练参数连续且可微的损失函数。这个原因与为什么我们希望激活函数可微的原因相同，这将在稍后变得清晰。
- en: 'As we discussed earlier, the most natural loss function — and the one whose
    expected value we truly want to minimize — would be the 0-1 loss function, but
    this function would not have a continuous dependence on the parameters of the
    model: it would take ”discrete jumps” as the classifier changes its behavior.
    Therefore, we need to look for alternative loss functions that are indeed continuous
    and differentiable while still measuring the loss in a manner that is reasonable
    and natural enough for classification problems.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，最自然的损失函数——我们真正想要最小化的期望值——将是0-1损失函数，但这个函数不会对模型的参数有连续的依赖：它会随着分类器行为的改变而出现“离散跳跃”。因此，我们需要寻找其他损失函数，这些函数确实是连续且可微的，同时在分类问题中测量损失的方式既合理又自然。
- en: Another somewhat naive yet much better choice would be to take the **mean**
    **squared error** as our loss function. For the purposes of our problem, we know
    that the neural network returns a continuous value between ![0](img/file12.png
    "0") and ![1](img/file13.png "1"), and we know that — ideally — the closer this
    value is to ![0](img/file12.png "0") or ![1](img/file13.png "1"), the more likely
    it corresponds to a negative or positive input respectively. In order to do the
    classification, we set a threshold at ![\left. 1\slash 2 \right.](img/file136.png
    "\left. 1\slash 2 \right.") and get a discrete label, but, in order to compute
    the loss function, we should actually look at that continuous output! In this
    way, if we let ![M_{\theta}(x)](img/file1102.png "M_{\theta}(x)") be the continuous
    value in ![\lbrack 0,1\rbrack](img/file1145.png "\lbrack 0,1\rbrack") returned
    by the model for a given input ![x](img/file269.png "x"), and we let ![y \in \{
    0,1\}](img/file1146.png "y \in \{ 0,1\}") be its corresponding label, we could
    take our loss function to be
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相对简单但更好的选择是将**均方误差**作为我们的损失函数。对于我们的问题，我们知道神经网络返回一个介于![0](img/file12.png "0")和![1](img/file13.png
    "1")之间的连续值，而且我们知道——理想情况下——这个值越接近![0](img/file12.png "0")或![1](img/file13.png "1")，它就越可能分别对应于负或正输入。为了进行分类，我们设置一个阈值为![\left.
    1\slash 2 \right.](img/file136.png "\left. 1\slash 2 \right.")并得到一个离散标签，但是，为了计算损失函数，我们实际上应该查看那个连续输出！这样，如果我们让![M_{\theta}(x)](img/file1102.png
    "M_{\theta}(x)")是模型对于给定输入![x](img/file269.png "x")返回的![\lbrack 0,1\rbrack](img/file1145.png
    "\lbrack 0,1\rbrack")中的连续值，并且我们让![y \in \{ 0,1\}](img/file1146.png "y \in \{ 0,1\}")是其对应的标签，我们可以将我们的损失函数取为
- en: '| ![L(\theta;x,y) = \left( {M_{\theta}(x) - y} \right)^{2},](img/file1147.png
    "L(\theta;x,y) = \left( {M_{\theta}(x) - y} \right)^{2},") |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| ![L(\theta;x,y) = \left( {M_{\theta}(x) - y} \right)^{2},](img/file1147.png
    "L(\theta;x,y) = \left( {M_{\theta}(x) - y} \right)^{2},") |'
- en: where we have grouped in ![\theta](img/file89.png "\theta") all the parameters
    (weights and biases) on which our neural network ![M](img/file704.png "M") depends.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将所有依赖于我们的神经网络![M](img/file704.png "M")的参数（权重和偏差）分组在![\theta](img/file89.png
    "\theta")中。
- en: Of course, in order to compute the training loss (the expected value over the
    training dataset), we would just take the average value over the training dataset,
    and analogously for the validation loss. This is usually called the **mean squared
    error** (**MSE**) because, well, it is the average of the error squared.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了计算训练损失（在训练数据集上的期望值），我们只需在训练数据集上取平均值，对于验证损失也是如此。这通常被称为**均方误差**（**MSE**），因为，嗯，它是误差平方的平均值。
- en: 'The MSE is a good loss function, but when it comes to binary classifiers, there
    is actually an even better candidate: the **binary cross-entropy**. It is computed
    as'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差是一个好的损失函数，但当涉及到二元分类器时，实际上有一个更好的候选者：**二元交叉熵**。它是这样计算的
- en: '| ![H(\theta;x,y) = - y\log\left( {M_{\theta}(x)} \right) - (1 - y)\log\left(
    {1 - M_{\theta}(x)} \right).](img/file1148.png "H(\theta;x,y) = - y\log\left(
    {M_{\theta}(x)} \right) - (1 - y)\log\left( {1 - M_{\theta}(x)} \right).") |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ![H(\theta;x,y) = - y\log\left( {M_{\theta}(x)} \right) - (1 - y)\log\left(
    {1 - M_{\theta}(x)} \right).](img/file1148.png "H(\theta;x,y) = - y\log\left(
    {M_{\theta}(x)} \right) - (1 - y)\log\left( {1 - M_{\theta}(x)} \right).") |'
- en: Now, this may seem like a very complicated expression, but it is actually a
    very elegant and powerful loss function! For starters, if the output of the model
    is differentiable and continuous with respect to its trainable parameters, so
    is the loss (that is easy to check, just go back to Calculus 101). And that’s
    not all. The following exercise may help you realize why the binary cross-entropy
    function is a great choice function for binary classifiers.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个表达式可能看起来非常复杂，但实际上它是一个非常优雅且强大的损失函数！首先，如果模型的输出相对于其可训练参数是可微的和连续的，那么损失也是（这很容易检查，只需回到微积分101即可）。而且不仅如此。以下练习可能有助于你意识到为什么二元交叉熵函数是二元分类器的优秀选择函数。
- en: Exercise 8.4
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.4
- en: Show that the output of the binary cross-entropy loss function ![H(\theta;x,y)](img/file1149.png
    "H(\theta;x,y)") is ![0](img/file12.png "0") if ![M_{\theta}(x) = y](img/file1150.png
    "M_{\theta}(x) = y") and that it diverges to ![\infty](img/file1151.png "\infty")
    as ![M_{\theta}(x)](img/file1102.png "M_{\theta}(x)") approaches the opposite
    label to ![y](img/file270.png "y") (this is, as ![\left. M_{\theta}(x)\rightarrow
    1 \right.](img/file1152.png "\left. M_{\theta}(x)\rightarrow 1 \right.") if ![y_{i}
    = 0](img/file1153.png "y_{i} = 0") and as ![\left. M(x)\rightarrow 0 \right.](img/file1154.png
    "\left. M(x)\rightarrow 0 \right.") if ![y = 1](img/file769.png "y = 1")).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 证明二元交叉熵损失函数 ![H(\theta;x,y)](img/file1149.png "H(\theta;x,y)") 的输出在 ![M_{\theta}(x)
    = y](img/file1150.png "M_{\theta}(x) = y") 时为 ![0](img/file12.png "0")，并且当 ![M_{\theta}(x)](img/file1102.png
    "M_{\theta}(x)") 接近 ![y](img/file270.png "y") 的相反标签时发散到 ![\infty](img/file1151.png
    "\infty")（这意味着，如果 ![y_{i} = 0](img/file1153.png "y_{i} = 0")，则 ![left. M_{\theta}(x)\rightarrow
    1 \right.](img/file1152.png "\left. M_{\theta}(x)\rightarrow 1 \right.")；如果 ![y
    = 1](img/file769.png "y = 1")，则 ![left. M(x)\rightarrow 0 \right.](img/file1154.png
    "\left. M(x)\rightarrow 0 \right."））。
- en: And, with this, our shiny new loss function is ready to be used. However, there
    is one last element we still have to take care of, one that we have so far neglected
    and ignored. Yes, in the following section, we shall give optimization algorithms
    the attention and care that they deserve!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们这个闪亮的新损失函数已经准备好使用。然而，我们还有一个最后的元素需要处理，这是我们迄今为止忽视和忽略的。是的，在接下来的章节中，我们将给予优化算法它们应得的关注和照顾！
- en: 8.2.3 Gradient descent
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2.3 梯度下降
- en: You are now reading this book, probably in the comfort of your home, college
    library, or office. But life changes in the most unexpected ways, and, maybe,
    in a couple of weeks, you will find yourself at the top of a mountain, blindfolded
    (don’t ask us why) and tasked with the mission of reaching the bottom of a nearby
    valley. If this happened, what would you do?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可能正在家中、大学图书馆或办公室舒适地阅读这本书。但是生活以最意想不到的方式改变，也许在几周后，你会发现自己在山顶上，蒙着眼睛（不要问我们为什么），被赋予了到达附近山谷底部的任务。如果这种情况发生了，你会怎么做？
- en: You don’t have to be a survival expert to accomplish this task. It’s true that
    — for undisclosed reasons — you are blindfolded, so you can’t see where the valley
    is, but, hey, you can still move around, can’t you? So, you could take some small
    steps in whichever direction you feel is leading you downwards with the highest
    slope. And you could just repeat that process several times and, eventually, you
    would reach the bottom of a valley.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要成为生存专家就能完成这个任务。诚然——由于未公开的原因——你被蒙上了眼睛，所以你看不见山谷在哪里，但，嘿，你仍然可以四处移动，不是吗？所以，你可以朝着你认为能让你向下移动的最高斜率的方向迈出一些小步。你可以重复这个过程几次，最终你会到达山谷的底部。
- en: Of course, as you descend, you will have to be careful with how big your steps
    are. Take steps that are too big, and you may go from the top of a mountain to
    the top of another one, skipping all the valleys in between (some medical doctors
    have suggested this might not be anatomically possible, but, well, you get what
    we mean). On the other hand, make your steps too small and it is going to take
    you forever to reach the valley. So, you will have to find a sweet spot!
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在你下降的过程中，你必须小心你的步子有多大。步子太大，你可能会从一个山峰跳到另一个山峰的顶端，中间的所有山谷都跳过了（一些医生建议这可能从解剖学上是不可能的，但，嗯，你明白我们的意思）。另一方面，如果你的步子太小，你可能永远也到不了山谷。所以，你必须找到一个合适的平衡点！
- en: Anyhow, how does this seemingly crazy thought experiment relate to machine learning?
    Let’s see.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样，这个看似疯狂的思维实验如何与机器学习相关呢？让我们看看。
- en: '**Gradient descent algorithms** We now have a powerful-enough model that depends
    on some parameters. Moreover, since we have made wise life choices, we also have
    a loss function ![L](img/file1012.png "L") that depends continuously on and is
    differentiable with respect to these parameters (that is because we picked some
    smooth activation functions and the binary cross-entropy).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降算法** 我们现在有一个足够强大的模型，它依赖于一些参数。此外，由于我们做出了明智的生活选择，我们还拥有一个损失函数 ![L](img/file1012.png
    "L")，它连续依赖于这些参数，并且对这些参数是可微的（这是因为我们选择了一些平滑的激活函数和二元交叉熵）。'
- en: 'By doing this, we have effectively reduced our machine learning problem to
    the problem of minimizing a loss function, which is a differentiable function
    on some variables (the trainable parameters). And how do we do this? Using the
    Force…Sorry, we got carried away. We meant: using calculus!'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: The ”getting to the valley” problem that we discussed before is — as you may
    have very well guessed by now — a simple analogy that will help us illustrate
    the **gradient descent method**. This method is just an algorithm that will allow
    us to minimize a differentiable function, and we can think of it as the mathematical
    equivalent of taking small steps in the steepest downward direction on a mountain.
    We should warn you that the remaining content of this subsection might be somewhat
    dense. Please, don’t let technicalities overwhelm you. If this were a song, it’d
    be perfectly fine not to know its lyrics; all that would matter is for you to
    be familiar with its rhythm!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: As you may remember from the sweet old days of undergraduate calculus, whenever
    you have a differentiable function ![\left. f:R^{N}\rightarrow R \right.](img/file1155.png
    "\left. f:R^{N}\rightarrow R \right.") (for those of you less familiar with mathematical
    notation, this is a fancy way of saying that ![f](img/file778.png "f") has ![N](img/file784.png
    "N") real-number inputs and returns a single real-number output), the direction
    in which it decreases more steeply at a point ![x](img/file269.png "x") is given
    by ![- \nabla f(x)](img/file1156.png "- \nabla f(x)"), where ![\nabla f(x)](img/file1157.png
    "\nabla f(x)") is the **gradient** **vector** at ![x](img/file269.png "x"), and
    is computed as
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\nabla f(x) = \left( {\left. \frac{\partial f}{\partial x_{1}} \right&#124;_{x},\ldots,\left.
    \frac{\partial f}{\partial x_{n}} \right&#124;_{x}} \right),](img/file1158.png
    "\nabla f(x) = \left( {\left. \frac{\partial f}{\partial x_{1}} \right&#124;_{x},\ldots,\left.
    \frac{\partial f}{\partial x_{n}} \right&#124;_{x}} \right),") |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: where ![\left. \partial\slash\partial x_{i} \right.](img/file1159.png "\left.
    \partial\slash\partial x_{i} \right.") denotes the partial derivative operator
    with respect to a variable ![x_{i}](img/file714.png "x_{i}"). So, if we want to
    move towards a minimum at a given point, we will have to move in the direction
    of ![- \nabla f(x)](img/file1156.png "- \nabla f(x)"), but by what amount?
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical equivalent of the size of a step is going to be a parameter
    ![\tau](img/file1160.png "\tau") known as the **learning rate**. And, in this
    way, given a learning rate ![\tau](img/file1160.png "\tau") and an initial configuration
    ![\theta_{0}](img/file1045.png "\theta_{0}") of the parameters of our model, we
    can try to find the parameters that minimize the loss function ![L](img/file1012.png
    "L") by computing, iteratively, new parameters according to the rule
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\theta_{k + 1} = \theta_{k} - \tau\nabla L(\theta_{k}).](img/file1161.png
    "\theta_{k + 1} = \theta_{k} - \tau\nabla L(\theta_{k}).") |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| ![θ_{k + 1} = θ_{k} - τ∇L(θ_{k}).](img/file1161.png "θ_{k + 1} = θ_{k} -
    τ∇L(θ_{k}).") |'
- en: There are some algorithms that dynamically adjust this step size from an initial
    learning rate as the optimization progresses. One such algorithm is **Adam** (short
    for **Adaptive Moment Estimator**), which is one of the best gradient descents
    algorithms out there; it will actually be our go-to choice.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有些算法会随着优化过程的进行动态调整这个步长大小，从初始学习率开始。其中一种算法是**Adam**（即**自适应矩估计器**），这是目前最好的梯度下降算法之一；它实际上将成为我们的首选选择。
- en: Important note
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is important to pick the learning rate wisely. If it is too small, the training
    will be very slow. If it is too large, you may find yourself taking huge strides
    that jump whole valleys, and the training may never be successful.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 智能地选择学习率非常重要。如果它太小，训练将会非常缓慢。如果太大，你可能会发现自己迈出巨大的步伐，跳过整个山谷，训练可能永远不会成功。
- en: Of course, in order for gradient descent algorithms to work, you need to be
    able to compute the gradient of the loss function. There are several ways to do
    this; for example, you could always estimate gradients numerically. But, when
    working with certain models such as neural networks, you can employ a technique
    known as **backpropagation**, which enables the efficient computation of exact
    gradients. You may learn more about the technical details in Geron’s exceptional
    book [[104](ch030.xhtml#Xhandsonml), Chapter 10].
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了让梯度下降算法能够工作，你需要能够计算损失函数的梯度。有几种方法可以做到这一点；例如，你总是可以数值估计梯度。但是，当处理某些模型，如神经网络时，你可以采用一种称为**反向传播**的技术，它能够高效地计算精确梯度。你可以在Gerón的杰出著作中了解更多关于技术细节[[104](ch030.xhtml#Xhandsonml)，第10章]]。
- en: To learn more…
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多...
- en: The method of backpropagation has been one of the key developments leading to
    the great success of deep learning that we are experiencing today. Although this
    technique was already known in the 1960s, it was popularized for training neural
    networks by the work of Geoffrey Hinton and his collaborators. Hinton, together
    with Yoshua Bengio, Demis Hassabis, and Yann LeCun, received the 2022 Princess
    of Asturias Award for Technical and Scientific Research for outstanding work in
    the field of neural networks. You can learn a lot about the inception of backpropagation
    and about the history of neural networks research by reading the excellent *Architects
    of Intelligence*, in which Martin Ford interviews Bengio, Hassabis, Hinton, LeCun,
    and many other prominent figures in artificial intelligence [[40](ch030.xhtml#Xford2018architects)].
    By the way, Demis Hassabis is, in great part, responsible for the success of AlphaGo,
    one of the examples of reinforcement learning that we mentioned earlier in this
    chapter.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的方法一直是导致我们今天所经历的深度学习巨大成功的关键发展之一。尽管这种技术在20世纪60年代就已经为人所知，但它是由Geoffrey Hinton及其合作者推广用于训练神经网络的。Hinton与Yoshua
    Bengio、Demis Hassabis和Yann LeCun一起，因在神经网络领域的杰出工作而获得了2022年阿斯图里亚斯公主技术科学奖。通过阅读优秀的《智能建筑师》，你可以了解反向传播的起源以及神经网络研究的历史，这本书中Martin
    Ford采访了Bengio、Hassabis、Hinton、LeCun以及其他许多人工智能领域的杰出人物[[40](ch030.xhtml#Xford2018architects)]。顺便说一句，Demis
    Hassabis在很大程度上是AlphaGo成功的原因之一，这是我们本章前面提到的强化学习的一个例子。
- en: '**Mini-batch gradient descent** When the training dataset is large, computing
    the gradient of the loss function — as a function of the optimizable parameters
    of the model — can slow down the training significantly. In order to speed up
    the training, you can resort to the technique of **mini-batch gradient descent**.
    With this optimization method, the training dataset is split into batches of a
    fixed **batch size**. The gradient of the loss function is then computed on each
    of these batches, and the results are used to approximate the gradient of the
    global loss function: this is, the loss function on the whole training dataset.
    When we use this technique, we need to be careful with the batch size that we
    use: make it too small, the training will be very unstable; make it too large,
    the training will be too slow. As with the learning rate, it’s all a matter of
    finding an equilibrium! However, in some cases, speed is of the essence, and we
    go to the extreme, using batches of just one input. This is called **stochastic
    gradient descent**. On the other hand, when the batch includes all the elements
    in the dataset, we say that we are using **batch gradient** **descent**.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Now we do have all that we need to train our first model. We have a dataset,
    we know what our model should look like, we have picked a loss function and we
    know how to optimize it. So let’s make this work! For this, we will use TensorFlow
    and scikit-learn.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.4 Getting in the (Tensor)Flow
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We already have our dataset ready, and we could split it manually into training,
    validation, and test datasets, but there are already some good-quality machine
    learning packages with functions that help you do that. One of these packages
    is `sklearn`, which implements a `train_test_split` function. It splits a dataset
    into a training and test dataset (it doesn’t return a validation dataset, but
    we can work our way around that). It does so by taking as arguments the dataset
    and the labels array; in addition, it has some optional arguments to specify whether
    the dataset should be shuffled and the proportions in which the dataset should
    be split. In order to get a training, validation, and test dataset with proportions
    ![0.8](img/file1162.png "0.8"), ![0.1](img/file1163.png "0.1"), and ![0.1](img/file1163.png
    "0.1") respectively, we just need to use this function twice: once to get a training
    dataset (size ![0.8](img/file1162.png "0.8")) and a test dataset (size ![0.2](img/file1091.png
    "0.2")), and once more to split the test dataset in half, yielding a validation
    dataset and a test dataset of relative size ![0.1](img/file1163.png "0.1") each.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Following convention, we will denote the datasets as variables `x` and the
    labels as variables `y`. In this way, we can run the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Notice how the function returns four arrays in the following order: the data
    for the training dataset, the data for the test dataset, the labels for the training
    dataset, and the labels for the test dataset. One important thing about the `train_test_split`
    function is that it can use **stratification**. If we had also provided the arguments
    `stratify` `=` `labels` and `stratify` `=` `y_test`, this would have meant that,
    when splitting the data into training and test examples, it would have kept the
    exact proportion of positive and negative classes from the original data (or at
    least as close to exact as possible). This can be important, especially if we
    are working with unbalanced datasets in which one class is much more abundant
    than the other. If we are not careful, we could end up with a dataset in which
    the minority class is non-existent.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the data is perfectly prepared, it is time for us to focus on the
    model. For our problem, we are going to use a neural network with the following
    components:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: An input layer with two inputs
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three intermediate (also known as **hidden**) layers with ELU activation functions
    and with ![8](img/file506.png "8"), ![16](img/file619.png "16"), and ![8](img/file506.png
    "8") neurons respectively
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output layer with a single neuron that uses the sigmoid activation function
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s now try to digest this specification a little bit. Because of the nature
    of the problem, we know that our model needs two inputs and one output, hence
    the sizes of the input and output layers. What is more, we want to get an output
    normalized between ![0](img/file12.png "0") and ![1](img/file13.png "1"), so it
    makes sense to use the sigmoid activation function in the output layer. Now, we
    need to find a way to get from ![2](img/file302.png "2") neurons in the first
    layer to ![1](img/file13.png "1") neuron in the output layer. We could use hidden
    layers with ![2](img/file302.png "2") or ![1](img/file13.png "1") layers…but that
    wouldn’t yield a very powerful neural network. Thus, we have progressively scaled
    the size of the neural network: first going from ![2](img/file302.png "2") to
    ![8](img/file506.png "8"), then from ![8](img/file506.png "8") to ![16](img/file619.png
    "16"), then down from ![16](img/file619.png "16") to ![8](img/file506.png "8"),
    to finally reach the output layer with 1 neuron.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: How do we define such a model in TensorFlow? Well, after doing the necessary
    imports and setting a seed (remember that it is an important part if we want this
    to be reproducible!), all it takes is to define what is known as a **Keras** **sequential
    model**.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is pretty self-explanatory:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And that is how we can create our model, storing it as an object of the `Sequential`
    class.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Once you have defined a Keras `model`, like the sequential model that we have
    just considered, you can print a visual summary of it by running the instruction
    `print``(``model``.``summary``())`. This summary lists all the layers of the model
    together with their shape, and also displays a count of all the model parameters.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can train this model, we will need to **compile it**, associating
    it with an optimization algorithm and a loss function. This is done by calling
    the `compile` method and giving it the arguments `optimizer` and `loss`. In our
    case, we seek to use the Adam optimizer (just with its default parameters) and
    the binary cross entropy loss function. We can thus compile our model as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When we instantiate the Adam optimizer without providing any arguments, the
    learning rate is set, by default, to ![10^{- 3}](img/file1164.png "10^{- 3}").
    We may change this value — and we will very often do! — by setting a value for
    the optional argument `learning_rate`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.5 Training the model
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we are ready to train our model. This will be done by calling the `fit`
    method. But before we do that, let’s explore in some detail the most important
    arguments that we have to and can pass to this method:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The first argument that `fit` admits is the dataset `x`. It should be an array
    containing the inputs that need to be passed to the model in order to train it.
    In our case, that would be `x_tr`.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second argument that we can send is the array of labels `y`. Of course,
    the dimensions of `x` and `y` need to match. In our case, we will set `y` to be
    `y_tr`.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are using an optimizer that relies on gradient descent, you may want
    to resort to mini-batch gradient descent. For this purpose, you can give an integer
    value to the `batch_size` argument, which defaults to ![32](img/file771.png "32")
    (thus, by default, mini-batch gradient descent is used). If you do not want to
    use mini-batch gradient descent, you should set `batch_size` to `None`; that is
    what we will do.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we discussed gradient descent, we saw how these gradient descent algorithms
    are **iterative**: they work by computing a sequence of points that, in principle,
    should converge to a (local) minimum. But this raises the question of how many
    optimization cycles the algorithm should make — how many such points in the sequence
    it should compute. You may fix how many steps, also known as **epochs**, you want
    the optimization algorithm to take. This is done by setting a value for the `epochs`
    argument, which defaults to ![1](img/file13.png "1"). In our case, we will use
    ![8](img/file506.png "8") epochs.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want to use some validation data, as it is our case, we can pass it through
    the `validation_data` argument. The value of this argument should be a tuple with
    the validation dataset in the first entry and the corresponding labels in the
    second one. Thus, in our case, we would set `validation_data` to `(``x_val``,`
    `y_val` `)`.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may have noticed that the whole process of extracting a training, validation,
    and test dataset can be somewhat tiresome. Well, it turns out that TensorFlow
    can help out here. In principle, we could just have given TensorFlow a dataset
    with both the training and validation data and told it in which proportions they
    should be split by setting a value in the `validation_split` argument. This value
    must be a float between ![0](img/file12.png "0") and ![1](img/file13.png "1")
    representing the proportion of the training dataset that should be used for validation.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By doing this, we would save ourselves a ”split”, but we would still have to
    extract a test dataset on our own.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: We have only covered some of the possibilities offered by TensorFlow — the ones
    that we will use most often. If you feel comfortable enough with the material
    that we have seen so far and want to explore TensorFlow in depth, you should check
    out the documentation ([https://www.tensorflow.org/api_docs/python/tf](https://www.tensorflow.org/api_docs/python/tf)).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'The way we will then train our model will be the following:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And, upon executing this instruction on an interactive shell, we will get the
    following output:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When seeing this, the first thing we should do is comparing the training loss
    with the validation loss — just to stay away from overfitting! In our case, we
    see that these two are close enough and have evolved following similar decreasing
    trends during the training. That is indeed a good sign!
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed how we have saved the output of the `fit` method in an
    object that we have called `history` in which TensorFlow will store information
    about the training. For example, the training and validation losses at the end
    of each epoch is recorded in a dictionary that we could access as `history``.``history`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 8.5
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Plot on a single graph the evolution of the training and validation losses through
    the epochs, relying on the information contained in the `history` object.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we have manually set the number of epochs to ![8](img/file506.png
    "8"), but this is not always the best strategy. Ideally, we would like to fix
    a maximum number of epochs that is reasonably large, but we would want the training
    to stop as soon as the loss is not improving. This is known as **early stopping**,
    and it can be easily used in TensorFlow.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use early stopping in TensorFlow, we first need to create an `EarlyStopping`
    object in which we specify how we want early stopping to behave. Let’s say that
    we want to train our model until, for three consecutive epochs, the validation
    loss doesn’t decrease more than ![0.001](img/file1165.png "0.001") after each
    epoch. To do this, we would have to invoke the following object:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: And then, when calling the `fit` method, we would just have to pass the optional
    argument `callbacks` `=` `[``early_stp``]`. It’s as easy as that!
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case, now we have trained our model. If we want our model to process
    any inputs, we can use the `predict` method, passing an array with any number
    of valid inputs. For example, in our case, if we wanted to get the output of the
    model on the test dataset, we could retrieve `model``.``predict``(``x_test``)`.
    However, this will give us the continuous values returned by the model (which
    will range from ![0](img/file12.png "0") to ![1](img/file13.png "1")), not a label!
    In order to get a discrete label (![0](img/file12.png "0") or ![1](img/file13.png
    "1")), we need to set a threshold. Naturally, we will set it to ![0.5](img/file1166.png
    "0.5"). Thus, if we want to get the labels that our model would predict, we would
    have to run the following:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Of course, now we have to decide whether or not this training has been successful,
    so we should assess the performance of our model on the test dataset. In order
    to do this, we may simply compute the **accuracy** of our model on the test dataset,
    that is, we may compute the proportion of inputs in the test dataset that are
    correctly classified by our model.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do this, we can use the `accuracy_score` function from `sklearn``.``metrics`:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In our case, we got ![89.2\%](img/file1167.png "89.2\%") accuracy. This seems
    like a pretty decent value, but we should always consider accuracy values in the
    context of each problem. For some tasks, ![89.2\%](img/file1167.png "89.2\%")
    can indeed be marvelous, but for others it can be simply disappointing. Imagine,
    for instance, that you have a problem in which ![99\%](img/file1168.png "99\%")
    of the examples belong to one class. Then, it is trivial to obtain at least ![99\%](img/file1168.png
    "99\%") accuracy! You just need to classify all the inputs as belonging to the
    majority class. In the next few pages, we will introduce tools to take this kind
    of situation into account and better quantify classification performance.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 8.6
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Re-train the model under the following conditions and compute the accuracy
    of the resulting model:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the learning rate to ![10^{- 6}](img/file1169.png "10^{- 6}")
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the learning rate to ![10^{- 6}](img/file1169.png "10^{- 6}") and increasing
    the number of epochs to ![1,000](img/file1170.png "1,000")
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the size of the training dataset to ![20](img/file588.png "20")
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In which cases is the resulting model less accurate? Why?
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Does overfitting occur in any of these scenarios? How could you identify it?
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have assessed the accuracy of our model just by measuring the proportion
    of elements that it would correctly classify by setting a threshold of ![0.5](img/file1166.png
    "0.5"). There are nevertheless other metrics of the performance of a binary classifier.
    We will study them in the next subsection!
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.6 Binary classifier performance
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Whenever you have a binary classifier, any output can belong to one of the
    four categories depicted in the following table:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Classified as positive | Classified as negative |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| Actual positive | True positive | False negative |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| Actual negative | False positive | True negative |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: The abbreviations TP, FN, FP, and TN are also used to denote the number of true
    positives, false negatives, false positives, and true negatives (respectively)
    produced by a classifier over a given dataset. These quantities are used very
    often. In fact, a common way of assessing the performance of a classifier is by
    looking at its **confusion matrix** (usually over the test dataset), which is
    nothing more than the matrix
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\begin{pmatrix} \text{TP} & \text{FN} \\ \text{FP} & \text{TN} \\ \end{pmatrix}.](img/file1171.png
    "\begin{pmatrix} \text{TP} & \text{FN} \\ \text{FP} & \text{TN} \\ \end{pmatrix}.")
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: 'To get started, we can now compute the confusion matrix for the binary classifier
    that we have just trained over the test dataset. For this, we can use the `confusion_matrix`
    function from `sklearn``.``metrics`, which requires two arguments: an array of
    predicted labels and an array of true labels:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Upon executing this piece of code, we get the following confusion matrix for
    our classifier:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\begin{pmatrix} {24} & {20} \\ 7 & {199} \\ \end{pmatrix}.](img/file1172.png
    "\begin{pmatrix} {24} & {20} \\ 7 & {199} \\ \end{pmatrix}.") |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: This matrix shows that there are very few false positives compared to the number
    of true negatives, but almost as many false negatives as true positives. This
    means that our classifier does a very good job of picking up the negative class
    but it is not so good at identifying the positive one. In a moment, we will discuss
    how to quantify this more precisely.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Although we have focused just on binary classifiers, confusion matrices can
    also be defined for classification problems in which there are ![n](img/file244.png
    "n") classes. They have ![n](img/file244.png "n") rows and ![n](img/file244.png
    "n") columns, and the entry in row ![k](img/file317.png "k") column ![l](img/file514.png
    "l") represents the number of elements that actually belong to class ![k](img/file317.png
    "k") but that are labeled as class ![l](img/file514.png "l") by the system.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, if you fix one of the ![n](img/file244.png "n") classes as the
    positive one and consider the rest as negative, you can obtain TP, FP, TN, and
    FN for that particular class.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrices are very informative, and the quantities in them can help
    us define several metrics of the performance of a binary classifier. For instance,
    the usual accuracy metric can be defined by
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\text{Acc} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP}
    + \text{FN}}.](img/file1173.png "\text{Acc} = \frac{\text{TP} + \text{TN}}{\text{TP}
    + \text{TN} + \text{FP} + \text{FN}}.") |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: Other interesting metrics are the **positive predictive value** and the **sensitivity**,
    which are defined respectively as
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '| ![P = \frac{\text{TP}}{\text{TP} + \text{FP}},\qquad S = \frac{\text{TP}}{\text{TP}
    + \text{FN}}.](img/file1174.png "P = \frac{\text{TP}}{\text{TP} + \text{FP}},\qquad
    S = \frac{\text{TP}}{\text{TP} + \text{FN}}.") |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: The positive predictive value is also known as the **precision** and the sensitivity
    is also known as the **recall** of the classifier.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a trade-off between ![P](img/file1.png "P") and ![S](img/file73.png
    "S"). Obtaining a perfect recall is trivial: you just need to classify every input
    as positive. But then, you will have a low precision. Similarly, it is easy to
    obtain very good values of precision: only classify an example as positive if
    you are extremely sure that it is positive. But then the recall will be very low.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, an interesting metric is the ![F_{1}](img/file1175.png "F_{1}")
    score, defined as the harmonic mean of ![P](img/file1.png "P") and ![S](img/file73.png
    "S"):'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '| ![F_{1} = \frac{2}{\frac{1}{P} + \frac{1}{S}} = \frac{2PS}{P + S}.](img/file1176.png
    "F_{1} = \frac{2}{\frac{1}{P} + \frac{1}{S}} = \frac{2PS}{P + S}.") |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: It is easy to see how this score can range from ![0](img/file12.png "0") (the
    score of the worst possible classifier) to ![1](img/file13.png "1") (the score
    of a perfect classifier). Moreover, a high ![F_{1}](img/file1175.png "F_{1}")
    score means that we are not favoring recall over precision or precision over recall.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: If you are mathematically oriented, you may have realized that our expression
    for ![F_{1}](img/file1175.png "F_{1}") is actually undefined for ![P = S = 0](img/file1177.png
    "P = S = 0"), but we can trivially extend it by continuity to take the value ![F_{1}
    = 0](img/file1178.png "F_{1} = 0") there.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to compute these metrics, we may use the `classification_report` function
    from `sklearn``.``metrics`. In our case, we may run the following:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This yields the following output:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And in this table, we can see all the metrics that we have mentioned. You can
    see that the scores are returned for both the case in which ![0](img/file12.png
    "0") is the positive class and for the case when ![1](img/file13.png "1") is positive
    instead (in our case, we have considered ![0](img/file12.png "0") to be positive,
    so we would look at the first row). By the way, the **support** of a class is
    meant to represent the number of elements in the class that can be found in the
    dataset. Also, the **macro average** of each metric is just the plain average
    of the values of the metric obtained by taking each class as positive. The weighted
    average is like the macro average, but weighted by the proportion of elements
    of each class in the dataset.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that we have a binary classifier that returns a continuous output
    between ![0](img/file12.png "0") and ![1](img/file13.png "1") before cutting through
    a threshold in order to assign a label. As we saw earlier, we could just measure
    the performance of our classifier by using a bunch of metrics. But if we want
    to get a broader perspective of how our classifier could work for any threshold,
    we can take another approach.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Using the entries of the confusion matrix over a dataset, we may define the
    **true positive rate** as the proportion
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}},](img/file1179.png
    "\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}},") |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: that is, the proportion of examples from the positive class that are actually
    classified as positive. On the other hand, we can analogously define the **false**
    **positive rate** as the quotient
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}.](img/file1180.png
    "\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}.") |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: 'The **Receiver Operating Characteristic curve** or **ROC curve** of a classifier
    that returns continuous values is computed over a given dataset by plotting, for
    every possible choice of threshold, a point with a ![Y](img/file11.png "Y") coordinate
    given by the corresponding TPR and an ![X](img/file9.png "X") coordinate with
    the FPR for that threshold. As the threshold increases from ![0](img/file12.png
    "0") to ![1](img/file13.png "1"), this will give rise to a finite sequence of
    points. The curve is obtained by joining these through straight lines. Notice
    that we evaluate the performance of the classifier with different levels of ”demand”
    for classifying an input as positive. When the threshold is high, it will be harder
    to classify something as positive; the FPR will be low — great! — but the TPR
    will probably be also low. On the other hand, for low values of the threshold,
    it will be easier for an input to be classified as positive: the TPR will be high
    — yay! — but that can also cause the false positives to go up.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Sounds familiar? This is the same kind of trade-off that we discussed when we
    defined precision and recall. The difference is that, in this case, we are taking
    into account the behavior of the classifier for every possible choice of threshold,
    giving us a global assessment. Plotting the ROC curve can be very informative
    because it can also help in selecting classification thresholds that are more
    suitable for our problem. For instance, if you are trying to detect whether a
    given patient has a certain serious illness, it may pay off to have some false
    positives — people that may need to undergo additional medical tests — at the
    cost of having very low false negatives. The ROC curve can help you there by identifying
    points at which the TPR is high and the FPR is acceptable.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to plot a ROC curve, we can use the `roc_curve` function from `sklearn``.``metrics`.
    It will return the ![X](img/file9.png "X") and ![Y](img/file11.png "Y") coordinates
    of the points of the curve. In our particular case, we may run the following piece
    of code:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice how we have dropped part of the output of the `roc_curve` function; in
    particular, the return object that we ignore yields an array that includes the
    thresholds at which the classifier accuracy changes (you can refer to the documentation
    at [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)
    for more information). The output that we got can be found in *Figure* [*8.5*](#Figure8.5).
    Notice that we have manually drawn a dashed line between ![(0,0)](img/file613.png
    "(0,0)") and ![(1,1)](img/file1181.png "(1,1)"). That is meant to represent the
    ROC curve that could be generated by a random classifier, one that assigns an
    input to a class with probability proportional to the size of that class, and
    it is an important visual aid. That’s because any curves above that dashed line
    are ROC curves of classifiers that have some real classification power.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: ROC curve (solid line) for the classifier that we have trained.](img/file1182.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: '**Figure 8.5**: ROC curve (solid line) for the classifier that we have trained.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: There are some interesting features in this ROC curve, so let’s discuss it a
    little bit. To start with, notice that the points ![(0,0)](img/file613.png "(0,0)")
    and ![(1,1)](img/file1181.png "(1,1)") always belong to the ROC curve of any classifier
    because they are achieved with the highest and lowest thresholds, respectively.
    In the first case, no input is assigned to the positive class, so we have neither
    TPs nor FPs. In the second one, all inputs are assigned to the positive class,
    so we have neither FNs nor TNs.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, we can observe in our graph that, from ![(0,0)](img/file613.png
    "(0,0)"), the ROC curve starts moving horizontally, increasing the FPR without
    increasing the TPR. This means that there are some examples in the test dataset
    that the model very confidently classifies as belonging to the positive class
    but that, in fact, are negative. This is undesirable, of course. We would like
    our ROC curve to go up — increasing the TPR — without moving to the right. And
    that is exactly what happens after that first hiccup. We observe a long segment
    in which the TPR goes up without any increase in the FPR. If we need our classifier
    to have high precision, we could select the threshold that achieves TPR of about
    ![0.71](img/file1183.png "0.71") with FPR of only about ![0.02](img/file1184.png
    "0.02"). On the other hand, if we need high recall, we can select the point in
    the curve where the TPR is already ![1](img/file13.png "1") with a FPR of about
    ![0.5](img/file1166.png "0.5"). For a more balanced classifier, notice that there
    is a point in the ROC curve with TPR around ![0.91](img/file1185.png "0.91") and
    FPR below ![0.21](img/file1186.png "0.21").
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the ideal classifier would have a ROC curve that goes all the way
    from ![(0,0)](img/file613.png "(0,0)") to ![(1,0)](img/file1187.png "(1,0)").
    That would mean that there is a threshold for which all the positive examples
    are classified as positive, while no negative example is assigned to the positive
    class. That’s just perfection! From there, the ROC curve would go straight to
    ![(1,1)](img/file1181.png "(1,1)"): we have already found all the positive examples
    so the TPR cannot increase, but by decreasing the threshold we will eventually
    increase the FPR from ![0](img/file12.png "0") to ![1](img/file13.png "1").'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, that kind of perfect ROC curve is only achievable for extremely simple
    classification problems. However, we can still compare our actual model to that
    ideal classifier by computing the **area under the ROC curve**, often abbreviated
    as **AUC**. Since the ROC curve of the perfect classifier would have area equal
    to 1, we can consider that the closer the AUC of a classifier is to 1, the better
    its global performance is. In the same way, a random classifier would have an
    ROC curve that is a straight line from ![(0,0)](img/file613.png "(0,0)") to ![(1,1)](img/file1181.png
    "(1,1)"), so its AUC would be ![0.5](img/file1166.png "0.5"). Hence, classifiers
    whose AUC is higher than ![0.5](img/file1166.png "0.5") have some actual classification
    power beyond just random guessing.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Having the coordinates of the points that define the ROC curve, we can easily
    get the AUC score using the `auc` function from `sklearn``.``metrics`:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In our case, we get an AUC score of approximately ![0.9271](img/file1188.png
    "0.9271"). Again, this seems like a great value, but let us stress once again
    that it all depended on the difficulty of the problem — and the one we have been
    considering is not particularly hard. Also, remember that the AUC is a global
    performance metric that takes into account every possible threshold of your classifier.
    At the end of the day, you need to commit to just one threshold value, and a high
    AUC might not mean much if, for your particular threshold choice, the accuracy,
    precision, and recall are not that great.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: That was a lot of information! In any case, for most practical purposes, all
    that you will need to know is summarized in the following note.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Given a binary classifier with continuous output, we may compute its receiver
    operating characteristic curve (also known as the ROC curve) over a dataset. The
    higher the area under that curve, the higher the classifying power of the classifier.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'We refer to the area under the ROC curve of a classifier as its AUC (short
    for ”area under the curve”):'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: An AUC of ![1](img/file13.png "1") corresponds to a perfect classifier
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AUC of ![0.5](img/file1166.png "0.5") would match that of a random classifier
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AUC of ![0](img/file12.png "0") corresponds to a classifier that always returns
    the wrong output
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By now, we should have a decent understanding of (classical) machine learning,
    and you may be wondering where does the ”quantum” part begin? It begins now.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Quantum-classical models
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general terms, quantum machine learning refers to the application of machine
    learning techniques — only that quantum computing is involved at same stage of
    the process. Maybe you use a quantum computer in some part a model that you wish
    to train. Maybe you wish to use data generated by some quantum process. Maybe
    you use a quantum computer to process quantum-generated data. As you can imagine,
    the subject of quantum machine learning, as a whole, is broad enough to accommodate
    for a wide range of ideas and applications.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'In an attempt to categorize it all a little bit, we can follow the useful classification
    shown in Schuld’s and Petruccione’s book [[106](ch030.xhtml#Xschuld)] and divide
    quantum machine learning into four different flavors, which are depicted in *Figure*
    [*8.6*](#Figure8.6), according to the classical or quantum nature of the data
    and processing devices that are used:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: We could consider part of quantum machine learning all the quantum-inspired
    classical machine learning techniques; that is, all the classical machine learning
    methods that draw ideas from quantum computing. In this case, both the data and
    the computers are classical, but there is some quantum flavor involved in the
    process. This is represented as CC in the chart. Since there are no actual quantum
    computers involved in this approach, we will not study this kind of method.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, we can also consider part of quantum machine learning any classical
    machine learning algorithms that rely on quantum data; for our purposes, we can
    just think of it as data generated by quantum processes, or as the application
    of classical machine learning to quantum computing. This is the QC block in the
    chart. In this approach, machine learning is a tool rather than an end, so we
    will not be covering these techniques.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The kind of machine learning that we will focus on in this book is the one
    represented by the CQ label in the chart: machine learning that relies on classical
    data and uses quantum computing in the model or the training.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lastly, there is also a very interesting QQ category. These techniques work
    on quantum data using quantum computing in the models themselves or in the training
    processes. Notice that — as opposed to CQ quantum machine learning — in this scenario,
    the quantum data need not be obtained from measurements: quantum states could
    be directly fed into a quantum model, for instance. This is an area of great promise
    (see, for instance, the recent paper by Huang et al. [[54](ch030.xhtml#Xhuang2022quantum)]),
    but the required technologies are still immature, so we will not be talking about
    this approach in much detail.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.6: The four big families of quantum machine learning, categorized
    according to the nature of the models and data that they use ](img/file1189.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: '**Figure 8.6**: The four big families of quantum machine learning, categorized
    according to the nature of the models and data that they use'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'Our plan, then, is to focus on CQ quantum machine learning: machine learning
    on classical data that relies on quantum computing. Now, within this category,
    there is still a fairly broad range of possibilities. We could use quantum computing
    on the model and also in the optimization process. There are already many interesting
    proposals for how quantum computing could speed up traditional machine learning
    models, but these approaches cannot, in general, be used on our current quantum
    hardware. For this reason, we will not discuss them in this book — but if you
    are interested in learning more about them, we can recommend the excellent paper
    by Biamonte et al. [[108](ch030.xhtml#Xbiamonte-qml)].'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we will devote ourselves, heart and soul, to the study of fully quantum-oriented
    models that can be run on NISQ devices. These models will be trained on classical
    data and, in general, we will use purely classical optimization techniques.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following chapters, we will study the following models:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantum support vector machines**. We will soon explore what support vector
    machines are and how they can be trained using classical machine learning. We
    will also see how their quantum version is just a particular case of a general
    support vector machine in which we use quantum computers to map data into a space
    of quantum states.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantum neural networks**. We will then explore a purely quantum model: quantum
    neural networks. This model runs fully on a quantum computer, and its behavior
    is inspired by classical neural networks.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid networks**. In the subsequent chapter, we will learn how to combine
    quantum neural networks with other classical models (most commonly, neural networks).
    We will refer to these models as hybrid networks.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantum generative adversarial networks**. Lastly, we will study generative
    adversarial networks and cover how the components of these models can be replaced
    by quantum circuits.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As in the rest of this book, our approach will be very hands-on and practical.
    If you wish to broaden your theoretical background on quantum machine learning,
    you can also have a look at the book by Maria Schuld and Francesco Petruccione
    [[106](ch030.xhtml#Xschuld)].
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have explored some basic concepts and ideas that lie at
    the foundation of machine learning. And we haven’t just explored them from a theoretical
    point of view: we have also seen them come to life.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: We have learned what machine learning is all about, and we have discussed some
    of the most common approaches used to make it a reality. In particular, we have
    learned that many machine learning problems can be reduced to the minimization
    of a loss function through some optimization algorithm on a suitable model.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: We have also studied in some depth classical neural networks, and we have used
    an industry-standard machine learning framework (TensorFlow) to train one.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we have wrapped up this chapter by introducing what quantum machine
    learning is all about and having a sneak peek into the rest of the chapters of
    this part of the book.***
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
