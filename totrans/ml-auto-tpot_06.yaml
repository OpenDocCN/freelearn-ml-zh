- en: '*Chapter 4*: Exploring Classification with TPOT'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you'll continue going through hands-on examples of automated
    machine learning. You will learn how to handle classification tasks with TPOT
    in an automated manner by going through three complete datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover essential topics such as dataset loading, cleaning, necessary
    data preparation, and exploratory data analysis. Then, we'll dive deep into classification
    with TPOT. You will learn how to train and evaluate automated classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Before training models automatically, you will see how good models can be obtained
    with basic classification algorithms, such as logistic regression. This model
    will serve as the baseline that TPOT needs to outperform.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying automated classification modeling to the Iris dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying automated classification modeling to the Titanic dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this chapter, You will need to have Python and TPOT installed in
    your computer with Python and TPOT installed. Refer to [*Chapter 2*](B16954_02_Final_SK_ePub.xhtml#_idTextAnchor036),
    *Deep Dive into TPOT*, for detailed instructions on environment setup. If the
    concept of classification is entirely new to you, refer to [*Chapter 1*](B16954_01_Final_SK_ePub.xhtml#_idTextAnchor014),
    *Machine Learning and the Idea of Automation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the source code and dataset for this chapter here: [https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter04](https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter04)'
  prefs: []
  type: TYPE_NORMAL
- en: Applying automated classification models to the iris dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start simple, with one of the most basic datasets out there – the Iris
    dataset ([https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)).
    The challenge here won't be to build an automated model but to build a model that
    can outperform the baseline model. The Iris dataset is so simple that even the
    most basic classification algorithm can achieve high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of that, you should focus on getting the classification basics down
    in this section. You''ll have enough time to worry about performance later:'
  prefs: []
  type: TYPE_NORMAL
- en: As with the regression section, the first thing you should do is import the
    required libraries and load the dataset. You'll need `n``umpy`, `pandas`, `matplotlib`,
    and `seaborn` for starters. The `matplotlib.rcParams` module is imported to tweak
    the default stylings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s the code snippet for library imports and dataset loading:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here is the output returned by the `head()` function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Head of the Iris dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.1 – Head of the Iris dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Great – just what we need to get started.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to check if data quality is good enough to be passed to a
    machine learning algorithm. The first step here is to check for missing values.
    The following code snippet does just that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Missing value counts per column for the Iris dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.2 – Missing value counts per column for the Iris dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There seem to be no missing values, so we can proceed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's now check for class distribution in the target variable. This refers to
    the number of instances belonging to each class – `setosa`, `virginica`, and `versicolor`,
    in this case. Machine learning models are known to perform poorly if a severe
    class imbalance is present.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet visualizes the class distribution:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The visualization is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Iris dataset target variable distribution'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.3 – Iris dataset target variable distribution
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Iris dataset is as nice as they come – so once again, nothing for us to
    do preparation-wise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The final step in the data exploratory analysis and preparation is to check
    for correlation. A high correlation between features typically means there's some
    redundancy in the dataset, at least to a degree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet plots a correlation matrix with annotations:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The correlation matrix is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Correlation matrix of the Iris dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.4 – Correlation matrix of the Iris dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As expected, there's a strong correlation between most of the features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You're now familiar with the Iris dataset, which means we can move on to modeling
    the next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's build a baseline model with a logistic regression algorithm first. It
    will serve as a starting model that TPOT needs to outperform.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first step in the process is the train/test split. The following code snippet
    does just that, and it also prints the number of instances in both sets:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The number of instances is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Number of instances in train and test sets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.5 – Number of instances in train and test sets
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's build the baseline model next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As mentioned earlier, we''ll use logistic regression for the job. The code
    snippet below fits a logistic regression model, makes the predictions on the test
    set, and prints a confusion matrix of actual and predicted values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The corresponding confusion matrix is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Logistic regression confusion matrix for the Iris dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy score is shown in the following image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Accuracy on the test set with logistic regression for the Iris
    dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.7 – Accuracy on the test set with logistic regression for the Iris
    dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: And there you have it – 97% accuracy and only a single misclassification out
    of the box, with the simplest classification algorithm. Let's see if TPOT can
    outperform that next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s build an automated classification model next. We''ll optimize for accuracy
    and train for 10 minutes – similar to what we did in [*Chapter 3*](B16954_03_Final_SK_ePub.xhtml#_idTextAnchor051),
    *Exploring Regression with TPOT*. The code snippet below imports TPOT, instantiates
    a pipeline optimizer, and trains the model on the training datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'TPOT managed to fit 18 generations on my machine, which are shown in the following
    figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B16954_04_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 4.8 – Output of a TPOT pipeline optimization on the Iris dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s see if training an automated model managed to increase accuracy. You
    can use the following snippet to obtain the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy score is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Accuracy on the test set with an automated model for the Iris
    dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.9 – Accuracy on the test set with an automated model for the Iris dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, the accuracy on the test set didn't improve. If you were to
    make a scatter plot of the target variable and features, you would see some overlap
    for the *virginica* and *versicolor* classes. That's most likely the case here,
    and no amount of training would manage to correctly classify this single instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There''s only two things left to do here, and both are optional. The first
    one is to see what TPOT declared as an optimal pipeline after 10 minutes of training.
    The following code snippet will output that pipeline to the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The corresponding pipeline is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Optimal TPOT pipeline for the Iris dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.10 – Optimal TPOT pipeline for the Iris dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As always, you can also export the pipeline with the `export()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The entire Python code is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Python code for an optimal TPOT pipeline for the Iris dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_04_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Python code for an optimal TPOT pipeline for the Iris dataset
  prefs: []
  type: TYPE_NORMAL
- en: And there you have it – your first fully automated classification model with
    TPOT. Yes, the dataset was as basic as they come, but the principle always remains
    the same. We'll make automated models on a more complex dataset next, so there
    will be time to get your hands dirty.
  prefs: []
  type: TYPE_NORMAL
- en: Applying automated classification modeling to the titanic dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're now going to apply automated TPOT classification modeling to a slightly
    more complicated dataset. You'll get your hands dirty with the Titanic dataset
    ([https://gist.githubusercontent.com/michhar/2dfd2de0d4f8727f873422c5d959fff5/raw/fa71405126017e6a37bea592440b4bee94bf7b9e/titanic.csv](https://gist.githubusercontent.com/michhar/2dfd2de0d4f8727f873422c5d959fff5/raw/fa71405126017e6a37bea592440b4bee94bf7b9e/titanic.csv))
    – a dataset containing various attributes and descriptions of passengers who did
    and did not survive the Titanic accident.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to build an automated model capable of predicting whether a passenger
    would have survived the accident, based on various input features, such as passenger
    class, gender, age, cabin, number of siblings, spouses, parents, and children,
    among other features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by loading the libraries and the dataset next:'
  prefs: []
  type: TYPE_NORMAL
- en: As always, the first step is to load in the libraries and the dataset. You'll
    need `numpy`, `pandas`, `matplotlib`, and `seaborn` to get you started. The `Matplotlib.rcParams`
    module is also imported, just to make the visualizations a bit more appealing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet imports the libraries, loads in the dataset, and
    displays the first five rows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calling the `head()` function returns the first five rows of the dataset. They
    are shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Head of the Titanic dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.12 – Head of the Titanic dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can now proceed with the exploratory data analysis and preparation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The first step in the exploratory data analysis and preparation is to check
    for missing values. The following code line does just that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding line of code reports back the number of missing values per column
    in the dataset, as shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Missing values count per column for the Titanic dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.13 – Missing values count per column for the Titanic dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, there are a lot of missing values present in the dataset. Most
    of the missing values are in the `Age` and `Cabin` attributes. It's easy to understand
    for `Cabin` – the value is missing if the passenger didn't have their own cabin.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We'll deal with these missing values later, but for now, let's shift our focus
    to data visualization, so you can better understand the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To avoid code duplication, let's define a single function for displaying a bar
    chart. The function shows a bar chart with column counts on top of the bars. It
    also allows you to specify for which dataset column you want to draw a bar chart,
    values for the title, *x*-axis label, and *y*-axis label, and also offsets for
    the counts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can find the code for this function here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You'll use this function extensively during the next couple of pages. The goal
    is to visualize how categorical variables are distributed, so you can get a better
    understanding of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To start, let's visualize how many passengers have survived and how many haven't.
    The previously declared `make_bar_chart()` function comes in handy for the job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet makes the visualization:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The visualization is displayed in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Target class distribution for the Titanic dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.14 – Target class distribution for the Titanic dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, most of the passengers didn't survive the Titanic accident.
    This information alone doesn't tell you much because you don't know how many passengers
    survived per gender, passenger class, and other attributes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can use the `make_bar_chart()` function to make this type of visualization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's continue our data visualization journey by visualizing the number of passengers
    in each passenger class. You can use the same `make_bar_chart()` function for
    this visualization. Just make sure to change the parameters accordingly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet visualizes the number of passengers per passenger
    class. The lower the class number, the better – a more expensive ticket, better
    service, and who knows, maybe a higher chance of survival:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The visualization is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Number of passengers per passenger class'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.15 – Number of passengers per passenger class
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, most of the passengers belong to the third class. This is expected,
    as there were more workers on board than rich people.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the next step in the data visualization phase, let's see how the `Sex` attribute
    is distributed. This will give us insight into whether there were more women or
    men on board and how large the difference was.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet makes the visualization:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The visualization is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Number of passengers per gender'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.16 – Number of passengers per gender
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, there were definitely more men aboard. This is connected with
    the conclusion made in the previous visualization, where we concluded that there
    were many workers on board.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Most of the workers are male, so this visualization makes sense.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's take a little break from the bar charts and visualize a continuous variable
    for change. The goal is to make a histogram of the `Fare` attribute, which will
    show the distribution of the amounts paid for the ticket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet draws a histogram for the mentioned attribute:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The histogram is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Distribution of the Fare variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.17 – Distribution of the Fare variable
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It looks like most of the passengers paid 30 dollars or less for a ticket. As
    always, there are extreme cases. It seems like a single passenger paid around
    500 dollars for the trip. Not a wise decision, taking into consideration how things
    ended.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's do something a bit different now. The `Name` attribute is more or less
    useless in this format. But if you take a closer look, you can see that every
    value in the mentioned attribute is formatted identically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This means we can keep the single word after the first comma and store it in
    a new variable. We'll call this variable `Title` because it represents passenger
    titles (for example, Mr., Miss., and so on).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code snippet extracts the Title value to a new attribute and
    uses the `make_bar_chart()` function to visually represent different titles among
    Titanic passengers:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Distribution of the passenger titles'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.18 – Distribution of the passenger titles
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once again, these are expected results. Most of the passengers have common titles,
    such as Mr. and Miss. There's just a handful of them with unique titles. You could
    leave this column as is or turn it into a binary column – the value is zero if
    a title is common, and one otherwise. You'll see how to do that next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: That's about enough with regards to the exploratory data analysis. We've made
    quite a few visualizations, but you can always make more on your own.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It''s now time to prepare the dataset for machine learning. The steps are described
    here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) Drop the columns that are of no use – `Ticket` and `PassengerId`. The first
    one is just a collection of dummy letters and numbers and is of no use for predictive
    modeling. The second one is an arbitrary ID, most likely generated with a database
    sequence. You can remove both by calling the `drop()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Remap values in the `Sex` attribute to integers. The textual values *male*
    and *female* can't be passed to a machine learning algorithm directly. Some form
    of conversion is a must – so replace males with 0 and females with 1\. The `replace()`
    function is the perfect candidate for the job.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Use the previously generated `Title` column and convert it into a binary
    one – the value is zero if the title is common (for example, Mr., Miss., and Mrs.)
    and one otherwise. You can then rename the column to something a bit more appropriate,
    such as `Title_Unusal`. The `Name` column isn't needed anymore, so delete it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Handle missing values in the `Cabin` column by turning this attribute into
    a binary one – the value is zero if the value for the cabin is missing, and one
    otherwise. Name this new column `Cabin_Known`. After that, you can delete the
    `Cabin` column because it's not needed anymore, and it can't be passed to a machine
    learning model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Create dummy variables with the `Embarked` attribute. This attribute indicates
    the port on which the passengers entered the ship. You be the judge of whether
    this attribute is even necessary, but we'll keep it for TPOT to decide. After
    declaring dummy variables, concatenate them to the original dataset and delete
    the `Embarked` column.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f) Handle missing values in the `Age` attribute somehow. There are many sophisticated
    methods, such as *KNN imputing* or *MissForest imputing*, but for simplicity's
    sake, just impute the missing values with a simple average.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code snippet shows you how to apply all of the mentioned transformations:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can take a peek at the prepared dataset by examining the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.19 – Prepared Titanic dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.19 – Prepared Titanic dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: And that's all you have to do with regard to data preparation. Scaling/standardization
    is not required, as TPOT will decide whether that step is necessary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We'll begin with predictive modeling shortly – just one step remains.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before you can train a classification model, you''ll have to split the dataset
    into training and testing subsets. Keep in mind the `random_state` parameter –
    use the same value if you want the same data split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last code line prints the number of instances in training and testing subsets.
    You can see the numbers in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.20 – Number of instances in training and testing sets (Titanic)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.20 – Number of instances in training and testing sets (Titanic)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now you're ready to train predictive models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s start with a baseline model – logistic regression. We''ll train it on
    the train set and evaluate it on the test set. The following code snippet trains
    the model and prints the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see the confusion matrix in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.21 – Logistic regression confusion matrix (Titanic)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.21 – Logistic regression confusion matrix (Titanic)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It looks like there's the same number of false positives and false negatives
    (23). If we take ratios into account, there are more false negatives. In translation,
    the baseline model is more likely to say that the passenger survived even if they
    didn't.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Interpreting the confusion matrix is great, but what if you want to look at
    a concrete number instead? Since this is a classification problem, you could use
    accuracy. But there's a "better" metric – **F1 score**. The value for this metric
    ranges between 0 and 1 (higher is better) and represents a harmonic mean between
    precision and recall.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s how to calculate it with Python:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The value of the F1 score on the test set is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Logistic regression F1 score on the test set (Titanic)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.22 – Logistic regression F1 score on the test set (Titanic)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The value of 0.74 isn't bad for a baseline model. Can TPOT outperform it? Let's
    train an automated model and see what happens.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In a similar fashion as before, we'll train an automated classification model
    for 10 minutes. Instead of accuracy, we'll optimize for the F1 score. By doing
    so, we can compare the F1 scores of an automated model with the baseline one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet trains the model on the training set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following figure, you can see the output printed in the notebook during
    training. TPOT managed to train for 7 generations in 10 minutes, and the score
    increases as the model is training:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.23 – TPOT pipeline optimization output (Titanic)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.23 – TPOT pipeline optimization output (Titanic)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are free to leave the model to train for longer than 10 minutes. Still,
    this time frame should be enough to outperform the baseline model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's take a look at the value of the F1 score on the test set now. Remember,
    anything above 0.7415 means TPOT outperformed the baseline model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet prints the F1 score:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The corresponding F1 score is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.24 – TPOT optimized model F1 score on the test set (Titanic)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.24 – TPOT optimized model F1 score on the test set (Titanic)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It looks like TPOT outperformed the baseline model – as expected.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In case you''re more trustworthy of basic metrics, such as accuracy, here''s
    how you can compare it between baseline and automated models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Corresponding accuracy scores are shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Accuracies of the baseline model and TPOT optimized model on
    the test set (Titanic)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.25 – Accuracies of the baseline model and TPOT optimized model on the
    test set (Titanic)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, the simple accuracy metric tells a similar story – the model
    built by TPOT is still better than the baseline one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We are near the end of this practical example. There are two optional things
    left to do. The first one is to take a look at the optimal pipeline. You can obtain
    it with the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The optimal pipeline is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.26 – TPOT optimized pipeline (Titanic)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_04_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.26 – TPOT optimized pipeline (Titanic)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, TPOT used extreme gradient boosting to solve this classification
    problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, you can convert the optimal pipeline into Python code. Doing so makes
    the process of sharing the code that much easier. You can find the code for doing
    so here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The full source code for the automated pipeline is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.27 – Source code for the optimized TPOT pipeline (Titanic)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_04_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.27 – Source code for the optimized TPOT pipeline (Titanic)
  prefs: []
  type: TYPE_NORMAL
- en: And that does it for solving classification problems on the Titanic dataset
    in an automated fashion. You've now built two fully automated classification machine
    learning solutions. Let's wrap up this chapter next.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was the second hands-on chapter in the book. You've learned how to solve
    classification machine learning tasks in an automated fashion with two in-depth
    examples on well-known datasets. Without any kind of doubt, you are now ready
    to use TPOT to solve any type of classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you know how to solve regression and classification tasks. But what
    about parallel training? What about neural networks? The following chapter, [*Chapter
    5*](B16954_05_Final_SK_ePub.xhtml#_idTextAnchor065)*, Parallel Training with TPOT
    and Dask*, will teach you what parallel training is and how to utilize it with
    TPOT. Later, in [*Chapter 6*](B16954_06_Final_SK_ePub.xhtml#_idTextAnchor073),
    *Getting Started with Deep Learning – Crash Course in Neural Networks*, you'll
    reinforce your knowledge of basic deep learning and neural networks. As the icing
    on the cake, you'll learn how to use deep learning with TPOT in [*Chapter 7*](B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086),
    *Neural Network Classifier with TPOT*.
  prefs: []
  type: TYPE_NORMAL
- en: Please feel encouraged to practice solving classification problems automatically
    with tools and techniques covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Q&A
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you explore the distribution of a categorical variable with bar charts?
    Explain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the confusion matrix and the terms true positive, true negative, false
    positive, and false negative.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is precision? Explain by giving a practical example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is recall? Explain by giving a practical example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the difference between accuracy and F1 score? When would you use F1 over
    accuracy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does "1" in the F1 score mean? Can this number be altered? What happens
    in that scenario?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During training, does TPOT output the value of your scoring metric for the train
    set or the test set? Explain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
