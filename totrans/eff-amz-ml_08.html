<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Creating Datasources from Redshift</h1>
            </header>

            <article>
                
<p>In this chapter, we will use the power of SQL queries to address non-linear datasets. Creating datasources in Redshift or RDS gives us the potential for upstream SQL-based feature engineering prior to the datasource creation. We implemented a similar approach in <span class="ChapterrefPACKT"><a href="08d9b49a-a25c-4706-8846-36be9538b087.xhtml" target="_blank">Chapter 4</a></span><span>,</span> <em>Loading and Preparing the Dataset</em>, by leveraging the new AWS Athena service to apply preliminary transformations on the data before creating the datasource.<br/>
This enabled us to expand the <kbd>Titanic</kbd> dataset by creating new features, such as the <kbd>Deck</kbd> number, replacing the <kbd>Fare</kbd> with its log or replacing missing values for the <kbd>Age</kbd> variable. The SQL transformations were simple, but allowed us to expand the original dataset in a very flexible way. The <strong>AWS Athena</strong> service is S3 based. It allows us to run SQL queries on datasets hosted on S3 and dump the results in S3 buckets. We were still creating Amazon ML datasources from S3, but simply adding an extra data preprocessing layer to massage the dataset.</p>
<p>AWS offers two other SQL services from which it is possible to create datasources: RDS and Redshift. Datasource creation is very similar for both RDS and Redshift, and we will focus on creating datasources in Redshift via the Python SDK <kbd>Boto3</kbd>. The key point for us is that RDS/Redshift-based datasources are created directly via SQL queries, thus giving us the opportunity for further feature engineering. Redshift also integrates seamlessly with AWS Kinesis, a service we'll explore for streaming data in <a href="https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=1053" target="_blank">Chapter 9</a>, <em>Building a Streaming Data Analysis Pipeline</em>. </p>
<p>Amazon Machine Learning is built on intrinsically linear models leveraging with good results quantile binning data transformation as a method to handle non-linearities in the dataset. Polynomial regression is another important machine learning method used to deal with non-linear datasets. We will use our new SQL powers to implement polynomial regression using Amazon ML. </p>
<p>In this chapter, you will learn the following:</p>
<ul>
<li>Choosing between RDS and Redshift</li>
<li>How to create a Redshift database with PostgreSQL</li>
<li>How to load your S3 data into Redshift</li>
<li>How to create a datasource from Redshift</li>
<li>What is polynomial regression</li>
<li>How to use polynomial regression with Amazon ML</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Choosing between RDS and Redshift</h1>
            </header>

            <article>
                
<p>AWS offers no less than six different cloud database and SQL/NoSQL services: RDS, Aurora, DynamoDB, Redshift, Athena, and AWS Database Migration Service! Out of all these services, only two are compatible with Amazon Machine Learning: RDS and Redshift. You can store data in either service and create datasources from these sources. The datasource creation methods for the two services have similar parameters, but differ quite significantly when it comes to the underlying AWS service communication.</p>
<p>RDS and Redshift are very different services. Redshift is a data warehouse used to answer a few complex and long running queries on large datasets, while RDS is made for frequent, small, and fast queries. Redshift is more suited for massive parallel processing to perform operations on millions of rows of data with minimal latency, while RDS offers a server instance that runs a given database. RDS offers several different database types – MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, and Amazon Aurora, while Redshift is Amazon's own analytics database offering based on the <strong>ParAccel</strong> technology and running a fork of PostgreSQL. You connect to Redshift using standard ODBC and JDBC connections.<br/>
Amazon Redshift and PostgreSQL have a number of very important differences that you must be aware of as you build your database in Redshift. Many functions, data types, and PostgreSQL features are not supported in Amazon Redshift. More info is available at <a href="http://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html" target="_blank">http://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html</a>.</p>
<div class="packt_tip">A more in-depth explanation of the difference between RDS and Redshift can be found on this thread: <a href="https://www.quora.com/What-is-the-difference-between-redshift-and-RDS" target="_blank">https://www.quora.com/What-is-the-difference-between-redshift-and-RDS</a></div>
<p>In the context of Amazon ML, an important difference between the two services is that the Amazon ML web console only allows for datasource creation from S3 and Redshift, but not from RDS, as shown in this screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="134" src="assets/B05028_09_01.png" width="555"/></div>
<p>The Python SDK and the AWS CLI, however, both allow datasource creation from RDS and Redshift.</p>
<div class="packt_tip">
<p><strong>SDK:</strong></p>
<ul>
<li><kbd>create_data_source_from_rds()</kbd>:  <a href="http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_rds" target="_blank">http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_rds</a></li>
<li><kbd>create_data_source_from_redshift()</kbd>: <a href="http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_redshift" target="_blank">http://boto3.readthedocs.io/en/latest/reference/service/machinelearning.html#MachineLearning.Client.create_data_source_from_redshift</a></li>
</ul>
<p><strong>CLI</strong>:</p>
<ul>
<li><kbd>create-data-source-from-rds</kbd>: <a href="http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-rds.html" target="_blank">http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-rds.html</a></li>
</ul>
<ul>
<li><kbd>create-data-source-from-redshift</kbd>: <a href="http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-redshift.html" target="_blank">http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-redshift.html</a></li>
</ul>
</div>
<p>We now compare the parameters required by the Python SDK to connect to either services:</p>
<ul>
<li>Redshift parameters:</li>
</ul>
<pre>
        {<br/>            "DatabaseInformation": {<br/>              "DatabaseName": "string",<br/>              "ClusterIdentifier": "string"<br/>            },<br/>            "SelectSqlQuery": "string",<br/>            "DatabaseCredentials": {<br/>              "Username": "string",<br/>              "Password": "string"<br/>            },<br/>            "S3StagingLocation": "string",<br/>            "DataRearrangement": "string",<br/>            "DataSchema": "string",<br/>            "DataSchemaUri": "string"<br/>        }
</pre>
<ul>
<li>RDS parameters:</li>
</ul>
<pre>
        {<br/>            "DatabaseInformation": {<br/>              "DatabaseName": "string"<br/>              "InstanceIdentifier": "string",<br/>            },<br/>            "SelectSqlQuery": "string",<br/>            "DatabaseCredentials": {<br/>              "Username": "string",<br/>              "Password": "string"<br/>            },<br/>            "S3StagingLocation": "string",<br/>            "DataRearrangement": "string",<br/>            "DataSchema": "string",<br/>            "DataSchemaUri": "string",<br/>            "ResourceRole": "string",<br/>            "ServiceRole": "string",<br/>            "SubnetId": "string",<br/>            "SecurityGroupIds": ["string", ...]<br/>        }
</pre>
<p>The difference between the two sets of parameters lies in the way we allow access to the data store. Both sets include <kbd>DatabaseInformation</kbd>, <kbd>DatabaseCredentials</kbd>, <kbd>SelectSqlQuery</kbd>, <kbd>DataSchema</kbd>, and <kbd>DataRearrangement</kbd>. RDS also requires manually setting up two roles with the right policies: <kbd>ResourceRole: DataPipelineDefaultRole</kbd> and <kbd>ServiceRole:DataPipelineDefaultResourceRole</kbd>.</p>
<p>RDS is more adapted to our volume of data than Redshift, and we should use RDS instead of Redshift for our machine learning project. However, we previously found that manually creating the roles and policies for RDS required an in-depth knowledge of AWS inner permissions workings, which was too complex for this book. Although the parameters and concepts are very similar for creating datasources from RDS and Redshift, in the background, they differ quite a lot. RDS datasource creation involves the creation of AWS data pipelines, another AWS service that allows you to process and move data between different AWS computeing and storage services. Having to set up data pipelines adds a non-trivial layer of complexity to the whole project.</p>
<p>Redshift, on the other hand, does not require building a data pipeline and setting permissions, roles, and policies to create datasources. In the end, this extra simplicity made Redshift more adapted to this book, as we wanted to keep the focus on the machine learning side of things and not delve into the intricacies of AWS access roles and policies, although RDS would have been more suited for our low volume of data. </p>
<div class="packt_infobox">
<p><strong>Redshift</strong>: Presenting Redshift in depth far exceeds the scope of this book. We recommend the <em>Getting Started with Amazon Redshift</em> book by <em>Stefan Bauer, Packt</em> (<a href="https://www.packtpub.com/big-data-and-business-intelligence/getting-started-amazon-redshift" target="_blank">https://www.packtpub.com/big-data-and-business-intelligence/getting-started-amazon-redshift)</a>, the AWS documentation (<a href="https://aws.amazon.com/redshift/getting-started/" target="_blank">https://aws.amazon.com/redshift/getting-started/</a>) and this blog post for a good introduction to Cluster Configuration (<a href="https://www.periscopedata.com/amazon-redshift-guide/cluster-configuration" target="_blank">https://www.periscopedata.com/amazon-redshift-guide/cluster-configuration</a>).</p>
</div>
<p>Let's start with Redshift, using the AWS Redshift console to create a PostgreSQL-based instance and load the <kbd>Titanic</kbd> dataset we already have available in our S3 bucket.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating a Redshift instance</h1>
            </header>

            <article>
                
<p>Log in to your AWS account, and go to the Redshift dashboard at <a href="https://console.aws.amazon.com/redshift/" target="_blank">https://console.aws.amazon.com/redshift/</a>.</p>
<p>Creating a database in <span>Redshift</span> is quite simple and well handled by the AWS Redshift wizard. First, click on the <span class="packt_screen">Launch Cluster</span> button. In the first screen, we define the <span class="packt_screen">Cluster identifier*</span> as <kbd>amlpackt</kbd>, the <span class="packt_screen">Database name</span> as <kbd>amlpacktdb</kbd>, and the <span class="packt_screen">Master user name</span> as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="375" src="assets/B05028_09_02.png" width="303"/></div>
<p>In the next screen, we choose the default parameters to configure the node, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="263" src="assets/B05028_09_03.png" width="420"/></div>
<p>Choose the default settings for the next configuration screen, but make sure that the cluster is publicly accessible. You do not need a public IP:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="95" src="assets/B05028_09_04.png" width="281"/></div>
<p>Choose the Machine Learning/RDS VPC security group:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class=" image-border" height="83" src="assets/B05028_09_05.png" width="508"/></div>
<p>The final validation screen prior to launching the cluster will inform you about the associated costs as shown here:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="182" src="assets/B05028_09_06.png" width="610"/></div>
<p>It will take a few minutes once you click on the final <span class="packt_screen">Launch Cluster</span> button for the cluster to be ready. We will connect to the newly created database using Psql. Other external connection types are available through JDBC and ODBC. The cluster information page shows the <span class="packt_screen">Endpoint</span> URL that you need to connect to the newly created database:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="268" src="assets/B05028_09_07.png" width="646"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Connecting through the command line</h1>
            </header>

            <article>
                
<p>Psql is a command line program that acts as the primary front-end to a Postgresql database. It provides a number of shell commands (<kbd>pg_dump</kbd> to dump the content of a database, <kbd>createdb</kbd> to create a database, and many others). It also has a number of meta commands to list elements and display information (see the <strong>Psql cheatsheet</strong> information box). </p>
<div class="packt_infobox"><strong>Psql cheatsheet:</strong><br/>
<kbd>q</kbd>: Quit/Exit<br/>
<kbd>d __table__</kbd>: Show table definition, including triggers<br/>
<kbd>dt</kbd> : List tables<br/>
<kbd>df</kbd>: List functions<br/>
<kbd>dv</kbd>: List views<br/>
<kbd>x</kbd>: Pretty-format query results instead of the not-so-useful ASCII tables<br/>
<kbd>connect __database__</kbd>: Connect to a database<br/>
<kbd>l</kbd>: List databases<br/>
Adding a <kbd>+</kbd> on a <kbd>d</kbd> command will show more results: compare, for instance, d titanic with <kbd>d+ titanic</kbd><br/>
see <a href="http://postgresguide.com/utilities/psql.html" target="_blank">http://postgresguide.com/utilities/psql.html</a> for more info on Psql</div>
<p>You can now connect from the terminal to your database using <strong>Psql</strong> with the following command, using the endpoint URL (<kbd>amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com</kbd>) for the host:</p>
<pre>
<strong>$ psql --host=amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com --port=5439 --username=alexperrier --password --dbname=amlpacktdb</strong>
</pre>
<p>The <kbd>amlpacktdb</kbd> database is, of course, empty: </p>
<pre>
<strong>alexperrier@amlpacktdb=&gt; dt<br/> No relations found.</strong>
</pre>
<p>There are many ways we can import data into a Redshift database. To keep things simple, we will upload a CSV file available on S3 to a Redshift table with the copy command, as follows:</p>
<pre>
<strong>alexperrier@amlpacktdb=&gt; copy &lt;table name&gt; from '&lt;s3 path to csv file&gt;' CREDENTIALS 'aws_access_key_id=&lt;aws access key id&gt;;aws_secret_access_key=&lt;aws secret access key&gt;' CSV;</strong>
</pre>
<p>When using the copy command, Redshift needs to authenticate with the S3 service. Authentication between AWS services can be implemented in two ways:</p>
<ul>
<li><strong>User-based</strong>: authentication is granted by  passing the user access keys. It is a simpler mean of granting access between AWS services but it is not always available.</li>
<li><strong>Role-based</strong>: authentication requires creating roles with the right policies and permissions. It is a preferred andmore secure mean of authentication than user-based authentication. However, it requires extra roles and policies creation steps and is less straightforward to setup. </li>
</ul>
<p>More info on user versus role based authentication for AWS services is available at <a href="http://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-authentication-access-control.html" target="_blank">http://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-authentication-access-control.html</a>. In our copy example, we plan to use the aws access keys of our main AWS user. But before we can copy data into the table, we first need to create it with an SQL query. For the <kbd>Titanic</kbd> CSV file we have been working on, the table creation query is as follows:</p>
<pre>
CREATE TABLE IF NOT EXISTS titanic (<br/>  id integer primary key,<br/>  pclass integer,<br/>  survived boolean,<br/>  name varchar(255),<br/>  sex varchar(255),<br/>  age real,<br/>  sibsp integer,<br/>  parch integer,<br/>  ticket varchar(255),<br/>  fare real,<br/>  cabin varchar(255),<br/>  embarked char(1),<br/>  boat varchar(8),<br/>   body varchar(8),<br/>  home_dest varchar(255)<br/>);
</pre>
<p>The table now exists in our Redshift database, as shown by the <kbd>dt</kbd> command:</p>
<pre>
<strong>alexperrier@amlpacktdb=&gt; dt<br/> List of relations<br/> Schema | Name | Type | Owner<br/> --------+---------+-------+-------------<br/> public | titanic | table | alexperrier<br/> (1 row)</strong>
</pre>
<p>The table structure is as expected, <span>as shown by the  </span><kbd>d+</kbd><span> command</span>:</p>
<pre>
<strong>alexperrier@amlpacktdb=&gt; d+ titanic<br/> Table "public.titanic"<br/> Column | Type | Modifiers | Storage | Stats target | Description<br/> -----------+------------------------+------------------------------------------------------+----------+--------------+-------------<br/> id | integer | not null default nextval('titanic_id_seq'::regclass) |<br/> plain | | pclass | integer | | plain | |<br/> survived | boolean | | plain | |<br/> name | character varying(255) | | extended | |<br/> sex | character varying(255) | | extended | |<br/> age | real | | plain | |<br/> sibsp | integer | | plain | |<br/> parch | integer | | plain | |<br/> ticket | character varying(255) | | extended | |<br/> fare | real | | plain | |<br/> cabin | character varying(255) | | extended | |<br/> embarked | character(1) | | extended | |<br/> boat | character varying(8) | | extended | |<br/> body | character varying(8) | | extended | |<br/> home_dest | character varying(255) | | extended | |<br/> Indexes:<br/> "titanic_pkey" PRIMARY KEY, btree (id)</strong>
</pre>
<p>We can now upload the CSV file to S3 and fill in our table by running the following commands from your terminal:</p>
<pre>
<strong># Load file on S3<br/>$ aws s3 cp data/titanic.csv s3://aml.packt/data/ch9/<br/># connect to database via psql<br/>$ psql --host=amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com --port=5439 --username=alexperrier --password --dbname=amlpacktdb<br/># upload data from your S3 location into the titanic table<br/>$ copy titanic from 's3://aml.packt/data/ch9/titanic.csv' CREDENTIALS 'aws_access_key_id=&lt;access key id&gt;;aws_secret_access_key=&lt;access secret key&gt;' CSV;</strong>
</pre>
<p>Note that the CSV file should not include the CSV headers. To verify that the copy command worked, we can count the number of records in the Titanic table by running the following query: <br/></p>
<pre>
<strong>alexperrier@amlpacktdb=&gt; select count(*) from titanic;<br/> -[ RECORD 1 ]<br/> count | 1309</strong>
</pre>
<p>The result shows we now have 1309 records in the titanic table.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Executing Redshift queries using Psql</h1>
            </header>

            <article>
                
<p>We've seen that we can connect to our database with the following <kbd>Psql</kbd> command:</p>
<pre>
<strong>$ psql -h amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com -p 5439 -U alexperrier --password -d amlpacktdb</strong>
</pre>
<p>We then need to type in our password. To shorten the line and avoid having to type the password each time, we can set both the connection string and the password as shell environment variables. In your terminal, execute the following command to create a global <kbd>REDSHIFT_CONNECT</kbd> shell variable:</p>
<pre>
<strong>$ export REDSHIFT_CONNECT='-h amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com -p 5439 -U alexperrier -d amlpacktdb'</strong>
</pre>
<p>Similarly for the password, execute the command:</p>
<pre>
<strong>$ export PGPASSWORD=your_password</strong>
</pre>
<p>From now on, you can connect to the database with the simple command:</p>
<pre>
<strong>$ psql $REDSHIFT_CONNECT</strong>
</pre>
<p>Note that <kbd>REDSHIFT_CONNECT</kbd> is a variable name we chose, while <kbd>PGPASSWORD</kbd> is a predefined shell variable name that is recognized by Psql.</p>
<p>We now have a choice in the way we can run queries on our Redshift database. We can perform either of the following steps:</p>
<ul>
<li><kbd>Psql</kbd> into the database shell and type some SQL queries:</li>
</ul>
<pre>
<strong>        $ psql $REDSHIFT_CONNECT</strong><br/><strong>        alexperrier@amlpacktdb=&gt; select count(*) from titanic;</strong>
</pre>
<ul>
<li>Write the SQL query into a file (for instance, <kbd>my_file.sql</kbd>) and, from the terminal, run the command:</li>
</ul>
<pre>
<strong>        $ psql $REDSHIFT_CONNECT -f my_file.sql</strong>
</pre>
<ul>
<li>Run the query directly with the <kbd>Psql</kbd> command:</li>
</ul>
<pre>
<strong>        $ psql $REDSHIFT_CONNECT -c 'SELECT count(*) FROM my_table'</strong>
</pre>
<p>We are now ready to work on our dataset. S<span>ince we have already worked extensively on the Titanic dataset, we will use another dataset for the rest of the chapter. Let's create an artificial dataset that exhibits strong non-linear patterns.</span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating our own non-linear dataset</h1>
            </header>

            <article>
                
<p>A good way to create a non-linear dataset is to mix sines with different phases. The dataset we will work with in this chapter is created with the following Python script and exported to a CSV file:</p>
<pre>
import numpy as np<br/>n_samples = 1000<br/>de_linearize = lambda X: np.cos(1.5 * np.pi * X) + np.cos( 5 * np.pi * X )<br/>X = np.sort(np.random.rand(n_samples)) * 2<br/>y = de_linearize(X) + np.random.randn(n_samples) * 0.1
</pre>
<p>As usual, <em>X</em> is the predictor, and <em>y</em> the outcome. You can use variations on that script to easily generate other non-linear datasets. Note that we have used a <kbd>lambda</kbd> function, which is a Pythonic way of declaring a function on the spot when needed. Then we shuffle the dataset by sorting randomly (<kbd>np.random.rand(n_samples)</kbd>). We then save the data to a CSV file (<kbd>nonlinear.csv</kbd>) using the <strong>Pandas</strong> dataframe:</p>
<pre>
import pandas as pd<br/>df = pd.DataFrame( {'X':X, 'y': y} )<br/>df = df.sample(frac=1) # shuffles the entire dataframe<br/>df.to_csv('data/nonlinear.csv', index = False)
</pre>
<p>Plotting the data gives the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="299" src="assets/B05028_09_11.png" width="299"/></div>
<p>It is obviously not linear. A line has no hope of approximating, let alone predicting the outcome <em>y</em> from the predictor <em>X</em>. Now that we have a highly non-linear dataset available, we need to upload it to Redshift.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Uploading the nonlinear data to Redshift</h1>
            </header>

            <article>
                
<p>We first need to create the table that will host the data. We will call that table <kbd>nonlinear</kbd>. It only has three columns: an index, the predictor <kbd>X</kbd>, and the outcome <kbd>y</kbd>:</p>
<pre>
<strong>CREATE TABLE IF NOT EXISTS nonlinear (</strong><br/><strong>  id integer primary key,</strong><br/><strong>  x1 real,</strong><br/><strong>  y real</strong><br/><strong>);</strong>
</pre>
<p>Once the table is created, we can upload the CSV file to S3, connect to the database, and import the data in the non-linear table with the command:</p>
<pre>
<strong>copy nonlinear from 's3://aml.packt/data/ch9/nonlinear.csv' CREDENTIALS 'aws_access_key_id=&lt;access key id&gt;;aws_secret_access_key=&lt;access secret key&gt;' CSV;</strong>
</pre>
<p>We can verify that the <kbd>nonlinear</kbd> table now has a thousand rows with the query:</p>
<pre>
<strong>$ psql $REDSHIFT_CONNECT -c "select count(*) from nonlinear"<br/> &gt; count<br/>&gt; 1000<br/> &gt;(1 row)</strong>
</pre>
<p>Our data has been uploaded to Redshift. We are ready to create datasources and train and evaluate models! But before we dive into model building on this dataset, let's introduce the polynomial regression method, which will allow us to deal with this highly non-linear dataset.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Introducing polynomial regression</h1>
            </header>

            <article>
                
<p>In two dimensions, where we have a predictor and an outcome, linear modeling is all about finding the best line that approximates your data. In three dimensions (two predictors and one outcome), the idea is then to find the best plane, or the best flat surface, that approximates your data. In the <kbd>N</kbd> dimension, the surface becomes an hyperplane, but the goal is always the same – to find the hyperplane of dimension <em>N-1</em> that gives the best approximation for regression or that separates the classes the best for classification. That hyperplane is always flat.</p>
<p>Coming back to the very non-linear two-dimensional dataset we created, it is obvious that no line can properly approximate the relation between the predictor and the outcome. There are many different methods to model non-linear data, including polynomial regression, step functions, splines, and <strong>Generalized additive models</strong> (<strong>GAM</strong>). See <em>Chapter 7</em> of <em>An Introduction to Statistical Learning</em> by <em>James, Witten</em>, <em>Hastie</em> and <em>Tibshirani Springer, 2013</em> for a great introduction to these methods. The book is available in PDF at <a href="http://www-bcf.usc.edu/~gareth/ISL/" target="_blank">http://www-bcf.usc.edu/~gareth/ISL/</a>. We will apply the polynomial regression method.</p>
<p>Polynomial regression consists in replacing the standard linear model:</p>
<div class="CDPAlignLeft CDPAlign"><img height="17" src="assets/B05028_09_12.png" width="100"/></div>
<p class="CDPAlignLeft">Here, <em>ŷ</em> is the predicted outcome, <em>x</em> the predictor, (<em>w<sub>o</sub>, w<sub>1</sub></em>) the linear model coefficients. By a polynomial function of order <em>k:</em></p>
<div class="CDPAlignLeft CDPAlign"><img height="23" src="assets/B05028_09_13.png" width="260"/></div>
<p>The power of the polynomial regression approach is that we can use the same linear modeling method as with the linear model, and we can therefore still use Amazon ML SGD to find the coefficients {<em>w<sub>k</sub></em>} of the polynomial regression equation. In the next section, we will train successive models by increasing the degree of the polynomial.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Establishing a baseline</h1>
            </header>

            <article>
                
<p>We first need to establish a baseline. Amazon ML quantile binning transformation is Amazon Machine Learning preferred method of dealing with non-linearities in a dataset. Let's establish how a simple linear model performs with Amazon's default recipe. We will create a baseline score using the usual AWS console tools. This time, we choose to create a data source from Redshift and not from S3. Fill the information as shown in the next screenshot, and click on <span class="packt_screen">Test access</span> to check your access, and at the same time, make Amazon ML create the necessary IAM role. Finally, click on <span class="packt_screen">Verify</span> once you're done:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="253" src="assets/B05028_09_08.png" width="606"/></div>
<p>Amazon ML handles all the role and policy creation in the background by creating the following resources:</p>
<ul>
<li>A new role: <span class="packt_screen">AmazonMLRedshift_us-east-1_amlpackt</span>. We will use the <span class="packt_screen">arn</span> related to this role when creating datsources using the Python SDK.</li>
<li>Two new policies attached to this role:
<ul>
<li><kbd>AmazonMLS3LocationAccess_aml.packt</kbd></li>
<li><kbd>AmazonMLRedshiftAccess_us-east-1_amlpackt</kbd></li>
</ul>
</li>
<li>AWS also sets up the <kbd>Trust Relationship</kbd> so that <kbd>roleAmazonMLRedshift_us-east-1_amlpackt</kbd> is able assume a <kbd>machinelearning</kbd> service role.</li>
</ul>
<p>Creating these roles and policies manually requires a solid understanding of access permissions between services in AWS. Creating them using the console is a significant time-saver. The next steps are standard schema and target definition, and data source creation. The default schema generated by Amazon ML is as follows:</p>
<pre>
{<br/>  "version" : "1.0",<br/>  "rowId" : null,<br/>  "rowWeight" : null,<br/>  "targetAttributeName" : "y",<br/>  "dataFormat" : "CSV",<br/>  "dataFileContainsHeader" : false,<br/>  "attributes" : [ {<br/>    "attributeName" : "x",<br/>    "attributeType" : "NUMERIC"<br/>  }, {<br/>    "attributeName" : "y",<br/>    "attributeType" : "NUMERIC"<br/>  } ],<br/>  "excludedAttributeNames" : [ ]<br/>}
</pre>
<p>We reuse that schema later on by saving the JSON string into the <kbd>data/nonlinear.schema</kbd> file and uploading it to S3 with <kbd>aws s3 cp data/nonlinear.schema s3://aml.packt/data/ch9/</kbd> .<br/>
Once the datasource is available, we can create and evaluate a model via the console. The recipe generated by Amazon ML during the model creation uses the quantile binning transformation on the predictor with 500 bins, which may seem like a large value since we only have 700 samples in the training dataset. The auto-generated Amazon ML recipe is as follows:</p>
<pre>
{<br/>  "groups": {<br/>    "NUMERIC_VARS_QB_500": "group('x')"<br/>  },<br/>  "assignments": {},<br/>  "outputs": [<br/>    "ALL_CATEGORICAL",<br/>    "quantile_bin(NUMERIC_VARS_QB_500,500)"<br/>  ]<br/>}
</pre>
<p>We train a model with L2 mild regularization and 100 passes, and evaluate that model on 30% of our dataset. We obtain the following results: </p>
<ul>
<li>With quantile binning
<ul>
<li>RMSE: 0.1540 </li>
<li>Baseline RMSE: <span>1.034</span></li>
</ul>
</li>
<li>Without quantile binning
<ul>
<li>RMSE: 1.0207 </li>
<li>Baseline <span>RMSE</span>: <span>1.025</span></li>
</ul>
</li>
</ul>
<p>Quantile binning correctly handles the non-linearities and results in a pretty good score, while a raw linear model does not fare much better than the baseline. In the case of linear regression, Amazon ML baseline is simply the mean of the outcome in the training dataset. </p>
<p>Let's see if we can beat these results with polynomial regression.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Polynomial regression in Amazon ML</h1>
            </header>

            <article>
                
<p>We will use <kbd>Boto3</kbd> and Python SDK and follow the same method of generating the parameters for datasources that we used in <em><span class="item-title"><span><a href="https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609" target="_blank">Chapter 7</a>,</span> <span>Command Line and SDK</span></span></em>, to do the <strong>Monte Carlo</strong> validation: we will generate features corresponding to power 2 of <em>x</em> to power <kbd>P</kbd> of <kbd>x</kbd> and run <kbd>N</kbd> Monte Carlo cross-validation. The pseudo-code is as follows:</p>
<pre>
for each power from 2 to P:<br/>    write sql that extracts power 1 to P from the nonlinear table<br/>    do N times<br/>        Create training and evaluation datasource<br/>        Create model<br/>        Evaluate model<br/>        Get evaluation result<br/>        Delete datasource and model<br/>    Average results
</pre>
<p>In this exercise, we will go from <em>2 to 5</em> powers of <em>x</em> and do 5 trials for each model. The Python code to create a datasource from Redshift using <kbd>create_data_source_from_rds()</kbd> is as follows:</p>
<pre>
response = client.create_data_source_from_redshift(<br/>    DataSourceId='string',<br/>    DataSourceName='string',<br/>    DataSpec={<br/>        'DatabaseInformation': {<br/>            'InstanceIdentifier': 'amlpackt',<br/>            'DatabaseName': 'amlpacktdb'<br/>        },<br/>        'SelectSqlQuery': 'select x, y from nonlinear order by random()',<br/>        'DatabaseCredentials': {<br/>            'Username': 'alexperrier',<br/>            'Password': 'my_password'<br/>        },<br/>    'S3StagingLocation': 's3://aml.packt/data/ch9/',<br/>    'DataRearrangement': '{"splitting":{"percentBegin":0,"percentEnd":70 }<br/>  }',<br/>    'DataSchemaUri': 's3://aml.packt/data/ch9/nonlinear.csv.schema'<br/> },<br/> RoleARN='arn:aws:iam::178277513911:role/service-role/AmazonMLRedshift_us-east-1_amlpackt',<br/> ComputeStatistics=True<br/>)
</pre>
<p>Beyond the obvious parameters (<kbd>Database Information</kbd>, <kbd>DataSchemaURI</kbd>, <kbd>DataSourceId</kbd>, and <kbd>DataSourceName</kbd>), you need to find the value for the <span class="packt_screen">Role ARN</span> identifier. Go to the <span class="packt_screen">IAM</span> console, click on <span class="packt_screen">R</span><span class="packt_screen">oles,</span> and then on the <span class="packt_screen">AmazonMLRedshift_us-east-1_amlpackt</span> role in order to find the <span class="packt_screen">Role</span> <span class="packt_screen">ARN</span> string:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="78" src="assets/B05028_09_10.png" width="655"/></div>
<p>The <kbd>DataRearrangement</kbd> string will depend on the nature of the datasource with a 0% to 70% split for the training datasource and a 70% to 100% for the evaluation datasource. The <kbd>SelectSqlQuery</kbd> is where we are going to do the feature engineering and create new variables as powers of <em>x</em>.</p>
<p>For instance, the following query generates an x to the power of 2 variable:</p>
<pre>
select x, power(x,2) as x2, y from nonlinear order by random()
</pre>
<p>This query also generates an x to the power of 3 variable:</p>
<pre>
select x, power(x,2) as x2, power(x,3) as x3, y from nonlinear order by random()
</pre>
<p>Besides having to generate new queries for each new set or variables, we also need to generate a new schema. <span>The original schema for the non-linear dataset is as follows:</span></p>
<pre>
{<br/>  "version" : "1.0",<br/>  "rowId" : null,<br/>  "rowWeight" : null,<br/>  "targetAttributeName" : "y",<br/>  "dataFormat" : "CSV",<br/>  "dataFileContainsHeader" : false,<br/>  "attributes" : [ {<br/>    "attributeName" : "x",<br/>    "attributeType" : "NUMERIC"<br/>  }, {<br/>    "attributeName" : "y",<br/>    "attributeType" : "NUMERIC"<br/>  } ],<br/>  "excludedAttributeNames" : [ ]<br/>}
</pre>
<p>We modify this original schema by adding the following element to the schema attributes list for each new power of x variable:</p>
<pre>
{ <br/>    "attributeName" : "x{N}", <br/>    "attributeType" : "NUMERIC"<br/>}
</pre>
<p>In order to run our trials, compare different feature sets, and do cross-validation to select the best model, we need to write a set of Python functions. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Driving the trials in Python</h1>
            </header>

            <article>
                
<p>So far, we have written sequential code in Python. Writing simple object-oriented code instead is always a time saver in the end. The code is more organized, maintainable, and less prone to becoming unusable after a while. Taking the time to write simple classes with clear initialization, instances and class methods will make your code much simpler and robust in the end. With that in mind, we will now write a <kbd>NonLinear</kbd> class for our experiment. </p>
<p>Let's first write down the different functions of that class that generate some of the fields that depend on the power of the polynomial regression:</p>
<ul>
<li>This function takes in a power <kbd>p</kbd> and returns the SQL query:</li>
</ul>
<pre>
      def generate_sql(self, p):<br/>        powers = [ 'power(x,{0}) as x{0}'.format(i) for i in range(1,p+1) ]<br/>        return 'select ' + ','.join(powers) + ', y from nonlinear order by<br/>        random()'
</pre>
<ul>
<li>This function takes in the name of the data split (training versus evaluation) and returns the string, formatted as JSON, which will be required during the datasource creation:</li>
</ul>
<pre>
      def generate_data_rearrangement(self,split):<br/>           if split == 'training':<br/>              pct_begin = 0<br/>              pct_end = 70<br/>           else:<br/>              pct_begin = 70<br/>              pct_end = 100<br/>       return json.dumps( { "splitting": <br/>       {"percentBegin":pct_begin,"percentEnd":pct_end } } )
</pre>
<ul>
<li>Finally, the following function takes in the power <kbd>p</kbd> and returns the schema JSON string:</li>
</ul>
<pre>
      def generate_schema(self, p):<br/>      attributes = [ { "attributeName" : "x{0}".format(i), "attributeType"<br/>      : "NUMERIC" } for i in range(1,p+1) ]<br/>     attributes.append({ "attributeName" : "y", "attributeType" : "NUMERIC"  <br/>     })<br/>     return json.dumps({ "version" : "1.0",<br/>         "rowId" : None,<br/>         "rowWeight" : None,<br/>         "targetAttributeName" : "y",<br/>         "dataFormat" : "CSV",<br/>         "dataFileContainsHeader" : False,<br/>         "attributes" : attributes,<br/>         "excludedAttributeNames" : [ ]<br/>     })
</pre>
<p>The next three functions use the machine learning client to create the datasources, the model, and the evaluation. They are very similar to the scripts we wrote in <span class="item-title"><a href="https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609">C</a></span><span class="item-title"><a href="https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609">hapter</a></span> <em><span class="item-title"><span><a href="https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609">7</a>,</span> <span>Command Line and SDK</span></span></em>.</p>
<ul>
<li>The data source creation takes in a power <kbd>p</kbd>, and an index <kbd>k</kbd> for the cross-validation, and splits the nature of the datasource created. The script calls the <kbd>generate_sql</kbd> and <kbd>generate_data_rearrangement</kbd> methods:</li>
</ul>
<pre>
      def create_datasource(self, p, k, split ):<br/>        print("Create datasource {0} {1} {2} {3}".format(p,k,split, <br/>        self.prefix))<br/>        return self.client.create_data_source_from_redshift(<br/>        DataSourceId = "ds_{2}_{3}_p{0}_{1}".format(p,k,split, self.prefix),<br/>        DataSourceName = "DS {2} {3} p{0} {1}".format(p,k,split, <br/>         self.prefix),<br/>        DataSpec = {<br/>          'DatabaseInformation': {<br/>            'DatabaseName': 'amlpacktdb',<br/>            'ClusterIdentifier': 'amlpackt'<br/>           },<br/>           'SelectSqlQuery': self.generate_sql(p),<br/>           'DatabaseCredentials': {<br/>             'Username': 'alexperrier',<br/>             'Password': 'password'<br/>            },<br/>            'S3StagingLocation': 's3://aml.packt/data/ch9/',<br/>            'DataRearrangement': self.generate_data_rearrangement(split),<br/>            'DataSchema': self.generate_schema(p)<br/>          },<br/>          RoleARN='arn:aws:iam::178277513911:role/service-role<br/>          /AmazonMLRedshift_us-east-1_amlpackt',<br/>          ComputeStatistics=True<br/>        )
</pre>
<ul>
<li>The create model method also takes in the power <kbd>p</kbd> and index <kbd>k</kbd>:</li>
</ul>
<pre>
      def create_model(self, p, k):<br/>        print("Create model {0} {1} {2}".format(p, k, self.prefix))<br/>        return self.client.create_ml_model(<br/>        MLModelId = "mdl_{2}_p{0}_{1}".format(p,k, self.prefix),<br/>        MLModelName = "MDL {2} p{0} {1}".format(p,k, self.prefix),<br/>        MLModelType = 'REGRESSION',<br/>        Parameters = self.sgd_parameters,<br/>        TrainingDataSourceId = self.ds_training['DataSourceId'] ,<br/>        Recipe = json.dumps(self.recipe)<br/>      )
</pre>
<ul>
<li>Finally, the create evaluation method is as follows:</li>
</ul>
<pre>
      def create_evaluation(self, p, k):<br/>      print("Create evaluation {0} {1} {2}".format(p, k, self.prefix))<br/><br/>      return self.client.create_evaluation(<br/>      EvaluationId = "eval_{2}_p{0}_{1}".format(p,k, self.prefix),<br/>      EvaluationName = "EVAL {2} p{0} {1}".format(p,k, self.prefix),<br/>      MLModelId = self.model['MLModelId'],<br/>      EvaluationDataSourceId= self.ds_evaluation['DataSourceId']<br/>      )
</pre>
<p>We use <kbd>create_sql(p)</kbd> and <kbd>create_schema(p)</kbd> to render the <kbd>SelectSqlQuery</kbd> and <kbd>Data.Schema</kbd> fields when creating datasources. The model creation function uses two class items not yet initialized: <kbd>sgd_parameters</kbd> and <kbd>recipe</kbd>. The datasource creation function returns the response from the Amazon ML <kbd>create_data_source_from_redshift</kbd> function. We memorize the response in <kbd>ds_training</kbd> and <kbd>ds_evaluation</kbd> and use these items to pass on the appropriate <kbd>DataSourceId</kbd> in the model and evaluation creation functions.</p>
<p>The global code for running all the different evaluations is this: </p>
<pre>
# Initialize the object <br/>nl = NonLinear(max_p, n_crossval, prefix)<br/># Run all the datasources, models and evaluations creation  <br/>nl.run_all_trials()<br/># Wait until the evaluations are finished and get the results<br/>nl.get_results()<br/># Export the results to a csv file<br/>nl.to_csv(filename)<br/># Free the resources<br/>nl.delete_resources()
</pre>
<p>These functions are defined by the following:</p>
<pre>
import pandas as pd<br/>import boto3<br/>import json<br/>import csv<br/><br/>class NonLinear():<br/><br/> def __init__(self, max_p, n_crossval, prefix):<br/> self.trials = []<br/> self.max_p = max_p<br/> self.n_crossval = n_crossval<br/> self.prefix = prefix<br/> self.client = boto3.client('machinelearning')<br/> self.sgd_parameters = {<br/> "sgd.shuffleType": "auto",<br/> "sgd.l2RegularizationAmount": "1.0E-06",<br/> "sgd.maxPasses": "100"<br/> }<br/><br/> self.recipe = {<br/> "groups" : {},<br/> "assignments" : { },<br/> "outputs": ["ALL_INPUTS"]<br/> # "outputs": ["quantile_bin(ALL_NUMERIC,200)"]<br/> }<br/><br/> def run_all_trials(self):<br/> for p in range(1,self.max_p+1):<br/> for k in range(self.n_crossval):<br/> self.trials.append( self.run_trial(p,k) )<br/><br/> def run_trial(self, p, k ):<br/> self.ds_training = self.create_datasource(p, k, 'training')<br/> self.ds_evaluation = self.create_datasource(p, k, 'evaluation')<br/> self.model = self.create_model(p,k)<br/> self.evaluation = self.create_evaluation(p,k)<br/> return {<br/> "p": p,<br/> "k": k,<br/> "ds_training_id": self.ds_training['DataSourceId'],<br/> "ds_evaluation_id": self.ds_evaluation['DataSourceId'],<br/> "model_id": self.model['MLModelId'],<br/> "evaluation_id": self.evaluation['EvaluationId'],<br/> "rmse": None<br/> }<br/><br/> def get_results(self):<br/> results = []<br/> for trial in self.trials:<br/><br/> waiter = self.client.get_waiter('evaluation_available')<br/> print("Waiting on evaluation {0} to finish ".format( trial['evaluation_id'] ) )<br/> waiter.wait(FilterVariable='DataSourceId', EQ=trial['ds_evaluation_id'] )<br/><br/> response = self.client.get_evaluation( EvaluationId=trial['evaluation_id'] )<br/> rmse = float(response['PerformanceMetrics']['Properties']['RegressionRMSE'])<br/> trial["rmse"] = rmse<br/> results.append(trial)<br/> print("Evaluation score {0}".format(rmse))<br/> self.trials = results<br/><br/> def delete_resources(self):<br/> # Now delete the resources<br/> print("Deleting datasources and model")<br/> for trial in self.trials:<br/> response = self.client.delete_data_source(<br/> DataSourceId = trial['ds_training_id']<br/> )<br/> response = self.client.delete_data_source(<br/> DataSourceId = trial['ds_evaluation_id']<br/> )<br/> response = self.client.delete_ml_model(<br/> MLModelId = trial['model_id']<br/> )<br/><br/> def to_csv(self, filename):<br/> print("exporting to csv {0}".format(filename))<br/> keys = self.trials[0].keys()<br/> with open(filename, 'w') as output_file:<br/> dict_writer = csv.DictWriter(output_file, keys)<br/> dict_writer.writeheader()<br/> dict_writer.writerows(self.trials)
</pre>
<p>The integral code is available on GitHub at <a href="https://github.com/alexperrier/packt-aml">https://github.com/alexperrier/packt-aml</a>. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Interpreting the results</h1>
            </header>

            <article>
                
<p>The following graph shows the different RMSE obtained for the five cross-validations and the different polynomial degrees (1 to 5):</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="426" src="assets/B05028_09_15.png" width="426"/></div>
<p>We see that the best fit is obtained for the polynomes of degrees <em>3</em> and <em>4</em>. In the end, the overall RMSE of our models based on polynomial regression models is not good compared to the RMSE obtained with quantile binning. Polynomial regression gives RMSE values at best around 0.85, while the RMSE with quantile binning was found to be around 0.15. Quantile binning, as it is done by Amazon ML, beats polynomial regression by a large factor.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we saw how to use Redshift as a datasource for Amazon ML. Although RDS could also have been used to create datasources, Redshift is much easier to use with Amazon ML as all the access configuration is taken care of by the AWS wizard. </p>
<p>We have shown how to use simple SQL queries on Redshift to carry out feature engineering and implement a polynomial regression approach on a highly non-linear dataset. We have also shown how to generate the required SQL queries, schemas, and recipes to carry out the Monte Carlo cross-validation.</p>
<p>In the next chapter, we will build on our Redshift integration and start streaming data using the AWS Kinesis service.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>