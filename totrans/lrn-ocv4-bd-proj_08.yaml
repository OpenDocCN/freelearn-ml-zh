- en: Video Surveillance, Background Modeling, and Morphological Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn how to detect a moving object in a video
    taken from a static camera. This is used extensively in video surveillance systems.
    We will discuss the different characteristics that can be used to build this system.
    We will learn about background modeling and see how we can use it to build a model
    of the background in a live video. Once we do this, we will combine all the blocks
    to detect the object of interest in the video.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you should be able to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is naive background subtraction?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is frame differencing?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we build a background model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we identify a new object in a static video?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is morphological image processing and how is it related to background modeling?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we achieve different effects using morphological operators?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires familiarity with the basics of the C++ programming language.
    All the code used in this chapter can be downloaded from the following GitHub
    link: [https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_08](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_08).
    The code can be executed on any operating system, though it is only tested on
    Ubuntu.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2SfqzRo](http://bit.ly/2SfqzRo)'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding background subtraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Background subtraction is very useful in video surveillance. Basically, the
    background subtraction technique performs really well in cases where we have to
    detect moving objects in a static scene. How is this useful for video surveillance?
    The process of video surveillance involves dealing with constant data flow. The
    data stream keeps coming in and we need to analyze it to recognize any suspicious
    activity. Let's consider the example of a hotel lobby. All the walls and furniture
    have a fixed location. If we build a background model, we can use it to identify
    suspicious activity in the lobby. We are taking advantage of the fact that the
    background scene remains static (which happens to be true in this case). This
    helps us avoid any unnecessary computational overhead. As the name indicates,
    this algorithm works by detecting and assigning each pixel of an image to two
    classes, either the background (assumed static and stable) or the foreground,
    and subtracting it from the current frame to obtain the foreground image part,
    which includes moving objects such as persons, cars, and so on. With the static
    assumption, the foreground objects will naturally correspond to objects or people
    moving in front of the background.
  prefs: []
  type: TYPE_NORMAL
- en: In order to detect moving objects, we need to build a model of the background.
    This is not the same as direct frame differencing, because we are actually modeling
    the background and using this model to detect moving objects. When we say that
    we are modeling the background, we are basically building a mathematical formula
    that can be used to represent the background. This is much better than the simple
    frame-differencing technique. This technique tries to detect static parts of the
    scene and then include small updates in the build statistic formula of the background
    model. This background model is then used to detect background pixels. So, it's
    an adaptive technique that can adjust according to the scene.
  prefs: []
  type: TYPE_NORMAL
- en: Naive background subtraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start the discussion from the beginning. What does a background subtraction
    process look like? Consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cbf0d44-4e2a-4214-a7ab-899166d16f4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous image represents the background scene. Now, let''s introduce a
    new object into this scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86da43cc-6484-44cc-b145-0e022c5da798.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, there is a new object in the scene. So, if we compute the difference
    between this image and our background model, you should be able to identify the
    location of the TV remote:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5c47d1f-cfbd-4860-a7fa-e1ed8ccf8e22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The overall process looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e067d24c-4808-41e4-bead-526939174095.png)'
  prefs: []
  type: TYPE_IMG
- en: Does it work well?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's a reason we call it the **naive** approach! It works under ideal conditions
    and, as we know, nothing is ideal in the real world. It does a reasonably good
    job of computing the shape of the given object, but it does so under some constraints.
    One of the main requirements of this approach is that the color and intensity
    of the object should be sufficiently different from that of the background. Some
    of the factors that affect this kind of algorithm are image noise, lighting conditions,
    and autofocus in cameras.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a new object enters our scene and stays there, it will be difficult to
    detect new objects that are in front of it. This is because we are not updating
    our background model, and the new object is now a part of our background. Consider
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93f3155d-4cf6-4c86-86f9-fd03569dbfb2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s say a new object enters our scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/948d01ff-ce36-45ff-842f-bfd3c6c61f82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We detect this to be a new object, which is fine! Let''s say another object
    comes into the scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0665a410-2e58-4614-9565-c1682e249b9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It will be difficult to identify the location of these two different objects
    because their locations are overlapping. Here''s what we get after subtracting
    the background and applying the threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33e38517-b173-4bcb-8a68-0c38527fab88.png)'
  prefs: []
  type: TYPE_IMG
- en: In this approach, we assume that the background is static. If some parts of
    our background start moving, those parts will start getting detected as new objects.
    So, even movements that are minor, say a waving flag, will cause problems in our
    detection algorithm. This approach is also sensitive to changes in illumination
    and it cannot handle any camera movement. Needless to say, it's a delicate approach!
    We need something that can handle all these things in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Frame differencing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that we cannot keep a static background image pattern that can be used
    to detect objects. One of the ways to fix this would be by using frame differencing.
    It is one of the simplest techniques we can use to see what parts of the video
    are moving. When we consider a live video stream, the difference between successive
    frames gives a lot of information. The concept is fairly straightforward! We just
    take the difference between successive frames and display the differences between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'If I move my laptop rapidly, we can see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1bfa52d-7bf1-44bb-a853-2f9678f49599.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of the laptop, let''s move the object and see what happens. If I rapidly
    shake my head, it will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a604d5b7-cb16-4cdd-833f-a498254df1df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the previous images, only the moving parts of the video
    get highlighted. This gives us a good starting point to see what areas are moving
    in the video. Let''s look at the function to compute the frame differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Frame differencing is fairly straightforward! You compute the absolute differences
    between the current frame and the previous frame, and between the current frame
    and the next frame. We then take these frame differences and apply a bitwise **AND**
    operator. This will highlight the moving parts in the image. If you just compute
    the difference between the current frame and the previous frame, it tends to be
    noisy. Hence, we need to use the bitwise AND operator between successive frame
    differences to get some stability when we see the moving objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the function that can extract and return a frame from the webcam:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, it''s pretty straightforward. We just need to resize the frame
    and convert it to grayscale. Now that we have the helper functions ready, let''s
    look at the main function and see how it all comes together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How well does it work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we can see, frame differencing addresses a couple of important problems
    we faced earlier. It can quickly adapt to lighting changes or camera movement.
    If an object comes in to the frame and stays there, it will not be detected in
    future frames. One of the main concerns of this approach is about detecting uniformly
    colored objects. It can only detect the edges of a uniformly colored object. The
    reason is that a large portion of this object will result in very low pixel differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f8d809a-d3a8-4829-ad24-ef555b51ccb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say this object moved slightly. If we compare this with the previous
    frame, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8abbc81-a938-4c4c-b582-c1547bf10bb7.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, we have very few pixels that are labeled on that object. Another concern
    is that it is difficult to detect whether an object moving toward the camera or
    away from it.
  prefs: []
  type: TYPE_NORMAL
- en: The Mixture of Gaussians approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we talk about **Mixture of Gaussians** (**MOG**), let''s see what a
    **mixture model** is. A mixture model is just a statistical model that can be
    used to represent the presence of subpopulations within our data. We don''t really
    care about what category each data point belongs to. All we need to do is identify
    that the data has multiple groups inside it. If we represent each subpopulation
    using the Gaussian function, then it''s called Mixture of Gaussians. Let''s consider
    the following photograph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ceed2f7-c122-4df1-804e-7803c4343330.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, as we gather more frames in this scene, every part of the image will gradually
    become a part of the background model. This is what we discussed earlier in the
    *Frame differencing* section as well. If a scene is static, the model adapts itself
    to make sure the background model is updated. The foreground mask, which is supposed
    to represent the foreground object, looks like a black image at this point because
    every pixel is part of the background model.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV has multiple algorithms implemented for the Mixture of Gaussians approach.
    One of them is called **MOG** and the other is called **MOG2:** refer to this
    link for a detailed explanation: [http://docs.opencv.org/master/db/d5c/tutorial_py_bg_subtraction.html#gsc.tab=0](http://docs.opencv.org/master/db/d5c/tutorial_py_bg_subtraction.html#gsc.tab=0).
    You will also be able check out the original research papers that were used to
    implement these algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s wait for some time and then introduce a new object into the scene. Let''s
    look at what the new foreground mask looks like, using the MOG2 approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ee38a6f-6e03-4193-9f5d-3ced8ffc8ead.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the new objects are being identified correctly. Let''s look
    at the interesting part of the code (you can get the full code in the `.cpp` files):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What happened in the code?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s quickly go through the code and see what''s happening there. We use
    the Mixture of Gaussians model to create a background subtractor object. This
    object represents the model that will be updated as and when we encounter new
    frames from the webcam. We initialized two background subtraction modelsâ€”`BackgroundSubtractorMOG`
    and `BackgroundSubtractorMOG2`. They represent two different algorithms that are
    used for background subtraction. The first one refers to the paper by *P*. *KadewTraKuPong*
    and *R*. *Bowden,* titled *An Improved Adaptive Background Mixture Model for Real-time
    Tracking with Shadow Detection*. You can check it out at [http://personal.ee.surrey.ac.uk/Personal/R.Bowden/publications/avbs01/avbs01.pdf](http://personal.ee.surrey.ac.uk/Personal/R.Bowden/publications/avbs01/avbs01.pdf).
    The second one refers to the paper by *Z*. *Zivkovic,* titled *Improved Adaptive
    Gaussian Mixture Model for Background Subtraction*. You can check it out here:
    [http://www.zoranz.net/Publications/zivkovic2004ICPR.pdf](http://www.zoranz.net/Publications/zivkovic2004ICPR.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start an infinite `while` loop and continuously read the input frames from
    the webcam. With each frame, we update the background model, as indicated in the
    following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The background model gets updated in these steps. Now, if a new object enters
    the scene and stays there, it will become part of the background model. This helps
    us overcome one of the biggest shortcomings of the **naive** background subtraction
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Morphological image processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed earlier, background subtraction methods are affected by many
    factors. Their accuracy depends on how we capture the data and how it's processed.
    One of the biggest factors that affects these algorithms is the noise level. When
    we say **noise**, we are talking about things such as graininess in an image and
    isolated black/white pixels. These issues tend to affect the quality of our algorithms.
    This is where morphological image processing comes into play. Morphological image
    processing is used extensively in a lot of real-time systems to ensure the quality
    of the output. Morphological image processing refers to processing the shapes
    of features in the image; for example, you can make a shape thicker or thinner.
    Morphological operators rely not on how the pixels are ordered in an image, but
    on their values. This is why they are really well suited to manipulating shapes
    in binary images. Morphological image processing can be applied to grayscale images
    as well, but the pixel values will not matter much.
  prefs: []
  type: TYPE_NORMAL
- en: What's the underlying principle?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Morphological operators use a structuring element to modify an image. What is
    a structuring element? A structuring element is basically a small shape that can
    be used to inspect a small region in the image. It is positioned at all the pixel
    locations in the image so that it can inspect that neighborhood. We basically
    take a small window and overlay it on a pixel. Depending on the response, we take
    appropriate action at that pixel location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f6ac4ba-93be-4e3f-8517-4b5968ced5d9.png)'
  prefs: []
  type: TYPE_IMG
- en: We are going to apply a bunch of morphological operations to this image to see
    how the shape changes.
  prefs: []
  type: TYPE_NORMAL
- en: Slimming the shapes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We achieve this effect using an operation called **erosion**. This is the operation
    that makes a shape thinner by peeling the boundary layers of all the shapes in
    the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ec264b3-8600-4f74-99fb-308208b2475f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the function that performs morphological erosion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can check out the full code in the `.cpp` files to understand how to use
    this function. We basically build a structuring element using a built-in OpenCV
    function. This object is used as a probe to modify each pixel based on certain
    conditions. These conditions refer to what's happening around that particular
    pixel in the image. For example, is it surrounded by white pixels? Or is it surround
    by black pixels? Once we have an answer, we take the appropriate action.
  prefs: []
  type: TYPE_NORMAL
- en: Thickening the shapes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use an operation called **dilation** to achieve thickening. This is the
    operation that makes a shape thicker by adding boundary layers to all the shapes
    in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d747f0d4-9b2d-4ed4-b07e-f7b8ebe05435.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the code to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Other morphological operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here are some other interesting morphological operators. Let's look at the output
    image first. We can look at the code at the end of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Morphological opening
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the operation that **opens** a shape. This operator is frequently used
    for noise removal in images. It''s basically erosion followed by dilation. Morphological
    opening removes small objects from the foreground in the image by placing them
    in the background:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33df80c3-af99-436a-ac63-dec98b9d9492.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the function to perform morphological opening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As we can see here, we apply **erosion** and **dilation** on the image to perform
    morphological opening.
  prefs: []
  type: TYPE_NORMAL
- en: Morphological closing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the operation that **closes** a shape by filling the gaps, as shown
    in the following screenshot. This operation is also used for noise removal. It''s
    basically dilation followed by erosion. This operation removes tiny holes in the
    foreground by changing small objects in the background into the foreground:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e089e366-df33-45b3-a766-5bdd5b8194c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s quickly look at the function to perform morphological closing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Drawing the boundary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We achieve this using a morphological gradient. This is the operation that
    draws the boundary around a shape by taking the difference between the dilation
    and erosion of an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c644866-9615-48d3-83a2-e7a56e5c51b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the function to perform morphological gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Top Hat transform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This transform extracts finer details from the images. This is the difference
    between the input image and its morphological opening. This gives us the objects
    in the image that are smaller than the structuring element and brighter than the
    surroundings. Depending on the size of the structuring element, we can extract
    various objects in the given image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc19cfb9-b944-406d-8c56-1f397c02d459.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you look at the output image carefully, you can see those black rectangles.
    It means that the structuring element was able to fit in there, and so those regions
    are blackened out. Here is the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Black Hat transform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This transform extract finer details from the image as well. This is the difference
    between the morphological closing of an image and the image itself. This gives
    us the objects in the image that are smaller than the structuring element and
    darker than its surroundings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aad1cad8-45b7-4ece-8b18-10f973bc1d45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the function to perform a Black Hat transform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the algorithms that are used for background
    modeling and morphological image processing. We discussed naive background subtraction
    and its limitations. We looked at how to get motion information using frame differencing
    and how it can be limiting when we want to track different types of objects. This
    led to our discussion about the Mixture of Gaussians. We discussed the formula
    and how we can implement it. We then discussed morphological image processing,
    which can be used for various purposes, and different operations were covered
    to show the use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to discuss object tracking and the various
    techniques that can be used to do it.
  prefs: []
  type: TYPE_NORMAL
