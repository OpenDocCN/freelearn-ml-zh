<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer443">
    <h1 class="chapterNumber">9</h1>
    <h1 class="chapterTitle" id="_idParaDest-216">Finding Groups of Data – Clustering with k-means</h1>
    <p class="normal">Have you ever spent time watching a crowd? If so, you have likely seen some recurring personalities. Perhaps a certain type of person, identified by a freshly pressed suit and a briefcase, comes to typify the “fat cat” business executive. A 20-something wearing skinny jeans, a flannel shirt, and sunglasses might be dubbed a “hipster,” while a woman unloading children from a minivan may be labeled a “soccer mom.”</p>
    <p class="normal">Of course, these types of stereotypes are dangerous to apply to individuals, as no two people are exactly alike. Yet, understood as a way to describe a collective, the labels capture some underlying aspect of similarity shared among the individuals within the group.</p>
    <p class="normal">As you will soon learn, the act of clustering, or spotting patterns in data, is not much different from spotting patterns in groups of people. This chapter describes:</p>
    <ul>
      <li class="bulletList">The ways clustering tasks differ from the classification tasks we examined previously</li>
      <li class="bulletList">How clustering defines a group and how such groups are identified by k-means, a classic and easy-to-understand clustering algorithm</li>
      <li class="bulletList">The steps needed to apply clustering to a real-world task of identifying marketing segments among teenage social media users</li>
    </ul>
    <p class="normal">Before jumping into action, we’ll begin by taking an in-depth look at exactly what clustering entails.</p>
    <h1 class="heading-1" id="_idParaDest-217">Understanding clustering</h1>
    <p class="normal">Clustering is <a id="_idIndexMarker1020"/>an unsupervised machine learning task that automatically divides the data into <strong class="keyWord">clusters</strong>, or <a id="_idIndexMarker1021"/>groups of similar items. It does this without having been told how the groups should look ahead of time. Because we do not tell the machine specifically what we’re looking for, clustering is used for knowledge discovery rather than prediction. It provides an insight into the natural groupings found within data.</p>
    <p class="normal">Without advanced knowledge of what comprises a cluster, how can a computer possibly know where one group ends and another begins? The answer is simple: clustering is guided by the principle that items inside a cluster should be very similar to each other, but very different from those outside. The definition of similarity might vary across applications, but the basic idea is always the same: group the data such that related elements are placed together.</p>
    <p class="normal">The resulting clusters can then be used for action. For instance, you might find clustering methods employed in applications such as:</p>
    <ul>
      <li class="bulletList">Segmenting customers into groups with similar demographics or buying patterns for targeted marketing campaigns</li>
      <li class="bulletList">Detecting anomalous behavior, such as unauthorized network intrusions, by identifying patterns of use falling outside known clusters</li>
      <li class="bulletList">Simplifying extremely “wide” datasets—those with a large number of features—by creating a small number of categories to describe rows with relatively homogeneous values of the features</li>
    </ul>
    <p class="normal">Overall, clustering is useful whenever diverse and varied data can be exemplified by a much smaller number of groups. It results in meaningful and actionable data structures, which reduce complexity and provide insight into patterns of relationships.</p>
    <h2 class="heading-2" id="_idParaDest-218">Clustering as a machine learning task</h2>
    <p class="normal">Clustering is<a id="_idIndexMarker1022"/> somewhat different from the classification, numeric prediction, and pattern detection tasks we’ve examined so far. In each of these tasks, the goal was to build a model that relates features to an outcome, or to relate some features to other features. Each of these tasks describes existing patterns within data. In contrast, the goal of clustering is to create new data. In clustering, unlabeled examples are given a new cluster label, which has been inferred entirely from the relationships within the data. For this reason, you will sometimes see a clustering task referred <a id="_idIndexMarker1023"/>to as <strong class="keyWord">unsupervised classification</strong> because, in a sense, it classifies unlabeled examples.</p>
    <p class="normal">The catch is that the class labels obtained from an unsupervised classifier are without intrinsic meaning. Clustering will tell you which groups of examples are closely related—for instance, it might return groups A, B, and C—but it’s up to you to apply an actionable and meaningful label, and to tell the story of what makes an “A” different from a “B.” To see how this impacts the clustering task, let’s consider a simple hypothetical example.</p>
    <p class="normal">Suppose you <a id="_idIndexMarker1024"/>were organizing a conference on the topic of data science. To facilitate professional networking and collaboration, you planned to seat people at one of three tables according to their research specialties. Unfortunately, after sending out the conference invitations, you realize that you forgot to include a survey asking which discipline the attendee would prefer to be seated within.</p>
    <p class="normal">In a stroke of brilliance, you realize that you might be able to infer each scholar’s research specialty by examining their publication history. To this end, you begin collecting data on the number of articles each attendee has published in computer science-related journals and the number of articles published in math- or statistics-related journals. Using the data collected for the scholars, you create a scatterplot:</p>
    <figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" src="../Images/B17290_09_01.png"/></figure>
    <p class="packt_figref">Figure 9.1: Visualizing scholars by their math and computer science publication data</p>
    <p class="normal">As expected, there seems to be a pattern. We might guess that the upper-left corner, which represents people with many computer science publications but few articles on math, is a cluster of computer scientists. Following this logic, the lower-right corner might be a group of mathematicians or statisticians. Similarly, the upper-right corner, those with both math and computer science experience, may be machine learning experts. </p>
    <p class="normal">Applying these labels results in the following visualization:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_09_02.png"/></figure>
    <p class="packt_figref">Figure 9.2: Clusters can be identified based on presumptions about the scholars in each group</p>
    <p class="normal">Our <a id="_idIndexMarker1025"/>groupings were formed visually; we simply identified clusters as closely grouped data points. Yet, despite the seemingly obvious groupings, without personally asking each scholar about their academic specialty, we have no way to know whether the groups are truly homogeneous. The labels are qualitative, presumptive judgments about the types of people in each group, based on a limited set of quantitative data.</p>
    <p class="normal">Rather than defining the group boundaries subjectively, it would be nice to use machine learning to define them objectively. Given the axis-parallel splits in the previous figure, our problem seems like an obvious application for decision trees, as described in <em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>. This would provide us with a clean rule like “if a scholar has few math publications, then they are a computer science expert.” Unfortunately, there’s a problem with this plan. Without data on the true class value for each point, a supervised learning algorithm would have no ability to learn such a pattern, as it would have no way of knowing what splits would result in homogenous groups.</p>
    <p class="normal">In contrast to supervised learning, clustering algorithms use a process very similar to what we did by visually inspecting the scatterplot. Using a measure of how closely the examples are related, homogeneous groups can be identified. In the next section, we’ll start<a id="_idIndexMarker1026"/> looking at how clustering algorithms are implemented.</p>
    <div class="packt_tip">
      <p class="normal">This example highlights an interesting application of clustering. If you begin with unlabeled data, you can use clustering to create class labels. From there, you could apply a supervised learner such as decision trees to find the most important predictors of these classes. This is an example of semi-supervised learning as described in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introducing Machine Learning</em>.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-219">Clusters of clustering algorithms</h2>
    <p class="normal">Just as there are<a id="_idIndexMarker1027"/> many approaches to building a predictive model, there are multiple approaches to performing the descriptive task of clustering. Many such methods are listed on the following site, the CRAN task view for clustering: <a href="https://cran.r-project.org/view=Cluster"><span class="url">https://cran.r-project.org/view=Cluster</span></a>. Here, you will find numerous R packages used for discovering natural groupings in data. The various algorithms are distinguished mainly by two characteristics:</p>
    <ul>
      <li class="bulletList">The <strong class="keyWord">similarity metric</strong>, which<a id="_idIndexMarker1028"/> provides the quantitative measure of how closely two examples are related</li>
      <li class="bulletList">The <strong class="keyWord">agglomeration function</strong>, which <a id="_idIndexMarker1029"/>governs the process of assigning examples to clusters based on their similarity</li>
    </ul>
    <p class="normal">Even though there may be subtle differences between the approaches, they can of course be clustered in various ways. Multiple such typologies exist, but a simple three-part framework helps understand the main distinctions. Using this approach, the three main clusters of clustering algorithms, listed from simplest to most sophisticated, are as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Hierarchical methods</strong>, which<a id="_idIndexMarker1030"/> create a family tree-style hierarchy that positions the most similar examples more closely in the graph structure</li>
      <li class="bulletList"><strong class="keyWord">Partition-based methods</strong>, which treat <a id="_idIndexMarker1031"/>the examples as points in multidimensional space, and attempt to find boundaries in this space that lead to relatively homogenous groups</li>
      <li class="bulletList"><strong class="keyWord">Model or density-based methods</strong>, which <a id="_idIndexMarker1032"/>rely on statistical principles and/or the density of points to discover fuzzy boundaries between the clusters; in some cases, examples may be partially assigned to multiple clusters, or even to no cluster at all</li>
    </ul>
    <p class="normal">Despite <strong class="keyWord">hierarchical clustering</strong> being <a id="_idIndexMarker1033"/>the simplest of the methods, it is not without a pair of interesting upsides. Firstly, it results in a hierarchical graph visualization<a id="_idIndexMarker1034"/> called a <strong class="keyWord">dendrogram</strong>, which depicts the associations between examples such that the most similar examples are positioned more closely in the hierarchy. </p>
    <p class="normal">This can be a useful tool to understand which examples and subsets of examples are the most tightly grouped. Secondly, hierarchical clustering does not require a predefined expectation of how many clusters exist in the dataset. Instead, it implements a process in which, at one extreme, every example is included in a single, giant cluster with all other examples; at the other extreme, every example is found in a tiny cluster containing only itself; and in between, examples may be included in other clusters of varying sizes.</p>
    <p class="normal"><em class="italic">Figure 9.3</em> illustrates a <a id="_idIndexMarker1035"/>hypothetical dendrogram for a simple dataset containing eight examples, labeled A through H. Notice that the most closely related examples (depicted via proximity on the <em class="italic">x</em> axis) are linked more closely as siblings in the diagram. For instance, examples D and E are the most similar and thus are the first to be grouped. However, all eight examples are eventually linked to one large cluster, or may be included in any number of clusters in between. Slicing the dendrogram horizontally at different positions creates varying numbers of clusters, as shown for three and five clusters:</p>
    <figure class="mediaobject"><img alt="Chart, diagram, box and whisker chart  Description automatically generated" src="../Images/B17290_09_03.png"/></figure>
    <p class="packt_figref">Figure 9.3: Hierarchical clustering produces a dendrogram that depicts the natural groupings for the desired number of clusters</p>
    <p class="normal">The dendrogram<a id="_idIndexMarker1036"/> for hierarchical clustering can be grown using “bottom-up” or “top-down” approaches. The former is called <strong class="keyWord">agglomerative clustering</strong>, and<a id="_idIndexMarker1037"/> begins with each example in its own cluster, then connects the most similar examples first until all examples are connected in a single cluster. The latter is<a id="_idIndexMarker1038"/> called <strong class="keyWord">divisive clustering</strong>, and begins with a single large cluster and ends with all examples in their own individual clusters. </p>
    <p class="normal">When connecting examples to groups of examples, different metrics may be used, such as the example’s similarity to the most similar, least similar, or average member of the group. A more complex metric known as <strong class="keyWord">Ward’s method</strong> does<a id="_idIndexMarker1039"/> not use similarity between examples, but instead considers a measure of cluster homogeneity to construct the linkages. In any case, the result is a hierarchy that aims to group the most similar examples into any number of subgroups. </p>
    <p class="normal">The flexibility of the hierarchical clustering technique comes at a cost, which is computational complexity due to the need to compute the similarity between each example and every other. As the number of examples (<em class="italic">N</em>) grows, the number of calculations grows as <em class="italic">N*N = N</em><sup class="superscript-italic" style="font-style: italic;">2</sup>, as does the memory needed for the similarity matrix that stores the result. For this reason, hierarchical clustering is used only on very small datasets and is not demonstrated in this chapter. However, the <code class="inlineCode">hclust()</code> function included in R’s <code class="inlineCode">stats</code> package provides a simple implementation, which is installed with R by default.</p>
    <div class="packt_tip">
      <p class="normal">Clever implementations of divisive clustering have the potential to be slightly more computationally efficient than agglomerative clustering as the algorithm may stop early if it is unnecessary to create larger numbers of clusters. This being said, both agglomerative and divisive clustering are examples of “greedy” algorithms as defined in <em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>, because they use data on a first-come, first-served basis and are thus not guaranteed to produce the overall optimal set of clusters for a given dataset.</p>
    </div>
    <p class="normal"><strong class="keyWord">Partition-based clustering</strong> methods <a id="_idIndexMarker1040"/>have a distinct efficiency advantage over hierarchical clustering in that they apply heuristic methods to divide the data into clusters without the need to evaluate the similarity between every pair of examples. We’ll explore a widely used partition-based method in greater detail shortly, but for now it suffices to understand that this method is concerned with finding boundaries between clusters rather than connecting examples to one another—an approach that requires far fewer comparisons between examples. This heuristic can be quite computationally efficient, but one caveat is that it is somewhat rigid or even arbitrary when it comes to group assignments. For example, if five clusters are requested, it will partition examples into all five clusters; if some examples fall on the boundary between two clusters, these will be placed somewhat arbitrarily yet firmly into one cluster or the other. Similarly, if four or six clusters might have split the data better, this would not be as apparent as it would be with the hierarchical clustering dendrogram.</p>
    <p class="normal">The more <a id="_idIndexMarker1041"/>sophisticated <strong class="keyWord">model-based and density-based clustering</strong> methods <a id="_idIndexMarker1042"/>address some of these issues of inflexibility by estimating the probability that an example belongs to each cluster, rather than merely assigning it to a single cluster. Some of them may allow the cluster boundaries to follow the natural patterns identified in the data rather than forcing a strict divide between the groups. Model-based approaches often assume a statistical distribution from which the examples are believed to have been pulled.</p>
    <p class="normal">One such approach, known<a id="_idIndexMarker1043"/> as <strong class="keyWord">mixture modeling</strong>, attempts to disentangle datasets composed of examples pulled from a mixture of statistical distributions—typically Gaussian (the normal bell curve). For example, imagine you have a dataset composed of voice data from a mixture of male and female vocal registers, as depicted in <em class="italic">Figure 9.4</em> (note that the distributions are hypothetical and are not based on real-world data). Although there is some overlap between the two, the average male tends to have a lower register than the average female.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_09_04.png"/></figure>
    <p class="packt_figref">Figure 9.4: Mixture modeling assigns each example a probability of belonging to one of the underlying distributions</p>
    <p class="normal">Given the <a id="_idIndexMarker1044"/>unlabeled overall distribution (the bottom part of the figure), a mixture model would be capable of assigning a probability that any given example belongs to the cluster of males or the cluster of females, incredibly, without ever having been trained separately on male or female voices in the top part of the figure! This is possible by discovering the statistical parameters like the mean and standard deviation that are most likely to have generated the observed overall distribution, under the assumption that a specific number of distinct distributions were involved—in this case, two Gaussian distributions.</p>
    <p class="normal">As an unsupervised method, the mixture model would have no way of knowing that the left distribution is males and the right distribution is females, but this would be readily apparent to a human observer comparing the records with a high likelihood of males being in the left cluster versus the right one. The downside of this technique is that not only does it require knowledge of how many distributions are involved but it also requires an assumption of the types of distributions. This may be too rigid for many real-world clustering tasks.</p>
    <p class="normal">Another powerful clustering technique<a id="_idIndexMarker1045"/> called <strong class="keyWord">DBSCAN</strong> is named after the “density-based spatial clustering of applications with noise” approach it uses to identify natural clusters in data. This award-winning technique is incredibly flexible and does well with many of the challenges of clustering, such as adapting to the dataset’s natural number of clusters, being flexible about the boundaries between clusters, and not assuming a particular statistical distribution for the data.</p>
    <p class="normal">While the implementation details are beyond the scope of this book, the DBSCAN algorithm can be understood intuitively as a process of creating neighborhoods of examples that are all within a given radius of other examples in the cluster. A predefined number of <strong class="keyWord">core points</strong> within<a id="_idIndexMarker1046"/> a specified radius forms the initial cluster nucleus, and points that are within a specified radius of any of the core points are then added to the cluster and comprise the outermost boundary of the cluster. Unlike many other clustering algorithms, some examples will not be assigned any cluster at all, as any points that are not close enough to a core point will be treated as noise.</p>
    <p class="normal">Although DBSCAN is powerful and flexible, it may require experimentation to optimize the parameters to fit the data, such as the number of points comprising the core, or the allowed radius between points, which adds time complexity to the machine learning project. Of course, just because model-based methods are more sophisticated does not imply they are the best fit for every clustering project. As we will see throughout the remainder of this chapter, a simpler partition-based method can perform surprisingly well on a challenging real-world clustering task.</p>
    <div class="note">
      <p class="normal">Although mixture modeling and DBSCAN are not demonstrated in this chapter, there are R packages that can be used to apply these methods to your own data. The <code class="inlineCode">mclust</code> package fits a model to mixtures of Gaussian distributions, and the <code class="inlineCode">dbscan</code> package provides a fast implementation of the DBSCAN algorithm.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-220">The k-means clustering algorithm</h2>
    <p class="normal">The <strong class="keyWord">k-means algorithm</strong> is perhaps <a id="_idIndexMarker1047"/>the most often used clustering method and is an example of a partition-based clustering approach. Having been studied for several decades, it serves as the foundation for many more sophisticated clustering techniques. If you understand the simple principles it uses, you will have the knowledge needed to understand nearly any clustering algorithm in use today.</p>
    <div class="note">
      <p class="normal">As k-means has evolved over time, there are many implementations of the algorithm. An early approach is described in <em class="italic">A k-means clustering algorithm, Hartigan, J.A., Wong, M.A., Applied Statistics, 1979, Vol. 28, pp. 100-108</em>.</p>
    </div>
    <p class="normal">Even though <a id="_idIndexMarker1048"/>clustering methods have evolved since the inception of k-means, this is not to imply that k-means is obsolete. In fact, the method may be more popular now than ever. The following table lists some reasons why <a id="_idIndexMarker1049"/>k-means<a id="_idIndexMarker1050"/> is still used widely:</p>
    <table class="table-container" id="table001-7">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Uses simple principles that can be explained in non-statistical terms</li>
              <li class="bulletList">Highly flexible and can be adapted with simple adjustments to address many of its shortcomings</li>
              <li class="bulletList">Performs well enough under many real-world use cases</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Not as sophisticated as more modern clustering algorithms</li>
              <li class="bulletList">Because it uses an element of random chance, it is not guaranteed to find the optimal set of clusters</li>
              <li class="bulletList">Requires a reasonable guess as to how many clusters naturally exist in the data</li>
              <li class="bulletList">Not ideal for non-spherical clusters or clusters of widely varying density</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">If the name k-means <a id="_idIndexMarker1051"/>sounds familiar to you, you may be recalling the <strong class="keyWord">k-nearest neighbors</strong> (<strong class="keyWord">k-NN</strong>) algorithm presented in <em class="chapterRef">Chapter 3</em>, <em class="italic">Lazy Learning – Classification Using Nearest Neighbors</em>. As you will soon see, k-means has more in common with k-NN than just the letter k.</p>
    <p class="normal">The k-means algorithm assigns each of the <em class="italic">n</em> examples to one of the <em class="italic">k</em> clusters, where <em class="italic">k</em> is a number that has been determined ahead of time. The goal is to minimize the differences in feature values of examples within each cluster and maximize the differences between clusters.</p>
    <p class="normal">Unless <em class="italic">k</em> and <em class="italic">n</em> are extremely small, it is not feasible to compute the optimal clusters across all possible combinations of examples. Instead, the algorithm uses a heuristic process that <a id="_idIndexMarker1052"/>finds <strong class="keyWord">locally optimal</strong> solutions. Put simply, this means that it starts with an initial guess for the cluster assignments and then modifies the assignments slightly to see if the changes improve the homogeneity within the clusters.</p>
    <p class="normal">We will cover the process in depth shortly, but the algorithm essentially involves two phases. First, it assigns examples to an initial set of <em class="italic">k</em> clusters. Then, it updates the assignments by adjusting the cluster boundaries according to the examples that currently fall into the cluster. The process of updating and assigning occurs several times until changes no longer improve the cluster fit. At this point, the process stops, and the clusters are finalized.</p>
    <div class="packt_tip">
      <p class="normal">Due to the heuristic nature of k-means, you may end up with somewhat different results by making only slight changes to the starting conditions. If the results vary dramatically, this could indicate a problem. For instance, the data may not have natural groupings, or the value of <em class="italic">k</em> has been poorly chosen. With this in mind, it’s a good idea to try a cluster analysis more than once to test the robustness of your findings.</p>
    </div>
    <p class="normal">To see how<a id="_idIndexMarker1053"/> the process of assigning and updating works in practice, let’s revisit the case of the hypothetical data science conference. Though this is a simple example, it will illustrate the basics of how k-means operates under the hood.</p>
    <h3 class="heading-3" id="_idParaDest-221">Using distance to assign and update clusters </h3>
    <p class="normal">As with k-NN, k-means<a id="_idIndexMarker1054"/> treats feature values as coordinates in a multidimensional feature space. For the conference data, there are only two features, so we can represent the feature space as a two-dimensional scatterplot as depicted previously.</p>
    <p class="normal">The <a id="_idIndexMarker1055"/>k-means algorithm begins by choosing <em class="italic">k</em> points in the feature space to serve as the cluster centers. These centers are the catalyst that spurs the remaining examples to fall into place. Often, the points are chosen by selecting <em class="italic">k</em> random examples from the training dataset. Because we hope to identify three clusters, using this method, <em class="italic">k = 3</em> points will be selected at random. </p>
    <p class="normal">These points are indicated by the star, triangle, and diamond in <em class="italic">Figure 9.5</em>:</p>
    <figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" src="../Images/B17290_09_05.png"/></figure>
    <p class="packt_figref">Figure 9.5: k-means clustering begins by selecting k random cluster centers</p>
    <p class="normal">It’s <a id="_idIndexMarker1056"/>worth noting that although the three cluster centers in the preceding diagram happen to be widely spaced apart, this will not always necessarily be the case. Because the starting points are selected at random, the three centers could have just as easily been three adjacent points. Combined <a id="_idIndexMarker1057"/>with the fact that the k-means algorithm is highly sensitive to the starting position of the cluster centers, a good or bad set of initial cluster centers may have a substantial impact on the final set of clusters.</p>
    <p class="normal">To address this problem, k-means can be modified to use different methods for choosing the initial centers. For example, one variant chooses random values occurring anywhere in the feature space rather than only selecting among values observed in the data. Another option is to skip this step altogether; by randomly assigning each example to a cluster, the algorithm can jump ahead immediately to the update phase. Each of these approaches adds a particular bias to the final set of clusters, which you may be able to use to improve your results.</p>
    <div class="note">
      <p class="normal">In 2007, an algorithm <a id="_idIndexMarker1058"/>called <strong class="keyWord">k-means++</strong> was introduced, which proposes an alternative method for selecting the initial cluster centers. It purports to be an efficient way to get much closer to the optimal clustering solution while reducing the impact of random chance. For more information, see <em class="italic">k-means++: The advantages of careful seeding, Arthur, D, Vassilvitskii, S, Proceedings of the eighteenth annual ACM-SIAM symposium on discrete algorithms, 2007, pp. 1,027–1,035</em>.</p>
    </div>
    <p class="normal">After choosing the initial cluster centers, the other examples are assigned to the cluster center that is nearest according to a distance function, which is used as a measure of similarity. You may recall that we used distance functions as similarity measures while learning about the k-NN supervised learning algorithm. Like k-NN, k-means traditionally uses Euclidean distance, but other distance functions can be used if desired. </p>
    <div class="packt_tip">
      <p class="normal">Interestingly, any function that returns a numeric measure of similarity could be used instead of a traditional distance function. In fact, k-means could even be adapted to cluster images or text documents by using a function that measures the similarity of pairs of images or texts.</p>
    </div>
    <p class="normal">To <a id="_idIndexMarker1059"/>apply the distance function, recall <a id="_idIndexMarker1060"/>that if <em class="italic">n</em> indicates the number of features, the formula for Euclidean distance between example <em class="italic">x</em> and example <em class="italic">y</em> is as follows:</p>
    <p class="center"><img alt="" src="../Images/B17290_09_001.png"/></p>
    <p class="normal">For instance, to compare a guest with five computer science publications and one math publication to a guest with zero computer science papers and two math papers, we could compute this in R as:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-built_in">sqrt</span><span class="hljs-punctuation">((</span><span class="hljs-number">5</span> <span class="hljs-operator">-</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span><span class="hljs-operator">^</span><span class="hljs-number">2</span> <span class="hljs-operator">+</span> <span class="hljs-punctuation">(</span><span class="hljs-number">1</span> <span class="hljs-operator">-</span> <span class="hljs-number">2</span><span class="hljs-punctuation">)</span><span class="hljs-operator">^</span><span class="hljs-number">2</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 5.09902
</code></pre>
    <p class="normal">Using the distance function in this way, we find the distance between each example and each cluster center. Each example is then assigned to the nearest cluster center.</p>
    <div class="packt_tip">
      <p class="normal">Keep in mind that because we are using distance calculations, all the features need to be numeric, and the values should be normalized to a standard range ahead of time. The methods presented in <em class="chapterRef">Chapter 3</em>, <em class="italic">Lazy Learning – Classification Using Nearest Neighbors</em>, will prove helpful for this task.</p>
    </div>
    <p class="normal">As shown in the following figure, the three cluster centers partition the examples into three partitions labeled <em class="italic">Cluster A</em>, <em class="italic">Cluster B</em>, and <em class="italic">Cluster C</em>. The dashed lines indicate the boundaries for<a id="_idIndexMarker1061"/> the <strong class="keyWord">Voronoi diagram</strong> created by the cluster centers. The Voronoi diagram indicates the areas that are closer to one cluster center than any other; the vertex where <a id="_idIndexMarker1062"/>all three boundaries meet is the maximal distance from all three cluster centers. </p>
    <p class="normal">Using these boundaries, we can easily see the regions claimed by each of the initial k-means seeds:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_09_06.png"/></figure>
    <p class="packt_figref">Figure 9.6: The initial cluster centers create three groups of “nearest” points</p>
    <p class="normal">Now that<a id="_idIndexMarker1063"/> the initial assignment phase has been completed, the k-means algorithm proceeds to the update phase. The first step of updating the clusters involves shifting the initial centers to a new location, known as<a id="_idIndexMarker1064"/> the <strong class="keyWord">centroid</strong>, which is calculated as the average position of the points currently assigned to that cluster. The following figure illustrates how as the cluster centers shift to the new centroids, the boundaries in the Voronoi diagram also shift, and a point that was once in <em class="italic">Cluster B</em> (indicated by an arrow) is added to <em class="italic">Cluster A</em>:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_09_07.png"/></figure>
    <p class="packt_figref">Figure 9.7: The update phase shifts the cluster centers, which causes the reassignment of one point</p>
    <p class="normal">As a result of this reassignment, the k-means algorithm will continue through another update<a id="_idIndexMarker1065"/> phase. After<a id="_idIndexMarker1066"/> shifting the cluster centroids, updating the cluster boundaries, and reassigning points into new clusters (as indicated by arrows), the figure looks like this:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_09_08.png"/></figure>
    <p class="packt_figref">Figure 9.8: After another update, two more points are reassigned to the nearest cluster center</p>
    <p class="normal">Because two more points were reassigned, another update must occur, which moves the centroids and updates the cluster boundaries. However, because these changes result in no reassignments, the k-means algorithm stops. The cluster assignments are now final:</p>
    <figure class="mediaobject"><img alt="Chart  Description automatically generated with medium confidence" src="../Images/B17290_09_09.png"/></figure>
    <p class="packt_figref">Figure 9.9: Clustering stops after the update phase results in no new cluster assignments</p>
    <p class="normal">The final clusters can be reported in one of two ways. First, you might simply report the cluster <a id="_idIndexMarker1067"/>assignments of A, B, or C for each example. Alternatively, you could report the coordinates<a id="_idIndexMarker1068"/> of the cluster centroids after the final update. </p>
    <p class="normal">Given either reporting method, you can compute the other; you can calculate the centroids using the coordinates of each cluster’s examples, or you can use the centroid coordinates to assign each example to its nearest cluster center.</p>
    <h3 class="heading-3" id="_idParaDest-222">Choosing the appropriate number of clusters</h3>
    <p class="normal">In the<a id="_idIndexMarker1069"/> introduction to k-means, we learned that the algorithm is sensitive to the randomly chosen cluster centers. Indeed, if we had selected a different combination of three starting points in the previous example, we may have found clusters that split the data differently from what we had expected. Similarly, k-means is sensitive to the number of clusters; the choice requires a delicate balance. Setting <em class="italic">k</em> to be very large will improve the homogeneity of the clusters and, at the same time, it risks overfitting the data.</p>
    <p class="normal">Ideally, you will have <em class="italic">a priori</em> knowledge (a prior belief) about the true groupings and you can apply this information to choose the number of clusters. For instance, if you clustered movies, you might begin by setting <em class="italic">k</em> equal to the number of genres considered for the Academy Awards. In the data science conference seating problem that we worked through previously, <em class="italic">k</em> might reflect the number of academic fields of study that invitees belong to.</p>
    <p class="normal">Sometimes, <a id="_idIndexMarker1070"/>the number of clusters is dictated by business requirements or the motivation for the analysis. For example, the number of tables in the meeting hall might dictate how many groups of people should be created from the data science attendee list. Extending this idea to another business case, if the marketing department only has the resources to create three distinct advertising campaigns, it might make sense to set <em class="italic">k = 3</em> to assign all the potential customers to one of the three appeals.</p>
    <p class="normal">Without any prior knowledge, one rule of thumb suggests setting <em class="italic">k</em> equal to the square root of <em class="italic">(n / 2)</em>, where <em class="italic">n</em> is the number of examples in the dataset. However, this rule of thumb is likely to result in an unwieldy number of clusters for large datasets. Luckily, there are other quantitative methods that can assist in finding a suitable k-means cluster set.</p>
    <p class="normal">A technique known as <a id="_idIndexMarker1071"/>the <strong class="keyWord">elbow method</strong> attempts to gauge how the homogeneity or heterogeneity within the clusters changes for various values of <em class="italic">k</em>. As illustrated in the following diagrams, the homogeneity within clusters is expected to increase as additional clusters are added; similarly, the heterogeneity within clusters should decrease with more clusters. Because you could continue to see improvements until each example is in its own cluster, the goal is not to maximize homogeneity or minimize heterogeneity endlessly, but rather to find <em class="italic">k</em> such that there are diminishing returns beyond that value. This value of <em class="italic">k</em> is known <a id="_idIndexMarker1072"/>as the <strong class="keyWord">elbow point</strong> because it bends like the elbow joint of the human arm.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated with medium confidence" src="../Images/B17290_09_10.png"/></figure>
    <p class="packt_figref">Figure 9.10: The elbow is the point at which increasing k results in relatively small improvements</p>
    <p class="normal">There are numerous statistics for measuring homogeneity and heterogeneity within clusters that can be used with the elbow method (the information box that follows provides a citation for more detail). Still, in practice, it is not always feasible to iteratively test a large number of <em class="italic">k</em> values. This is in part because clustering large datasets can be fairly time-consuming; clustering the data repeatedly is even worse. Furthermore, applications requiring the exact optimal set of clusters are rare. In most clustering applications, it suffices to choose a <em class="italic">k</em> value based on convenience rather than the one that creates the most homogenous clusters.</p>
    <div class="note">
      <p class="normal">For a thorough review of the vast assortment of cluster performance measures, refer to <em class="italic">On Clustering Validation Techniques, Halkidi, M, Batistakis, Y, Vazirgiannis, M, Journal of Intelligent Information Systems, 2001, Vol. 17, pp. 107-145</em>.</p>
    </div>
    <p class="normal">The <a id="_idIndexMarker1073"/>process of setting <em class="italic">k</em> itself can sometimes lead to interesting insights. By observing how the characteristics of the clusters change as <em class="italic">k</em> changes, one might infer where the data has naturally defined boundaries. Groups that are more tightly clustered will change very little, while less homogeneous groups will form and disband over time.</p>
    <p class="normal">In general, it may be wise to spend little time worrying about getting <em class="italic">k</em> exactly right. The next example will demonstrate how even a tiny bit of subject-matter knowledge borrowed from a Hollywood film can be used to set <em class="italic">k</em> such that actionable and interesting clusters are found. As clustering is unsupervised, the task is really about what you make of it; the value is in the insights you take away from the algorithm’s findings.</p>
    <h1 class="heading-1" id="_idParaDest-223">Finding teen market segments using k-means clustering</h1>
    <p class="normal">Interacting <a id="_idIndexMarker1074"/>with friends <a id="_idIndexMarker1075"/>on a <strong class="keyWord">social networking service</strong> (<strong class="keyWord">SNS</strong>), such as Facebook, TikTok, and Instagram, has become a rite of passage for teenagers around the <a id="_idIndexMarker1076"/>world. Having a relatively large amount of disposable income, these adolescents are a coveted demographic for businesses hoping to sell snacks, beverages, electronics, entertainment, and hygiene products.</p>
    <p class="normal">The many millions of teenage consumers using such sites have attracted the attention of marketers struggling to find an edge in an increasingly competitive market. One way to gain this edge is <a id="_idIndexMarker1077"/>to identify segments of teenagers who share similar tastes, so that clients can avoid targeting advertisements to teens with no interest in the product being sold. If it costs 10 dollars to display an advertisement to 1,000 website visitors—a measure<a id="_idIndexMarker1078"/> of <strong class="keyWord">cost per impression</strong>—the advertising budget will stretch further if we are selective about who is targeted. For instance, an advertisement for sporting apparel should be<a id="_idIndexMarker1079"/> targeted to clusters of individuals more likely to have an interest in sports.</p>
    <p class="normal">Given the text of teenagers’ SNS posts, we can identify groups that share common interests such as sports, religion, or music. Clustering can automate the process of discovering the natural segments in this population. However, it will be up to us to decide whether the clusters are interesting and how to use them for advertising. Let’s try this process from start to finish.</p>
    <h2 class="heading-2" id="_idParaDest-224">Step 1 – collecting data</h2>
    <p class="normal">For <a id="_idIndexMarker1080"/>this analysis, we will be using a dataset representing a random sample of 30,000 US high school students who had profiles on a well-known SNS in 2006. To protect the users’ anonymity, the SNS will remain unnamed. However, at the time the data was collected, the SNS was a popular web destination for US teenagers. Therefore, it is reasonable to assume that the profiles represent a wide cross-section of American adolescents in 2006.</p>
    <div class="note">
      <p class="normal">I compiled this dataset while conducting my own sociological research on teenage identities at the University of Notre Dame. If you use the data for research purposes, please cite this book chapter. The full dataset is available in the Packt Publishing GitHub repository for this book with the filename <code class="inlineCode">snsdata.csv</code>. To follow along interactively, this chapter assumes you have saved this file to your R working directory.</p>
    </div>
    <p class="normal">The data was sampled evenly across four high school graduation years (2006 through to 2009) representing the senior, junior, sophomore, and freshman classes at the time of data collection. Using an automated web crawler, the full text of the SNS profiles was downloaded, and each teen’s gender, age, and number of SNS friends were recorded.</p>
    <p class="normal">A text-mining tool was used to divide the remaining SNS page content into words. From the top 500 words appearing across all pages, 36 words were chosen to represent five categories of<a id="_idIndexMarker1081"/> interests: extracurricular activities, fashion, religion, romance, and antisocial behavior. The 36 words include terms such as <em class="italic">football</em>, <em class="italic">sexy</em>, <em class="italic">kissed</em>, <em class="italic">bible</em>, <em class="italic">shopping</em>, <em class="italic">death</em>, and <em class="italic">drugs</em>. The final dataset indicates, for each person, how many times each word appeared on the person’s SNS profile.</p>
    <h2 class="heading-2" id="_idParaDest-225">Step 2 – exploring and preparing the data</h2>
    <p class="normal">We’ll <a id="_idIndexMarker1082"/>use <code class="inlineCode">read.csv()</code> to load the dataset and convert the character data into factor types:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> teens <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"snsdata.csv"</span><span class="hljs-punctuation">,</span> stringsAsFactors <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Let’s also take <a id="_idIndexMarker1083"/>a quick look at the specifics of the data. The first several lines of the <code class="inlineCode">str()</code> output are as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> str<span class="hljs-punctuation">(</span>teens<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">'data.frame':	30000 obs. of  40 variables:
 $ gradyear    : int  2006 2006 2006 2006 2006 2006 2006 ...
 $ gender      : Factor w/ 2 levels "F","M": 2 1 2 1 NA 1 1 2 ...
 $ age         : num  19 18.8 18.3 18.9 19 ...
 $ friends     : int  7 0 69 0 10 142 72 17 52 39 ...
 $ basketball  : int  0 0 0 0 0 0 0 0 0 0 ...
</code></pre>
    <p class="normal">As we had expected, the data includes 30,000 teenagers with four variables indicating personal characteristics and 36 words indicating interests.</p>
    <p class="normal">Do you notice anything strange around the <code class="inlineCode">gender</code> row? If you looked carefully, you may have noticed the <code class="inlineCode">NA</code> value, which is out of place compared to the <code class="inlineCode">1</code> and <code class="inlineCode">2</code> values. The <code class="inlineCode">NA</code> is R’s way of telling us that the record <a id="_idIndexMarker1084"/>has a <strong class="keyWord">missing value</strong>—we do not know the person’s gender. Until now, we haven’t dealt with missing data, but it can be a significant problem for many types of analyses.</p>
    <p class="normal">Let’s see how substantial this problem is. One option is to use the <code class="inlineCode">table()</code> command, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    F     M 
22054  5222
</code></pre>
    <p class="normal">Although this tells us how many <code class="inlineCode">F</code> and <code class="inlineCode">M</code> values are present, the <code class="inlineCode">table()</code> function excluded the <code class="inlineCode">NA</code> values rather than treating them as a separate category. To include the <code class="inlineCode">NA</code> values (if there are any), we simply need to add an additional parameter:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">,</span> useNA <span class="hljs-operator">=</span> <span class="hljs-string">"ifany"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    F     M  &lt;NA&gt; 
22054  5222  2724
</code></pre>
    <p class="normal">Here, we <a id="_idIndexMarker1085"/>see that 2,724 records (nine percent) have missing gender data. Interestingly, there are over four times <a id="_idIndexMarker1086"/>as many females as males in the SNS data, suggesting that males are not as inclined to use this social media website as females.</p>
    <p class="normal">If you examine the other variables in the data frame, you will find that besides <code class="inlineCode">gender</code>, only <code class="inlineCode">age</code> has missing values. For numeric features, the default output for the <code class="inlineCode">summary()</code> function includes the count of <code class="inlineCode">NA</code> values:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>age<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
  3.086  16.310  17.290  17.990  18.260 106.900    5086
</code></pre>
    <p class="normal">A total of 5,086 records (17 percent) have missing ages. Also concerning is the fact that the minimum and maximum values seem to be unreasonable; it is unlikely that a three-year-old or a 106-year-old is attending high school. To ensure that these extreme values don’t cause problems for the analysis, we’ll need to clean them up before moving on.</p>
    <p class="normal">A more plausible range of ages for high school students includes those who are at least 13 years old and not yet 20 years old. Any age value falling outside this range should be treated the same as missing data—we cannot trust the age provided. To recode the <code class="inlineCode">age</code> variable, we can use the <code class="inlineCode">ifelse()</code> function, assigning <code class="inlineCode">teen$age</code> the original value of <code class="inlineCode">teen$age</code> if the age is at least 13 and less than 20 years; otherwise, it will receive the value <code class="inlineCode">NA</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> teens<span class="hljs-operator">$</span>age <span class="hljs-operator">&lt;-</span> ifelse<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>age <span class="hljs-operator">&gt;=</span> <span class="hljs-number">13</span> <span class="hljs-operator">&amp;</span> teens<span class="hljs-operator">$</span>age <span class="hljs-operator">&lt;</span> <span class="hljs-number">20</span><span class="hljs-punctuation">,</span>
                        teens<span class="hljs-operator">$</span>age<span class="hljs-punctuation">,</span> <span class="hljs-literal">NA</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">By rechecking the <code class="inlineCode">summary()</code> output, we see that the range now follows a distribution that looks much more like an actual high school:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>age<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
  13.03   16.30   17.27   17.25   18.22   20.00    5523
</code></pre>
    <p class="normal">Unfortunately, now we’ve created an even larger missing data problem. We’ll need to find a way to deal with these values before continuing with our analysis.</p>
    <h3 class="heading-3" id="_idParaDest-226">Data preparation – dummy coding missing values</h3>
    <p class="normal">An <a id="_idIndexMarker1087"/>easy solution for handling missing values is to exclude any record with a missing value. However, if you think through the implications of this practice, you might think twice before doing so—just because it is easy does not mean it is a good idea! The problem with this approach is that even if the missingness is not extensive, you can easily exclude large portions of the data.</p>
    <p class="normal">For example, suppose that in our data, the people with <code class="inlineCode">NA</code> values for gender are completely different from those with missing age data. This would imply that by excluding those missing either gender or age, you would exclude <em class="italic">9% + 17% = 26%</em> of the data, or over 7,500 records. And this is for missing data on only two variables! The larger the number of missing values present in a dataset, the more likely it is that any given record will be excluded. Fairly soon, you will be left with a tiny subset of data, or worse, the remaining records will be systematically different or non-representative of the full population.</p>
    <p class="normal">An alternative solution for categorical data like gender is to treat a missing value as a separate category. For instance, rather than limiting to female and male, we can add an additional category for unknown gender. This allows us to utilize dummy coding, which was covered in <em class="chapterRef">Chapter 3</em>, <em class="italic">Lazy Learning – Classification Using Nearest Neighbors</em>.</p>
    <p class="normal">If you recall, dummy coding involves creating a separate binary (1 or 0) valued dummy variable for each level of a nominal feature except one, which is held out to serve as the reference group. The reason one category can be excluded is because its status can be inferred from the other categories. For instance, if someone is not female and not unknown gender, they must be male. Therefore, in this case, we need to only create dummy variables for female and unknown gender:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> teens<span class="hljs-operator">$</span>female <span class="hljs-operator">&lt;-</span> ifelse<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>gender <span class="hljs-operator">==</span> <span class="hljs-string">"F"</span> <span class="hljs-operator">&amp;</span>
                           <span class="hljs-operator">!</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">),</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> teens<span class="hljs-operator">$</span>no_gender <span class="hljs-operator">&lt;-</span> ifelse<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">),</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">As you might expect, the <code class="inlineCode">is.na()</code> function tests whether the gender is equal to <code class="inlineCode">NA</code>. Therefore, the first statement assigns <code class="inlineCode">teens$female</code> the value <code class="inlineCode">1</code> if the gender is equal to <code class="inlineCode">F</code> and the<a id="_idIndexMarker1088"/> gender is not equal to <code class="inlineCode">NA</code>; otherwise, it assigns the value <code class="inlineCode">0</code>. In the second statement, if <code class="inlineCode">is.na()</code> returns <code class="inlineCode">TRUE</code>, meaning the gender is missing, then the <code class="inlineCode">teens$no_gender</code> variable is assigned <code class="inlineCode">1</code>; otherwise, it is assigned the value <code class="inlineCode">0</code>. </p>
    <p class="normal">To confirm that we did the work correctly, let’s compare our constructed dummy variables to the original <code class="inlineCode">gender</code> variable:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>gender<span class="hljs-punctuation">,</span> useNA <span class="hljs-operator">=</span> <span class="hljs-string">"ifany"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    F     M  &lt;NA&gt; 
22054  5222  2724 
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>female<span class="hljs-punctuation">,</span> useNA <span class="hljs-operator">=</span> <span class="hljs-string">"ifany"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    0     1 
 7946 22054 
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>no_gender<span class="hljs-punctuation">,</span> useNA <span class="hljs-operator">=</span> <span class="hljs-string">"</span><span class="hljs-string">ifany"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    0     1 
27276  2724
</code></pre>
    <p class="normal">The number of <code class="inlineCode">1</code> values for <code class="inlineCode">teens$female</code> and <code class="inlineCode">teens$no_gender</code> matches the number of <code class="inlineCode">F</code> and <code class="inlineCode">NA</code> values respectively, so the coding has been performed correctly.</p>
    <h3 class="heading-3" id="_idParaDest-227">Data preparation – imputing the missing values</h3>
    <p class="normal">Next, let’s <a id="_idIndexMarker1089"/>eliminate the 5,523 missing ages. As <code class="inlineCode">age</code> is a numeric feature, it doesn’t make sense to create an additional category for unknown values—where would you rank “unknown” relative to the other ages? Instead, we’ll use a different strategy known as <strong class="keyWord">imputation</strong>, which involves filling in the missing data with a guess as to the true value.</p>
    <p class="normal">Can you think of a way we might be able to use the SNS data to make an informed guess about a teenager’s age? If you are thinking of using the graduation year, you’ve got the right idea. Most people in a graduation cohort were born within a single calendar year. If we can identify the typical age for each cohort, then we will have a reasonable approximation of the age of a student in that graduation year.</p>
    <p class="normal">One way to find a typical value is by calculating the average, or mean, value. If we try to apply the <code class="inlineCode">mean()</code> function as we have done for previous analyses, there’s a problem:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> mean<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>age<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] NA
</code></pre>
    <p class="normal">The issue is that the mean value is undefined for a vector containing missing data. As our age data contains missing values, <code class="inlineCode">mean(teens$age)</code> returns a missing value. We can correct this by adding an additional <code class="inlineCode">na.rm</code> parameter to remove the missing values before calculating the mean:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> mean<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>age<span class="hljs-punctuation">,</span> na.rm <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 17.25243
</code></pre>
    <p class="normal">This reveals <a id="_idIndexMarker1090"/>that the average student in our data is about 17 years old. This only gets us part of the way there; we actually need the average age for each graduation year. You might first attempt to calculate the mean four times, but one of the benefits of R is that there’s usually a way to avoid repeating oneself. In this case, the <code class="inlineCode">aggregate()</code> function is the tool for the job. It computes statistics for subgroups of data. Here, it calculates the mean age by graduation year after removing the <code class="inlineCode">NA</code> values:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> aggregate<span class="hljs-punctuation">(</span>data <span class="hljs-operator">=</span> teens<span class="hljs-punctuation">,</span> age <span class="hljs-operator">~</span> gradyear<span class="hljs-punctuation">,</span> mean<span class="hljs-punctuation">,</span> na.rm <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  gradyear      age
1     2006 18.65586
2     2007 17.70617
3     2008 16.76770
4     2009 15.81957
</code></pre>
    <p class="normal">The <code class="inlineCode">aggregate()</code> output is in a data frame. This would require extra work to merge back into our original data. As an alternative, we can use the <code class="inlineCode">ave()</code> function, which returns a vector with the means of each group repeated such that the resulting vector is the same length as the original vector. Where <code class="inlineCode">aggregate()</code> returns one average age for each graduation year (a total of four values), the <code class="inlineCode">ave()</code> function returns a value for all 30,000 teenagers reflecting the average age of students in that student’s graduation year (the same four values are repeated to reach a total of 30,000 values).</p>
    <p class="normal">When using the <code class="inlineCode">ave()</code> function, the first parameter is the numeric vector for which the group averages are to be computed, the second parameter is the categorical vector supplying the group assignments, and the <code class="inlineCode">FUN</code> parameter is the function to be applied to the numeric vector. In our case, we need to define a new function that computes the mean with the <code class="inlineCode">NA</code> values removed. The full command is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> ave_age <span class="hljs-operator">&lt;-</span> ave<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>age<span class="hljs-punctuation">,</span> teens<span class="hljs-operator">$</span>gradyear<span class="hljs-punctuation">,</span> FUN <span class="hljs-operator">=</span>
                  <span class="hljs-keyword">function</span><span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> mean<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">,</span> na.rm <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">))</span>
</code></pre>
    <p class="normal">To impute these means onto the missing values, we need one more <code class="inlineCode">ifelse()</code> call to use the <code class="inlineCode">ave_age</code> value only if the original age value was <code class="inlineCode">NA</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> teens<span class="hljs-operator">$</span>age <span class="hljs-operator">&lt;-</span> ifelse<span class="hljs-punctuation">(</span><span class="hljs-built_in">is.na</span><span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>age<span class="hljs-punctuation">),</span> ave_age<span class="hljs-punctuation">,</span> teens<span class="hljs-operator">$</span>age<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">summary()</code> results<a id="_idIndexMarker1091"/> show that the missing values have now been eliminated:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>teens<span class="hljs-operator">$</span>age<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  13.03   16.28   17.24   17.24   18.21   20.00
</code></pre>
    <p class="normal">With the data ready for analysis, we are ready to dive into the interesting part of this project. Let’s see if our efforts have paid off.</p>
    <h2 class="heading-2" id="_idParaDest-228">Step 3 – training a model on the data</h2>
    <p class="normal">To<a id="_idIndexMarker1092"/> cluster the teenagers into marketing segments, we’ll use an implementation of k-means in the <code class="inlineCode">stats</code> package, which should be included in your R installation by default. Although there is no shortage of more sophisticated k-means functions available in other R packages, the <code class="inlineCode">kmeans()</code> function in the default <code class="inlineCode">stats</code> package is widely used and provides a simple yet powerful implementation of the algorithm.</p>
    <figure class="mediaobject"><img alt="Text  Description automatically generated" src="../Images/B17290_09_11.png"/></figure>
    <p class="packt_figref">Figure 9.11: K-means clustering syntax</p>
    <p class="normal">The <code class="inlineCode">kmeans()</code> function requires a data frame or matrix containing only numeric data and a parameter specifying <em class="italic">k</em>, the desired number of clusters. If you have these two things ready, the actual process of building the model is simple. The trouble is that choosing the right combination of data and clusters can be a bit of an art; sometimes a great deal of trial and error is involved.</p>
    <p class="normal">We’ll start <a id="_idIndexMarker1093"/>our cluster analysis by considering only the 36 features that measure the number of times various interest-based keywords appeared in the text of the teenagers’ social media profiles. In other words, we will not cluster based on age, graduation year, gender, or number of friends. Of course, we <em class="italic">could</em> use these four features if desired, but <em class="italic">choose</em> not to, since any clusters built upon them would be less insightful than those built upon interests. This is primarily because age and gender are already de facto clusters whereas the interest-based clusters are yet to be discovered in our data. Secondarily, what will be more interesting later is to see whether the interest clusters are associated with the gender and popularity features held out from the clustering process. If the interest-based clusters are predictive of these individual characteristics, this provides evidence that the clusters may be useful.</p>
    <p class="normal">To avoid the chance of accidentally including the other features, let’s make a data frame called <code class="inlineCode">interests</code>, by subsetting the data frame to include only the 36 keyword columns:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> interests <span class="hljs-operator">&lt;-</span> teens<span class="hljs-punctuation">[</span><span class="hljs-number">5</span><span class="hljs-operator">:</span><span class="hljs-number">40</span><span class="hljs-punctuation">]</span>
</code></pre>
    <p class="normal">If you recall from <em class="chapterRef">Chapter 3</em>, <em class="italic">Lazy Learning – Classification Using Nearest Neighbors</em>, a common practice employed prior to any analysis using distance calculations is to normalize or z-score-standardize the features such that each utilizes the same range. By doing so, you can avoid a problem in which some features dominate solely because they have a larger range of values than the others.</p>
    <p class="normal">The process of z-score standardization rescales features such that they have a mean of zero and a standard deviation of one. This transformation changes the interpretation of the data in a way that may be useful here. Specifically, if someone mentions basketball three times on their profile, without additional information, we have no idea whether this implies they like basketball more or less than their peers. On the other hand, if the z-score is three, we know that they mentioned basketball many more times than the average teenager.</p>
    <p class="normal">To apply z-score standardization to the <code class="inlineCode">interests</code> data frame, we can use the <code class="inlineCode">scale()</code> function with <code class="inlineCode">lapply()</code>. Since <code class="inlineCode">lapply()</code> returns a list object, it must be coerced back to data frame form using the <code class="inlineCode">as.data.frame()</code> function, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> interests_z <span class="hljs-operator">&lt;-</span> as.data.frame<span class="hljs-punctuation">(</span>lapply<span class="hljs-punctuation">(</span>interests<span class="hljs-punctuation">,</span> scale<span class="hljs-punctuation">))</span>
</code></pre>
    <p class="normal">To <a id="_idIndexMarker1094"/>confirm that the transformation worked correctly, we can compare the summary statistics of the <code class="inlineCode">basketball</code> column in the old and new <code class="inlineCode">interests</code> data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>interests<span class="hljs-operator">$</span>basketball<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.0000  0.0000  0.0000  0.2673  0.0000 24.0000 
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>interests_z<span class="hljs-operator">$</span>basketball<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-0.3322 -0.3322 -0.3322  0.0000 -0.3322 29.4923
</code></pre>
    <p class="normal">As expected, the <code class="inlineCode">interests_z</code> dataset transformed the basketball feature to have a mean of zero and a range that spans above and below zero. Now, a value less than zero can be interpreted as a person having fewer-than-average mentions of basketball in their profile. A value greater than zero implies that the person mentioned basketball more frequently than the average.</p>
    <p class="normal">Our last decision involves deciding how many clusters to use for segmenting the data. If we use too many clusters, we may find them too specific to be useful; conversely, choosing too few may result in heterogeneous groupings. You should feel comfortable experimenting with the value of <em class="italic">k</em>. If you don’t like the result, you can easily try another value and start over.</p>
    <div class="packt_tip">
      <p class="normal">Choosing the number of clusters is easier if you are familiar with the analysis population. Having a hunch about the true number of natural groupings can save some trial and error.</p>
    </div>
    <p class="normal">To help choose the number of clusters in the data, I’ll defer to one of my favorite films, <em class="italic">The Breakfast Club</em>, a coming-of-age comedy released in 1985 and directed by John Hughes. The teenage characters in this movie are self-described in terms of five identities:</p>
    <ul>
      <li class="bulletList">A <em class="italic">brain</em> – also commonly known as a “nerd” or “geek”</li>
      <li class="bulletList">An <em class="italic">athlete</em> – sometimes also known as a “jock” or “prep”</li>
      <li class="bulletList">A <em class="italic">basket case</em> – slang terminology for an anxious or neurotic individual, and depicted in the film as an anti-social outcast</li>
      <li class="bulletList">A <em class="italic">princess</em> – portrayed as a popular, affluent, and stereotypically feminine girl</li>
      <li class="bulletList">A <em class="italic">criminal</em> – represents the traditional “burnout” identity described in sociological research as engaging in rebellious anti-school and anti-authority behaviors</li>
    </ul>
    <p class="normal">Even <a id="_idIndexMarker1095"/>though the movie depicts five specific identity groups, they have been described throughout popular teenage fiction for many years, and although the stereotypes have evolved over time, American teenagers are likely to understand them intuitively. Thus, five seems like a reasonable starting point for <em class="italic">k</em>, though admittedly, it is unlikely to capture the full spectrum of high-school identities.</p>
    <p class="normal">To use the k-means algorithm to divide the teenagers’ interest data into five clusters, we use the <code class="inlineCode">kmeans()</code> function on the <code class="inlineCode">interests</code> data frame. Note that because k-means utilizes random starting points, the <code class="inlineCode">set.seed()</code> function is used to ensure that the results match the output in the examples that follow. If you recall from previous chapters, this command initializes R’s random number generator to a specific sequence. In the absence <a id="_idIndexMarker1096"/>of this statement, the results may vary each time the k-means algorithm is run. Running the k-means clustering process as follows creates a list named <code class="inlineCode">teen_clusters</code>, which stores the properties of each of the five clusters:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">2345</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> teen_clusters <span class="hljs-operator">&lt;-</span> kmeans<span class="hljs-punctuation">(</span>interests_z<span class="hljs-punctuation">,</span> <span class="hljs-number">5</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Let’s dig in and see how well the algorithm has divided the teenagers’ interest data.</p>
    <div class="packt_tip">
      <p class="normal">If you find that your results differ from those shown in the sections that follow, ensure that the <code class="inlineCode">set.seed(2345)</code> command is run immediately prior to the <code class="inlineCode">kmeans()</code> function. Additionally, because the behavior of R’s random number generator changed with R version 3.6, your results may also vary slightly from those shown here if you are using an older version of R.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-229">Step 4 – evaluating model performance</h2>
    <p class="normal">Evaluating<a id="_idIndexMarker1097"/> clustering results can be somewhat subjective. Ultimately, the success or failure of the model hinges on whether the clusters are useful for their intended purpose. As the goal of this analysis was to identify clusters of teenagers with similar interests for marketing purposes, we will largely measure our success in qualitative terms. For other clustering applications, more quantitative measures of success may be needed.</p>
    <p class="normal">One of the most basic ways to evaluate the utility of a set of clusters is to examine the number of examples falling in each of the groups. If some groups are too large or too small, then they are less likely to be very useful. </p>
    <p class="normal">To obtain the size of the <code class="inlineCode">kmeans()</code> clusters, simply examine the <code class="inlineCode">teen_clusters$size</code> component as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> teen_clusters<span class="hljs-operator">$</span>size
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1]  1038   601  4066  2696 21599
</code></pre>
    <p class="normal">Here we see the five clusters we requested. The smallest cluster has 601 teenagers (2 percent) while the largest has 21,599 (72 percent). Although the large gap between the number of people in the largest and smallest clusters is slightly concerning, without examining these groups more carefully, we will not know whether this indicates a problem. It may be the case that the clusters’ size disparity indicates something real, such as a big group of teenagers who share similar interests, or it may be a random fluke caused by the initial k-means cluster centers. We’ll know more as we start to look at each cluster’s characteristics.</p>
    <div class="packt_tip">
      <p class="normal">Sometimes, k-means may find extremely small clusters—occasionally as small as a single point. This can happen if one of the initial cluster centers happen to fall on outliers far from the rest of the data. It is not always clear whether to treat such small clusters as a true finding that represents a cluster of extreme cases, or a problem caused by random chance. If you encounter this issue, it may be worth re-running the k-means algorithm with a different random seed to see whether the small cluster is robust to different starting points.</p>
    </div>
    <p class="normal">For a more in-depth look at the clusters, we can examine the coordinates of the cluster centroids using the <code class="inlineCode">teen_clusters$centers</code> component, which is as follows for the first four interests:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> teen_clusters<span class="hljs-operator">$</span>centers
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    basketball    football      soccer   softball
1  0.362160730  0.37985213  0.13734997  0.1272107
2 -0.094426312  0.06691768 -0.09956009 -0.0379725
3  0.003980104  0.09524062  0.05342109 -0.0496864
4  1.372334818  1.19570343  0.55621097  1.1304527
5 -0.186822093 -0.18729427 -0.08331351 -0.1368072
</code></pre>
    <p class="normal">The rows<a id="_idIndexMarker1098"/> of the output (labeled <code class="inlineCode">1</code> to <code class="inlineCode">5</code>) refer to the five clusters, while the numbers across each row indicate the cluster’s average value for the interest listed at the top of the column. Because the values are z-score-standardized, positive values are above the overall mean level for all teenagers and negative values are below the overall mean. </p>
    <p class="normal">For example, the fourth row has the highest value in the <code class="inlineCode">basketball</code> column, which means that cluster <code class="inlineCode">4</code> has the highest average interest in basketball among all the clusters.</p>
    <p class="normal">By examining whether clusters fall above or below the mean level for each interest category, we can discover patterns that distinguish the clusters from one another. In practice, this involves printing the cluster centers and searching through them for any patterns or extreme values, much like a word search puzzle but with numbers. The following annotated screenshot shows a highlighted pattern for each of the five clusters, for 18 of the 36 teenager interests:</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_09_12.png"/></figure>
    <p class="packt_figref">Figure 9.12: To distinguish clusters, it can be helpful to highlight patterns in the coordinates of their centroids</p>
    <p class="normal">Given this snapshot of the interest data, we can already infer some characteristics of the clusters. Cluster four is substantially above the mean interest level on nearly all the sports, which suggests that this may be a group of <em class="italic">athletes</em> per <em class="italic">The</em> <em class="italic">Breakfast Club</em> stereotype. Cluster three includes the most mentions of cheerleading, dancing, and the word “hot.” Are these the so-called princesses?</p>
    <p class="normal">By continuing<a id="_idIndexMarker1099"/> to examine the clusters in this way, it is possible to construct a table listing the dominant interests of each of the groups. In the following table, each cluster is shown with the features that most distinguish it from the other clusters, and <em class="italic">The</em> <em class="italic">Breakfast Club</em> identity that seems to most accurately capture the group’s characteristics.</p>
    <p class="normal">Interestingly, cluster five is distinguished by the fact that it is unexceptional: its members had lower-than-average levels of interest in every measured activity. It is also the single largest group in terms of the number of members. How can we reconcile these apparent contradictions? One potential explanation is that these users created a profile on the website but never posted any interests.</p>
    <figure class="mediaobject"><img alt="Diagram, table  Description automatically generated" src="../Images/B17290_09_13.png"/></figure>
    <p class="packt_figref">Figure 9.13: A table can be used to list important dimensions of each cluster</p>
    <p class="normal">When sharing the results of a segmentation analysis with stakeholders, it is often helpful to apply memorable and informative labels <a id="_idIndexMarker1100"/>known as <strong class="keyWord">personas</strong>, which simplify and capture the essence of the groups, such as <em class="italic">The</em> <em class="italic">Breakfast Club</em> typology applied here. The risk in adding such labels is that they can obscure the groups’ nuances and possibly even offend the group members if negative stereotypes are used. For wider dissemination, provocative labels like “Criminals” and “Princesses” might be replaced by more neutral terminology like “Edgy Adolescents” and “Trendy Teenagers.” Additionally, because even relatively harmless labels can bias our thinking, important patterns can be missed if labels are understood as the whole truth rather than a simplification of complexity.</p>
    <p class="normal">Given <a id="_idIndexMarker1101"/>memorable labels and a table as depicted in <em class="italic">Figure 9.13</em>, a marketing executive would have a clear mental picture of five types of teenage visitors to the social networking website. Based on these personas, the executive could sell targeted advertising impressions to businesses with products relevant to one or more of the clusters. In the next section, we will see how the cluster labels can be applied back to the original <a id="_idIndexMarker1102"/>population for such uses.</p>
    <div class="packt_tip">
      <p class="normal">It is possible to visualize the results of a cluster analysis using techniques that flatten the multidimensional feature data into two dimensions, then color the points according to cluster assignment. The <code class="inlineCode">fviz_cluster()</code> function in the <code class="inlineCode">factoextra</code> package allows such visualizations to be constructed quite easily. If this is of interest to you, load the package and try the command <code class="inlineCode">fviz_cluster(teen_clusters, interests_z, geom = "point")</code> to see such a visualization for the teenage SNS clusters. Although the visual is of limited use for the SNS example due to the large number of overlapping points, sometimes, it can be a helpful tool for presentation purposes. To better understand how to create and understand these plots, see <em class="chapterRef">Chapter 15</em>, <em class="italic">Making Use of Big Data</em>.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-230">Step 5 – improving model performance</h2>
    <p class="normal">Because <a id="_idIndexMarker1103"/>clustering creates new information, the performance of a clustering algorithm depends at least somewhat on both the quality of the clusters themselves and what is done with that information. In the preceding section, we demonstrated that the five clusters provided useful and novel insights into the interests of teenagers. By that measure, the algorithm appears to be performing quite well. Therefore, we can now focus our effort on turning these insights into action.</p>
    <p class="normal">We’ll begin by applying the clusters back to the full dataset. The <code class="inlineCode">teen_clusters</code> object created by the <code class="inlineCode">kmeans()</code> function includes a component named <code class="inlineCode">cluster</code>, which contains the cluster assignments for all 30,000 individuals in the sample. We can add this as a column to the <code class="inlineCode">teens</code> data frame with the following command:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> teens<span class="hljs-operator">$</span>cluster <span class="hljs-operator">&lt;-</span> teen_clusters<span class="hljs-operator">$</span>cluster
</code></pre>
    <p class="normal">Given this new data, we can start to examine how the cluster assignment relates to individual characteristics. For example, here’s the personal information for the first five teenagers in the SNS data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> teens<span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-operator">:</span><span class="hljs-number">5</span><span class="hljs-punctuation">,</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"cluster"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"</span><span class="hljs-string">gender"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"age"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"friends"</span><span class="hljs-punctuation">)]</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  cluster gender    age friends
1       5      M 18.982       7
2       3      F 18.801       0
3       5      M 18.335      69
4       5      F 18.875       0
5       1   &lt;NA&gt; 18.995      10
</code></pre>
    <p class="normal">Using the <code class="inlineCode">aggregate()</code> function, we can also look at the demographic characteristics of the clusters. The mean age does not vary much by cluster, which is not too surprising, as teen identities are often set well before high school. This is depicted as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> aggregate<span class="hljs-punctuation">(</span>data <span class="hljs-operator">=</span> teens<span class="hljs-punctuation">,</span> age <span class="hljs-operator">~</span> cluster<span class="hljs-punctuation">,</span> mean<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  cluster      age
1       1 17.09319
2       2 17.38488
3       3 17.03773
4       4 17.03759
5       5 17.30265
</code></pre>
    <p class="normal">On the other hand, there are some substantial differences in the proportion of females by cluster. This is a very interesting finding, as we didn’t use gender data to create the clusters, yet the clusters are still predictive of gender:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> aggregate<span class="hljs-punctuation">(</span>data <span class="hljs-operator">=</span> teens<span class="hljs-punctuation">,</span> female <span class="hljs-operator">~</span> cluster<span class="hljs-punctuation">,</span> mean<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  cluster    female
1       1 0.8025048
2       2 0.7237937
3       3 0.8866208
4       4 0.6984421
5       5 0.7082735
</code></pre>
    <p class="normal">Recall that overall, about 74 percent of the SNS users are female. Cluster three, the so-called <em class="italic">princesses</em>, is nearly 89 percent female, while clusters four and five are only about 70 percent female. These disparities imply that there are differences in the interests that teenage boys and girls discuss on their social networking pages.</p>
    <p class="normal">Given our <a id="_idIndexMarker1104"/>success in predicting gender, you might suspect that the clusters are also predictive of the number of friends the users have. This hypothesis seems to be supported by the data, which is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> aggregate<span class="hljs-punctuation">(</span>data <span class="hljs-operator">=</span> teens<span class="hljs-punctuation">,</span> friends <span class="hljs-operator">~</span> cluster<span class="hljs-punctuation">,</span> mean<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  cluster  friends
1       1 30.66570
2       2 32.79368
3       3 38.54575
4       4 35.91728
5       5 27.79221
</code></pre>
    <p class="normal">On average, <em class="italic">princesses</em> have the most friends (38.5), followed by <em class="italic">athletes</em> (35.9) and<em class="italic"> brains </em>(32.8). On the low end are <em class="italic">criminals</em> (30.7) and <em class="italic">basket cases</em> (27.8). As with gender, the <a id="_idIndexMarker1105"/>connection between a teenager’s number of friends and their predicted cluster is remarkable given that we did not use the friendship data as an input to the clustering algorithm. Also interesting is the fact that the number of friends seems to be related to the stereotype of each cluster’s high-school popularity: the stereotypically popular groups tend to have more friends in reality.</p>
    <p class="normal">The association between group membership, gender, and number of friends suggests that the clusters can be useful predictors of behavior. Validating their predictive ability in this way may make the clusters an easier sell when they are pitched to the marketing team, ultimately improving the performance of the algorithm.</p>
    <div class="packt_tip">
      <p class="normal">Just as the characters in <em class="italic">The Breakfast Club</em> ultimately come to realize that “each one of us is a brain, an athlete, a basket case, a princess, and a criminal,” it is important for data scientists to realize that the labels or personas we attribute to each cluster are stereotypes, and individuals may embody the stereotype to a greater or lesser degree. Keep this caveat in mind when acting on the results of a cluster analysis; a group may be relatively homogenous, but each member is still unique.</p>
    </div>
    <h1 class="heading-1" id="_idParaDest-231">Summary</h1>
    <p class="normal">Our findings support the popular adage that “birds of a feather flock together.” By using machine learning methods to cluster teenagers with others who have similar interests, we were able to develop a typology of teenage identities, which was predictive of personal characteristics such as gender and number of friends. These same methods can be applied to other contexts with similar results.</p>
    <p class="normal">This chapter covered only the fundamentals of clustering. There are many variants of the k-means algorithm, as well as many other clustering algorithms, which bring unique biases and heuristics to the task. Based on the foundation in this chapter, you will be able to understand these clustering methods and apply them to new problems.</p>
    <p class="normal">In the next chapter, we will begin to look at methods for measuring the success of a learning algorithm that are applicable across many machine learning tasks. While our process has always devoted some effort to evaluating the success of learning, in order to obtain the highest degree of performance, it is crucial to be able to define and measure it in the strictest terms.</p>
    <h1 class="heading-1" id="_idParaDest-232">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 4000 people at:</p>
    <p class="normal"><a href="https://packt.link/r"><span class="url">https://packt.link/r</span></a></p>
    <p class="normal"><img alt="" src="../Images/r.jpg"/></p>
  </div>
</body></html>