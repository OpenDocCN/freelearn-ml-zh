- en: Working with AWS Comprehend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a data scientist, knowing how machine learning algorithms work is very important.
    However, it may not be efficient to build your own machine learning models to
    perform certain tasks, as it takes a lot of effort and time to design an optimal
    algorithm. In [Chapter 10](b83ce0ca-e2d7-43f5-9e82-21edb54250c9.xhtml), *Working
    with AWS Comprehend*, [Chapter 11](b6601397-10a0-4a94-ba9f-32b5bfcdbb06.xhtml), *Using
    AWS Rekognition* and [Chapter 12](f9e097f0-ee26-456d-9360-7d0d3743e3a6.xhtml), *Building
    Conversational Interfaces Using AWS Lex*, we will look at the **machine learning
    as a s****ervice** (**MLaaS**) product that you can access in AWS. These products
    allow you to use models that are pre-trained in AWS using either the AWS dashboard
    or API calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Amazon Comprehend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing Amazon Comprehend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing entity recognition using Comprehend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing sentiment analysis using Comprehend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing text classification using Comprehend APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Amazon Comprehend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Comprehend is a service available in AWS that offers **natural language
    processing** (**NLP**) algorithms. NLP is a field in machine learning that analyzes
    human (natural) languages and can identify various attributes of these languages.
    In most of our previous chapters, we looked at examples of structured data. The
    data had predefined features and was organized as rows of observations. However,
    a natural language dataset is more complicated to process. Such datasets are called
    **unstructured datasets**, as the structure of the features is not well-defined.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, algorithms are needed to extract structure and information from a text
    document. For example, a natural language has words that are arranged using a
    grammatical structure. Natural-language sentences also have keywords, which contain
    more information regarding places, people, and other details. They also have a
    context, which is very hard to learn, and the same words may convey different
    meanings based on how they are arranged.
  prefs: []
  type: TYPE_NORMAL
- en: The field of NLP studies how to process these text documents and extract information
    from them. NLP not only involves clustering and classifying the documents, but
    also preprocessing the data to extract important keywords and entity information
    from the text. Based on the domain of the text documents, different preprocessing
    is required, as the styles of written documents change. For example, medical and
    legal texts are written with a lot of jargon and are well-structured. However,
    if you are using an NLP algorithm to process Twitter data, the text may be composed
    of poor grammar and hashtags. Hence, based on the domain of the data, you need
    a separate process to preprocess the data and how the models should be trained.
    Domain expertise is generally required when training NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Comprehend provides tools to both train machine learning models and use
    pre-trained models to perform NLP tasks. It provides real-time dashboards to analyze
    text data and also provides tools to train machine learning algorithms using their
    UI.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore four NLP tasks that can be accomplished using
    AWS Comprehend. We will also suggest when a data scientist should employ ready-to-use
    tools and when they should invest time in building their own machine learning
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing AmazonComprehend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Amazon Comprehend](https://aws.amazon.com/comprehend/) is available to use
    on the AWS Console. When you log into the AWS Management Console, search for Amazon
    Comprehend in the AWS Services box. Selecting Amazon Comprehend will take you
    to the AWS Comprehend start screen, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f00d8f41-3c7c-4046-bfed-4f8cf06da30d.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on Launch Comprehend when you get to this screen, which will take you
    to the AWS Comprehend dashboard. You should be able to access the algorithms used
    in the following sections from this page.
  prefs: []
  type: TYPE_NORMAL
- en: Named-entity recognition using Comprehend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Named-entity recognition** (**NER**) is a field in NLP that tags mentions
    of named entities in unstructured text. Named entities are names of people, places,
    organizations, and so on. For example, consider the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: Tim Cook traveled to New York for an Apple store opening.
  prefs: []
  type: TYPE_NORMAL
- en: In this sentence, there are three named entities. Tim Cook is the name of a
    person, New York is the name of a city (location), and Apple is the name of an
    organization. Hence, we need an NER model that can detect these entities. Note
    that Apple is an ambiguous noun, as it can be the name of a company or a fruit.
    The NER algorithm should understand the context in which the term is used and
    identify it accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS Comprehend offers a good NER tool that can be used to identify entities.
    This tool can be used in real-time via their dashboard or using their APIs. AWS
    Comprehend detects the following entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Commercial Item**: Brand names'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Date**: Dates in different formats'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event**: Names of concerts, festivals, elections, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Location**: Names of cities, countries, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Organization**: Names of companies and governmental organizations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Person**: Names of people'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantity**: Commonly used units used to quantify a number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Title**: Names of movies, books, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To access the AWS dashboard for NER, go to the Real-time Analysis tab in the
    menu. You can then add input text in the text box provided on the page. The following
    screenshot demonstrated how Amazon Comprehend performs the NER task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9c4ab81-b32e-4ca8-be65-168e7402f1b9.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the NER tool in Amazon Comprehend automatically labels the
    entities in the sentence. Along with labeling the categories of the entities,
    it also gives us a confidence score. This score can be used to determine whether
    we trust the results from the tool.
  prefs: []
  type: TYPE_NORMAL
- en: The NER tool in Amazon Comprehend can also be accessed using the API provided
    by AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how you can call the Comprehend tool to get the entity
    scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You use the `boto3` package, which is an AWS tool package for Python. We first
    initialize the Comprehend client and then pass our text to the client to get a
    JSON response with information about the named entities. In the following code
    block we can see the response we receive from the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Thus, parsing the JSON can get us information regarding the entities in the
    text.
  prefs: []
  type: TYPE_NORMAL
- en: You can also train a custom NER algorithm in AWS Comprehend using the Customization
    | Custom entity recognition option in the left-hand side menu. You can add training
    sample documents and a list of annotations for entities. The algorithm automatically
    learns how to label these entities in the correct context and updates the existing
    models.
  prefs: []
  type: TYPE_NORMAL
- en: NER algorithms are applied in various applications. One of their important applications
    is in the field of News Aggregation. You can automatically generate tags for a
    document so that users can search for documents based on the entities in them.
    NER is also useful in the field of recommendation algorithms, where NER is used
    to detect keywords and we can create a news-recommendation algorithm. We can build
    a collaborative filtering model that can recommend articles about entities that
    readers of a current article may also be interested in.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis using Comprehend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment analysis algorithms analyze text and categorize it based on the sentiments
    or opinions in the text. Sentiment analysis detects subjective opinions that are
    expressed in text. For example, reviews on Amazon Marketplace give a good or a
    bad review of a product. Using sentiment analysis, we can detect whether a review
    is positive or negative. We can also recognize emotional nuances in a review,
    such as whether the reviewer was angry, excited, or neutral about a given product.
    In this age of social media, we have a large number of avenues to voice our opinions
    on products, movies, politics, and so on. Data scientists use sentiment analysis
    algorithms to analyze a large amount of data and extract opinions regarding a
    certain entity based on unstructured text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon Comprehend makes the task of sentiment analysis easy by providing a
    real-time dashboard to analyze the sentiment in text. You can access the Sentiment
    Analysis dashboard the same way you did for the NER algorithm. We''ll provide
    two examples of how Comprehend can perform sentiment analysis on our data. I looked
    at two reviews on Amazon that were positive and negative and used Comprehend to
    perform sentiment analysis on them. Consider the first example, as seen in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7961739-0c56-4aee-ac29-d97eff366f0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this example, the reviewer has used words such as disappointed. These terms
    have negative connotations. However, sentiment analysis algorithms can detect
    that the user also used a negative before this word and correctly predict that
    this text has a positive sentiment. Similarly, consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc145b08-2999-4f2d-b8d4-4a8c31ac9089.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the reviewer was initially happy regarding the product, but
    then had issues. Hence, the reviewer was not happy with the product. Hence, the
    sentiment analysis algorithm correctly predicts that the confidence of the review
    being negative is 70%. However, it also predicts that there are some mixed sentiments
    in this review and provides confidence of 22%. We use the soft-max methodology
    to pixel the sentiment with the highest confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis can also be accessed using the Amazon API. Here, we provide
    example code that shows how we can call the sentiment analysis API using the `boto3`
    Python package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This API call returns the following JSON with the data regarding the sentiment
    of the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can use the API to classify a large number of reviews to detect what the
    overall sentiment is for a given product.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis is a very powerful tool that companies use to analyze social
    media data to detect the overall sentiment regarding their products and also to
    determine why users are unhappy with their products. Movie review aggregators,
    such as Rotten Tomatoes, also use them to detect whether reviews are positive
    or negative so that they can classify them and generate aggregated scores.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification using Comprehend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text classification is the process of classifying text documents into categories.
    Similar to the classification algorithms that we studied in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying
    Twitter Feeds with Naive Bayes* to [Chapter 6](c940bfe6-b849-4179-b8f8-65e5d44652d6.xhtml),
    *Analyzing Visitor Patterns to Make Recommendations*, text classification algorithms
    also generate models based on labeled training observations. The classification
    model can then be applied to any observation to predict its class. Moreover, the
    same algorithms that we studied in the previous chapters, such as [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying
    Twitter Feeds with Naive Bayes*, [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms*, and [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting
    User Behavior with Tree-Based Methods,* can also be used for text classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Text data is unstructured data. Hence, we need to generate features from text
    documents so that those features can be used as input for our classification model.
    For text datasets, features are generally terms in the document. For example,
    consider the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: Tim Cook traveled to New York for an Apple store opening.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the class of this document as `Technology`. This sentence will
    be translated into structured data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `Tim Cook` | `traveled` | `to` | `New York` | `Apple` | `Store` | `Opening`
    | `Microsoft` | `Google` | `Class` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `1` | `1` | `1` | `1` | `1` | `1` | `0` | `0` | `Technology` |'
  prefs: []
  type: TYPE_TB
- en: Each term will be considered a feature in the dataset. Hence, for a large dataset
    with many documents, the feature set can be as large as the lexicon of that language.
    The value of the features is set to `0` or `1` based on whether that term exists
    in that document. As our example contains words such as `Tim Cook` and `New York`,
    the value of those features for this observation is set to `1`. As the terms Microsoft
    and Google are not present in the sentence, the value of those features is set
    to `0`. The `Class` variable is set to `Technology`.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will show a step-by-step methodology on how to train custom
    classifiers on Comprehend. We'll use a popular text classification dataset called
    **20 Newsgroups** to generate a machine learning model that can mark a review
    as positive or negative. The dataset can be downloaded from [https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups).
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be downloaded as separate text files that are organized into
    20 folders. Each folder name represents the category of documents in the folder. The
    dataset is a publicly available dataset. It contains news articles that are categorized
    into the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '`alt.atheism`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comp.graphics`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comp.os.ms-windows.misc`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comp.sys.ibm.pc.hardware`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comp.sys.mac.hardware`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comp.windows.x`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`misc.forsale`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rec.autos`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rec.motorcycles`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rec.sport.baseball`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rec.sport.hockey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sci.crypt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sci.electronics`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sci.med`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sci.space`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`soc.religion.christian`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`talk.politics.guns`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`talk.politics.mideast`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`talk.politics.misc`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`talk.religion.misc`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can use the following steps to train the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to download and preprocess the data into a format that is
    readable by the Comprehend tools. Comprehend requires the training data to be
    in the following format in CSV (comma-separated values):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **Category** | **Document** |'
  prefs: []
  type: TYPE_TB
- en: Hence, once you download the dataset, convert the data into the preceding format
    and upload it to your S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: You can access the Custom Classification tool on the Comprehend dashboard, on
    the left-hand side under the Customization tab. To train the model, you have to
    click on the Train Classifier option. Note that Comprehend allows you to train
    your machine learning models and store them on this dashboard so that you can
    use them in the future.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you click on the Train Classifier option, you will see the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/787277e7-a142-425a-8ee4-0d946a21b8ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Name the classifier and select the language of the documents. Add your S3 location,
    where the training CSV document is stored. After you select the correct role,
    you can tag the classifier with relevant values, which can help you to search
    them in the future. Once you have added all the information, click on Train classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4d8e2c52-835f-4526-a351-5064fe5b913f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will be taken back to the dashboard screen where you will see that the
    classifier training is in progress. Once the training is done, the status of the
    classifier will be marked as Trained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d08fd414-7ac7-4685-97a9-1479bba25557.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can then click on the classifier to see the evaluation metrics of the model.
    As you can see, our classification model has an accuracy of 90%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cc1fe664-413f-46be-af89-870f0a42ce6a.png)'
  prefs: []
  type: TYPE_IMG
- en: As we now have a classifier that is trained, you can get predictions for any
    document using this model. We create a `test.csv` file that contains 100 documents
    to get predictions from this model. We preprocess the data to create a CSV file
    with one document per line. To start the prediction process, click on the Create
    Job option shown on the preceding screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will take you to another screen, where you can add details on which file
    you want to use for testing and where the output should be stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b2840cf-efee-495c-8a5b-e612436a9d68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the Create analysis job screen, add the details about the classifier to
    be used: where the input data is stored (on S3) and an S3 location where the output
    is stored. You can either specify the input data as one document per line or one
    document per file and point the input data to the directory that contains all
    the files. In our example, since the `test.csv` file contains one document on
    each line, we use that format.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you click on Create Job, it will automatically classify the documents and
    store the output in the output location. The output is stored in JSON format,
    where each line of the `output` file contains JSON that gives the analysis of
    that line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is an example of the output that was generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Thus, you can see that our model labeled the first line in our input file as
    `"alt.atheism"` with a confidence score of 86.42%.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also create a document classifier and prediction jobs using the Amazon
    Comprehend APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this function will automatically generate the same classifier that
    we created in the previous steps. You can access your ARN value from the Roles
    tab on the My Security Credentials page. This is the ARN value of the same IAM
    role we created in step 3\. The output data config location will automatically
    get a confusion metric of the evaluation of the classifier and the response string
    will be returned as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The string will be the Amazon resource name that identifies the classifier.
    You can also run prediction jobs using the API. The following code can be used
    to generate the predictions for your input files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will start the exact same classification job that we created
    on the dashboard. Thus, you can control when you want to use a certain classifier
    and generate predictions on different datasets as required. The response of the
    function will be the status of the job. The job will also generate a job ID, that
    you can ping to check the status of the job using the `describe_document_classification_job()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have generated a custom document classifier using Comprehend tools
    on AWS. These tools will help you to create these classifiers quickly without
    having to worry about what classification algorithms to select, how to tune the
    parameters, and so on. Amazon automatically updates the algorithms used by Comprehend
    based on the expertise of their research teams. However, the main disadvantage
    is that Comprehend tools can be costly if you are running operations on large
    datasets, as they charge you per prediction. You can access the pricing information
    for AWS Comprehend at [https://aws.amazon.com/comprehend/pricing/](https://aws.amazon.com/comprehend/pricing/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we studied how to use a built-in machine learning tool called
    Comprehend in AWS. We briefly discussed the field of NLP and provided an introduction
    to its sub-fields, such as NER and sentiment analysis. We also studied how to
    create a custom document classifier in Comprehend using the dashboard it provides.
    Moreover, we studied how to access Comprehend's APIs using the `boto3` package
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: These tools are fascinating as they will help you to create complex machine
    learning models quickly and start applying them in your applications. A data scientist
    who has cursory knowledge in the field of NLP can now train sophisticated machine
    learning models and use them to make optimal decisions. However, the question
    most data scientists face is whether the pricing provided by such tools is more
    economical than building algorithms in-house using Python packages. Note that
    Comprehend adds a layer of abstraction between data scientists and the machine
    learning models by making them worry about the underlying cluster configurations.
    In our experience, we use these tools during the rapid prototyping phases of our
    projects to evaluate a product. If we decide to move to production, it is easy
    to calculate the cost differences between using the AWS tools versus building
    algorithms in-house and maintaining them on our clusters.
  prefs: []
  type: TYPE_NORMAL
- en: We will introduce the Amazon Rekognition in the next chapter. This service is
    used for image recognition and is an out of the box solution for Object detection
    and similar applications
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your task is to perform NER on a large dataset using APIs provided by Amazon
    Comprehend. Use the annotated NER dataset provided in the Kaggle competition to
    create a custom entity recognition in Comprehend ([https://www.kaggle.com/abhinavwalia95/chemdner-iob-annotated-chemical-named-etities](https://www.kaggle.com/abhinavwalia95/chemdner-iob-annotated-chemical-named-etities)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply sentiment analysis on the Yelp dataset in Kaggle and then evaluate whether
    your predictions match the review score ([https://www.kaggle.com/yelp-dataset/yelp-dataset](https://www.kaggle.com/yelp-dataset/yelp-dataset)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
