<html><head></head><body>
		<div id="_idContainer376">
			<h1 id="_idParaDest-61"><a id="_idTextAnchor064"/>Chapter 4: Supervised Graph Learning</h1>
			<p><strong class="bold">Supervised learning</strong> (<strong class="bold">SL</strong>) most probably represents the majority of practical <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) tasks. Thanks to more and more active and effective data collection activities, it is very common nowadays to deal with labeled datasets.</p>
			<p>This is also true for graph data, where labels can be assigned to nodes, communities, or even to an entire structure. The task, then, is to learn a mapping function between the input and the label (also known as a target or an annotation).</p>
			<p>For example, given a graph representing a social network, we might be asked to guess which user (node) will close their account. We can learn this predictive function by training graph ML on <strong class="bold">retrospective data</strong>, where each user is labeled as "faithful" or "quitter" based on whether they closed their account after a few months.</p>
			<p>In this chapter, we will explore the concept of SL and how it can be applied on graphs. Therefore, we will also be providing an overview of the main supervised graph embedding methods. The following topics will be covered:</p>
			<ul>
				<li>The supervised graph embedding roadmap</li>
				<li>Feature-based methods</li>
				<li>Shallow embedding methods</li>
				<li>Graph regularization methods</li>
				<li>Graph <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) </li>
			</ul>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor065"/>Technical requirements</h1>
			<p>We will be using <em class="italic">Jupyter</em> Notebooks with <em class="italic">Python</em> 3.8 for all of our exercises. In the following code block, you can see a list of the Python libraries that will be installed for this chapter using <strong class="source-inline">pip</strong> (for example, run <strong class="source-inline">pip install networkx==2.5</strong> on the command line):</p>
			<p class="source-code">Jupyter==1.0.0</p>
			<p class="source-code">networkx==2.5</p>
			<p class="source-code">matplotlib==3.2.2</p>
			<p class="source-code">node2vec==0.3.3</p>
			<p class="source-code">karateclub==1.0.19</p>
			<p class="source-code">scikit-learn==0.24.0</p>
			<p class="source-code">pandas==1.1.3</p>
			<p class="source-code">numpy==1.19.2</p>
			<p class="source-code">tensorflow==2.4.1</p>
			<p class="source-code">neural-structured-learning==1.3.1</p>
			<p class="source-code">stellargraph==1.2.1</p>
			<p>In the rest of this book, if not clearly stated, we will refer to <strong class="source-inline">nx</strong> as the result of the <strong class="source-inline">import networkx as nx</strong> Python command.</p>
			<p>All code files relevant to this chapter are available at <a href="https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter04">https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter04</a>.</p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor066"/>The supervised graph embedding roadmap </h1>
			<p>In SL, a training set <a id="_idIndexMarker422"/>consists of a sequence of ordered pairs (<em class="italic">x</em>, <em class="italic">y</em>), where <em class="italic">x</em> is a set of input features (often signals defined on graphs) and <em class="italic">y</em> is the output label assigned to it. The goal of the ML models, then, is to learn the function mapping each <em class="italic">x</em> value to each <em class="italic">y</em> value. Common supervised tasks include predicting user properties in a large social network or predicting molecules' attributes, where each molecule is a graph.</p>
			<p>Sometimes, however, not all instances can be provided with a label. In this scenario, a typical dataset consists of a small set of labeled instances and a larger set of unlabeled instances. For such situations, <strong class="bold">semi-SL</strong> (<strong class="bold">SSL</strong>) is <a id="_idIndexMarker423"/>proposed, whereby algorithms aim to exploit label dependency information reflected by available label information in order to learn the predicting function for the unlabeled samples.</p>
			<p>With regard to supervised graph ML techniques, many algorithms have been developed. However as previously reported by different scientific papers (<a href="https://arxiv.org/abs/2005.03675">https://arxiv.org/abs/2005.03675</a>), they can be grouped into macro-groups such as <strong class="bold">feature-based methods</strong>, <strong class="bold">shallow embedding methods</strong>, <strong class="bold">regularization methods</strong>, and <strong class="bold">graph neural networks</strong> (<strong class="bold">GNNs</strong>), as graphically depicted in the following diagram:</p>
			<div>
				<div id="_idContainer288" class="IMG---Figure">
					<img src="image/B16069_04_01.jpg" alt="Figure 4.1 – Hierarchical structure of the different supervised embedding algorithms described in this book "/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Hierarchical structure of the different supervised embedding algorithms described in this book </p>
			<p>In the following sections, you <a id="_idIndexMarker424"/>will learn the main principles behind each group of algorithms. We will try to provide insight into the most well-known algorithms in the field as well, as these can be used to solve real-world problems.</p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor067"/>Feature-based methods </h1>
			<p>One very simple (yet powerful) method for <a id="_idIndexMarker425"/>applying ML on graphs is to consider the encoding function as a simple embedding lookup. When dealing with supervised tasks, one simple way of doing this is to exploit graph properties. In <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs</em>, we have learned how graphs (or nodes in a graph) can be described by means of structural properties, each "encoding" important information from the graph itself.</p>
			<p>Let's forget graph ML for a moment: in classical supervised ML, the task is to find a function that maps a set of (descriptive) features of an instance to a particular output. Such features should be carefully engineered so that they are sufficiently representative to learn that concept. Therefore, as the number of petals and the sepal length might be good descriptors for a flower, when describing a graph we might rely on its average degree, its global efficiency, and its characteristic path length.</p>
			<p>This shallow approach acts in two steps, outlined as follows:</p>
			<ol>
				<li>Select a set of <em class="italic">good</em> descriptive graph properties.</li>
				<li>Use such properties as input for a traditional ML algorithm.</li>
			</ol>
			<p>Unfortunately, there is <a id="_idIndexMarker426"/>no general definition of <em class="italic">good</em> descriptive properties, and their choice strictly depends on the specific problem to solve. However, you can still compute a wide variety of graph properties and then perform <em class="italic">feature selection</em> to select the most informative ones. <strong class="bold">Feature selection</strong> is a widely studied <a id="_idIndexMarker427"/>topic in ML, but providing details about the various methods is outside the scope of this book. However, we refer you to the book <em class="italic">Machine Learning Algorithms – Second Edition </em>(<a href="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789347999">https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789347999</a>), published by Packt Publishing, for further reading on this subject.</p>
			<p>Let's now see a practical example of how such a basic method can be applied. We will be performing a supervised graph classification task by using a <strong class="source-inline">PROTEINS</strong> dataset. The <strong class="source-inline">PROTEINS</strong> dataset contains several graphs representing protein structures. Each graph is labeled, defining whether the protein is an enzyme or not. We will follow these next steps:</p>
			<ol>
				<li value="1">First, let's load the dataset through the <strong class="source-inline">stellargraph</strong> Python library, as follows:<p class="source-code">from stellargraph import datasets</p><p class="source-code">from IPython.display import display, HTML</p><p class="source-code">dataset = datasets.PROTEINS()</p><p class="source-code">graphs, graph_labels = dataset.load()</p></li>
				<li>For computing graph properties, we will be using <strong class="source-inline">networkx</strong>, as described in <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs</em>. To that end, we need to convert graphs from the <strong class="source-inline">stellargraph</strong> format to the <strong class="source-inline">networkx</strong> format. This can be done in two steps: first, convert the graphs from the <strong class="source-inline">stellargraph</strong> representation to <strong class="source-inline">numpy</strong> adjacency matrices. Then, use the adjacency matrices to retrieve the <strong class="source-inline">networkx</strong> representation. In addition, we also transform the labels (which are stored as a <strong class="source-inline">pandas</strong> Series) to a <strong class="source-inline">numpy</strong> array, which can be better exploited by the evaluation functions, as we will see in the next steps. The code is illustrated in the following snippet:<p class="source-code"># convert from StellarGraph format to numpy adj matrices</p><p class="source-code">adjs = [graph.to_adjacency_matrix().A for graph in graphs]</p><p class="source-code"># convert labels from Pandas.Series to numpy array</p><p class="source-code">labels = graph_labels.to_numpy(dtype=int)</p></li>
				<li>Then, for each graph, we <a id="_idIndexMarker428"/>compute global metrics to describe it. For this example, we have chosen the number of edges, the average cluster coefficient, and the global efficiency. However, we suggest you compute several other properties you may find worth exploring. We can extract the graph metrics using <strong class="source-inline">networkx</strong>, as follows:<p class="source-code">import numpy as np</p><p class="source-code">import networkx as nx</p><p class="source-code">metrics = []</p><p class="source-code">for adj in adjs:</p><p class="source-code">  G = nx.from_numpy_matrix(adj)</p><p class="source-code">  # basic properties</p><p class="source-code">  num_edges = G.number_of_edges()</p><p class="source-code">  # clustering measures</p><p class="source-code">  cc = nx.average_clustering(G)</p><p class="source-code">  # measure of efficiency</p><p class="source-code">  eff = nx.global_efficiency(G)</p><p class="source-code">  metrics.append([num_edges, cc, eff]) </p></li>
				<li>We can now exploit <strong class="source-inline">scikit-learn</strong> utilities to create train and test sets. In our experiments, we will be using 70% of the dataset as the training set and the remainder as the test set. We can do that by using the <strong class="source-inline">train_test_split</strong> function provided by <strong class="source-inline">scikit-learn</strong>, as follows:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(metrics, labels, test_size=0.3, random_state=42)</p></li>
				<li>It's now time for training a proper ML algorithm. We chose a <strong class="bold">support vector machine</strong> (<strong class="bold">SVM</strong>) for <a id="_idIndexMarker429"/>this task. More precisely, the SVM is trained to minimize the difference between the <a id="_idIndexMarker430"/>predicted labels and the actual labels (the ground truth). We can do this by using the <strong class="source-inline">SVC</strong> module of <strong class="source-inline">scikit-learn</strong>, as follows:<p class="source-code">from sklearn import svm</p><p class="source-code">from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score</p><p class="source-code">clf = svm.SVC()</p><p class="source-code">clf.fit(X_train, y_train)</p><p class="source-code"> y_pred = clf.predict(X_test)</p><p class="source-code">print('Accuracy', accuracy_score(y_test,y_pred))</p><p class="source-code"> print('Precision', precision_score(y_test,y_pred))</p><p class="source-code"> print('Recall', recall_score(y_test,y_pred))</p><p class="source-code"> print('F1-score', f1_score(y_test,y_pred))</p><p>This should be the output of the previous snippet of code: </p><p class="source-code">Accuracy 0.7455</p><p class="source-code">Precision 0.7709</p><p class="source-code">Recall 0.8413</p><p class="source-code">F1-score 0.8045</p></li>
			</ol>
			<p>We used <strong class="source-inline">Accuracy</strong>, <strong class="source-inline">Precision</strong>, <strong class="source-inline">Recall</strong>, and <strong class="source-inline">F1-score</strong> to evaluate how well the algorithm is performing on the <a id="_idIndexMarker431"/>test set. We achieved about 80% for the F1 score, which is already quite good for such a naïve task.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor068"/>Shallow embedding methods </h1>
			<p>As we already described in <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning</em>, shallow embedding methods are a subset of graph embedding methods that learn node, edge, or graph representation for only a finite set of input data. They cannot be applied to other instances different <a id="_idIndexMarker432"/>from the ones used to train the model. Before starting our discussion, it is important to define how supervised and unsupervised shallow embedding algorithms differ. </p>
			<p>The main difference <a id="_idIndexMarker433"/>between unsupervised and supervised embedding methods essentially lies in the task they <a id="_idIndexMarker434"/>attempt to solve. Indeed, if unsupervised shallow embedding algorithms try to learn a good graph, node, or edge representation in order to build well-defined clusters, the supervised algorithms try to find the best solution for a prediction task such as node, label, or graph classification.</p>
			<p>In this section, we will explain in detail some of those supervised shallow embedding algorithms. Moreover, we will enrich our description by providing several examples of how to use those algorithms in Python. For all the algorithms described in this section, we will present a custom implementation using the base classes available in the <strong class="source-inline">scikit-learn</strong> library. </p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor069"/>Label propagation algorithm</h2>
			<p>The label propagation algorithm <a id="_idIndexMarker435"/>is a <a id="_idIndexMarker436"/>well-known semi-supervised algorithm widely applied in data science and used to solve the node classification task. More precisely, the algorithm <em class="italic">propagates</em> the label of a given node to its neighbors or to nodes having a high probability of being reached from that node. </p>
			<p>The general idea behind this approach is quite simple: given a graph with a set of labeled and unlabeled nodes, the labeled nodes propagate their label to the nodes having the highest probability of being reached. In the following diagram, we can see an example of a graph having labeled and unlabeled nodes: </p>
			<div>
				<div id="_idContainer289" class="IMG---Figure">
					<img src="image/B16069_04_02.jpg" alt="Figure 4.2 – Example of a graph with two labeled nodes (class 0 in red and class 1 in green) and six unlabeled nodes"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – Example of a graph with two labeled nodes (class 0 in red and class 1 in green) and six unlabeled nodes</p>
			<p>According to <em class="italic">Figure 4.2</em>, using the <a id="_idIndexMarker437"/>information of the labeled nodes (node <strong class="bold">0</strong> and <strong class="bold">6</strong>), the algorithm will calculate the probability of moving to another <a id="_idIndexMarker438"/>unlabeled node. The nodes having the highest probability from a labeled node will get the label of that node.</p>
			<p>Formally, let <img src="image/B16069__04_001.png" alt=""/> be a graph and let <img src="image/B16069__04_002.png" alt=""/> be a set of labels. Since the algorithm is semi-supervised, just a subset of nodes will have an assigned label. Moreover, let <img src="image/B16069__04_003.png" alt=""/> be the adjacency matrix of the input graph G and <img src="image/B16069__04_004.png" alt=""/> be the diagonal degree matrix where each element <img src="image/B16069__04_005.png" alt=""/> is defined as follows:</p>
			<div>
				<div id="_idContainer295" class="IMG---Figure">
					<img src="image/B16069__04_006.jpg" alt=""/>
				</div>
			</div>
			<p>In other words, the only nonzero elements of the degree matrix are the diagonal elements whose values are given by the degree of the node represented by the row. In the following figure, we can see the diagonal degree matrix of the graph represented in <em class="italic">Figure 4.2</em>:</p>
			<div>
				<div id="_idContainer296" class="IMG---Figure">
					<img src="image/B16069_04_03.jpg" alt="Figure 4.3 – Diagonal degree matrix for the graph in Figure 4.2&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – Diagonal degree matrix for the graph in Figure 4.2</p>
			<p>From <em class="italic">Figure 4.3</em>, it is possible to <a id="_idIndexMarker439"/>see how only the diagonal elements of the matrix contain nonzero values, and those values represent the <a id="_idIndexMarker440"/>degree of the specific node. We also need to introduce the transition matrix <img src="image/B16069__04_007.png" alt=""/>. This matrix defines the probability of a node being reached from another node. More precisely, <img src="image/B16069__04_008.png" alt=""/> is the probability of reaching node <img src="image/B16069__04_009.png" alt=""/> from node <img src="image/B16069__04_010.png" alt=""/>. The following figure shows the transition matrix <img src="image/B16069__04_011.png" alt=""/> for the graph depicted in <em class="italic">Figure 4.2</em>:</p>
			<div>
				<div id="_idContainer302" class="IMG---Figure">
					<img src="image/B16069_04_04.jpg" alt="Figure 4.4 – Transition matrix for the graph in Figure 4.2&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – Transition matrix for the graph in Figure 4.2</p>
			<p>In <em class="italic">Figure 4.4</em>, the matrix shows the <a id="_idIndexMarker441"/>probability of reaching an end node given a start node. For instance, from the first row of the matrix, we <a id="_idIndexMarker442"/>can see how from node 0 it is possible to reach, with equal probability of 0.5, only nodes 1 and 2. If we defined with <img src="image/B16069__04_012.png" alt=""/> the initial label assignment, the probability of label assignment for each node obtained using the <img src="image/B16069__04_013.png" alt=""/> matrix can be computed as <img src="image/B16069__04_014.png" alt=""/>. The <img src="image/B16069__04_015.png" alt=""/> matrix computed for the graph in <em class="italic">Figure 4.2</em> is shown in the following figure:</p>
			<div>
				<div id="_idContainer307" class="IMG---Figure">
					<img src="image/B16069_04_05.jpg" alt="Figure 4.5 – Solution obtained using the matrix for the graph in Figure 4.2&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – Solution obtained using the matrix for the graph in Figure 4.2</p>
			<p>From <em class="italic">Figure 4.5</em>, we can see that using the transition matrix, node 1 and node 2 have a probability of being assigned to the <img src="image/B16069__04_016.png" alt=""/> label of 0.5 and 0.33 respectively, while node 5 and node 6 have a probability of being assigned to the <img src="image/B16069__04_017.png" alt=""/> label of 0.33 and 0.5, respectively. </p>
			<p>Moreover, if we better analyze <em class="italic">Figure 4.5</em>, we can see two main problems, as follows: </p>
			<ul>
				<li>With this solution, it is possible to assign only to nodes [1 2] and [5 7] a probability associated with a label.</li>
				<li>The initial labels of nodes 0 and 6 are different from the one defined in <img src="image/B16069__04_018.png" alt=""/>. </li>
			</ul>
			<p>In order to solve the first point, the <a id="_idIndexMarker443"/>algorithm will perform <img src="image/B16069__04_019.png" alt=""/> different iterations; at each iteration <img src="image/B16069__04_020.png" alt=""/>, the algorithm will compute the <a id="_idIndexMarker444"/>solution for that iteration, as follows:</p>
			<div>
				<div id="_idContainer313" class="IMG---Figure">
					<img src="image/B16069__04_021.jpg" alt=""/>
				</div>
			</div>
			<p>The algorithm stops its iteration when a certain condition is met. The second problem is solved by the label propagation algorithm by imposing, in the solution of a given iteration <img src="image/B16069__04_022.png" alt=""/>, the labeled nodes to have the initial class values. For example, after computing the result visible in <em class="italic">Figure 4.5</em>, the algorithm will force the first line of the result matrix to be <img src="image/B16069__04_023.png" alt=""/> and the seventh line of the matrix to be <img src="image/B16069__04_024.png" alt=""/>.</p>
			<p>Here, we propose a modified version of the <strong class="source-inline">LabelPropagation</strong> class available in the <strong class="source-inline">scikit-learn</strong> library. The main reason behind this choice is given by the fact that the <strong class="source-inline">LabelPropagation</strong> class takes as input a matrix representing a dataset. Each row of the matrix represents a sample, and each column represents a feature. </p>
			<p>Before performing a <strong class="source-inline">fit</strong> operation, the <strong class="source-inline">LabelPropagation</strong> class internally executes the <strong class="source-inline">_build_graph</strong> function. This <a id="_idIndexMarker445"/>function will build, using a parametric kernel (<strong class="bold">k-nearest neighbors</strong> (<strong class="bold">kNN</strong>) and radial basis functions are available for use inside the <strong class="source-inline">_get_kernel</strong> function), a graph describing the input dataset. As a result, the original dataset is transformed into a graph (in its adjacency matrix representation) where each node is a sample (a row of the input dataset) and each edge is an <em class="italic">interaction</em> between the samples. </p>
			<p>In our specific case, the input dataset is already a graph, so we need to define a new class capable of dealing with a <strong class="source-inline">networkx</strong> graph and performing the computation operation on the original graph. The goal is achieved by creating a new class—namely, <strong class="source-inline">GraphLabelPropagation—b</strong>y extending the <strong class="source-inline">ClassifierMixin</strong>, <strong class="source-inline">BaseEstimator</strong>, and <strong class="source-inline">ABCMeta</strong> base classes. The algorithm proposed here is mainly used in order to help you understand the concept behind the algorithm. The whole algorithm is provided in the <strong class="source-inline">04_supervised_graph_machine_learning/02_Shallow_embeddings.ipynb</strong> notebook available in the <a id="_idIndexMarker446"/>GitHub repository of this book. In <a id="_idIndexMarker447"/>order to describe the algorithm, we will use only the <strong class="source-inline">fit(X,y)</strong> function as a reference. The code is illustrated in the following snippet:</p>
			<p class="source-code">class GraphLabelPropagation(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):</p>
			<p class="source-code"> </p>
			<p class="source-code">     def fit(self, X, y):</p>
			<p class="source-code">        X, y = self._validate_data(X, y)</p>
			<p class="source-code">        self.X_ = X</p>
			<p class="source-code">        check_classification_targets(y)</p>
			<p class="source-code">        D = [X.degree(n) for n in X.nodes()]</p>
			<p class="source-code">        D = np.diag(D)</p>
			<p class="source-code">        # label construction</p>
			<p class="source-code">        # construct a categorical distribution for classification only</p>
			<p class="source-code">       unlabeled_index = np.where(y==-1)[0]</p>
			<p class="source-code">       labeled_index = np.where(y!=-1)[0]</p>
			<p class="source-code">       unique_classes = np.unique(y[labeled_index])</p>
			<p class="source-code">       self.classes_ = unique_classes</p>
			<p class="source-code">       Y0 = np.array([self.build_label(y[x], len(unique_classes)) if x in labeled_index else np.zeros(len(unique_classes)) for x in range(len(y))])</p>
			<p class="source-code"> </p>
			<p class="source-code">       A = inv(D)*nx.to_numpy_matrix(G)</p>
			<p class="source-code">       Y_prev = Y0</p>
			<p class="source-code">       it = 0</p>
			<p class="source-code">       c_tool = 10</p>
			<p class="source-code">       while it &lt; self.max_iter &amp; c_tool &gt; self.tol:</p>
			<p class="source-code">           Y = A*Y_prev</p>
			<p class="source-code">           #force labeled nodes</p>
			<p class="source-code">           Y[labeled_index] = Y0[labeled_index]</p>
			<p class="source-code">           it +=1</p>
			<p class="source-code">           c_tol = np.sum(np.abs(Y-Y_prev))</p>
			<p class="source-code">           Y_prev = Y</p>
			<p class="source-code">       self.label_distributions_ = Y</p>
			<p class="source-code">       return self</p>
			<p>The <strong class="source-inline">fit(X,y)</strong> function takes as <a id="_idIndexMarker448"/>input a <strong class="source-inline">networkx</strong> graph <img src="image/B16069__04_025.png" alt=""/> and an <a id="_idIndexMarker449"/>array <img src="image/B16069__04_026.png" alt=""/> representing the labels assigned to each node. Nodes without labels should have a representative value of -1. The <strong class="source-inline">while</strong> loop performs the real computation. More precisely, it computes the <img src="image/B16069__04_027.png" alt=""/> value at each iteration and forces the labeled nodes in the solution to be equal to their original input value. The algorithm performs the computation until the two stop conditions are satisfied. In this implementation, the following two criteria have been used:</p>
			<ul>
				<li><strong class="bold">Number of iterations</strong>: The algorithm runs the computation until a given number of iterations has been performed.</li>
				<li><strong class="bold">Solution tolerance error</strong>: The algorithm runs the computation until the absolute difference of the solution obtained in two consecutive iterations, <img src="image/B16069__04_028.png" alt=""/> and <img src="image/B16069__04_029.png" alt=""/>, is lower than a given threshold value.</li>
			</ul>
			<p>The algorithm can be applied to the <a id="_idIndexMarker450"/>example <a id="_idIndexMarker451"/>graph depicted in <em class="italic">Figure 4.2</em> using the following code:</p>
			<p class="source-code">glp = GraphLabelPropagation()</p>
			<p class="source-code">y = np.array([-1 for x in range(len(G.nodes()))])</p>
			<p class="source-code">y[0] = 0</p>
			<p class="source-code">y[6] = 1</p>
			<p class="source-code">glp.fit(G,y)</p>
			<p class="source-code"> glp.predict_proba(G)</p>
			<p>The result obtained by the algorithm is shown in the following diagram:</p>
			<div>
				<div id="_idContainer322" class="IMG---Figure">
					<img src="image/B16069_04_06.jpg" alt="Figure 4.6 – Result of the label propagation algorithm on the graph of Figure 4.2: on the left, the final labeled graph; on the right, the final probability assignment matrix"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Result of the label propagation algorithm on the graph of Figure 4.2: on the left, the final labeled graph; on the right, the final probability assignment matrix</p>
			<p>In <em class="italic">Figure 4.6</em>, we can see the <a id="_idIndexMarker452"/>results of the algorithm applied to the example shown in <em class="italic">Figure 4.2</em>. From the final probability <a id="_idIndexMarker453"/>assignment matrix, it is possible to see how the probability of the initial labeled nodes is 1 due to the constraints of the algorithm and how nodes that are "near" to labeled nodes get their label.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor070"/>Label spreading algorithm</h2>
			<p>The label spreading algorithm is <a id="_idIndexMarker454"/>another semi-supervised shallow embedding algorithm. It was built in order to overcome one big <a id="_idIndexMarker455"/>limitation of the label propagation <a id="_idIndexMarker456"/>method: the <strong class="bold">initial labeling</strong>. Indeed, according to the label propagation algorithm, the initial labels cannot be modified in the training process and, in each iteration, they are forced to be equal to their original value. This constraint could generate incorrect results when the initial labeling is affected by errors or noise. As a consequence, the error will be propagated in all nodes of the input graph. </p>
			<p>In order to solve this limitation, the label spreading algorithm tries to relax the constraint of the original labeled data, allowing the labeled input nodes to change their label during the training process. </p>
			<p>Formally, let <img src="image/B16069__04_030.png" alt=""/> be a graph and let <img src="image/B16069__04_031.png" alt=""/> be a set of labels (since the algorithm is semi-supervised, just a subset of nodes will have an assigned label), and let <img src="image/B16069__04_032.png" alt=""/> and <img src="image/B16069__04_033.png" alt=""/> be the adjacency matrix diagonal degree matrix of graph G, respectively. Instead of computing the probability transition matrix, the label spreading algorithm uses the <a id="_idIndexMarker457"/>normalized graph <strong class="bold">Laplacian matrix</strong>, defined as follows:</p>
			<div>
				<div id="_idContainer327" class="IMG---Figure">
					<img src="image/B16069__04_034.jpg" alt=""/>
				</div>
			</div>
			<p>As with label propagation, this <a id="_idIndexMarker458"/>matrix can be seen as a sort of compact low-dimensional representation of the connections defined in the <a id="_idIndexMarker459"/>whole graph. This matrix can be easily computed using <strong class="source-inline">networkx</strong> with the following code:</p>
			<p class="source-code">from scipy.linalg import fractional_matrix_power</p>
			<p class="source-code">D_inv = fractional_matrix_power(D, -0.5)</p>
			<p class="source-code"> L = D_inv*nx.to_numpy_matrix(G)*D_inv</p>
			<p>As a result, we get the following:</p>
			<div>
				<div id="_idContainer328" class="IMG---Figure">
					<img src="image/B16069_04_07.jpg" alt="Figure 4.7 – The normalized graph Laplacian matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – The normalized graph Laplacian matrix</p>
			<p>The most important difference between the label spreading and label propagation algorithms is related to the function used to extract the labels. If we defined with <img src="image/B16069__04_035.png" alt=""/> the initial label assignment, the probability of a label assignment for each node obtained using the <img src="image/B16069__04_036.png" alt=""/> matrix can be computed as follows:</p>
			<div>
				<div id="_idContainer331" class="IMG---Figure">
					<img src="image/B16069__04_037.jpg" alt=""/>
				</div>
			</div>
			<p>As with label propagation, label spreading has an iterative process to compute the final solution. The algorithm will perform <img src="image/B16069__04_038.png" alt=""/> different iterations; in each iteration <img src="image/B16069__04_039.png" alt=""/>, the algorithm will compute the solution for that iteration, as follows:</p>
			<div>
				<div id="_idContainer334" class="IMG---Figure">
					<img src="image/B16069__04_040.jpg" alt=""/>
				</div>
			</div>
			<p>The algorithm stops its iteration <a id="_idIndexMarker460"/>when a certain condition is met. It is important to underline the term <img src="image/B16069__04_041.png" alt=""/> of the equation. Indeed, as we said, label spreading does not force the labeled element of the <a id="_idIndexMarker461"/>solution to be equal to its original value. Instead, the algorithm uses a regularization parameter <img src="image/B16069__04_042.png" alt=""/> to weight the influence of the original solution at each iteration. This allows us to explicitly impose the "quality" of the original solution and its influence in the final solution.</p>
			<p>As with the label propagation algorithm, in the following code snippet, we propose a modified version of the <strong class="source-inline">LabelSpreading</strong> class available in the <strong class="source-inline">scikit-learn</strong> library due to the motivations we already mentioned in the previous section. We propose the <strong class="source-inline">GraphLabelSpreading</strong> class by extending our <strong class="source-inline">GraphLabelPropagation</strong> class, since the only difference will be in the <strong class="source-inline">fit()</strong> method of the class. The whole algorithm is provided in the <strong class="source-inline">04_supervised_graph_machine_learning/02_Shallow_embeddings.ipynb</strong> notebook available in the GitHub repository of this book:</p>
			<p class="source-code">class GraphLabelSpreading(GraphLabelPropagation):</p>
			<p class="source-code">    def fit(self, X, y):</p>
			<p class="source-code">        X, y = self._validate_data(X, y)</p>
			<p class="source-code">        self.X_ = X</p>
			<p class="source-code">        check_classification_targets(y)</p>
			<p class="source-code">        D = [X.degree(n) for n in X.nodes()]</p>
			<p class="source-code">        D = np.diag(D)</p>
			<p class="source-code">        D_inv = np.matrix(fractional_matrix_power(D,-0.5))</p>
			<p class="source-code">        L = D_inv*nx.to_numpy_matrix(G)*D_inv</p>
			<p class="source-code">        # label construction</p>
			<p class="source-code">        # construct a categorical distribution for classification only</p>
			<p class="source-code">        labeled_index = np.where(y!=-1)[0]</p>
			<p class="source-code">        unique_classes = np.unique(y[labeled_index])</p>
			<p class="source-code">        self.classes_ = unique_classes</p>
			<p class="source-code">         Y0 = np.array([self.build_label(y[x], len(unique_classes)) if x in labeled_index else np.zeros(len(unique_classes)) for x in range(len(y))])</p>
			<p class="source-code"> </p>
			<p class="source-code">        Y_prev = Y0</p>
			<p class="source-code">        it = 0</p>
			<p class="source-code">        c_tool = 10</p>
			<p class="source-code">        while it &lt; self.max_iter &amp; c_tool &gt; self.tol:</p>
			<p class="source-code">           Y = (self.alpha*(L*Y_prev))+((1-self.alpha)*Y0)</p>
			<p class="source-code">            it +=1</p>
			<p class="source-code">            c_tol = np.sum(np.abs(Y-Y_prev))</p>
			<p class="source-code">            Y_prev = Y</p>
			<p class="source-code">        self.label_distributions_ = Y</p>
			<p class="source-code">        return self</p>
			<p>Also in this class, the <strong class="source-inline">fit()</strong> function is the focal point. The function takes as input a <strong class="source-inline">networkx</strong> graph <img src="image/B16069__04_043.png" alt=""/> and an array <img src="image/B16069__04_044.png" alt=""/> representing the labels assigned to each node. Nodes without labels should have a representative value of -1. The <strong class="source-inline">while</strong> loop computes the <img src="image/B16069__04_045.png" alt=""/> value at each iteration, weighting the influence of the initial labeling via the parameter <img src="image/B16069__04_046.png" alt=""/>. Also, for <a id="_idIndexMarker462"/>this algorithm, the <a id="_idIndexMarker463"/>number of iterations and the difference between two consecutive solutions are used as stop criteria.</p>
			<p>The algorithm can be applied to the example graph depicted in <em class="italic">Figure 4.2</em> using the following code:</p>
			<p class="source-code">gls = GraphLabelSpreading()</p>
			<p class="source-code">y = np.array([-1 for x in range(len(G.nodes()))])</p>
			<p class="source-code">y[0] = 0</p>
			<p class="source-code">y[6] = 1</p>
			<p class="source-code">gls.fit(G,y)</p>
			<p class="source-code"> gls.predict_proba(G)</p>
			<p>In the following diagram, the result obtained by the algorithm is shown:</p>
			<div>
				<div id="_idContainer341" class="IMG---Figure">
					<img src="image/B16069_04_08.jpg" alt="Figure 4.8 – Result of the label propagation algorithm on graph in Figure 4.2: on the left, the final labeled graph; on the right, the final probability assignment matrix"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – Result of the label propagation algorithm on graph in Figure 4.2: on the left, the final labeled graph; on the right, the final probability assignment matrix</p>
			<p>The result visible in the diagram shown in <em class="italic">Figure 4.8</em> looks similar to the one obtained using the label propagation algorithm. The main difference is related to the probability of label assignment. Indeed, in this case, it is possible to see how nodes 0 and 6 (the ones having an initial labeling) have a <a id="_idIndexMarker464"/>probability of 0.5, which is <a id="_idIndexMarker465"/>significantly lower compared to the probability of 1 obtained using the label propagation algorithm. This behavior is expected since the influence of the initial label assignment is weighted by the regularization parameter <img src="image/B16069__04_047.png" alt=""/>. </p>
			<p>In the next section, we will continue our description of supervised graph embedding methods. We will describe how network-based information helps regularize the training and create more robust models.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor071"/>Graph regularization methods</h1>
			<p>Shallow embedding methods <a id="_idIndexMarker466"/>described in the previous section show how topological information and relations between data points can be encoded and leveraged in order to build more robust classifiers and address semi-supervised tasks. In general terms, network information can be extremely useful in constraining models and enforcing the output to be smooth within neighboring nodes. As we have already seen in previous sections, this idea can be efficiently used in semi-supervised tasks, when propagating the information on neighbor unlabeled nodes. </p>
			<p>On the other hand, this can also be used to regularize the learning phase in order to create more robust models that tend to better generalize to unseen examples. Both the label propagation and the label spreading algorithms we have seen previously can be implemented as a cost function to be minimized when we add an additional regularization term. Generally, in supervised tasks, we can write the cost function to be minimized in the following form:</p>
			<div>
				<div id="_idContainer343" class="IMG---Figure">
					<img src="image/B16069__04_048.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069__04_049.png" alt=""/> and <img src="image/B16069__04_050.png" alt=""/> represent the labeled and unlabeled samples, and the second term acts as a regularization term that depends on the topological information of the graph <img src="image/B16069__04_051.png" alt=""/>.</p>
			<p>In this section, we will further <a id="_idIndexMarker467"/>describe such an idea and see how this can be very powerful, especially when regularizing the training of neural networks, which—as you might know—naturally tend to overfit and/or need large amounts of data to be trained efficiently. </p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor072"/>Manifold regularization and semi-supervised embedding </h2>
			<p><strong class="bold">Manifold regularization</strong> (Belkin et al., 2006) extends <a id="_idIndexMarker468"/>the label propagation <a id="_idIndexMarker469"/>framework by parametrizing the <a id="_idIndexMarker470"/>model function in the <strong class="bold">reproducing kernel Hilbert space</strong> (<strong class="bold">RKHS</strong>) and using as a <a id="_idIndexMarker471"/>supervised loss function (first term in the previous equation) the <strong class="bold">mean square error</strong> (<strong class="bold">MSE</strong>) or the <a id="_idIndexMarker472"/>hinge loss. In <a id="_idIndexMarker473"/>other words, when training an SVM or a least squares fit, they apply a graph regularization term based on the Laplacian matrix <em class="italic">L</em>, as follows:</p>
			<div>
				<div id="_idContainer347" class="IMG---Figure">
					<img src="image/B16069__04_052.jpg" alt=""/>
				</div>
			</div>
			<p>For this reason, these <a id="_idIndexMarker474"/>methods are generally labeled as <strong class="bold">Laplacian regularization</strong>, and such a <a id="_idIndexMarker475"/>formulation leads to <strong class="bold">Laplacian regularized least squares</strong> (<strong class="bold">LapRLS</strong>) and <strong class="bold">LapSVM</strong> classifications. Label <a id="_idIndexMarker476"/>propagation and label spreading can be seen as a special case of manifold regularization. Besides, these algorithms can also be used in the <a id="_idIndexMarker477"/>case of no-labeled data (first term in the equation disappearing) reducing to <strong class="bold">Laplacian eigenmaps</strong>. </p>
			<p>On the other hand, they can also be used in the case of a fully labeled dataset, in which case the preceding terms constrain the training phase to regularize the training and achieve more robust models. Moreover, being the classifier parametrized in the RKHS, the model can be used on unobserved samples and does not require test samples to belong to the input graph. In this sense, it is therefore an <em class="italic">inductive</em> model. </p>
			<p><strong class="bold">Manifold learning</strong> still represents a <a id="_idIndexMarker478"/>form of shallow learning, whereby the parametrized function does not leverage on any form of intermediate embeddings. <strong class="bold">Semi-supervised embedding</strong> (Weston et al., 2012) extends the concepts of graph regularization to deeper architectures by imposing the constraint and the smoothness of the function on intermediate layers of a neural network. Let's define <img src="image/B16069__04_053.png" alt=""/> as the intermediate output of the <em class="italic">k</em>th hidden layer. The regularization term proposed in the semi-supervised embedding framework reads as follows:</p>
			<div>
				<div id="_idContainer349" class="IMG---Figure">
					<img src="image/B16069__04_054.jpg" alt=""/>
				</div>
			</div>
			<p>Depending on where the <a id="_idIndexMarker479"/>regularization is imposed, three <a id="_idIndexMarker480"/>different configurations (shown in <em class="italic">Figure 4.9</em>) can be achieved, as follows:</p>
			<ul>
				<li>Regularization is applied to the final output of the network. This corresponds to a generalization of the <a id="_idIndexMarker481"/>manifold learning technique to multilayer neural networks.</li>
				<li>Regularization is applied to an intermediate layer of the network, thus regularizing the embedding representation.</li>
				<li>Regularization is <a id="_idIndexMarker482"/>applied to an auxiliary network that shares the first k-1 layers. This basically corresponds to training an unsupervised embedding network while simultaneously training a supervised network. This technique basically imposes a derived regularization on the first k-1 layers that are constrained by the unsupervised network as well and simultaneously promotes an embedding of the network nodes.</li>
			</ul>
			<p>The following diagram shows an illustration of the three different configurations—with their similarities and differences—that can be achieved using a semi-supervised embedding framework: </p>
			<div>
				<div id="_idContainer350" class="IMG---Figure">
					<img src="image/B16069_04_09.jpg" alt="Figure 4.9 – Semi-supervised embedding regularization configurations: graph regularization, indicated by the cross, can be applied to the output (left), to an intermediate layer (center), or to an auxiliary network (right)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – Semi-supervised embedding regularization configurations: graph regularization, indicated by the cross, can be applied to the output (left), to an intermediate layer (center), or to an auxiliary network (right)</p>
			<p>In its original formulation, the <a id="_idIndexMarker483"/>loss function used for the embeddings is the one derived from the Siamese network formulation, shown as follows:</p>
			<div>
				<div id="_idContainer351" class="IMG---Figure">
					<img src="image/B16069__04_055.jpg" alt=""/>
				</div>
			</div>
			<p>As can be seen by this <a id="_idIndexMarker484"/>equation, the loss function ensures the embeddings of neighboring nodes stay close. On the other hand, non-neighbors <a id="_idIndexMarker485"/>are instead pulled apart to a <a id="_idIndexMarker486"/>distance (at least) specified by the threshold <img src="image/B16069__04_056.png" alt=""/>. As compared to the regularization based on the Laplacian <img src="image/B16069__04_057.png" alt=""/> (although for neighboring points, the penalization factor is effectively recovered), the one shown here is generally easier to be optimized by gradient descent. </p>
			<p>The best choice among the three configurations presented in <em class="italic">Figure 4.9</em> is largely influenced by the data at your disposal as well as on your specific use case—that is, whether you need a regularized model output or to learn a high-level data representation. However, you should always keep in mind that when using softmax layers (usually done at the output layer), the regularization based on the hinge loss may not be very appropriate or suited for log probabilities. In such cases, regularized embeddings and relative loss should instead be introduced at intermediate layers. However, be aware that <a id="_idIndexMarker487"/>embeddings lying in <a id="_idIndexMarker488"/>deeper layers are generally harder to be <a id="_idIndexMarker489"/>trained and require a careful <a id="_idIndexMarker490"/>tuning of learning rate and margins to be used. </p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor073"/>Neural Graph Learning</h2>
			<p><strong class="bold">Neural graph learning</strong> (<strong class="bold">NGL</strong>) basically generalizes the <a id="_idIndexMarker491"/>previous <a id="_idIndexMarker492"/>formulations and, as we will see, makes it possible to seamlessly apply graph regularization to any form of a neural network, including <a id="_idIndexMarker493"/>CNNs and <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>). In particular, there exists an extremely <a id="_idIndexMarker494"/>powerful framework named <strong class="bold">Neural Structured Learning</strong> (<strong class="bold">NSL</strong>) that allows us to extend in a very few lines of code a neural network implemented in TensorFlow with graph regularization. The networks can be of any kind: natural or synthetic. </p>
			<p>When synthetic, graphs can be generated in different ways, using—for instance—embeddings learned in an unsupervised manner and/or by using a similarity/distance metric between samples using their features. You can also generate synthetic graphs using adversarial examples. Adversarial examples are artificially generated samples obtained by perturbing actual (real) examples in such a way that we confound the network, trying to force a prediction error. These very carefully designed samples (obtained by perturbing a given sample in the gradient-descent direction in order to maximize errors) can be connected to their related samples, thus generating a graph. These connections can then be used to train a graph-regularized version of the network, allowing us to obtain models that are more robust against adversarially generated examples.</p>
			<p>NGL extends the regularization by augmenting the tuning parameters for graph regularization in neural networks, decomposing the contribution of labeled-labeled, labeled-unlabeled, and unlabeled-unlabeled relations using three parameters, <img src="image/B16069__04_058.png" alt=""/>, <img src="image/B16069__04_059.png" alt=""/>, and <img src="image/B16069__04_060.png" alt=""/>, respectively, as follows:</p>
			<p><img src="image/B16069__04_061.png" alt=""/></p>
			<p>The function <img src="image/B16069__04_062.png" alt=""/> represents a generic distance between two vectors—for instance, the L2 norm <img src="image/B16069__04_063.png" alt=""/>. By varying the coefficients and the definition of <img src="image/B16069__04_064.png" alt=""/>, we can arrive at the different algorithms seen <a id="_idIndexMarker495"/>previously as limiting behavior, as follows: </p>
			<ul>
				<li>When <img src="image/B16069_04_065a.png" alt=""/> <img src="image/B16069_04_065b.png" alt=""/>we <a id="_idIndexMarker496"/>retrieve the non-regularized version of a neural network.</li>
				<li>When only <img src="image/B16069__04_066.png" alt=""/>, we recover a fully supervised formulation where relationships between nodes act to regularize the training.</li>
				<li>When we substitute <img src="image/B16069__04_067.png" alt=""/> (which are parametrized by a set of alpha coefficients) with a set of values <img src="image/B16069__04_068.png" alt=""/> (to be learned) that map each sample to its instance class, we recover the label propagation formulation.</li>
			</ul>
			<p>Loosely speaking, the NGL formulations can be seen as a non-linear version of the label propagation and label spreading algorithms, or as a form of a graph-regularized neural network for which the manifold learning or semi-supervising embedding can be obtained. </p>
			<p>We will now apply NGL to a practical example, where you will learn how to use graph regularization in neural networks. To do so, we will use the NLS framework (<a href="https://github.com/tensorflow/neural-structured-learning">https://github.com/tensorflow/neural-structured-learning</a>), which is a library built on top of TensorFlow that makes it possible to implement graph regularization with only a few lines of codes on top of standard neural networks. </p>
			<p>For our example, we will be using the <strong class="source-inline">Cora</strong> dataset, which is a labeled dataset that consists of 2,708 scientific papers in computer science that have been classified into seven classes. Each paper represents a node that is connected to other nodes based on citations. In total, there are 5,429 links in the network. </p>
			<p>Moreover, each node is further described by a 1,433-long vector of binary values (0 or 1) that represent a <a id="_idIndexMarker497"/>dichotomic <strong class="bold">bag-of-words</strong> (<strong class="bold">BOW</strong>) representation of the paper: a one-hot encoding algorithm indicating the presence/absence of a word in a given vocabulary made up of 1,433 terms. The <strong class="source-inline">Cora</strong> dataset can be downloaded directly from the <strong class="source-inline">stellargraph</strong> library with a few lines of code, as follows:</p>
			<p class="source-code">from stellargraph import datasets</p>
			<p class="source-code">dataset = datasets.Cora()</p>
			<p class="source-code">dataset.download()</p>
			<p class="source-code">G, labels = dataset.load()</p>
			<p>This returns two outputs, outlined as follows:</p>
			<ul>
				<li><strong class="source-inline">G</strong>, which is the citation network containing the network nodes, edges, and the features describing the BOW representation.</li>
				<li><strong class="source-inline">labels</strong>, which is a <strong class="source-inline">pandas</strong> Series that provides the mapping between the paper ID and one of the classes, as follows: <p class="source-code">['Neural_Networks', 'Rule_Learning', 'Reinforcement_Learning', </p><p class="source-code">'Probabilistic_Methods', 'Theory', 'Genetic_Algorithms', 'Case_Based']</p></li>
			</ul>
			<p>Starting from this <a id="_idIndexMarker498"/>information, we create a <a id="_idIndexMarker499"/>training set and a validation set. In the training samples, we will include information relating to neighbors (which may or may not belong to the training set and therefore have a label), and this will be used to regularize the training. </p>
			<p>Validation samples, on the other hand, will not have neighbor information and the predicted label will only depend on the node features—namely, the BOW representation. Therefore, we will leverage both labeled and unlabeled samples (semi-supervised task) in order to produce an inductive model that can also be used against unobserved samples. </p>
			<p>To start with, we conveniently structure the node features as a DataFrame, whereas we store the graph as an adjacency matrix, as follows:</p>
			<p class="source-code">adjMatrix = pd.DataFrame.sparse.from_spmatrix(</p>
			<p class="source-code">        G.to_adjacency_matrix(), </p>
			<p class="source-code">        index=G.nodes(), columns=G.nodes()</p>
			<p class="source-code">)</p>
			<p class="source-code">features = pd.DataFrame(G.node_features(), index=G.nodes())</p>
			<p>Using <strong class="source-inline">adjMatrix</strong>, we implement a helper function that is able to retrieve the closest <strong class="source-inline">topn</strong> neighbors of a node, returning the node ID and the edge weight, as illustrated in the following code snippet:</p>
			<p class="source-code">def getNeighbors(idx, adjMatrix, topn=5):</p>
			<p class="source-code">    weights = adjMatrix.loc[idx]</p>
			<p class="source-code">    neighbors = weights[weights&gt;0]\</p>
			<p class="source-code">         .sort_values(ascending=False)\</p>
			<p class="source-code">         .head(topn)</p>
			<p class="source-code">    return [(k, v) for k, v in neighbors.iteritems()]</p>
			<p>Using the preceding <a id="_idIndexMarker500"/>information together with the <a id="_idIndexMarker501"/>helper function, we can merge the information into a single DataFrame, as follows:</p>
			<p class="source-code">dataset = {</p>
			<p class="source-code">    index: {</p>
			<p class="source-code">        "id": index,</p>
			<p class="source-code">        "words": [float(x) </p>
			<p class="source-code">                  for x in features.loc[index].values], </p>
			<p class="source-code">        "label": label_index[label],</p>
			<p class="source-code">        "neighbors": getNeighbors(index, adjMatrix, topn)</p>
			<p class="source-code">    }</p>
			<p class="source-code">    for index, label in labels.items()</p>
			<p class="source-code">}</p>
			<p class="source-code">df = pd.DataFrame.from_dict(dataset, orient="index")</p>
			<p>This DataFrame represents the node-centric feature space. This would suffice if we were to use a regular classifier <a id="_idIndexMarker502"/>that does not exploit the information of the <a id="_idIndexMarker503"/>relationships between nodes. However, in order to allow the computation of the graph-regularization term, we need to join the preceding DataFrame with information relating to the neighborhood of each node. We then define a function able to retrieve and join the neighborhood information, as follows:</p>
			<p class="source-code">def getFeatureOrDefault(ith, row):</p>
			<p class="source-code">    try:</p>
			<p class="source-code">        nodeId, value = row["neighbors"][ith]</p>
			<p class="source-code">        return {</p>
			<p class="source-code">            f"{GRAPH_PREFIX}_{ith}_weight": value,</p>
			<p class="source-code">            f"{GRAPH_PREFIX}_{ith}_words": df.loc[nodeId]["words"]</p>
			<p class="source-code">        } </p>
			<p class="source-code">     except:</p>
			<p class="source-code">        return {</p>
			<p class="source-code">            f"{GRAPH_PREFIX}_{ith}_weight": 0.0,</p>
			<p class="source-code">            f"{GRAPH_PREFIX}_{ith}_words": [float(x) for x in np.zeros(1433)]</p>
			<p class="source-code">        } </p>
			<p class="source-code">def neighborsFeatures(row):</p>
			<p class="source-code">    featureList = [getFeatureOrDefault(ith, row) for ith in range(topn)]</p>
			<p class="source-code">    return pd.Series(</p>
			<p class="source-code">        {k: v </p>
			<p class="source-code">         for feat in featureList for k, v in feat.items()}</p>
			<p class="source-code">    )</p>
			<p>As shown in the preceding code snippet, when the neighbors are less than <strong class="source-inline">topn</strong>, we set the weight and the one-hot encoding of the words to <strong class="source-inline">0</strong>. The <strong class="source-inline">GRAPH_PREFIX</strong> constant is a prefix that is to be prepended to all <a id="_idIndexMarker504"/>features that will later be used by the <strong class="source-inline">nsl</strong> library to regularize the training. Although it can be changed, in the following code snippet we will keep its value equal to the default value: <strong class="source-inline">"NL_nbr"</strong>.</p>
			<p>This function can be applied to the DataFrame in order to compute the full feature space, as follows:</p>
			<p class="source-code">neighbors = df.apply(neighborsFeatures, axis=1)</p>
			<p class="source-code">allFeatures = pd.concat([df, neighbors], axis=1)</p>
			<p>We now have in <strong class="source-inline">allFeatures</strong> all the ingredients we need to implement our graph-regularized model. </p>
			<p>We start by splitting our dataset into a training set and a validation set, as follows:</p>
			<p class="source-code">n = int(np.round(len(labels)*ratio))  </p>
			<p class="source-code">labelled, unlabelled = model_selection.train_test_split(</p>
			<p class="source-code">    allFeatures, train_size=n, test_size=None, stratify=labels</p>
			<p class="source-code">)</p>
			<p>By changing the ratio, we <a id="_idIndexMarker505"/>can change the amount of labeled versus unlabeled data points. As the ratio decreases, we expect the performance of standard non-regularized classifiers to reduce. However, such a reduction can be compensated by leveraging network information provided by unlabeled data. We thus expect graph-regularized neural networks to provide better performance thanks to the augmented information they leverage. For the following code snippet, we will assume a <strong class="source-inline">ratio</strong> value equal to <strong class="source-inline">0.2</strong>.</p>
			<p>Before feeding this data into our neural network, we convert the DataFrame into a TensorFlow tensor and dataset, which is a convenient representation that will allow the model to refer to feature names in its input layers. </p>
			<p>Since the input features have different data types, it is best to handle the dataset creation separately for <strong class="source-inline">weights</strong>, <strong class="source-inline">words</strong>, and <strong class="source-inline">labels</strong> values, as follows:</p>
			<p class="source-code">train_base = {</p>
			<p class="source-code">    "words": tf.constant([</p>
			<p class="source-code">         tuple(x) for x in labelled["words"].values</p>
			<p class="source-code">    ]),</p>
			<p class="source-code">    "label": tf.constant([</p>
			<p class="source-code">         x for x in labelled["label"].values</p>
			<p class="source-code">    ])</p>
			<p class="source-code"> }</p>
			<p class="source-code">train_neighbor_words = {</p>
			<p class="source-code">    k: tf.constant([tuple(x) for x in labelled[k].values])</p>
			<p class="source-code">    for k in neighbors if "words" in k</p>
			<p class="source-code">}</p>
			<p class="source-code">train_neighbor_weights = {</p>
			<p class="source-code">^    k: tf.constant([tuple([x]) for x in labelled[k].values])</p>
			<p class="source-code">    for k in neighbors if "weight" in k</p>
			<p class="source-code">} </p>
			<p>Now that we have the tensor, we <a id="_idIndexMarker506"/>can merge <a id="_idIndexMarker507"/>all this information into a TensorFlow dataset, as follows:</p>
			<p class="source-code">trainSet = tf.data.Dataset.from_tensor_slices({</p>
			<p class="source-code">    k: v</p>
			<p class="source-code">    for feature in [train_base, train_neighbor_words,</p>
			<p class="source-code">                    train_neighbor_weights]</p>
			<p class="source-code">    for k, v in feature.items()</p>
			<p class="source-code">})</p>
			<p>We can similarly create a validation set. As mentioned previously, since we want to design an inductive algorithm, the validation dataset does not need any neighborhood information. The code is illustrated in the following snippet:</p>
			<p class="source-code">validSet = tf.data.Dataset.from_tensor_slices({</p>
			<p class="source-code">    "words": tf.constant([</p>
			<p class="source-code">       tuple(x) for x in unlabelled["words"].values</p>
			<p class="source-code">    ]),</p>
			<p class="source-code">    "label": tf.constant([</p>
			<p class="source-code">       x for x in unlabelled["label"].values</p>
			<p class="source-code">    ])</p>
			<p class="source-code"> })</p>
			<p>Before feeding the dataset <a id="_idIndexMarker508"/>into the <a id="_idIndexMarker509"/>model, we split the features from the labels, as follows:</p>
			<p class="source-code">def split(features):</p>
			<p class="source-code">    labels=features.pop("label")</p>
			<p class="source-code">    return features, labels</p>
			<p class="source-code">trainSet = trainSet.map(f)</p>
			<p class="source-code"> validSet = validSet.map(f)</p>
			<p>That's it! We have generated the inputs to our model. We could also inspect one sample batch of our dataset by printing the values of features and labels, as shown in the following code block: </p>
			<p class="source-code">for features, labels in trainSet.batch(2).take(1):</p>
			<p class="source-code">    print(features)</p>
			<p class="source-code">    print(labels)</p>
			<p>It is now time to create our first model. To do this, we start from a simple architecture that takes as input the one-hot representation and has two hidden layers, composed of a <strong class="source-inline">Dense</strong> layer plus a <strong class="source-inline">Dropout</strong> layer with 50 units each, as follows:</p>
			<p class="source-code">inputs = tf.keras.Input(</p>
			<p class="source-code">    shape=(vocabularySize,), dtype='float32', name='words'</p>
			<p class="source-code">)</p>
			<p class="source-code">cur_layer = inputs</p>
			<p class="source-code">for num_units in [50, 50]:</p>
			<p class="source-code">    cur_layer = tf.keras.layers.Dense(</p>
			<p class="source-code">        num_units, activation='relu'</p>
			<p class="source-code">    )(cur_layer)</p>
			<p class="source-code">    cur_layer = tf.keras.layers.Dropout(0.8)(cur_layer)</p>
			<p class="source-code">outputs = tf.keras.layers.Dense(</p>
			<p class="source-code">    len(label_index), activation='softmax',</p>
			<p class="source-code">    name="label"</p>
			<p class="source-code">)(cur_layer)</p>
			<p class="source-code">model = tf.keras.Model(inputs, outputs=outputs)</p>
			<p>Indeed, we could also train <a id="_idIndexMarker510"/>this model <a id="_idIndexMarker511"/>without graph regularization by simply compiling the model to create a computational graph, as follows:</p>
			<p class="source-code">model.compile(</p>
			<p class="source-code">    optimizer='adam',</p>
			<p class="source-code">    loss='sparse_categorical_crossentropy',</p>
			<p class="source-code">    metrics=['accuracy']</p>
			<p class="source-code">)</p>
			<p>And then, we could run it as usual, also allowing the history file to be written to disk in order to be monitored using <strong class="source-inline">TensorBoard</strong>, as illustrated in the following code snippet:</p>
			<p class="source-code">from tensorflow.keras.callbacks import TensorBoard</p>
			<p class="source-code">model.fit(</p>
			<p class="source-code">    trainSet.batch(128), epochs=200, verbose=1,</p>
			<p class="source-code">    validation_data=validSet.batch(128),</p>
			<p class="source-code">    callbacks=[TensorBoard(log_dir='/tmp/base)]</p>
			<p class="source-code">)</p>
			<p>At the end of the process, we should have something similar to the following output:</p>
			<p class="source-code">Epoch 200/200</p>
			<p class="source-code">loss: 0.7798 – accuracy: 06795 – val_loss: 1.5948 – val_accuracy: 0.5873</p>
			<p>With a top performance around 0.6 in accuracy, we now need to create a graph-regularized version of the preceding model. First of all, we need to recreate our model from scratch. This is <a id="_idIndexMarker512"/>important when comparing the results. If we were to use layers already initialized and used in the previous <a id="_idIndexMarker513"/>model, the layer weights would not be random but would be used with the ones already optimized in the preceding run. Once a new model has been created, adding a graph regularization technique to be used at training time can be done in just a few lines of code, as follows:</p>
			<p class="source-code">import neural_structured_learning as nsl</p>
			<p class="source-code">graph_reg_config = nsl.configs.make_graph_reg_config(</p>
			<p class="source-code">    max_neighbors=2,</p>
			<p class="source-code">    multiplier=0.1,</p>
			<p class="source-code">    distance_type=nsl.configs.DistanceType.L2,</p>
			<p class="source-code">    sum_over_axis=-1)</p>
			<p class="source-code">graph_reg= nsl.keras.GraphRegularization(</p>
			<p class="source-code">     model, graph_reg_config)</p>
			<p>Let's analyze the different hyperparameters of the regularization, as follows:</p>
			<ul>
				<li><strong class="source-inline">max_neighbors</strong> tunes the number of neighbors that ought to be used for computing the regularization loss for each node.</li>
				<li><strong class="source-inline">multiplier</strong> corresponds to the coefficients that tune the importance of the regularization loss. Since we only consider labeled-labeled and labeled-unlabeled, this effectively corresponds to <img src="image/B16069__04_069.png" alt=""/> and <img src="image/B16069__04_070.png" alt=""/>.</li>
				<li><strong class="source-inline">distance_type</strong> represents the pairwise distance <img src="image/B16069__04_071.png" alt=""/> to be used.</li>
				<li><strong class="source-inline">sum_over_axis</strong> sets whether the weighted average sum should be calculated with respect to features (when set to <strong class="source-inline">None</strong>) or to samples (when set to -1).</li>
			</ul>
			<p>The graph-regularized model can be <a id="_idIndexMarker514"/>compiled and run in the <a id="_idIndexMarker515"/>same way as before with the following commands:</p>
			<p class="source-code">graph_reg.compile(</p>
			<p class="source-code">    optimizer='adam',</p>
			<p class="source-code">    loss='sparse_categorical_crossentropy',    metrics=['accuracy']</p>
			<p class="source-code">)</p>
			<p class="source-code">model.fit(</p>
			<p class="source-code">    trainSet.batch(128), epochs=200, verbose=1,</p>
			<p class="source-code">    validation_data=validSet.batch(128),</p>
			<p class="source-code">    callbacks=[TensorBoard(log_dir='/tmp/nsl)]</p>
			<p class="source-code">)</p>
			<p>Note that the loss function now also accounts for the graph-regularization term, as defined previously. Therefore, we now also introduce information coming from neighboring nodes that regularizes the training of our neural network. The preceding code, after about 200 iterations, provides the following output:</p>
			<p class="source-code">Epoch 200/200</p>
			<p class="source-code">loss: 0.9136 – accuracy: 06405 – scaled_graph_loss: 0.0328 - val_loss: 1.2526 – val_accuracy: 0.6320</p>
			<p>As you can see, graph regularization, when compared to the vanilla version, has allowed us to boost the performance in terms of accuracy by about 5%. Not bad at all!</p>
			<p>You can perform several experiments, changing the ratio of labeled/unlabeled samples, the number of neighbors to be used, the regularization coefficient, the distance, and more. We encourage you to play around with the notebook that is provided with this book to explore the effect of different parameters yourself. </p>
			<p>In the right panel of the <a id="_idIndexMarker516"/>following screenshot, we show the dependence of the performance measured by the accuracy as the <a id="_idIndexMarker517"/>supervised ratio increases. As expected, performance increases as the ratio increases. On the left panel, we show the accuracy increments on the validation set for various configuration of neighbors and supervised ratio, defined by</p>
			<div>
				<div id="_idContainer369" class="IMG---Figure">
					<img src="image/B16069__04_072.jpg" alt=""/>
				</div>
			</div>
			<p class="figure">:</p>
			<div>
				<div id="_idContainer370" class="IMG---Figure">
					<img src="image/B16069_04_10(merged).jpg" alt="Figure 4.10 – (Left) Accuracy on the validation set for the graph-regularized neural networks with neighbors = 2 and various supervised ratios; (Right) accuracy increments on the validation set for the graph-regularized neural networks compared to the vanilla version"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – (Left) Accuracy on the validation set for the graph-regularized neural networks with neighbors = 2 and various supervised ratios; (Right) accuracy increments on the validation set for the graph-regularized neural networks compared to the vanilla version</p>
			<p>As can be seen in <em class="italic">Figure 4.10</em>, almost all graph-regularized versions outperform the vanilla models. The only exceptions are configuration neighbors = 2 and ratio = 0.5, for which the two models perform very similarly. However, the curve has a clear positive trend and we reasonably expect the graph-regularized version to outperform the vanilla model for a larger number of epochs. </p>
			<p>Note that in the notebook, we also use another interesting feature of TensorFlow for creating the datasets. Instead of using a <strong class="source-inline">pandas</strong> DataFrame, as we did previously, we will create a dataset using the TensorFlow <strong class="source-inline">Example</strong>, <strong class="source-inline">Features</strong>, and <strong class="source-inline">Feature</strong> classes, which, besides providing a high-level description of samples, also allow us to serialize the input data (using <strong class="source-inline">protobuf</strong>) to make them compatible across platforms and programming languages. </p>
			<p>If you are interested in <a id="_idIndexMarker518"/>further using <strong class="source-inline">TensorFlow</strong> both for prototyping models and deploying them into production via data-driven <a id="_idIndexMarker519"/>applications (maybe written in other languages), we strongly advise you to dig further into these concepts. </p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor074"/>Planetoid</h2>
			<p>The methods discussed <a id="_idIndexMarker520"/>so far provide graph regularization that is <a id="_idIndexMarker521"/>based on the Laplacian matrix. As we have seen in previous chapters, enforcing constraints based on <img src="image/B16069__04_073.png" alt=""/> ensures that first-order proximity is preserved. Yang et al. (2016) proposed a method to extend graph regularization in order to also account for higher-order proximities. Their approach, which they named <strong class="bold">Planetoid</strong> (short for <strong class="bold">Predicting Labels And Neighbors with Embeddings Transductively Or Inductively from Data</strong>), extends skip-gram methods used for computing node embeddings to incorporate node-label information. </p>
			<p>As we have seen in the previous chapter, skip-gram methods are based on generating random walks through a graph and then using the generated sequences to learn embeddings via a skip-gram model. The following diagram shows how the unsupervised version is modified to account for the supervised loss: </p>
			<div>
				<div id="_idContainer372" class="IMG---Figure">
					<img src="image/B16069_04_11.jpg" alt="Figure 4.11 – Sketch of the Planetoid architecture: the dashed line represents a parametrized function that allows the method to extend from transductive to inductive"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11 – Sketch of the Planetoid architecture: the dashed line represents a parametrized function that allows the method to extend from transductive to inductive</p>
			<p>As shown in <em class="italic">Figure 4.11</em>, embeddings are <a id="_idIndexMarker522"/>fed to both of the following:</p>
			<ul>
				<li>A softmax layer to <a id="_idIndexMarker523"/>predict the graph context of the sampled random-walk sequences</li>
				<li>A set of hidden layers that combine together with the hidden layers derived from the node features in order to predict the class labels</li>
			</ul>
			<p>The cost function to be minimized to train the combined network is composed of a supervised and an unsupervised loss—<img src="image/B16069__04_074.png" alt=""/> and <img src="image/B16069__04_075.png" alt=""/>, respectively. The unsupervised loss is analogous to the one used with skip-gram with negative sampling, whereas the supervised loss minimizes the conditional probability and can be written as follows:</p>
			<div>
				<div id="_idContainer375" class="IMG---Figure">
					<img src="image/B16069__04_076.jpg" alt=""/>
				</div>
			</div>
			<p>The preceding formulation is <em class="italic">transductive</em> as it requires samples to belong to the graph in order to be applied. In a semi-supervised task, this method can be efficiently used to predict labels for unlabeled examples. However, it cannot be used for unobserved samples. As shown by the dashed line in <em class="italic">Figure 4.11</em>, an inductive version of the Planetoid algorithm can be obtained by <a id="_idIndexMarker524"/>parametrizing the <a id="_idIndexMarker525"/>embeddings as a function of the node features, via dedicated connected layers. </p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor075"/>Graph CNNs</h1>
			<p>In <a href="B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 3</em></a>, <em class="italic">Unsupervised Graph Learning</em>, we have learned the main concepts behind GNNs and <strong class="bold">graph convolutional networks</strong> (<strong class="bold">GCNs</strong>). We have also learned the difference between spectral graph convolution and spatial graph convolution. More precisely, we have <a id="_idIndexMarker526"/>further seen that GCN layers can be used to encode graphs or nodes under unsupervised settings by learning how to preserve graph properties such as node similarity.</p>
			<p>In this chapter, we will explore such methods under supervised settings. This time, our goal is to learn graphs or node representations that can accurately <em class="italic">predict node or graph labels</em>. It is indeed worth noting that the encoding function remains the same. What will change is the objective!</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor076"/>Graph classification using GCNs</h2>
			<p>Let's consider again our <strong class="source-inline">PROTEINS</strong> dataset. Let's <a id="_idIndexMarker527"/>load the dataset as follows:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">from stellargraph import datasets</p>
			<p class="source-code">dataset = datasets.PROTEINS()</p>
			<p class="source-code">graphs, graph_labels = dataset.load()</p>
			<p class="source-code"># necessary for converting default string labels to int</p>
			<p class="source-code">labels = pd.get_dummies(graph_labels, drop_first=True)</p>
			<p>In the following example, we are going to use (and compare) one of the most widely used GCN algorithms for graph classification: <em class="italic">GCN</em> by Kipf and Welling:</p>
			<ol>
				<li value="1"><strong class="source-inline">stellargraph</strong>, which we are using for building the model, uses <strong class="source-inline">tf.Keras</strong> as the backend. According to its specific criteria, we need a data generator to feed the model. More precisely, since we are addressing a supervised graph classification problem, we can use an instance of the <strong class="source-inline">PaddedGraphGenerator</strong> class of <strong class="source-inline">stellar</strong><strong class="source-inline">graph</strong>, which automatically resolves differences in the number of nodes by using padding. Here is the code required for this step:<p class="source-code">from stellargraph.mapper import PaddedGraphGenerator</p><p class="source-code">generator = PaddedGraphGenerator(graphs=graphs)</p></li>
				<li>We are now ready to actually create our first model. We will create and stack together four GCN layers <a id="_idIndexMarker528"/>through the <strong class="source-inline">utility</strong> function of <strong class="source-inline">stellargraph</strong>, as follows:<p class="source-code">from stellargraph.layer import DeepGraphCNN</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p><p class="source-code">from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten</p><p class="source-code">from tensorflow.keras.losses import binary_crossentropy</p><p class="source-code">import tensorflow as tf</p><p class="source-code">nrows = 35  # the number of rows for the output tensor</p><p class="source-code">layer_dims = [32, 32, 32, 1]</p><p class="source-code"># backbone part of the model (Encoder)</p><p class="source-code"> dgcnn_model = DeepGraphCNN(</p><p class="source-code">    layer_sizes=layer_dims,</p><p class="source-code">    activations=["tanh", "tanh", "tanh", "tanh"],</p><p class="source-code">    k=nrows,</p><p class="source-code">    bias=False,</p><p class="source-code">    generator=generator,</p><p class="source-code">)</p></li>
				<li>This <em class="italic">backbone</em> will be concatenated to <strong class="bold">one-dimensional</strong> (<strong class="bold">1D</strong>) convolutional layers and fully connected layers using <strong class="source-inline">tf.Keras</strong>, as follows:<p class="source-code"># necessary for connecting the backbone to the head</p><p class="source-code">gnn_inp, gnn_out = dgcnn_model.in_out_tensors()</p><p class="source-code"># head part of the model (classification)</p><p class="source-code"> x_out = Conv1D(filters=16, kernel_size=sum(layer_dims), strides=sum(layer_dims))(gnn_out)</p><p class="source-code">x_out = MaxPool1D(pool_size=2)(x_out)</p><p class="source-code"> x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)</p><p class="source-code">x_out = Flatten()(x_out)</p><p class="source-code"> x_out = Dense(units=128, activation="relu")(x_out)</p><p class="source-code"> x_out = Dropout(rate=0.5)(x_out)</p><p class="source-code">predictions = Dense(units=1, activation="sigmoid")(x_out)</p></li>
				<li>Let's create and compile a model using <strong class="source-inline">tf.Keras</strong> utilities. We will train the model with a <strong class="source-inline">binary_crossentropy</strong> loss function (to measure the difference between <a id="_idIndexMarker529"/>predicted labels and ground truth) with the <strong class="source-inline">Adam</strong> optimizer and a <em class="italic">learning rate</em> of 0.0001. We will also monitor the accuracy metric while training. The code is illustrated in the following snippet:<p class="source-code">model = Model(inputs=gnn_inp, outputs=predictions)</p><p class="source-code">model.compile(optimizer=Adam(lr=0.0001), loss=binary_crossentropy, metrics=["acc"])</p></li>
				<li>We can now exploit <strong class="source-inline">scikit-learn</strong> utilities to create train and test sets. In our experiments, we will be using 70% of the dataset as a training set and the remainder as a test set. In addition, we need to use the <strong class="source-inline">flow</strong> method of the generator to supply them to the model. The <a id="_idIndexMarker530"/>code to achieve this is shown in the following snippet:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">train_graphs, test_graphs = train_test_split(</p><p class="source-code">graph_labels, test_size=.3, stratify=labels,)</p><p class="source-code">gen = PaddedGraphGenerator(graphs=graphs)</p><p class="source-code">train_gen = gen.flow(</p><p class="source-code">    list(train_graphs.index - 1),</p><p class="source-code">    targets=train_graphs.values,</p><p class="source-code">    symmetric_normalization=False,</p><p class="source-code">    batch_size=50,</p><p class="source-code">)</p><p class="source-code">test_gen = gen.flow(</p><p class="source-code">    list(test_graphs.index - 1),</p><p class="source-code">    targets=test_graphs.values,</p><p class="source-code">    symmetric_normalization=False,</p><p class="source-code">    batch_size=1,</p><p class="source-code">)</p></li>
				<li>It's now time for training. We train the model for 100 epochs. However, feel free to play with the hyperparameters to gain better performance. Here is the code for this:<p class="source-code">epochs = 100</p><p class="source-code">history = model.fit(train_gen, epochs=epochs, verbose=1,</p><p class="source-code"> validation_data=test_gen, shuffle=True,)</p><p>After 100 epochs, this should be the <a id="_idIndexMarker531"/>output:</p><p class="source-code">Epoch 100/100</p><p class="source-code">loss: 0.5121 – acc: 0.7636 – val_loss: 0.5636 – val_acc: 0.7305</p></li>
			</ol>
			<p>Here, we are achieving about 76% accuracy on the training set and about 73% accuracy on the test set.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor077"/>Node classification using GraphSAGE</h2>
			<p>In the next example, we will train <strong class="source-inline">GraphSAGE</strong> to classify <a id="_idIndexMarker532"/>nodes of the <strong class="source-inline">Cora</strong> dataset.</p>
			<p> Let's first load the <a id="_idIndexMarker533"/>dataset using <strong class="source-inline">stellargraph</strong> utilities, as follows:</p>
			<p class="source-code">dataset = datasets.Cora()</p>
			<p class="source-code">G, nodes = dataset.load()</p>
			<p>Follow this list of steps to train <strong class="source-inline">GraphSAGE</strong> to classify nodes of the <strong class="source-inline">Cora</strong> dataset:</p>
			<ol>
				<li value="1">As in the previous example, the first step is to split the dataset. We will be using 90% of the dataset as a training set and the remainder for testing. Here is the code for this step:<p class="source-code">train_nodes, test_nodes = train_test_split(nodes, train_size=0.1,test_size=None, stratify=nodes)</p></li>
				<li>This time, we will <a id="_idIndexMarker534"/>convert labels using <strong class="bold">one-hot representation</strong>. This representation is often used for classification tasks and usually leads to better performance. Specifically, let <strong class="source-inline">c</strong> be the number of possible targets (seven, in the case of the <strong class="source-inline">Cora</strong> dataset), and each label will be converted in a vector of size <strong class="source-inline">c</strong>, where all the elements are <strong class="source-inline">0</strong> except for the one corresponding to the target class. The code is illustrated in the following snippet:<p class="source-code">from sklearn import preprocessing</p><p class="source-code">label_encoding = preprocessing.LabelBinarizer()</p><p class="source-code">train_labels = label_encoding.fit_transform(train_nodes)</p><p class="source-code"> test_labels = label_encoding.transform(test_nodes)</p></li>
				<li>Let's create a <a id="_idIndexMarker535"/>generator to feed the data into the model. We will be <a id="_idIndexMarker536"/>using an instance of the <strong class="source-inline">GraphSAGENodeGenerator</strong> class of <strong class="source-inline">stellargraph</strong>. We will use the <strong class="source-inline">flow</strong> method to feed the model with the train and test sets, as follows:<p class="source-code">from stellargraph.mapper import GraphSAGENodeGenerator</p><p class="source-code">batchsize = 50</p><p class="source-code">n_samples = [10, 5, 7]</p><p class="source-code"> generator = GraphSAGENodeGenerator(G, batchsize, n_samples)</p><p class="source-code">train_gen = generator.flow(train_nodes.index, train_labels, shuffle=True)</p><p class="source-code"> test_gen = generator.flow(test_labels.index, test_labels)</p></li>
				<li>Finally, let's create the model and compile it. For this exercise, we will be using a <strong class="source-inline">GraphSAGE</strong> encoder with three layers of 32, 32, and 16 dimensions, respectively. The encoder will then be connected to a dense layer with <em class="italic">softmax</em> activation to perform the classification. We will use an <strong class="source-inline">Adam</strong> optimizer with a <em class="italic">learning rate</em> of 0.03 and <strong class="source-inline">categorical_crossentropy</strong> as the loss function. The code is illustrated in the following snippet:<p class="source-code">from stellargraph.layer import GraphSAGE</p><p class="source-code">from tensorflow.keras.losses import categorical_crossentropy</p><p class="source-code">graphsage_model = GraphSAGE(layer_sizes=[32, 32, 16], generator=generator, bias=True, dropout=0.6,)</p><p class="source-code">gnn_inp, gnn_out = graphsage_model.in_out_tensors()</p><p class="source-code">outputs = Dense(units=train_labels.shape[1], activation="softmax")(gnn_out)</p><p class="source-code"># create the model and compile</p><p class="source-code">model = Model(inputs=gnn_inp, outputs=outputs)</p><p class="source-code">model.compile(optimizer=Adam(lr=0.003), loss=categorical_crossentropy, metrics=["acc"],)</p></li>
				<li>It's now time to <a id="_idIndexMarker537"/>train the model. We will train the <a id="_idIndexMarker538"/>model for 20 epochs, as follows: <p class="source-code">model.fit(train_gen, epochs=20, validation_data=test_gen, verbose=2, shuffle=False)</p></li>
				<li>This should be the output:<p class="source-code">Epoch 20/20</p><p class="source-code">loss: 0.8252 – acc: 0.8889 – val_loss: 0.9070 – val_acc: 0.8011</p></li>
			</ol>
			<p>We achieved about 89% accuracy over the training set and about 80% accuracy over the test set.</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor078"/>Summary </h1>
			<p>In this chapter, we have learned how supervised ML can be effectively applied on graphs to solve real problems such as node and graph classification. </p>
			<p>In particular, we first analyzed how graph and node properties can be directly used as features to train classic ML algorithms. We have seen shallow methods and simple approaches to learning node, edge, or graph representations for only a finite set of input data.</p>
			<p>We have than learned how regularization techniques can be used during the learning phase in order to create more robust models that tend to generalize better.</p>
			<p>Finally, we have seen how GNNs can be applied to solve supervised ML problems on graphs. </p>
			<p>But what can those algorithms be useful for? In the next chapter, we will explore common problems on graphs that need to be solved through ML techniques.</p>
		</div>
	</body></html>