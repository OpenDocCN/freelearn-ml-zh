<html><head></head><body>
		<div id="_idContainer089">
			<h1 id="_idParaDest-96" class="chapter-number"><a id="_idTextAnchor095"/>5</h1>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor096"/>Data Analysis</h1>
			<p>One of the fundamental principles behind any large-scale data science procedure is the simple fact that any <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) model produced is only as good as the data on which it is trained. Beginner data scientists often make the mistake of assuming that they just need to find the right ML model for their use case and then simply train or fit the data to the model. However, nothing could be further from the truth. Getting the best possible model requires exploring the data, with the goal being to fully understand the data. Once the data scientist understands the data and how the ML model can be trained on it, the data scientist often spends most of their time further cleaning and modifying the data, also referred to as wrangling the data, to prepare it for model training <span class="No-Break">and building.</span></p>
			<p>While this data analysis task may seem conceptually straightforward, the task becomes far more complicated when we factor in the <em class="italic">type</em> (images, text, tabular, and so on) and the <em class="italic">amount</em>/<em class="italic">volume</em> of data we are exploring. Furthermore, where the data is stored and getting access to it can also make the exercise even more overwhelming for the data scientist. For example, useful ML data may be stored within a data warehouse or located within various relational databases, often requiring various tools or programmatic API calls to mine the right data. Likewise, key information may be located across multiple file servers or within various buckets of a cloud-based object store. Locating the data and ensuring the correct permissions to access the data can further delay a data scientist from <span class="No-Break">getting started.</span></p>
			<p>So, with these challenges in mind, in this chapter, we will review some practical ways to explore, understand, and essentially wrangle different types, as well as large quantities of data, to train ML models. Additionally, we will examine some of the capabilities and services that AWS provides to make this task <span class="No-Break">less daunting.</span></p>
			<p>Therefore, this chapter will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Exploring data <span class="No-Break">analysis methods</span></li>
				<li>Reviewing AWS services for <span class="No-Break">data analysis</span></li>
				<li>Analyzing large amounts of structured and <span class="No-Break">unstructured data</span></li>
				<li>Processing data at scale <span class="No-Break">on AWS</span></li>
			</ul>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor097"/>Technical requirements</h1>
			<p>You should have the following prerequisites before getting started with <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Familiarity with AWS services and their <span class="No-Break">basic usage.</span></li>
				<li>A web browser (for the best experience, it is recommended that you use a Chrome or <span class="No-Break">Firefox browser).</span></li>
				<li>An AWS account (if you are unfamiliar with how to get started with an AWS account, you can go to this <span class="No-Break">link: </span><a href="https://aws.amazon.com/getting-started/"><span class="No-Break">https://aws.amazon.com/getting-started/</span></a><span class="No-Break">).</span></li>
				<li>Familiarity with the AWS Free Tier (the Free Tier will allow you to access some of the AWS services for free, depending on resource limits. You can familiarize yourself with these limits at this <span class="No-Break">link: </span><a href="https://aws.amazon.com/free/"><span class="No-Break">https://aws.amazon.com/free/</span></a><span class="No-Break">).</span></li>
				<li>Example Jupyter notebooks for this chapter are provided in the companion GitHub <span class="No-Break">repository (</span><a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter05"><span class="No-Break">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter05</span></a><span class="No-Break">).</span></li>
			</ul>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor098"/>Exploring data analysis methods</h1>
			<p>As highlighted at the outset <a id="_idIndexMarker434"/>of this chapter, the task of gathering and exploring these various sources of data can seem somewhat daunting. So, you may be wondering at this point <em class="italic">where and how to begin the data analysis process?</em> To answer this question, let’s explore some of the methods we can use to analyze your data and prepare it for the <span class="No-Break">ML task.</span></p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor099"/>Gathering the data</h2>
			<p>One of the first<a id="_idIndexMarker435"/> steps to getting started with a data analysis task is to gather the relevant data from various silos into a specific location. This single location is commonly referred to as a data lake. Once the relevant data has been co-located into a single data lake, the activity<a id="_idIndexMarker436"/> of moving data in or out of the lake becomes <span class="No-Break">significantly easier.</span></p>
			<p>For example, let’s imagine for<a id="_idIndexMarker437"/> a moment that a data scientist is tasked with building a product recommendation model. Using the data lake as a central store, they can query a customer database to get all the customer-specific data, typically from a relational database or a data warehouse, and then combine the customer’s clickstream data from the web application’s transaction logs to get a common source of all the required information to predict product recommendations. Moreover, by sourcing product-specific image data from the product catalog, the data scientist can further explore the various characteristics of product images that may enhance or contribute to the ML model’s <span class="No-Break">predictive potential.</span></p>
			<p>So, once that holistic dataset is gathered together and stored in a common repository or data store, we can move on to the next approach to data analysis, which is understanding the <span class="No-Break">data structure.</span></p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor100"/>Understanding the data structure</h2>
			<p>Once the data has been <a id="_idIndexMarker438"/>gathered into a common location, before a data scientist can fully investigate how it can be used to suggest an ML hypothesis, we need to understand the structure of the data. Since the dataset may be created from multiple sources, understanding the structure of the data is important before it can be <span class="No-Break">analyzed effectively.</span></p>
			<p>For instance, if we continue with the product recommendation example, the data scientist may work with structured customer data in the form of a tabular dataset from a relational database or data warehouse. Added to this, when pulling the customer interaction data from the web servers, the data scientist may work with time series or JSON formatted data, commonly referred to as<a id="_idIndexMarker439"/> semi-structured data. Lastly, when incorporating product images into the mix, the data scientist will deal with image data, which is an example of <span class="No-Break">unstructured data.</span></p>
			<p>So, understanding the nature or structure of the data determines how to extract the critical information we need and analyze it effectively. Moreover, knowing the type of data we’re dealing with will also influence the type of tools, such as <strong class="bold">Application Programming Interfaces</strong> (<strong class="bold">APIs</strong>), and <a id="_idIndexMarker440"/>even the infrastructure resources required to <span class="No-Break">understand data.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">We will be exploring these tools and infrastructure resources in depth further on in <span class="No-Break">the chapter.</span></p>
			<p>Once we understand the <a id="_idIndexMarker441"/>data structure, we can apply this understanding to another technique of data analysis, that is, describing the <span class="No-Break">data itself.</span></p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor101"/>Describing the data</h2>
			<p>Once we understand the <a id="_idIndexMarker442"/>data’s structure, we can describe or summarize the characteristics of the data to further explore how these characteristics influence our overall hypothesis. This methodology is commonly referred to as applying <strong class="bold">descriptive statistics</strong> to the data, whereby <a id="_idIndexMarker443"/>a data scientist will try to describe and understand the various features of the dataset by summarizing the collective properties of each feature within the data in terms of centrality, variability, and <span class="No-Break">data counts.</span></p>
			<p>Let’s explore what each of these terms means to see how they can be used to describe <span class="No-Break">the data.</span></p>
			<h3>Determining central tendency</h3>
			<p>By using descriptive statistics to <a id="_idIndexMarker444"/>summarize the central tendency of the data, we are essentially focusing on the average, middle, or center position within the distribution of a specific feature of the dataset. This gives the data scientist an idea of what is normal or average about a feature of the dataset, allowing them to compare these averages with other features of the data or even the entirety of <span class="No-Break">the data.</span></p>
			<p>For example, let’s say that customer A visited our website 10 times a day but only purchased 1 item. By comparing the average visits of customer A with the total number of customer visits, we can see how customer A ranks in comparison. If we then compare the number of items purchased with the total number of items, we can further gauge what is considered normal based on the <span class="No-Break">customer’s ranking.</span></p>
			<h3>Measuring variability</h3>
			<p>Using<a id="_idIndexMarker445"/> descriptive statistics to measure the variability of the data or how the data is spread is extremely important in ML. Understanding how the data is distributed will give the data scientist a good idea of whether the data is proportional or not. For example, in the product recommendation use case where we have data with a greater spread of customers who purchase books versus customers who purchase lawnmowers. In this case, when a model is trained on this data, it will be biased toward recommending <a id="_idIndexMarker446"/>books <span class="No-Break">over lawnmowers.</span></p>
			<h3>Counting the data points</h3>
			<p>Not to be confused with dataset<a id="_idIndexMarker447"/> sizes, dataset counts refer to the number or quantity of individual data points within the dataset. Summarizing the number of data points for each feature within the dataset can further help the data scientist to verify whether they have an adequate number of data points or observations for each feature. Having sufficient quantities of data points will further help to justify the <span class="No-Break">overall hypothesis.</span></p>
			<p>Additionally, by comparing the individual quantities of data points for each feature, the data scientist can determine whether there are any missing data points. Since the majority of ML algorithms don’t deal well with missing data, the data scientist can circumvent any unnecessary issues during the model training process by dealing with these missing values during the data <span class="No-Break">analysis process.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">While these previously shown descriptive techniques can help us understand the characteristics of the data, a separate branch <a id="_idIndexMarker448"/>of statistics, called <strong class="bold">inferential statistics</strong>, is also required to measure and understand how features interact with one another within the entire dataset. This factor is important when dealing with large quantities of data where inferential techniques will need to be applied if we don’t have a mechanism to analyze large datasets <span class="No-Break">at scale.</span></p>
			<p>We’ve all heard the saying that <em class="italic">a picture paints a thousand words</em>. So, once we have a good understanding of the dataset’s characteristics, another important data analysis technique is to visualize <span class="No-Break">these characteristics.</span></p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor102"/>Visualizing the data</h2>
			<p>While summarizing the characteristics of the data provides useful information to the data scientist, we are essentially adding more data to the analysis task. Plotting or charting this additional<a id="_idIndexMarker449"/> information can potentially reveal further characteristics of the data that summary and inferential statistics <span class="No-Break">may miss.</span></p>
			<p>For example, using visualization to understand the variance and spread of the data points, a data scientist may use a bar chart to group the various data points into <em class="italic">bins</em> with equal ranges to visualize the distribution of data points in each <em class="italic">bin</em>. Furthermore, by using a boxplot, a data scientist can visualize whether there are any outlying data points that influence the overall distribution of <span class="No-Break">the data.</span></p>
			<p>Depending on the type of data and structure of the dataset, many different types of plots and charts can be used to visualize the data. It is outside the scope of this chapter to dive into each and every type of plot available and how it can be used. However, it is sufficient to say that data visualization is an essential methodology for exploratory data analysis to verify data quality and help the data scientist become more familiar with the structure and characteristics of <span class="No-Break">the dataset.</span></p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor103"/>Reviewing the data analytics life cycle</h2>
			<p>While there are<a id="_idIndexMarker450"/> many other data analysis methodologies, of which we have only touched on four, we can summarize the overall data analysis methodology in the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Identify the use case and questions that need to be answered from the data, plus the features the ML model needs <span class="No-Break">to predict.</span></li>
				<li>Gather or mine the data into a common <span class="No-Break">data store.</span></li>
				<li>Explore and describe <span class="No-Break">the data.</span></li>
				<li>Visualize <span class="No-Break">the data.</span></li>
				<li>Clean the data and prepare it for model training, plus account for any <span class="No-Break">missing data.</span></li>
				<li>Engineer new features to enhance the hypothesis and improve the ML model’s <span class="No-Break">predictive capability.</span></li>
				<li>Rinse and repeat to ensure that the data, as well as the ML model, addresses the business <span class="No-Break">use case.</span></li>
			</ol>
			<p>Now that we have reviewed some of the important data analysis methodologies and the analysis life cycle, let’s review <a id="_idIndexMarker451"/>some of the capabilities and services that AWS provides to apply these techniques, especially <span class="No-Break">at scale.</span></p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor104"/>Reviewing the AWS services for data analysis</h1>
			<p>AWS provides multiple<a id="_idIndexMarker452"/> services that are geared to help the data scientist analyze either structured, semi-structured, or unstructured data at scale. A common style across all these services is to provide users with the flexibility of choice to<a id="_idIndexMarker453"/> match the right aspects of each service as it applies to the use case. At times, it may seem confusing to the user which service to leverage for their <span class="No-Break">use case.</span></p>
			<p>Thus, in this section, we will map some of the AWS capabilities to the methodologies we’ve reviewed in the <span class="No-Break">previous section.</span></p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor105"/>Unifying the data into a common store</h2>
			<p>To address the requirement of storing the relevant global population of data from multiple sources in a<a id="_idIndexMarker454"/> common store, AWS provides the Amazon<strong class="bold"> Simple Storage Service</strong> (<strong class="bold">S3</strong>) object storage <a id="_idIndexMarker455"/>service, allowing users to store structured, semi-structured, and unstructured data as objects <span class="No-Break">within buckets.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you are unfamiliar with<a id="_idIndexMarker456"/> the S3 service, how it works, and how to use it, you can review the S3 product page <span class="No-Break">here: </span><a href="https://aws.amazon.com/s3/"><span class="No-Break">https://aws.amazon.com/s3/</span></a><span class="No-Break">.</span></p>
			<p>Consequently, S3 is the best place to create a data lake as it has unrivaled security, availability, and scalability. Incidentally, S3 also provides multiple additional resources to bring data into <span class="No-Break">the store.</span></p>
			<p>However, setting up and managing data lakes can be time-consuming and intricate, and may take up to several weeks to set it up, based on your requirements. It often requires loading data from multiple different sources, setting up partitions, enabling encryption, and providing auditable<a id="_idIndexMarker457"/> access. Subsequently, AWS provides <strong class="bold">AWS Lake Formation</strong> (<a href="https://aws.amazon.com/lake-formation">https://aws.amazon.com/lake-formation</a>) to build secure data lakes in <span class="No-Break">mere days.</span></p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor106"/>Creating a data structure for analysis</h2>
			<p>As was highlighted in<a id="_idIndexMarker458"/> the previous section, understanding the underlying structure of our data is critical to extracting the key<a id="_idIndexMarker459"/> information needed for analysis. So, once our data is stored in S3, we can leverage <strong class="bold">Amazon Athena</strong> (<a href="https://aws.amazon.com/athena">https://aws.amazon.com/athena</a>) using <strong class="bold">Structured Query Language</strong> (<strong class="bold">SQL</strong>), or leverage <strong class="bold">Amazon EMR</strong> (<a href="https://aws.amazon.com/emr/">https://aws.amazon.com/emr/</a>) to analyze large-scale data, using open source tooling, such as <strong class="bold">Apache Spark</strong> (<a href="https://spark.apache.org/">https://spark.apache.org/</a>) and the <strong class="bold">PySpark</strong> (<a href="https://spark.apache.org/docs/latest/api/python/index.html?highlight=pyspark">https://spark.apache.org/docs/latest/api/python/index.html?highlight=pyspark</a>) Python interface. Let’s explore these analytics services further by starting with an overview of <span class="No-Break">Amazon Athena.</span></p>
			<h3>Reviewing Amazon Athena</h3>
			<p>Athena makes it easy to <a id="_idIndexMarker460"/>define a schema, a conceptual design of the data structure, and query the structured or semi-structured data in S3 using SQL, making it easy for the data scientist to obtain key information for analysis on <span class="No-Break">large datasets.</span></p>
			<p>One critical aspect of<a id="_idIndexMarker461"/> Athena is the fact that it is <strong class="bold">serverless</strong>. This is of key importance to data scientists because there are no requirements for building and managing infrastructure resources. This means that data scientists immediately start their analysis tasks without needing to rely on a platform or infrastructure team to develop and build an <span class="No-Break">analytics architecture.</span></p>
			<p>However, the expertise to perform SQL queries may or may not be within a data scientist’s wheelhouse<a id="_idIndexMarker462"/> since the majority of practitioners are more familiar with Python data analysis tools, such as <strong class="bold">pandas</strong> (<a href="https://pandas.pydata.org/">https://pandas.pydata.org/</a>). This is where Spark and EMR come in. Let’s review how Amazon EMR <span class="No-Break">can help.</span></p>
			<h3>Reviewing Amazon EMR</h3>
			<p>Amazon <strong class="bold">EMR</strong> or <strong class="bold">Elastic MapReduce</strong> is essentially a managed infrastructure provided by AWS, on which you can run Apache <a id="_idIndexMarker463"/>Spark. Since it’s a managed service, EMR allows the infrastructure team to easily provision, manage, and automatically scale large Spark clusters, allowing data scientists to run petabyte-scale analytics on their data using tools they are <span class="No-Break">familiar with.</span></p>
			<p>There are two key points to be aware of when leveraging EMR and Spark for data analysis. Firstly, unlike Athena, EMR is not serverless and requires an infrastructure team to provision and manage a cluster of EMR nodes. While these tasks have been automated when using EMR, taking between 15 to 20 minutes to provision a cluster, the fact still remains that these infrastructure resources require a build-out before the data scientist can <span class="No-Break">leverage them.</span></p>
			<p>Secondly, EMR with Spark <a id="_idIndexMarker464"/>makes use of <strong class="bold">Resilient Distributed Datasets</strong> (<strong class="bold">RDDs</strong>) (<a href="https://spark.apache.org/docs/3.2.1/rdd-programming-guide.html#resilient-distributed-datasets-rdds">https://spark.apache.org/docs/3.2.1/rdd-programming-guide.html#resilient-distributed-datasets-rdds</a>) to perform petabyte-scale data analysis by alleviating the memory limitations often imposed when using pandas. Essentially, this allows the data scientist to perform analysis tasks on the entire population of data as opposed to extracting a small enough sample to fit into memory, performing the descriptive analysis tasks on the said sample, and then inferring the analysis back onto the global population. Having the ability to execute an analysis of all of the data in a single step can significantly reduce the time taken for the data scientist to describe and understand <span class="No-Break">the data.</span></p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor107"/>Visualizing the data at scale</h2>
			<p>As if ingesting and <a id="_idIndexMarker465"/>analyzing large-scale datasets isn’t complicated enough for a data scientist, using programmatic visualization<a id="_idIndexMarker466"/> libraries such as Matplotlib (<a href="https://matplotlib.org/">https://matplotlib.org/</a>) and<a id="_idIndexMarker467"/> Seaborn (https://seaborn.pydata.org/) can further complicate the <span class="No-Break">analysis task.</span></p>
			<p>So, in order to assist data scientists in visualizing data and gaining additional insights plus performing both descriptive and<a id="_idIndexMarker468"/> inferential statistics, AWS provides the <strong class="bold">Amazon QuickSight</strong> (<a href="https://aws.amazon.com/quicksight/">https://aws.amazon.com/quicksight/</a>) service. QuickSight allows data scientists to connect to their data on S3, as well as other data sources, to create interactive charts <span class="No-Break">and plots.</span></p>
			<p>Furthermore, leveraging QuickSight for data visualization tasks does not require the data scientist to rely on their <a id="_idIndexMarker469"/>infrastructure teams to provision resources as QuickSight is <span class="No-Break">also serverless.</span></p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor108"/>Choosing the right AWS service</h2>
			<p>As you can imagine, AWS<a id="_idIndexMarker470"/> provides many more services and capabilities for large-scale data analysis, with S3, Athena, and QuickSight being only a few of the more common technologies that specifically focus on data analytics tasks. Choosing the right capability is dependent on the use case and may require integrating other infrastructure resources. The key takeaway from this brief introduction to these services is that, where possible, data scientists should not be burned by having to manage resources outside of the already complicated task of <span class="No-Break">data analysis.</span></p>
			<p>Therefore, from the perspective of the data scientist or ML practitioner, AWS provides a dedicated service with <a id="_idIndexMarker471"/>capabilities specifically dedicated to common ML tasks called <strong class="bold">Amazon </strong><span class="No-Break"><strong class="bold">SageMaker</strong></span><span class="No-Break"> (</span><a href="https://aws.amazon.com/sagemaker/"><span class="No-Break">https://aws.amazon.com/sagemaker/</span></a><span class="No-Break">).</span></p>
			<p>Therefore, in the next section, we will demonstrate how SageMaker can help with analyzing large-scale data for ML without the data scientist having to personally manage or rely on infrastructure teams to <span class="No-Break">manage resources.</span></p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor109"/>Analyzing large amounts of structured and unstructured data</h1>
			<p>Up until this point in the<a id="_idIndexMarker472"/> chapter, we have reviewed some of the typical methods for large-scale data analysis and introduced some of the key AWS services that focus on making the analysis task easier for users. In this section, we will practically introduce Amazon SageMaker as a comprehensive service that allows both the novice as well as the experienced ML practitioner to perform these data <span class="No-Break">analysis tasks.</span></p>
			<p>While SageMaker is a fully managed infrastructure provided by AWS along with tools and workflows that cater to each step of the ML process, it also <a id="_idIndexMarker473"/>offers a fully <strong class="bold">Integrated Development Environment</strong> (<strong class="bold">IDE</strong>) specifically for ML development called <strong class="bold">Amazon SageMaker Studio</strong> (<a href="https://aws.amazon.com/sagemaker/studio/">https://aws.amazon.com/sagemaker/studio/</a>). SageMaker Studio provides a data scientist with the <a id="_idIndexMarker474"/>capabilities to develop, manage, and view each part of the ML life cycle, including exploratory <span class="No-Break">data analysis.</span></p>
			<p>But, before jumping into a hands-on example where we can perform large-scale data analysis using Studio, we need to configure a SageMaker domain. A SageMaker Studio domain comprises a set of<a id="_idIndexMarker475"/> authorized data scientists, pre-built data science tools, and security guard rails. Within the domain, these users can share access to AWS analysis services, ML experiment data, visualizations, and <span class="No-Break">Jupyter notebooks.</span></p>
			<p>Let’s <span class="No-Break">get started.</span></p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor110"/>Setting up EMR and SageMaker Studio</h2>
			<p>We will use an <strong class="bold">AWS CloudFormation</strong> (<a href="https://aws.amazon.com/cloudformation/">https://aws.amazon.com/cloudformation/</a>) template to <a id="_idIndexMarker476"/>perform the <span class="No-Break">following tasks:</span></p>
			<ul>
				<li>Launch a SageMaker Studio domain along with <span class="No-Break">a </span><span class="No-Break"><em class="italic">studio-user</em></span></li>
				<li>Create a<a id="_idIndexMarker477"/> standard EMR cluster with no authentication enabled, including the other infrastructure required, such as a <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>), subnets, and <span class="No-Break">other resources</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">You will incur a cost for EMR when you launch this CloudFormation template. Therefore, make sure to refer to the <em class="italic">Clean up</em> section at the end of <span class="No-Break">the chapter.</span></p>
			<p class="callout">The CloudFormation template that we will use in the book is originally taken from <a href="https://aws-ml-blog.s3.amazonaws.com/artifacts/sma-milestone1/template_no_auth.yaml">https://aws-ml-blog.s3.amazonaws.com/artifacts/sma-milestone1/template_no_auth.yaml</a> and has been modified to run the code provided with <span class="No-Break">the book.</span></p>
			<p>To get started with launching the CloudFormation template, use your AWS account to run the <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker478"/></span><span class="No-Break"> steps:</span></p>
			<ol>
				<li value="1">Log into your AWS account and open the SageMaker management console (<a href="https://console.aws.amazon.com/sagemaker/home">https://console.aws.amazon.com/sagemaker/home</a>), preferably as an admin user. If you don’t have admin user access, make sure you have permission to create an EMR cluster, Amazon SageMaker Studio, and S3. You can refer to <em class="italic">Required Permissions</em> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-required-permissions.html">https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-required-permissions.html</a>) for details on the <span class="No-Break">permission needed.</span></li>
				<li>Go to the S3 bucket and upload the contents of the S3 folder from the GitHub repository. Go to the <strong class="source-inline">templates</strong> folder, click on <strong class="source-inline">template_no_auth.yaml</strong>, and copy <span class="No-Break"><strong class="source-inline">Object URL</strong></span><span class="No-Break">.</span></li>
				<li>Make sure you have the <strong class="source-inline">artifacts</strong> folder parallel to the <strong class="source-inline">templates</strong> folder in the S3 bucket <span class="No-Break">as well.</span></li>
				<li>Search for the <strong class="source-inline">CloudFormation</strong> service and click <span class="No-Break">on it.</span></li>
				<li>Once in the CloudFormation console, click on the <strong class="bold">Create stack</strong> orange button, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B18493_05_001.jpg" alt="Figure 5.1 – AWS CloudFormation console"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – AWS CloudFormation console</p>
			<ol>
				<li value="6">In the <strong class="bold">Specify template</strong> section, select <strong class="bold">Amazon S3 URL</strong> as <strong class="bold">Template source</strong> and<a id="_idIndexMarker479"/> enter <strong class="bold">Amazon S3 URL</strong> noted in <em class="italic">step 2</em>, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2,</em> and click on the <span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break"> button:</span></li>
			</ol>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B18493_05_002.jpg" alt="Figure 5.2 – Create stack"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Create stack</p>
			<ol>
				<li value="7">Enter the stack name of your choice and click on the <span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break"> button.</span></li>
				<li>On the <strong class="bold">Configure stack options</strong> page, keep the default settings and click on the <strong class="bold">Next</strong> button at the<a id="_idIndexMarker480"/> bottom of <span class="No-Break">the page.</span></li>
				<li>On the <strong class="bold">Review</strong> page, scroll to the bottom of the screen, click on the <strong class="bold">I acknowledge that AWS CloudFormation might create IAM resources with custom names</strong> checkbox, and click on the <strong class="bold">Create stack</strong> button, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B18493_05_003.jpg" alt="Figure 5.3 – Review stack"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Review stack</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The CloudFormation template will take 5-10 minutes <span class="No-Break">to launch.</span></p>
			<ol>
				<li value="10">Once it is launched, go to <strong class="bold">Amazon SageMaker</strong>, click on <strong class="bold">SageMaker Studio</strong>, and you will <a id="_idIndexMarker481"/>see <strong class="bold">SageMaker Domain</strong> and <strong class="bold">studio-user</strong> configured for you, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B18493_05_004.jpg" alt="Figure 5.4 – SageMaker Domain"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – SageMaker Domain</p>
			<ol>
				<li value="11">Click on the <strong class="bold">Launch app</strong> dropdown next to <strong class="bold">studio-user</strong> and select <strong class="bold">Studio</strong>, as shown in the preceding screenshot. After this, you will be presented with a new JupyterLab <span class="No-Break">interface (</span><a href="https://jupyterlab.readthedocs.io/en/latest/user/interface.html"><span class="No-Break">https://jupyterlab.readthedocs.io/en/latest/user/interface.html</span></a><span class="No-Break">).</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">It is recommended that you familiarize yourself with the Studio UI by reviewing the Amazon SageMaker Studio UI documentation (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html">https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html</a>), as we will be referencing many of the SageMaker-specific widgets and views throughout <span class="No-Break">the chapter.</span></p>
			<ol>
				<li value="12">To make it <a id="_idIndexMarker482"/>easier to run the various examples within the book using the Studio UI, we will clone the source code from the companion GitHub repository. Within the Studio UI, click on the <strong class="bold">Git</strong> icon in the left sidebar and once the resource panel opens, click on the <strong class="bold">Clone a repository</strong> button to launch the <strong class="bold">Clone a repo</strong> dialog box, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B18493_05_005.jpg" alt="Figure 5.5 – Clone a repo"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Clone a repo</p>
			<ol>
				<li value="13">Enter the URL for the companion repository (<a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS.git">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS.git</a>) and click the <span class="No-Break"><strong class="bold">CLONE</strong></span><span class="No-Break"> button.</span></li>
				<li>The cloned repository will now appear in the <strong class="bold">File Browser</strong> panel of the Studio UI. Double-click on the newly cloned <strong class="source-inline">Applied-Machine-Learning-and-High-Performance-Computing-on-AWS</strong> folder to <span class="No-Break">expand it.</span></li>
				<li>Then double-click on the <strong class="source-inline">Chapter05</strong> folder to open it <span class="No-Break">for browsing.</span></li>
			</ol>
			<p>We are now ready to <a id="_idIndexMarker483"/>analyze large amounts of structured data using SageMaker Studio. However, before we can start the analysis, we need to acquire the data. Let’s take a look at how to <span class="No-Break">do that.</span></p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor111"/>Analyzing large amounts of structured data</h2>
			<p>Since the objective of this section is to provide a hands-on example for analyzing large-scale structured data, our <a id="_idIndexMarker484"/>first task will be to synthesize a large amount of data. Using the Studio UI, execute the <span class="No-Break">following steps:</span></p>
			<ol>
				<li value="1">Using the left <strong class="bold">File Browser</strong> panel, double-click on the <strong class="source-inline">1_data_generator.ipynb</strong> file to launch the <span class="No-Break">Jupyter notebook.</span></li>
				<li>When prompted with the <strong class="bold">Set up notebook environment</strong> dialog box, ensure that <strong class="bold">Data Science</strong> is selected from the <strong class="bold">Image</strong> drop-down box, as well as <strong class="bold">Python 3</strong> for the <strong class="bold">Kernel</strong> option. <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.6</em> shows an example of the <span class="No-Break">dialog box:</span></li>
			</ol>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B18493_05_006.jpg" alt="Figure 5.6 – Set up notebook environment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Set up notebook environment</p>
			<ol>
				<li value="3">Once these options have been set, click the <strong class="bold">Select</strong> button <span class="No-Break">to continue.</span></li>
				<li>Next, you should see a <strong class="bold">Starting notebook kernel…</strong> message. The notebook kernel will take a couple of minutes <span class="No-Break">to load.</span></li>
				<li>Once the notebook<a id="_idIndexMarker485"/> has loaded, run the notebook by clicking on the <strong class="bold">Kernel</strong> menu and selecting the <strong class="bold">Restart kernel and Run All </strong><span class="No-Break"><strong class="bold">Cells…</strong></span><span class="No-Break"> option.</span></li>
			</ol>
			<p>After the notebook has executed all the code cells, we can dive into exactly what the notebook does, starting with a review of <span class="No-Break">the dataset.</span></p>
			<h3>Reviewing the dataset</h3>
			<p>The dataset we will be using<a id="_idIndexMarker486"/> within this example is the California housing dataset (<a href="https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html">https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html</a>). This dataset was derived from the 1990 US census, using one row per census block group. A block group is the smallest geographical unit for which the US Census Bureau publishes sample data. A block group typically has a population of 600 to <span class="No-Break">3,000 people.</span></p>
			<p>The dataset is incorporated into the <strong class="bold">scikit-learn</strong> or <strong class="source-inline">sklearn</strong> Python library (<a href="https://scikit-learn.org/stable/index.html">https://scikit-learn.org/stable/index.html</a>). The scikit-learn library includes a dataset module that allows us to download popular reference datasets, such as the California housing dataset, from <strong class="bold">StatLib Datasets </strong><span class="No-Break"><strong class="bold">Archive</strong></span><span class="No-Break"> (</span><span class="No-Break">http://lib.stat.cmu.edu/datasets/</span><span class="No-Break">).</span></p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Pace, R. Kelley, and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (<span class="No-Break">1997) 291-297.</span></p>
			<p>One key thing to be <a id="_idIndexMarker487"/>aware of is that this dataset only has 20,640 samples and is only around 400 KB in size. So, I’m sure you’ll agree that it doesn’t exactly qualify as a large amount of structured data. So, the primary objective of the notebook we’ve just executed is to use this dataset as a basis for synthesizing a much larger amount of structured data and then storing this new dataset on S3 <span class="No-Break">for analysis.</span></p>
			<p>Let’s walk through the code to see how this <span class="No-Break">is done.</span></p>
			<h3>Installing the Python libraries</h3>
			<p>The first five code <a id="_idIndexMarker488"/>cells within the notebook are used to install and upgrade the necessary Python libraries to ensure we have the correct versions for the <a id="_idIndexMarker489"/>SageMaker SDK, scikit-learn, and the <strong class="bold">Synthetic Data Vault</strong>. The following code snippet shows the consolidation of these five <span class="No-Break">code cells:</span></p>
			<pre class="source-code">
...
import sys
!{sys.executable} -m pip install "sagemaker&gt;=2.51.0"
!{sys.executable} -m pip install --upgrade -q "scikit-learn"
!{sys.executable} -m pip install "sdv"
import sklearn
sklearn.__version__
import sdv
sdv.__version__
...</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">There is no specific reason we upgrade and install the SageMaker and scikit-learn libraries except to ensure conformity across the examples within <span class="No-Break">this chapter.</span></p>
			<p>Once the required<a id="_idIndexMarker490"/> libraries have been installed, we load them and configure our global variables. The following code snippet shows how we import the libraries and configure the SageMaker default S3 <span class="No-Break">bucket parameters:</span></p>
			<pre class="source-code">
...
import os
from sklearn.datasets import fetch_california_housing
import time
import boto3
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import sagemaker
from sagemaker import get_execution_role
prefix = 'california_housing'
role = get_execution_role()
bucket = sagemaker.Session(boto3.Session()).default_bucket()
...</pre>
			<p>However, before we can synthesize a larger dataset and upload this to S3, we need to download the California housing dataset. As you can see from the following code snippet, we create two local folders called <strong class="source-inline">data</strong> and <strong class="source-inline">raw</strong>, then download the data using the <strong class="source-inline">fetch_california_housing()</strong> method from <strong class="source-inline">sklearn.datasets</strong>. The resultant data <a id="_idIndexMarker491"/>variable allows us to describe the data, as well as capture the data itself as a two-dimensional data structure <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">df_data</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
...
data_dir = os.path.join(os.getcwd(), "data")
os.makedirs(data_dir, exist_ok=True)
raw_dir = os.path.join(os.getcwd(), "data/raw")
os.makedirs(raw_dir, exist_ok=True)
data = fetch_california_housing(data_home=raw_dir, download_if_missing=True, return_X_y=False, as_frame=True)
...
df_data = data.data
...</pre>
			<p>The <strong class="source-inline">df_data</strong> variable is the essential representation of our structured data, with columns showing the data labels and rows showing the observations or records for each label. Think of this structure as similar to a spreadsheet or <span class="No-Break">relational table.</span></p>
			<p>Using the <strong class="source-inline">df_data</strong> variable, we further describe this structure as well as perform some of the descriptive statistics and visualization described in the <em class="italic">Exploring data analysis methods</em> section of this chapter. For example, the following code snippet shows how to describe the type of data we are dealing with. You will recall that understanding the data type is crucial for appreciating the overall schema or structure of <span class="No-Break">the data:</span></p>
			<pre class="source-code">
...
df_data.astype({'Population': 'int32'}).dtypes
...</pre>
			<p>Furthermore, we can define a Python function called <strong class="source-inline">plot_boxplot()</strong> to visualize the data included in<a id="_idIndexMarker492"/> the <strong class="source-inline">df_data</strong> variable. You will recall that visualizing the data provides further insight into the data. For example, as you can see from the next code snippet, we can visualize the overall distribution of the average number of rooms or <strong class="source-inline">avgNumrooms</strong> in <span class="No-Break">the house:</span></p>
			<pre class="source-code">
...
import matplotlib.pyplot as plt
def plot_boxplot(data, title):
    plt.figure(figsize =(5, 4))
    plt.boxplot(data)
    plt.title(title)
    plt.show()
...
df_data.drop(df_data[df_data['avgNumRooms'] &gt; 9].index, inplace = True)
df_data.drop(df_data[df_data['avgNumRooms'] &lt;= 1].index, inplace = True)
plot_boxplot(df_data.avgNumRooms, 'rooms')
...</pre>
			<p>As we can see from <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.7</em>, the resultant boxplot from the code indicates that the average number of rooms for the California housing data <span class="No-Break">is </span><span class="No-Break"><strong class="bold">5</strong></span><span class="No-Break">:</span></p>
			<p class="IMG---Figure"><img src="image/B18493_05_007.png" alt="Figure 5.7 – Average number of rooms"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – Average number of rooms</p>
			<p>Additionally, you will note from <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.7</em> that there is somewhat of an equal distribution to the upper and lower bounds of the data. This indicates that we have a good distribution<a id="_idIndexMarker493"/> of data for the average number of bedrooms and therefore, we don’t need to augment this <span class="No-Break">data point.</span></p>
			<p>Finally, you will recall from the <em class="italic">Counting the data points</em> section that we can circumvent any unnecessary issues during the model training process by determining whether or not there are any missing values in the data. For example, the next code snippet shows how we can review a sum of any missing values in the <span class="No-Break"><strong class="source-inline">df_data</strong></span><span class="No-Break"> variable:</span></p>
			<pre class="source-code">
...
df_data.isna().sum()
...</pre>
			<p>While we’ve only covered a few analytics methodologies to showcase the analytics life cycle, a key takeaway from these examples is that the data is easy to analyze since it’s small enough to fit into memory. So, as data scientists, we did not have to capture a sample of the global population to analyze the data and then infer that analysis back onto the larger dataset. Let’s see whether this holds true once we synthesize a <span class="No-Break">larger dataset.</span></p>
			<h3>Synthesizing large data</h3>
			<p>The last part of the notebook involves using the Synthetic Data Vault (<a href="https://sdv.dev/SDV/index.html">https://sdv.dev/SDV/index.html</a>), or the <strong class="source-inline">sdv</strong> Python library. This ecosystem of Python libraries uses ML models that specifically focus on learning from structured tabular and time series datasets and on<a id="_idIndexMarker494"/> creating synthetic data that carries the same format, statistical properties, and structure as the <span class="No-Break">original dataset.</span></p>
			<p>In our example notebook, we use a TVAE (<a href="https://arxiv.org/abs/1907.00503">https://arxiv.org/abs/1907.00503</a>) model to generate a larger version of the California housing data. For example, the following code snippet shows how we define and train a TVAE model on the <span class="No-Break"><strong class="source-inline">df_data</strong></span><span class="No-Break"> variable:</span></p>
			<pre class="source-code">
...
from sdv.tabular import TVAE
model = TVAE(rounding=2)
model.fit(df_data)
model_dir = os.path.join(os.getcwd(), "model")
os.makedirs(model_dir, exist_ok=True)
model.save(f'{model_dir}/tvae_model.pkl')
...</pre>
			<p>Once we’ve trained the model, we can load it to generate 1 million new observations or rows in a variable called <strong class="source-inline">synthetic_data</strong>. The following code snippet shows an example <span class="No-Break">of this:</span></p>
			<pre class="source-code">
...
from sdv.tabular import TVAE
model = TVAE.load(f'{model_dir}/tvae_model.pkl')
synthetic_data = model.sample(1000000)
...</pre>
			<p>Finally, as shown, we use the following code snippet to compress the data and leverage the SageMaker <a id="_idIndexMarker495"/>SDK’s <strong class="source-inline">upload_data()</strong> method to store the data <span class="No-Break">in S3:</span></p>
			<pre class="source-code">
...
sess = boto3.Session()
sagemaker_session = sagemaker.Session(boto_session=sess)
synthetic_data.to_parquet('data/raw/data.parquet.gzip', compression='gzip')
rawdata_s3_prefix = "{}/data/raw".format(prefix)
raw_s3 = sagemaker_session.upload_data(path="./data/raw/data.parquet.gzip", key_prefix=rawdata_s3_prefix)
...</pre>
			<p>With a dataset of 1 million rows stored in S3, we finally have an example of a large amount of structured data. Now we can use this data to demonstrate how to leverage the highlighted analysis methods at scale on structured data using <span class="No-Break">Amazon EMR.</span></p>
			<h3>Analyzing large-scale data using an EMR cluster with SageMaker Studio</h3>
			<p>To get started with <a id="_idIndexMarker496"/>analyzing the large-scale synthesized dataset we’ve just created, we can execute the following steps in the <span class="No-Break">Studio UI:</span></p>
			<ol>
				<li value="1">Using the left-hand navigation panel, double-click on the <strong class="source-inline">2_data_exploration_spark.ipynb</strong> notebook to <span class="No-Break">launch it.</span></li>
				<li>As we saw with the previous example, when prompted with the <strong class="bold">Set up notebook environment</strong> dialog box, select <strong class="bold">SparkMagic</strong> as <strong class="bold">Image</strong> and <strong class="bold">PySpark</strong> <span class="No-Break">as </span><span class="No-Break"><strong class="bold">Kernel</strong></span><span class="No-Break">.</span></li>
				<li>Once the notebook is ready, click on the <strong class="bold">Kernel</strong> menu option and once again select the <strong class="bold">Restart kernel and Run All Cells…</strong> option to execute the <span class="No-Break">entire notebook.</span></li>
			</ol>
			<p>While the notebook is <a id="_idIndexMarker497"/>running, we can start reviewing what we are trying to accomplish within the various code cells. As you can see from the first code cell, we connect to the EMR cluster we provisioned in the <em class="italic">Setting up EMR and SageMaker </em><span class="No-Break"><em class="italic">Studio</em></span><span class="No-Break"> section:</span></p>
			<pre class="source-code">
%load_ext sagemaker_studio_analytics_extension.magics
%sm_analytics emr connect --cluster-id <strong class="bold">&lt;EMR Cluster ID&gt;</strong> --auth-type None</pre>
			<p>In the next code cell, shown by the following code, we read the synthesized dataset from S3. Here we create a <strong class="source-inline">housing_data</strong> variable by using PySpark’s <strong class="source-inline">sqlContext</strong> method to read the raw data <span class="No-Break">from S3:</span></p>
			<pre class="source-code">
housing_data=sqlContext.read.parquet('s3://<strong class="bold">&lt;SageMaker Default Bucket&gt;</strong>/california_housing/data/raw/data.parquet.gzip')</pre>
			<p>Once we have this variable assigned, we can use PySpark and the EMR cluster to execute the various data analysis tasks on the entire population of the data without having to ingest a sample dataset on which to perform <span class="No-Break">the analysis.</span></p>
			<p>While the notebook provides multiple examples of different analysis methodologies that are specific to the data, we will focus on the few examples that relate to the exploration we’ve already performed on the original California housing dataset to illustrate how these same methodologies can be applied <span class="No-Break">at scale.</span></p>
			<h4>Reviewing the data structure and counts</h4>
			<p>As already mentioned, understanding<a id="_idIndexMarker498"/> the type of data, its structure, and the counts is an important part of the analysis. To perform this analysis on the entirety of <strong class="source-inline">housing_data</strong>, we can execute the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
print((housing_data.count(), len(housing_data.columns)))
housing_data.printSchema()</pre>
			<p>Executing this code produces the following output, where we can see that we have 1 million observations, as <a id="_idIndexMarker499"/>well as the data types for <span class="No-Break">each feature:</span></p>
			<pre class="source-code">
(1000000, 9)
Root
|-- medianIncome: double (nullable = true)
|-- medianHousingAge: double (nullable = true)
|-- avgNumRooms: double (nullable = true)
|-- avgNumBedrooms: double (nullable = true)
|-- population: double (nullable = true)
|-- avgHouseholdMembers: double (nullable = true)
|-- latitude: double (nullable = true)
|-- longitude: double (nullable = true)
|-- medianHouseValue: double (nullable = true)</pre>
			<p>Next, we can determine whether or not there are any missing values and how to deal <span class="No-Break">with them.</span></p>
			<h4>Handling missing values</h4>
			<p>You will recall that<a id="_idIndexMarker500"/> ensuring that there are no missing values is an important methodology for any data analysis. To expose any missing data, we can run the following code to create a count of any missing values for each column or feature within this <span class="No-Break">large dataset:</span></p>
			<pre class="source-code">
from pyspark.sql.functions import isnan, when, count, col
housing_data.select([count(when(isnan(c), c)).alias(c) for c in housing_data.columns]).show()</pre>
			<p>If we do find any missing values, there are a number of techniques we can use to deal with them. For example, we delete rows containing missing values, using the <strong class="source-inline">dropna()</strong> method on the <strong class="source-inline">housing_data</strong> variable. Alternatively, depending on the number of missing values, we can use imputation techniques to infer a value based on the mean or median of <span class="No-Break">the </span><span class="No-Break"><a id="_idIndexMarker501"/></span><span class="No-Break">feature.</span></p>
			<h4>Analyzing the centrality and variability of the data</h4>
			<p>Remember that <a id="_idIndexMarker502"/>understanding how the data is distributed will give us a good idea of whether the data is proportional or not. This analysis task also provides an idea of whether we have outliers within our data that skew the distribution or spread. Previously, it was emphasized that visualizing the data distribution using bar charts and boxplots can further assist in determining the variability of <span class="No-Break">the data.</span></p>
			<p>To accommodate this task, the following code highlights an example of capturing the features we wish to analyze and plotting their distribution as a <span class="No-Break">bar chart:</span></p>
			<pre class="source-code">
import matplotlib.pyplot as plt
df = housing_data.select('avgNumRooms', 'avgNumBedrooms', 'population').toPandas()
df.hist(figsize=(10, 8), bins=20, edgecolor="black")
plt.subplots_adjust(hspace=0.3, wspace=0.5)
plt.show()
%matplot plt</pre>
			<p>After executing this code on the large data, we can see an example of the resultant distribution for the average number of rooms (<strong class="source-inline">avgNumRooms</strong>), the average number of bedrooms (<strong class="source-inline">avgNumBedrooms</strong>), and block population (<strong class="source-inline">population</strong>) features in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B18493_05_008.jpg" alt="Figure 5.8 – Feature distribution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – Feature distribution</p>
			<p>As you can see from <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.8</em>, both the <strong class="source-inline">avgNumBedrooms</strong> and <strong class="source-inline">population</strong> features are not centered around the mean or average for the feature. Additionally, the spread for the <strong class="source-inline">avgNumBedrooms</strong> feature is significantly skewed toward the lower end of the spectrum. This factor could indicate that there are potential outliers or too many data points that are consolidated <a id="_idIndexMarker503"/>between <strong class="bold">1.00</strong> and <strong class="bold">1.05</strong>. This fact is further confirmed if we use the following code to create a boxplot of the <span class="No-Break"><strong class="source-inline">avgNumBedrooms</strong></span><span class="No-Break"> feature:</span></p>
			<pre class="source-code">
plot_boxplot(df.avgNumBedrooms, 'Boxplot for Average Number of Bedrooms')
%matplot plt</pre>
			<p>The resultant boxplot from running this code cell is shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B18493_05_009.jpg" alt="Figure 5.9 – Boxplot for the average number of bedrooms"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Boxplot for the average number of bedrooms</p>
			<p><span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.9</em> clearly shows that there are a number of outliers that cause the data to be skewed. Therefore, we need to resolve these discrepancies as part of our data analysis and before ML models can be trained on our large dataset. The following code snippet shows how to query the <a id="_idIndexMarker504"/>data from the boxplot values and then simply remove it, to create a variable <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">housing_df_with_no_outliers</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
import pyspark.sql.functions as f
columns = ['avgNumRooms', 'avgNumBedrooms', 'population']
housing_df_with_no_outliers = housing_data.where(
    (housing_data.avgNumRooms&lt;= 8) &amp;
    (housing_data.avgNumRooms&gt;=2) &amp;
    (housing_data.avgNumBedrooms&lt;=1.12) &amp;
    (housing_data.population&lt;=1500) &amp;
    (housing_data.population&gt;=250))</pre>
			<p>Once we have our <strong class="source-inline">housing_df_with_no_outliers</strong>, we can use the following code to create a new boxplot of the variability of the <span class="No-Break"><strong class="source-inline">avgNumBedrooms</strong></span><span class="No-Break"> feature:</span></p>
			<pre class="source-code">
df = housing_df_with_no_outliers.select('avgNumRooms', 'avgNumBedrooms', 'population').toPandas()
plot_boxplot(df.avgNumBedrooms, 'Boxplot for Average Number of Bedrooms')
%matplot plt</pre>
			<p><span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.10</em> shows an <a id="_idIndexMarker505"/>example of a boxplot produced from executing <span class="No-Break">this code:</span></p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B18493_05_010.jpg" alt="Figure 5.10 – Boxplot of the average number of bedrooms"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Boxplot of the average number of bedrooms</p>
			<p>From <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.10</em>, we can clearly see that the outliers have been removed. Subsequently, we can perform a similar procedure on the <strong class="source-inline">avgNumRooms</strong> and <span class="No-Break"><strong class="source-inline">population</strong></span><span class="No-Break"> features.</span></p>
			<p>While these examples only show some of the important methodologies highlighted within the data analysis life cycle, an important takeaway from this exercise is that due to the integration of SageMaker Studio and EMR, we’re able to accomplish the data analysis tasks on large-scale structured data without having to capture a sample of the global population and then infer that analysis back onto the larger dataset. However, along with analyzing the data at scale, we also need to ensure that any preprocessing tasks are also executed <span class="No-Break">at scale.</span></p>
			<p>Next, we will review <a id="_idIndexMarker506"/>how to automate these preprocessing tasks at scale <span class="No-Break">using SageMaker.</span></p>
			<h4>Preprocessing the data at scale</h4>
			<p>The SageMaker <a id="_idIndexMarker507"/>service takes care of the heavy lifting and scaling of data transformation or preprocessing tasks using one of its core components<a id="_idIndexMarker508"/> called <strong class="bold">Processing jobs</strong> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html">https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html</a>). While Processing jobs allows a user to leverage built-in images for scikit-learn or even custom images, they also reduce the heavy-lifting task of provisioning ephemeral Spark clusters (https://docs.aws.amazon.com/sagemaker/latest/dg/use-spark-processing-container.html). This means that data scientists can perform large-scale data transformations automatically without having to create or have the infrastructure team create an EMR cluster. All that’s required of the data scientist is to convert the processing code cell into a Python script <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">preprocess.py</strong></span><span class="No-Break">.</span></p>
			<p>The following code snippet shows how the code to remove outliers can be converted into a <span class="No-Break">Python script:</span></p>
			<pre class="source-code">
%%writefile preprocess.py
...
def main():
    parser = argparse.ArgumentParser(description="app inputs and outputs")
    parser.add_argument("--bucket", type=str, help="s3 input bucket")
    parser.add_argument("--s3_input_prefix", type=str, help="s3 input key prefix")
    parser.add_argument("--s3_output_prefix", type=str, help="s3 output key prefix")
    args = parser.parse_args()
    spark = SparkSession.builder.appName("PySparkApp").getOrCreate()
    housing_data=spark.read.parquet(f's3://{args.bucket}/{args.s3_input_prefix}/data.parquet.gzip')
    housing_df_with_no_outliers = housing_data.where((housing_data.avgNumRooms&lt;= 8) &amp;
                   (housing_data.avgNumRooms&gt;=2) &amp;
                   (housing_data.avgNumBedrooms&lt;=1.12) &amp;
                    (housing_data.population&lt;=1500) &amp;
                    (housing_data.population&gt;=250))
    (train_df, validation_df) = housing_df_with_no_outliers.randomSplit([0.8, 0.2])
    train_df.write.parquet("s3://" + os.path.join(args.bucket, args.s3_output_prefix, "train/"))
    validation_df.write.parquet("s3://" + os.path.join(args.bucket, args.s3_output_prefix, "validation/"))
if __name__ == "__main__":
    main()
...</pre>
			<p>Once the Python script is <a id="_idIndexMarker509"/>created, we can load the appropriate SageMaker SDK libraries and configure the S3 locations for the input data as well as the transformed output data, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
%local
import sagemaker
from time import gmtime, strftime
sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()
bucket = sagemaker_session.default_bucket()
timestamp = strftime("%Y-%m-%d-%H-%M-%S", gmtime())
prefix = "california_housing/data_" + timestamp
s3_input_prefix = "california_housing/data/raw"
s3_output_prefix = prefix + "/data/spark/processed"</pre>
			<p>Finally, we can instantiate an<a id="_idIndexMarker510"/> instance of the SageMaker <strong class="source-inline">PySparkProcessor()</strong> class (<a href="https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor">https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor</a>) as a <strong class="source-inline">spark_processor</strong> variable, as can be seen in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
%local
from sagemaker.spark.processing import PySparkProcessor
spark_processor = PySparkProcessor(
    base_job_name="sm-spark",
    framework_version="2.4",
    role=role,
    instance_count=2,
    instance_type="ml.m5.xlarge",
    max_runtime_in_seconds=1200,
)</pre>
			<p>With the <strong class="source-inline">spark_processor</strong> variable defined, we can then call the <strong class="source-inline">run()</strong> method to execute a SageMaker Processing job. The following code demonstrates how to call the <strong class="source-inline">run()</strong> method and supply the <strong class="source-inline">preprocess.py</strong> script along with the input and output locations for the data <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">arguments</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
spark_processor.run(
    submit_app="preprocess.py",
    arguments=[
        "--bucket",
        bucket,
        "--s3_input_prefix",
        s3_input_prefix,
        "--s3_output_prefix",
        s3_output_prefix,
    ],
)</pre>
			<p>In the background, SageMaker will create an ephemeral Spark cluster and execute the <strong class="source-inline">preprocess.py</strong> script on the input data. Once the data transformations are completed, SageMaker will<a id="_idIndexMarker511"/> store the resultant dataset on S3 and then decommission the Spark cluster, all while redirecting the execution log output back to the <span class="No-Break">Jupyter notebook.</span></p>
			<p>While this technique makes the complicated task of analyzing large amounts of structured data much easier to scale, there is still the question of how to perform a similar procedure on <span class="No-Break">unstructured data.</span></p>
			<p>Let’s review how to solve this <span class="No-Break">problem next.</span></p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor112"/>Analyzing large amounts of unstructured data</h2>
			<p>In this section, we will use unstructured data (horse and human images) downloaded from <a href="https://laurencemoroney.com/datasets.html">https://laurencemoroney.com/datasets.html</a>. This dataset can be used to train a binary image classification <a id="_idIndexMarker512"/>model to classify horses and humans in the image. From the SageMaker Studio, launch the <strong class="source-inline">3_unstructured_data_s3.ipynb</strong> notebook with <strong class="bold">PyTorch 1.8 Python 3.6 CPU Optimized</strong> selected from the <strong class="bold">Image</strong> drop-down box as well as <strong class="bold">Python 3</strong> for the <strong class="bold">Kernel</strong> option. Once the notebook is opened, restart the kernel and run all the cells as mentioned in the <em class="italic">Analyzing large-scale data using an EMR cluster with SageMaker </em><span class="No-Break"><em class="italic">Studio</em></span><span class="No-Break"> section.</span></p>
			<p>After the notebook has executed all the code cells, we can dive into exactly what the <span class="No-Break">notebook does.</span></p>
			<p>As you can see in the notebook, we first download the horse-or-human data from <a href="https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip">https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip</a> and then unzip <span class="No-Break">the file.</span></p>
			<p>Once we have the data, we will convert the images to high resolution using the <strong class="bold">EDSR</strong> model provided by the <strong class="bold">Hugging Face</strong> <span class="No-Break"><strong class="source-inline">super-image</strong></span><span class="No-Break"> library:</span></p>
			<ol>
				<li value="1">We will first <a id="_idIndexMarker513"/>download the pretrained model with <strong class="source-inline">scale = 4</strong>, which means that we intend to increase the resolution of the images four times, as shown in the following <span class="No-Break">code block:</span><pre class="source-code">
from super_image import EdsrModel, ImageLoader</pre><pre class="source-code">
from PIL import Image</pre><pre class="source-code">
import requests</pre><pre class="source-code">
model = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=4)</pre></li>
				<li>Next, we will iterate through the folder containing the images, use the pretrained model to convert each image to high resolution, and save it, as shown in the following <span class="No-Break">code block:</span><pre class="source-code">
import os</pre><pre class="source-code">
from os import listdir</pre><pre class="source-code">
folder_dir = "horse-or-human/"</pre><pre class="source-code">
for folder in os.listdir(folder_dir):</pre><pre class="source-code">
    folder_path = f'{folder_dir}{folder}'</pre><pre class="source-code">
    for image_file in os.listdir(folder_path):</pre><pre class="source-code">
        path = f'{folder_path}/{image_file}'</pre><pre class="source-code">
        image = Image.open(path)</pre><pre class="source-code">
        inputs = ImageLoader.load_image(image)</pre><pre class="source-code">
        preds = model(inputs)</pre><pre class="source-code">
        ImageLoader.save_image(preds, path)</pre></li>
			</ol>
			<p>You can check the file size of one of the images to confirm that the images have been converted to high resolution. Once the images have been converted to high resolution, you can optionally duplicate the<a id="_idIndexMarker514"/> images to increase the number of files and finally upload them to S3 bucket. We will use the images uploaded to the S3 bucket for running a SageMaker <span class="No-Break">training job.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">In this example, we will walk you through the option of running a training job with <strong class="bold">PyTorch</strong> using the SageMaker training feature using the data stored in an S3 bucket. You can also choose to use other frameworks as well, such as <strong class="bold">TensorFlow</strong> and <strong class="bold">MXNet</strong>, which are also supported <span class="No-Break">by SageMaker.</span></p>
			<p>In order to use PyTorch, we will first import the <strong class="source-inline">sagemaker.pytorch</strong> module, using which we will<a id="_idIndexMarker515"/> define the <strong class="bold">SageMaker PyTorch Estimator</strong>, as shown in the following <span class="No-Break">code block:</span></p>
			<pre class="source-code">
from sagemaker.pytorch import PyTorch
estimator = PyTorch(entry_point='train.py',
                    source_dir='src',
                    role=role,
                    instance_count=1,
                    instance_type='ml.g4dn.8xlarge',
                    framework_version='1.8.0',
                    py_version='py3',
                    sagemaker_session=sagemaker_session,
                    hyperparameters={'epochs':5,
                                     'subset':2100,
                                     'num_workers':4,
                                     'batch_size':500},
                   )</pre>
			<p>In the estimator object, as you can see from the code snippet, we need to provide configuration parameters. In this <a id="_idIndexMarker516"/>case, we need to define the <span class="No-Break">following parameters:</span></p>
			<ul>
				<li><strong class="source-inline">instance_count</strong>: This is the number <span class="No-Break">of instances</span></li>
				<li><strong class="source-inline">instance_type</strong>: This is the type of instance on which the training job will <span class="No-Break">be launched</span></li>
				<li><strong class="source-inline">framework_version</strong>: This is the framework version of PyTorch to be used <span class="No-Break">for training</span></li>
				<li><strong class="source-inline">py_version</strong>: This is the <span class="No-Break">Python version</span></li>
				<li><strong class="source-inline">source_dir</strong>: This is the folder path within the notebook, which contains the <span class="No-Break">training script</span></li>
				<li><strong class="source-inline">entry_point</strong>: This is the name of the Python <span class="No-Break">training script</span></li>
				<li><strong class="source-inline">hyperparameters</strong>: This is the list of hyperparameters that will be used by the <span class="No-Break">training script</span></li>
			</ul>
			<p>Once we have defined the PyTorch estimator, we will define the <strong class="source-inline">TrainingInput</strong> object, which will take the S3 location of the input data, content type, and input mode as parameters, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
from sagemaker.inputs import TrainingInput
train = TrainingInput(s3_input_data,content_type='image/png',input_mode='File')</pre>
			<p>The <strong class="source-inline">input_mode</strong> parameter can take the following values; in our case, we are using the <span class="No-Break"><strong class="source-inline">File</strong></span><span class="No-Break"> value:</span></p>
			<ul>
				<li><strong class="source-inline">None</strong>: Amazon SageMaker will use the input mode specified in the base <span class="No-Break"><strong class="source-inline">Estimator</strong></span><span class="No-Break"> class</span></li>
				<li><strong class="source-inline">File</strong>: Amazon SageMaker copies the training dataset from the S3 location to a <span class="No-Break">local directory</span></li>
				<li><strong class="source-inline">Pipe</strong>: Amazon SageMaker streams data directly from S3 to the container via a <span class="No-Break"><em class="italic">Unix-named pipe</em></span></li>
				<li><strong class="source-inline">FastFile</strong>: Amazon SageMaker streams data from S3 on demand instead of downloading the entire dataset before <span class="No-Break">training begins</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">You can see the complete list of <a id="_idIndexMarker517"/>parameters for the PyTorch estimator at this <span class="No-Break">link: </span><a href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html"><span class="No-Break">https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html</span></a><span class="No-Break">.</span></p>
			<p>Once we have configured the PyTorch <strong class="source-inline">Estimator</strong> and <strong class="source-inline">TrainingInput</strong> objects, we are now ready to start the training job, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
estimator.fit({'train':train})</pre>
			<p>When we run <strong class="source-inline">estimator.fit</strong>, it will launch one training instance of the <strong class="source-inline">ml.g4dn.8xlarge</strong> type, install the <strong class="source-inline">PyTorch 1.8</strong> container, copy the training data from <strong class="source-inline">S3 location</strong> and the <strong class="source-inline">train.py</strong> script to the local directory on the training instance, and will finally run the training script that you have provided in the estimator configuration. Once the training job is completed, SageMaker will automatically terminate all the resources that it has launched, and you will only be charged for the amount of time the training job <span class="No-Break">was running.</span></p>
			<p>In this example, we used a simple training script that involved loading the data using the PyTorch <strong class="source-inline">DataLoader</strong> object and iterating through the images. In the following section, we’ll see how to process data at scale <span class="No-Break">using AWS.</span></p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor113"/>Processing data at scale on AWS</h1>
			<p>In the previous section, <em class="italic">Analyzing large amounts of unstructured data</em>, the data was stored in an <a id="_idIndexMarker518"/>S3 bucket, which was used for training. There will be scenarios where you will need to load data faster for training instead of waiting for the training job to copy the data from S3 locally into your training instance. In these scenarios, you can store the <a id="_idIndexMarker519"/>data on a file system, such as <strong class="bold">Amazon Elastic File System</strong> (<strong class="bold">EFS</strong>) or <strong class="bold">Amazon FSx,</strong> and mount it to the training instance, which will be faster than storing the data in S3 location. The code for this is in the <strong class="source-inline">3_unstructured_data.ipynb</strong> notebook. Refer to the <strong class="bold">Optimize it with data on EFS</strong> and <strong class="bold">Optimize it with data on FSX</strong> sections in<a id="_idIndexMarker520"/> <span class="No-Break">the notebook.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before you run the <strong class="bold">Optimize it with data on EFS</strong> and <strong class="bold">Optimize it with data on FSX</strong> sections, please launch the CloudFormation <strong class="source-inline">template_filesystems.yaml</strong> template, in a similar fashion as we did in the <em class="italic">Setting up EMR and SageMaker </em><span class="No-Break"><em class="italic">Studio</em></span><span class="No-Break"> section.</span></p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor114"/>Cleaning up</h1>
			<p>Let’s terminate the EMR cluster, which we launched in the <em class="italic">Setting up EMR and SageMaker Studio</em> section, as it will not be used in the later chapters of <span class="No-Break">the book.</span></p>
			<p>Let’s start by logging into the <a id="_idIndexMarker521"/>AWS console and following the steps <span class="No-Break">given here:</span></p>
			<ol>
				<li value="1">Search <strong class="source-inline">EMR</strong> in the <span class="No-Break">AWS console.</span></li>
				<li>You will see the active <strong class="bold">EMR-Cluster-sm-emr</strong> cluster. Select the checkbox against the EMR cluster name and click on the <strong class="bold">Terminate</strong> button, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B18493_05_011.jpg" alt="Figure 5.11 – List EMR cluster"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – List EMR cluster</p>
			<ol>
				<li value="3">Click on the red <strong class="bold">Terminate</strong> button in the pop-up window, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B18493_05_012.jpg" alt="Figure 5.12 – Terminate EMR cluster"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – Terminate EMR cluster</p>
			<ol>
				<li value="4">It will take a few minutes to terminate the EMR cluster, and once completed, <strong class="bold">Status</strong> will change <span class="No-Break">to </span><span class="No-Break"><strong class="bold">Terminated</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>Let’s summarize what <a id="_idIndexMarker522"/>we’ve learned in <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor115"/>Summary</h1>
			<p>In this chapter, we explored various data analysis methods, reviewed some of the AWS services for analyzing data, and launched a CloudFormation template to create an EMR cluster, SageMaker Studio domain, and other useful resources. We then did a deep dive into code for analyzing both structured and unstructured data and suggested a few methods for optimizing its performance. This will help you to prepare your data for training <span class="No-Break">ML models.</span></p>
			<p>In the next chapter, we will see how we can train large models on large amounts of data in a distributed fashion to speed up the <span class="No-Break">training process.</span></p>
		</div>
		<div>
			<div id="_idContainer090" class="IMG---Figure">
			</div>
		</div>
	</body></html>