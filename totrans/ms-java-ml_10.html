<html><head></head><body><div class="appendix" title="Appendix&#xA0;A.&#xA0;Linear Algebra"><div class="titlepage"><div><div><h1 class="title"><a id="appA"/>Appendix A. Linear Algebra</h1></div></div></div><p>Linear <a id="id1910" class="indexterm"/>algebra is of primary importance in machine learning and it gives us an array of tools that are especially handy for the purpose of manipulating data and extracting patterns from it. Moreover, when data must be processed in batches as in much machine learning, great runtime efficiencies are gained from using the "vectorized" form as an alternative to traditional looping constructs when implementing software solutions in optimization or data pre-processing or any number of operations in analytics. </p><p>We will consider only the domain of real numbers in what follows. Thus, a vector <span class="inlinemediaobject"><img src="graphics/B05137_10_image001.jpg" alt="Linear Algebra"/></span> represents an array of <span class="emphasis"><em>n</em></span> real-valued numbers. A matrix <span class="inlinemediaobject"><img src="graphics/B05137_10_image004.jpg" alt="Linear Algebra"/></span> is a two-dimensional array of <span class="emphasis"><em>m</em></span> rows and <span class="emphasis"><em>n</em></span> columns of real-valued numbers.</p><p>Some key concepts from the foundation of linear algebra are presented here.</p><div class="section" title="Vector"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec81"/>Vector</h1></div></div></div><p>The vector <span class="strong"><strong>x</strong></span> (lowercase, bold, by convention; equivalently, <span class="inlinemediaobject"><img src="graphics/B05137_10_image131.jpg" alt="Vector"/></span>) can be thought of as a point in <span class="emphasis"><em>n</em></span>-dimensional <a id="id1911" class="indexterm"/>space. Conventionally, we mean column-vector when we say vector. The <span class="emphasis"><em>transpose</em></span> of a column vector is a <span class="emphasis"><em>row</em></span> vector with the same number of elements, arranged in a single row.</p><div class="mediaobject"><img src="graphics/B05137_10_image005.jpg" alt="Vector"/></div><p>  </p><div class="mediaobject"><img src="graphics/B05137_10_image006.jpg" alt="Vector"/></div><p>
</p><div class="section" title="Scalar product of vectors"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec158"/>Scalar product of vectors</h2></div></div></div><p>Also <a id="id1912" class="indexterm"/>known as the dot product, the scalar product is defined <a id="id1913" class="indexterm"/>for two vectors of equal length. The result of the operation is a scalar value and is obtained by summing over the products of the corresponding elements of the vectors. Thus, given vectors <span class="strong"><strong>x</strong></span> and <span class="strong"><strong>y</strong></span>: </p><div class="mediaobject"><img src="graphics/B05137_10_image132.jpg" alt="Scalar product of vectors"/></div><p>The dot product <span class="strong"><strong>x</strong></span>T<span class="strong"><strong>y</strong></span> is given as:</p><div class="mediaobject"><img src="graphics/B05137_10_image133.jpg" alt="Scalar product of vectors"/></div></div></div></div>
<div class="section" title="Matrix"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec82"/>Matrix</h1></div></div></div><p>A <a id="id1914" class="indexterm"/>matrix is a two-dimensional array of numbers. Each element can be indexed by its row and column position. Thus, a 3 x 2 matrix:</p><div class="mediaobject"><img src="graphics/B05137_10_image008.jpg" alt="Matrix"/></div><div class="section" title="Transpose of a matrix"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec159"/>Transpose of a matrix</h2></div></div></div><p>Swapping <a id="id1915" class="indexterm"/>columns for rows in a matrix produces the transpose. Thus, the transpose of <span class="strong"><strong>A</strong></span> is a 2 x 3 matrix:</p><div class="mediaobject"><img src="graphics/B05137_10_image010.jpg" alt="Transpose of a matrix"/></div><div class="section" title="Matrix addition"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec182"/>Matrix addition</h3></div></div></div><p>Matrix <a id="id1916" class="indexterm"/>addition is defined as element-wise summation of two matrices with the same shape. Let <span class="strong"><strong>A</strong></span> and <span class="strong"><strong>B</strong></span> be two <span class="emphasis"><em>m</em></span> x <span class="emphasis"><em>n</em></span> matrices. Their sum <span class="strong"><strong>C</strong></span> can be written as follows:</p><p>
<span class="strong"><strong>C</strong></span>i,j = <span class="strong"><strong>A</strong></span>i,j + <span class="strong"><strong>B</strong></span>i,j</p></div><div class="section" title="Scalar multiplication"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec183"/>Scalar multiplication</h3></div></div></div><p>Multiplication <a id="id1917" class="indexterm"/>with a scalar produces a matrix where each element is scaled by the scalar value. Here <span class="strong"><strong>A</strong></span> is multiplied by the scalar value <span class="emphasis"><em>d</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_10_image015.jpg" alt="Scalar multiplication"/></div></div><div class="section" title="Matrix multiplication"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec184"/>Matrix multiplication</h3></div></div></div><p>Two <a id="id1918" class="indexterm"/>matrices <span class="strong"><strong>A</strong></span> and <span class="strong"><strong>B </strong></span>can be multiplied if the number of columns of <span class="strong"><strong>A</strong></span> equals the number of rows of <span class="strong"><strong>B</strong></span>. If <span class="strong"><strong>A</strong></span> has dimensions <span class="emphasis"><em>m</em></span> x <span class="emphasis"><em>n</em></span> and <span class="strong"><strong>B</strong></span> has dimensions <span class="emphasis"><em>n</em></span> x <span class="emphasis"><em>p</em></span>, then the product <span class="strong"><strong>AB</strong></span> has dimensions <span class="emphasis"><em>m</em></span> x <span class="emphasis"><em>p</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_10_image019.jpg" alt="Matrix multiplication"/></div><div class="section" title="Properties of matrix product"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec262"/>Properties of matrix product</h4></div></div></div><p>Distributivity <a id="id1919" class="indexterm"/>over addition: A(B + C) = AB + AC</p><p>Associativity: A(BC) = (AB)C</p><p>Non-commutativity: AB ≠ BA</p><p>Vector dot-product is commutative: <span class="strong"><strong>x</strong></span>T<span class="strong"><strong>y</strong></span> = <span class="strong"><strong>y</strong></span>T<span class="strong"><strong>x</strong></span>
</p><p>Transpose of product is product of transposes: (<span class="strong"><strong>AB</strong></span>)T = <span class="strong"><strong>A</strong></span>T<span class="strong"><strong>B</strong></span>T</p><div class="section" title="Linear transformation"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl5sec110"/>Linear transformation</h5></div></div></div><p>There <a id="id1920" class="indexterm"/>is a special importance to the product of a matrix and a vector in linear algebra. Consider the product of a 3 x 2 matrix <span class="strong"><strong>A</strong></span> and a 2 x 1 vector <span class="strong"><strong>x</strong></span> producing a 3 x 1 vector <span class="emphasis"><em>y</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_10_image025.jpg" alt="Linear transformation"/></div><p>  </p><div class="mediaobject"><img src="graphics/B05137_10_image026.jpg" alt="Linear transformation"/></div><p> </p><div class="mediaobject"><img src="graphics/B05137_10_image027.jpg" alt="Linear transformation"/></div><p>
</p><div class="mediaobject"><img src="graphics/B05137_10_image028.jpg" alt="Linear transformation"/></div><p>               (C)</p><div class="mediaobject"><img src="graphics/B05137_10_image029.jpg" alt="Linear transformation"/></div><p>                     (R)</p><p>It is <a id="id1921" class="indexterm"/>useful to consider two views of the preceding matrix-vector product, namely, the column picture (<span class="strong"><strong>C</strong></span>) and the row picture (<span class="strong"><strong>R</strong></span>). In the column picture, the product can be seen as a linear combination of the column vectors of the matrix, whereas the row picture can be thought of as the dot products of the rows of the matrix with the vector <span class="inlinemediaobject"><img src="graphics/B05137_10_image030.jpg" alt="Linear transformation"/></span>
</p></div><div class="section" title="Matrix inverse"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl5sec111"/>Matrix inverse</h5></div></div></div><p>The <a id="id1922" class="indexterm"/>product of a matrix with its inverse is the Identity matrix. Thus:</p><div class="mediaobject"><img src="graphics/B05137_10_image031.jpg" alt="Matrix inverse"/></div><p>The matrix inverse, if it exists, can be used to solve a system of simultaneous equations represented <a id="id1923" class="indexterm"/>by the preceding vector-matrix product equation. Consider a system of equations:</p><p>
<span class="emphasis"><em>x</em></span>1 + 2<span class="emphasis"><em>x</em></span>2 = 3</p><p>3<span class="emphasis"><em>x</em></span>1 + 9<span class="emphasis"><em>x</em></span>2 = 21</p><p>This can be expressed as an equation involving the matrix-vector product:</p><div class="mediaobject"><img src="graphics/B05137_10_image034.jpg" alt="Matrix inverse"/></div><p>We can solve for the variables <span class="emphasis"><em>x</em></span>1 and <span class="emphasis"><em>x</em></span>2 by multiplying both sides by the matrix inverse:</p><div class="mediaobject"><img src="graphics/B05137_10_image035.jpg" alt="Matrix inverse"/></div><p> </p><div class="mediaobject"><img src="graphics/B05137_10_image036.jpg" alt="Matrix inverse"/></div><p>
</p><p>The matrix inverse can be calculated by different methods. The reader is advised to view Prof. Strang's MIT lecture: <a class="ulink" href="http://bit.ly/10vmKcL">bit.ly/10vmKcL</a>.</p></div><div class="section" title="Eigendecomposition"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl5sec112"/>Eigendecomposition</h5></div></div></div><p>Matrices <a id="id1924" class="indexterm"/>can be decomposed to factors that can give us valuable insight into the transformation that the matrix represents. Eigenvalues <a id="id1925" class="indexterm"/>and eigenvectors are obtained as the result of eigendecomposition. For a given square matrix <span class="strong"><strong>A</strong></span>, an eigenvector is a non-zero vector that is transformed into a scaled version of itself when multiplied by the matrix. The scalar multiplier is the eigenvalue. All scalar multiples of an eigenvector are also eigenvectors:</p><p>
<span class="strong"><strong>A</strong></span>
<span class="strong"><strong>v</strong></span> = <span class="emphasis"><em>λ</em></span>
<span class="strong"><strong>v</strong></span>
</p><p>In the preceding example, <span class="strong"><strong>v</strong></span> is an eigenvector and λ is the eigenvalue.</p><p>The eigenvalue equation of matrix <span class="strong"><strong>A</strong></span> is given by:</p><p>(<span class="strong"><strong>A</strong></span>
<span class="strong"><strong> </strong></span>— <span class="emphasis"><em>λ</em></span>
<span class="strong"><strong>I</strong></span>)<span class="strong"><strong>v</strong></span> = 0</p><p>The non-zero solution for the eigenvalues is given by the roots of the characteristic polynomial equation of degree <span class="emphasis"><em>n</em></span> represented by the determinant:</p><div class="mediaobject"><img src="graphics/B05137_10_image041.jpg" alt="Eigendecomposition"/></div><p>The eigenvectors can then be found by solving for <span class="emphasis"><em>v</em></span> in <span class="strong"><strong>Av</strong></span> = <span class="emphasis"><em>λ</em></span>
<span class="strong"><strong>v</strong></span>.</p><p>Some matrices, called diagonalizable matrices, can be built entirely from their eigenvectors and eigenvalues. If <span class="strong"><strong>Λ</strong></span> is the diagonal matrix with the eigenvalues of matrix A on its principal diagonal, and <span class="strong"><strong>Q</strong></span> is the matrix whose columns are the eigenvectors of <span class="strong"><strong>A</strong></span>:</p><div class="mediaobject"><img src="graphics/B05137_10_image043.jpg" alt="Eigendecomposition"/></div><p>Then <span class="strong"><strong>A = Q Λ Q</strong></span>-1.</p></div><div class="section" title="Positive definite matrix"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl5sec113"/>Positive definite matrix</h5></div></div></div><p>If a <a id="id1926" class="indexterm"/>matrix has only positive eigenvalues, it is <a id="id1927" class="indexterm"/>called a <span class="strong"><strong>positive definite matrix</strong></span>. If the eigenvalues <a id="id1928" class="indexterm"/>are positive or zero, the matrix is called a <span class="strong"><strong>positive semi-definite matrix</strong></span>. With positive definite matrices, it is true that:</p><p>
<span class="strong"><strong>x</strong></span>T<span class="emphasis"><em> </em></span>
<span class="strong"><strong>Ax</strong></span>
<span class="emphasis"><em> ≥</em></span> 0</p></div></div></div><div class="section" title="Singular value decomposition (SVD)"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec185"/>Singular value decomposition (SVD)</h3></div></div></div><p>SVD is a <a id="id1929" class="indexterm"/>decomposition of any rectangular matrix <span class="strong"><strong>A</strong></span> of dimensions <span class="emphasis"><em>n</em></span> x <span class="emphasis"><em>p</em></span> and is written as the product of three matrices:</p><div class="mediaobject"><img src="graphics/B05137_10_image140.jpg" alt="Singular value decomposition (SVD)"/></div><p>
<span class="strong"><strong>U</strong></span> is defined to be <span class="emphasis"><em>n</em></span> x <span class="emphasis"><em>n</em></span>, <span class="strong"><strong>S</strong></span> is a diagonal <span class="emphasis"><em>n</em></span> x <span class="emphasis"><em>p</em></span> matrix, and <span class="strong"><strong>V</strong></span> is <span class="emphasis"><em>p</em></span> x <span class="emphasis"><em>p</em></span>. <span class="strong"><strong>U</strong></span> and <span class="strong"><strong>V</strong></span> are orthogonal matrices; that is:</p><div class="mediaobject"><img src="graphics/B05137_10_image141.jpg" alt="Singular value decomposition (SVD)"/></div><p>The diagonal values of <span class="strong"><strong>S</strong></span> are called the singular values of <span class="strong"><strong>A</strong></span>. Columns of <span class="strong"><strong>U</strong></span> are called left singular vectors of <span class="strong"><strong>A</strong></span> and those of <span class="strong"><strong>V</strong></span> are called right singular vectors of <span class="strong"><strong>A</strong></span>. The left singular vectors are orthonormal eigenvectors of <span class="strong"><strong>A</strong></span>T<span class="strong"><strong>A</strong></span> and the right singular vectors are orthonormal eigenvectors of <span class="strong"><strong>AA</strong></span>T.</p><p>The SVD <a id="id1930" class="indexterm"/>representation expands the original data into a coordinate system such that the covariance matrix is a diagonal matrix.</p></div></div></div></body></html>