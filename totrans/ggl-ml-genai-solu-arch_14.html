<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer172" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-247"><a id="_idTextAnchor309" class="calibre6 pcalibre pcalibre1"/>12</h1>
<h1 id="_idParaDest-248" class="calibre5"><a id="_idTextAnchor310" class="calibre6 pcalibre pcalibre1"/>Bias, Explainability, Fairness, and Lineage</h1>
<p class="calibre3">Now that we have learned all of the steps required to build and deploy models in Google Cloud and to automate the <a id="_idIndexMarker1258" class="calibre6 pcalibre pcalibre1"/>entire <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) model development life cycle, it’s time to dive into yet more advanced concepts that are fundamental to developing and maintaining <span>high-quality models.</span></p>
<p class="calibre3">In addition to our models providing predictions that are as accurate as possible for a given use case, we need to ensure that the predictions provided by our models are as fair as possible and that they do not exhibit bias or prejudice against any individuals or <span>demographic groups.</span></p>
<p class="calibre3">The topics of bias, fairness, and explainability are at the forefront of ML research today. This chapter discusses these concepts in detail and explains how to effectively incorporate these concepts into our ML workloads. Specifically, we will cover the following topics in <span>this chapter:</span></p>
<ul class="calibre16">
<li class="calibre8">An overview of bias, fairness, and explainability in <strong class="bold">artificial </strong><span><strong class="bold">intelligence</strong></span><span> (</span><span><strong class="bold">AI</strong></span><span>)/ML</span></li>
<li class="calibre8">How to detect and mitigate bias <span>in datasets</span></li>
<li class="calibre8">Using explainability to understand ML models and <span>reduce bias</span></li>
<li class="calibre8">The importance of lineage tracking in ML <span>model development</span></li>
</ul>
<p class="calibre3">Let’s begin by defining and describing the <span>relevant concepts.</span></p>
<h1 id="_idParaDest-249" class="calibre5"><a id="_idTextAnchor311" class="calibre6 pcalibre pcalibre1"/>An overview of bias, explainability, and fairness in AI/ML</h1>
<p class="calibre3">While the terms “bias,” “explainability,” and “fairness” are not specific to ML, in this section, we will explore these terms as they apply to the development and use of <span>ML models.</span></p>
<h2 id="_idParaDest-250" class="calibre9"><a id="_idTextAnchor312" class="calibre6 pcalibre pcalibre1"/>Bias</h2>
<p class="calibre3">Bias in AI/ML refers<a id="_idIndexMarker1259" class="calibre6 pcalibre pcalibre1"/> to tendencies or prejudices in data and algorithms that can lead to unfair outcomes. One of the most common sources of bias in ML model development is when biases exist in the training data; for example, when the data points in the training data do not fairly represent the reality or the population that the model’s predictions will serve, which we refer<a id="_idIndexMarker1260" class="calibre6 pcalibre pcalibre1"/> to as <strong class="bold">data bias</strong>. For example, using a dataset in which the data points predominantly represent only one demographic group to train a model can result in poorer performance when that model is required to make predictions based on data points that represent other demographic groups. More specifically, this is an example of something<a id="_idIndexMarker1261" class="calibre6 pcalibre pcalibre1"/> called <strong class="bold">sampling bias</strong>. To tie this to a real-world scenario, let’s imagine that we’re training a model to perform facial recognition. If we train the model with images of people mainly from one specific demographic group, the model may not perform facial recognition tasks well when it is later presented with images of people from other demographic groups. There are a number of different ways in which we may encounter bias during the development and use of ML models, which we outline in the <span>following paragraphs.</span></p>
<h3 class="calibre11">Collection or measurement bias</h3>
<p class="calibre3">A major cause of <a id="_idIndexMarker1262" class="calibre6 pcalibre pcalibre1"/>data bias is the manner in <a id="_idIndexMarker1263" class="calibre6 pcalibre pcalibre1"/>which<a id="_idIndexMarker1264" class="calibre6 pcalibre pcalibre1"/> the data is collected or measured. Sometimes, bias can occur due to the way we measure or frame a problem, which might not correctly represent the underlying concept that we’re trying to measure. For example, let’s imagine that we own a small company and we want to expand our product offerings to attract new customers. We may decide to use a survey to gather data to train ML models to predict what kinds of new products we should offer, based on how popular those new products are likely to be. There are a number of ways in which we could distribute this survey, such as via email or via physical mail. In the case of email, we might decide to send the survey to all of our current customers, because it’s likely that we would already have their email addresses in our database. In the case of physical email, we may decide to send it out to everybody in the same postal code area, city, state, or country in which our company is <span>physically located.</span></p>
<p class="calibre3">Unfortunately, both of those methods may unexpectedly introduce bias into the datasets used to train our models. For example, perhaps we traditionally happened to appeal to only a specific type of customer in the past. If we use our current customer base as the survey group, we may not get good data points for products that would appeal to new customer demographics. Similarly, if we send out a survey to everybody in a specific geographical area, such as a postal code, city, state, or even country, the people living in that area may not represent diverse demographic groups, which could inadvertently introduce biases into the resulting dataset. This phenomenon is sometimes referred to<a id="_idIndexMarker1265" class="calibre6 pcalibre pcalibre1"/> as <strong class="bold">response bias</strong>, in which the people providing the required information may be biased in some way, and may therefore provide biased responses. To make this even more complicated, the way in which we phrase questions in the survey could accidentally bias <span>the respondents.</span></p>
<p class="calibre3">In this scenario, we also need to be aware of <strong class="bold">observer bias</strong> or <strong class="bold">subjectivity</strong>, in which the people running the survey may be biased in some way. In fact, our example of deciding to email our current customers, or to send physical mail to people in a specific area, is a type of observer bias, in which we decided to survey specific groups of people based <a id="_idIndexMarker1266" class="calibre6 pcalibre pcalibre1"/>on <a id="_idIndexMarker1267" class="calibre6 pcalibre pcalibre1"/>factors<a id="_idIndexMarker1268" class="calibre6 pcalibre pcalibre1"/> such as currently available data (for example, our customers’ email addresses) or proximity to our <span>company’s location.</span></p>
<h3 class="calibre11">Pre-existing bias</h3>
<p class="calibre3">These are<a id="_idIndexMarker1269" class="calibre6 pcalibre pcalibre1"/> generally biases that already exist in society. Such<a id="_idIndexMarker1270" class="calibre6 pcalibre pcalibre1"/> biases can stem from factors such as cultural, historical, and societal norms, consisting of ingrained beliefs and practices that shape the way individuals view <span>the world.</span></p>
<p class="calibre3">One of the most common ways to train a model is to use historical data that has been recorded. However, data from the past might be tainted with biases that were inherent in those times — I think we can all agree that the societal norms of the 1950s or even the 1990s are quite different from today’s standards, especially in terms of fairness across different demographic groups. If AI/ML models are trained on such data without correction, they will likely perpetuate those biases. It’s important to understand that unaddressed pre-existing biases can not only perpetuate but sometimes even amplify stereotypes, leading to unfair or discriminatory outcomes. For example, a job recommendation system might be biased toward recommending higher-paying jobs to men if it is trained on historical data that exhibits <span>this bias.</span></p>
<p class="calibre3">Another common type of bias in this context<a id="_idIndexMarker1271" class="calibre6 pcalibre pcalibre1"/> is <strong class="bold">confirmation bias</strong>, in which people may subconsciously tend to select and interpret data in ways that confirm their <span>pre-existing beliefs.</span></p>
<p class="calibre3">If data collection processes do not check for pre-existing biases, they can lead to data that’s not truly representative of the reality or population it’s supposed to depict. Models trained on such <a id="_idIndexMarker1272" class="calibre6 pcalibre pcalibre1"/>data may learn and replicate those biases<a id="_idIndexMarker1273" class="calibre6 pcalibre pcalibre1"/> in their predictions <span>or actions.</span></p>
<h3 class="calibre11">Algorithmic bias</h3>
<p class="calibre3">This refers to biases<a id="_idIndexMarker1274" class="calibre6 pcalibre pcalibre1"/> that are introduced by the design of the<a id="_idIndexMarker1275" class="calibre6 pcalibre pcalibre1"/> algorithms themselves, not just the data they are trained on. This is an increasingly important topic, considering that algorithms are used to implement an ever-expanding plethora of important decisions in modern society, such as credit approvals, recruitment processes, and <span>medical diagnoses.</span></p>
<p class="calibre3">Algorithmic bias can be more subtle and difficult to detect. For example, such bias could simply stem from the algorithm developers themselves. If the team developing the algorithm lacks diversity, it might not foresee or recognize potential biases in how the algorithm is implemented. We also need to recognize the possibility of accidentally developing biased feedback loops in ML systems. For example, in systems where algorithms are continuously trained with new data, biased outcomes can reinforce the input biases, and over time, this can lead to increasingly <span>skewed results.</span></p>
<p class="calibre3">Just like the other types of biases discussed previously in this section, unaddressed algorithmic bias can lead to the perpetuation of stereotypes, misinformation, and <span>unjust practices.</span></p>
<p class="calibre3">It’s important to note that in this section, we have discussed only some of the most common types of biases that can affect ML model development. This is an active area of research, and additional types of bias exist that we may not be explicitly aware of as we develop and use <span>ML models.</span></p>
<p class="calibre3">Figuratively speaking, the concept of bias represents one side of a coin, and on the other side of that coin is the concept of fairness, which we explore in more <span>detail next.</span></p>
<h2 id="_idParaDest-251" class="calibre9"><a id="_idTextAnchor313" class="calibre6 pcalibre pcalibre1"/>Fairness</h2>
<p class="calibre3">Fairness in AI and ML <a id="_idIndexMarker1276" class="calibre6 pcalibre pcalibre1"/>refers to the practice of developing algorithms in a way that prevents discrimination and promotes equity. While fairness is a rather straightforward concept to define, such as treating all people equally, in practice, it can be difficult to monitor and uphold. A little later in this chapter, we will look at mechanisms to monitor and enhance the fairness of ML models, but let’s first describe the concept in a bit more detail in the context of ML. Just as in the previous section related to bias, we will discuss a few different ways in which we can define and measure fairness with regard to ML models, which we outline in the <span>following subsections.</span></p>
<h3 class="calibre11">Representational fairness</h3>
<p class="calibre3">This is the first line <a id="_idIndexMarker1277" class="calibre6 pcalibre pcalibre1"/>of defense against bias in ML model development, especially with regard to data bias. Representational fairness aims to ensure that the data used in model training and validation contains a fair representation of each of the various demographic groups that the resulting model’s predictions will affect; for example, ensuring that genders and ethnic groups are represented fairly in <span>the dataset.</span></p>
<h3 class="calibre11">Procedural fairness</h3>
<p class="calibre3">This involves<a id="_idIndexMarker1278" class="calibre6 pcalibre pcalibre1"/> ensuring that the processes for data collection, data handling, and algorithm development are equitable and do not favor any particular group. In <a href="B18143_13.xhtml#_idTextAnchor328" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 13</em></span></a>, we will discuss the concept of governance in great detail, especially in relation to data governance, and the important role it plays in helping to ensure that potential biases are addressed or mitigated in datasets. Many enterprises employ entire teams or organizations dedicated to outlining and upholding governance requirements. In the case of procedural fairness, the focus is not so much on the contents of the data, but rather on the fairness of the processes that lead to the development, deployment, and operation of ML models. Some key components of procedural fairness are represented by <a id="_idIndexMarker1279" class="calibre6 pcalibre pcalibre1"/>the <strong class="bold">TAIC</strong> framework, which consists of <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Transparency</strong>, such as clear documentation of the methods, data sources, and decisions made throughout the model’s <span>life cycle</span></li>
<li class="calibre8"><strong class="bold">Accountability</strong>, meaning that there should be clarity on who is responsible for various stages of model development, deployment, <span>and monitoring</span></li>
<li class="calibre8"><strong class="bold">Impartiality</strong>, meaning that procedures should not favor or prejudice any individuals <span>or groups</span></li>
<li class="calibre8"><strong class="bold">Consistency</strong>, meaning<a id="_idIndexMarker1280" class="calibre6 pcalibre pcalibre1"/> that the procedures used to develop a model should be reproducible in a <span>consistent manner</span></li>
</ul>
<p class="calibre3">Google Cloud Vertex AI provides mechanisms that help to audit procedural fairness, which we will explore<a id="_idIndexMarker1281" class="calibre6 pcalibre pcalibre1"/> in more detail in later sections of this chapter, as well as in the practical activities that accompany <span>this chapter.</span></p>
<h3 class="calibre11">Outcome fairness</h3>
<p class="calibre3">While representational<a id="_idIndexMarker1282" class="calibre6 pcalibre pcalibre1"/> fairness mainly focuses on the inputs used to train ML models (that is, the contents of the training data) and procedural fairness focuses on the procedures used to develop and deploy ML models, outcome fairness, as the name suggests, focuses on the results produced by ML models. Specifically, it aims to ensure that the outcomes produced by a model are equitable and do not disproportionately benefit or harm any group. Of course, one way to measure this is to monitor the predictions made by a model to determine whether any bias appears to be present. In this chapter, we will explore some mechanisms and metrics that can be used for <span>this purpose.</span></p>
<p class="calibre3">As I mentioned when discussing bias in the previous section, the concepts of bias and fairness in ML are still very active and evolving areas of research. This is why fairness can be a complex topic in practice, and it’s important to understand that achieving one type of fairness can sometimes lead to a violation of another type. Moreover, what is considered “fair” can be context-dependent and might vary between communities, as well as <span>over time.</span></p>
<p class="calibre3">Now that we’ve introduced the concepts that define the two-sided coin of bias and fairness, the next step will be to introduce a topic that is inherently linked to those two concepts, which is referred to <span>as </span><span><strong class="bold">explainability</strong></span><span>.</span></p>
<h2 id="_idParaDest-252" class="calibre9"><a id="_idTextAnchor314" class="calibre6 pcalibre pcalibre1"/>Explainability</h2>
<p class="calibre3">Explainability in ML focuses on the ability of a human to understand <a id="_idIndexMarker1283" class="calibre6 pcalibre pcalibre1"/>and explain how the outputs of an ML model were produced. This is often also referred to<a id="_idIndexMarker1284" class="calibre6 pcalibre pcalibre1"/> as the <strong class="bold">interpretability</strong> of a model. These concepts continue to grow in importance as ML models become increasingly complex over time. For example, it’s pretty easy to interpret and explain how the outputs of a simple linear regression model were produced, because we have well-defined mathematical formulae that describe that process, such as <em class="italic">y = a + bx</em>, where <em class="italic">y</em> (the output) is a linear transformation of <em class="italic">x</em> (the input). Similarly, for decision tree models, we can logically trace the decision path through the tree. However, when we’re dealing with <a id="_idIndexMarker1285" class="calibre6 pcalibre pcalibre1"/>large <strong class="bold">neural networks</strong> (<strong class="bold">NNs</strong>) containing millions or even billions of parameters, and incorporating different types of architectures and activation functions, we need some additional tools to help us understand how those models are making their decisions, which we will explore in <span>this chapter.</span></p>
<p class="calibre3">When we discussed procedural fairness in the previous section, we talked about the importance of transparency. Explainability links closely to the concept of transparency, as it seeks to ensure that people can understand the process by which predictions are produced from a given model. For example, if we own a bank and one of our models is responsible for granting or declining credit applications, we want to thoroughly understand how it makes <span>those decisions.</span></p>
<p class="calibre3">We can talk about explainability in <a id="_idIndexMarker1286" class="calibre6 pcalibre pcalibre1"/>terms of <strong class="bold">global explainability</strong>, in which we try to understand the general logic the model applies to make predictions across all input<a id="_idIndexMarker1287" class="calibre6 pcalibre pcalibre1"/> instances, or <strong class="bold">local explainability</strong>, which involves explaining why a model made a specific decision for a particular instance (for example, understanding why a specific customer’s credit application <span>was declined).</span></p>
<p class="calibre3">Overall, explainability is critical for building trust in AI systems, complying with legal requirements, and ensuring that humans can intervene effectively in <span>decision-making processes.</span></p>
<p class="calibre3">Now that we’ve introduced and explained the overall concepts, let’s start diving in deeper. Our first deep dive in this chapter will be on the topic of bias <span>in datasets.</span></p>
<h1 id="_idParaDest-253" class="calibre5"><a id="_idTextAnchor315" class="calibre6 pcalibre pcalibre1"/>How to detect and mitigate bias in datasets</h1>
<p class="calibre3">In this section, we explore how to detect bias in our datasets, and there are various tools and methods we can use for this purpose. In fact, we’ve already covered some of them in previous chapters of this book, such as data exploration <span>and visualization.</span></p>
<h2 id="_idParaDest-254" class="calibre9"><a id="_idTextAnchor316" class="calibre6 pcalibre pcalibre1"/>Data exploration and visualization</h2>
<p class="calibre3">When we explore<a id="_idIndexMarker1288" class="calibre6 pcalibre pcalibre1"/> our datasets using visualization, for example, charts such as histograms and scatter plots can help visualize disparities in data distribution for different demographic groups. Similarly, we’ve already explored descriptive statistics such as mean, median, mode, and variance to understand the contents of our datasets. If there are significant disparities in these statistics between subgroups, it may suggest the presence of bias in <span>the dataset.</span></p>
<h2 id="_idParaDest-255" class="calibre9"><a id="_idTextAnchor317" class="calibre6 pcalibre pcalibre1"/>Specific tools for detecting dependencies between features</h2>
<p class="calibre3">We also want to<a id="_idIndexMarker1289" class="calibre6 pcalibre pcalibre1"/> test for potential correlations or dependence between features in our dataset in order to understand whether the values of some features are significantly influenced by the values of others. While this is something that we generally want to do as part of our regular data exploration and feature engineering anyway (that is, we need to understand underlying patterns in our data as much as possible), it becomes even more important in the context of bias and fairness, especially with regard to potential links between the target variable and protected attributes such as gender <span>or ethnicity.</span></p>
<p class="calibre3">There are specific tools for detecting dependencies among features, such as <strong class="bold">Pearson’s Correlation Coefficient</strong>, which <a id="_idIndexMarker1290" class="calibre6 pcalibre pcalibre1"/>measures the linear relationship between numeric variables, or the <strong class="bold">Chi-Squared Test for Independence</strong>, which <a id="_idIndexMarker1291" class="calibre6 pcalibre pcalibre1"/>can be used to determine if there is a significant association between two <span>categorical variables.</span></p>
<h2 id="_idParaDest-256" class="calibre9"><a id="_idTextAnchor318" class="calibre6 pcalibre pcalibre1"/>Mechanisms incorporating model prediction results</h2>
<p class="calibre3">In addition to the<a id="_idIndexMarker1292" class="calibre6 pcalibre pcalibre1"/> aforementioned methods, we can use mechanisms that go beyond just examining the dataset by also taking a model’s outputs into account, which can then help us to link any observed biases back to the training data and process. For example, we can use <strong class="bold">disparate impact analysis</strong> (<strong class="bold">DIA</strong>) to compare the ratio of favorable outcomes for different groups and measure whether any specific groups tend<a id="_idIndexMarker1293" class="calibre6 pcalibre pcalibre1"/> to get more favorable outcomes than any others. Let’s take a look at DIA in <span>more detail.</span></p>
<h3 class="calibre11">DIA</h3>
<p class="calibre3">In the case <a id="_idIndexMarker1294" class="calibre6 pcalibre pcalibre1"/>of DIA, we <a id="_idIndexMarker1295" class="calibre6 pcalibre pcalibre1"/>generally identify what is referred to as a <strong class="bold">privileged group</strong> and<a id="_idIndexMarker1296" class="calibre6 pcalibre pcalibre1"/> an <strong class="bold">unprivileged group</strong> (also<a id="_idIndexMarker1297" class="calibre6 pcalibre pcalibre1"/> referred to <a id="_idIndexMarker1298" class="calibre6 pcalibre pcalibre1"/>as a <strong class="bold">protected group</strong>), based on some protected characteristic such as gender or race, and we compare the outputs of a given model for both of those groups to try to determine whether the model appears to bias in either direction (positively or negatively) with regard to those groups. We measure this disparity using a metric referred to <a id="_idIndexMarker1299" class="calibre6 pcalibre pcalibre1"/>as the <strong class="bold">disparate impact ratio</strong> (<strong class="bold">DIR)</strong>, which is defined by the <span>following formula:</span></p>
<p class="calibre3"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" height="68" src="image/12.png" width="232" class="calibre155"/></p>
<p class="calibre3">Here, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mi&gt;U&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" height="32" src="image/13.png" width="126" class="calibre156"/> represents the probability of a favorable outcome for the unprivileged group, and <span>P</span><span>(</span><span>F</span><span>P</span><span>)</span> represents the probability of a favorable outcome for the <span>privileged group.</span></p>
<p class="calibre3">The DIR value can be interpreted in the <span>following way:</span></p>
<ul class="calibre16">
<li class="calibre8">When DIR is equal to 1, it indicates perfect fairness, where both groups receive favorable outcomes at the <span>same rate</span></li>
<li class="calibre8">When DIR is greater than 1, it indicates that the unprivileged group is more likely to receive <span>favorable outcomes</span></li>
<li class="calibre8">When DIR is less than 1, it indicates that the unprivileged group is less likely to receive <span>favorable outcomes</span></li>
</ul>
<p class="calibre3">A commonly accepted threshold is a DIR value of between 0.8 and 1.25; values outside this range often indicate<a id="_idIndexMarker1300" class="calibre6 pcalibre pcalibre1"/> potential <strong class="bold">disparate </strong><span><strong class="bold">impact</strong></span><span> (</span><span><strong class="bold">DI</strong></span><span>).</span></p>
<p class="calibre3">Note that DIA can be performed directly on a dataset by using the target feature, or it can use a combination of the input features and a <span>model’s predictions.</span></p>
<p class="calibre3">In the next section, we will dive into the concept of explainability in ML in more detail, and explore how we can use explainability frameworks to detect and address bias. We will also expand<a id="_idIndexMarker1301" class="calibre6 pcalibre pcalibre1"/> on the concepts covered<a id="_idIndexMarker1302" class="calibre6 pcalibre pcalibre1"/> in this section, and discuss how they relate to explainability <span>and fairness.</span></p>
<h1 id="_idParaDest-257" class="calibre5"><a id="_idTextAnchor319" class="calibre6 pcalibre pcalibre1"/>Using explainability to understand ML models and 
reduce bias</h1>
<p class="calibre3">We introduced the<a id="_idIndexMarker1303" class="calibre6 pcalibre pcalibre1"/> concept of explainability at a high level in the previous section. This section dives further into this topic, introducing tools that can be used to gain insights into how ML models are working at <span>inference time.</span></p>
<h2 id="_idParaDest-258" class="calibre9"><a id="_idTextAnchor320" class="calibre6 pcalibre pcalibre1"/>Explainability techniques, methods, and tools</h2>
<p class="calibre3">Let’s begin by <a id="_idIndexMarker1304" class="calibre6 pcalibre pcalibre1"/>exploring some popular techniques, methods, and tools that we can use for implementing explainability in ML, which we describe in the <span>following subsections.</span></p>
<h3 class="calibre11">Performing data exploration</h3>
<p class="calibre3">By now, it should<a id="_idIndexMarker1305" class="calibre6 pcalibre pcalibre1"/> hopefully be clear that<a id="_idIndexMarker1306" class="calibre6 pcalibre pcalibre1"/> understanding the data used to train our models is one of the first steps in explaining how the model makes decisions, and it is also one of the first lines of defense to identify and combat <span>potential biases.</span></p>
<p class="calibre3">In the practical activities associated with this chapter, we explore the “Adult Census Income” dataset (<a href="https://archive.ics.uci.edu/dataset/2/adult" class="calibre6 pcalibre pcalibre1">https://archive.ics.uci.edu/dataset/2/adult</a>), which is known to contain imbalances with regard to race and gender. The dataset comprises information pertaining to people, such as their race, gender, education they’ve received, and their current annual income, expressed as either “&lt;=50K” (less than or equal to $50,000 per year) or “&gt;50K” (more than $50,000 per year), which creates a binary classification use case when the income is used as the <span>target variable.</span></p>
<p class="calibre3">When exploring this data, we can ask questions such as <span>the following:</span></p>
<ol class="calibre7">
<li class="calibre8">Are feature values, such as race and gender, represented somewhat evenly <span>or unevenly?</span></li>
<li class="calibre8">Are there any correlations between some feature values and the income earned by <span>that person?</span></li>
</ol>
<p class="calibre3">We can easily<a id="_idIndexMarker1307" class="calibre6 pcalibre pcalibre1"/> see imbalances in the dataset by <a id="_idIndexMarker1308" class="calibre6 pcalibre pcalibre1"/>using data visualization techniques. Let’s take a look at <span>some examples.</span></p>
<h4 class="calibre20">Income distribution by gender</h4>
<p class="calibre3">The following <a id="_idIndexMarker1309" class="calibre6 pcalibre pcalibre1"/>code will show us the distributions of each gender in the dataset, with regard to the two different income categories (that is, people who earn less than or equal to $50,000 per year, or people who earn more than $50,000 per year). You can open the Jupyter notebook that accompanies this chapter if you’d like to follow along with the code examples that we will review. We can again use the same Vertex AI Workbench-Notebook Instance that we created in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a> for this purpose. Please open JupyterLab on that notebook instance. In the directory explorer on the left side of the screen, navigate to the <strong class="source-inline">Chapter-12</strong> directory and open the <strong class="source-inline">bias-explainability.ipynb</strong> notebook. You can choose <strong class="bold">Python (Local)</strong> as the kernel. Again, you can run each cell in the notebook by selecting the cell and pressing <em class="italic">Shift</em> + <em class="italic">Enter</em> on your keyboard. In addition to the relevant code, the notebook contains markdown text that describes what the code <span>is doing:</span></p>
<pre class="source-code">
plt.figure(figsize=(10, 6))
sns.countplot(x='income', hue='sex', data=adult_data)
plt.title('Income Distribution by Gender')
plt.show()</pre> <p class="calibre3">This will display a graph like the one shown in <span><em class="italic">Figure 12</em></span><span><em class="italic">.1</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer161">
<img alt="Figure 12.1: Income distribution by gender" src="image/B18143_12_1.jpg" class="calibre157"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 12.1: Income distribution by gender</p>
<p class="calibre3">In <span><em class="italic">Figure 12</em></span><em class="italic">.1</em>, we can see that, overall, more people in the dataset earn more than $50,000 per year than<a id="_idIndexMarker1310" class="calibre6 pcalibre pcalibre1"/> those who do not. We can also see that the number of males in each group far exceeds the number of females in each group. This also tells us the entire dataset consists of more data points related to men <span>than women.</span></p>
<h4 class="calibre20">Income distribution by race</h4>
<p class="calibre3">The following code<a id="_idIndexMarker1311" class="calibre6 pcalibre pcalibre1"/> will show us the distributions of each race in the dataset, with regard <span>to income:</span></p>
<pre class="source-code">
plt.figure(figsize=(15, 8))
sns.countplot(x='income', hue='race', data=adult_data)
plt.title('Income Distribution by Race')
plt.show()</pre> <p class="calibre3">This will display a graph like the one shown in <span><em class="italic">Figure 12</em></span><span><em class="italic">.2</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer162">
<img alt="Figure 12.2: Income distribution by race" src="image/B18143_12_2.jpg" class="calibre158"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 12.2: Income distribution by race</p>
<p class="calibre3">In <span><em class="italic">Figure 12</em></span><em class="italic">.2</em>, for both the “&lt;=50K” category and the “&gt;50K” category, we see that the data contains many more data points for White people than any other race. This can be seen as a type of bias in the dataset. As we introduced earlier in this chapter, this bias may exist for multiple potential reasons, such as bias in the collection of the data, or it may happen due to other factors such as geographic location. This particular dataset represents the population of a specific area, which may somewhat explain its apparent bias toward a particular race. For example, if the data were collected in Asia, then it would contain many more data points for Asian people than any other race, or if it were collected in central Africa, then it would contain many more data points for Black people than any other race. It’s important to note any imbalances in the data and determine how they may affect the training of an ML model and who that ML model is intended to serve. Generally, if features in the dataset have much higher numbers of instances of a specific value, then an ML model’s predictions will likely reflect that in <span>some way.</span></p>
<p class="calibre3">In the Jupyter notebook that accompanies this chapter, we also assess other types of distributions in<a id="_idIndexMarker1312" class="calibre6 pcalibre pcalibre1"/> the data, such as occupational distribution by gender and educational distribution by race. I encourage you to use the Jupyter notebook to explore the data in more detail. For now, let’s move on and look at implementing DIA in <span>more detail.</span></p>
<h3 class="calibre11">Implementing DIA</h3>
<p class="calibre3">The following is the <a id="_idIndexMarker1313" class="calibre6 pcalibre pcalibre1"/>code we use to implement <a id="_idIndexMarker1314" class="calibre6 pcalibre pcalibre1"/>DIA in our <span>Jupyter notebook:</span></p>
<pre class="source-code">
pivot_gender_income = adult_data.pivot_table(index='sex', 
    columns='income', values='age', aggfunc='count')
pivot_gender_income['rate'] = pivot_gender_income['&gt;50K'] / (
    pivot_gender_income['&gt;50K'] + pivot_gender_income['&lt;=50K'])
DI = pivot_gender_income.loc['Female', 'rate'] / (
    pivot_gender_income.loc['Male', 'rate'])</pre> <p class="calibre3">The code first creates a pivot table of the <strong class="source-inline">adult_data</strong> DataFrame, grouped by gender and income, with the count of people in each group as the value. Then, it adds a new column to the pivot table called <strong class="source-inline">rate</strong>, which is the proportion of people in each group who earn more than $50,000. Finally, it calculates the DI by dividing the rate for females by the rate <span>for males.</span></p>
<p class="calibre3">This is a very simple example of implementing DIA on a dataset that is known to contain gender imbalances. DIA can be much more complex and may require some domain expertise to implement effectively, depending on the contents of the dataset and the intended function of <a id="_idIndexMarker1315" class="calibre6 pcalibre pcalibre1"/>an ML model trained on <span>that </span><span><a id="_idIndexMarker1316" class="calibre6 pcalibre pcalibre1"/></span><span>data.</span></p>
<p class="calibre3">Next, let’s discuss the topic of <span>feature importance.</span></p>
<h2 id="_idParaDest-259" class="calibre9"><a id="_idTextAnchor321" class="calibre6 pcalibre pcalibre1"/>Feature importance</h2>
<p class="calibre3">Feature<a id="_idIndexMarker1317" class="calibre6 pcalibre pcalibre1"/> importance <a id="_idIndexMarker1318" class="calibre6 pcalibre pcalibre1"/>evaluates the impact that each feature has on the predictions made by a model. To explore this concept, let’s imagine we have a dataset that contains information about people, and the features in the dataset include height, age, eye color, and whether or not they like coffee. We want to use this data to train a model to predict the likelihood of each person being a successful basketball player. Do you think any of the input features in our dataset may be more important than any others in terms of influencing the outcome? Is it likely that height would be more important or less important than eye color in determining whether a person is likely to be a successful basketball player? Is age an important factor? In this case, we’re describing the concept of feature importance in a simple example. In reality, we may be dealing with datasets that contain thousands of features, and those features may not always represent easily interpretable concepts such as height, age, and eye color. For this reason, we can use tooling to help us to get these kinds <a id="_idIndexMarker1319" class="calibre6 pcalibre pcalibre1"/>of insights. The following<a id="_idIndexMarker1320" class="calibre6 pcalibre pcalibre1"/> subsections describe the tools we can use for <span>this purpose.</span></p>
<h3 class="calibre11">Feature importance tools built into popular ML libraries</h3>
<p class="calibre3">In the Jupyter<a id="_idIndexMarker1321" class="calibre6 pcalibre pcalibre1"/> notebook that accompanies this chapter, we use scikit-learn to train a binary classifier model, and we use the <strong class="source-inline">feature_importances_</strong> attribute to inspect the apparent importance of each feature. This is an attribute of certain scikit-learn estimators, particularly tree-based estimators such as decision trees, random forests, and gradient boosted trees, which provide an array of importance scores for each feature, in which higher values indicate more importance. The following code snippet uses the <strong class="source-inline">feature_importances_</strong> attribute to create a graph similar to that shown in <span><em class="italic">Figure 12</em></span><span><em class="italic">.3</em></span><span>:</span></p>
<pre class="source-code">
feat_importances = pd.Series(clf.feature_importances_, 
    index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')</pre> <div class="calibre2">
<div class="img---figure" id="_idContainer163">
<img alt="Figure 12.3: Feature importance" src="image/B18143_12_3.jpg" class="calibre159"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 12.3: Feature importance</p>
<p class="calibre3">In <span><em class="italic">Figure 12</em></span><em class="italic">.3</em>, we can see that features such as <strong class="source-inline">age</strong>, <strong class="source-inline">hours-per-week</strong>, and <strong class="source-inline">capital-gain</strong> seem to be pretty important features with regard to predicting income. Does the influence of those features seem intuitive <span>to you?</span></p>
<p class="calibre3">Note that feature importance does not imply causality. Just because a feature is deemed important doesn’t mean it causes the target variable to change; only that there’s <span>an association.</span></p>
<p class="calibre3">While we’re specifically using the <strong class="source-inline">feature_importances_</strong> attribute in scikit-learn in our Jupyter notebook, other popular ML libraries also provide similar mechanisms. For example, TensorFlow’s boosted trees (<strong class="source-inline">tf.estimator.BoostedTreesClassifier</strong> or <strong class="source-inline">tf.estimator.BoostedTreesRegressor</strong>) also provide <strong class="source-inline">feature_importances_</strong> as a property to get the importance of each feature. Similarly, LightGBM and CatBoost provide <strong class="source-inline">feature_importances_</strong> and <strong class="source-inline">get_feature_importance()</strong>, respectively, and XGBoost provides the <strong class="source-inline">plot_importance()</strong> function <span>for visualization.</span></p>
<p class="calibre3">While <strong class="source-inline">feature_importances_</strong>, and similar mechanisms from other ML libraries, can be very<a id="_idIndexMarker1322" class="calibre6 pcalibre pcalibre1"/> useful, there are yet more advanced tools that we can use in order to assess feature importance, which I will <span>describe next.</span></p>
<h4 class="calibre20">Partial dependence plots</h4>
<p class="calibre3"><strong class="bold">Partial dependence plots</strong> (<strong class="bold">PDPs</strong>) are graphical visualizations used to understand the <a id="_idIndexMarker1323" class="calibre6 pcalibre pcalibre1"/>relationship <a id="_idIndexMarker1324" class="calibre6 pcalibre pcalibre1"/>between an input feature (or a set of input features) and a model’s predicted outcome. With PDPs, we change the value of just one feature, while keeping the values of all other features constant, in order to determine how the different values of that particular feature impact the prediction. PDPs can also be used with more than one input feature at a time, which can reveal interactions among multiple features. There’s also an extended form of PDPs called <strong class="bold">individual conditional expectation (ICE) plots</strong>. While PDPs show the average effect of a feature on<a id="_idIndexMarker1325" class="calibre6 pcalibre pcalibre1"/> predictions, ICE plots display the effect of a feature on predictions for <span>individual instances.</span></p>
<p class="calibre3">With PDPs, for every unique value of the feature of interest, the following high-level steps <span>are performed:</span></p>
<ol class="calibre7">
<li class="calibre8">Set the feature of interest to that value for every instance in <span>the dataset.</span></li>
<li class="calibre8">Make predictions using <span>the model.</span></li>
<li class="calibre8">Average <span>the predictions.</span></li>
<li class="calibre8">Plot the averaged predictions against the unique values of <span>the feature.</span></li>
</ol>
<p class="calibre3">The following code uses the <strong class="source-inline">PartialDependenceDisplay</strong> attribute from scikit-learn to create and display a <span>PDP graph:</span></p>
<pre class="source-code">
features = ['age', 'hours-per-week']
PartialDependenceDisplay.from_estimator(clf, X, features)</pre> <p class="calibre3">This code will <a id="_idIndexMarker1326" class="calibre6 pcalibre pcalibre1"/>produce a graph similar to the one shown in <span><em class="italic">Figure 12</em></span><span><em class="italic">.4</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer164">
<img alt="Figure 12.4: PDP" src="image/B18143_12_4.jpg" class="calibre160"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 12.4: PDP</p>
<p class="calibre3">In the PDP shown in <span><em class="italic">Figure 12</em></span><em class="italic">.4</em>, the model appears to predict that people tend to start increasingly earning more money from the age of about 20 onward until they reach the age of around 60, after which their income begins to decrease. This is somewhat intuitive, considering that 20 years of age is considered early adulthood, and people often tend to retire in their 60s. Similarly, the model predicts that working more hours per week may<a id="_idIndexMarker1327" class="calibre6 pcalibre pcalibre1"/> result in <span>higher</span><span><a id="_idIndexMarker1328" class="calibre6 pcalibre pcalibre1"/></span><span> income.</span></p>
<p class="calibre3">Next, we move on to discuss more advanced feature importance and <span>explanation mechanisms.</span></p>
<h4 class="calibre20">Local Interpretable Model-agnostic Explanations</h4>
<p class="calibre3">As I mentioned <a id="_idIndexMarker1329" class="calibre6 pcalibre pcalibre1"/>earlier in this chapter, some models are naturally more easily interpretable and understandable (and therefore explainable) than others. Examples provided were linear regression models and decision trees, versus large NNs with thousands, millions, or even billions of parameters (perhaps soon to be trillions!). <strong class="bold">local interpretable model-agnostic explanations</strong> (<strong class="bold">LIME</strong>) takes advantage of this fact, by training<a id="_idIndexMarker1330" class="calibre6 pcalibre pcalibre1"/> a simpler, <strong class="bold">surrogate</strong> model that<a id="_idIndexMarker1331" class="calibre6 pcalibre pcalibre1"/> can be more easily explained than the target model. The idea is that even if the overall model is complex and non-linear, it can be approximated well by a simpler, <span>interpretable model.</span></p>
<p class="calibre3">LIME’s inner workings are quite complex, and I would recommend reading the original paper (<em class="italic">arXiv:1602.04938</em>) if you want to delve into that level of detail. Such algorithmic details are generally not required for the activities of a solutions architect, and you would mainly <a id="_idIndexMarker1332" class="calibre6 pcalibre pcalibre1"/>need to understand what LIME is used for without needing to dive into the academic details of its inner workings. Moving on, then, the following code provides an example of how to <span>use LIME:</span></p>
<pre class="source-code">
explainer = lime_tabular.LimeTabularExplainer(
    X_train.values, training_labels=y_train,
    feature_names=X.columns.tolist(), 
    class_names=['&lt;=50K', '&gt;50K'], 
    mode='classification')
exp = explainer.explain_instance(
    X_test.values[0], clf.predict_proba, 
    num_features=10)
exp.show_in_notebook()</pre> <p class="calibre3">The code will produce visualizations similar to those shown in <span><em class="italic">Figure 12</em></span><span><em class="italic">.5</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer165">
<img alt="Figure 12.5: LIME outputs" src="image/B18143_12_5.jpg" class="calibre161"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 12.5: LIME outputs</p>
<p class="calibre3">At the top of <span><em class="italic">Figure 12</em></span><em class="italic">.5</em>, we see the model’s prediction for the instance that was used as input. In this case, since it’s a binary classification problem, it shows the class that the model<a id="_idIndexMarker1333" class="calibre6 pcalibre pcalibre1"/> predicted, along with the probability score associated with that prediction. We also see a horizontal bar chart representing the influence of various features on the prediction. Each bar represents a feature and its impact. The length of the bar indicates the significance of the influence, and its direction (left or right) indicates the direction of the influence (for example, toward the “&lt;=50K” or “&gt;50K” class). Each bar is labeled with the feature name and a small descriptor, which indicates how that feature was quantified for the specific instance being interpreted. This provides a clear indication of how the feature value for that specific instance influenced <span>the prediction.</span></p>
<p class="calibre3">It’s important to highlight that LIME’s explanations are local. They are specifically tailored to the instance in question and show how the model made its prediction for that one instance, not a general rule for all data. Next, we explore a mechanism that can help with both local and <a id="_idIndexMarker1334" class="calibre6 pcalibre pcalibre1"/>global model interpretations and is perhaps one of the most popular feature importance and explanation mechanisms in <span>the industry.</span></p>
<h4 class="calibre20">SHapley Additive exPlanations</h4>
<p class="calibre3"><strong class="bold">Shapley Additive exPlanations</strong> (<strong class="bold">SHAP</strong>) helps<a id="_idIndexMarker1335" class="calibre6 pcalibre pcalibre1"/> to explain ML model predictions using a <a id="_idIndexMarker1336" class="calibre6 pcalibre pcalibre1"/>concept <a id="_idIndexMarker1337" class="calibre6 pcalibre pcalibre1"/>called <strong class="bold">Shapley values</strong>. These values come from a field of mathematics <a id="_idIndexMarker1338" class="calibre6 pcalibre pcalibre1"/>referred to as <strong class="bold">game theory</strong> or, more <a id="_idIndexMarker1339" class="calibre6 pcalibre pcalibre1"/>specifically, <strong class="bold">cooperative game theory</strong>, and they were originally introduced in the 1950s by Lloyd Shapley. The study of game theory looks at competitive situations (called “games”) where the results of one player’s choices depend on what other players do. Cooperative game theory is a sub-branch that looks at games where players can make alliances or <strong class="bold">coalitions</strong> with other players and work together as a team to benefit the coalition as a whole, rather than just looking out for their <span>own interests.</span></p>
<p class="calibre3">Diving in further, let’s imagine a game in which the winner will receive a payout of $100. People can either play the game individually, in which case an individual would simply receive $100 if they win, or they can form a team and work together to try to win the $100 payout. If three people form a team and they win the payout, in reality, it’s likely they would just split the payout into three equal parts. However, what if the payout should be <strong class="bold">fairly</strong> divided based on the contribution of each person? This is where Shapley values come into <span>the picture.</span></p>
<p class="calibre3">Shapley values <a id="_idIndexMarker1340" class="calibre6 pcalibre pcalibre1"/>represent the <strong class="bold">average marginal contribution</strong> of each player over all possible coalitions they could enter, as well as all possible orders in which they could enter each coalition. Again, striking a balance of how much detail to cover on this subject, I will not include the complex mathematical details here but would recommend reading the original paper if you would like to dive into those details, the formal reference for which is <span>provided here:</span></p>
<p class="calibre3"><em class="italic">Shapley, L.S. (1953). A Value for n-Person Games. In “Contributions to the Theory of Games volume II”, H.W. Kuhn and A.W. Tucker (eds.), Princeton </em><span><em class="italic">University Press.</em></span></p>
<p class="calibre3">Here, we will focus on how Shapley values are used in the context of ML model explainability. We begin with the<a id="_idIndexMarker1341" class="calibre6 pcalibre pcalibre1"/> concept of an <strong class="bold">average prediction</strong>, which represents the average of all predictions our model makes over our entire dataset. For regression models, this is simply the mean of all predicted outputs. For classification models, it is the average predicted probability for a given class over the <span>entire dataset.</span></p>
<p class="calibre3">When computing Shapley values for a particular instance (that is, a row of input data), the contributions of each feature are measured with respect to how they move the prediction for that instance away from the average prediction. The Shapley value for a feature then captures its average marginal contribution over all possible combinations of features. For example, consider a binary classification model that predicts whether a bank loan will be defaulted on. If, on average, the model predicts a 5% probability of default over the entire dataset, this 5% would be the “average prediction.” Now, for a particular loan application, if the model predicts a 20% probability of default, Shapley values help attribute this 15% deviation from the average to each feature in the input (for example, the applicant’s income, employment status, credit score, and so on). Each feature’s Shapley value will indicate how much, on average, that feature contributes to the deviation of the prediction from the <span>average prediction.</span></p>
<p class="calibre3">The following code <a id="_idIndexMarker1342" class="calibre6 pcalibre pcalibre1"/>provides an example of how to get feature importance insights using the <span>SHAP library:</span></p>
<pre class="source-code">
shap_values = shap.TreeExplainer(clf).shap_values(X_test)
shap.summary_plot(shap_values[1], X_test, plot_type="bar")</pre> <p class="calibre3">In the code, a <strong class="source-inline">TreeExplainer</strong> object from the SHAP library is being created. This specific explainer is optimized for tree-based models, such as decision trees, random forests, and gradient boosted trees. The <strong class="source-inline">clf</strong> instance that we pass to it is a tree-based model that we’ve trained. Once the explainer is created, we use the <strong class="source-inline">.shap_values()</strong> method to compute the SHAP values for each sample in our <strong class="source-inline">X_test</strong> dataset. We then visualize the average impact of each feature on the model’s predictions by using a bar plot (the longer the bar, the greater the feature’s importance). The code will produce a graph similar to the one shown in <span><em class="italic">Figure 12</em></span><span><em class="italic">.6</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer166">
<img alt="Figure 12.6: SHAP outputs" src="image/B18143_12_6.jpg" class="calibre162"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 12.6: SHAP outputs</p>
<p class="calibre3">As we can see in <span><em class="italic">Figure 12</em></span><em class="italic">.6</em>, perhaps surprisingly, a person’s marital status appears to have the most impact on the model’s output. Considering that the SHAP value for a feature is the average contribution of that feature value to every possible prediction (averaged over all instances), it takes into account intricate interactions with other features, as well as the impact of the <span>feature itself.</span></p>
<p class="calibre3">While we can<a id="_idIndexMarker1343" class="calibre6 pcalibre pcalibre1"/> import tools such as SHAP into our notebooks, we can also use Vertex AI APIs to get explanations directly from models hosted in Vertex AI. The next section describes how to <span>do this.</span></p>
<h3 class="calibre11">Getting explanations from a deployed model in Vertex AI</h3>
<p class="calibre3">Conveniently, Vertex AI <a id="_idIndexMarker1344" class="calibre6 pcalibre pcalibre1"/>provides APIs and an SDK that we can use to get explanations<a id="_idIndexMarker1345" class="calibre6 pcalibre pcalibre1"/> from our models. In the Jupyter notebook that accompanies this chapter, we use the <strong class="source-inline">projects.locations.endpoints.explain</strong> API to get explanations from the model that we deployed in our MLOps pipeline in the previous chapter. The following is a snippet of the code we use to <span>do so:</span></p>
<pre class="source-code">
endpoint = aiplatform.Endpoint(endpoint_name)
response = endpoint.explain(instances=[instance_dict], parameters={})
    for explanation in response.explanations:
        print(" explanation")
        # Feature attributions.
        attributions = explanation.attributions
        for attribution in attributions:
            print("  attribution")
            print("   baseline_output_value:", 
                attribution.baseline_output_value)
            print("   instance_output_value:", 
                attribution.instance_output_value)
            # Convert feature_attributions to a dictionary and print
            feature_attributions_dict = dict(
                attribution.feature_attributions)
            print("   feature_attributions:", 
                feature_attributions_dict)
            print("   approximation_error:", 
                attribution.approximation_error)
            print("   output_name:", attribution.output_name)</pre> <p class="calibre3">The code will produce an output similar to <span>the following:</span></p>
<pre class="source-code">
   baseline_output_value: 0.7774810791015625
   instance_output_value: 0.09333677589893341
   feature_attributions: {'dense_input': [-0.1632179390639067, 0.0, -0.2642899513244629, 0.0, 0.174240517243743, 0.0, 0.0, -0.5113637685775757, 0.001784586161375046, 0.1180541321635246, -0.03459173887968063, -0.004760145395994187]}
   approximation_error: 0.007384564012290227
   output_name: dense_3</pre> <p class="calibre3">The fields in the response can be interpreted <span>as follows:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="source-inline">baseline_output_value</strong>: This is the model’s output value for the baseline instance. A baseline is a reference point (such as an average or neutral instance) against which the prediction for our instance of interest is compared. The difference <a id="_idIndexMarker1346" class="calibre6 pcalibre pcalibre1"/>in the model’s output between the instance of interest <a id="_idIndexMarker1347" class="calibre6 pcalibre pcalibre1"/>and the baseline helps us understand the contributions of <span>each feature.</span></li>
<li class="calibre8"><strong class="source-inline">instance_output_value</strong>: This is the model’s output value for the instance we passed in for explanation. In the context of a binary classifier, this can be interpreted as the probability of the instance belonging to the <span>positive class.</span></li>
<li class="calibre8"><span><strong class="source-inline">feature_attributions_dict</strong></span><span>:</span><ul class="calibre70"><li class="calibre8"><strong class="source-inline">'dense_input'</strong>: This is the name of the input tensor to <span>the model.</span></li><li class="calibre8">The list of numbers represents the importance or attribution of each corresponding feature in the input for the given prediction. The length of this list matches the number of features in our <span>model’s input:</span><ul class="calibre70"><li class="calibre8">Each number represents the marginal contribution of that feature toward the model’s prediction for the specific instance we’re explaining, relative to the baseline. In other words, how much did this feature move the prediction from the <span>average/baseline prediction?</span></li><li class="calibre8">Positive values indicate that the feature pushed the model’s output in the positive class’s direction. For binary classification, this usually means it made the model more confident in classifying the instance as the <span>positive class.</span></li><li class="calibre8">Negative values indicate that the feature pushed the model’s output in the negative <span>class’s direction.</span></li><li class="calibre8">Zero or close to zero suggests that the feature didn’t have a significant impact on the prediction for this <span>particular instance.</span></li></ul></li></ul></li>
<li class="calibre8"><strong class="source-inline">approximation_error</strong>: This is the error in the approximation used to compute attribution values. Explanation methods often use approximations to compute attributions. The approximation error gives an idea of the confidence we can have in the attribution values (a smaller error typically indicates more <span>reliable attributions).</span></li>
<li class="calibre8"><strong class="source-inline">output_name</strong>: This is the name of the model’s <span>output tensor.</span></li>
</ul>
<p class="calibre3">Congratulations! You have successfully retrieved an explanation for an input sent to a model hosted in Vertex AI. To dive into more detail on Vertex Explainable AI, you can reference its documentation at the following <span>link: </span><a href="https://cloud.google.com/vertex-ai/docs/explainable-ai/overview" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/vertex-ai/docs/explainable-ai/overview</span></a><span>.</span></p>
<p class="calibre3">So far, we’ve talked about getting and using explanations to understand our models. What if<a id="_idIndexMarker1348" class="calibre6 pcalibre pcalibre1"/> we<a id="_idIndexMarker1349" class="calibre6 pcalibre pcalibre1"/> found that some of our model’s predictions were unfair? What kinds of actions could we take to counteract that? One type of explanation mechanism we can use for this purpose is <strong class="bold">counterfactual explanations</strong>, which we <span>explore next.</span></p>
<h3 class="calibre11">Counterfactual explanations</h3>
<p class="calibre3">Counterfactual <a id="_idIndexMarker1350" class="calibre6 pcalibre pcalibre1"/>explanations<a id="_idIndexMarker1351" class="calibre6 pcalibre pcalibre1"/> revolve around the question, “What would need to change in my input data to alter the decision of a predictive model?” They describe a hypothetical alternative to an observed outcome that would have occurred if certain conditions were met. This could be a minimal change in input features that would change the prediction to a <span>specified output.</span></p>
<p class="calibre3">Going back to our loan approval as an example, suppose an applicant, John, is denied a loan based on features such as his income, credit score, and employment history. A counterfactual explanation might tell John: “If your income was $10,000 higher, you would have been approved for <span>the loan.”</span></p>
<p class="calibre3">Counterfactual explanations are important for many reasons. They help individuals affected by a model’s prediction to understand why a decision was made. They help data scientists understand how to augment their models based on various criteria, and they are also important for regulatory <span>compliance</span><span> reasons.</span></p>
<p class="calibre3">To find a counterfactual, we need to define a distance measure in the feature space. The goal is often to find the counterfactual instance that’s closest to the original instance but results in a different prediction. This is often framed as an optimization problem, where the objective is to minimize the distance between the original instance and its counterfactual, while still resulting in a <span>different prediction.</span></p>
<p class="calibre3">Bear in mind that for some models, especially <strong class="bold">deep NNs</strong> (<strong class="bold">DNNs</strong>), finding counterfactuals<a id="_idIndexMarker1352" class="calibre6 pcalibre pcalibre1"/> can be computationally challenging. It’s also important to note that counterfactuals might suggest changes that are impossible or very hard to achieve in real life, so their real-world feasibility needs to be evaluated. In the Jupyter notebook that accompanies this <a id="_idIndexMarker1353" class="calibre6 pcalibre pcalibre1"/>chapter, we <a id="_idIndexMarker1354" class="calibre6 pcalibre pcalibre1"/>perform some simple counterfactual processing. Bear in mind that counterfactuals are a highly complex topic in a quickly <span>evolving field.</span></p>
<p class="calibre3">Next, let’s look at some additional mechanisms for <span>reducing bias.</span></p>
<h2 id="_idParaDest-260" class="calibre9"><a id="_idTextAnchor322" class="calibre6 pcalibre pcalibre1"/>Reducing bias and enhancing fairness</h2>
<p class="calibre3">In this section, we <a id="_idIndexMarker1355" class="calibre6 pcalibre pcalibre1"/>discuss proactive steps we can use to<a id="_idIndexMarker1356" class="calibre6 pcalibre pcalibre1"/> reduce bias and enhance fairness in our datasets and ML models in each phase of the model development <span>life cycle.</span></p>
<h3 class="calibre11">Starting with the data</h3>
<p class="calibre3">As with anything<a id="_idIndexMarker1357" class="calibre6 pcalibre pcalibre1"/> in the field of data science, the data is usually a good place to start. If available, we should gather more data and ensure that the data is as balanced as possible. During data preprocessing, we can also mitigate bias by using resampling techniques to adjust the representation of underrepresented groups in the training data, either by oversampling minority groups or undersampling majority groups. During feature engineering, we can create or modify features to reduce their potential <span>discriminatory effect.</span></p>
<h3 class="calibre11">During model training</h3>
<p class="calibre3">During the model <a id="_idIndexMarker1358" class="calibre6 pcalibre pcalibre1"/>training process, we could <a id="_idIndexMarker1359" class="calibre6 pcalibre pcalibre1"/>introduce <strong class="bold">fairness constraints</strong> to ensure equal opportunity by making sure both protected and non-protected groups have equal positive and negative rates, for example. There are numerous fairness metrics and constraints for this purpose, such as <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Demographic parity</strong> or <strong class="bold">statistical parity</strong>, which requires that the probability of a positive outcome should be the same across the <span>different groups</span></li>
<li class="calibre8"><strong class="bold">Equal opportunity</strong>, which mandates equality in true positive rates across <span>different groups</span></li>
<li class="calibre8"><strong class="bold">Equalized odds</strong>, which extends equal opportunity by requiring both true positive rates and false positive rates to be equal <span>across groups</span></li>
<li class="calibre8"><strong class="bold">Treatment equality</strong>, which requires the ratio of false negatives to false positives to be equal <span>across groups</span></li>
</ul>
<p class="calibre3">Fairness constraints can be implemented as a type of regularization during model training. There are even some fairness-aware algorithms we could use that are explicitly designed to handle fairness concerns, such <a id="_idIndexMarker1360" class="calibre6 pcalibre pcalibre1"/>as <span><strong class="bold">Fair k-Means</strong></span><span>.</span></p>
<h3 class="calibre11">Postprocessing</h3>
<p class="calibre3">There are also<a id="_idIndexMarker1361" class="calibre6 pcalibre pcalibre1"/> some steps we can take during postprocessing, such as adjusting decision thresholds for different groups to ensure fairness metrics such as equal opportunity or demographic parity, and we could also adjust model predictions to ensure they are fair <span>across groups.</span></p>
<p class="calibre3">It is, of course, also important that we continuously monitor for fairness concerns in real-world predictions and retrain models as necessary. In critical decision-making scenarios, we could consider having a human in the loop to audit or override <span>model decisions.</span></p>
<p class="calibre3">Note that fairness enhancement methods might lead to a trade-off with model accuracy. The challenge is often to find a balance between fairness and accuracy that’s acceptable for the given application. It’s also crucial to understand which fairness metric is most relevant to your specific problem, as different fairness metrics can sometimes be in conflict with <span>each other.</span></p>
<p class="calibre3">To wrap up this section on explainability and fairness, let’s take a brief look at some other libraries we<a id="_idIndexMarker1362" class="calibre6 pcalibre pcalibre1"/> can use for assessing and implementing <span>these concepts.</span></p>
<h2 id="_idParaDest-261" class="calibre9"><a id="_idTextAnchor323" class="calibre6 pcalibre pcalibre1"/>Additional libraries</h2>
<p class="calibre3">Fortunately, there is an<a id="_idIndexMarker1363" class="calibre6 pcalibre pcalibre1"/> ever-growing list of libraries being developed for the purpose of assessing and promoting explainability and fairness. This section describes examples of <span>such libraries.</span></p>
<h3 class="calibre11">What-if Tool</h3>
<p class="calibre3">The <strong class="bold">What-if Tool</strong> (<strong class="bold">WIT</strong>) was <a id="_idIndexMarker1364" class="calibre6 pcalibre pcalibre1"/>originally <a id="_idIndexMarker1365" class="calibre6 pcalibre pcalibre1"/>developed by Google and is an early explainability tool with a visual interface that allows the inspection of a model’s predictions, comparison across different models, and examination of potential bias. It is relatively easy to use and does not require much coding, and it includes support for many of the concepts we discussed in this chapter, such as counterfactuals <span>and PDPs.</span></p>
<h3 class="calibre11">AI Fairness 360</h3>
<p class="calibre3">IBM’s <strong class="bold">AI Fairness 360</strong> (<strong class="bold">AIF360</strong>) is an open source library that includes a set of fairness<a id="_idIndexMarker1366" class="calibre6 pcalibre pcalibre1"/> metrics<a id="_idIndexMarker1367" class="calibre6 pcalibre pcalibre1"/> for datasets and models, and algorithms for mitigating bias. It can be used to provide detailed explanations to understand fairness metrics and their implications in a given context, and it enables users to visually explore bias in their datasets and models. It can also help in identifying if a trained model is producing biased outcomes and provides some tools to help mitigate bias in each phase of the model development life cycle, such <a id="_idIndexMarker1368" class="calibre6 pcalibre pcalibre1"/>as preprocessing, training, <span>and postprocessing.</span></p>
<h3 class="calibre11">EthicalML/XAI</h3>
<p class="calibre3">This is an open source<a id="_idIndexMarker1369" class="calibre6 pcalibre pcalibre1"/> Python library designed to <a id="_idIndexMarker1370" class="calibre6 pcalibre pcalibre1"/>support interpretable ML and responsible AI. It includes tools for preprocessing, model interpretability, bias detection, and visualization, and it supports concepts we discussed in this chapter, such as feature importance, Shapley values, LIME, <span>and DIA.</span></p>
<h3 class="calibre11">Fairlearn</h3>
<p class="calibre3">Fairlearn is another <a id="_idIndexMarker1371" class="calibre6 pcalibre pcalibre1"/>open source Python-based project <a id="_idIndexMarker1372" class="calibre6 pcalibre pcalibre1"/>that aims to help data scientists improve the fairness of AI systems. It includes algorithms for mitigating unfairness in classification and regression models, and fairness metrics for comparison. Its primary goal is to help ML practitioners reduce unfair disparities in predictions by understanding metrics and algorithms, and it provides an interactive UI experience for model assessment and comparison, which includes fairness metrics and an assessment dashboard. It again supports mitigation techniques in various phases of the ML model development life cycle, such as preprocessing, training, <span>and postprocessing.</span></p>
<p class="calibre3">There are many more explainability and fairness libraries in addition to the ones mentioned here, but these ones are particularly popular libraries that are currently available in the industry. Google Cloud has recently launched more specific model evaluation mechanisms and metrics for fairness. At the time of writing this in October 2023, these mechanisms are still in preview mode and are not yet generally available. You can find out more about these features in the documentation at the following <span>link: </span><a href="https://cloud.google.com/vertex-ai/docs/evaluation/intro-evaluation-fairness" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/vertex-ai/docs/evaluation/intro-evaluation-fairness</span></a></p>
<p class="callout-heading">A note on generative AI</p>
<p class="callout"><em class="italic">Chapters 14</em> through <em class="italic">17</em> in this book are dedicated<a id="_idIndexMarker1373" class="calibre6 pcalibre pcalibre1"/> to <strong class="bold">generative AI</strong> (<strong class="bold">GenAI</strong>), which is a relatively new subset of AI/ML. In those chapters, we will explore the concepts <a id="_idIndexMarker1374" class="calibre6 pcalibre pcalibre1"/>of <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) and how they differ from other types of ML models that we’ve already covered in this book. LLMs are typically trained on extremely large datasets and therefore acquire vast amounts of knowledge that can be applied to many different kinds of use cases. In those later chapters, we will learn how LLMs can be used as auto-raters to open up new kinds of evaluation techniques for ML models, including specific evaluations focusing on bias, explainability, <span>and fairness.</span></p>
<p class="calibre3">A final topic for us <a id="_idIndexMarker1375" class="calibre6 pcalibre pcalibre1"/>to explore in this chapter is the <a id="_idIndexMarker1376" class="calibre6 pcalibre pcalibre1"/>concept of lineage tracking. In the next section, we delve into this topic in detail and assess its importance in the context of explainability <span>and fairness.</span></p>
<h1 id="_idParaDest-262" class="calibre5"><a id="_idTextAnchor324" class="calibre6 pcalibre pcalibre1"/>The importance of lineage tracking in ML model development</h1>
<p class="calibre3">We’ve touched<a id="_idIndexMarker1377" class="calibre6 pcalibre pcalibre1"/> on the concept of lineage<a id="_idIndexMarker1378" class="calibre6 pcalibre pcalibre1"/> tracking in earlier chapters, and now we will explore it in more detail. When we talk about lineage tracking, we’re referring to tracking all of the steps and artifacts that were used to create a given ML model. This includes items such as <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8">The <span>source datasets</span></li>
<li class="calibre8">All transformations that were performed on <span>those datasets</span></li>
<li class="calibre8">All intermediate datasets that <span>were created</span></li>
<li class="calibre8">Which algorithm was used to train a model on the <span>resulting data</span></li>
<li class="calibre8">Which hyperparameters and values were used during <span>model training</span></li>
<li class="calibre8">Which platform and tools were used in <span>the training</span></li>
<li class="calibre8">If a hyperparameter tuning job was used, details of <span>that job</span></li>
<li class="calibre8">Details of any evaluation steps performed on the <span>resulting model</span></li>
<li class="calibre8">If the model is being served for online inference, details of the endpoint at which the model <span>is hosted</span></li>
</ul>
<p class="calibre3">The preceding list is not exhaustive. We generally want to track every step that was used to create a model and all of the inputs and outputs in <span>each step.</span></p>
<p class="calibre3">Why do we need to do this? Lineage tracking is important for many reasons. It complements the concept of explainability. While lineage tracking by itself will not necessarily explain why a model behaves in a particular manner, it is certainly important for researchers to understand how the model was created. It’s also important for reproducibility and collaboration. We’ve talked about the complexity some companies encounter when they need to manage thousands of ML models created by many different teams. If a model is behaving problematically, understanding its lineage will help in troubleshooting. If one team wants to build on the work another team has already performed, such as a model they have trained or datasets they have created, understanding the lineage of those artifacts will help the consuming team to be more productive in those endeavors. Also, in order to continually enhance a model’s performance, we need to know how that model was created. Lineage is also important, and sometimes required, for governance and <span>compliance reasons.</span></p>
<p class="calibre3">Fortunately, Google Cloud provides tools that help us to track lineage. For example, Dataplex can be used to track data lineage, and the Vertex ML Metadata service can help us track all steps and artifacts in our ML model development life cycle. Next, we will take a look <a id="_idIndexMarker1379" class="calibre6 pcalibre pcalibre1"/>at<a id="_idIndexMarker1380" class="calibre6 pcalibre pcalibre1"/> Vertex ML Metadata in more detail; let’s first start with some terminology used by the Vertex ML <span>Metadata service.</span></p>
<h2 id="_idParaDest-263" class="calibre9"><a id="_idTextAnchor325" class="calibre6 pcalibre pcalibre1"/>ML metadata service terminology</h2>
<p class="calibre3"><strong class="bold">Executions</strong> represent<a id="_idIndexMarker1381" class="calibre6 pcalibre pcalibre1"/> the steps or operations in an ML workflow, such as data preprocessing, model training, or evaluation. <strong class="bold">Artifacts</strong> represent the inputs and outputs of each step, such as datasets, models, or evaluation metrics. <strong class="bold">Events</strong> represent the relationships between executions and artifacts, such as “Artifact X was produced by Execution Y” or “Artifact X was used as an input by Execution Y.” Events help us to establish lineage data by associating artifacts and executions with each other. <strong class="bold">Contexts</strong> represent logical groupings that bundle related artifacts and executions together. An example of a context would be a specific pipeline run or a <span>model version.</span></p>
<p class="calibre3">Collectively, all of the aforementioned resources are referred to as <strong class="bold">metadata resources</strong>, and they are described by a <strong class="bold">MetadataSchema</strong>, which describes the schema for particular types of metadata resources. In addition to the predefined metadata resources, we can also store custom metadata in the Vertex ML Metadata service. All tracked metadata is stored<a id="_idIndexMarker1382" class="calibre6 pcalibre pcalibre1"/> in a <strong class="bold">MetadataStore</strong>, and all of this information can be used to <a id="_idIndexMarker1383" class="calibre6 pcalibre pcalibre1"/>create a <strong class="bold">Lineage Graph</strong>, which is a visual representation that connects artifacts, executions, and contexts, and shows the relationships and flow <span>between them.</span></p>
<p class="calibre3">Note that, as with most Google Cloud resources, access to the metadata resources can be controlled using Google <a id="_idIndexMarker1384" class="calibre6 pcalibre pcalibre1"/>Cloud <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>), which is important for security and <span>compliance reasons.</span></p>
<p class="calibre3">Now that we’ve <a id="_idIndexMarker1385" class="calibre6 pcalibre pcalibre1"/>covered the main terminology and concepts, let’s start reviewing some metadata in <span>Vertex AI.</span></p>
<h2 id="_idParaDest-264" class="calibre9"><a id="_idTextAnchor326" class="calibre6 pcalibre pcalibre1"/>Lineage tracking in Vertex AI</h2>
<p class="calibre3">To explore the<a id="_idIndexMarker1386" class="calibre6 pcalibre pcalibre1"/> lineage tracking features in Vertex AI, we will use the <a id="_idIndexMarker1387" class="calibre6 pcalibre pcalibre1"/>MLOps pipeline we built in <a href="B18143_11.xhtml#_idTextAnchor288" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 11</em></span></a> as <span>an example.</span></p>
<p class="calibre3">In the Google Cloud console, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">Navigate to <strong class="bold">Vertex AI</strong> &gt; <span><strong class="bold">Pipelines</strong></span><span>.</span></li>
<li class="calibre8">Click on the name of the pipeline run that was created in <a href="B18143_11.xhtml#_idTextAnchor288" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 11</em></span></a> (it should be the most recent pipeline run unless you have run other pipelines in this Google Cloud project in <span>the meantime).</span></li>
<li class="calibre8">You’ll see the execution graph for the <span>pipeline run.</span></li>
<li class="calibre8">At the top of the screen, click the toggle button to the left of the words <strong class="bold">Expand Artifacts</strong> (see <span><em class="italic">Figure 12</em></span><em class="italic">.7</em> <span>for reference):</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer167">
<img alt="Figure 12.7: Expand Artifacts" src="image/B18143_12_7.jpg" class="calibre163"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 12.7: Expand Artifacts</p>
<ol class="calibre7">
<li value="5" class="calibre8">We can now start exploring the metadata related to each of the steps and artifacts in<a id="_idIndexMarker1388" class="calibre6 pcalibre pcalibre1"/> <span>our </span><span><a id="_idIndexMarker1389" class="calibre6 pcalibre pcalibre1"/></span><span>pipeline.</span></li>
<li class="calibre8">You’ll also notice that the <strong class="bold">Pipeline run analysis</strong> section on the right side of the screen contains lots of information about this pipeline run. The <strong class="bold">SUMMARY</strong> tab provides information about the pipeline run itself, including the parameters that were used as inputs. Those are the parameters we defined in our pipeline definition in <a href="B18143_11.xhtml#_idTextAnchor288" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 11</em></span></a><span>.</span></li>
<li class="calibre8">We can click on elements in the pipeline execution graph in order to see metadata related to that specific element. Let’s start right at the beginning. We want to know which dataset was used as an initial input to our pipeline. Considering that the first step in our pipeline is the data preprocessing step, and that step fetches the dataset, click on the preprocessing step, and its metadata will be shown on the<a id="_idIndexMarker1390" class="calibre6 pcalibre pcalibre1"/> right <a id="_idIndexMarker1391" class="calibre6 pcalibre pcalibre1"/>side of the screen, as depicted in <span><em class="italic">Figure 12</em></span><span><em class="italic">.8</em></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer168">
<img alt="Figure 12.8: Preprocessing step details" src="image/B18143_12_8.jpg" class="calibre164"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 12.8: Preprocessing step details</p>
<ol class="calibre7">
<li value="8" class="calibre8">The green arrow in <span><em class="italic">Figure 12</em></span><em class="italic">.8</em> is pointing to the <strong class="source-inline">source_dataset</strong> input parameter, which provides the path to the source dataset (the actual details are redacted in the image in order to obscure my <span>bucket name).</span></li>
<li class="calibre8">We can also see the <strong class="source-inline">preprocessed_data_path</strong> parameter value, which provides the path to the folder in which the preprocessing script will store the resulting processed data. If you scroll down (not shown in the screenshot), you will also see the <strong class="source-inline">main_python_file_uri</strong> parameter value, which provides the path to the PySpark script that we used in the preprocessing step in our pipeline. In fact, if we click on the <strong class="bold">VIEW JOB</strong> button, we can view the details of the actual Serverless Spark job that was used to execute our script in Google Cloud Dataproc, including its <span>execution logs.</span></li>
<li class="calibre8">Now we’ve <a id="_idIndexMarker1392" class="calibre6 pcalibre pcalibre1"/>successfully<a id="_idIndexMarker1393" class="calibre6 pcalibre pcalibre1"/> tracked our source dataset, the script and job that performed transformations on that dataset, and the resulting processed dataset that was used to train our model, let’s move to the next step in our pipeline, which is the model <span>training step.</span></li>
<li class="calibre8">Click on the <strong class="source-inline">custom-training-job</strong> step in our pipeline execution graph. In the information panel on the right, perhaps the most important parameter is the <strong class="source-inline">worker_pool_specs</strong> parameter. As depicted in <span><em class="italic">Figure 12</em></span><em class="italic">.9</em>, this parameter provides a lot of information about how our model is trained, such as the dataset that was used for training (which is the output of the previous, preprocessing step), the location at which the trained model artifacts are saved, the container image that was used to run our custom training code, the hyperparameter values used during training, as well as the machine type and number of machines that were used by the <span>training job:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer169">
<img alt="Figure 12.9: worker_pool_specs" src="image/B18143_12_9.jpg" class="calibre165"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 12.9: worker_pool_specs</p>
<ol class="calibre7">
<li value="12" class="calibre8">Again, if we click on the <strong class="bold">VIEW JOB</strong> button at the top of the screen, we can see the actual job that ran on the Vertex AI training service to train our model, as well as the <a id="_idIndexMarker1394" class="calibre6 pcalibre pcalibre1"/>execution <a id="_idIndexMarker1395" class="calibre6 pcalibre pcalibre1"/>logs for <span>that job.</span></li>
<li class="calibre8">Because we used a custom script to train our model and simply saved the artifacts in Google Cloud Storage, at this point in the pipeline, our model is referred to as an <strong class="bold">Unmanaged model</strong>. In order to start using the model more appropriately, we needed to register it in the Vertex AI Model Registry. We use an <strong class="source-inline">importer</strong> job to import our model artifact for <span>that purpose.</span></li>
<li class="calibre8">The <strong class="source-inline">model-upload</strong> step in the pipeline is what registers our model in the Vertex AI Model Registry. If you click on that step in the execution graph and review its metadata, in the <strong class="bold">Output Parameters</strong> section, you will see the URI for the resulting resource in the Vertex AI <span>Model Registry.</span></li>
<li class="calibre8">The remaining steps, <strong class="source-inline">endpoint-create</strong> and <strong class="source-inline">model-deploy</strong>, have similar formats. As their names suggest, the <strong class="source-inline">endpoint-create</strong> step creates an endpoint in the Vertex AI prediction service, and the <strong class="source-inline">model-deploy</strong> step deploys our model to that endpoint. Their output parameters will show the URIs for the resources created by <span>those steps.</span></li>
<li class="calibre8">I want to draw your attention to the <strong class="source-inline">endpoint</strong> and <strong class="source-inline">model</strong> artifacts in the pipeline. If you click on those, and click the <strong class="bold">View Lineage</strong> button that appears in the information panel on the right-hand side of the screen, it will take you directly to the console for the Vertex AI Metadata service and will show you another view of how the steps and artifacts relate to each other, as depicted in <span><em class="italic">Figure 12</em></span><em class="italic">.10</em>. Again, clicking <a id="_idIndexMarker1396" class="calibre6 pcalibre pcalibre1"/>on each element in the graph <a id="_idIndexMarker1397" class="calibre6 pcalibre pcalibre1"/>will display metadata for <span>that element:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer170">
<img alt="Figure 12.10: Lineage graph in Vertex AI Metadata service console" src="image/B18143_12_10.jpg" class="calibre166"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 12.10: Lineage graph in Vertex AI Metadata service console</p>
<p class="calibre3">In addition to getting metadata insights via the Google Cloud console, we can also use the Vertex AI SDK and API directly to query and manage metadata programmatically. For example, the following piece of code will list all artifacts in our Google <span>Cloud project:</span></p>
<pre class="source-code">
aiplatform.Artifact.list()</pre> <p class="calibre3">Similarly, the following lines will list all executions and contexts in our Google <span>Cloud project:</span></p>
<pre class="source-code">
aiplatform.Execution.list()
aiplatform.Context.list()</pre> <p class="calibre3">We have now successfully tracked every step and artifact that was used to create our model. Next, let’s explore the <strong class="bold">Experiments</strong> feature in Vertex AI that is closely related to <span>lineage tracking.</span></p>
<h3 class="calibre11">Vertex ML Experiments</h3>
<p class="calibre3">When we created <a id="_idIndexMarker1398" class="calibre6 pcalibre pcalibre1"/>our pipeline definition in <a href="B18143_11.xhtml#_idTextAnchor288" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 11</em></span></a>, we<a id="_idIndexMarker1399" class="calibre6 pcalibre pcalibre1"/> specified an experiment name with which to associate our pipeline runs. This provides another view of how to group the steps and artifacts related to our pipeline runs and the model versions they create. This feature can be useful for sharing and collaboration, as well as for comparing different model versions against each other. To view the experiment associated with our pipeline runs, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the Google Cloud console, navigate to <strong class="bold">Vertex AI</strong> &gt; <span><strong class="bold">Experiments</strong></span><span>.</span></li>
<li class="calibre8">Click on the name of the experiment that we specified in our MLOps <span>chapter (</span><span><strong class="source-inline">aiml-sa-mlops-experiment</strong></span><span>).</span></li>
<li class="calibre8">Click on the name of the most recent run, and explore the <strong class="bold">ARTIFACTS</strong> tab, as shown in <span><em class="italic">Figure 12</em></span><span><em class="italic">.11</em></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer171">
<img alt="Figure 12.11: Vertex AI Experiments – Artifacts view" src="image/B18143_12_11.jpg" class="calibre167"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 12.11: Vertex AI Experiments – Artifacts view</p>
<ol class="calibre7">
<li value="4" class="calibre8">Click on the links that are shown under each artifact ID to see those artifacts, as well as their metadata in the Vertex ML Metadata service (it will bring you back to the screens we already explored in the previous section; this is just another way to access the <span>same metadata).</span></li>
</ol>
<p class="calibre3">We’ve covered a lot <a id="_idIndexMarker1400" class="calibre6 pcalibre pcalibre1"/>of information in this chapter – let’s summarize <a id="_idIndexMarker1401" class="calibre6 pcalibre pcalibre1"/>what we’ve discussed before we move on to the <span>next chapter.</span></p>
<h1 id="_idParaDest-265" class="calibre5"><a id="_idTextAnchor327" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we explored the concepts of bias, explainability, fairness, and lineage. We started off by examining some of the common types of bias that can occur at various steps in the ML model development life cycle. This included sources of bias such as pre-existing bias, algorithmic bias, and collection or measurement bias, which further included sub-categories such as sampling bias, response bias, and observer bias. We talked about how to inspect for bias, using techniques such as data exploration <span>and DIA.</span></p>
<p class="calibre3">Next, we dived into the use of explainability techniques to understand how our models make their decisions at inference time and to assess their fairness, particularly with regard to understanding how the input features in our dataset could influence our models’ predictions. We used tools such as PDPs and SHAP for these purposes. We then looked at how to use Vertex AI to get explanations from models that were hosted on Vertex AI endpoints. Going beyond simply getting explanations, we then discussed how to proactively counteract bias by using counterfactual analysis to pose questions such as “What would need to change in my input data to alter the decision of a <span>predictive model?”</span></p>
<p class="calibre3">Finally, we covered the topic of lineage tracking and its importance in terms of explainability, as well as other factors such as collaboration, troubleshooting, and compliance. We walked through an ML pipeline that we had created in a previous chapter and looked at the metadata associated with each component of the pipeline, including all of the steps and artifacts that were used to create a <span>specific model.</span></p>
<p class="calibre3">While the chapters before this one focused on everything that’s required to build and run an ML model, this chapter focused on more advanced topics such as ensuring that our models are explainable and fair. The next chapter in this book continues in a similar vein. We are no longer just looking at the mechanics of how to build and deploy ML models but are now incorporating broader ethical and architectural considerations. In the next chapter, we dive further into the topics of governance, compliance, and architectural best practices in the model development <span>life cycle.</span></p>
</div>
</div></body></html>