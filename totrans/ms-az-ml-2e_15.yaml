- en: '*Chapter 12*: Distributed Machine Learning on Azure'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：Azure上的分布式机器学习'
- en: In the previous chapter, we learned about hyperparameter tuning through search
    and optimization, using HyperDrive as well as Automated Machine Learning as a
    special case of hyperparameter optimization, involving feature engineering, model
    selection, and model stacking. Automated Machine Learning is **machine learning
    as a service** (**MLaaS**), whereby the only input is your data, an ML task, and
    an error metric. It's hard to imagine running all experiments and parameter combinations
    for Automated Machine Learning on a single machine or a single CPU/GPUwe are looking
    into ways to speed up the training process through parallelization and distributed
    computing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了通过搜索和优化进行超参数调整，使用HyperDrive以及自动机器学习作为超参数优化的特例，涉及特征工程、模型选择和模型堆叠。自动机器学习是**机器学习即服务**（**MLaaS**），其中唯一的输入是您的数据、一个机器学习任务和一个错误度量。很难想象在单个机器或单个CPU/GPU上运行所有自动机器学习的实验和参数组合，我们正在寻找通过并行化和分布式计算加速训练过程的方法。
- en: In this chapter, we will look into distributed and parallel computing algorithms
    and frameworks for efficiently training ML models in parallel. The goal of this
    chapter is to build an environment in Azure where you can speed up the training
    process of classical ML and deep learning models by adding more machines to your
    training environment, thereby scaling out the cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨用于在并行中高效训练机器学习模型的分布式和并行计算算法和框架。本章的目标是在Azure中构建一个环境，通过向您的训练环境添加更多机器，从而扩展集群，加速经典机器学习和深度学习模型的训练过程。
- en: First, we will take a look at the different methods and fundamental building
    blocks for **distributed ML**. You will grasp the difference between training
    independent models in parallel, as done in HyperDrive and Automated Machine Learning,
    and training a single model ensemble on a large dataset in parallel by partitioning
    the training data. We will then look into distributed ML for single models and
    discover data-distributed and model-distributed training methods. Both methods
    are often used in real-world scenarios for speeding up or enabling the training
    of large deep neural networks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨**分布式机器学习**的不同方法和基本构建块。您将了解在HyperDrive和自动机器学习中并行训练独立模型的方法，以及通过划分训练数据并行训练单个模型集成在大型数据集上的方法。然后，我们将探讨针对单个模型的分布式机器学习，并发现数据分布式和模型分布式训练方法。这两种方法通常在现实场景中用于加速或实现大型深度神经网络的训练。
- en: After that, we will discover the most popular frameworks for distributed ML
    and how they can be used in Azure and in combination with Azure Machine Learning
    compute. The transition between execution engines, communication libraries, and
    functionality for distributed ML libraries is smooth but often hard to understand.
    However, after reading this chapter, you will understand the difference between
    running Apache Spark in Databricks with MLlib and using Horovod, Gloo, PyTorch,
    and TensorFlow parameter servers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将了解分布式机器学习中最流行的框架以及如何在Azure中使用它们，以及如何与Azure机器学习计算结合使用。分布式机器学习库之间的执行引擎、通信库和功能之间的过渡是平滑的，但往往难以理解。然而，在阅读本章后，您将了解在Databricks中使用MLlib运行Apache
    Spark和使用Horovod、Gloo、PyTorch和TensorFlow参数服务器之间的区别。
- en: In the final section, we will take a look at two practical examples of how to
    implement the functionality we'll be covering in Azure and integrate it with Azure
    Machine Learning compute.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们将探讨如何在Azure中实现我们将要介绍的功能，并将其与Azure机器学习计算集成。
- en: 'This chapter covers the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Exploring methods for distributed ML
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索分布式机器学习的方法
- en: Using distributed ML in Azure
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Azure中使用分布式机器学习
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use the following Python libraries and versions to
    create decision-tree-based ensemble classifiers:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下Python库和版本来创建基于决策树的集成分类器：
- en: '`azureml-core 1.34.0`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-core 1.34.0`'
- en: '`azureml-sdk 1.34.0`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-sdk 1.34.0`'
- en: '`horovod 0.23.0`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horovod 0.23.0`'
- en: '`tensorflow 2.6.0`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow 2.6.0`'
- en: '`pyspark 3.2.0`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pyspark 3.2.0`'
- en: '`numpy 1.19.5`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy 1.19.5`'
- en: '`pandas 1.3.2`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas 1.3.2`'
- en: '`scikit-learn 0.24.2`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn 0.24.2`'
- en: Similar to previous chapters, you can execute this code using either a local
    Python interpreter or a notebook environment hosted in Azure Machine Learning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，您可以使用本地Python解释器或托管在Azure机器学习中的笔记本环境执行此代码。
- en: All code examples in this chapter can be found in the GitHub repository for
    this book, found at [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter12](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter12).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有代码示例都可以在本书的GitHub仓库中找到，该仓库位于[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter12](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter12)。
- en: Exploring methods for distributed ML
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索分布式机器学习的方法
- en: 'The journey of implementing ML pipelines is very similar for a lot of users
    and is often similar to the steps described in the previous chapters. When users
    start switching from experimentation to real-world data or from small examples
    to larger models, they often experience a similar issue: training large parametric
    models on large amounts of data—especially DL models—takes a very long time. Sometimes,
    epochs last hours, and training takes days to converge.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多用户来说，实现机器学习流水线的旅程非常相似，通常与前面章节中描述的步骤相似。当用户开始从实验转向现实世界数据或从小示例转向更大模型时，他们经常会遇到类似的问题：在大量数据上训练大型参数模型——特别是深度学习（DL）模型——需要非常长的时间。有时，一个epoch需要数小时，训练需要数天才能收敛。
- en: Waiting hours or even days for a model to converge means precious time wasted
    for many engineers, as it makes it a lot harder to interactively tune the training
    process. Therefore, many ML engineers need to speed up their training process
    by leveraging various distributed computing techniques. The idea of distributed
    ML is as simple as speeding up a training process by adding more compute resources.
    In the best case, the training performance improves linearly by adding more machines
    to the training cluster (scaling out). In this section, we will take a look at
    the most common patterns of distributed ML and try to understand and reason about
    them. In the next section of this chapter, we will also apply them to some real-world
    examples.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 等待数小时甚至数天以等待模型收敛，对于许多工程师来说意味着宝贵时间的浪费，因为它使得交互式调整训练过程变得更加困难。因此，许多机器学习工程师需要通过利用各种分布式计算技术来加速他们的训练过程。分布式机器学习的理念就像通过添加更多计算资源来加速训练过程一样简单。在最佳情况下，通过向训练集群添加更多机器（扩展），训练性能可以线性提高。在本节中，我们将探讨分布式机器学习最常见的模式，并尝试理解和推理它们。在本章的下一节中，我们还将将这些模式应用于一些实际案例。
- en: Most modern ML pipelines use some of the techniques discussed in this chapter
    to speed up the training process once their data or models become larger. This
    is similar to the need for big data platforms—such as Spark, Hive, and so on—for
    data preprocessing, once the data gets large. Hence, while this chapter seems
    overly complex, we would recommend revisiting it whenever you are waiting for
    your model to converge or want to produce better results faster.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代机器学习（ML）流水线在数据或模型变得更大时，会使用本章讨论的一些技术来加速训练过程。这类似于大数据平台（如Spark、Hive等）在数据变得庞大时进行数据预处理的需求。因此，尽管本章看起来过于复杂，但我们建议在等待模型收敛或希望更快地产生更好结果时重新审视它。
- en: 'There are generally three patterns for leveraging distributed computing for
    ML, as presented here:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如此，利用分布式计算进行机器学习通常有三种模式，如下所示：
- en: Training independent models on small data in parallel
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行地在小数据上训练独立的模型
- en: Training copies of a model in parallel on different subsets of the data
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行地在数据的不同子集上训练模型的副本
- en: Training different parts of the same model in parallel
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行训练同一模型的各个部分
- en: Let's take a look at each of these methods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一查看这些方法。
- en: Training independent models on small data in parallel
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行地在小数据上训练独立的模型
- en: 'We will first look at the easiest example: training (small) independent models
    on a (small) dataset. A typical use case for this parallel training is performing
    a hyperparameter search or the optimization of a classic ML model or a small neural
    network. This is very similar to what we covered in [*Chapter 11*](B17928_11_ePub.xhtml#_idTextAnchor178),
    *Hyperparameter Tuning and Automated Machine Learning*. Even Automated Machine
    Learning—where multiple individual independent models are trained and compared—uses
    this approach under the hood. In parallel training, we aim to speed up the training
    of multiple independent models with different parameters by training these models
    in parallel.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先来看一个最简单的例子：在（小）数据集上训练（小）独立模型。这种并行训练的典型用例是进行超参数搜索或经典机器学习模型或小型神经网络的优化。这与我们在[*第11章*](B17928_11_ePub.xhtml#_idTextAnchor178)中讨论的内容非常相似，即*超参数调整和自动机器学习*。即使是自动机器学习——其中多个独立的模型被训练和比较——在底层也使用这种方法。在并行训练中，我们的目标是通过并行训练这些模型来加速具有不同参数的多个独立模型的训练。
- en: 'The following diagram shows this case, where instead of training the individual
    models in sequence on a single machine, we train them in parallel:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示显示了这种情况，其中我们不是在单台机器上按顺序训练单个模型，而是在并行训练它们：
- en: '![Figure 12.1 – Parallel processing ](img/B17928_12_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图12.1 – 并行处理](img/B17928_12_01.jpg)'
- en: Figure 12.1 – Parallel processing
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – 并行处理
- en: You can see that no communication or synchronization is required during the
    training process of the individual models. This means that we can train either
    on multiple CPUs/GPUs on the same machine or on multiple machines.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，在单个模型的训练过程中不需要通信或同步。这意味着我们可以在同一台机器上的多个CPU/GPU上训练，或者在不同的机器上训练。
- en: 'When using Azure Machine Learning for hyperparameter tuning, this parallelization
    is easy to achieve by configuring an Azure Machine Learning compute target with
    multiple nodes and selecting the number of concurrent runs through the `max_concurrent_runs`
    parameter of the HyperDrive configuration. In Azure Machine Learning HyperDrive,
    all it takes is to specify an estimator and `param_sampling`, and submit the HyperDrive
    configuration as an experiment in order to run the individual task in parallel,
    as shown here:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Azure Machine Learning进行超参数调整时，通过配置具有多个节点的Azure Machine Learning计算目标并选择HyperDrive配置的`max_concurrent_runs`参数中的并发运行数量，可以轻松实现这种并行化。在Azure
    Machine Learning HyperDrive中，只需指定一个估计器和`param_sampling`，然后将HyperDrive配置作为实验提交，就可以并行运行单个任务，如下所示：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here are some formulas to compute the value for `max_concurrent_runs` for HyperDrive
    or any other distributed computing setup:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些公式来计算HyperDrive或其他任何分布式计算设置中`max_concurrent_runs`的值：
- en: For CPU-based training, the maximal number of concurrent training runs is limited
    by the number of available CPUs and compute nodes. The available physical memory
    is also a limitation, but swapping to virtual memory allow us to consume more
    memory than is physically available.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于基于CPU的训练，最大并发训练运行次数受可用CPU和计算节点数量的限制。可用的物理内存也是一个限制因素，但通过交换到虚拟内存，我们可以消耗比物理可用更多的内存。
- en: For GPU-based training, the maximal number of concurrent training runs is limited
    by the number of available GPUs and compute nodes, as well as the amount of available
    GPU memory. Typically, one training run is pinned to one physical GPU, but through
    GPU virtualization we can also train multiple models on a single physical GPU
    if enough GPU memory is available.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于基于GPU的训练，最大并发训练运行次数受可用GPU和计算节点数量以及可用GPU内存量的限制。通常，一个训练运行被固定到一个物理GPU上，但通过GPU虚拟化，如果足够的GPU内存可用，我们也可以在单个物理GPU上训练多个模型。
- en: 'Here is a guide to how to estimate how much memory a single model will consume:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何估计单个模型将消耗多少内存的指南：
- en: '**Size of a single parameter**:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**单个参数的大小**：'
- en: 'Half-precision float: 16 bits (2 bytes).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半精度浮点数：16位（2字节）。
- en: 'Single-precision float: 32 bits (4 bytes)—this is often the default.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单精度浮点数：32位（4字节）——这通常是默认值。
- en: 'Double-precision float: 64 bits (8 bytes).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双精度浮点数：64位（8字节）。
- en: '**Number of parameters required for a model**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型所需的参数数量**：'
- en: 'Parametric model: Sum of all parameters'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数模型：所有参数的总和
- en: 'Non-parametric model: Number of representations (for example, decision trees)
    * number of a representation''s parameters'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非参数模型：表示数量（例如，决策树）* 表示的参数数量
- en: 'Then, you multiply additional factors, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您需要乘以额外的因素，如下所示：
- en: 'Models using backpropagation: overall memory * 2'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反向传播的模型：总体内存 * 2
- en: 'Models using batching: overall memory * batch size'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用批次的模型：总体内存 * 批次大小
- en: 'Models using (recurrent) states: memory per state * number of recurrent steps'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用（递归）状态的模型：每个状态的记忆 * 递归步骤数
- en: While this use case seems very similar, let's move on to the next use case where
    we are given a large dataset that cannot be copied onto every machine.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个用例看起来非常相似，但让我们继续到下一个用例，其中我们得到一个无法复制到每台机器上的大型数据集。
- en: Training a model ensemble on large datasets in parallel
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行在大数据集上训练模型集成
- en: The next thing we will discuss is a very common optimization within ML, particularly
    when training models on large datasets. In order to train models, we usually require
    a large amount of data that rarely all fits into the memory of a single machine.
    Therefore, it is often required to split the data into chunks and train multiple
    individual models on the different chunks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要讨论的是机器学习中的一个非常常见的优化，尤其是在在大数据集上训练模型时。为了训练模型，我们通常需要一个大量数据，这些数据很少全部适合单台机器的内存。因此，通常需要将数据分割成块，并在不同的块上训练多个单个模型。
- en: 'The following screenshot shows two ways of splitting data into smaller chunks—by
    splitting the rows horizontally (left) or by splitting the columns vertically
    (right):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了两种将数据分割成更小块的方法——通过水平分割行（左）或通过垂直分割列（右）：
- en: '![Figure 12.2 – Data split: horizontal (row-wise) versus vertical (column-wise)
    ](img/B17928_12_02.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2 – 数据分割：水平（行方向）与垂直（列方向）](img/B17928_12_02.jpg)'
- en: 'Figure 12.2 – Data split: horizontal (row-wise) versus vertical (column-wise)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – 数据分割：水平（行方向）与垂直（列方向）
- en: You could also mix both techniques to extract a subset from your training data.
    Whenever you are using tools from the big data domain—such as MapReduce, Hive,
    or Spark—partitioning your data will help you to speed up your training process
    or enable training over huge amounts of data in the first place.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以混合这两种技术从你的训练数据中提取子集。无论何时你使用大数据领域中的工具——如MapReduce、Hive或Spark——分割你的数据将有助于你加快训练过程或最初就允许在大量数据上训练。
- en: 'A good example of performing data-distributed training is to train a massive
    tree ensemble of completely separate decision-tree models, also called a random
    forest. By splitting the data into many thousands of randomized chunks, you can
    train one decision tree per chunk of data and combine all trained trees into a
    single ensemble model. Apache Hivemall is a library based on Hive and Spark that
    does exactly this on either of the two execution engines. Here is an example of
    training multiple XGBoost multi-class ensemble models on Hive using **Hive Query
    Language** (**HiveQL**) and Apache Hivemall:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 执行数据分布式训练的一个好例子是训练一个由完全独立的决策树模型组成的巨大树集成，也称为随机森林。通过将数据分割成成千上万的随机块，你可以为每个数据块训练一个决策树，并将所有训练好的树组合成一个单一集成模型。Apache
    Hivemall是一个基于Hive和Spark的库，在两个执行引擎中的任何一个上都可以做到这一点。以下是一个使用**Hive查询语言**（**HiveQL**）和Apache
    Hivemall在Hive上训练多个XGBoost多类集成模型的示例：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding function, we use the `cluster` keyword to randomly move rows
    of data to the reducers. This will partition the data horizontally and train an
    XGBoost model per partition on each reducer. By defining the number of reducers,
    we also define the number of models trained in parallel. The resulting models
    are stored in a table where each row defines the parameters of one model. In a
    prediction, we would simply combine all individual models and perform an average-voting
    criterion to retrieve the final result.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数中，我们使用`cluster`关键字随机移动数据行到reducer。这将水平分割数据并在每个reducer上为每个分区训练一个XGBoost模型。通过定义reducer的数量，我们也定义了并行训练的模型数量。生成的模型存储在一个表中，其中每一行定义了一个模型的参数。在预测中，我们只需简单地将所有单个模型组合起来，并执行平均投票标准以检索最终结果。
- en: 'Another example of this approach would be a standard Spark pipeline that trains
    multiple independent models on vertical and horizontal data partitions. When we''ve
    finished training the individual models, we can use an average-voting criterion
    during inference to find the optimal result for a prediction task. Here is a small
    example script for training multiple models on horizontally partitioned data in
    parallel using Python, PySpark, and scikit-learn:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的另一个例子是一个标准的Spark管道，它在垂直和水平数据分区上训练多个独立模型。当我们完成单个模型的训练后，我们可以在推理期间使用平均投票标准来找到预测任务的优化结果。以下是一个使用Python、PySpark和scikit-learn并行在水平分割数据上训练多个模型的示例脚本：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding function, we can now load almost any amount of data and repartition
    it such that each partition fits into the local memory of a single node. If we
    have 1 `collect()` method to return all trained models to the head node.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数中，我们现在可以加载几乎任何数量的数据，并重新分区，使得每个分区都能适应单个节点的本地内存。如果我们有1个`collect()`方法来将所有训练好的模型返回给头节点。
- en: We could have also decided to just store the models from each individual worker
    on disk or in a distributed filesystem, but it might be nicer to just combine
    the results on a single node. In this example, you see we have the freedom to
    choose either of the two methods because all models are independent of each other.
    This is not true for cases where the models are suddenly dependent on each other—for
    example, when minimizing a global gradient or splitting a single model over multiple
    machines, which are both common use cases when training DNNs in the same way.
    In this case, we need some new operators to steer the control flow of the data
    and gradients. Let's look into these operators in the following section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以选择将每个单独工作节点的模型存储在磁盘上或在分布式文件系统中，但将结果合并到单个节点上可能更好。在这个例子中，我们看到我们有选择这两种方法之一的自由，因为所有模型都是相互独立的。但这并不适用于模型突然相互依赖的情况——例如，当最小化全局梯度或在一个模型上分割多个机器时，这两种情况都是训练DNN时的常见用例。在这种情况下，我们需要一些新的操作符来引导数据和梯度的控制流。让我们在下一节中探讨这些操作符。
- en: Fundamental building blocks for distributed ML
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式机器学习的基本构建块
- en: As we saw in the previous example, we need some fundamental building blocks
    or operators to manage the data flow in a distributed system. We call these operators
    **collective algorithms**. These algorithms implement common synchronization and
    communication patterns for distributed computing and are required when training
    ML models. Before we jump into distributed training methods for DNNs, we will
    have a quick look at these patterns to understand the foundations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个示例所示，我们在分布式系统中管理数据流需要一些基本的构建块或操作符。我们把这些操作符称为**集体算法**。这些算法实现了分布式计算中的常见同步和通信模式，并且在训练机器学习模型时是必需的。在我们深入探讨深度神经网络（DNN）的分布式训练方法之前，我们将快速浏览这些模式以了解其基础。
- en: 'The most common communication patterns in distributed systems are listed here:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式系统中最常见的通信模式如下：
- en: One-to-one
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对一
- en: One-to-many (also called *broadcast* or *scatter* patterns)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对多（也称为*broadcast*或*scatter*模式）
- en: Many-to-one (also called *gather* or *reduce* patterns)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对一（也称为*gather*或*reduce*模式）
- en: Many-to-many (also called *all-gather* or *all-reduce* patterns)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对多（也称为*all-gather*或*all-reduce*模式）
- en: 'The following screenshot gives a great overview of these patterns and shows
    how the data flows between the individual actors of a system:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图提供了这些模式的一个很好的概述，并展示了数据如何在系统的各个个体之间流动：
- en: '![Figure 12.3 – Communication patterns in distributed systems ](img/B17928_12_03.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3 – 分布式系统中的通信模式](img/B17928_12_03.jpg)'
- en: Figure 12.3 – Communication patterns in distributed systems
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – 分布式系统中的通信模式
- en: We can immediately think back to the hyperparameter optimization technique of
    Bayesian optimization. First, we need to **broadcast** the training data from
    the master to all worker nodes. Then, we can choose parameter combinations from
    the parameter space on the master and broadcast those to the worker nodes as well.
    Finally, we perform training on the worker nodes, before then **gathering** all
    the model validation scores from the worker nodes on the master. By comparing
    the scores and applying Bayes' theorem, we can predict the next possible parameter
    combinations and repeat broadcasting them to the worker nodes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即回想起贝叶斯优化技术中的超参数优化技巧。首先，我们需要将训练数据从主节点**广播**到所有工作节点。然后，我们可以在主节点的参数空间中选择参数组合，并将这些广播到工作节点。最后，我们在工作节点上执行训练，然后在主节点上**收集**所有模型验证分数。通过比较分数并应用贝叶斯定理，我们可以预测下一个可能的参数组合，并将它们重复广播到工作节点。
- en: 'Did you notice something in the preceding algorithm? How can we know that all
    worker nodes finished the training process, and gather all scores from all worker
    nodes? To do this, we will use another building block called synchronization,
    or **barrier synchronization**. With barrier synchronization, we can schedule
    the execution of a task such that it needs to wait for all other distributed tasks
    to be finished. The following screenshot shows a good overview of the synchronization
    pattern in multiprocessors:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到了前面算法中的某个地方吗？我们如何知道所有工作节点已经完成了训练过程，并从所有工作节点收集所有分数？为了做到这一点，我们将使用另一个构建块，称为同步，或**屏障同步**。使用屏障同步，我们可以安排任务的执行，使其需要等待所有其他分布式任务完成。以下截图展示了多处理器中同步模式的良好概述：
- en: '![Figure 12.4 – Synchronization mechanism ](img/B17928_12_04.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.4 – 同步机制](img/B17928_12_04.jpg)'
- en: Figure 12.4 – Synchronization mechanism
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 – 同步机制
- en: As you can see, we implicitly used these algorithms already in the previous
    chapter, where they were hidden from us behind the term *optimization*. Now, we
    will use them explicitly by changing the optimizers in order to train a single
    model over multiple machines.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已经在上一章中隐式地使用了这些算法，当时它们被隐藏在*优化*这个术语背后。现在，我们将通过更改优化器来显式地使用它们，以便在多台机器上训练单个模型。
- en: As you might have already realized, these patterns are not new and are used
    by your operating system many times per second. However, in this case, we can
    take advantage of these patterns and apply them to the execution graph of a distributed
    training process, and through specialized hardware (for example, by connecting
    two GPUs together using **InfiniBand** (**IB**)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，这些模式并不新鲜，并且你的操作系统每秒会多次使用它们。然而，在这种情况下，我们可以利用这些模式并将它们应用于分布式训练过程的执行图，并通过专用硬件（例如，通过使用**InfiniBand**（**IB**）连接两个
    GPU）。
- en: In order to use this collective algorithm with a different level of hardware
    support (GPU support and vectorization), you need to select a communication backend.
    These backends are libraries that often run as a separate process and implement
    communication and synchronization patterns. Popular libraries for collective algorithms
    include **Gloo**, **Message Passing Interface** (**MPI**), and **NVIDIA Collective
    Communications Library** (**NCCL**).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用这个集体算法与不同级别的硬件支持（GPU 支持和向量化），你需要选择一个通信后端。这些后端是通常作为单独进程运行的库，并实现通信和同步模式。用于集体算法的流行库包括
    **Gloo**、**消息传递接口**（**MPI**）和 **NVIDIA 集体通信库**（**NCCL**）。
- en: Most DL frameworks, such as PyTorch or TensorFlow, provide their own higher-level
    abstractions on one of these communication backends—for example, PyTorch **Remote
    Procedure Call** (**RPC**) and a TensorFlow **parameter server (PS)**. Instead
    of using a different execution and communication framework, you could also choose
    a general-purpose framework for distributed computing, such as Spark.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习框架，如 PyTorch 或 TensorFlow，在这些通信后端之一上提供了自己的高级抽象——例如，PyTorch **远程过程调用**（**RPC**）和
    TensorFlow **参数服务器（PS**）。你不必使用不同的执行和通信框架，也可以选择一个通用的分布式计算框架，如 Spark。
- en: Important Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'The PyTorch documentation has an up-to-date guide on when to use which collective
    communication library: https://pytorch.org/docs/stable/distributed.html#which-backend-to-use.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 文档提供了一个关于何时使用哪个集体通信库的最新指南：https://pytorch.org/docs/stable/distributed.html#which-backend-to-use.
- en: As you can see, the list of possible choices is endless, and multiple combinations
    are possible. We haven't even talked about Horovod, a framework used to add distributed
    training to other DL frameworks through distributed optimizers. The good part
    is that most of these frameworks and libraries are provided in all Azure Machine
    Learning runtimes as well as being supported through the Azure ML SDK. This means
    you will often only specify the desired backend, supply your model to any specific
    framework, and let Azure Machine Learning handle the setup, initialization, and
    management of these tools. We will see this in action in the second half of this
    chapter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，可能的选择列表是无限的，并且有多种组合可能。我们甚至还没有谈到 Horovod，这是一个框架，用于通过分布式优化器将分布式训练添加到其他深度学习框架中。好事是，大多数这些框架和库都包含在所有
    Azure Machine Learning 运行时中，并且通过 Azure ML SDK 得到支持。这意味着你通常只需要指定所需的后端，将你的模型提供给任何特定框架，然后让
    Azure Machine Learning 处理这些工具的设置、初始化和管理。我们将在本章的后半部分看到这一点。
- en: Speeding up deep learning with data-parallel training
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用数据并行训练加速深度学习
- en: Another variation of distributed data-parallel training is very common in DL.
    In order to speed up the training of larger models, we can run multiple training
    iterations with different chunks of data on distributed copies of the same model.
    This is especially crucial when each training iteration takes a significant amount
    of time (for example, multiple seconds), which is a typical scenario for training
    large DNNs where we want to take advantage of multi-GPU environments.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习（DL）中，分布式数据并行训练的另一种变体非常常见。为了加快大型模型的训练速度，我们可以在同一模型的分布式副本上运行多个不同数据块的训练迭代。这在每次训练迭代需要显著时间（例如，数秒）的情况下尤为重要，这对于我们想要利用多GPU环境的训练大型深度神经网络（DNNs）是一个典型场景。
- en: 'Data-distributed training for DL is based on the idea of using a **distributed
    gradient descent** (**DGD**) algorithm, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的数据分布式训练基于使用**分布式梯度下降**（**DGD**）算法的思想，如下所示：
- en: Distribute a copy of the model to each node.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型的一个副本分发到每个节点。
- en: Distribute a chunk of data to each node.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据块分发给每个节点。
- en: Run a full pass through the network on each node and compute the gradient.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个节点上运行整个网络，并计算梯度。
- en: Collect all gradients on a single node and compute the average gradient.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在单个节点上收集所有梯度并计算平均梯度。
- en: Send the average gradient to all nodes.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将平均梯度发送到所有节点。
- en: Update all models using the average gradient.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用平均梯度更新所有模型。
- en: 'The following diagram shows this in action for multiple models, running the
    forward/backward pass individually and sending the gradient back to the parameter
    server:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了多个模型在行动中的情况，它们分别运行正向/反向传递，并将梯度发送回参数服务器：
- en: '![Figure 12.5 – Data-parallel training ](img/B17928_12_05.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图12.5 – 数据并行训练](img/B17928_12_05.jpg)'
- en: Figure 12.5 – Data-parallel training
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 – 数据并行训练
- en: As seen here, the server computes the average gradient, which is sent back to
    all other nodes. We can immediately see that, suddenly, communication is required
    between the worker nodes and a primary node (let's call it the *parameter server*),
    and that synchronization is required too while waiting for all models to finish
    computing the gradient.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如此看来，服务器计算平均梯度，并将其发送回所有其他节点。我们可以立即看到，突然之间，工作节点和主节点（让我们称其为*参数服务器*）之间需要通信，并且在等待所有模型完成梯度计算时也需要同步。
- en: A great example of this use case is speeding up the training process of DL models
    by parallelizing the backpropagation step and combining the gradients from each
    node to an overall gradient. TensorFlow currently supports this distribution mode
    using a so-called parameter server. The *Horovod* framework developed at Uber
    provides a handy abstraction for distributed optimizers and plugs into many available
    ML frameworks or distributed execution engines, such as TensorFlow, PyTorch, and
    Apache Spark. We will take a look at practical examples of using Horovod and Azure
    Machine Learning in the *Horovod – a distributed DL training framework* section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种用例的一个很好的例子是通过并行化反向传播步骤并将每个节点的梯度组合到整体梯度中来加速深度学习模型的训练过程。TensorFlow目前使用所谓的参数服务器支持这种分布模式。Uber开发的*Horovod*框架为分布式优化器提供了一个方便的抽象，并可以插入许多可用的机器学习框架或分布式执行引擎，如TensorFlow、PyTorch和Apache
    Spark。我们将在*Horovod – 分布式深度学习训练框架*部分中查看使用Horovod和Azure机器学习的实际示例。
- en: Training large models with model-parallel training
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型并行训练训练大型模型
- en: Lastly, another common use case in DL is to train models that are larger than
    the provided GPU memory of a single GPU. This approach is a bit trickier as it
    requires the model execution graph to be split among different GPUs or even different
    machines. While this is not a big problem in CPU-based execution and is often
    done in Spark, Hive, or TensorFlow, we also need to transfer the intermediate
    results between multiple GPU memories. In order to do this effectively, extra
    hardware and drivers such as **Infiniband** (GPU-to-GPU communication) and **GPUDirect**
    (efficient GPU memory access) are required.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，深度学习中的一个常见用例是训练比单个GPU提供的GPU内存更大的模型。这种方法有点复杂，因为它需要将模型执行图分割到不同的GPU或甚至不同的机器上。虽然这在基于CPU的执行中不是大问题，并且通常在Spark、Hive或TensorFlow中完成，但我们还需要在多个GPU内存之间传输中间结果。为了有效地做到这一点，需要额外的硬件和驱动程序，例如**Infiniband**（GPU到GPU通信）和**GPUDirect**（高效的GPU内存访问）。
- en: 'The following diagram displays the difference between computing multiple gradients
    in parallel (on the left) and computing a single forward pass of a distributed
    model (on the right):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了并行计算多个梯度（在左侧）和计算分布式模型的单个前向传播（在右侧）之间的差异：
- en: '![Figure 12.6 – Model-parallel training ](img/B17928_12_06.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图12.6 – 模型并行训练](img/B17928_12_06.jpg)'
- en: Figure 12.6 – Model-parallel training
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6 – 模型并行训练
- en: The latter is a lot more complicated as data has to be exchanged during forward
    and backward passes between multiple GPUs and/or multiple nodes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 后者要复杂得多，因为数据需要在多个GPU和/或多个节点之间的正向和反向传播过程中进行交换。
- en: 'In general, we choose between two scenarios: multi-GPU training on a single
    machine and multi-GPU training on multiple machines. As you might expect, the
    latter is a lot more difficult, as it requires communication between and the synchronization
    of multiple machines over a network.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在两种场景之间进行选择：单机多GPU训练和多机多GPU训练。正如你所预期的那样，后者要复杂得多，因为它需要在网络中多个机器之间进行通信和同步。
- en: 'In the following script, we create a simple model running distributed on two
    GPUs using PyTorch. Using `.to(''cuda:*'')` methods throughout the model, we define
    the GPU on which an operation should be performed. In addition, we also need to
    add the same annotation to the input data for these computations:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下脚本中，我们使用PyTorch创建了一个在两个GPU上运行的简单模型。在整个模型中使用`.to('cuda:*')`方法，我们定义了操作应该在哪个GPU上执行。此外，我们还需要将这些相同的注释添加到这些计算的数据输入中：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can see in the preceding code, we configure the network to compute the
    first fully connected layer on GPU `0` whereas the second fully connected layer
    is computed on GPU `1`. When configuring forward steps, we also need to configure
    the inputs to both layers accordingly.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们配置网络在GPU `0` 上计算第一个全连接层，而第二个全连接层则在GPU `1` 上计算。在配置前向步骤时，我们还需要相应地配置两个层的输入。
- en: 'Training the model using a built-in optimizer and loss function is not very
    different from non-distributed models. The only difference is that we also have
    to define the target GPU for the training labels so that the loss can be computed,
    as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内置优化器和损失函数训练模型与非分布式模型并没有太大区别。唯一的区别是我们还必须定义训练标签的目标GPU，以便计算损失，如下所示：
- en: '[PRE4]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, we have split individual layers to run on multiple GPUs, while
    the data between these layers needs to be transferred during forward and backward
    passes. We have to apply code changes to the model itself in order to specify
    which parts of the model should run on which GPU.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们将单个层分割以在多个GPU上运行，而这些层之间的数据需要在正向和反向传播过程中进行传输。我们必须对模型本身进行代码更改，以指定模型的哪些部分应该在哪个GPU上运行。
- en: Important Note
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Please note that we could also make this split dynamic, such that we split the
    model into *x* consecutive subgraphs that are executed on *x* GPUs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们也可以使这种分割动态化，将模型分割成*x*个连续的子图，这些子图在*x*个GPU上执行。
- en: It's interesting to note that many of the techniques discussed in this chapter
    can be combined. We could, for example, train one multi-GPU model per machine,
    while partitioning the data into chunks and computing multiple parts of the gradient
    on multiple machines—hence adopting a data-distributed model-parallel approach.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，本章讨论的许多技术可以结合起来使用。例如，我们可以在每台机器上训练一个多GPU模型，同时将数据分成块，并在多台机器上计算多个梯度的多个部分——因此采用数据分布式模型并行方法。
- en: In the next section, we will learn how to put these concepts into practice.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何将这些概念付诸实践。
- en: Using distributed ML in Azure
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Azure中使用分布式机器学习
- en: The *Exploring methods for distributed ML* section contained an overwhelming
    amount of different parallelization scenarios, various communication backends
    for collective algorithms, and code examples using different ML frameworks and
    even execution engines. The amount of choice when it comes to ML frameworks is
    quite large, and making an educated decision is not easy. This choice gets even
    more complicated as some frameworks are supported out of the box in Azure Machine
    Learning while others have to be installed, configured, and managed by the user.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: “探索分布式机器学习的方法”部分包含了大量的不同并行化场景、用于集体算法的各种通信后端以及使用不同机器学习框架甚至执行引擎的代码示例。在机器学习框架的选择上，可供选择的空间相当大，做出明智的决定并不容易。当一些框架在Azure机器学习上直接支持时，而其他框架则需要用户安装、配置和管理，这种选择变得更加复杂。
- en: In this section, we will go through the most common scenarios, learn how to
    choose the correct combination of frameworks, and implement a distributed ML pipeline
    in Azure.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨最常见的场景，学习如何选择正确的框架组合，并在Azure中实现分布式ML管道。
- en: 'In general, you have three choices for running distributed ML in Azure, as
    follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在Azure中运行分布式机器学习有三种选择，如下所示：
- en: The first obvious choice is using Azure Machine Learning, the notebook environment,
    the Azure Machine Learning SDK, and Azure Machine Learning compute clusters. This
    will be the easiest solution for many complex use cases. Huge datasets can be
    stored on Azure Blob Storage, and models can be trained as data-parallel and/or
    model-parallel models with different communication backends. Everything is managed
    for you by wrapping your training script with an estimator abstraction.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个明显的选择是使用Azure机器学习，笔记本环境，Azure机器学习SDK和Azure机器学习计算集群。这将是对许多复杂用例来说最简单的解决方案。大量数据可以存储在Azure
    Blob存储中，模型可以训练为数据并行和/或模型并行模型，使用不同的通信后端。所有这些都将通过将您的训练脚本包装在估计器抽象中来由您管理。
- en: The second choice is to use a different authoring and execution engine for your
    code instead of Azure Machine Learning notebooks and Azure Machine Learning compute
    clusters. A popular option is Azure Databricks with integrated interactive notebooks
    and Apache Spark as a distributed execution engine. Using Databricks, you can
    use the pre-built ML images and auto-scaling clusters, which provides a great
    environment for running distributed ML training.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种选择是使用不同的代码编写和执行引擎，而不是Azure机器学习笔记本和Azure机器学习计算集群。一个流行的选项是集成了交互式笔记本和Apache
    Spark作为分布式执行引擎的Azure Databricks。使用Databricks，您可以使用预构建的ML镜像和自动扩展集群，这为运行分布式ML训练提供了一个极佳的环境。
- en: The third choice is to build and roll out your own custom solution. To do so,
    you need to build a separate cluster with virtual machines or Kubernetes and orchestrate
    the setup, installation, and management of the infrastructure and code. While
    this is the most flexible solution, it is also—by far—the most complex and time-consuming
    to set up.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三种选择是构建并推出您自己的定制解决方案。为此，您需要构建一个包含虚拟机或Kubernetes的独立集群，并编排基础设施和代码的设置、安装和管理。虽然这是最灵活的解决方案，但它也是迄今为止设置最复杂、耗时最长的。
- en: For this book, we will first look into Horovod optimizers, Azure Databricks,
    and Apache Spark before diving deeper into Azure Machine Learning.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书，我们将在深入研究Azure机器学习之前，首先了解Horovod优化器、Azure Databricks和Apache Spark。
- en: Horovod – a distributed deep learning training framework
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Horovod – 一个分布式深度学习训练框架
- en: '**Horovod** is a framework for enabling **distributed DL** and was initially
    developed and made open source by Uber. It provides a unified way to support the
    distributed training of existing DL training code for the following supported
    frameworks—TensorFlow, Keras, PyTorch, and Apache MXNet. The design goal was to
    make the transition from single-node training to data-parallel training extremely
    simple for any existing project, and hence enable these models to train faster
    on multiple GPUs in a distributed environment.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**Horovod**是一个用于启用**分布式深度学习**的框架，最初由Uber开发和开源。它为以下支持的框架提供了统一的方式来支持现有深度学习训练代码的分布式训练——TensorFlow、Keras、PyTorch和Apache
    MXNet。设计目标是使任何现有项目从单节点训练到数据并行训练的过渡变得极其简单，从而使得这些模型能够在分布式环境中更快地使用多个GPU进行训练。'
- en: 'Horovod is an excellent choice as a drop-in replacement for optimizers in any
    of the supported frameworks for data-parallel training. It integrates nicely with
    the supported frameworks through initialization and update steps or update hooks,
    by simply abstracting the GPUs from the DL code. From a user''s perspective, only
    minimal code changes have to be done to support data-parallel training for your
    model. Let''s take a look at an example using Keras and implement the following
    steps:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Horovod是作为任何支持框架中优化器的即插即用替代品的绝佳选择，用于数据并行训练。它通过初始化和更新步骤或更新钩子与支持的框架很好地集成，通过简单地从深度学习代码中抽象GPU。从用户的角度来看，只需对代码进行最小更改即可支持模型的数据并行训练。让我们通过使用Keras的示例来查看以下步骤：
- en: Initialize Horovod.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化Horovod。
- en: Configure Keras to read GPU information from Horovod.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置Keras从Horovod读取GPU信息。
- en: Load a model and split training data.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个模型并分割训练数据。
- en: Wrap the Keras optimizer as a Horovod distributed optimizer.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Keras优化器包装为Horovod分布式优化器。
- en: Implement model training.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现模型训练。
- en: Execute the script using `horovodrun`.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`horovodrun`执行脚本。
- en: 'The detailed steps are listed here:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 详细步骤如下：
- en: 'The first step is the same for any script using Horovod—we first need to load
    `horovod` from the correct package and initialize it, as follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于任何使用Horovod的脚本，第一步都是相同的——我们首先需要从正确的包中加载`horovod`并初始化它，如下所示：
- en: '[PRE5]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we need to perform a custom setup step, which varies depending on the
    framework used. This step will set up the GPU configuration for the framework,
    and ensure that it can call the abstracted versions through Horovod. The code
    is illustrated in the following snippet:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要执行一个自定义设置步骤，这个步骤取决于所使用的框架。这一步将为框架设置GPU配置，并确保它可以通过Horovod调用抽象版本。以下代码片段展示了这一点：
- en: '[PRE6]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we can simply take our single-node, single-GPU Keras model and define
    all parameters and the training and validation data. There is nothing special
    required during this step, as we can see here:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以简单地使用我们的单节点、单GPU Keras模型，并定义所有参数以及训练和验证数据。在这个步骤中不需要任何特殊要求，正如我们在这里可以看到的：
- en: '[PRE7]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we arrive at the magical part, where we wrap the framework optimizer—in
    this case, Adadelta from Keras—as a Horovod distributed optimizer. For all subsequent
    code, we will simply use the distributed optimizer instead of the default one.
    We also need to adjust the learning rate to the number of used GPUs, as the resulting
    gradient will be averaged from the individual changes. This can be done using
    the following code:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们来到了神奇的部分，我们将框架优化器（在这种情况下，是Keras的Adadelta）包装成Horovod分布式优化器。对于所有后续代码，我们将简单地使用分布式优化器而不是默认的优化器。我们还需要调整学习率到使用的GPU数量，因为最终的梯度将是来自各个单独变化的平均值。这可以通过以下代码完成：
- en: '[PRE8]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The remaining part looks fairly simple. It involves compiling the model, fitting
    the model, and evaluating the model, just as with the single-node counterpart.
    It''s worth mentioning that we need to add a callback to initialize all gradients
    during the training process. The code is illustrated in the following snippet:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余的部分看起来相当简单。它包括编译模型、拟合模型和评估模型，就像单节点版本一样。值得注意的是，我们需要在训练过程中添加一个回调来初始化所有梯度。以下代码片段展示了这一点：
- en: '[PRE9]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: When looking at the preceding code, it's fair to say that Horovod is not over-promising
    on making it easy to extend your code for distributed execution using a data-parallel
    approach and distributed gradient computation. If you have looked into the native
    TensorFlow or PyTorch versions, you will have seen that this requires far fewer
    code changes and is a lot more readable and portable than a parameter server or
    RPC framework.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看前面的代码时，可以说Horovod并没有过分承诺使代码扩展到使用数据并行方法和分布式梯度计算进行分布式执行变得容易。如果你已经调查了原生的TensorFlow或PyTorch版本，你会看到这需要更少的代码更改，并且比参数服务器或RPC框架更易于阅读和移植。
- en: 'The Horovod framework uses a communication based on MPI to handle collective
    algorithms under the hood, and usually requires one running process per GPU per
    node. However, it can also run on top of the Gloo backend or a custom MPI backend
    through a configuration option. Here is a sample snippet of how to use the `horovodrun`
    command to start a training process on two machines, `server1` and `server2`,
    each using four separate GPUs:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Horovod框架在底层使用基于MPI的通信来处理集体算法，通常每个节点每个GPU需要一个运行进程。然而，它也可以通过配置选项在Gloo后端或自定义MPI后端上运行。以下是如何使用`horovodrun`命令在两台机器`server1`和`server2`上启动训练过程的示例片段，每台机器使用四个独立的GPU：
- en: '[PRE10]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Running and debugging Horovod on your own cluster can still be painful when
    you only want to speed up your training progress by scaling out your cluster.
    Therefore, Azure Machine Learning compute provides a wrapper that does all the
    heavy lifting for you, requiring only a training script with Horovod annotations.
    We will see this in the *Training models with Horovod on Azure Machine Learning*
    section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当你只想通过扩展你的集群来加速训练进度时，在自己的集群上运行和调试Horovod仍然可能很痛苦。因此，Azure Machine Learning计算提供了一个包装器，为你完成所有繁重的工作，只需要一个带有Horovod注释的训练脚本。我们将在“在Azure
    Machine Learning上使用Horovod训练模型”部分看到这一点。
- en: Model-parallel training can be combined with Horovod by using the model-parallel
    features of the underlying framework and using only one Horovod process per machine
    instead of per GPU. However, this is a custom configuration and is currently not
    supported in Azure Machine Learning.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行训练可以通过使用底层框架的模型并行功能，并且每个机器只使用一个Horovod进程而不是每个GPU来实现与Horovod的结合。然而，这是一个自定义配置，目前在Azure
    Machine Learning中尚不支持。
- en: Implementing the HorovodRunner API for a Spark job
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为Spark作业实现HorovodRunner API
- en: In many companies, ML is an additional data processing step on top of existing
    data pipelines. Therefore, if you have huge amounts of data and you are already
    managing Spark clusters or using Azure Databricks to process that data, it is
    easy to also add distributed training capabilities.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多公司中，机器学习是现有数据管道之上的一个附加数据处理步骤。因此，如果您有大量数据，并且您已经在管理Spark集群或使用Azure Databricks处理这些数据，那么添加分布式训练功能也很容易。
- en: As we have seen in the *Exploring methods for distributed ML* section of this
    chapter, we can simply train multiple models using parallelization or by partitioning
    the training data. However, we could also train DL models and benefit from distributed
    ML techniques to speed up the training process.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章的“探索分布式机器学习的方法”部分所看到的，我们可以简单地通过并行化或分割训练数据来训练多个模型。然而，我们也可以训练深度学习模型，并从分布式机器学习技术中受益，以加快训练过程。
- en: When using the Databricks ML runtime, you can leverage Horovod for Spark to
    distribute your training process. This functionality is available through the
    `HorovodRunner` API and is powered by Spark's barrier-mode execution engine to
    provide a stable communication backend for long-running jobs. Using `HorovodRunner`
    on the head node, it will send the training function to the workers and start
    the function using the MPI backend. This all happens under the hood within the
    Spark process.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Databricks ML运行时，您可以使用Horovod为Spark分发您的训练过程。此功能通过`HorovodRunner` API提供，并由Spark的barrier-mode执行引擎提供稳定的通信后端，为长时间运行的工作提供支持。在头节点上使用`HorovodRunner`，它将训练函数发送到工作节点，并使用MPI后端启动该函数。所有这些都在Spark进程的幕后发生。
- en: 'Again, this is one of the reasons why Horovod is quite easy to use, as it is
    literally just a drop-in replacement for your current optimizer. Imagine that
    you usually run your Keras model on Azure Databricks using the PySpark engine;
    however, you would like to add Horovod to speed up the training process by leveraging
    other machines in the cluster and splitting the gradient descent over multiple
    machines. In order to do so, you would have to add literally only two lines of
    code to the example from the previous section, as seen here:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这正是Horovod易于使用的原因之一，因为它实际上只是您当前优化器的直接替换。想象一下，您通常在Azure Databricks上使用PySpark引擎运行您的Keras模型；然而，您希望添加Horovod以利用集群中的其他机器并分割梯度下降到多台机器上，以加快训练过程。为此，您只需在上一节示例中添加两行代码即可，如下所示：
- en: '[PRE11]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code snippet, we observe that we only need to initialize `HorovodRunner()`
    with the number of worker nodes. Calling the `run()` method with the training
    function will automatically start the new workers and the MPI communication backend
    and will send the training code to the workers, executing the training in parallel.
    Therefore, you can now add data-parallel training to your long-running Spark ML
    jobs.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们可以观察到，我们只需要用工作节点数初始化`HorovodRunner()`。调用`run()`方法并传入训练函数将自动启动新的工作节点和MPI通信后端，并将训练代码发送到工作节点，并行执行训练。因此，您现在可以将数据并行训练添加到您的长时间运行的Spark
    ML作业中。
- en: Training models with Horovod on Azure Machine Learning
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Azure Machine Learning上使用Horovod训练模型
- en: One of the benefits of moving to a cloud service is that you can consume functionality
    as a service rather than managing infrastructure on your own. Good examples are
    managed databases, lambda functions, managed Kubernetes, or container instances,
    where choosing a managed service means that you can focus on your application
    code while the infrastructure is managed for you in the cloud.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 转向云服务的一个好处是，您可以以服务的形式消费功能，而不是自己管理基础设施。好的例子包括托管数据库、lambda函数、托管Kubernetes或容器实例，选择托管服务意味着您可以专注于您的应用程序代码，而基础设施则由云为您管理。
- en: The Azure Machine Learning service sits in a similar spot where you can consume
    many of the different functionalities through an SDK (such as model management,
    optimization, training, and deployments) so that you don't have to maintain an
    ML cluster infrastructure. This brings a huge benefit when it comes to speeding
    up DNNs through distributed ML. If you have stuck with Azure Machine Learning
    compute until now, then moving to data-parallel training is as difficult as adding
    a single parameter to your training configuration—for any of the various choices
    discussed in this chapter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning 服务位于一个类似的位置，你可以通过 SDK（例如模型管理、优化、训练和部署）使用许多不同的功能，这样你就不必维护
    ML 集群基础设施。当涉及到通过分布式 ML 加速 DNN 时，这带来了巨大的好处。如果你一直坚持使用 Azure Machine Learning 计算服务，那么迁移到数据并行训练就像在你的训练配置中添加单个参数一样简单——对于本章讨论的任何各种选择。
- en: Let's think about running the Keras training script in data-parallel mode using
    a Horovod optimizer in a distributed environment. You need to make sure all the
    correct versions of your tools are set up (from **Compute Unified Device Architecture**
    (**CUDA**) to **CUDA Deep Neural Network** (**cuDNN**), GPUDirect, MPI, Horovod,
    TensorFlow, and Keras) and play together nicely with your current operating system
    and hardware. Then, you need to distribute the training code to all machines,
    start the MPI process, and then call the script using Horovod and the relevant
    command-line argument on every machine in the cluster. And we haven't even talked
    about authentication, data access, or auto-scaling.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下在分布式环境中使用 Horovod 优化器以数据并行模式运行 Keras 训练脚本。你需要确保所有正确的工具版本都已设置好（从 **Compute
    Unified Device Architecture** （**CUDA**）到 **CUDA Deep Neural Network** （**cuDNN**），GPUDirect，MPI，Horovod，TensorFlow
    和 Keras），并且与你的当前操作系统和硬件良好地协同工作。然后，你需要将训练代码分发到所有机器上，启动 MPI 进程，然后使用 Horovod 和集群中每台机器的相关命令行参数调用脚本。而且我们还没有讨论认证、数据访问或自动扩展。
- en: 'With Azure Machine Learning, you get an ML environment that just works and
    will be kept up to date for you. Let''s take a look at the previous Horovod and
    Keras training script, which we stored in a `train.py` file. Now, similar to the
    previous chapters, we create an estimator to wrap the training call for the Azure
    Machine Learning SDK. To enable multi-GPU data-parallel training using Horovod
    and the MPI backend, we simply add the relevant parameters. The resulting script
    looks like this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Azure Machine Learning，你将获得一个即用型 ML 环境，它将为你保持最新。让我们看看之前的 Horovod 和 Keras
    训练脚本，我们将其存储在 `train.py` 文件中。现在，类似于之前的章节，我们创建一个估计器来封装 Azure Machine Learning SDK
    的训练调用。要使用 Horovod 和 MPI 后端启用多 GPU 数据并行训练，我们只需添加相关参数。生成的脚本看起来像这样：
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Using the `use_gpu` flag, we can enable GPU-specific machines and their corresponding
    images with precompiled binaries for our Azure Machine Learning compute cluster.
    Using `node_count` and `process_count_per_node`, we specify the level of concurrency
    for the data-parallel training, where `process_count_per_node` should correspond
    with the number of GPUs available per node. Finally, we set the `distributed_backend`
    parameter to `mpi` to enable the MPI communication backend for this estimator.
    Another possible option would be using `ps` to enable the TensorFlow `ParameterServer`
    backend.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `use_gpu` 标志，我们可以启用具有预编译二进制的 GPU 特定机器及其对应图像，用于我们的 Azure Machine Learning
    计算集群。使用 `node_count` 和 `process_count_per_node`，我们指定数据并行训练的并发级别，其中 `process_count_per_node`
    应与每个节点可用的 GPU 数量相对应。最后，我们将 `distributed_backend` 参数设置为 `mpi` 以启用此估计器的 MPI 通信后端。另一个可能的选项是使用
    `ps` 来启用 TensorFlow 的 `ParameterServer` 后端。
- en: 'Finally, to start up the job, we simply submit the experiment, which will automatically
    set up the MPI session on each node and call the training script with the relevant
    arguments for us. I don''t know how you feel about this, but for me, this is a
    really big step forward from the previous manual examples. The following line
    of code shows how you can submit the experiment:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了启动作业，我们只需提交实验，它将自动在每个节点上设置 MPI 会话，并使用相关参数调用训练脚本。我不知道你对此有何感想，但对我来说，这真的是从之前的手动示例中迈出的巨大一步。以下代码行显示了如何提交实验：
- en: '[PRE13]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Wrapping your training as part of an Azure Machine Learning estimator gives
    you the benefit of fine-tuning your training script configuration for multiple
    environments, be it multi-GPU data-parallel models for distributed gradient descent
    training or single-node instances for fast inference. By combining distributed
    DL with Azure Machine Learning compute auto-scaling clusters, you can get the
    most from the cloud by using pre-built managed services instead of manually fiddling
    with infrastructure and configurations.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的训练作为 Azure Machine Learning 估计器的一部分，您可以享受到为多个环境微调您的训练脚本配置的好处，无论是用于分布式梯度下降训练的多
    GPU 数据并行模型，还是用于快速推理的单节点实例。通过结合分布式深度学习与 Azure Machine Learning 计算自动缩放集群，您可以通过使用预构建的托管服务而不是手动调整基础设施和配置，从云中获得最大收益。
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Distributed ML is a great approach to scaling out your training infrastructure
    in order to gain speed in your training process. It is applied in many real-world
    scenarios and is very easy to use with Horovod and Azure Machine Learning.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式机器学习是一种很好的方法，可以扩展您的训练基础设施，以在训练过程中获得速度。它在许多实际场景中得到应用，并且与 Horovod 和 Azure Machine
    Learning 非常容易使用。
- en: Parallel execution is similar to hyperparameter searching, while distributed
    execution is similar to Bayesian optimization, which we discussed in detail in
    the previous chapter. Distributed executions need methods to perform communication
    (such as one-to-one, one-to-many, many-to-one, and many-to-many) and synchronization
    (such as barrier synchronization) efficiently. These so-called collective algorithms
    are provided by communication backends (MPI, Gloo, and NCCL) and allow efficient
    GPU-to-GPU communication.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 并行执行类似于超参数搜索，而分布式执行类似于我们在上一章中详细讨论的贝叶斯优化。分布式执行需要方法来高效地执行通信（如一对一、一对多、多对一和多对多）和同步（如屏障同步）。这些所谓的集体算法由通信后端（MPI、Gloo
    和 NCCL）提供，并允许高效的 GPU 到 GPU 通信。
- en: DL frameworks build higher-level abstractions on top of communication backends
    to perform model-parallel and data-parallel training. In data-parallel training,
    we partition the input data to compute multiple independent parts of the model
    on different machines and add up the results in a later step. A common technique
    in DL is distributed gradient descent, where each node performs gradient descent
    on a partition of the input batch, and a master collects all the separate gradients
    to compute the overall average gradient of the combined model. In model-parallel
    training, you distribute a single model over multiple machines. This is often
    the case when a model doesn't fit into the GPU memory of a single GPU.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习框架在通信后端之上构建了高级抽象，以执行模型并行和数据并行训练。在数据并行训练中，我们将输入数据分区，在不同的机器上计算模型的多个独立部分，并在后续步骤中汇总结果。深度学习中的一个常见技术是分布式梯度下降，其中每个节点在输入批次的分区上执行梯度下降，而主节点收集所有单独的梯度来计算组合模型的总体平均梯度。在模型并行训练中，您将单个模型分布到多个机器上。当模型不适合单个
    GPU 的 GPU 内存时，这种情况通常发生。
- en: Horovod is an abstraction on top of existing optimizers of other ML frameworks,
    such as TensorFlow, Keras, PyTorch, and Apache MXNet. It provides an easy-to-use
    interface to add data-distributed training to an existing model without many code
    changes. While you could run Horovod on a standalone cluster, the Azure Machine
    Learning service provides good integration by wrapping its functionality as an
    estimator object. You learned how to run Horovod on an Azure Machine Learning
    compute cluster to speed up your training process through distributed ML with
    a few lines of Horovod initialization and a wrapper over the current optimizer.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Horovod 是在 TensorFlow、Keras、PyTorch 和 Apache MXNet 等其他机器学习框架的现有优化器之上的一种抽象。它提供了一个易于使用的接口，可以在不进行许多代码更改的情况下向现有模型添加数据分布式训练。虽然您可以在独立集群上运行
    Horovod，但 Azure Machine Learning 服务通过将其功能封装为估计器对象提供了良好的集成。您学习了如何在 Azure Machine
    Learning 计算集群上运行 Horovod，通过几行 Horovod 初始化和当前优化器的包装来加速您的训练过程。
- en: In the next chapter, we will use all the knowledge from the previous chapters
    to train recommendation engines on Azure. Recommendation engines often build on
    top of other NLP feature extraction or classification models and hence combine
    many of the techniques we have learned about so far.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用前几章的所有知识，在 Azure 上训练推荐引擎。推荐引擎通常建立在其他 NLP 特征提取或分类模型之上，因此结合了我们迄今为止学到的许多技术。
