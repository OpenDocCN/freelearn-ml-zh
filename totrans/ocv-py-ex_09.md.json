["```py\nimport cv2\n\n# Compute the frame difference\ndef frame_diff(prev_frame, cur_frame, next_frame):\n    # Absolute difference between current frame and next frame\n    diff_frames1 = cv2.absdiff(next_frame, cur_frame)\n\n    # Absolute difference between current frame and # previous frame\n    diff_frames2 = cv2.absdiff(cur_frame, prev_frame)\n\n    # Return the result of bitwise 'AND' between the # above two resultant images\n    return cv2.bitwise_and(diff_frames1, diff_frames2)\n\n# Capture the frame from webcam\ndef get_frame(cap):\n    # Capture the frame\n    ret, frame = cap.read()\n\n    # Resize the image\n    frame = cv2.resize(frame, None, fx=scaling_factor,\n            fy=scaling_factor, interpolation=cv2.INTER_AREA)\n\n    # Return the grayscale image\n    return cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n\nif __name__=='__main__':\n    cap = cv2.VideoCapture(0)\n    scaling_factor = 0.5\n\n    prev_frame = get_frame(cap)\n    cur_frame = get_frame(cap)\n    next_frame = get_frame(cap)\n\n    # Iterate until the user presses the ESC key\n    while True:\n        # Display the result of frame differencing\n        cv2.imshow(\"Object Movement\", frame_diff(prev_frame, cur_frame, next_frame))\n\n        # Update the variables\n        prev_frame = cur_frame\n        cur_frame = next_frame\n        next_frame = get_frame(cap)\n\n        # Check if the user pressed ESC\n        key = cv2.waitKey(10)\n        if key == 27:\n            break\n\n    cv2.destroyAllWindows()\n```", "```py\nimport cv2\nimport numpy as np\n\n# Capture the input frame from webcam\ndef get_frame(cap, scaling_factor):\n    # Capture the frame from video capture object\n    ret, frame = cap.read()\n\n    # Resize the input frame\n    frame = cv2.resize(frame, None, fx=scaling_factor,\n            fy=scaling_factor, interpolation=cv2.INTER_AREA)\n\n    return frame\n\nif __name__=='__main__':\n    cap = cv2.VideoCapture(0)\n    scaling_factor = 0.5\n\n    # Iterate until the user presses ESC key\n    while True:\n        frame = get_frame(cap, scaling_factor)\n\n        # Convert the HSV colorspace\n        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n\n        # Define 'blue' range in HSV colorspace\n        lower = np.array([60,100,100])\n        upper = np.array([180,255,255])\n\n        # Threshold the HSV image to get only blue color\n        mask = cv2.inRange(hsv, lower, upper)\n\n        # Bitwise-AND mask and original image\n        res = cv2.bitwise_and(frame, frame, mask=mask)\n        res = cv2.medianBlur(res, 5)\n\n        cv2.imshow('Original image', frame)\n        cv2.imshow('Color Detector', res)\n\n        # Check if the user pressed ESC key\n        c = cv2.waitKey(5)\n        if c == 27:\n            break\n\n    cv2.destroyAllWindows()\n```", "```py\nimport sys\n\nimport cv2\nimport numpy as np\n\nclass ObjectTracker(object):\n    def __init__(self):\n        # Initialize the video capture object\n        # 0 -> indicates that frame should be captured\n        # from webcam\n        self.cap = cv2.VideoCapture(0)\n\n        # Capture the frame from the webcam\n        ret, self.frame = self.cap.read()\n\n        # Downsampling factor for the input frame\n        self.scaling_factor = 0.5\n        self.frame = cv2.resize(self.frame, None, fx=self.scaling_factor,\n                    fy=self.scaling_factor, interpolation=cv2.INTER_AREA)\n\n        cv2.namedWindow('Object Tracker')\n        cv2.setMouseCallback('Object Tracker', self.mouse_event)\n\n        self.selection = None\n        self.drag_start = None\n        self.tracking_state = 0\n\n    # Method to track mouse events\n    def mouse_event(self, event, x, y, flags, param):\n        x, y = np.int16([x, y])\n\n        # Detecting the mouse button down event\n        if event == cv2.EVENT_LBUTTONDOWN:\n            self.drag_start = (x, y)\n            self.tracking_state = 0\n\n        if self.drag_start:\n            if flags & cv2.EVENT_FLAG_LBUTTON:\n                h, w = self.frame.shape[:2]\n                xo, yo = self.drag_start\n                x0, y0 = np.maximum(0, np.minimum([xo, yo], [x, y]))\n                x1, y1 = np.minimum([w, h], np.maximum([xo, yo], [x, y]))\n                self.selection = None\n\n                if x1-x0 > 0 and y1-y0 > 0:\n                    self.selection = (x0, y0, x1, y1)\n\n            else:\n                self.drag_start = None\n                if self.selection is not None:\n                    self.tracking_state = 1\n\n    # Method to start tracking the object\n    def start_tracking(self):\n        # Iterate until the user presses the Esc key\n        while True:\n            # Capture the frame from webcam\n            ret, self.frame = self.cap.read()\n            # Resize the input frame\n            self.frame = cv2.resize(self.frame, None, fx=self.scaling_factor,\n                        fy=self.scaling_factor, interpolation=cv2.INTER_AREA)\n\n            vis = self.frame.copy()\n\n            # Convert to HSV colorspace\n            hsv = cv2.cvtColor(self.frame, cv2.COLOR_BGR2HSV)\n\n            # Create the mask based on predefined thresholds.\n            mask = cv2.inRange(hsv, np.array((0., 60., 32.)),\n                        np.array((180., 255., 255.)))\n\n            if self.selection:\n                x0, y0, x1, y1 = self.selection\n                self.track_window = (x0, y0, x1-x0, y1-y0)\n                hsv_roi = hsv[y0:y1, x0:x1]\n                mask_roi = mask[y0:y1, x0:x1]\n\n                # Compute the histogram\n                hist = cv2.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )\n\n                # Normalize and reshape the histogram\n                cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX);\n                self.hist = hist.reshape(-1)\n\n                vis_roi = vis[y0:y1, x0:x1]\n                cv2.bitwise_not(vis_roi, vis_roi)\n                vis[mask == 0] = 0\n\n            if self.tracking_state == 1:\n                self.selection = None\n\n                # Compute the histogram back projection\n                prob = cv2.calcBackProject([hsv], [0], self.hist, [0, 180], 1)\n\n                prob &= mask\n                term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n\n                # Apply CAMShift on 'prob'\n                track_box, self.track_window = cv2.CamShift(prob, self.track_window, term_crit)\n\n                # Draw an ellipse around the object\n                cv2.ellipse(vis, track_box, (0, 255, 0), 2)\n\n            cv2.imshow('Object Tracker', vis)\n\n            c = cv2.waitKey(5)\n            if c == 27:\n                break\n\n        cv2.destroyAllWindows()\n\nif __name__ == '__main__':\n    ObjectTracker().start_tracking()\n```", "```py\nimport cv2\nimport numpy as np\n\ndef start_tracking():\n    # Capture the input frame\n    cap = cv2.VideoCapture(0)\n\n    # Downsampling factor for the image\n    scaling_factor = 0.5\n\n    # Number of frames to keep in the buffer when you\n    # are tracking. If you increase this number,\n    # feature points will have more \"inertia\"\n    num_frames_to_track = 5\n\n    # Skip every 'n' frames. This is just to increase the speed.\n    num_frames_jump = 2\n\n    tracking_paths = []\n    frame_index = 0\n\n    # 'winSize' refers to the size of each patch. These patches\n    # are the smallest blocks on which we operate and track\n    # the feature points. You can read more about the parameters\n    # here: http://goo.gl/ulwqLk\n    tracking_params = dict(winSize  = (11, 11), maxLevel = 2,\n            criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n\n    # Iterate until the user presses the ESC key\n    while True:\n        # read the input frame\n        ret, frame = cap.read()\n\n        # downsample the input frame\n        frame = cv2.resize(frame, None, fx=scaling_factor,\n                fy=scaling_factor, interpolation=cv2.INTER_AREA)\n\n        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        output_img = frame.copy()\n\n        if len(tracking_paths) > 0:\n            prev_img, current_img = prev_gray, frame_gray\n            feature_points_0 = np.float32([tp[-1] for tp in tracking_paths]).reshape(-1, 1, 2)\n\n            # Compute feature points using optical flow. You can\n            # refer to the documentation to learn more about the\n            # parameters here: http://goo.gl/t6P4SE\n            feature_points_1, _, _ = cv2.calcOpticalFlowPyrLK(prev_img, current_img, feature_points_0,\n                    None, **tracking_params)\n            feature_points_0_rev, _, _ = cv2.calcOpticalFlowPyrLK(current_img, prev_img, feature_points_1,\n                    None, **tracking_params)\n\n            # Compute the difference of the feature points\n            diff_feature_points = abs(feature_points_0- feature_points_0_rev).reshape(-1, 2).max(-1)\n\n            # threshold and keep the good points\n            good_points = diff_feature_points < 1\n\n            new_tracking_paths = []\n\n            for tp, (x, y), good_points_flag in zip(tracking_paths,\n                        feature_points_1.reshape(-1, 2), good_points):\n                if not good_points_flag:\n                    continue\n\n                tp.append((x, y))\n\n                # Using the queue structure i.e. first in,\n                # first out\n                if len(tp) > num_frames_to_track:\n                    del tp[0]\n\n                new_tracking_paths.append(tp)\n\n                # draw green circles on top of the output image\n                cv2.circle(output_img, (x, y), 3, (0, 255, 0), -1)\n\n            tracking_paths = new_tracking_paths\n\n            # draw green lines on top of the output image\n            cv2.polylines(output_img, [np.int32(tp) for tp in tracking_paths], False, (0, 150, 0))\n\n        # 'if' condition to skip every 'n'th frame\n        if not frame_index % num_frames_jump:\n            mask = np.zeros_like(frame_gray)\n            mask[:] = 255\n            for x, y in [np.int32(tp[-1]) for tp in tracking_paths]:\n                cv2.circle(mask, (x, y), 6, 0, -1)\n\n            # Extract good features to track. You can learn more\n            # about the parameters here: http://goo.gl/BI2Kml\n            feature_points = cv2.goodFeaturesToTrack(frame_gray,\n                    mask = mask, maxCorners = 500, qualityLevel = 0.3,\n                    minDistance = 7, blockSize = 7)\n\n            if feature_points is not None:\n                for x, y in np.float32(feature_points).reshape (-1, 2):\n                    tracking_paths.append([(x, y)])\n\n        frame_index += 1\n        prev_gray = frame_gray\n\n        cv2.imshow('Optical Flow', output_img)\n\n        # Check if the user pressed the ESC key\n        c = cv2.waitKey(1)\n        if c == 27:\n            break\n\nif __name__ == '__main__':\n    start_tracking()\n    cv2.destroyAllWindows()\n```", "```py\nimport cv2\nimport numpy as np\n\n# Capture the input frame\ndef get_frame(cap, scaling_factor=0.5):\n    ret, frame = cap.read()\n\n    # Resize the frame\n    frame = cv2.resize(frame, None, fx=scaling_factor,\n            fy=scaling_factor, interpolation=cv2.INTER_AREA)\n\n    return frame\n\nif __name__=='__main__':\n    # Initialize the video capture object\n    cap = cv2.VideoCapture(0)\n\n    # Create the background subtractor object\n    bgSubtractor = cv2.BackgroundSubtractorMOG()\n\n    # This factor controls the learning rate of the algorithm.\n    # The learning rate refers to the rate at which your model\n    # will learn about the background. Higher value for\n    # 'history' indicates a slower learning rate. You\n    # can play with this parameter to see how it affects\n    # the output.\n    history = 100\n\n    # Iterate until the user presses the ESC key\n    while True:\n        frame = get_frame(cap, 0.5)\n\n        # Apply the background subtraction model to the # input frame\n        mask = bgSubtractor.apply(frame, learningRate=1.0/history)\n\n        # Convert from grayscale to 3-channel RGB\n        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n\n        cv2.imshow('Input frame', frame)\n        cv2.imshow('Moving Objects', mask & frame)\n\n        # Check if the user pressed the ESC key\n        c = cv2.waitKey(10)\n        if c == 27:\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n```"]