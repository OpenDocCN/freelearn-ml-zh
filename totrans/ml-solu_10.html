<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 10. Face Recognition and Face Emotion Recognition"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Face Recognition and Face Emotion Recognition</h1></div></div></div><p>In the previous chapter, we looked at how to detect objects such as a car, chair, cat, and dog, using Convolutional Neural Networks and the YOLO (You Only Look Once) algorithm. In this chapter, we will be detecting human faces. Apart from that, we will be looking at expressions of the human face, such as a human face seeming happy, neutral, sad, and so on. So, this chapter will be interesting, because we are going to focus on some of the latest techniques of face detection and face emotion recognition. We are dividing this chapter into two parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Face detection</li><li class="listitem" style="list-style-type: disc">Face emotion recognition</li></ul></div><p>First, we will cover how face detection works, and after that, we will move on to the face emotion recognition part. In general, we will cover the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introducing the problem statement</li><li class="listitem" style="list-style-type: disc">Setting up the coding environment</li><li class="listitem" style="list-style-type: disc">Understanding the concepts of face recognition</li><li class="listitem" style="list-style-type: disc">Approaches for implementing face recognition</li><li class="listitem" style="list-style-type: disc">Understanding the dataset for face emotion recognition</li><li class="listitem" style="list-style-type: disc">Understanding the concepts of face emotion recognition</li><li class="listitem" style="list-style-type: disc">Building the face emotion recognition model</li><li class="listitem" style="list-style-type: disc">Understanding the testing matrix</li><li class="listitem" style="list-style-type: disc">Testing the model</li><li class="listitem" style="list-style-type: disc">Problems with the existing approach</li><li class="listitem" style="list-style-type: disc">How to optimize the existing approach<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the process for optimization</li></ul></div></li><li class="listitem" style="list-style-type: disc">The best approach<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the best approach</li></ul></div></li><li class="listitem" style="list-style-type: disc">Summary</li></ul></div><div class="section" title="Introducing the problem statement"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec101"/>Introducing the problem statement</h1></div></div></div><p>We want to develop two applications. One application will recognize human faces, and the other will recognize the emotion of the human faces. We will discuss both of them in this section. We will look at what exactly we want to develop.</p><div class="section" title="Face recognition application"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec168"/>Face recognition application</h2></div></div></div><p>This application <a id="id1166" class="indexterm"/>should basically identify human faces from an image or a real-time video stream. Refer to the following photo; it will help you understand what I mean by identifying faces from an image or a real-time video stream:</p><div class="mediaobject"><img src="Images/B08394_10_01.jpg" alt="Face recognition application" width="745" height="559"/><div class="caption"><p>Figure 10.1: Demo output for understanding the face recognition application</p><p>Images source: https://unsplash.com/photos/Q13lggdvtVY</p></div></div><p>As you can see in the preceding figure (Figure 10.1), when we provide any image as the input, in the first step, the machine can recognize the number of human faces present in the image. As the output, we can get cropped images of the faces.</p><p>Beyond this, I also want the application to identify the name of the person based on the face. I think <a id="id1167" class="indexterm"/>you are familiar with this kind of application. Let me remind you. When you upload an image on Facebook, the face recognition mechanism of Facebook immediately recognizes names of people who are part of that image, and suggests that you tag them in your image. We will develop similar functionality here in terms of the face recognition application. Now let's move on to another part of the application.</p></div><div class="section" title="Face emotion recognition application"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec169"/>Face emotion recognition application</h2></div></div></div><p>In this <a id="id1168" class="indexterm"/>part of the application, we want to build an application that can detect the type of emotion on a human face. We will try to recognize the following seven emotions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Anger</li><li class="listitem" style="list-style-type: disc">Disgust</li><li class="listitem" style="list-style-type: disc">Fear</li><li class="listitem" style="list-style-type: disc">Happiness</li><li class="listitem" style="list-style-type: disc">Sadness</li><li class="listitem" style="list-style-type: disc">Surprise</li><li class="listitem" style="list-style-type: disc">Neutral</li></ul></div><p>So, we will categorize facial emotions into these seven types. This kind of application will be helpful to know what kind of feeling the person is experiencing, and this insight will help in performing sentiment analysis, body language analysis, and so on.</p><p>Here, we will first build the face recognition application, and after that, we will move on to the face emotion recognition application.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Setting up the coding environment"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec102"/>Setting up the coding environment</h1></div></div></div><p>In this section, we <a id="id1169" class="indexterm"/>will set up the coding environment for the face recognition application. We will look at how to install dependencies. We will be installing the following two libraries:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">dlib</li><li class="listitem" style="list-style-type: disc">face_recognition</li></ul></div><p>Let's begin the installation process.</p><div class="section" title="Installing dlib"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec170"/>Installing dlib</h2></div></div></div><p>In order <a id="id1170" class="indexterm"/>to install the dlib library, we need to perform <a id="id1171" class="indexterm"/>the following steps. We can install this library either on a Linux operating system (OS), or on macOS. Let's follow the stepwise instructions:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download the source code of dlib by executing this command:<div class="informalexample"><pre class="programlisting">
<code class="literal">sudo git clone https://github.com/davisking/dlib.git</code>.</pre></div></li><li class="listitem">Now jump to the <code class="literal">dlib </code>directory by executing this command: <code class="literal">cd dlib</code>.</li><li class="listitem">Now we need to build the main <code class="literal">dlib </code>library, so we need to execute the following commands stepwise:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><code class="literal">sudo mkdir build</code>.</li><li class="listitem"><code class="literal">cd build</code>.</li><li class="listitem"><code class="literal">cmake .. -DDLIB_USE_CUDA=0 -DUSE_AVX_INSTRUCTIONS=1</code>.</li><li class="listitem"><code class="literal">cmake --build</code>.</li></ol></div></li></ol></div><p>Once the project has been built successfully, you can move to the next installation steps. You also need to install <span class="strong"><strong>OpenCV</strong></span>. The installation steps for OpenCV have already been given in <a class="link" href="ch10.xhtml" title="Chapter 10. Face Recognition and Face Emotion Recognition">Chapter 10,</a> <span class="emphasis"><em>Real-Time Object Detection</em></span>.</p></div><div class="section" title="Installing face_recognition"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec171"/>Installing face_recognition</h2></div></div></div><p>In order <a id="id1172" class="indexterm"/>to install the <code class="literal">face_recognition</code> library, we need to <a id="id1173" class="indexterm"/>execute the following commands:</p><div class="informalexample"><pre class="programlisting">$ sudo pip install face_recognition (This command is for python 2.7)
$ sudo pip3 install face_recognition (This command is for python 3.3+)</pre></div><p>The preceding commands install the <code class="literal">face_recognition</code> library only if we have a perfectly installed <code class="literal">dlib</code>.</p><p>Once the <a id="id1174" class="indexterm"/>preceding two libraries have been installed, we can <a id="id1175" class="indexterm"/>move on to the next section, in which we will be discussing the key concepts of face recognition.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the concepts of face recognition"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec103"/>Understanding the concepts of face recognition</h1></div></div></div><p>In this section, we <a id="id1176" class="indexterm"/>will look at the major concepts of face recognition. These concepts will include the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the face recognition dataset</li><li class="listitem" style="list-style-type: disc">The algorithm for face recognition</li></ul></div><div class="section" title="Understanding the face recognition dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec172"/>Understanding the face recognition dataset</h2></div></div></div><p>You may <a id="id1177" class="indexterm"/>wonder why I haven't discussed anything related to the dataset until now. This is because I don't want to confuse you by providing all the details about the datasets of two different applications. The dataset that we will cover here is going to be used for <span class="strong"><strong>face recognition</strong></span>.</p><p>If you want to build a face recognition engine from scratch, then you can use following datasets:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">CAS-PEAL Face Dataset</li><li class="listitem" style="list-style-type: disc">Labeled Faces in the Wild</li></ul></div><p>Let's discuss them in further detail.</p><div class="section" title="CAS-PEAL Face Dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec122"/>CAS-PEAL Face Dataset</h3></div></div></div><p>This is <a id="id1178" class="indexterm"/>a huge dataset for face recognition <a id="id1179" class="indexterm"/>tasks. It has various types of face images. It contains face images with different sources of variations, especially Pose, Emotion, Accessories, and Lighting (PEAL) for face recognition tasks.</p><p>This dataset <a id="id1180" class="indexterm"/>contains 99,594 images of 1,040 individuals, of which 595 are male individuals and 445 are female individuals. The <a id="id1181" class="indexterm"/>captured images of the individuals are with varying poses, emotions, accessories, and lighting. Refer to the following photo to see this. You can also refer to the following link if you want to see the sample dataset: <a class="ulink" href="http://www.jdl.ac.cn/peal/index.html">http://www.jdl.ac.cn/peal/index.html</a>.</p><div class="mediaobject"><img src="Images/B08394_10_02.jpg" alt="CAS-PEAL Face Dataset" width="926" height="189"/><div class="caption"><p>Figure 10.2: CAS-PEAL Face Dataset sample image</p><p>Image source: http://www.jdl.ac.cn/peal/Image/Pose_normal/NormalCombination-9-Cameras.jpg</p></div></div><p>You can <a id="id1182" class="indexterm"/>download this dataset from the following link: <a class="ulink" href="http://www.jdl.ac.cn/peal/download.htm">http://www.jdl.ac.cn/peal/download.htm</a>
</p></div><div class="section" title="Labeled Faces in the Wild"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec123"/>Labeled Faces in the Wild</h3></div></div></div><p>This dataset <a id="id1183" class="indexterm"/>is also referred to as the LFW dataset. It is used in the <code class="literal">face_recognition</code> library. We will be using this library to build our face recognition application. This dataset contains more than 13,000 images of faces collected from the web. Each face is labeled with the name of the person pictured. So, the dataset is a labeled dataset. There are 1,680 people pictured with two or more distinct face images in the dataset. You can refer to the sample dataset using the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_03.jpg" alt="Labeled Faces in the Wild" width="574" height="216"/><div class="caption"><p>Figure 10.3: Sample images from the LFW dataset</p><p>Image source: http://vis-www.cs.umass.edu/lfw/person/AJ_Cook.html</p></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>You can <a id="id1184" class="indexterm"/>find out more about this dataset by clicking on <a class="ulink" href="http://vis-www.cs.umass.edu/lfw/index.html">http://vis-www.cs.umass.edu/lfw/index.html</a>. You can also download the dataset by using the same link. You can refer to the Caltech 10,000 web faces dataset as well by clicking on <a class="ulink" href="http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/">http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/</a>. You should also refer to the INRIA Person Dataset, which will be quite useful. The link for the INRIA Person Dataset is <a class="ulink" href="http://pascal.inrialpes.fr/data/human/">http://pascal.inrialpes.fr/data/human/</a>.</p></div></div><p>In order to build the face recognition application, we will be using the <code class="literal">face_recognition</code> library. We are using the pre-trained model provided by this library via its API. We will certainly explore the algorithm and the concept behind this pre-trained model and library. So let's begin!</p></div></div><div class="section" title="Algorithms for face recognition"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec173"/>Algorithms for face recognition</h2></div></div></div><p>In this section, we will look at the core algorithm that is used for face recognition. The name of <a id="id1185" class="indexterm"/>the algorithm is <span class="strong"><strong>Histogram of Oriented Gradients</strong></span> (<span class="strong"><strong>HOG</strong></span>). We will see how HOG is used in face recognition tasks. A face recognition (FR) task is basically a classification task, as we are detecting the face from the image as well as trying to identify the person's name with the help of the person's face. HOG is a good option to try out.</p><p>The other approach is to use the Convolutional Neural Network (CNN). In this section, we will <a id="id1186" class="indexterm"/>also cover CNN for the FR task. So, let's start out with HOG!</p><div class="section" title="Histogram of Oriented Gradients (HOG)"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec124"/>Histogram of Oriented Gradients (HOG)</h3></div></div></div><p>The HOG <a id="id1187" class="indexterm"/>algorithm is one of <a id="id1188" class="indexterm"/>the best approaches for state-of-the-art results for face recognition. The HOG method was introduced by Dalal and Triggs in their seminal 2005 paper, available at <a class="ulink" href="http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf">http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf</a>. The HOG image descriptor and a linear Support Vector Machine can be used to train highly accurate classifiers that can classify human detectors. So, HOG can be applied to an FR task as well. First, we will cover the basic intuition behind the algorithm.</p><p>HOG is a type of feature descriptor. A feature descriptor is a representation of an image that simplifies the image by extracting useful information and ignoring the information. Here, our focus will be on the faces only. So, we will be ignoring other objects, if there are any. The LWF dataset has less noise, so the task of generating an accurate feature descriptor is comparatively easy. The step-by-step process is as follows:</p><p>
<span class="strong"><strong>Step 1</strong></span>: In order to find the faces in image, we will start by converting our color image into black and white, because we don't need color data to recognize the face. Refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_04.jpg" alt="Histogram of Oriented Gradients (HOG)" width="748" height="311"/><div class="caption"><p>Figure 10.4:  Converting a color image to a black and white image</p></div></div><p>
<span class="strong"><strong>Step 2</strong></span>: In this step, we will look at every single pixel in our image at a time. For every single pixel, we <a id="id1189" class="indexterm"/>want to look at pixels that directly surround it. Refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_05.jpg" alt="Histogram of Oriented Gradients (HOG)" width="762" height="415"/><div class="caption"><p>Figure 10.5: Process of scanning every single pixel of the image</p></div></div><p>Step 3: Here, our <a id="id1190" class="indexterm"/>goal is to find out how dark the current pixel is with respect to the pixels directly surrounding it. We need to draw an arrow that indicates the direction in which the pixels of the image are getting darker. In order to achieve this, we scan the entire image. Refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_06.jpg" alt="Histogram of Oriented Gradients (HOG)" width="502" height="184"/><div class="caption"><p>Figure 10.6: An arrow direction from a light pixel to a dark pixel</p></div></div><p>As you can see in the preceding diagram, we have considered a pixel and the other pixels surrounding it. By looking at the pixels, we can easily figure out that the arrow head is <a id="id1191" class="indexterm"/>pointing toward the darker pixel.</p><p>Step 4: If we <a id="id1192" class="indexterm"/>repeat this process for every single pixel in the image, then we will end up with every pixel being replaced with arrows. These arrows are called <span class="emphasis"><em>gradients</em></span>. These <span class="emphasis"><em>gradients </em></span>show the flow from light to dark pixels across the entire image. Refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_07.jpg" alt="Histogram of Oriented Gradients (HOG)" width="765" height="845"/><div class="caption"><p>Figure 10.7: Gradient arrows for the entire image</p></div></div><p>In the <a id="id1193" class="indexterm"/>preceding diagram, you can see the kind of output we get after generating a gradient for the input image. The scanning <a id="id1194" class="indexterm"/>of the entire image might seem like a random thing to do, but there is a reason for replacing the pixels with gradients. If we analyze the original pixel values directly, then really dark images and really light images of the same person will have totally different pixel values, which make things more complicated when we try to recognize the face of the person. Here, we are considering the direction in which the brightness of the pixel changes. We find that both really dark images and really light images of the same person will end up with the exact same representation for the face. This kind of representation will be easy for us to deal with <a id="id1195" class="indexterm"/>in terms of the face recognition task. This is the main reason for generating the gradient for the entire image. There is one challenge, though, which we will discuss in the next step.</p><p>Step 5: Saving <a id="id1196" class="indexterm"/>the gradient for every single pixel gives us too much information, and there is the chance that we may use this amount of information inefficiently. So, we need to obtain the bare minimum information that we will be using for the FR task. We will achieve this by just considering the basic flow of lightness or darkness at a higher level, so we can see the basic pattern of the image. The process for achieving this is given in step 6.</p><p>Step 6: We will break up this image into small squares of 16 x16 pixels each. In each square, we will count the number of gradient points in each major direction, which means we will count how many arrows point up, point down, point right, point left, and so on. After counting this, we will replace that square in the image with the arrow directions that were the strongest. The end result is that we convert the original image into a simple representation that captures the basic structure of a face. Refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_08.jpg" alt="Histogram of Oriented Gradients (HOG)" width="1000" height="491"/><div class="caption"><p>Figure 10.8: Simple face representation in the HOG version</p></div></div><p>This kind of representation is easy to process for the FR task; it's called the HOG version of the image. It represents the features that we will consider in the FR task, and that is why this <a id="id1197" class="indexterm"/>representation is referred to <a id="id1198" class="indexterm"/>as an HOG features descriptor.</p><p>Step 7: In order to find out the faces in this HOG image, we have to find out the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces. Refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_09.jpg" alt="Histogram of Oriented Gradients (HOG)" width="748" height="495"/><div class="caption"><p>Figure 10.9: Process of recognizing a face using the HOG version of our image</p></div></div><p>Using this <a id="id1199" class="indexterm"/>technique, we can <a id="id1200" class="indexterm"/>easily recognize the faces in any image.</p></div><div class="section" title="Convolutional Neural Network (CNN) for FR"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec125"/>Convolutional Neural Network (CNN) for FR</h3></div></div></div><p>In this <a id="id1201" class="indexterm"/>section, we will look at how <a id="id1202" class="indexterm"/>a CNN can be used to recognize the faces from the images. This section is divided into two parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Simple CNN architecture</li><li class="listitem" style="list-style-type: disc">Understanding how a CNN works for FR</li></ul></div><div class="section" title="Simple CNN architecture"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl4sec63"/>Simple CNN architecture</h4></div></div></div><p>I don't want to get too deep into how a CNN works, as I have already provided most of the necessary details in <a class="link" href="ch09.xhtml" title="Chapter 9. Building a Real-Time Object Recognition App">Chapter 9</a>, <span class="emphasis"><em>Building Real-Time Object Detection</em></span>; however, I want to remind <a id="id1203" class="indexterm"/>you about some necessary stuff regarding CNN. First, refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_10.jpg" alt="Simple CNN architecture" width="1000" height="350"/><div class="caption"><p>Figure 10.10: The CNN architecture</p></div></div><p>As you can see in the preceding diagram, there is a convolutional layer, a pooling layer, a fully connected layer, and an output layer. There are different activation functions, penalties, and SoftMax functions involved. This is high-level information. For this FR task, we can use three convolutional and pooling layers with ReLU as activation functions. You can add more layers, but it will become more computationally expensive to train.</p></div><div class="section" title="Understanding how CNN works for FR"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl4sec64"/>Understanding how CNN works for FR</h4></div></div></div><p>Intuitively, the CNN <a id="id1204" class="indexterm"/>model performs the following steps in order to build a good FR application. The basic process is as follows:</p><p>Step 1: Look at a picture. Crop the images that contain only faces.</p><p>Step 2: Now, in this step, we focus on a face and try to understand that even if a face is turned in a weird direction, or an image is taken in bad lighting, we need to identify the proper placement of the face in this kind of image. Step 3 will give us a solution.</p><p>Step 3: In order to identify the face from any image, whether that image is taken in bad lighting conditions, or the orientation of the face seems totally weird, we need to identify the face. To achieve that, we pick out unique features of the face that can be used to tell <a id="id1205" class="indexterm"/>us something unique about the person's face. With the help of these unique features, we can identify the face of the same person, as well as the face of the different persons. These features can include how big the eyes are, how long the face is, and so on. There are 68 specific points that should be considered; and they are called landmarks. These points are defined based on the face landmark estimation. Refer to the following paper to get more details about this: <a class="ulink" href="http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf">http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf</a>. Take a look at the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_11.jpg" alt="Understanding how CNN works for FR" width="504" height="1000"/><div class="caption"><p>Figure 10.11: 68 points for face landmark estimation</p></div></div><p>Step 4: We need to identify the person's face with their name, so in order to achieve this, we will compare the unique features of that face with all the people we already know in <a id="id1206" class="indexterm"/>order to determine the person's name. Suppose you have added the images for Bill Gates, Barack Obama, and so on. You have generated the unique features for their faces, and now we will compare their unique facial features with these already generated facial features, and if the features are similar, then we get to know the name of the person, which is Barack Obama or Bill Gates in the given image. The identification of the person based on their facial features is a classification problem, which can easily be solved by CNN. We are generating a face embedding vector of size 128 measurements. As an input, we should provide this face embedding vector. Once we complete the training, our application <a id="id1207" class="indexterm"/>will be ready to identify the person's name.</p><p>Step 5: The trained model looks at all the faces we have measured in the past, and looks at the person who has the closest measurements to our faces' measurements. That is our match.</p><p>The preceding approach is for our CNN-based FR and real-time face recognition task. We have covered the basic concepts and the idea behind the algorithms that are used in the FR task. Now let's start the implementation.</p></div></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Approaches for implementing face recognition"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec104"/>Approaches for implementing face recognition</h1></div></div></div><p>In this section, <a id="id1208" class="indexterm"/>we will be implementing the FR application. We are using the <code class="literal">face_recognition</code> library. We have already configured the environment for that. We will be implementing the following approaches here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The HOG-based approach</li><li class="listitem" style="list-style-type: disc">The CNN-based approach</li><li class="listitem" style="list-style-type: disc">Real-time face recognition</li></ul></div><p>Now let's start coding!</p><div class="section" title="Implementing the HOG-based approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec174"/>Implementing the HOG-based approach</h2></div></div></div><p>In this <a id="id1209" class="indexterm"/>approach, we are using the <a id="id1210" class="indexterm"/>HOG algorithm to find out two things: the total number of faces in the image, and the paces. We are using the API of the <code class="literal">face_recgnition</code> library. You can find the code by clicking on the following GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_example.py">https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_example.py</a>. The code snippet is provided in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_12.jpg" alt="Implementing the HOG-based approach" width="883" height="392"/><div class="caption"><p>Figure 10.12: Code snippet for the HOG-based approach for FR</p></div></div><p>In the <a id="id1211" class="indexterm"/>preceding diagram, we have given  an image as input, and with the help of the API of the <code class="literal">face_recognition</code> library, we can find the pixel location of the face in an image. Here, we will <a id="id1212" class="indexterm"/>also count how many faces there are in an image, and with the help of the <code class="literal">Image </code>library, we can crop the faces from the given image. You can find the output of this script in the following figure:</p><div class="mediaobject"><img src="Images/B08394_10_13.jpg" alt="Implementing the HOG-based approach" width="648" height="104"/><div class="caption"><p>Figure 10.13: Output of the HOG-based approach for FR</p></div></div><p>Refer to the cropped face output in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_10_14.jpg" alt="Implementing the HOG-based approach" width="131" height="159"/><div class="caption"><p>Figure 10.14: Cropped face output</p></div></div><p>As you can see in the last output diagram with the help of a simple API, we can build a simple face recognition application. This approach is kind of a baseline approach for us.</p><p>Now let's <a id="id1213" class="indexterm"/>move on to the CNN-based <a id="id1214" class="indexterm"/>approach. The HOG-based approach is less accurate compared to the CNN-based approach. If we use GPU for the CNN-based approach, then we can train the model in a less amount of time. Now let's look at the code for the CNN-based approach.</p></div><div class="section" title="Implementing the CNN-based approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec175"/>Implementing the CNN-based approach</h2></div></div></div><p>In this <a id="id1215" class="indexterm"/>approach, we will be using the <code class="literal">face_recognition</code> library, where we specified the name of the model. Our model's name is <code class="literal">cnn</code>. This <a id="id1216" class="indexterm"/>particular approach will load the pre-trained model via the <code class="literal">face_recognition</code> API, and we can generate a more accurate output. You can find the code by clicking on the following GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_GPU_example.py">https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_GPU_example.py</a>. Refer to the code snippet given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_10_15.jpg" alt="Implementing the CNN-based approach" width="884" height="409"/><div class="caption"><p>Figure 10.15: Code snippet for the CNN-based approach for FR</p></div></div><p>Here, the <a id="id1217" class="indexterm"/>implementation code is almost the same as <a id="id1218" class="indexterm"/>earlier, but the difference is that we have provided the model name as <code class="literal">cnn </code>during the API call. You can see the output of this implementation in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_16.jpg" alt="Implementing the CNN-based approach" width="772" height="461"/><div class="caption"><p>Figure 10.16: Output of the CNN-based approach for FR</p></div></div><p>The output <a id="id1219" class="indexterm"/>of this implementation is the same as <a id="id1220" class="indexterm"/>the last one. This version of the implementation is fast, and has better accuracy. Now let's try to implement the FR task for a real-time video stream.</p></div><div class="section" title="Implementing real-time face recognition"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec176"/>Implementing real-time face recognition</h2></div></div></div><p>In this section, we will implement the FR task for a real-time video stream. We will try to identify the <a id="id1221" class="indexterm"/>name of the person <a id="id1222" class="indexterm"/>who has appeared in the video. Doesn't that sound interesting? Let's begin. You can find the code by clicking on the following GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Face_recognition/blob/master/Real_time_face_detection.py">https://github.com/jalajthanaki/Face_recognition/blob/master/Real_time_face_detection.py</a>.</p><p>Again, we are using the API of the <code class="literal">face_recognition</code> library. We are using <code class="literal">OpenCV </code>as well. First of all, we need to feed the sample image of the person with the person's name, so <a id="id1223" class="indexterm"/>that the machine <a id="id1224" class="indexterm"/>can learn the name of the person and identify it during the testing. In this implementation, I have fed the image of Barack Obama and Joe Biden. You can add your image as well. If the face features are familiar and match with the already-fed images, then the script returns the name of the person, and if the face features are not familiar to the given image, then that person's face is tagged as <span class="emphasis"><em>Unknown. </em></span>Refer to the implementation in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_17.jpg" alt="Implementing real-time face recognition" width="740" height="878"/><div class="caption"><p>Figure 10.17: Implementation of real-time FR</p></div></div><p>As you can see in the preceding code, I have provided sample images of Barack Obama and Joe Biden. I have also provided the names of the people whose images I'm feeding to my script. I have used the same face recognition API for detecting and recognizing the face in the <a id="id1225" class="indexterm"/>video stream. When you run the script, your webcam streams your real-time video and this script detects <a id="id1226" class="indexterm"/>the face, and if you provide the image of the person that the machine knows, then it identifies it correctly this time as well. Refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_18.jpg" alt="Implementing real-time face recognition" width="1000" height="474"/><div class="caption"><p>Figure 10.18: Output of the real-time FR</p></div></div><p>As you can see, I have not provided my image to the machine, so it identifies me as <span class="strong"><strong>Unknown</strong></span>, whereas it can identify Barack Obama's image. You can also find the animated image by clicking on <a class="ulink" href="https://github.com/jalajthanaki/Face_recognition/blob/master/img/Demo.gif">https://github.com/jalajthanaki/Face_recognition/blob/master/img/Demo.gif</a>.</p><p>We are finished with the first part of the chapter, which entails developing an application that can recognize human faces, as well as identify the name of the person based on their face. We implemented three different variations of FR.</p><p>In the upcoming section, we will look at how to develop a face emotion recognition (FER) <a id="id1227" class="indexterm"/>application. We need different <a id="id1228" class="indexterm"/>kinds of datasets to build this application, so we will start by understanding a dataset for FER.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the dataset for face emotion recognition"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec105"/>Understanding the dataset for face emotion recognition</h1></div></div></div><p>To develop <a id="id1229" class="indexterm"/>an FER application, we are considering <a id="id1230" class="indexterm"/>the <code class="literal">FER2013</code> dataset. You can download this dataset from <a class="ulink" href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data">https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data</a>. We need <a id="id1231" class="indexterm"/>to know the basic details about this dataset. The dataset credit goes to Pierre-Luc Carrier and Aaron Courville as part of an ongoing research project.</p><p>This dataset consists of 48x48 pixel grayscale images of faces. The task is to categorize each of the faces based on the emotion that has been shown in the image in the form of facial expressions. The seven categories are as follows, and for each of them there is a numeric label that expresses the category of the emotion:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">0</code> = Anger</li><li class="listitem" style="list-style-type: disc"><code class="literal">1</code> = Disgust</li><li class="listitem" style="list-style-type: disc"><code class="literal">2</code> = Fear</li><li class="listitem" style="list-style-type: disc"><code class="literal">3</code> = Happiness</li><li class="listitem" style="list-style-type: disc"><code class="literal">4</code> = Sadness</li><li class="listitem" style="list-style-type: disc"><code class="literal">5</code> = Surprise</li><li class="listitem" style="list-style-type: disc"><code class="literal">6</code> = Neutral</li></ul></div><p>This dataset has the <code class="literal">fer2013.csv</code> file. This csv file will be used as our training dataset. Now let's look at the attributes of the file. There are three columns in the file, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Emotion</strong></span>: This column contains the numeric label of the facial expression. For fear, this column contains the value <code class="literal">2</code>; for sadness, this column contains the value <code class="literal">4</code>, and so on.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Pixels</strong></span>: This <a id="id1232" class="indexterm"/>column contains the pixel values of the individual images. It represents the matrix of pixel values of the image.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Usage</strong></span>: This <a id="id1233" class="indexterm"/>column contains the general tag about whether the particular data record will be used for training purposes or for testing purposes. There are three labels that are part of <a id="id1234" class="indexterm"/>this column, and those are <span class="emphasis"><em>Training</em></span>,<span class="emphasis"><em> PublicTest</em></span>, and<span class="emphasis"><em> PrivateTest</em></span>. For training purposes, there are 28,709 data samples. The public test set consist of 3,589 data samples, and the private test set consists of another 3,589 data samples.</li></ul></div><p>Now let's cover the concepts that will help us develop the FER application.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the concepts of face emotion recognition"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec106"/>Understanding the concepts of face emotion recognition</h1></div></div></div><p>We are <a id="id1235" class="indexterm"/>using Convolutional Neural Network (CNN) to develop the FER application. Earlier, we looked at the basic architecture of CNN. In order to develop FER applications, we will be using the following CNN architecture and optimizer. We are building CNN that is two layers deep. We will be using two fully connected layers and the SoftMax function to categorize the facial emotions.</p><p>We will be using several layers made of the convolutional layer, followed by the ReLU (Rectified Linear Unit) layer, followed by the max pooling layer. Refer to the following diagram, which will help you conceptualize the arrangement of the CNN layers. Let's look at the working of CNN. We will cover the following layers:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The convolutional layer</li><li class="listitem" style="list-style-type: disc">The ReLU layer</li><li class="listitem" style="list-style-type: disc">The pooling layer</li><li class="listitem" style="list-style-type: disc">The fully connected layer</li><li class="listitem" style="list-style-type: disc">The SoftMax layer</li></ul></div><div class="section" title="Understanding the convolutional layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec177"/>Understanding the convolutional layer</h2></div></div></div><p>In this layer, we <a id="id1236" class="indexterm"/>will feed our image in the form of pixel values. We are using a sliding window of 3 x 3 dimension, which slides through the entire image. The area that is chosen by the sliding window is called the <span class="emphasis"><em>receptive field</em></span>. It is the patch of the image. A sliding window is just the matrix of a 3 x 3 dimension, and it can scan the entire image. By using the sliding window, we scan nine pixel values of the image using the matrix of a 3 x 3 dimension. This receptive field or a piece of the image is the input of the convolutional network.</p><p>Refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_19.jpg" alt="Understanding the convolutional layer" width="1000" height="649"/><div class="caption"><p>Figure 10.19: Sliding window and receptive field</p></div></div><p>This receptive <a id="id1237" class="indexterm"/>field carries the values in the form of pixel values of the input image. These pixel values are called feature maps, features, filter, weight matrix, or kernel. We already had a matrix of 3 x 3, which is referred to as the feature map. The size of the feature map is one of the hyperparameters. We can take the n x n matrix, where n &gt;=1. Here, we have considered a 3 x 3 matrix to understand the operation. Now it's time to perform a simple math operation, the steps for which are as follows:</p><p>Step 1: Get the feature map. Here, the feature map means the image patch that is generated in the form of a receptive field.</p><p>Step 2: We need to perform the dot product between the feature map and the entire image. Again, we scan the entire image using the sliding window, and generate the value of dot products.</p><p>Step 3: We need <a id="id1238" class="indexterm"/>to sum up all the values that we get from obtaining the dot product.</p><p>Step 4: We need to divide the value of the summation for the dot product by the total number of pixels in the feature. In this explanation, we have a total of nine pixels, so we will divide the sum by 9. As an output, we get the image that is referred to as a feature image.</p><p>Refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_20.jpg" alt="Understanding the convolutional layer" width="822" height="384"/><div class="caption"><p>Figure 10.20: Math operation for the convolutional layer</p></div></div><p>We will repeat this operation for almost all the possible positions of the image, and we will try out all the possible combinations, which is the reason why this operation is referred to as convolutional. Now let's look at the ReLU layer.</p></div><div class="section" title="Understanding the ReLU layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec178"/>Understanding the ReLU layer</h2></div></div></div><p>This layer <a id="id1239" class="indexterm"/>basically introduces nonlinearity to the convolutional network. Here, we need to use the <code class="literal">activation</code> function. For this application, we have chosen the Rectified Linear Unit as the activation function. This layer performs some sort of normalization to our feature map. Let's see what it does to our feature map:</p><p>Step 1: This layer takes the feature map as the input that is generated by the convolutional layer.</p><p>Step 2: This layer just converts the negative values into zero.</p><p>Refer the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_21.jpg" alt="Understanding the ReLU layer" width="1000" height="314"/><div class="caption"><p>Figure 10.21: Operations performed by the ReLU layer</p></div></div><p>Now it is time to look at the pooling layer.</p></div><div class="section" title="Understanding the pooling layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec179"/>Understanding the pooling layer</h2></div></div></div><p>Using this <a id="id1240" class="indexterm"/>layer, we shrink the image. We will be using the max pooling operation here. We need to perform the following steps:</p><p>Step 1: We need to feed the feature map as the input, and this time, the output of the ReLU layer is given as the input to this layer.</p><p>Step 2: We need to pick up the window size. Generally, we pick up the size of 2 x 2 pixels or 3 x 3 pixels. We will be taking 2 x 2 as our window size.</p><p>Step 3: We scan the entire image based on this window size, and we will take the maximum value from four pixel values.</p><p>You can <a id="id1241" class="indexterm"/>understand the operation by referring to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_22.jpg" alt="Understanding the pooling layer" width="390" height="183"/><div class="caption"><p>Figure 10.22: Operations performed by the max pooling layer</p></div></div><p>We can do deep stacking of these layers as deep as we need. You can repeat the convolutional, ReLU, and pooling layers an n number of times in order to make CNN deep.</p></div><div class="section" title="Understanding the fully connected layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec180"/>Understanding the fully connected layer</h2></div></div></div><p>The output <a id="id1242" class="indexterm"/>of all the layers is passed on to the fully connected layer. This layer has a voting mechanism. All image patches are considered for votes. The image patches of 2 x 2 matrices are arranged in a horizontal way. The vote depends on how strongly a value predicts the face expression. If certain values of this layer are high, it means they are close to 1, and if certain values of this layer are low, it means they are close to 0. For each category, certain cell values are close to 1 and others are 0, and this way, our network will predict the category. Refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_23.jpg" alt="Understanding the fully connected layer" width="1000" height="475"/><div class="caption"><p>Figure 10.23: Intuitive understanding of the fully connected layer</p></div></div><p>There is <a id="id1243" class="indexterm"/>only one math operation that's been performed here. We are taking average values. As you can see in preceding diagram, the first, fourth, fifth, tenth, and eleventh cells of the fully connected layer are predicting one category, so we need to sum up all the values present in those cells and find their average. This average value tells us how confident our network is when it predicts the class. We can stack up as many fully connected layers as we want. Here, the number of neurons is the hyperparameter.</p></div><div class="section" title="Understanding the SoftMax layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec181"/>Understanding the SoftMax layer</h2></div></div></div><p>We can <a id="id1244" class="indexterm"/>also use the SoftMax layer, which converts the feature's values into the probability value. The equation for this is as follows:</p><div class="mediaobject"><img src="Images/B08394_10_24.jpg" alt="Understanding the SoftMax layer" width="130" height="65"/><div class="caption"><p>Figure 10.24: The SoftMax equation</p></div></div><p>This layer takes the feature values, and using the preceding equation, it generates the probability <a id="id1245" class="indexterm"/>value. Refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_25.jpg" alt="Understanding the SoftMax layer" width="1000" height="690"/><div class="caption"><p>Figure 10.25: Process of calculating the SoftMax function</p></div></div></div><div class="section" title="Updating the weight based on backpropagation"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec182"/>Updating the weight based on backpropagation</h2></div></div></div><p>The weight <a id="id1246" class="indexterm"/>of the CNN network has been updated based on the backpropagation technique. We will measure the difference between our predicted answer and the actual answer. Based on this error measurement, we calculate the gradient for the loss function, which tells us whether we should increase the weight or decrease it. If the predicted answer and the actual answers are the same then there will be no change in weights.</p><p>We have understood most of the core concepts of CNN that we will be using for developing face emotion recognition model.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Building the face emotion recognition model"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec107"/>Building the face emotion recognition model</h1></div></div></div><p>In this <a id="id1247" class="indexterm"/>section, we will implement the application of FER using CNN. For coding purposes, we will be using the <code class="literal">TensorFlow</code>,<code class="literal"> TFLearn</code>,<code class="literal"> OpenCV</code>, and <code class="literal">Numpy</code> libraries. You can find the code by using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow">https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow. </a>These are the steps that we need to follow:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Preparing the data</li><li class="listitem">Loading the data</li><li class="listitem">Training the model</li></ol></div><div class="section" title="Preparing the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec183"/>Preparing the data</h2></div></div></div><p>In this section, we <a id="id1248" class="indexterm"/>will be preparing the dataset that can be used in our application. As you know, our dataset is in grayscale. We have two options. One is that we need to use only black and white images, and if we are using black and white images, then there will be two channels. The second option is that we can convert the grayscale pixel values into RGB (red, green, and blue) images and build the CNN with three channels. For our development purposes, we are using two channels as our images are in grayscale.</p><p>First of all, we are loading the dataset and converting it into the <code class="literal">numpy </code>array. After the conversion, we will save it as a <code class="literal">.npy</code> format so that we can load that dataset as and when needed. We are saving actual data records in one file and the labels of their data records in <a id="id1249" class="indexterm"/>another file. Our input datafile name is <code class="literal">fer2013.csv</code>. Our output file that contains the data is <code class="literal">data_set_fer2013.npy</code>, and labels are present in the <code class="literal">data_labels_fer2013.npy</code> file. The script name that performs this task is <code class="literal">csv_to_numpy.py</code>. You can refer to its code using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/tree/master/data">https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/tree/master/data</a>
</p><p>Refer to the code snippet for loading the dataset in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_26.jpg" alt="Preparing the data" width="604" height="419"/><div class="caption"><p>Figure 10.26: Code snippet for loading the dataset</p></div></div><p>The code <a id="id1250" class="indexterm"/>for the <code class="literal">helper</code> function is given in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_27.jpg" alt="Preparing the data" width="921" height="901"/><div class="caption"><p>Figure 10.27: Code snippet for the helper function</p></div></div><p>Now let's look at how we can load the data that we have saved in the <code class="literal">.npy</code> format.</p></div><div class="section" title="Loading the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec184"/>Loading the data</h2></div></div></div><p>In this section, we will look at how we are going to use the dataset we have prepared so that we <a id="id1251" class="indexterm"/>can use it for training. Here, we create a separate script to help us load the data. In this, we define a test dataset that we will be using during testing.</p><p>This is a simple and straightforward code. You can find the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_10_28.jpg" alt="Loading the data" width="653" height="567"/><div class="caption"><p>Figure 10.28: Code snippet for the data loader script</p></div></div><p>This class and its methods will be used when we write the script for training. You can see the code <a id="id1252" class="indexterm"/>of this script using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/dataset_loader.py">https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/dataset_loader.py</a>.</p></div><div class="section" title="Training the model"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec185"/>Training the model</h2></div></div></div><p>In this section, we <a id="id1253" class="indexterm"/>will look at how to train the model, so that it can recognize the facial emotion. These are the steps that we will be performing. You can find the code for this training step by referring to this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py">https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py</a>.</p><div class="section" title="Loading the data using the dataset_loader script"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec126"/>Loading the data using the dataset_loader script</h3></div></div></div><p>Here, we <a id="id1254" class="indexterm"/>are loading the dataset with the help of the script that we have written and understood in the last section. You can find the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_10_29.jpg" alt="Loading the data using the dataset_loader script" width="572" height="312"/><div class="caption"><p>Figure 10.29: Loading the data during the training of the model</p></div></div><p>Now let's <a id="id1255" class="indexterm"/>build the CNN that is actually used for training.</p></div><div class="section" title="Building the Convolutional Neural Network"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec127"/>Building the Convolutional Neural Network</h3></div></div></div><p>In this step, we will be building the CNN that will be used for training purposes. Here, we have three <a id="id1256" class="indexterm"/>layers of the convolutional network, the ReLU layer, and the pooling layer. The first two layers have 64 neurons, and the last one has 128 neurons. We have added the dropout layer. For some neurons, the dropout layer sets the value to zero. This layer selects the neurons that have not changed their weight for a long time, or have not been activated for a long time. This will make our training more effective. We have two fully connected layers, and one fully connected layer using the SoftMax function to derive the probability for the facial emotion class. We are using the <code class="literal">momentum</code> function for performing the gradient descent. Here, our loss function is categorical cross-entropy. Refer to the code snippet in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_30.jpg" alt="Building the Convolutional Neural Network" width="642" height="387"/><div class="caption"><p>Figure 10.30: Code snippet for building CNN</p></div></div><p>Now let's <a id="id1257" class="indexterm"/>see how to perform training.</p></div><div class="section" title="Training for the FER application"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec128"/>Training for the FER application</h3></div></div></div><p>In this step, we <a id="id1258" class="indexterm"/>need to start training so that our model can learn to predict facial emotions. In this step, we will be defining some hyperparameters for training. Refer to the code snippet in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_31.jpg" alt="Training for the FER application" width="569" height="305"/><div class="caption"><p>Figure 10.31: Code snippet for performing training</p></div></div><p>As you <a id="id1259" class="indexterm"/>can see in the preceding diagram, we have set the epoch to <code class="literal">100</code>. The training batch size is <code class="literal">50</code>. We can see the <code class="literal">shuffle </code>parameter, which acts as the flag. The value of this parameter is <code class="literal">true</code>, which indicates that we are shuffling our dataset during training.</p><p>The command to start training is <code class="literal">$ python emotion_recognition.py train</code>.</p></div><div class="section" title="Predicting and saving the trained model"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec129"/>Predicting and saving the trained model</h3></div></div></div><p>In this step, we <a id="id1260" class="indexterm"/>are defining the <code class="literal">predict </code>method. This method <a id="id1261" class="indexterm"/>helps us generate a prediction. We have also defined the method that can help us save the trained model. We need to save the model, because we can load it as and when needed for testing. You can find the code snippet in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_32.jpg" alt="Predicting and saving the trained model" width="480" height="108"/><div class="caption"><p>Figure 10.32: Code snippet for predicting the class</p></div></div><p>Refer to the code snippet in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_33.jpg" alt="Predicting and saving the trained model" width="556" height="80"/><div class="caption"><p>Figure 10.33: Code snippet for saving the trained model</p></div></div><p>Now it's time to look at the testing matrix. After that, we need to test our trained model. So, before testing our model, we should understand the testing matrix.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the testing matrix"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec108"/>Understanding the testing matrix</h1></div></div></div><p>In this section, we will look at the testing matrix for the facial emotion application. The concept of <a id="id1262" class="indexterm"/>testing is really simple. We need to start observing the training steps. We are tracking the values for loss and accuracy. Based on that, we can decide the accuracy of our model. Doesn't this sound simple? We have trained the model for 30 epochs. This amount of training requires more than three hours. We have achieved 63.88% training accuracy. Refer to the code snippet in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_34.jpg" alt="Understanding the testing matrix" width="829" height="862"/><div class="caption"><p>Figure 10.34: Training progress for getting an idea of training accuracy</p></div></div><p>This is the <a id="id1263" class="indexterm"/>training accuracy. If we want to check the accuracy on the validation dataset, then that is given in the training step as well. We have defined the validation set. With the help of this validation dataset, the trained model generates its prediction. We compare the predicted class and actual class labels. After that, we generate the validation accuracy that you can see in the preceding diagram. Here, <code class="literal">val_acc</code> is 66.37%, which is great. To date, this application has been able to <a id="id1264" class="indexterm"/>achieve up to 65 to 70% accuracy.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Testing the model"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec109"/>Testing the model</h1></div></div></div><p>Now we <a id="id1265" class="indexterm"/>need to load the trained model and test it. Here, we will be using the video stream. The FER application will detect the emotion based on my facial expression. You can refer to the code using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py">https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py</a>.</p><p>You can find the code snippet for this in the following figure:</p><div class="mediaobject"><img src="Images/B08394_10_35.jpg" alt="Testing the model" width="727" height="448"/><div class="caption"><p>Figure 10.35: Code snippet for loading the trained model and performing testing</p></div></div><p>In order to start testing, we need to execute the following command:</p><p>
<code class="literal">$ python emotion_recognition.py poc</code>
</p><p>This testing <a id="id1266" class="indexterm"/>will use your webcam. I have some demo files that I want to share here. Refer to the following figure:</p><div class="mediaobject"><img src="Images/B08394_10_36.jpg" alt="Testing the model" width="643" height="547"/><div class="caption"><p>Figure 10.36: Code snippet for FER application identifying the emotion of disgust</p></div></div><p>Also refer to the the following figure:</p><div class="mediaobject"><img src="Images/B08394_10_37.jpg" alt="Testing the model" width="642" height="482"/><div class="caption"><p>Figure 10.37:  tThe FER application identifying the happy emotion</p></div></div><p>Refer to <a id="id1267" class="indexterm"/>the code snippet in the following figure:</p><div class="mediaobject"><img src="Images/B08394_10_38.jpg" alt="Testing the model" width="642" height="482"/><div class="caption"><p>Figure 10.38: Code snippet for the FER application identifying the neutral emotion</p></div></div><p>Refer to <a id="id1268" class="indexterm"/>the code snippet given in the following figure:</p><div class="mediaobject"><img src="Images/B08394_10_39.jpg" alt="Testing the model" width="642" height="482"/><div class="caption"><p> Figure 10.39: Code snippet for the FER application identifying the angry emotion</p></div></div><p>Now let's <a id="id1269" class="indexterm"/>look at how we can improve this approach.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Problems with the existing approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec110"/>Problems with the existing approach</h1></div></div></div><p>In this section, we will <a id="id1270" class="indexterm"/>list all the points that create problems. We should try to improve them. The following are things that I feel we can improve upon:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If you find out that class sampling is not proper in your case, then you can adopt the sampling methods</li><li class="listitem" style="list-style-type: disc">We can add more layers to our neural network</li></ul></div><p>We can try different gradient descent techniques.</p><p>In this approach, training takes a lot of time that means training is computationally expensive. When we trained the model, we used GPUs <a id="id1271" class="indexterm"/>even though GPU training takes a long time. We can use multiple GPUs, but that is expensive, and a cloud instance with multiple GPUs is not affordable. So, if we can use transfer learning in this application, or use the pre-trained model, then we will achieve better results.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="How to optimize the existing approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec111"/>How to optimize the existing approach</h1></div></div></div><p>As you <a id="id1272" class="indexterm"/>have seen in the previous section, because of the lack of computation hardware, we have achieved a 66% accuracy rate. In order to improve the accuracy further, we can use the pre-trained model, which will be more convenient.</p><div class="section" title="Understanding the process for optimization"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec186"/>Understanding the process for optimization</h2></div></div></div><p>There are <a id="id1273" class="indexterm"/>a few problems that I have described in the previous sections. We can add more layers to our CNN, but that will become more computationally expensive, so we are not going to do that. We have sampled our dataset well, so we do not need to worry about that.</p><p>As part of the optimization process, we will be using the pre-trained model that is trained by using the <code class="literal">keras </code>library. This model uses many layers of CNNs. It will be trained on multiple GPUs. So, we will be using this pre-trained model, and checking how this will turn out.</p><p>In the upcoming section, we will be implementing the code that can use the pre-trained model.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="The best approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec112"/>The best approach</h1></div></div></div><p>We have <a id="id1274" class="indexterm"/>achieved approximately a 66% accuracy rate; for an FER application, the best accuracy will be approximately 69%. We will achieve this by using the pre-trained model. So, let's look at the implementation, and how we can use it to achieve the best possible outcome.</p><div class="section" title="Implementing the best approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec187"/>Implementing the best approach</h2></div></div></div><p>In this <a id="id1275" class="indexterm"/>section, we will be implementing the best possible approach for the FER application. This pre-trained model has been built by using dense and deep convolutional layers. Because of the six-layer deep CNN, and with the help of the stochastic gradient descent (SGD) technique, we can build the pre-trained model. The number of neurons for each layer were 32, 32, 64, 64, 128,128, 1,024, and 512, respectively. All layers are using ReLU as an activation function. The 3 x 3 matrix will be used to generate the initial feature map, and the 2 x 2 matrix will be used to generate the max pooling. You can download the model from this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Facial_emotion_recognition_using_Keras">https://github.com/jalajthanaki/Facial_emotion_recognition_using_Keras</a>
</p><p>You can look at the code by referring to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_10_40.jpg" alt="Implementing the best approach" width="697" height="893"/><div class="caption"><p> Figure 10. 40: Code snippet for using the pre-trained FER model</p></div></div><p>In the preceding code, we loaded the pre-trained <code class="literal">keras </code>model. We are providing two provisions. We can use this script for detecting the facial expression from the image as well as <a id="id1276" class="indexterm"/>by providing the video stream.</p><p>If you want to test the facial expression present in any image, then we need to execute the <code class="literal">$ python image_test.py tes.jpg</code> command. I have applied it to this model on the <code class="literal">tes.jpg</code> image. You can see the output image as follows:</p><div class="mediaobject"><img src="Images/B08394_10_41.jpg" alt="Implementing the best approach" width="738" height="914"/><div class="caption"><p>Figure 10.41: Output of the FER application for the image</p></div></div><p>If you want to test the model for the video stream, then you need to execute this command: <code class="literal">$python realtime_facial_expression.py</code>.</p><p>Refer to <a id="id1277" class="indexterm"/>the output in the following figure:</p><div class="mediaobject"><img src="Images/B08394_10_42.jpg" alt="Implementing the best approach" width="642" height="482"/><div class="caption"><p>Figure 10.42: Output of the FER application for the video stream</p></div></div><p>You can find the output file in the following figure:</p><div class="mediaobject"><img src="Images/B08394_10_43.jpg" alt="Implementing the best approach" width="642" height="482"/><div class="caption"><p>Figure 10.43: Output of the FER application for the video stream</p></div></div><p>This application <a id="id1278" class="indexterm"/>provides us with approximately 67% accuracy, which is great.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec113"/>Summary</h1></div></div></div><p>In this chapter, we looked at how to develop the face detection application using the <code class="literal">face_recognition</code> library, which uses the HOG-based model to identify the faces in the images. We have also used the pre-trained convolutional neural network, which identifies the faces from a given image. We developed real-time face recognition to detect the names of people. For face recognition, we used a pre-trained model and already available libraries. In the second part of the chapter, we developed the face emotion recognition application, which can detect seven major emotions a human face can carry. We used <code class="literal">TensorFlow</code>, <code class="literal">OpenCV</code>, <code class="literal">TFLearn</code>, and <code class="literal">Keras</code> in order to build the face emotion recognition model. This model has fairly good accuracy for predicting the face emotion. We achieved the best possible accuracy of 67%.</p><p>Currently, the computer vision domain is moving quickly in terms of research. You can explore many fresh and cool concepts, such as <code class="literal">deepfakes </code>and 3D human pose estimation (machine vision) by the Facebook AI Research group. You can refer to the <code class="literal">deepfakes </code>GitHub repository by clicking here: <a class="ulink" href="https://github.com/deepfakes/faceswap">https://github.com/deepfakes/faceswap</a>. You can refer to the paper on 3D human pose estimation by clicking on this link: <a class="ulink" href="https://arxiv.org/pdf/1705.03098.pdf">https://arxiv.org/pdf/1705.03098.pdf</a>. Both the concepts are new and fresh, so you can refer to them and make some fun applications.</p><p>The next chapter will be the last chapter of this book. In the course of the chapter, we will try to make a gaming bot. This chapter will heavily use reinforcement learning techniques. We will be developing a bot that can play Atari games on its own. So keep reading!</p></div></div>



  </body></html>