- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Foundations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础
- en: '**Emotions** play a key role in our daily lives. Some people define them as
    the reactions that we as human beings experience as a response to events or situations,
    some describe them simply as a class of feelings, and others say they describe
    physiological states and are generated subconsciously. Psychologists describe
    emotions as “*a complex state of feeling that results in physical and psychological
    changes that influence thought and behavior.*” So, it appears that although we
    feel emotions, they are much harder to describe.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**情绪**在我们的日常生活中起着关键作用。有些人将它们定义为人类对事件或情况的反应，有些人简单地描述它们为一种情感类别，而有些人说它们描述生理状态，并且是无意识地产生的。心理学家将情绪描述为“*一种复杂的情感状态，导致身体和心理变化，从而影响思维和行为。*”因此，尽管我们感受到情绪，但它们很难描述。'
- en: Our brains play a crucial role when creating and processing emotions. Historically,
    it was believed that each emotion was located in a specific part of the brain.
    However, research has shown that there is no single region of the brain that’s
    responsible for processing emotions – several brain regions are activated when
    emotions are being processed. Furthermore, different parts of the brain can generate
    the same emotion and different parts can also contribute to generating an emotion.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑在创造和处理情绪时起着至关重要的作用。历史上，人们认为每种情绪都位于大脑的特定部分。然而，研究表明，没有单一的大脑区域负责处理情绪——当处理情绪时，多个大脑区域会被激活。此外，大脑的不同部分可以产生相同的情绪，不同的部分也可以有助于产生情绪。
- en: The reality may even be that *emotion* and *sentiment* are experiences that
    result from combined influences of biological, cognitive, and social aspects.
    Whatever the case, emotions matter because they help us decide what actions to
    do, how to negotiate tricky situations, and, at a basic level, how to survive.
    Different emotions rule our everyday lives; for example, we make decisions based
    on whether we are happy, angry, or sad, and we choose our daily pastimes and routines
    based on the emotions they facilitate. So, emotions are important, and understanding
    them may make our lives easier.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，*情绪*和*情感*可能是由于生物、认知和社会方面的综合影响而产生的体验。无论情况如何，情绪都很重要，因为它们帮助我们决定采取什么行动，如何谈判复杂的情况，以及在基本层面上如何生存。不同的情绪统治着我们的日常生活；例如，我们根据我们是快乐、愤怒还是悲伤来做出决定，我们根据它们促进的情绪选择我们的日常娱乐和例行公事。因此，情绪很重要，理解它们可能会让我们的生活变得更简单。
- en: In this chapter, you will learn about the main concepts and differences between
    sentiment analysis and emotion analysis, and also understand why emotion analysis
    is important in the modern world. By combining this with a basic introduction
    to **natural language processing** (**NLP**) and machine learning, we will lay
    the foundations for successfully using these techniques for emotion analysis.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解情感分析和情感分析之间的主要概念和区别，并了解为什么情感分析在现代世界中很重要。通过结合对**自然语言处理**（**NLP**）和机器学习的基本介绍，我们将为成功使用这些技术进行情感分析奠定基础。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Emotions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情绪
- en: Sentiment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感
- en: Why is emotion analysis important?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么情感分析很重要？
- en: Introduction to natural language processing
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理简介
- en: Introduction to machine learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: Emotions
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情绪
- en: This book is about writing programs that can detect emotions expressed in texts,
    particularly informal texts. Emotions play a crucial role in our daily lives.
    They impact how we feel, how we think, and how we behave. Consequently, it stands
    to reason that they impact the decisions we make. If this is the case, then being
    able to detect emotions from written text (for example, social media posts) is
    a useful thing to do because the impact it would have on many practical everyday
    applications in sectors such as marketing, industry, health, and security would
    be huge.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是关于编写能够检测文本中表达的情绪的程序，尤其是非正式文本。情绪在我们的日常生活中起着至关重要的作用。它们影响我们的感受、我们的思考和我们的行为。因此，可以合理地认为它们会影响我们的决策。如果情况是这样的话，那么能够从书面文本（例如社交媒体帖子）中检测情绪是有用的，因为它将对许多实际日常应用产生巨大影响，这些应用领域包括营销、工业、健康和安全。
- en: However, while it is clear that we all experience emotions and that they play
    a significant role in our plans and actions, it is much less clear what they *are*.
    Given that we are about to embark on a detailed study of how to write programs
    to detect them, it is perhaps worth beginning by investigating the notion of what
    an emotion is and looking at the various theories that attempt to pin them down.
    This is a topic that has fascinated philosophers and psychologists from antiquity
    to the present day, and it is still far from settled. We will briefly look at
    a number of the most prominent theories and approaches. This overview will not
    lead us to a definitive view, but before we start trying to identify them in written
    texts, we should at least become aware of the problems that people still have
    in pinning them down.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虽然很清楚我们都会体验到情绪，并且它们在我们的计划和行动中起着重要作用，但它们究竟是什么却并不那么清楚。鉴于我们即将开始详细研究如何编写程序来检测它们，也许首先调查一下情绪是什么的概念，并看看试图将它们固定下来的各种理论是值得的。这是一个从古代到现代一直吸引着哲学家和心理学家的主题，而且至今仍未有定论。我们将简要地看看一些最突出的理论和方法。这个概述不会引导我们得出一个明确的观点，但在我们开始尝试在书面文本中识别它们之前，我们至少应该意识到人们在固定它们时仍然存在的问题。
- en: Darwin believed that emotions allowed humans and animals to survive and reproduce.
    He argued that they evolved, were adaptive, and that all humans, and even other
    animals, expressed emotion through similar behaviors. He believed that emotions
    had an evolutionary history that could be traced across cultures and species.
    Today, psychologists agree that emotions such as fear, surprise, disgust, happiness,
    and sadness can be regarded as universal regardless of culture.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 达尔文认为，情绪使人类和动物能够生存和繁衍。他论证说，情绪是进化的，具有适应性，所有人类，甚至其他动物，都通过类似的行为表达情绪。他相信情绪有一个可以跨越文化和物种追溯的进化历史。今天，心理学家们一致认为，恐惧、惊讶、厌恶、快乐和悲伤等情绪可以被视为普遍存在的，无论文化如何。
- en: The James-Lange theory proposes that it is our physical responses that are responsible
    for emotions. For example, if someone jumps out at you from behind a bush, your
    heart rate will increase, and it is this increase that causes the individual to
    feel fear. The facial-feedback theory builds on this idea and suggests that physical
    activity is responsible for influencing emotion, for example, if you smile, likely,
    you will automatically feel happier than if you did not smile. However, Cannon-Bard’s
    theory refutes James-Lange, instead suggesting that people experience emotional
    and physical responses simultaneously. The Schachter-Singer theory is a cognitive
    theory of emotion that suggests that it is our thoughts that are responsible for
    emotions, and similarly, cognitive appraisal theory suggests that thinking must
    come before experiencing an emotion. For instance, the brain might understand
    a situation as threatening, and hence fear is experienced.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 詹姆斯-兰格理论提出，情绪的产生是由我们的生理反应所负责的。例如，如果有人从灌木丛后突然跳出来吓你，你的心率会增加，正是这种增加导致了个体感到恐惧。面部反馈理论在此基础上进一步提出，身体活动是影响情绪的原因，例如，如果你微笑，你可能会自动感到比不微笑时更快乐。然而，坎农-巴德理论反驳了詹姆斯-兰格理论，反而提出人们同时体验到情绪和生理反应。沙赫特-辛格理论是一种情绪的认知理论，它认为情绪的产生是由我们的思想所负责的，同样，认知评估理论也提出，思考必须先于体验情绪。例如，大脑可能将某种情况理解为威胁，因此产生了恐惧感。
- en: 'To try to obtain a deeper understanding of emotions, let’s look at the three
    main theories of emotion:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试更深入地理解情绪，让我们来看看情绪的三个主要理论：
- en: '**Physiological**: Psychologists have the view that emotions are formed when
    a bodily response is triggered by a stimulus, so as the individual experiences
    physiological changes, this is also experienced as an emotion'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生理学**：心理学家认为，情绪是在生理反应被刺激触发时形成的，因此，当个体经历生理变化时，这也被体验为情绪。'
- en: '**Neurological**: Biologists claim that hormones (for example, estrogen, progesterone,
    and testosterone) that are produced by the body’s glands impact the chemistry
    and circuitry of the brain and these lead to emotional responses'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经学**：生物学家声称，由身体腺体产生的激素（例如，雌激素、孕酮和睾酮）会影响大脑的化学和电路，这些影响导致了情绪反应。'
- en: '**Cognitive**: Cognitive scientists believe that thoughts and other mental
    activities play a crucial role in forming emotions'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**认知**：认知科学家认为，思想和其他心理活动在形成情绪中起着至关重要的作用。'
- en: In all likelihood, all three theories are probably valid to some extent. It
    has also been postulated that instead of thinking of these as mutually exclusive,
    it is more likely that they are complementary and that each explains and accounts
    for a different aspect of what we think of as an emotion.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，这三个理论在某种程度上都是有效的。也有人提出，与其将这些理论视为相互排斥的，不如认为它们更可能是互补的，并且每个理论都解释和说明了我们认为是情感的不同方面。
- en: Although emotions have been studied for many decades, it is probably still true
    that we still do not fully understand emotions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管情感已经研究了几十年，但很可能我们仍然没有完全理解情感。
- en: Humans can experience a huge number of emotions, but only a handful are considered
    basic. However, the number of emotions considered in emotion analysis research
    is not always limited to just these basic emotions. Furthermore, it is not straightforward
    to demarcate emotions, and hence boundaries are very rarely clearly defined.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 人类可以体验到大量的情感，但只有少数被认为是基本的。然而，在情感分析研究中考虑的情感数量并不总是仅限于这些基本情感。此外，区分情感并不简单，因此边界很少被明确界定。
- en: 'We will now consider what are known as the *primary emotions*. These have been
    described as a reaction to an event or situation, or the immediate strong first
    reaction experienced when something happens. There has been much research on identifying
    these primary emotions, but there is still no general agreement, and different
    models have been suggested by eminent researchers such as Ekman, Plutchik, and
    Parrot. Some emotions such as anger, fear, joy, and surprise are universally agreed
    upon. However, the same is not true for other emotions, with disagreements on
    the emotions that constitute the basic emotions and the number of these emotions.
    Although there is, again, no consensus on which model is best at covering basic
    emotions, the models proposed by Ekman and Plutchik are most commonly used. There
    are two popular approaches: **categorical** and **dimensional**.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将考虑所谓的*基本情感*。这些情感被描述为对事件或情况的反应，或者当某事发生时立即强烈的第一反应。在识别这些基本情感方面已经进行了大量研究，但仍然没有达成普遍共识，杰出的研究人员如Ekman、Plutchik和Parrot提出了不同的模型。一些情感，如愤怒、恐惧、快乐和惊讶，是普遍认可的。然而，对于其他情感，关于构成基本情感的情感和这些情感的数量存在分歧。尽管再次没有关于哪个模型最适合涵盖基本情感的共识，但Ekman和Plutchik提出的模型最常被使用。有两种流行的方法：**类别**和**维度**。
- en: Categorical
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类别
- en: Ekman is an advocate of the categorical theory, which suggests that emotions
    arise from separate neural systems. This approach also suggests that there are
    a limited number of primary, distinct emotions, such as anger, anxiety, joy, and
    sadness. Ekman suggested that primary emotions must have a distinct facial expression
    that is recognizable across all cultures. For example, the corners of the lips
    being turned down demonstrates sadness – and this facial expression is recognized
    universally as portraying sadness. Similarly, smiling with teeth exposed and the
    corners of the mouth pointing upwards is universally recognized as joy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Ekman是类别理论的倡导者，该理论认为情感源于不同的神经网络系统。这种方法还表明，存在有限数量的基本、独特的情感，如愤怒、焦虑、快乐和悲伤。Ekman提出，基本情感必须具有在所有文化中都能识别的独特面部表情。例如，嘴唇角向下弯曲表示悲伤——这种面部表情被普遍认为是描绘悲伤的。同样，露出牙齿微笑，嘴角向上，被普遍认为是快乐。
- en: 'Amazingly, people blind from birth use the same facial expressions when expressing
    sadness and joy. They have never seen these facial expressions, so it is impossible
    that these expressions were learned. It is much more likely that these are an
    integral part of human nature. Using this understanding of distinct, universal
    facial expressions, Ekman proposed six primary emotions (Ekman, 1993):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，从出生就失明的人表达悲伤和快乐时使用相同的面部表情。他们从未见过这些表情，因此这些表情不可能被学习。更有可能的是，这些是人性的一部分。利用对独特、普遍面部表情的理解，Ekman提出了六种基本情感（Ekman，1993）：
- en: Anger
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 愤怒
- en: Disgust
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 厌恶
- en: Fear
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恐惧
- en: Joy
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快乐
- en: Sadness
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 悲伤
- en: Surprise
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 惊讶
- en: Ekman suggested that these *basic* emotions were biologically primitive and
    have evolved to increase the reproductive fitness of animals and that all other
    emotions were combinations of these eight primary emotions. Later, Eckman expanded
    this list to include other emotions that he considered basic, such as embarrassment,
    excitement, contempt, shame, pride, satisfaction, and amusement.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 艾克曼认为这些*基本*情绪在生物学上是原始的，并且已经进化以增加动物的繁殖适应性，而且所有其他情绪都是这八种基本情绪的组合。后来，艾克曼扩大了这个列表，包括他认为的基本情绪，如尴尬、兴奋、轻蔑、羞愧、自豪、满意和娱乐。
- en: 'Another of the most influential works in the area of emotions is Plutchik’s
    psychoevolutionary theory of emotion. Plutchik proposed eight primary emotions
    (Plutchik, 2001):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在情绪领域最具影响力的另一项工作是普鲁奇克的情绪心理进化理论。普鲁奇克提出了八种基本情绪（普鲁奇克，2001年）：
- en: Anger
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 愤怒
- en: Anticipation
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期待
- en: Disgust
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 厌恶
- en: Fear
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恐惧
- en: Joy
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快乐
- en: Sadness
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 悲伤
- en: Surprise
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 惊讶
- en: Trust
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信任
- en: From this theory, Plutchik developed a Wheel of Emotions (see *Figure 1**.1*).
    This wheel was developed to help understand the nuances of emotion and how emotions
    contrast. It has eight sectors representing the eight emotions. Emotions intensify
    as they move from outside toward the center of the wheel. For example, annoyance
    increases to anger and then further increases to outright rage. Each sector of
    the circle has an opposite emotion that is placed directly opposite in the wheel.
    For example, the opposite of sadness is joy, and the opposite of anger is fear.
    It also shows how different emotions can be combined.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个理论出发，普鲁奇克发展了一个情绪轮（见图*1**.1*）。这个轮子是为了帮助理解情绪的细微差别以及情绪如何对比而开发的。它有八个区域代表八种情绪。情绪随着它们从轮子的外围向中心移动而增强。例如，烦恼增加到愤怒，然后进一步增加到彻底的愤怒。圆圈的每个区域都有一个直接相对的相反情绪，位于轮子的直接对面。例如，悲伤的对立面是快乐，愤怒的对立面是恐惧。它还显示了不同情绪如何结合。
- en: '![Figure 1.1 – Plutchik’s Wheel of Emotions](img/B18714_01_01.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – 普鲁奇克情绪轮](img/B18714_01_01.jpg)'
- en: Figure 1.1 – Plutchik’s Wheel of Emotions
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 普鲁奇克情绪轮
- en: Although Ekman and Plutchik’s theories are the most common, there are other
    works, but there is little agreement on what the basic emotions are. However,
    in the area of emotion analysis research, Ekman and Plutchik’s models are the
    most often used classification schemes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然艾克曼和普鲁奇克的理论是最常见的，但还有其他作品，但对于基本情绪是什么并没有达成共识。然而，在情绪分析研究领域，艾克曼和普鲁奇克的模型是最常用的分类方案。
- en: Dimensional
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度
- en: The dimensional approach posits that to understand emotional experiences, the
    fundamental dimensions of valence (the *goodness* and *badness* of the emotion)
    and arousal (the *intensity* of the emotion) are vital. This approach suggests
    that a common and interconnected neurophysiological system is responsible for
    all affective states. Every emotion can then be defined in terms of these two
    measures, so the plane can be viewed as a continuous two-dimensional space, with
    dimensions of valence and arousal, and each point in the place corresponds to
    a separate emotion state.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 维度方法认为，为了理解情绪体验，情绪的基本维度（情绪的*好坏*）和唤醒（情绪的*强度*）是至关重要的。这种方法表明，一个共同的、相互关联的神经生理系统负责所有情感状态。每个情绪都可以用这两个指标来定义，因此平面可以被视为一个连续的两维空间，其维度为情绪的好坏和唤醒，空间中的每个点都对应一个单独的情绪状态。
- en: '![Figure 1.2 – Russell’s circumplex model](img/B18714_01_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – 拉塞尔的环状模型](img/B18714_01_02.jpg)'
- en: Figure 1.2 – Russell’s circumplex model
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 拉塞尔的环状模型
- en: 'The most common dimensional model is Russell’s circumplex model ((Russell,
    1980): see *Figure 1**.2*). The model posits that emotions are made up of two
    core dimensions: valence and arousal. *Figure 1**.2* shows that valence ranges
    from −1 (unpleasant) to 1 (pleasant), and arousal also ranges from −1 (calm) to
    1 (excited). Each emotion is then a linear combination of these two dimensions.
    For example, anger is an unpleasant emotional state (a negative valence) with
    a high intensity (a positive arousal). Other basic emotions can be seen in *Figure
    1**.2* with their approximate positions in the two-dimensional space.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的维度模型是拉塞尔的环状模型（拉塞尔，1980年：见图*1**.2*）。该模型认为，情绪由两个核心维度组成：情绪的好坏和唤醒。*图1**.2*显示，情绪的好坏范围从-1（不愉快）到1（愉快），唤醒也范围从-1（平静）到1（兴奋）。然后每个情绪都是这两个维度的线性组合。例如，愤怒是一种不愉快的情绪状态（负面的情绪好坏）具有高强度（正面的唤醒）。其他基本情绪可以在*图1**.2*中看到，它们在二维空间中的大致位置。
- en: Some emotions have similar arousal and valence (for example, grief and rage).
    Hence, a third dimension (control) has also been suggested that can be used to
    distinguish between these. Control ranges from *no control* to *full control*.
    So, the entire range of human emotions can be represented as a set of points in
    the three-dimensional space using these three dimensions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一些情感具有相似的唤醒和效价（例如，悲伤和愤怒）。因此，还提出了第三个维度（控制），可以用来区分这些情感。控制范围从*无控制*到*完全控制*。因此，使用这三个维度，整个人类情感的整个范围都可以表示为三维空间中的一组点。
- en: The dimensional model has a poorer resolution of emotions; that is, it is harder
    to distinguish between ambiguous emotions. The categorical model is simpler to
    understand, but some emotions are not part of the set of basic emotions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 维度模型在情感分辨率上较差；也就是说，区分模糊情感更困难。分类模型更容易理解，但有些情感并不属于基本情感集合。
- en: Most emotion analysis research uses a categorical perspective; there seems to
    be a lack of research using the dimensional approach.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情感分析研究都采用分类视角；似乎缺乏使用维度方法的研究。
- en: Sentiment
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感
- en: There is a second closely-related term known as **sentiment**. The terms sentiment
    and emotion seem to be used in an ad hoc manner, with different writers using
    them almost interchangeably. Given the difficulty we have found in working out
    what emotions are, and in deciding exactly how many emotions there are, having
    yet another ill-defined term is not exactly helpful. To try to clarify the situation,
    note that when people work on sentiment mining, they generally make use of a simple,
    limited system of classification using *positive*, *negative*, and *neutral* cases.
    This is a much simpler scheme to process and ascertain, and yields results that
    are also easier to understand. In some ways, emotion analysis may be regarded
    as an *upgrade* to sentiment analysis; a more complex solution that analyzes much
    more than the simple positive and negative markers and instead tries to determine
    specific emotions (anger, joy, sadness). This may be more useful but also involves
    much more effort, time, and cost. Emotion and sentiment are, thus, not the same.
    An emotion is a complex psychological state, whereas a sentiment is a mental attitude
    that is created through the very existence of the emotion.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个与之密切相关的术语，称为**情感**。情感和情绪这两个术语似乎是以一种临时的方式使用的，不同的作者几乎可以互换使用。鉴于我们在确定情感是什么以及确切有多少情感方面遇到的困难，再加上另一个定义不明确的术语并不完全有帮助。为了尝试澄清情况，请注意，当人们从事情感挖掘时，他们通常使用一个简单、有限的分类系统，包括*积极*、*消极*和*中性*案例。这是一个更简单的方案，更容易处理和确认，并且产生的结果也更容易理解。在某种程度上，情感分析可能被视为对情感分析的*升级*；一个更复杂的解决方案，它分析的内容远不止简单的积极和消极标记，而是试图确定特定的情感（愤怒、快乐、悲伤）。这可能更有用，但也需要更多的努力、时间和成本。因此，情感和情绪不是同一个概念。情感是一种复杂的心理状态，而情绪是通过情感的存在而产生的心理态度。
- en: For us, sentiment refers exclusively to an expressed opinion that is positive,
    negative, or neutral. There is some degree of overlap here because, for example,
    emotions such as joy and love could both be considered positive sentiments. It
    may be that the terms simply have different granularity – in the same way that
    ecstasy, joy, and contentment provide a fine-grained classification of a single
    generic emotion class that we might call happiness, happiness and love are a fine-grained
    classification of the general notion of feeling positive. Alternatively, it may
    be that sentiment is the name for one of the axes in the dimensional model – for
    example, the valence axis in Russell’s analysis. Given the range of theories of
    emotion, it seems best to just avoid having another term for much the same thing.
    In this book, we will stick to the term emotion; we will take an entirely pragmatic
    approach by accepting some set of labels from an existing theory such as Plutchik’s
    or Russell’s as denoting emotions, without worrying too much about what it is
    that they denote. We can all agree that *I hate the people who did that and I
    wish they were all dead* expresses hate and anger, and that it is overall negative,
    even if we’re not sure what hate and anger are or what the scale from negative
    to positive actually measures.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，情感仅指表达出的正面、负面或中性的观点。在这里有一些重叠，因为例如快乐和爱都可以被视为积极的情感。可能这些术语只是具有不同的粒度——就像狂喜、快乐和满足提供了对单一通用情感类别（我们可能称之为快乐）的精细分类一样，快乐和爱是对积极感受这一普遍概念的精细分类。或者，情感可能是维度模型中的一个轴的名称——例如，在拉塞尔的分析中的效价轴。鉴于情感理论的多样性，似乎最好的做法是避免为几乎相同的事物使用另一个术语。在这本书中，我们将坚持使用“情感”这个术语；我们将采取完全实用主义的方法，接受来自现有理论（如普拉奇克或拉塞尔的理论）的一些标签，作为表示情感的标志，而不太关心它们究竟代表什么。我们都可以同意，“*我恨那些做那种事的人，我希望他们都死了*”表达了仇恨和愤怒，而且整体上是负面的，即使我们不确定仇恨和愤怒是什么，或者从负面到正面的尺度实际上衡量的是什么。
- en: Now that we know a bit more about what emotion is and how it is categorized
    and understood, it is essential to understand why emotion analysis is an important
    topic.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对情感有了更多的了解，知道了它是如何被分类和理解的，因此理解情感分析为何是一个重要的话题是至关重要的。
- en: Why emotion analysis is important
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析为何重要
- en: The amount of data generated daily from online sources such as social media
    and blogs is staggering. In 2019, Forbes estimated this to be around 2.5 quintillion
    bytes of data, though this figure is more
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 来自社交媒体和博客等在线来源的每日数据量令人震惊。2019年，《福布斯》估计这一数字约为2.5千兆字节的数据，尽管现在这个数字可能更高。
- en: than likely even higher now. Due to this, much research has focused on using
    this data for analysis and for gaining hitherto unknown insights (for example,
    predicting flu trends and disease outbreaks using Twitter (now known as “X”) data).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 更有可能的是现在这个数字甚至更高。因此，许多研究都集中在利用这些数据进行分析，以及获得迄今为止未知的新见解（例如，使用Twitter（现称为“X”）数据预测流感趋势和疾病爆发）。
- en: Similarly, people are also increasingly expressing their opinions online – and
    many of these opinions are, explicitly or implicitly, highly emotional (for example,
    *I love summer*). Nowadays, social network platforms such as Facebook, LinkedIn,
    and Twitter are at the hub of everything we do. Twitter is one of the most popular
    social network platforms, with more than 300 million users using Twitter actively
    every month. Twitter is used by people from all walks of life; celebrities, movie
    stars, politicians, sports stars, and everyday people. Users post short messages,
    known as **tweets**, and, every day, millions share their opinions about themselves,
    news, sports, movies, and other topics. Consequently, this makes platforms such
    as Twitter rich sources of data for public opinion mining and sentiment analysis.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，人们也越来越倾向于在网上表达自己的观点——其中许多观点，无论是明确还是隐晦，都充满了强烈的情感（例如，*我爱夏天*）。如今，像Facebook、LinkedIn和Twitter这样的社交网络平台已经成为我们所有活动的中心。Twitter是最受欢迎的社交网络平台之一，每月有超过3亿用户活跃使用Twitter。来自各行各业的人们都在使用Twitter；名人、电影明星、政治家、体育明星以及普通人。用户发布简短的消息，称为**推文**，每天数百万的人分享他们对自身、新闻、体育、电影和其他主题的看法。因此，这使得像Twitter这样的平台成为了公众舆论挖掘和情感分析的数据宝库。
- en: As we have seen, emotions play an important role in human intelligence, decision-making,
    social interaction, perception, memory, learning, creativity, and much much more.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，情感在人类智能、决策、社会互动、感知、记忆、学习、创造力以及更多方面发挥着重要作用。
- en: 'Emotion analysis is the process of recognizing the emotions that are expressed
    through texts (for example, social media posts). It is a complex task because
    user-generated content, such as tweets, is typically understood as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是通过文本（例如，社交媒体帖子）识别表达出的情绪的过程。这是一个复杂的工作，因为用户生成的内容，如推文，通常被理解为如下：
- en: Written in natural language
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用自然语言书写
- en: Often unstructured, informal, and misspelled
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常是非结构化、非正式和拼错的
- en: Can contain slang and made-up words
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以包含俚语和虚构的单词
- en: Can contain emojis and emoticons where their usage does not always correspond
    to the reason for their original creation (for example, using the pizza emoji
    to express love)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以包含表情符号和表情符号，它们的用法并不总是与其原始创建的原因相对应（例如，使用披萨表情符号来表达爱）
- en: Furthermore, it is also entirely possible to express emotion without using any
    obvious emotion markers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，完全有可能在不使用任何明显的情感标记的情况下表达情感。
- en: 'One of the big unsolved problems in emotion analysis is detecting emotions
    such as anticipation, pessimism, and sarcasm. Consider the following tweet:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析中尚未解决的重大问题之一是检测诸如期待、悲观和讽刺等情绪。考虑以下推文：
- en: '*We lost* *again. Great.*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们* *又输了*。太棒了。'
- en: We humans are fairly knowledgeable when it comes to drilling down to the true
    meaning implied, and would understand that the user was being sarcastic. We know
    that a team losing again is not a good thing. Hence, by making use of this understanding,
    we can easily identify the implied meaning.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们人类在深入挖掘隐含的真实意义方面相当有知识，会理解用户是在讽刺。我们知道一支球队再次输球不是好事。因此，通过利用这种理解，我们可以轻松地识别隐含的意义。
- en: The problem is that simply considering each word that has sentiment in isolation
    will not do a good job. Instead, further rules must be applied to understand the
    context of the word. These rules will help the analyzer differentiate between
    sentences that might contain similar words but have completely different meanings.
    However, even with these rules, analyzers will still make mistakes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，仅仅考虑具有情感色彩的每个单词而孤立地看待它们，并不能做好这项工作。相反，必须应用进一步的规则来理解单词的上下文。这些规则将帮助分析者区分可能包含相似单词但意义完全不同的句子。然而，即使有了这些规则，分析者仍然会犯错误。
- en: Social media is now viewed as a valuable resource, so organizations are showing
    an increased interest in social media monitoring to analyze massive, free-form,
    short, user-generated text from social
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体现在被视为一项宝贵的资源，因此组织对社交媒体监控的兴趣日益增加，以分析来自社交媒体的大量、自由形式的简短、用户生成文本。
- en: media sites. Exploiting these allows organizations to gain insights into understanding
    their customer’s opinions, concerns, and needs about their products and services.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 媒体网站。利用这些网站可以让组织深入了解客户对其产品和服务意见、担忧和需求。
- en: Due to its real-time nature, governments are also interested in using social
    media to identify threats and monitor and analyze public responses to current
    events.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其实时性，政府也对使用社交媒体来识别威胁、监控和分析对当前事件的公众反应感兴趣。
- en: 'Emotion analysis has many interesting applications:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析有许多有趣的应用：
- en: '`#ShareACoke` by Coca-Cola, `#WantAnR8` by Audi, and `#BeTheFastest` by Virgin
    Media.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可口可乐的`#ShareACoke`，奥迪的`#WantAnR8`，以及维珍媒体的`#BeTheFastest`。
- en: '**Stock markets**: Academics have attempted to use Twitter to anticipate trends
    in financial markets. In 2013, the Associated Press Twitter account posted a (false)
    tweet stating that there had been explosions in the White House and that Obama
    was injured. The post was debunked very quickly but the stock markets still took
    a nosedive, resulting in hundreds of billions of dollars changing hands.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**股市**：学者们试图利用推特来预测金融市场趋势。2013年，美联社的推特账号发布了一条（虚假的）推文，称白宫发生了爆炸，奥巴马受伤。该帖子很快就被辟谣了，但股市仍然暴跌，导致数百亿美元易手。'
- en: '**Social studies**: Millions of people regularly interact with the world by
    tweeting, providing invaluable insights into their feelings, actions, routines,
    emotions, and behavior. This vast amount of public communication can be used to
    generate forecasts of various types of events. For example, large-scale data analysis
    of social media has demonstrated that not only did Brexit supporters have a more
    powerful, emotional message, but they were also more effective in the use of social
    media. They routinely outmuscled their rivals and had more vocal and active supporters
    across nearly all social media platforms. This led to the activation of a greater
    number of Leave supporters and enabled them to dominate social media platforms
    – thus influencing many undecided voters.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社会科学研究**：数百万的人通过发推文与世界互动，提供了关于他们的感受、行为、日常习惯、情绪和行为的有价值见解。如此大量的公共沟通可以用来预测各种类型的事件。例如，对社交媒体的大规模数据分析表明，不仅脱欧支持者有一个更有力、更具情感的信息，他们在社交媒体的使用上也非常有效。他们通常在几乎所有社交媒体平台上都压倒了对手，拥有更多声音和活跃的支持者。这导致了更多脱欧支持者的激活，并使他们能够主导社交媒体平台——从而影响了许多未决定投票的选民。'
- en: Gaining an understanding of emotions is also important for organizations to
    gain insights into public opinion about their products and services. However,
    it is also important to automate this process so that decisions can be made and
    actions can be taken in real-time. For example, analysis techniques can automatically
    analyze and process thousands of reviews about a particular product and extract
    insights that show whether consumers are satisfied with the product or service.
    This can be sentiment or emotion, although emotion may be more useful due to it
    being more granular.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 理解情感对于组织来说也很重要，以便深入了解公众对其产品和服务的态度。然而，自动化这一过程也很重要，以便能够实时做出决策并采取行动。例如，分析技术可以自动分析和处理关于特定产品的数千条评论，并提取出显示消费者是否对产品或服务满意的见解。这可能包括情感或情绪，尽管由于它更细致，情感可能更有用。
- en: Research has shown that tweets posted by dissatisfied users are shared more
    often and spread faster and wider than other types of tweets. Therefore, organizations
    have to provide customer services beyond the old-fashioned agent at the end of
    the phone line. Due to this, many organizations today also provide social media-based
    customer support in an attempt to head-off bad reviews and give a good impression.
    Nowadays, there is so much consumer choice, and it is so much easier for customers
    to switch to competitors, that it is vitally important for organizations to retain
    and increase their customer base. Hence, the quicker an organization reacts to
    a bad post, the better chance they have
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，不满意的用户发布的推文比其他类型的推文更频繁地被分享，传播得更快、更广。因此，组织必须提供超越传统电话线末端的客服。因此，许多组织今天也提供基于社交媒体的客户支持，试图阻止负面评论并留下好印象。如今，消费者选择如此之多，客户转向竞争对手变得如此容易，因此，组织保留并增加其客户群至关重要。因此，组织对负面帖子的反应越快，他们获得的机会就越好。
- en: of retaining the customer. Furthermore, there is no better advertising than
    word of mouth – such as that generated by happy customers. Emotion analysis is
    one way to quickly analyze hundreds of tweets, find the ones where customers are
    unhappy, and use this to drive other processes that attempt to resolve the problem
    before the customer becomes too unhappy and decides to take their business elsewhere.
    Emotion analysis not only requires data – it also generates a lot of data. This
    data can be further analyzed to determine, for example, what the top items on
    user wishlists are, or what the top user gripes are. These can then be used to
    drive the next iteration or version of the product or service.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 保留客户的重要性。此外，没有比口碑更好的广告了——比如由满意的客户产生的口碑。情感分析是快速分析数百条推文、找出客户不满意的推文，并利用这些信息来推动其他试图在客户变得过于不高兴并决定将业务转移到其他地方之前解决问题的流程的一种方法。情感分析不仅需要数据——它还会生成大量数据。这些数据可以进一步分析，例如，确定用户愿望清单上的顶级项目，或者确定用户的主要抱怨。这些信息可以用来推动产品或服务的下一迭代或版本。
- en: Although sentiment analysis and emotion analysis are not mutually exclusive
    and can be used in conjunction, the consensus is that sentiment analysis is not
    adequate for classifying something as complex, multi-layered, and nuanced as emotion.
    Simply taking the whole range of emotions and considering them as only positive,
    negative, or neutral runs the considerable risk of missing out on deeper insights
    and understandings.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管情感分析和情绪分析并非相互排斥，并且可以结合使用，但普遍观点认为，情感分析不足以对像情感这样复杂、多层次、细微的事物进行分类。简单地将整个情感范围视为只有积极、消极或中性，存在很大的风险，可能会错过更深层次的洞察和理解。
- en: Emotion analysis also provides more in-depth insights. Understanding why someone
    ignored or liked a post needs more than just a sentiment score. Furthermore, gaining
    *actionable* insights also requires more than just a sentiment score.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 情绪分析也提供了更深入的洞察。理解为什么有人忽略或喜欢某个帖子，需要的不仅仅是情感分数。此外，获得*可操作的*洞察也需要不仅仅是情感分数。
- en: Emotion analysis is a sub-field of NLP, so it makes sense to gain a better understanding
    of that next.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 情绪分析是自然语言处理的一个子领域，因此，更好地理解这一点是有意义的。
- en: Introduction to NLP
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）简介
- en: 'Sentiment mining is about finding the sentiments that are expressed by natural
    language texts – often quite short texts such as tweets and online reviews, but
    also larger items such as newspaper articles. There are many other ways of getting
    computers to do useful things with natural language texts and spoken language:
    you can write programs that can have conversations (with people or with each other),
    you can write programs to extract facts and events from articles and stories,
    you can write programs to translate from one language to another, and so on. These
    applications all share some basic notions and techniques, but they each lay more
    emphasis on some topics and less on others. In [*Chapter 4*](B18714_04.xhtml#_idTextAnchor093),
    *Preprocessing – Stemming, Tagging, and Parsing*, we will look at the things that
    matter most for sentiment mining, but we will give a brief overview of the main
    principles of NLP here. As noted, not all of the stages outlined here are needed
    for every application, but it is nonetheless useful to have a picture of how everything
    fits together when considering specific subtasks later.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 情感挖掘是关于寻找自然语言文本中表达的情感——通常是相当简短的文本，如推文和在线评论，但也包括更长的内容，如报纸文章。还有许多其他方法可以让计算机对自然语言文本和口语进行有用的处理：你可以编写可以进行对话的程序（与人们或彼此对话），你可以编写从文章和故事中提取事实和事件的程序，你可以编写从一种语言翻译到另一种语言的程序，等等。这些应用都共享一些基本的概念和技术，但它们各自更侧重于某些主题，而较少关注其他主题。在[*第4章*](B18714_04.xhtml#_idTextAnchor093)“预处理——词干提取、词性标注和解析”中，我们将探讨情感挖掘中最重要的事情，但在这里我们将简要概述自然语言处理的主要原则。正如所提到的，这里概述的所有阶段并非每个应用都需要，但考虑后续的具体子任务时，了解一切如何相互关联仍然是有用的。
- en: 'We will start with a couple of basic observations:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从几个基本观察开始：
- en: Natural language is *linear*. The fundamental form of language is speech, which
    is necessarily linear. You make one sound, and then you make another, and then
    you make another. There may be some variation in the way you make each sound –
    louder or softer, with a higher pitch or a lower one, quicker or slower – and
    this may be used to overlay extra information on the basic message, but fundamentally,
    spoken language is made up of a *sequence* of identifiable units, namely sounds;
    and since written language is just a way of representing spoken language, it too
    must be made up of a sequence of identifiable units.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言是*线性的*。语言的基本形式是口语，它必然是线性的。你发出一个声音，然后发出另一个，再然后是另一个。你在发出每个声音的方式上可能会有一些变化——声音可能更响或更轻，音调可能更高或更低，速度可能更快或更慢——这些变化可能被用来在基本信息上叠加额外的信息，但本质上，口语是由一系列可识别的单位组成的序列，即声音；而书面语言只是口语的表示方式，它也必须由一系列可识别的单位组成。
- en: Natural language is hierarchical. Smaller units are grouped into larger units,
    which are grouped into larger units, which are grouped into larger units, and
    so on. Consider the sentence smaller units are grouped into larger units. In the
    written form of English, for instance, the smallest units are characters; these
    are grouped into morphemes (meaning-bearing word-parts), as small er unit s are
    group ed into large er unit s, which are grouped into words (small-er unit-s are
    group-ed into large-er unit-s), which are grouped into base-level phrases ([small-er
    unit-s] [are group-ed] [into] [large-er unit-s]), which are grouped into higher-level
    phrases ([[small-er unit-s] [[are group-ed] [[into] [large-er unit-s]]]]]).
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言是分层的。较小的单位被组合成较大的单位，这些较大的单位又被组合成更大的单位，如此类推。以句子“较小的单位被组合成较大的单位”为例。在英语的书面形式中，例如，最小的单位是字符；这些字符被组合成语素（有意义的词部分），正如较小的单位被组合成较大的单位一样，这些较大的单位又被组合成单词（较小的单位被组合成较大的单位），这些单词又被组合成基础级别的短语（[较小的单位]
    [被组合] [成] [较大的单位]），这些短语又被组合成更高级别的短语（[[较小的单位] [[被组合] [[成] [较大的单位]]]]）。
- en: These two properties hold for all natural languages. All natural languages were
    spoken before they were written (some widely spoken languages have no universally
    accepted written form!), and hence are fundamentally linear. But they all express
    complex hierarchical relations, and hence to understand them, you have to be able
    to find the ways that smaller units are grouped into larger ones.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个特性适用于所有自然语言。所有自然语言在书写之前都是被说出来的（一些广泛使用的语言没有普遍接受的书写形式！），因此它们在本质上都是线性的。但它们都表达复杂的分层关系，因此要理解它们，你必须能够找到较小的单位组合成较大的单位的方式。
- en: 'What the bottom-level units are like, and how they are grouped, differs from
    language to language. The sounds of a language are made by moving your articulators
    (tongue, teeth, lips, vocal cords, and various other things) around while trying
    to expel air from your lungs. The sound that you get by closing and then opening
    your lips with your vocal cords tensed (/b/, as in the English word *bat*) is
    different from the sound you get by doing the same things with your lips while
    your vocal cords are relaxed (/p/, as in *pat*). Different languages use different
    combinations – Arabic doesn’t use /p/ and English doesn’t use the sound you get
    by closing the exit from the chamber containing the vocal cords (a **glottal stop**):
    the combinations that are used in a particular language are called its **phonemes**.
    Speakers of a language that don’t use a particular combination find it hard to
    distinguish words that use it from ones that use a very similar combination, and
    very hard to produce that combination when they learn a language that does.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 底层单位是什么样的，以及它们是如何组合的，因语言而异。一种语言的声音是通过移动你的发音器官（舌头、牙齿、嘴唇、声带以及各种其他东西）并试图从肺部排出空气来产生的。通过紧闭然后张开嘴唇并紧张声带得到的音（如在英语单词
    *bat* 中的 /b/）与通过放松声带并做同样的事情得到的音（如在 *pat* 中的 /p/）是不同的。不同的语言使用不同的组合——阿拉伯语不使用 /p/，而英语不使用通过关闭声带包含室的出口得到的音（一个**喉塞音**）：在特定语言中使用的组合被称为该语言的**音素**。不使用特定组合的语言使用者发现很难区分使用该组合的单词和非常相似的组合的单词，而且当他们学习使用该组合的语言时，很难产生那个组合。
- en: To make matters worse, the relationship between the bottom-level units in spoken
    language and written language can vary from language to language. The phonemes
    of a language can be represented in the written form of that language in a wide
    variety of ways. The written form may make use of **graphemes**, which are combinations
    of ways of making a shape out of strokes and marks (so, AAAAAA are all written
    by producing two near-vertical more-or-less-straight lines joined at the top with
    a cross-piece about half-way up), just as phonemes are combinations of ways of
    making a sound; a single phoneme may be represented by one grapheme (the short
    vowel /a/ from *pat* is represented in English by the character *a*) or by a combination
    of graphemes (the sound /sh/ from *should* is represented by the pair of graphemes
    *s* and *h*); a sound may have no representation in the written form (Arabic text
    omits short vowels and some other distinctions between phonemes); or there may
    simply be no connection between the written form and the way it is pronounced
    (written Chinese, Japanese kanji symbols). Given that we are going to be largely
    looking at text, we can at least partly ignore the wide variety of ways that written
    and spoken language are related, but we will still have to be aware that different
    languages combine the basic elements of the written forms in completely different
    ways to make up words.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，口语和书面语中底层单位之间的关系在不同的语言中可能会有所不同。一种语言的音素可以用该语言的书面形式以多种多样的方式来表示。书面形式可能会使用**图形符号**，这些是构成形状的笔画和标记的组合方式（因此，AAAAAA都是由两条几乎垂直的、或多或少直的线条组成，顶部相连，中间大约一半的位置有一个横杠），就像音素是构成声音的组合方式一样；一个音素可能由一个图形符号表示（来自
    *pat* 的短元音 /a/ 在英语中由字符 *a* 表示）或者由多个图形符号的组合表示（来自 *should* 的声音 /sh/ 由一对图形符号 *s*
    和 *h* 表示）；一个声音在书面形式中可能没有表示（阿拉伯文本省略了短元音和一些音素之间的其他区别）；或者书面形式和发音之间可能根本没有任何联系（书面汉语、日语的汉字符号）。鉴于我们将主要关注文本，我们至少可以部分地忽略书面语和口语之间关系的多样性，但我们仍然需要意识到，不同的语言以完全不同的方式组合书面形式的基本元素来构成单词。
- en: The bottom-level units of a language, then, are either identifiable sounds or
    identifiable marks. These are combined into groups that carry meaning – **morphemes**.
    A morpheme can carry quite a lot of meaning; for example, *cat* (made out of the
    graphemes *c*, *a*, and *t*) denotes a small mammal with pointy ears and an inscrutable
    outlook on life, whereas *s* just says that you’ve got more than one item of the
    kind you are thinking about, so *cats* denotes a group of several small mammals
    with pointy ears and an opaque view of the world. Morphemes of the first kind
    are sometimes called **lexemes**, with a single lexeme combining with one or more
    other morphemes to express a concept (so, the French lexeme *noir* (*black*) might
    combine with *e* (feminine) and *s* (plural) to make *noires* – several black
    female things). Morphemes that add information to a lexeme, such as about how
    many things were involved or when an event happened, are called **inflectional**
    morphemes, whereas ones that radically change their meaning (for example an *incomplete*
    solution to a problem is *not* complete) are called **derivational** morphemes,
    since they derive a new concept from the original. Again, most languages make
    use of inflectional and derivational morphemes to enrich the basic set of lexemes,
    but exactly how this works varies from language to language. We will revisit this
    at some length in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116) , *Sentiment
    Lexicons and Vector Space Models* since finding the core lexemes can be significant
    when we are trying to assign emotions to texts.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，语言的底层单位要么是可以识别的声音，要么是可以识别的标记。这些被组合成带有意义的组群——**词素**。一个词素可以承载相当多的意义；例如，“cat”（由词素“c”、“a”和“t”组成）表示一种长着尖耳朵和难以捉摸的生活态度的小型哺乳动物，而“s”只是表示你考虑的这类物品不止一个，所以“cats”表示一群有几个长着尖耳朵和世界观的哺乳动物。第一种类型的词素有时被称为**词素**，一个词素可以与一个或多个其他词素结合来表达一个概念（因此，法语词素“noir”（黑色）可以与“e”（阴性）和“s”（复数）结合形成“noires”——几个黑色的女性事物）。向词素添加信息的词素，如涉及的事物数量或事件发生的时间，被称为**屈折词素**，而那些根本改变其意义的词素（例如，一个问题的**不完整**解决方案不是完整的）被称为**派生词素**，因为它们从原始词素中派生出一个新概念。再次强调，大多数语言都使用屈折词素和派生词素来丰富基本的词素集，但具体如何操作因语言而异。我们将在[*第五章*](B18714_05.xhtml#_idTextAnchor116)，“情感词库和向量空间模型”中详细回顾这一点，因为当我们试图将情感分配给文本时，找到核心词素可能是重要的。
- en: A lexeme plus a suitable set of morphemes is often referred to as a **word**.
    Words are typically grouped into larger tree-like structures, with the way that
    they are grouped carrying a substantial part of the message conveyed by the text.
    In the sentence *John believes that Mary expects Peter to marry Susan*, for instance,
    *Peter to marry Susan* is a group that describes a particular kind of event, *Mary
    expects [Peter to marry Susan]* is a group that describes Mary’s attitude to this
    event, and *John believes [that Mary expected [Peter to marry Susan]]* is a group
    that describes John’s view of Mary’s expectation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一个词素加上一个合适的词素集通常被称为**词**。词通常被分组成更大的树状结构，它们分组的方式承载了文本传达的大部分信息。例如，在句子“John believes
    that Mary expects Peter to marry Susan”中，“Peter to marry Susan”是一个描述特定事件的组群，“Mary
    expects [Peter to marry Susan]”是一个描述玛丽对此事件态度的组群，而“John believes [that Mary expected
    [Peter to marry Susan]]”是一个描述约翰对玛丽期望看法的组群。
- en: Yet again, different languages carry out this kind of grouping in different
    ways, and there are numerous ways of approaching the task of analyzing the grouping
    in particular cases. This is not the place for a review of all the grammatical
    theories that have ever been proposed to analyze the ways that words get grouped
    together or of all the algorithms that have ever been proposed for applying those
    theories to specific cases (**parsers**), but there are a few general observations
    that are worth making.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，不同的语言以不同的方式执行这种分组，针对特定情况分析分组的方法有很多。这不是对所有曾经提出的分析词语分组方式的语法理论（**解析器**）或所有曾经提出的将这些理论应用于特定情况的算法的综述之处，但有一些一般的观察值得提出。
- en: Phrase structure grammar versus dependency grammar
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 短语结构语法与依存语法
- en: 'In some languages, groups are mainly formed by merging adjacent groups. The
    previous sentence, for instance, can be analyzed if we group it as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些语言中，组群主要是通过合并相邻的组群形成的。例如，如果我们这样分组，上一句话就可以进行分析：
- en: '*In some languages groups are mainly formed by merging* *adjacent groups*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*在某些语言中组主要由合并* *相邻组*'
- en: '*In [some languages]*np *groups are mainly formed by merging [**adjacent groups]*np'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*在某些语言中*np *组是主要由合并[**相邻组**]np'
- en: '*[In [some languages]]*pp *groups are mainly formed by [merging [**adjacent
    groups]]*vp'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*[在某些语言中]*pp *组主要由[通过[合并[**相邻组]]]]*vp'
- en: '*[In [some languages]]*pp *groups are mainly formed [by [merging [**adjacent
    groups]]]*pp'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*[在某些语言中]*pp *组主要由[通过[合并[**相邻组]]]]*pp'
- en: '*[In [some languages]]*pp *groups are mainly [formed [by [merging [**adjacent
    groups]]]]*vp'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*[在某些语言中]*pp *组主要由[通过[合并[**相邻组]]]]*vp'
- en: '*[In [some languages]]*pp *groups are [mainly [formed [by [merging [**adjacent
    groups]]]]]*vp'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*[在某些语言中]*pp *组是[主要[通过[合并[**相邻组]]]]]*vp'
- en: '*[In [some languages]]*pp *groups [are [mainly [formed [by [merging [**adjacent
    groups]]]]]]*vp'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*[在某些语言中]*pp *组[是[主要[通过[合并[**相邻组]]]]]*vp'
- en: '*[In [some languages]]*pp *[groups [are [mainly [formed [by [merging [**adjacent
    groups]]]]]]]*s'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*[在某些语言中]*pp *[组[是[主要[通过[合并[**相邻组]]]]]]]*s'
- en: '*[[In [some languages]][groups [are [mainly [formed [by [merging [**adjacent
    groups]]]]]]]]*s'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*[[在某些语言中][组[是[主要[通过[合并[**相邻组]]]]]]]*s'
- en: This tends to work well for languages where word order is largely fixed – no
    languages have completely fixed word order (for example, the preceding sentence
    could be rewritten as *Groups are mainly formed by merging adjacent groups in
    some languages* with very little change in meaning), but some languages allow
    more freedom than others. For languages such as English, analyzing the relationships
    between words in terms of adjacent phrases, such as using a **phrase structure
    grammar**, works quite well.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法对于词序基本固定的语言来说往往效果很好——没有语言有完全固定的词序（例如，前面的句子可以改写为“在某些语言中，组主要由相邻组合并形成”，意义变化很小），但有些语言的自由度比其他语言更大。对于像英语这样的语言，从相邻短语的角度分析词语之间的关系，例如使用**短语结构语法**，效果相当好。
- en: 'For languages where words and phrases are allowed to move around fairly freely,
    it can be more convenient to record pairwise relationships between words. The
    following tree describes the same sentence using a **dependency grammar** – that
    is, by assigning a parent word to every word (apart from the full stop, which
    we are taking to be the root of the tree):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于允许词语和短语相对自由移动的语言，记录词语之间的成对关系可能更方便。以下树形图使用**依存语法**——即，为每个词语（除了句号，我们将其视为树的根）分配一个父词来描述相同的句子：
- en: '![Figure 1.3 – Analysis of “In some languages, groups are mainly formed by
    merging adjacent groups” using a rule-based dependency parser](img/B18714_01_03.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3 – 使用基于规则的依存句法分析“在某些语言中，组主要由相邻组合并形成”](img/B18714_01_03.jpg)'
- en: Figure 1.3 – Analysis of “In some languages, groups are mainly formed by merging
    adjacent groups” using a rule-based dependency parser
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 使用基于规则的依存句法分析“在某些语言中，组主要由相邻组合并形成”
- en: 'There are many variations of phrase structure grammar and many variations of
    dependency grammar. Roughly speaking, dependency grammar provides an easier handle
    on languages where words can move around very freely, while phrase structure grammar
    makes it easier to deal with *invisible* items such as the subject of *merging*
    in the preceding example. The difference between the two is, in any case, less
    clear than it might seem from the preceding figure: a dependency tree can easily
    be transformed into a phrase structure tree by treating each subtree as a phrase,
    and a phrase structure tree can be transformed into a dependency tree if you can
    specify which item in a phrase is its **head** – for example, in the preceding
    phrase structure tree, the head of a group labeled as **nn** is its noun and the
    head of a group labeled as **np** is the head of **nn**.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 语法结构短语和依存语法有许多变体。粗略地说，依存语法为处理词语可以非常自由移动的语言提供了一种更简单的处理方法，而短语结构语法则使得处理诸如前例中“合并”的主语等“不可见”项目变得更容易。这两种语法之间的区别，在任何情况下，都不如前图所显示的那么清晰：一个依存树可以很容易地转换成一个短语结构树，通过将每个子树视为一个短语，而如果你能指定短语中的哪个项目是它的**中心词**——例如，在前面的短语结构树中，标记为**nn**的组中心词是名词，标记为**np**的组中心词是**nn**的中心词。
- en: Rule-based parsers versus data-driven parsers
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于规则的解析器与数据驱动解析器
- en: 'As well as having a theory of how to describe the structure of a piece of text,
    you need a program that applies that theory to specific texts – a **parser**.
    There are two ways to approach the development of a parser:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了有一个描述文本结构的理论外，你还需要一个程序将这个理论应用到特定的文本上——一个**解析器**。开发解析器有两种方法：
- en: '**Rule-based**: You can try to devise a set of rules that describe the way
    that a particular language works (a **grammar**), and then implement a program
    that tries to apply these rules to the texts you want analyzed. Devising such
    rules is difficult and time-consuming, and programs that try to apply them tend
    to be slow and fail if the target text does not obey the rules.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于规则**：你可以尝试制定一套规则来描述特定语言的工作方式（一种**语法**），然后实现一个程序，尝试将这些规则应用到你想分析的文本上。制定这样的规则是困难的且耗时，而试图应用这些规则的程序往往运行缓慢，如果目标文本不遵循这些规则，则可能会失败。'
- en: '**Data-driven**: You can somehow produce a set of analyses of a large number
    of texts (a **treebank**), and then implement a program that extracts patterns
    from these analyses. Producing a treebank is difficult and time-consuming – you
    need hundreds of thousands of examples, and the trees all have to be consistently
    annotated, which means that if this is to be done by people, then they have to
    be given consistent guidelines that cover every example they will see (which is,
    in effect, a grammar) (and if it is not done by people then you must already have
    an automated way of doing it, that is, a parser!).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据驱动**：你可以以某种方式产生大量文本的分析集（一个**树库**），然后实现一个程序，从这些分析中提取模式。制作树库是困难的且耗时——你需要数十万个例子，并且所有的树都必须一致性地标注，这意味着如果由人来完成，那么他们必须得到涵盖他们将会看到的每一个例子的统一指南（这在实际上是一种语法）（如果没有人来完成，那么你必须已经有一个自动化的方法来做这件事，即解析器！）。'
- en: 'Both approaches have advantages and disadvantages: when considering whether
    to use a dependency grammar or a phrase structure grammar and then when considering
    whether to follow a rule-based approach or a data-driven one, there are several
    criteria to be considered. Since *no* existing system optimizes all of these,
    you should think about which ones matter most for your application and then decide
    which way to go:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都有优点和缺点：在考虑是否使用依存语法或短语结构语法，以及考虑是否遵循基于规则的方法或数据驱动的方法时，有几个标准需要考虑。由于**没有**现有系统优化所有这些，你应该考虑哪些对你的应用最重要，然后决定走哪条路：
- en: '**Speed**: The first criterion to consider is the speed at which the parser
    runs. Some parsers can become very slow when faced with long sentences. The worst-case
    complexity of the standard **chart-parsing** algorithm for rule-based approaches
    is O(N3), where *N* is the length of the sentence, which means that for long sentences,
    the algorithm can take a *very* long time. Some other algorithms have much better
    complexity than this (the MALT (Nivre et al., 2006) and MST (McDonald et al.,
    2005) parsers, for instance, are linear in the length of the sentence), while
    others have much worse. If two parsers are equally good according to all the other
    criteria, then the faster one will be preferable, but there will be situations
    where one (or more) of the other criteria is more important.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：首先需要考虑的是解析器的运行速度。一些解析器在遇到长句时可能会变得非常慢。基于规则的标准的**图表解析**算法的复杂度最坏情况下是O(N^3)，其中*N*是句子的长度，这意味着对于长句，算法可能需要非常长的时间。有些其他算法的复杂度比这要好得多（例如，MALT（Nivre等，2006）和MST（McDonald等，2005）解析器，它们的复杂度与句子的长度成线性关系），而有些则更差。如果两个解析器在其他所有标准上都是一样的，那么更快的那个将被优先考虑，但也会有一些情况，其中一个（或多个）其他标准更为重要。'
- en: '**Robustness**: Some parsers, particularly rule-based ones, can fail to produce
    any analysis at all for some sentences. This will happen if the input is ungrammatical,
    but it will also happen if the rules are not a complete description of the language.
    A parser that fails to produce a perfectly grammatical input sentence is less
    useful than one that can analyze every grammatically correct sentence of the target
    language. It is less clear that parsers that will do something with every input
    sentence are necessarily more useful than ones that will reject some sentences
    as being ungrammatical. In some applications, detecting ungrammaticality is a
    crucial part of the task (for example, in language learning programs), but in
    any case, assigning an analysis to an ungrammatical sentence cannot be either
    right or wrong, and hence any program that makes use of such an analysis cannot
    be sure that it is doing the right thing.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒性**：一些解析器，尤其是基于规则的解析器，可能无法对某些句子产生任何分析。如果输入是不合语法的，这将会发生，但如果规则不是对语言的完整描述，也会发生。无法产生完全合语法输入句子分析的解析器比能够分析目标语言中每个合语法句子的解析器更不有用。对于任何输入句子都进行某些操作的分析器是否比拒绝一些不合语法句子的分析器更有用，这一点并不明确。在某些应用中，检测不合语法性是任务的关键部分（例如，在语言学习程序中），但在任何情况下，将分析分配给不合语法句子既不能是正确的，也不能是错误的，因此任何利用这种分析的程序都不能确定它在做正确的事情。'
- en: '**Accuracy**: A parser that assigns the *right* analysis to every input text
    will generally be more useful than one that does not. This does, of course, beg
    the question of how to decide what the right analysis is. For data-driven parsers,
    it is impossible to say what the right analysis of a sentence that does not appear
    in the treebank is. For rule-based parsers, any analysis that is returned will
    be right in the sense that it obeys the rules. So, if an analysis looks odd, you
    have to work out how the rules led to it and revise them accordingly.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：将正确分析分配给每个输入文本的解析器通常比没有这样做更有用。当然，这提出了一个问题，即如何决定正确的分析是什么。对于数据驱动解析器，不可能说出树库中未出现的句子的正确分析。对于基于规则的解析器，任何返回的分析都将符合规则，因此是正确的。所以，如果一个分析看起来很奇怪，你必须找出规则是如何导致它的，并相应地修改它们。'
- en: 'There is a trade-off between accuracy and robustness. A parser that fails to
    return any analysis at all in complex cases will produce fewer wrong analyses
    than one that tries to find some way of interpreting every input text: the one
    that simply rejects some sentences will have lower recall but may have higher
    precision, and that can be a good thing. It may be better to have a system that
    says *Sorry, I didn’t quite understand what you just said* than one that goes
    ahead with whatever it is supposed to be doing based on an incorrect interpretation.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '在准确性和鲁棒性之间存在权衡。在复杂情况下无法返回任何分析的解析器，其错误分析的数量将少于试图以某种方式解释每个输入文本的解析器：简单地拒绝一些句子的解析器将具有较低的召回率，但可能具有更高的精确率，这可能是好事。拥有一个系统说“抱歉，我没有完全理解你刚才说的话”可能比基于错误解释继续执行的系统要好。 '
- en: '**Sensitivity and consistency**: Sometimes, sentences that look superficially
    similar have different underlying structures. Consider the following examples:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**敏感性和一致性**：有些句子表面上看起来相似，但具有不同的深层结构。考虑以下例子：'
- en: a) I want to see the queen b) I went to see the queen
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: a) 我想去看女王 b) 我去看了女王
- en: '1(a) is the answer to *What do you want?* and 2(b) is the answer to *Why did
    you go?* If the structures that are assigned to these two sentences do not reflect
    the different roles for *to see the queen*, then it will be impossible to make
    this distinction:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 1(a) 是对“你想要什么？”的回答，而 2(b) 是对“你为什么去？”的回答。如果分配给这两个句子的结构没有反映“去看女王”的不同角色，那么将无法做出这种区分：
- en: '![Figure 1.4 – Trees for 1(a) and 1(b) from the Stanford dependency parser
    (Dozat et al., 2017)](img/B18714_01_04.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – 来自斯坦福依存句法分析器的 1(a) 和 1(b) 的树形图（Dozat 等人，2017）](img/B18714_01_04.jpg)'
- en: Figure 1.4 – Trees for 1(a) and 1(b) from the Stanford dependency parser (Dozat
    et al., 2017)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 来自斯坦福依存句法分析器的 1(a) 和 1(b) 的树形图（Dozat 等人，2017）
- en: a) One of my best friends is watching old movies b) One of my favorite pastimes
    is watching old movies
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: a) 我最好的朋友之一正在看老电影 b) 我最喜欢的消遣之一是看老电影
- en: '![Figure 1.5 – Trees for 2(a) and 2(b) from the Stanford dependency parser](img/B18714_01_05.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.5 – 来自斯坦福依存句法分析器的 2(a) 和 2(b) 的树形图](img/B18714_01_05.jpg)'
- en: Figure 1.5 – Trees for 2(a) and 2(b) from the Stanford dependency parser
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – 来自斯坦福依存句法分析器的 2(a) 和 2(b) 的树形图
- en: 'The **Stanford dependency parser** (**SDP**) trees both say that the subject
    (*One of my best friends*, *One of my favorite pastimes*) is carrying out the
    action of watching old movies – it is sitting in its most comfortable armchair
    with the curtains drawn and the TV on. The first of these makes sense, but the
    second doesn’t: pastimes don’t watch old movies. What we need is an equational
    analysis that says that *One of my favorite pastimes* and *watching old movies*
    are the same thing, as in *Figure 1**.6*:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**斯坦福依存句法分析器**（**SDP**）的树形图都表明主语（*One of my best friends*，*One of my favorite
    pastimes*）正在执行看老电影的动作——它正坐在最舒适的扶手椅上，窗帘拉上，电视打开。第一个例子是有意义的，但第二个例子则不然：消遣不会看老电影。我们需要的是一个等式分析，表明*One
    of my favorite pastimes*和*watching old movies*是同一件事，就像*Figure 1**.6*中所示：'
- en: '![Figure 1.6 – Equational analysis of “One of my favorite pastimes is watching
    old movies”](img/B18714_01_06.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 1.6 – “我最喜欢的消遣之一是看老电影”的等式分析](img/B18714_01_06.jpg)'
- en: Figure 1.6 – Equational analysis of “One of my favorite pastimes is watching
    old movies”
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 1.6 – “我最喜欢的消遣之一是看老电影”的等式分析
- en: Spotting that 2(b) requires an analysis like this, where my favorite pastime
    is the predication in an equational use of *be* rather than the agent of a watching-old-movies
    event, requires more detail about the words in question than is usually embodied
    in a treebank.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 发现2(b)需要这种分析，即我最喜欢的消遣是*be*的等式用法中的谓语，而不是看老电影事件的执行者，需要比通常在树库中体现的更多关于这些词的细节。
- en: 'It can also happen that sentences that look superficially different have very
    similar underlying structures:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能发生看似表面不同的句子具有非常相似的基本结构：
- en: a) Few great tenors are poor b) Most great tenors are rich
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: a) 很少有伟大的男高音是贫穷的 b) 大多数伟大的男高音是富有的
- en: 'This time, the SDP assigns quite different structures to the two sentences:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，SDP为两个句子分配了相当不同的结构：
- en: '![Figure 1.7 – Trees for 3(a) and 3(b) from the SDP](img/B18714_01_07.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 1.7 – SDP中的3(a)和3(b)的树形图](img/B18714_01_07.jpg)'
- en: Figure 1.7 – Trees for 3(a) and 3(b) from the SDP
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 1.7 – SDP中的3(a)和3(b)的树形图
- en: The analysis of 3(a) assigns *most* as a modifier of *great*, whereas the analysis
    of 3(b) assigns *few* as a modifier of *tenors*. *Most* can indeed be used for
    modifying adjectives, as in *He is the most annoying person I know*, but in 3(a),
    it is acting as something more like a determiner, just as *few* is in 3(b).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对3(a)的分析将*most*视为*great*的修饰语，而3(b)的分析将*few*视为*tenors*的修饰语。*Most*确实可以用来修饰形容词，例如*He
    is the most annoying person I know*，但在3(a)中，它更像是一个限定词的作用，就像3(b)中的*few*一样。
- en: a) There are great tenors who are rich b) Are there great tenors who are rich?
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: a) 有一些富有的男高音 b) 有富有的男高音吗？
- en: 'It is clear that 4(a) and 4(b) should have almost identical analyses – 4(b)
    is just 4(a) turned into a question. Again, this can cause problems for treebank-based
    parsers:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，4(a)和4(b)应该有几乎相同的分析——4(b)只是将4(a)转换成一个问题。同样，这也可能给基于树库的解析器带来问题：
- en: '![Figure 1.8 – Trees for 4(a) and 4(b) from MALTParser](img/B18714_01_08.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 1.8 – MALTParser中的4(a)和4(b)的树形图](img/B18714_01_08.jpg)'
- en: Figure 1.8 – Trees for 4(a) and 4(b) from MALTParser
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 1.8 – MALTParser中的4(a)和4(b)的树形图
- en: The analysis in *Figure 1**.8* for 4(a) makes *are* the head of the tree, with
    *there*, *great tenors who are rich*, and as daughters, whereas 4(b) is given
    *tenors* as its head and *are*, *there*, *great*, *who are rich*, and *?* as daughters.
    It would be difficult, given these analyses, to see that 4(a) is the answer to
    4(b)!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于4(a)的*Figure 1**.8*分析，将*are*视为树的主体，与*there*、*great tenors who are rich*以及作为女儿的成分，而4(b)将*tenors*作为其主体，以及*are*、*there*、*great*、*who
    are rich*和*?*作为女儿成分。在这些分析的基础上，很难看出4(a)是4(b)的答案！
- en: Treebank-based parsers frequently fail to cope with issues of the kind raised
    by the examples given here. The problem is that the treebanks on which they are
    trained tend not to include detailed information about the words that appear in
    them – that *went* is an intransitive verb and *want* requires a sentential complement,
    that friends are human and can therefore watch old movies while pastimes are events,
    and can therefore be equated with the activity of watching something, or that
    *most* can be used in a wide variety of ways.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树库的解析器经常无法处理这里给出的例子提出的问题。问题在于，它们训练所用的树库往往不包含关于其中出现的词的详细信息——*went*是不及物动词，*want*需要一个句子补语，朋友是人类，因此可以在看电影时观看老电影，而消遣是事件，因此可以与观看某物的活动等同起来，或者*most*可以用在多种方式中。
- en: It is not possible to say that all treebank-based parsers suffer from these
    problems, but several very widely used ones (the SDP, the version of MALT distributed
    with the NLTK, the EasyCCG parser (Lewis & Steedman, 2014), spaCy (Kitaev & Klein,
    2018)) do. Some of these issues are fairly widespread (the failure to distinguish
    1(a) and 1(b)), and some arise because of specific properties of either the treebank
    or the parsing algorithm. Most of the pre-trained models for parsers such as MALT
    and SPACY are trained on the well-known Wall Street Journal corpus, and since
    this treebank does not distinguish between sentences such as 1(a) and 1(b), it
    is impossible for parsers trained on it to do so. All the parsers listed previously
    assign different structures to 3(a) and 3(b), which may be a characteristic of
    the treebank or it may be some property of the training algorithms. It is worth
    evaluating the output of any such parser to check that it does give distinct analyses
    for obvious cases such as 1(a) and 1(b) and does give parallel analyses for obvious
    cases such as 4(a) and 4(b).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 并不能说所有基于树库的解析器都存在这些问题，但其中一些非常广泛使用的解析器（如SDP、与NLTK一起分发的MALT版本、EasyCCG解析器（Lewis
    & Steedman, 2014）、spaCy（Kitaev & Klein, 2018））确实存在。其中一些问题相当普遍（如无法区分1(a)和1(b)），而有些问题则是因为树库或解析算法的特定属性而产生的。大多数用于MALT和SPACY等解析器的预训练模型都是在著名的华尔街日报语料库上训练的，由于这个树库无法区分1(a)和1(b)这样的句子，因此在其上训练的解析器也无法做到这一点。之前列出的所有解析器都将3(a)和3(b)分配给不同的结构，这可能既可能是树库的特性，也可能是训练算法的某些属性。值得评估任何此类解析器的输出，以检查它是否确实为像1(a)和1(b)这样的明显情况提供了不同的分析，并且是否为像4(a)和4(b)这样的明显情况提供了并行分析。
- en: 'So, when choosing a parser, you have to weigh up a range of factors. Do you
    care if it sometimes makes mistakes? Do you want it to assign different trees
    to texts whose underlying representations are different (this isn’t quite the
    same as accuracy because it could happen that what the parser produces isn’t wrong,
    it just doesn’t contain all the information you need, as in 1(a) and 1(b))? Do
    you want it to always produce a tree, even for texts that don’t conform to any
    of the rules of normal language (should it produce a parse for *#anxious don’t
    know why ................. #worry* 😶 *slowly going #mad hahahahahahahahaha*)?
    Does it matter if it takes 10 or 20 seconds to parse some sentences? Whatever
    you do, *do not trust what anyone says about a parser*: try it for yourself, on
    the data that you are intending to use it on, and check that its output matches
    your needs.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，在选择解析器时，你必须权衡一系列因素。你是否关心它有时会犯错误？你是否希望它为具有不同底层表示的文本分配不同的树（这并不完全等同于准确性，因为可能发生的情况是解析器产生的结果并不错误，只是没有包含你需要的所有信息，就像1(a)和1(b)那样）？你是否希望它总是为文本生成树，即使对于不符合正常语言任何规则的文本（它应该为*#anxious
    don’t know why ................. #worry* 😶 *slowly going #mad hahahahahahahahaha*)生成解析吗？解析某些句子需要10秒或20秒是否重要？无论你做什么，*不要相信任何人关于解析器的说法*：自己尝试，在你打算使用它的数据上尝试，并检查其输出是否符合你的需求。'
- en: Semantics (the study of meaning)
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义学（对意义的研究）
- en: As we’ve seen, finding words, assigning them to categories, and finding the
    relationships between them is quite hard work. There would be no point in doing
    this work unless you had some application in mind that could make use of it. The
    key here is that the choice of words and the relationships between them are what
    allow language to carry messages, to have meaning. That’s why language is important;
    because it carries messages. Almost all application programs that do anything
    with natural language are concerned with the message carried by the input text,
    so almost all such programs have to identify the words that are present and the
    way they are arranged.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，寻找单词、将它们分配到类别中，并找出它们之间的关系是非常困难的。除非你有一个打算使用它的应用，否则做这项工作是没有意义的。关键在于，单词的选择以及它们之间的关系是语言能够传达信息、具有意义的原因。这就是为什么语言很重要的原因；因为它能够传达信息。几乎所有的自然语言处理应用程序都关注输入文本所携带的信息，因此几乎所有这样的程序都必须识别出存在的单词以及它们的排列方式。
- en: 'The study of how language encodes messages is known as semantics. As just noted,
    the message is encoded by the words that are present (**lexical semantics**) and
    the way they are arranged (**compositional semantics**). They are both crucial:
    you can’t understand the difference between *John loves Mary* and *John hates
    Mary* if you don’t know what *loves* and *hates* mean, and you can’t understand
    the difference between *John loves Mary* and *Mary loves John* if you don’t know
    how being the subject or object of a verb encodes the relationship between the
    things denoted by *John* and *Mary* and the event denoted by *loves*.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 研究语言如何编码信息的研究被称为语义学。正如刚才提到的，信息是通过现存的词语（**词汇语义学**）以及它们的排列方式（**组合语义学**）来编码的。它们都很关键：如果你不知道
    *loves* 和 *hates* 的意思，你就无法理解 *John loves Mary* 和 *John hates Mary* 之间的区别，而且如果你不知道动词的主语或宾语如何编码
    *John* 和 *Mary* 所指的事物与 *loves* 所指的事件之间的关系，你就无法理解 *John loves Mary* 和 *Mary loves
    John* 之间的区别。
- en: 'The key test for a theory of semantics is the ability to carry out inference
    between sets of natural language texts. If you can’t do the inferences in 1–7
    (where P1, …, Pn |- Q means that Q can be inferred from the premises P1, …, Pn),
    then you cannot be said to understand English:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 语义学理论的关键测试是进行自然语言文本集合之间的推理的能力。如果你不能在 1–7 中进行推理（其中 P1, …, Pn |- Q 表示 Q 可以从前提
    P1, …, Pn 中推断出来），那么你不能说理解英语：
- en: John hates Mary |- John dislikes Mary
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 约翰讨厌玛丽 |- 约翰不喜欢玛丽
- en: (a) John and Mary are divorced |- John and Mary are not married
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a) 约翰和玛丽离婚了 |- 约翰和玛丽现在没有结婚
- en: (b) John and Mary are divorced |- John and Mary used to be married
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b) 约翰和玛丽离婚了 |- 约翰和玛丽曾经是已婚的
- en: I saw a man with a big nose |- I saw a man
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我看到一个鼻子大的人 |- 我看到一个男人
- en: Every woman distrusts John, Mary is a woman |- Mary distrusts John
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个女人都不信任约翰，玛丽是一个女人 |- 玛丽不信任约翰
- en: I saw more than three pigeons |- I saw at least four birds
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我看到了三个以上的鸽子 |- 我看到了至少四只鸟
- en: I doubt that she saw anyone |- I do not believe she saw a fat man
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我怀疑她没有看到任何人 |- 我不相信她看到一个胖男人
- en: These are very simple inferences. If someone said that the conclusions didn’t
    follow from the premises, you would have to say that they just don’t understand
    English properly. They involve a range of different kinds of knowledge – simple
    entailment relationships between words (*hates* entails *dislikes* (1)); more
    complex relationships between words (getting divorced means canceling an existing
    marriage (2), so if John and Mary are divorced, then they are not now married
    but at one time they were); the fact that *a man with a big nose* is something
    that is a man and has a big nose plus the fact that *A and* *B* entails *A* (3);
    an understanding of how quantifiers work ((4) and (5)); combinations of all of
    these (6) – but they are all inferences that anyone who understands English would
    agree with.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是非常简单的推理。如果有人说结论并不从前提中得出，你不得不说是他们没有正确理解英语。它们涉及不同种类的知识——词语之间简单的蕴涵关系（*hates*
    蕴涵 *dislikes* (1)）；词语之间更复杂的关系（离婚意味着取消现有的婚姻（2），所以如果约翰和玛丽离婚了，那么他们现在没有结婚，但曾经是）；*一个鼻子大的人*
    是一个既是男人又有一个大鼻子的人的事实，以及 *A 和* *B* 蕴涵 *A* (3)；对量词如何工作的理解（(4) 和 (5)）；所有这些的组合（6）——但这些都是任何理解英语的人都会同意的推理。
- en: Some of this information can be fairly straightforwardly extracted from corpora.
    There is a great deal of work, for instance, on calculating the similarity between
    pairs of words, though extending that to cover entailments between words has proved
    more difficult. Some of it is much more difficult to find using data-driven methods
    – the relationships between *more than* and *at least*, for instance, cannot easily
    be found in corpora, and the complex concepts that lie behind the word *divorce*
    would also be difficult to extract unsupervised from a corpus.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一些信息可以从语料库中相当直接地提取出来。例如，有很多工作是在计算一对词语之间的相似性，尽管将其扩展到涵盖词语之间的蕴涵关系已经证明更加困难。其中一些使用数据驱动方法很难找到——例如，*more
    than* 和 *at least* 之间的关系在语料库中很难找到，而 *divorce* 这个词背后的复杂概念也难以从语料库中无监督地提取。
- en: Furthermore, some of it can be applied by using tree-matching algorithms of
    various kinds, from simple algorithms that just compute whether one tree is a
    subtree of another to more complex approaches that pay attention to polarity (that
    *doubt* flicks a switch that turns the direction of the matching algorithm round
    – *I know she loves him* |*- I know she likes him, I doubt she likes him* |- *I
    doubt she loves him*) and to the relationships between quantifiers (*the* |- *some,
    more than N* |- *at least N-1*) (Alabbas & Ramsay, 2013) (MacCartney & Manning,
    2014). Some of it requires more complex strategies, in particular examples with
    multiple premises (4), but all but the very simplest (for example, just treating
    a sentence as a bag of words) require accurate, or at least consistent, trees.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，其中一些可以通过使用各种类型的树匹配算法来应用，从简单的算法，仅计算一个树是否是另一个树的子树，到更复杂的算法，这些算法关注极性（*怀疑*会切换匹配算法的方向——*我知道她爱他*
    |*- 我知道她喜欢他，我怀疑她喜欢他* |- *我怀疑她爱他*）以及量词之间的关系（*这个* |- *一些，多于N* |- *至少N-1*）（Alabbas
    & Ramsay，2013）（MacCartney & Manning，2014）。其中一些需要更复杂的策略，特别是具有多个前提的例子（4），但除了最简单的（例如，仅仅将句子视为一个词袋）之外，都需要准确，或者至少是一致的树。
- en: Exactly how much of this machinery you need depends on your ultimate application.
    Fortunately for us, sentiment mining can be done reasonably effectively with fairly
    shallow approaches, but it should not be forgotten that there is a great deal
    more to understanding a text than simply knowing lexical relationships such as
    similarity or subsumption between words.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要多少这样的设备取决于你的最终应用。幸运的是，对于情感挖掘，我们可以使用相当浅显的方法来有效地完成，但不应忘记，理解文本远不止于知道单词之间的词汇关系，如相似性或包含关系。
- en: Before wrapping up this chapter, we will spend some time learning about machine
    learning, looking at various machine learning models, and then working our way
    through a sample project using Python.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束这一章之前，我们将花一些时间学习机器学习，研究各种机器学习模型，然后通过一个使用Python的示例项目来实践。
- en: Introduction to machine learning
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: Before discussing machine learning, it makes sense to properly understand the
    term artificial intelligence. Broadly speaking, artificial intelligence is a branch
    of computer science and is the idea that machines can be made to think and act
    just like us humans, without explicit programming instructions.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论机器学习之前，正确理解人工智能这个术语是有意义的。广义上讲，人工智能是计算机科学的一个分支，其理念是机器可以被制造得像我们人类一样思考和行动，而不需要明确的编程指令。
- en: 'There is a common misconception that artificial intelligence is a *new thing*.
    The term is widely considered to have been coined in 1956 by assistant Professor
    of Mathematics John McCarthy at the Dartmouth Summer Research Project on Artificial
    Intelligence. We are now in an AI boom – but it was not always so; artificial
    intelligence has a somewhat chequered history. Following on from the 1956 conference,
    funding flowed generously and rapid progress was made as researchers developed
    systems that could play chess and solve mathematical problems. Optimism was high,
    but progress stalled because promises made earlier about artificial intelligence
    were not able to be fulfilled, and hence the funding dried up; this cycle was
    repeated in the 1980s. The current boom we are experiencing is due to the timely
    advances and emergence of three key technologies:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 人们普遍存在一个误解，认为人工智能是一个*新事物*。这个术语普遍被认为是在1956年由数学助理教授约翰·麦卡锡在达特茅斯夏季人工智能研究项目上提出的。我们现在正处于人工智能的繁荣时期——但并非一直如此；人工智能有着一段有些波折的历史。在1956年的会议之后，资金大量涌入，研究人员开发了能够下棋和解决数学问题的系统，取得了快速进展。当时乐观情绪高涨，但由于之前关于人工智能的承诺未能实现，因此资金枯竭；这种周期在1980年代又重复了一次。我们现在经历的繁荣是由于三个关键技术的及时进步和出现：
- en: '**Big data**: Giving us the amounts of data required to be able to do artificial
    intelligence'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大数据**：赋予我们进行人工智能所需的数据量'
- en: '**High-speed high-capacity storage devices**: Giving us the ability to store
    the data'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高速高容量存储设备**：赋予我们存储数据的能力'
- en: '**GPUs**: Giving us the ability to process the data'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU**：赋予我们处理数据的能力'
- en: 'Nowadays, AI is everywhere. Here are some examples of AI:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，人工智能无处不在。以下是一些人工智能的例子：
- en: Chatbots (for example, customer service chatbots)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天机器人（例如，客户服务聊天机器人）
- en: Amazon Alexa, Apple’s Siri, and other smart assistants
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊Alexa、苹果的Siri以及其他智能助手
- en: Autonomous vehicles
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车
- en: Spam filters
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防垃圾邮件过滤器
- en: Recommendation engines
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐引擎
- en: 'According to experts, there are four types of AI:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 根据专家的说法，有四种类型的AI：
- en: '**Reactive**: This is the simplest type and involves machines programmed to
    always respond in the same predictable manner. They cannot learn.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反应性**：这是最简单的一种类型，涉及编程机器始终以相同可预测的方式响应。它们无法学习。'
- en: '**Limited memory**: This is the most common type of AI in use today. It combines
    pre-programmed information with historical data to perform tasks.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限记忆**：这是目前使用最广泛的AI类型。它将预编程的信息与历史数据相结合以执行任务。'
- en: '**Theory of mind**: This is a technology we may see in the future. The idea
    here is that a machine with a theory of mind AI will understand emotions, and
    then alter its own behavior accordingly as it interacts with humans.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**心智理论**：这是我们可能在将来看到的技术。这里的想法是，具有心智理论AI的机器将能够理解情绪，并在与人类互动时相应地改变自己的行为。'
- en: '**Self-aware**: This is the most advanced type of AI. Machines that are self-aware
    of their own emotions, and the emotions of those around them, will have a level
    of intelligence like human beings and will be able to make assumptions, inferences,
    and deductions. This is certainly one for the future as the technology for this
    doesn’t exist just yet.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自我意识**：这是最先进的AI类型。具有自我意识、能够感知自身情绪以及周围人情绪的机器将具有与人类相似的水平智能，并能够做出假设、推断和推理。这无疑是未来的一个方向，因为这项技术的技术尚未存在。'
- en: Machine learning is one way to exploit AI. Writing software programs to cater
    to all situations, occurrences, and eventualities is time-consuming, requires
    effort, and, in some cases, is not even possible. Consider the task of recognizing
    pictures of people. We humans can handle this task easily, but the same is not
    true for computers. Even more difficult is programming a computer to do this task.
    Machine learning tackles this problem by getting the machine to program itself
    by learning through experiences.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是利用AI的一种方式。编写软件程序以适应所有情况、事件和可能性既耗时又费力，在某些情况下甚至不可能。考虑识别人物图片的任务。我们人类可以轻松处理这个任务，但对于计算机来说并非如此。更难的是编程计算机来完成这个任务。机器学习通过让机器通过经验学习来自我编程来解决这个难题。
- en: 'There is no universally agreed-upon definition of machine learning that everyone
    subscribes to. Some attempts include the following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一个被所有人认可的机器学习的定义。一些尝试包括以下内容：
- en: A branch of computer science that focuses on the use of data and algorithms
    to imitate the way that humans learn
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机科学的一个分支，专注于使用数据和算法来模仿人类学习的方式
- en: The capability of machines to imitate intelligent human behavior
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器模仿人类智能行为的能力
- en: A subset of AI that allows machines to learn from data without being programmed
    explicitly
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许机器在没有明确编程的情况下从数据中学习的AI子集
- en: Machine learning needs data – and sometimes lots and lots of it.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习需要数据——有时需要大量的数据。
- en: Lack of data is a significant weak spot in AI. Without a reasonable amount of
    data, machines cannot perform and generate sensible results. Indeed, in some ways,
    this is just like how we humans operate – we look and learn and then apply that
    knowledge in new, unknown situations.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 数据不足是AI的一个重大弱点。没有合理数量的数据，机器无法执行并生成合理的成果。实际上，在某些方面，这就像我们人类的行为一样——我们观察、学习，然后将这些知识应用于新的、未知的情况。
- en: 'And, if we think about it, everyone has data. From the smallest sole trader
    to the largest organization, everyone will have sales data, purchase data, customer
    data, and more. The format of this data may differ between different organizations,
    but it is all useful data that can be used in machine learning. This data can
    be collected and processed and can be used to build machine learning models. Typically,
    this data is split into the following sets:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细思考，每个人都会有数据。从小规模的个体商人到最大的组织，每个人都会有销售数据、购买数据、客户数据等等。这些数据在不同组织中的格式可能不同，但它们都是可以用于机器学习的有用数据。这些数据可以被收集和处理，并可用于构建机器学习模型。通常，这些数据被分为以下几组：
- en: '**Training set**: This is always the largest of the datasets (typically 80%)
    and is the data that is used to train the machine learning models.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：这是数据集中最大的部分（通常是80%），是用于训练机器学习模型的那些数据。'
- en: '**Development set**: This dataset (10%) is used to tweak and try new parameters
    to find the ones that work the best for the model.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发集**：这个数据集（10%）用于调整和尝试新的参数，以找到最适合模型的最佳参数。'
- en: '**Test set**: This is used to test (validate) the model (10%). The model has
    already seen the training data, so it cannot be used to test the model, hence
    this dataset is required. This dataset also allows you to determine whether the
    model is working well or requires more training.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**：用于测试（验证）模型（10%）。模型已经看到了训练数据，因此不能用于测试模型，因此需要这个数据集。这个数据集还允许您确定模型是否运行良好或需要更多训练。'
- en: 'It is good practice to have both development and test datasets. The process
    of building models involves finding the best set of parameters that give the best
    results. These parameters are determined by making use of the development set.
    Without the development set, we would be reduced to using the same datasets for
    training, testing, and evaluation. This is undesirable, but it can also present
    further problems unless handled carefully. For example, the datasets should be
    constructed such that the original dataset class proportions are preserved across
    the test and training sets. Furthermore, as a general point, training data should
    be checked for the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 同时拥有开发和测试数据集是一种良好的做法。构建模型的过程涉及找到最佳参数集，以获得最佳结果。这些参数是通过使用开发集来确定的。如果没有开发集，我们就只能使用相同的数据集进行训练、测试和评估。这是不理想的，但如果不小心处理，也可能带来更多问题。例如，数据集应该构建得使得原始数据集的类别比例在测试集和训练集中得到保留。此外，作为一个一般性的观点，训练数据应该检查以下内容：
- en: It is relevant to the problem
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与问题相关
- en: It is large enough such that all use cases of the model are covered
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它足够大，以至于涵盖了模型的全部用例
- en: It is unbiased and contains no imbalance toward any particular category
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是无偏的，并且不包含对任何特定类别的失衡
- en: 'Modern toolkits such as `sklearn` (Pedregosa et al., 2011) provide ready-made
    functions that will easily split your dataset for you:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现代工具包，如`sklearn`（Pedregosa等，2011年）提供了现成的函数，可以轻松地为您分割数据集：
- en: '[PRE0]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'However, there are times when the data scientist will not have enough data
    available to be able to warrant splitting it multiple ways – for example, there
    is no data relevant to the problem, or the process to collect the data is too
    difficult, expensive, or time-consuming. This is known as **data scarcity** and
    it can be responsible for poor model performance. In such cases, various solutions
    may help alleviate the problem:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时数据科学家可能没有足够的数据可用，无法保证以多种方式分割数据——例如，没有与问题相关的数据，或者收集数据的过程过于困难、昂贵或耗时。这种情况被称为**数据稀缺**，它可能导致模型性能不佳。在这种情况下，各种解决方案可能有助于缓解问题：
- en: '**Augmentation**: For example, taking an image and performing processing (for
    example, rotation, scaling, and modifying the colors) so that new instances are
    slightly different'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强**：例如，取一个图像并执行处理（例如，旋转、缩放和修改颜色），使得新的实例略有不同'
- en: '**Synthetic data**: Data that is artificially generated using computer programs'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合成数据**：使用计算机程序人工生成的数据'
- en: To evaluate models where data is scarce, a technique known as k-fold cross-validation
    is used. This is discussed more fully in [*Chapter 2*](B18714_02.xhtml#_idTextAnchor061),
    briefly the dataset is split into a number (*k*) of groups; then, in turn, each
    group is taken as the test dataset with the remaining groups as the training dataset,
    and the model is fit and evaluated. This is repeated for each group, hence each
    member of the original dataset is used in the test dataset exactly once and in
    a training dataset k-1 times. Finally, the model accuracy is calculated by using
    the results from the individual evaluations.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估数据稀缺的模型，使用了一种称为k折交叉验证的技术。这在[*第2章*](B18714_02.xhtml#_idTextAnchor061)中有更详细的讨论，简而言之，数据集被分成k个组；然后，依次将每个组作为测试数据集，其余组作为训练数据集，对模型进行拟合和评估。对每个组重复此操作，因此原始数据集的每个成员在测试数据集中恰好使用一次，在训练数据集中使用k-1次。最后，通过使用个别评估的结果来计算模型准确率。
- en: 'This poses an interesting question about how much data is needed. There are
    no hard-and-fast rules but, generally speaking, the more the better. However,
    regardless of the amount of data, there are typically other issues that need to
    be addressed:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这提出了一个有趣的问题，即需要多少数据。没有硬性规定，但一般来说，越多越好。然而，无论数据量如何，通常还有其他需要解决的问题：
- en: Missing values
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值
- en: Inconsistencies
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不一致性
- en: Duplicate values
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复值
- en: Ambiguity
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模糊性
- en: Inaccuracies
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不准确
- en: Machine learning is important. It has many real-world applications that can
    allow businesses and individuals to save time, money, and effort by, for example,
    automating business processes. Consider a customer service center where staff
    are required to take calls, answer queries, and help customers. In such a scenario,
    machine learning can be used to handle some of the more simple repetitive tasks,
    hence relieving burden from staff and getting things done more quickly and efficiently.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习很重要。它有许多现实世界的应用，可以使企业和个人通过自动化业务流程等方式节省时间、金钱和精力。考虑一个客户服务中心，那里的员工需要接听电话、回答查询并帮助客户。在这种情况下，机器学习可以用来处理一些更简单、重复的任务，从而减轻员工负担，更快、更有效地完成任务。
- en: Machine learning has dramatically altered the traditional ways of doing things
    over the past few years. However, in many aspects, it still lags far behind human
    levels of performance. Often, the best solutions are hybrid human-in-the-loop
    solutions where humans are needed to perform final verification of the outcome.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，机器学习极大地改变了传统做事的方式。然而，在许多方面，它仍然远远落后于人类的表现水平。通常，最佳解决方案是混合人类在环解决方案，其中需要人类对结果进行最终验证。
- en: 'There are several types of machine learning:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种类型的机器学习：
- en: Supervised learning
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Semi-supervised learning
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Reinforcement learning
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'Supervised learning models must be trained with **labeled** data. Hence, both
    the inputs and the outputs of the model are specified. For example, a machine
    learning model could be trained with human-labeled images of apples and other
    fruits, labeled as *apple* and *non-apple*. This would allow the machine to learn
    the best way to identify pictures of apples. Supervised machine learning is the
    most common type of machine learning used today. In some ways, this matches how
    we humans function; we look and learn from experiences and then apply that knowledge
    in unknown, new situations to work out an answer. Technically speaking, there
    are two main types of supervised learning problems:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习模型必须使用**标记**数据进行训练。因此，模型的输入和输出都是指定的。例如，可以使用人工标记的苹果和其他水果的图像来训练机器学习模型，标记为*苹果*和*非苹果*。这将使机器学会最佳识别苹果图片的方法。监督机器学习是目前最常用的机器学习类型。在某种程度上，这与我们人类的功能相匹配；我们通过观察和学习经验，然后在新情况下应用这些知识来解决问题。从技术上讲，监督学习问题主要有两种类型：
- en: '**Classification**: Problems that involve predicting labels (for example, *apple*)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：涉及预测标签（例如，*苹果*）'
- en: '**Regression**: Problems that involve predicting a numerical value (for example,
    a house price)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：涉及预测数值（例如，房价）的问题'
- en: Both of these types of problems can have any number of inputs of any type. These
    problems are known as **supervised** from the idea that the output is supplied
    by a teacher that shows the system what to do.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种类型的问题可以有任意数量和类型的输入。这些问题被称为**监督**，因为输出是由一个教师提供的，该教师向系统展示如何操作。
- en: 'Unsupervised learning is a type of machine learning that, opposite to supervised
    learning, involves training algorithms on data that is **unlabeled**. Unsupervised
    algorithms examine datasets looking for meaningful patterns or trends that would
    not otherwise be apparent – that is, the target is for the algorithm to find the
    structure in the data on its own. For example, unsupervised machine learning algorithms
    can examine sales data and pinpoint the different types of products being purchased.
    However, the problem with this is that although these models can perform more
    complex tasks than their supervised counterparts, they are also much more unpredictable.
    Some use cases that adopt this approach are as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习**是一种机器学习类型，与监督学习相反，它涉及在**未标记**数据上训练算法。无监督算法检查数据集，寻找有意义模式或趋势，这些模式或趋势在其他情况下可能不明显——也就是说，目标是算法自己找到数据中的结构。例如，无监督机器学习算法可以检查销售数据，并确定被购买的不同类型的产品。然而，这个问题是，尽管这些模型可以执行比它们的监督版本更复杂的任务，但它们也更具不可预测性。采用此方法的用例如下：'
- en: '**Dimensionality reduction**: The process of reducing the number of inputs
    into a model by identifying the key (*principal*) components that capture the
    majority of the data without losing key information.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：通过识别捕获大多数数据但又不丢失关键信息的（*主要*）成分来减少模型输入数量的过程。'
- en: '**Association rules**: The process of finding associations between different
    inputs in the input dataset by discovering the probabilities of the co-occurrence
    of items. For example, when people buy ice cream, they also typically buy sunglasses.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关联规则**：通过发现不同输入项共现的概率来寻找输入数据集中不同输入之间的关联。例如，当人们购买冰淇淋时，他们通常也会购买太阳镜。'
- en: '**Clustering**: Finds hidden patterns in a dataset based on similarities or
    differences and groups the data into clusters or groups. Unsupervised learning
    can be used to perform clustering when the exact details of the clusters are unknown.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：根据相似性或差异在数据集中找到隐藏的模式，并将数据分组到簇或组中。当簇的确切细节未知时，可以使用无监督学习来执行聚类。'
- en: Semi-supervised learning is, unsurprisingly, a combination of supervised and
    unsupervised learning. A small amount of labeled data and a large amount of unlabeled
    data is used. This has the benefits of both unsupervised and supervised learning
    but at the same time avoids the challenges of requiring large amounts of labeled
    data. Consequently, models can be trained to label data without requiring huge
    amounts of labeled training data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习出人意料地是监督学习和无监督学习的结合。使用少量标记数据和大量未标记数据。这具有无监督学习和监督学习的优点，但同时也避免了需要大量标记数据的挑战。因此，可以训练模型对数据进行标记，而无需大量标记的训练数据。
- en: Reinforcement learning is about learning the best behavior so that the maximum
    reward is achieved. This behavior is learned by interacting with the environment
    and observing how it responds. In other words, the sequence of actions that maximize
    the reward must be independently discovered via a trial-and-error process. In
    this way, the model can learn the actions that result in success in an unseen
    environment.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是关于学习最佳行为以实现最大奖励。这种行为是通过与环境交互并观察其响应来学习的。换句话说，最大化奖励的动作序列必须通过试错过程独立发现。这样，模型可以学习在未见过的环境中导致成功的行为。
- en: 'Briefly, here are the typical steps that are followed in a machine learning
    project:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，以下是机器学习项目中通常遵循的典型步骤：
- en: '**Data collection**: Data can come from a database, Excel, or text file – essentially
    it can come from anywhere.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据收集**：数据可以来自数据库、Excel或文本文件——本质上可以来自任何地方。'
- en: '**Data preparation**: The quality of the data used is crucial. Hence, time
    must be spent fixing issues such as missing data and duplicates. Initial **exploratory
    data analysis** (**EDA**) is performed on the data to discover patterns, spot
    anomalies, and test theories about the data by using visual techniques.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据准备**：所使用数据的品质至关重要。因此，必须花费时间解决诸如数据缺失和重复等问题。对数据进行初步的**探索性数据分析**（EDA），通过可视化技术来发现模式、识别异常并测试关于数据的理论。'
- en: '**Model training**: An appropriate algorithm and model is chosen to represent
    the data. The data is split into training data for developing the model and test
    data for testing the model.'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型训练**：选择合适的算法和模型来表示数据。数据被分为用于开发模型的训练数据和用于测试模型的测试数据。'
- en: '**Evaluation**: To test the accuracy, the test data is used.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估**：为了测试准确性，使用测试数据。'
- en: '**Improve performance**: Here, a different model may be chosen, or other inputs
    may be used.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提高性能**：在这里，可以选择不同的模型，或使用其他输入。'
- en: Let’s start with the technical requirements.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从技术要求开始。
- en: Technical requirements
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: This book describes a series of experiments with machine learning algorithms
    – some standard algorithms, some developed especially for this book. These algorithms,
    along with various worked examples, are available as Python programs at [https://github.com/PacktPublishing/Machine-Learning-for-Emotion-Analysis/tree/main](https://github.com/PacktPublishing/Machine-Learning-for-Emotion-Analysis/tree/main),
    split into directories corresponding to the chapters in which the specific algorithms
    will be discussed.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本书描述了一系列机器学习算法的实验——一些是标准算法，一些是为本书特别开发的。这些算法以及各种示例程序，作为Python程序在[https://github.com/PacktPublishing/Machine-Learning-for-Emotion-Analysis/tree/main](https://github.com/PacktPublishing/Machine-Learning-for-Emotion-Analysis/tree/main)上提供，分为与特定算法讨论的章节相对应的目录。
- en: 'One of the reasons why we implemented these programs in Python is that there
    is a huge amount of useful material to build upon. In particular, there are good
    -quality, efficient implementations of several standard machine learning algorithms,
    and using these helps us be confident that where an algorithm doesn’t work as
    well as expected on some dataset, it is because the algorithm isn’t very well
    suited to that dataset, rather than that we just haven’t implemented it very well.
    Some of the programs in the repository use very particular libraries, but there
    are several packages that we will use throughout this book. These are listed here.
    If you are going to use the code in the repository – which we hope you will because
    looking at what actual programs do is one of the best ways of learning – you will
    need to install these libraries. Most of them can be installed very easily, either
    by using the built-in package installer `pip` or by following the directions on
    the relevant website:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以选择在Python中实现这些程序，其中一个原因是基于Python有大量的有用材料可以构建。特别是，有高质量、高效的几个标准机器学习算法的实现，使用这些算法可以帮助我们确信，如果一个算法在某些数据集上表现不如预期，那是因为该算法并不非常适合那个数据集，而不是因为我们没有很好地实现它。仓库中的一些程序使用了非常特定的库，但我们将在这本书中多次使用几个包。这些包在此列出。如果你打算使用仓库中的代码——我们希望你会这样做，因为查看实际程序是如何工作的是学习最好的方法之一——你需要安装这些库。大多数库都可以非常容易地安装，要么通过使用内置的包安装器`pip`，要么通过遵循相关网站上的说明：
- en: '`pandas` structures as inputs. You can install it by typing the following command
    in the command prompt:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`结构作为输入。你可以在命令提示符中键入以下命令来安装它：'
- en: '[PRE1]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Or you can go to [https://pandas.pydata.org/docs/getting_started/install.xhtml](https://pandas.pydata.org/docs/getting_started/install.xhtml)
    for other options.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，你可以访问[https://pandas.pydata.org/docs/getting_started/install.xhtml](https://pandas.pydata.org/docs/getting_started/install.xhtml)以获取其他选项。
- en: '**NumPy**: This is used primarily for its support of *N*-dimensional arrays.
    It has functions for linear algebra and matrices and is also used by other libraries.
    Python provides several collection classes that can be used to represent arrays,
    notably as lists, but they are computationally slow to work with – NumPy provides
    objects that are up to 50 times faster than Python lists. To install it, run the
    following command in the command prompt:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**：这个库主要用于其支持*N*维数组。它提供了线性代数和矩阵的功能，也被其他库使用。Python提供了几个集合类，可以用来表示数组，特别是作为列表，但它们在计算上非常慢——NumPy提供了对象，其速度比Python列表快50倍。要安装它，请在命令提示符中运行以下命令：'
- en: '[PRE2]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Alternatively, you can refer to the documentation for more options: [https://numpy.org/install/](https://numpy.org/install/).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以参考文档以获取更多选项：[https://numpy.org/install/](https://numpy.org/install/)。
- en: '**SciPy**: This provides a range of scientific functions built on top of NumPy,
    including ways of representing sparse arrays (arrays where most elements are 0)
    that can be manipulated thousands of times faster than standard NumPy arrays if
    the vast majority of elements are 0\. You can install it using the following command:'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SciPy**：这个库在NumPy之上提供了一系列科学函数，包括表示稀疏数组（大多数元素为0的数组）的方法，如果大多数元素为0，则可以比标准NumPy数组快数千倍地进行操作。你可以使用以下命令安装它：'
- en: '[PRE3]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can also refer to the SciPy documentation for more details: [https://scipy.org/install/](https://scipy.org/install/).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以参考SciPy文档以获取更多详细信息：[https://scipy.org/install/](https://scipy.org/install/)。
- en: '**scikit-learn (Pedregosa et al., 2011)**: This is used to build machine learning
    models as it has functions for building supervised and unsupervised machine learning
    models, analysis, and dimensionality reduction. A large part of this book is about
    investigating how well various standard machine learning algorithms work on particular
    datasets, and it is useful to have reliable good-quality implementations of the
    most widely used algorithms so that we are not distracted by issues due to the
    way we have implemented them.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scikit-learn（Pedregosa等人，2011）**：这个库用于构建机器学习模型，因为它提供了构建监督和未监督机器学习模型、分析和降维的功能。本书的大部分内容是关于调查各种标准机器学习算法在特定数据集上的表现如何，因此拥有最广泛使用的算法的可靠高质量实现是有用的，这样我们就不会被由于我们实现它们的方式引起的问题所分散注意力。'
- en: 'scikit-learn is also known as `sklearn` – when you want to import it into a
    program, you should refer to it as sklearn. You can install it as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn也被称为`sklearn`——当你想要将其导入程序时，你应该将其称为sklearn。你可以按照以下方式安装它：
- en: '[PRE4]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Refer to the documentation for more information: [https://scikit-learn.org/stable/install.xhtml](https://scikit-learn.org/stable/install.xhtml).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅文档：[https://scikit-learn.org/stable/install.xhtml](https://scikit-learn.org/stable/install.xhtml)。
- en: The `sklearn` implementations of the various algorithms generally make the internal
    representations of the data available to other programs. This can be particularly
    valuable when you are trying to understand the behavior of some algorithm on a
    given dataset and is something we will use extensively as we carry out our experiments.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`实现的各个算法通常使数据的内部表示可供其他程序使用。当你试图理解某个算法在给定数据集上的行为时，这尤其有价值，这是我们将在进行实验时广泛使用的东西。'
- en: '`pip`:'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip`：'
- en: '[PRE5]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For more information, refer to the TensorFlow documentation: [https://www.tensorflow.org/install](https://www.tensorflow.org/install).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅TensorFlow文档：[https://www.tensorflow.org/install](https://www.tensorflow.org/install)。
- en: You will not benefit from its use of parallelism unless you have a GPU or other
    hardware accelerator built into your machine, and training complex models is likely
    to be intolerably slow. We will consider how to use remote facilities such as
    Google Colab to obtain better performance in [*Chapter 9*](B18714_09.xhtml#_idTextAnchor172),
    *Exploring* *Transformers*. For now, just be aware that running `tensorflow` on
    a standard computer without any kind of hardware accelerator probably won’t do
    anything within a reasonable period.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的机器中没有内置GPU或其他硬件加速器，你将无法从其并行使用中受益，并且训练复杂模型可能会非常慢。我们将在第9章“探索*Transformers*”中考虑如何使用远程设施，如Google
    Colab，以获得更好的性能。现在，只需知道，在没有任何硬件加速器的情况下在标准计算机上运行`tensorflow`可能不会在合理的时间内产生任何效果。
- en: '**Keras**: This is also used for building neural networks. It is built on top
    of TensorFlow. It creates computational graphs to represent machine learning algorithms,
    so it is slow compared to other libraries. Keras comes as part of TensorFlow,
    so there is no need to install anything beyond TensorFlow itself.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keras**：这也可以用于构建神经网络。它建立在TensorFlow之上。它创建计算图来表示机器学习算法，因此与其他库相比较慢。Keras是TensorFlow的一部分，因此无需安装除TensorFlow本身之外的其他任何东西。'
- en: '`matplotlib`:'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`：'
- en: '[PRE6]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Refer to the documentation for more information: [https://matplotlib.org/stable/users/installing/index.xhtml](https://matplotlib.org/stable/users/installing/index.xhtml).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅文档：[https://matplotlib.org/stable/users/installing/index.xhtml](https://matplotlib.org/stable/users/installing/index.xhtml)。
- en: Matplotlib may install NumPy if you do not have it already installed, but it
    is more sensible to install them separately (NumPy first).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有安装NumPy，Matplotlib可能会为你安装它，但更合理的是单独安装它们（首先是NumPy）。
- en: '**Seaborn**: This is built on the top of Matplotlib, and is another library
    for creating visualizations. It is useful for making attractive plots and helps
    users explore and understand data. Seaborn makes it easy for users to switch between
    different visualizations. You can easily install Seaborn by running the following
    command:'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Seaborn**：这是建立在Matplotlib之上的，是另一个用于创建可视化的库。它有助于制作吸引人的图表，并帮助用户探索和理解数据。Seaborn使用户能够轻松地在不同的可视化之间切换。你可以通过运行以下命令轻松安装Seaborn：'
- en: '[PRE7]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For more installation options, please refer to [https://seaborn.pydata.org/installing.xhtml](https://seaborn.pydata.org/installing.xhtml).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多安装选项，请参阅[https://seaborn.pydata.org/installing.xhtml](https://seaborn.pydata.org/installing.xhtml)。
- en: We will use these libraries throughout this book, so we advise you to install
    them now, before trying out any of the programs and examples that we’ll discuss
    as we go along. You only have to install them once so that they will be available
    whenever you need them. We will specify any other libraries that the examples
    depend on as we go along, but from now on, we will assume that you have at least
    these ones.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在整本书中使用这些库，因此我们建议你现在就安装它们，在我们讨论程序和示例之前。你只需安装一次，这样它们就会在你需要时可用。我们将随着讨论的进行指定任何其他依赖于示例的库，但从此刻起，我们将假设你至少有这些库。
- en: A sample project
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个示例项目
- en: The best way to learn is by doing! In this section, we will discover how to
    complete a small machine learning project in Python. Completing, and understanding,
    this project will allow you to become familiar with machine learning concepts
    and techniques.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的学习方式是通过实践！在本节中，我们将发现如何在Python中完成一个小型机器学习项目。完成并理解这个项目将使你熟悉机器学习概念和技术。
- en: 'Typically, the first step in developing any Python program is to import the
    modules that are going to be needed using the `import` statement:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，开发任何Python程序的第一步是使用`import`语句导入将要需要的模块：
- en: '[PRE8]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Other imports are needed for this exercise; these can be found in the GitHub
    repository.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，还需要其他一些导入；这些可以在GitHub仓库中找到。
- en: 'The next step is to load the data that is needed to build the model. Like most
    tutorials, we will use the famous Iris dataset. The Iris dataset contains data
    on the length and width of sepals and petals. We will use `pandas` to load the
    dataset. The dataset can be downloaded from the internet and read from your local
    filesystem, as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是加载构建模型所需的数据。像大多数教程一样，我们将使用著名的Iris数据集。Iris数据集包含关于萼片和花瓣长度和宽度的数据。我们将使用`pandas`来加载数据集。数据集可以从互联网上下载，并从您的本地文件系统中读取，如下所示：
- en: '[PRE9]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Alternatively, `pandas` can read it directly from a URL:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，`pandas`可以直接从URL读取它：
- en: '[PRE10]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `read_csv` command returns a DataFrame. It is probably the most commonly
    used `pandas` object and is simply a two-dimensional data structure with rows
    and columns, just like a spreadsheet.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv`命令返回一个DataFrame。它可能是最常用的`pandas`对象，它只是一个具有行和列的二维数据结构，就像电子表格一样。'
- en: 'Since we will be using `sklearn`, it is interesting to see that `sklearn` also
    makes it easy to access the dataset:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用`sklearn`，因此看到`sklearn`也使得访问数据集变得容易是有趣的：
- en: '[PRE11]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now check that the dataset has been successfully loaded by using the
    `describe` function:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过使用`describe`函数来检查数据集是否已成功加载：
- en: '[PRE12]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `describe` function returns a descriptive summary of a DataFrame reporting
    values such as the mean, count, and standard deviation:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe`函数返回DataFrame的描述性摘要，报告如平均值、计数和标准差等值：'
- en: '[PRE13]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This function is useful to check that the data has been loaded correctly but
    also to provide a first glance at some interesting attributes of the data.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数对于检查数据是否已正确加载很有用，同时也可以提供对数据一些有趣属性的初步了解。
- en: 'Some other useful commands tell us more about the DataFrame:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他有用的命令告诉我们更多关于DataFrame的信息：
- en: 'This shows the first five elements in the DataFrame:'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这显示了DataFrame中的前五个元素：
- en: '[PRE14]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This shows the last five elements in the DataFrame:'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这显示了DataFrame中的最后五个元素：
- en: '[PRE15]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This describes the columns of the DataFrame:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这描述了DataFrame的列：
- en: '[PRE16]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This describes the number of rows and columns in the DataFrame:'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这描述了DataFrame中的行数和列数：
- en: '[PRE17]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It is usually a good idea to use these functions to check that the dataset has
    been successfully and correctly loaded into the DataFrame and that everything
    looks as it should.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用这些函数检查数据集是否已成功且正确地加载到DataFrame中，并且一切看起来都应该是明智的。
- en: It is also important to ensure that the dataset is balanced – that is, there
    are relatively equal numbers of each class.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 确保数据集是平衡的也很重要——也就是说，每个类别的数量相对相等。
- en: The majority of machine learning algorithms have been developed with the assumption
    that there are equal numbers of instances of each class. Consequently, imbalanced
    datasets present a big problem for machine learning models as this results in
    models with poor predictive performance.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法都是基于每个类别的实例数量相等的假设开发的。因此，不平衡的数据集对机器学习模型来说是一个大问题，因为这会导致模型预测性能不佳。
- en: 'In the Iris example, this means that we have to check that we have equal numbers
    of each type of flower. This can be verified by running the following command:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在Iris示例中，这意味着我们必须检查我们是否有每种类型的花朵数量相等。这可以通过运行以下命令来验证：
- en: '[PRE18]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This prints the following output:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印以下输出：
- en: '[PRE19]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can see that there are 50 examples of each variety. The next step is to create
    some visualizations. Although we used the `describe` function to get an idea of
    the statistical properties of the dataset, it is much easier to observe these
    in a visual form as opposed to in a table.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每种品种都有50个示例。下一步是创建一些可视化。尽管我们使用了`describe`函数来了解数据集的统计特性，但与表格相比，在视觉形式中观察这些特性要容易得多。
- en: 'Box plots (see *Figure 1**.9*) plot the distribution of data based on the sample
    minimum, the lower quartile, the median, the upper quartile, and the sample maximum.
    This helps us analyze the data to establish any outliers and the data variation
    to better understand each attribute:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图（见*图1**.9*）基于样本最小值、下四分位数、中位数、上四分位数和样本最大值来绘制数据的分布。这有助于我们分析数据，以确定任何异常值和数据变异，从而更好地理解每个属性：
- en: '[PRE20]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This outputs the following plot:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出以下图表：
- en: '![Figure 1.9 – Box plot](img/B18714_01_09.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9 – 箱线图](img/B18714_01_09.jpg)'
- en: Figure 1.9 – Box plot
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 箱线图
- en: Heatmaps are useful for understanding the relationships between attributes.
    Heatmaps are an important tool for data scientists to explore and visualize data.
    They represent the data in a two-dimensional format and allow the data to be summarized
    visually as a colored graph. Although we can use `matplotlib` to create heatmaps,
    it is much easier in `seaborn` and requires significantly fewer lines of code
    – something we like!
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 热力图有助于理解属性之间的关系。对于数据科学家来说，热量图是探索和可视化数据的重要工具。它们以二维格式表示数据，并允许将数据以彩色图形的形式进行视觉总结。虽然我们可以使用
    `matplotlib` 创建热量图，但在 `seaborn` 中要容易得多，并且需要显著更少的代码——这正是我们所喜欢的！
- en: '[PRE21]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This outputs the following heatmap:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下热量图：
- en: '![Figure 1.10 – Heatmap](img/B18714_01_10.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.10 – 热力图](img/B18714_01_10.jpg)'
- en: Figure 1.10 – Heatmap
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10 – 热力图
- en: 'The squares in the heatmap represent the correlation (a measure that shows
    how much two variables are related) between the variables. The correlation values
    range from -1 to +1:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 热力图中的正方形代表变量之间的相关性（一个衡量两个变量之间相关程度的指标）。相关性值范围从 -1 到 +1：
- en: The closer the value is to 1, the more positively correlated they are – that
    is, as one increases, so does the other
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值越接近 1，它们之间的正相关程度就越高——也就是说，一个增加，另一个也会增加
- en: Conversely, the closer the value is to -1, the more negatively correlated they
    are – that is, as one variable decreases, the other will increase
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，值越接近 -1，它们之间的负相关程度就越高——也就是说，一个变量减少，另一个会增加
- en: Values close to 0 indicate that there is no linear trend between the variables
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近 0 的值表示变量之间没有线性趋势
- en: In *Figure 1**.10*, the diagonals are all 1\. This is because, in those squares,
    the two variables are the same and hence the correlation is to itself. For the
    remainder, the scale shows that the lighter the color (toward the top of the scale),
    the higher the correlation. For example, the petal length and petal width are
    highly correlated, whereas petal length and sepal width are not. Finally, it can
    also be seen that the plot is symmetrical on both sides of the diagonal. This
    is because the same set of variables are paired in the squares that are the same.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 1.10* 中，对角线上的所有值都是 1。这是因为，在这些正方形中，两个变量是相同的，因此相关性是对自身的。对于其余部分，刻度显示颜色越浅（越接近刻度的顶部），相关性就越高。例如，花瓣长度和花瓣宽度高度相关，而花瓣长度和萼片宽度则不相关。最后，还可以看到图表在对角线两侧是对称的。这是因为相同的变量集在相同的正方形中配对。
- en: 'We can now build a model using the data and estimate the accuracy of the model
    on data that it has not seen previously. Let’s start by separating the data and
    the labels from each other by using Python:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用数据并估计模型在之前未见过的数据上的准确度来构建一个模型。让我们首先使用 Python 将数据和标签分开：
- en: '[PRE22]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Before we can train a machine learning model, it is necessary to split the
    data and labels into testing and training data. As discussed previously, we can
    use the `train_test_split` function from `sklearn`:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够训练机器学习模型之前，有必要将数据和标签分成测试数据和训练数据。如前所述，我们可以使用 `sklearn` 中的 `train_test_split`
    函数：
- en: '[PRE23]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The capital `X` and lowercase `y` are a nod to math notation, where it is common
    practice to write matrix variable names in uppercase and vector variable names
    using lowercase letters. This has no special Python function and these conventions
    can be ignored if desired. For now, note that `X` refers to data, and `y` refers
    to the associated labels. Hence, `X_train` can be understood to refer to an object
    that contains the training data.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 大写字母 `X` 和小写字母 `y` 是对数学符号的一种致敬，在数学中，通常习惯将矩阵变量名用大写字母书写，而将向量变量名用小写字母表示。这并没有特殊的
    Python 函数，如果需要的话，可以忽略这些约定。目前请注意，`X` 指的是数据，而 `y` 指的是相关的标签。因此，`X_train` 可以理解为指包含训练数据的对象。
- en: 'Before we can begin to work on the machine learning model, we must *normalize*
    the data. Normalization is a scaling technique that updates the numeric columns
    to use a common scale. This helps improve the performance, reliability, and accuracy
    of the model. The two most common normalization techniques are min-max scaling
    and standardization scaling:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始对机器学习模型进行工作之前，我们必须对数据进行 *归一化*。归一化是一种缩放技术，它将数值列更新为使用共同的比例。这有助于提高模型的表现、可靠性和准确性。最常用的两种归一化技术是最小-最大缩放和标准化缩放：
- en: '**Min-max scaling**: This method uses the minimum and maximum values for scaling
    and rescales the values so that they end up in the range 0 to 1 or -1 to 1\. It
    is most useful when the features are of different scales. It is typically used
    when the feature distribution is unknown, such as in k-NN or neural network models.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小-最大缩放**: 这种方法使用最小值和最大值进行缩放，并将值重新缩放，以便它们最终落在 0 到 1 或 -1 到 1 的范围内。当特征具有不同的尺度时，它最有用。通常在特征分布未知时使用，例如在
    k-NN 或神经网络模型中。'
- en: '**Standardization scaling**: This method uses the mean and the standard deviation
    to rescale values so that they have a mean of 0 and a variance of 1\. The resultant
    scaled values are not confined to a specific range. It is typically used when
    the feature distribution is normal.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化缩放**: 这种方法使用平均值和标准差来重新缩放值，以便它们具有 0 的平均值和 1 的方差。结果缩放值不受特定范围的限制。通常在特征分布为正态分布时使用。'
- en: 'It is uncommon to come across datasets that perfectly follow a certain specific
    distribution. Typically, every dataset will need to be standardized. For the Iris
    dataset, we will use sklearn’s `StandardScaler` to scale the data by making the
    mean of the data 0 and the standard deviation 1:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 遇到完美遵循某种特定分布的数据集是不常见的。通常，每个数据集都需要进行标准化。对于 Iris 数据集，我们将使用 sklearn 的 `StandardScaler`
    通过使数据的平均值变为 0 和标准差变为 1 来缩放数据：
- en: '[PRE24]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now that the data is ready, `sklearn` makes it easy for us to test and compare
    various machine learning models. A brief explanation of each model has been provided
    but don’t worry – we explain these models in more detail later in later chapters.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经准备好，`sklearn`使我们能够轻松测试和比较各种机器学习模型。已经为每个模型提供了简要说明，但不用担心——我们将在后面的章节中更详细地解释这些模型。
- en: Logistic regression
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: '**Logistic regression** is one of the most popular machine learning techniques.
    It is used to predict a categorical dependent variable using a set of independent
    variables and makes use of a *sigmoid* function. The sigmoid is a mathematical
    function that has values from 0 to 1 and asymptotes both values. This makes it
    useful for binary classification with 0 and 1 as potential output values:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归**是机器学习中最受欢迎的技术之一。它使用一组独立变量来预测分类的因变量，并使用一个*sigmoid*函数。sigmoid 是一个数学函数，其值在
    0 到 1 之间，并且两个值都有渐近线。这使得它在具有 0 和 1 作为潜在输出值的二分类中非常有用：'
- en: '[PRE25]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There is also a technique called linear regression but, as its name suggests,
    this is used for regression problems, whereas the current Iris problem is a classification
    problem.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一种称为线性回归的技术，但正如其名称所暗示的，它用于回归问题，而当前的 Iris 问题是一个分类问题。
- en: Support vector machines (SVMs)
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机（SVMs）
- en: '**Support vector machine** (**SVM**) is one of the best “out-of-the-box” classification
    techniques. SVM constructs a hyperplane that can then be used for classification.
    It works by calculating the distance between two observations and then determining
    a hyperplane that maximizes the distance between the closest members of separate
    classes. The linear **support vector classifier** (**SVC**) method (as used in
    the following example) applies a linear kernel function to perform classification:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）是最佳的“开箱即用”分类技术之一。SVM 构建一个超平面，然后可用于分类。它通过计算两个观测值之间的距离，然后确定一个最大化不同类别最近成员之间距离的超平面。以下示例中使用的线性**支持向量分类器**（**SVC**）方法应用线性核函数进行分类：'
- en: '[PRE26]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following parameters are used:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 以下参数被使用：
- en: '`random_state`: This controls the random number generation that is used to
    shuffle the data. In this example, a value hasn’t been set, hence a randomly initialized
    state is used. This means that results will vary between runs.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_state`: 这控制用于洗牌数据的随机数生成。在这个例子中，尚未设置值，因此使用随机初始化的状态。这意味着结果将在不同的运行之间变化。'
- en: '`gamma`: This controls how much influence a single data point has on the decision
    boundary. Low values mean “far” and high values mean “close.” In this example,
    gamma is set to “auto,” hence allowing it to automatically define its own value
    based on the characteristics of the dataset.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`: 这控制单个数据点对决策边界的影响程度。低值表示“远”，高值表示“近”。在这个例子中，gamma 设置为“auto”，因此它可以根据数据集的特征自动定义其值。'
- en: '`C`: This controls the trade-off between maximizing the distance between classes
    and correctly classifying the data.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`C`: 这控制最大化类别间距离和正确分类数据之间的权衡。'
- en: K-nearest neighbors (k-NN)
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K 近邻（k-NN）
- en: '**k-NN** is another widely used classification technique. k-NN classifies objects
    based on the closest training examples in the feature space. It is a simple algorithm
    that stores all available cases and classifies new cases by a majority vote of
    its *k* neighbors. The case being assigned to the class is the most common among
    its k-NNs measured by a distance function:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-NN** 是另一种广泛使用的分类技术。k-NN 根据特征空间中最接近的训练示例对对象进行分类。它是一个简单的算法，它存储所有可用的案例，并通过其
    *k* 个邻居的多数投票对新案例进行分类。被分配给类别的案例是其 k-NN 中最常见的，按距离函数衡量：'
- en: '[PRE27]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Decision trees
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: '**Decision trees** attempt to create a tree-like model that predicts the value
    of a variable by learning simple decision rules that are inferred from the data
    features. Decision trees classify examples by sorting down the tree from the root
    to a leaf node, with the leaf node providing the classification for our example:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**试图创建一个树形模型，通过学习从数据特征中推断出的简单决策规则来预测变量的值。决策树通过从根节点到叶节点的树形排序对示例进行分类，叶节点为我们提供的示例提供分类：'
- en: '[PRE28]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Random forest
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: '**Random forest** builds decision trees using different samples and then takes
    the majority vote as the answer. In other words, random forest builds multiple
    decision trees and then merges them to get a more accurate prediction. Due to
    its simplicity, it is also one of the most commonly used algorithms:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**通过使用不同的样本构建决策树，然后取多数投票作为答案。换句话说，随机森林构建多个决策树，然后将它们合并以获得更准确的预测。由于其简单性，它也是最常用的算法之一：'
- en: '[PRE29]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Neural networks
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: '**Neural networks** (also referred to as deep learning) are algorithms that
    are inspired by how the human brain works, and are designed to recognize numerical
    patterns. Neural networks consist of input and output *layers* and (optionally)
    hidden layers. These layers contain units (*neurons*) that transform the inputs
    into something useful for the output layer. These neurons are connected and work
    together. We will look at these in more detail later in this book.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络**（也称为深度学习）是受人类大脑工作方式启发的算法，旨在识别数值模式。神经网络由输入和输出 *层* 以及（可选）隐藏层组成。这些层包含将输入转换为对输出层有用的单元（*神经元*），这些神经元相互连接并协同工作。我们将在本书的后面更详细地探讨这些内容。'
- en: Making predictions
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行预测
- en: Once we have chosen and fit a machine learning model, it can easily be used
    to make predictions on new, unseen data – that is, take the final model and one
    or more data instances and then predict the classes for each of the data instances.
    The model is needed because the result classes are not known for the new data.
    The class for the unseen data can be predicted using scikit-learn’s `predict()`
    function.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们选择了并拟合了一个机器学习模型，它就可以很容易地用于对新、未见数据进行预测——也就是说，使用最终模型和一个或多个数据实例，然后预测每个数据实例的类别。模型是必需的，因为新数据的类别结果未知。可以使用
    scikit-learn 的 `predict()` 函数预测未见数据的类别。
- en: 'First, the unseen data must be transformed into a pandas DataFrame, along with
    the column names:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，必须将未见数据转换为 pandas DataFrame，并包含列名：
- en: '[PRE30]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This DataFrame can then be passed to scikit-learn’s `predict()` function to
    predict the class value:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以将此 DataFrame 传递给 scikit-learn 的 `predict()` 函数以预测类别值：
- en: '[PRE31]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: A sample text classification problem
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个文本分类问题的示例
- en: Given that this is a book on emotion classification and emotions are generally
    expressed in written form, it makes sense to take a look at how a text classification
    problem is tackled.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一本关于情感分类的书，而情感通常以书面形式表达，因此查看如何解决文本分类问题是有意义的。
- en: We have all received spam emails. These are typically emails that are sent to
    huge numbers of email addresses, usually for marketing or phishing purposes. Often,
    these emails are sent by bots. They are of no interest to the recipients and have
    not been requested by them. Consequently, email servers will often automatically
    detect and remove these messages by looking for recognizable phrases and patterns,
    and sometimes placing them into special folders labeled *Junk* or *Spam*.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都收到过垃圾邮件。这些通常是发送给大量电子邮件地址的邮件，通常用于营销或钓鱼目的。通常，这些邮件是由机器人发送的。它们对收件人没有兴趣，也没有被他们请求。因此，邮件服务器通常会自动检测并删除这些消息，通过寻找可识别的短语和模式，有时将它们放入标记为
    *垃圾* 或 *垃圾邮件* 的特殊文件夹中。
- en: In this example, we will build a spam detector and use the machine learning
    abilities of scikit-learn to train the spam detector to detect and classify text
    as spam and non-spam. There are many labeled datasets available online (for example,
    from Kaggle); we chose to use the dataset from [https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将构建一个垃圾邮件检测器，并使用scikit-learn的机器学习能力来训练垃圾邮件检测器，以检测和分类文本为垃圾邮件和非垃圾邮件。网上有许多标记好的数据集可用（例如，来自Kaggle）；我们选择使用来自[https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download)的数据集。
- en: The dataset contains SMS messages that have been collected for spam research.
    It contains 5,574 SMS messages in English that are labeled as spam or non-spam
    (ham). The file contains one message per line, and each line has two columns;
    the label and the message text.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含用于垃圾邮件研究的短信。它包含5,574条标记为垃圾邮件或非垃圾邮件（ham）的英文短信。文件每行一条消息，每行有两列；标签和消息文本。
- en: 'We have seen some of the basic `pandas` commands already, so let’s load the
    file and split it into training and test sets, as we did previously:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一些基本的`pandas`命令，所以让我们加载文件并将其分割成训练集和测试集，就像我们之前做的那样：
- en: '[PRE32]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The file may have an encoding error; for now, we will ignore this as it is not
    relevant to the task at hand.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 文件可能存在编码错误；目前，我们将忽略这一点，因为它与当前任务无关。
- en: 'A handy function called `CountVectorizer` is available in `sklearn`. This can
    be used to transform text into a vector of term-token counts. It is also able
    to preprocess the text data before generating the vector representations, hence
    it is an extremely useful function. `CountVectorizer` converts the raw text into
    a numerical vector representation, which makes it easy to use the text as inputs
    in machine learning tasks:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sklearn`中有一个叫做`CountVectorizer`的便捷函数。这可以用来将文本转换为一个词-标记计数的向量。它还能在生成向量表示之前对文本数据进行预处理，因此是一个非常有用的函数。`CountVectorizer`将原始文本转换为数值向量表示，这使得在机器学习任务中使用文本作为输入变得容易：
- en: '[PRE33]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Essentially, it assigns a number, randomly, to each word and then counts the
    number of occurrences of each. For example, consider the following sentence:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，它随机为每个单词分配一个数字，然后计算每个单词的出现次数。例如，考虑以下句子：
- en: '*The quick brown fox jumps over the* *lazy dog.*'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '*The quick brown fox jumps over the* *lazy dog.*'
- en: 'This would be converted as follows:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这将按以下方式转换：
- en: '| **word** | **the** | **quick** | **brown** | **fox** | **jumps** | **over**
    | **lazy** | **dog** |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| **单词** | **the** | **quick** | **brown** | **fox** | **jumps** | **over**
    | **lazy** | **dog** |'
- en: '| **index** | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| **索引** | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |'
- en: '| **count** | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| **计数** | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
- en: Notice that there are eight unique words, hence eight columns. Each column represents
    a unique word in the vocabulary. Each count row represents the item or row in
    the dataset. The values in the cells are the word counts. Armed with this knowledge
    about the types and counts of common words that appear in spam, the model will
    be able to classify text as spam or non-spam.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里有八个独特的单词，因此有八个列。每一列代表词汇表中的一个独特单词。每一行计数代表数据集中的项目或行。单元格中的值是单词计数。有了关于在垃圾邮件中出现的常见单词的类型和计数的知识，模型将能够将文本分类为垃圾邮件或非垃圾邮件。
- en: 'We will use the simple k-NN model introduced earlier:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前介绍过的简单k-NN模型：
- en: '[PRE34]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `fit()` function, as we have seen earlier, trains the model with the vectorized
    counts from the training data and the training labels. It compares its predictions
    against the real answers in `y_train` and then tunes its hyperparameters until
    it achieves the best possible accuracy. Note how here, since this is a classification
    task, the labels must also be passed to the `fit()` function. The Iris example
    earlier was a regression task; there were no labels, so we did not pass them into
    the `fit()` function:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()`函数，如我们之前所见，使用训练数据的向量化计数和训练标签来训练模型。它将其预测与`y_train`中的真实答案进行比较，然后调整其超参数，直到达到最佳可能的准确性。注意这里，由于这是一个分类任务，标签也必须传递给`fit()`函数。之前的Iris示例是一个回归任务；没有标签，所以我们没有将它们传递到`fit()`函数中：'
- en: '[PRE35]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We now have a model that we can use on the test data to test for accuracy:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个模型，可以在测试数据上使用来测试准确性：
- en: '[PRE36]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note how this time, we use `transform()` instead of `fit_transform()`. The difference
    is subtle but important. The `fit_transform()` function does `fit()`, followed
    by `transform()` – that is, it calculates the initial parameters, uses these calculated
    values to modify the training data, and generates a term-count matrix. This is
    needed when a model is being trained. The `transform()` method, on the other hand,
    only generates and returns the term-count matrix. The `score()` function then
    scores the prediction of the test data term-count matrix against the actual labels
    in test data labels, `y_test`, and even using a simplistic model we can classify
    spam with high accuracy and obtain reasonable results.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这次我们使用`transform()`而不是`fit_transform()`。这种区别虽然微妙但很重要。`fit_transform()`函数先执行`fit()`，然后执行`transform()`——也就是说，它计算初始参数，使用这些计算出的值来修改训练数据，并生成一个词频矩阵。这在模型训练时是必需的。另一方面，`transform()`方法仅生成并返回词频矩阵。然后`score()`函数对测试数据词频矩阵与测试数据标签中的实际标签`y_test`进行评分，即使使用简单的模型，我们也能以高精度对垃圾邮件进行分类并获得合理的结果。
- en: Summary
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we started by examining emotion and sentiment, and their origins.
    Emotion is not the same as sentiment; emotion is more fine-grained and is much
    harder to quantify and work with. Hence, we learned about the three main theories
    of emotion, with psychologists, neurologists, and cognitive scientists each having
    slightly different views as to how emotions are formed. We explored the approaches
    of Ekman and Plutchik, and how the categorical and dimensional models are laid
    out.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先考察了情感和情绪及其起源。情感不同于情绪；情感更为细致，且难以量化和工作。因此，我们学习了三种主要情感理论，心理学家、神经学家和认知科学家对情感形成各有不同的看法。我们探讨了Ekman和Plutchik的方法，以及分类和维度模型是如何构建的。
- en: We also examined the reasons why emotion analysis is important but difficult
    due to the nuances and difficulty of working with content written in natural language,
    particularly the kind of informal language we are concerned with in this book.
    We looked at the basic issues in NLP and will return to the most relevant aspects
    of NLP in [*Chapter 4*](B18714_04.xhtml#_idTextAnchor093), *Preprocessing – Stemming,
    Tagging, and Parsing*. Finally, we introduced machine learning and worked through
    some sample projects.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还考察了为什么情感分析因其细微之处和自然语言内容的处理难度而重要且困难。我们探讨了NLP的基本问题，并将回到第4章中与NLP最相关的方面，即*预处理——词干提取、标记和解析*。最后，我们介绍了机器学习，并完成了一些示例项目。
- en: In the next chapter, we will explore where to find suitable data, the steps
    needed to make it fit for purpose, and how to construct a dataset suitable for
    emotion analysis.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何找到合适的数据，使其适合目的所需的步骤，以及如何构建适合情感分析的数据集。
- en: References
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于本章涵盖的主题，请参阅以下资源：
- en: Alabbas, M., & Ramsay, A. M. (2013). *Natural language inference for Arabic
    using extendedtree edit distance with subtrees*. Journal of Artificial Intelligence
    Research, 48, 1–22.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alabbas, M., & Ramsay, A. M. (2013). *使用扩展树编辑距离和子树进行阿拉伯语的自然语言推理*. 人工智能研究杂志，48,
    1–22.
- en: 'Dozat, T., Qi, P., & Manning, C. D. (2017). *Stanford’s Graph-based Neural
    Dependency Parser at the CoNLL 2017 Shared Task*. Proceedings of the CoNLL 2017
    Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, 20–30\.
    [https://doi.org/10.18653/v1/K17-3002](https://doi.org/10.18653/v1/K17-3002).'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dozat, T., Qi, P., & Manning, C. D. (2017). *斯坦福图神经网络依存句法分析在CoNLL 2017共享任务中的应用*.
    CoNLL 2017共享任务：从原始文本到通用依存关系的多语言解析会议论文集, 20–30\. [https://doi.org/10.18653/v1/K17-3002](https://doi.org/10.18653/v1/K17-3002).
- en: Ekman, P. (1993). *Facial expression and emotion*. *American Psychologist*,
    *48(4)*, 384.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ekman, P. (1993). *面部表情与情绪*. *美国心理学家*, *48(4)*, 384.
- en: 'Kitaev, N., & Klein, D. (2018). *Constituency Parsing with a Self-Attentive
    Encoder*. Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), 2,676–2,686\. [https://doi.org/10.18653/v1/P18-1249](https://doi.org/10.18653/v1/P18-1249).'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kitaev, N., & Klein, D. (2018). *使用自注意力编码器的依存句法分析*. 第56届计算语言学协会年会论文集（第1卷：长篇论文），2,676–2,686\.
    [https://doi.org/10.18653/v1/P18-1249](https://doi.org/10.18653/v1/P18-1249).
- en: Lewis, M., & Steedman, M. (2014). *A* CCG Parsing with a Supertag-factored Model*.
    Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
    (EMNLP), 990–1,000\. [https://doi.org/10.3115/v1/D14-1107](https://doi.org/10.3115/v1/D14-1107).
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis, M., & Steedman, M. (2014). *A* CCG Parsing with a Supertag-factored Model*.
    Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
    (EMNLP), 990–1,000\. [https://doi.org/10.3115/v1/D14-1107](https://doi.org/10.3115/v1/D14-1107).
- en: MacCartney, B., & Manning, C. D. (2014). *Natural logic and natural language
    inference*. In Computing Meaning (pp. 129–147). Springer.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacCartney, B., & Manning, C. D. (2014). *Natural logic and natural language
    inference*. In Computing Meaning (pp. 129–147). Springer.
- en: McDonald, R., Pereira, F., Ribarov, K., & Hajič, J. (2005). *Non-Projective
    Dependency Parsing using Spanning Tree Algorithms*. Proceedings of Human Language
    Technology Conference and Conference on Empirical Methods in Natural Language
    Processing, 523–530\. [https://aclanthology.org/H05-1066](https://aclanthology.org/H05-1066).
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McDonald, R., Pereira, F., Ribarov, K., & Hajič, J. (2005). *Non-Projective
    Dependency Parsing using Spanning Tree Algorithms*. Proceedings of Human Language
    Technology Conference and Conference on Empirical Methods in Natural Language
    Processing, 523–530\. [https://aclanthology.org/H05-1066](https://aclanthology.org/H05-1066).
- en: 'Nivre, J., Hall, J., & Nilsson, J. (2006). MaltParser: *A data-driven parser-generator
    for dependency parsing*. Proceedings of the International Conference on Language
    Resources and Evaluation (LREC), 6, 2,216–2,219.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nivre, J., Hall, J., & Nilsson, J. (2006). MaltParser: *A data-driven parser-generator
    for dependency parsing*. Proceedings of the International Conference on Language
    Resources and Evaluation (LREC), 6, 2,216–2,219.'
- en: 'Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
    O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos,
    A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). *Scikit-learn:
    Machine Learning in Python*. Journal of Machine Learning Research, 12, 2,825–2,830.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
    O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos,
    A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). *Scikit-learn:
    Machine Learning in Python*. Journal of Machine Learning Research, 12, 2,825–2,830.'
- en: 'Plutchik, R. (2001). *The Nature of Emotions: Human emotions have deep evolutionary
    roots, a fact that may explain their complexity and provide tools for clinical
    practice*. *American Scientist*, 89(4), 344–350.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Plutchik, R. (2001). *The Nature of Emotions: Human emotions have deep evolutionary
    roots, a fact that may explain their complexity and provide tools for clinical
    practice*. *American Scientist*, 89(4), 344–350.'
- en: Russell, J. A. (1980). *A circumplex model of affect*. Journal of Personality
    and Social Psychology, 39(6), 1,161–1,178\. [https://doi.org/10.1037/h0077714](https://doi.org/10.1037/h0077714).
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russell, J. A. (1980). *A circumplex model of affect*. Journal of Personality
    and Social Psychology, 39(6), 1,161–1,178\. [https://doi.org/10.1037/h0077714](https://doi.org/10.1037/h0077714).
- en: Part 2:Building and Using a Dataset
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：构建和使用数据集
- en: The process of collecting data (e.g., tweets and news articles) is described
    in this part, followed by the preprocessing steps that are required to get good
    results to create a corpus.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集的过程（例如，推文和新闻文章）在本部分中描述，随后是创建语料库所需的前处理步骤。
- en: 'This part has the following chapters:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 2*](B18714_02.xhtml#_idTextAnchor061), *Building and Using a Dataset*'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第二章*](B18714_02.xhtml#_idTextAnchor061), *构建和使用数据集*'
- en: '[*Chapter 3*](B18714_03.xhtml#_idTextAnchor077), *Labeling Data*'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第三章*](B18714_03.xhtml#_idTextAnchor077), *数据标注*'
- en: '[*Chapter 4*](B18714_04.xhtml#_idTextAnchor093), *Preprocessing – Stemming,
    Tagging, and Parsing*'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第四章*](B18714_04.xhtml#_idTextAnchor093), *预处理 – 词干提取、词性标注和句法分析*'
