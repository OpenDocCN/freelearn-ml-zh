- en: '*Chapter 8*: Choosing Real-Time versus Batch Scoring'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you have experienced in the previous chapters, training AutoML models is
    simple and straightforward. Whether you choose to train a model using the **Azure
    Machine Learning Studio** (**AMLS**) **GUI** or code an AutoML solution in Python
    using Jupyter, you can build highly accurate **machine learning** (**ML**) models
    in minutes. However, you still need to learn how to deploy them. In Azure, there
    are two main ways you can deploy a previously trained ML model to score new data:
    **real-time** and **batch**.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will begin by learning what a **batch scoring solution**
    is, when to use it, and when it makes sense to retrain batch models. Continuing,
    you will learn what a **real-time scoring solution** is, when to use it, and when
    it makes sense to retrain real-time models. Finally, you will conclude by reading
    a variety of different scenarios and determining which type of scoring you should
    use. All scenarios are based on common problems faced by real companies.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the time you finish this chapter, you will have gained an invaluable skill:
    being able to identify when you should build a batch scoring solution and when
    you should build a real-time solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Batch scoring scenarios require you to build ML pipelines, which you will learn
    about in [*Chapter 9*](B16595_09_ePub.xhtml#_idTextAnchor129)*, Implementing a
    Batch Scoring Solution*. Real-time scoring scenarios, on the other hand, require
    you to build real-time scoring endpoints hosted on **Azure Kubernetes Service**
    (**AKS**), which are covered in [*Chapter 11*](B16595_11_ePub.xhtml#_idTextAnchor172)*,
    Implementing a Real-Time Scoring Solution*. Many times, organizations mistakenly
    implement the wrong type of solution, but you will be available to avoid that
    pitfall.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Architecting batch scoring solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecting real-time scoring solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining batch versus real-time scoring scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B16595_07_ePub.xhtml#_idTextAnchor094)*, Using the Many Models
    Solution Accelerator*, featured a lot of heavy Python coding. This chapter is
    a bit of a reprieve; you will not be coding, but you will be learning important
    skills through reading business scenarios and applying the proper solutions. As
    such, there are no technical requirements in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Architecting batch scoring solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Batch inferencing** refers to scoring new data points in batches on a recurring
    time-based schedule. New data is collected over time and subsequently scored,
    generating new predictions. This is the most common way modern companies use ML
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will learn how to architect a complete, end-to-end batch
    scoring solution using Azure AutoML-trained models. You will also learn why, and
    in what situations, you should prioritize batch scoring over real-time scoring
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the five-step batch scoring process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each batch scoring solution you make should follow a five-step process. This
    process begins by training and registering an ML model as you did in the previous
    chapters using AMLS. Regression, classification, and forecasting models all follow
    the same pattern. In order, the five steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train a model**. You can train a model either using the AMLS GUI as you did
    in [*Chapter 3*](B16595_03_ePub.xhtml#_idTextAnchor044)*, Training Your First
    AutoML Model*, or using Python on a compute instance as you did in [*Chapter 4*](B16595_04_ePub.xhtml#_idTextAnchor056)*,
    Building an AutoML Regression Solution*, [*Chapter 5*](B16595_05_ePub.xhtml#_idTextAnchor068)*,
    Building an AutoML Classification Solution*, and [*Chapter 6*](B16595_06_ePub.xhtml#_idTextAnchor081)*,
    Building an AutoML Forecasting Solution*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Register your model**. Once again, you can accomplish this either with the
    AMLS GUI or by using Python running on a compute instance. Registering a model
    saves it to your AMLS workspace and lets you reference it later.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Determine a schedule**. Common schedules for batch inferencing are hourly,
    weekly, or monthly, although you may want to schedule it more or less frequently
    based on the needs of your business problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Score data in batches**. Batch sizes run anywhere from a single data point
    to billions of data points at once. This step is where you run your batch inferencing
    code. In Azure, we use ML pipelines. You will learn how to code ML pipelines in
    [*Chapter 9*](B16595_09_ePub.xhtml#_idTextAnchor129)*, Implementing a Batch Scoring
    Solution*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deliver results**. Once you run your ML pipeline and make new predictions
    or forecasts, you need to send those results to either a report or a database.
    In Azure, sending results from AMLS to other databases is done with a tool called
    **Azure Data Factory** (**ADF**). You will learn how to use ADF in [*Chapter 10*](B16595_10_ePub.xhtml#_idTextAnchor151)*,
    Creating End-to-End AutoML Solutions*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 8.1* shows the whole end-to-end process. While you will implement a
    batch inference solution for your AutoML-trained models, you can use this same
    process to deploy any ML model. The key to ensuring the success of your solution
    lies in aligning your batch job schedule with the needs of your business:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Batch scoring process ](img/Figure_8.1_B16595.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Batch scoring process
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know the process, it's time to dive deeper into each of the steps.
    You're already familiar with training AutoML models and registering them to your
    AMLS workspace. Next, you will learn what to take into consideration when determining
    the schedule of your batch scoring solution.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling your batch scoring solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ultimately, you have trained an AutoML model to meet some business goal. Perhaps
    you need to decide which products to keep in your lineup and which ones to drop.
    Maybe you need to forecast product demand over the next quarter. You may be in
    charge of a professional sports team and need to decide which players to draft
    for the upcoming season. In any case, you need to make sure that you schedule
    your batch inferencing job in a way that makes sense and meets your business needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key to determining when you should schedule your job is based on three
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: How often the business needs to make the decision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How long your batch scoring job takes to run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, you need to know how often the business makes the decision that your
    model is trying to assist. In the case of a professional sports team making a
    drafting decision, this means that you only need to run your job once a year.
    If you work for a business that decides its product mix on a quarterly basis,
    your product demand model should be scheduled to run four times a year. Likewise,
    if you built a model for a fast food restaurant that tells them what food they
    should prepare for the next hour, your batch inferencing solution should run every
    hour. How often you run your model is called **model cadence**.
  prefs: []
  type: TYPE_NORMAL
- en: Second, you need to make sure that new data is available for your model to score.
    This is called **data availability**. Even if your business problem requires new
    predictions to be made once an hour, if your data only refreshes once a day, you
    should build a model that scores data once a day. In other words, you would need
    to train a forecasting model that predicts 24 hours out and scores it once a day
    instead of a forecasting model that predicts 1 hour out that runs 24 times a day.
    Always figure out data availability at the onset of your project. It will save
    you a lot of time.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, you need to pay attention to how long your batch job runs. Even if you
    want to score data every 5 minutes and you have new data available every 5 minutes,
    if your batch scoring job takes 10 minutes to complete you'll be limited in how
    often you can score and deliver results. In this case, consider switching to a
    real-time solution.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've determined your model cadence, data availability, and how long your
    batch job takes to run, the next step is to figure out when your job should run
    – down to the second.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, you would only run your model once all relevant data is available.
    For the professional sports example, you would want to have data on all players
    in the draft. For the hourly restaurant data, you would want up-to-the-minute
    foot-traffic, sales, weather, and traffic data to make your prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, however, you should always expect that there will be data
    availability issues. Some player data will be randomly missing. Sometimes, weather
    or foot-traffic data will be late for no reason at all. Sales data may be corrupted
    by canceled orders or maxed-out credit cards. For this reason, you should simply
    schedule your job to run at the last possible second it can. Make sure you include
    a buffer to account for **compute cluster startup time**.
  prefs: []
  type: TYPE_NORMAL
- en: Compute cluster startup time can vary substantially, so it's important that
    you run a lot of tests on your solution to get an idea of the maximum time it
    will take to spin up. Set your buffer to the maximum time your cluster takes to
    start up, so long as it seems reasonable. Usually, this should be no more than
    5 to 10 minutes. If it takes longer, open a support ticket. This way, you will
    ensure your job always runs on time.
  prefs: []
  type: TYPE_NORMAL
- en: Important tip
  prefs: []
  type: TYPE_NORMAL
- en: While you can set your minimum compute cluster nodes to `1` to achieve a faster,
    more consistent ramp-up time, this means that you are paying for usage 24 hours
    a day, 7 days a week, negating the cost savings advantage inherent to batch solutions.
    Setting compute cluster nodes to `0` will lead to substantial cost savings over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: By scheduling your job to run right around the same time the business will be
    reviewing your predictions to assist their decision making, you give as much time
    as possible for systems upstream to collect, transform, and fix data. You also
    give the business the best predictions you can, based on the most up-to-date data.
    You also need to consider how to batch score data.
  prefs: []
  type: TYPE_NORMAL
- en: Scoring data in batches and delivering results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Within AMLS, batch scoring takes place within an ML pipeline. ML pipelines require
    you to specify an environment, write a scoring script that accesses your model,
    and write results to a datastore, most likely to a filesystem sitting on a blob
    container in an Azure storage account in the form of a CSV file. You will learn
    how to do all of this in [*Chapter 9*](B16595_09_ePub.xhtml#_idTextAnchor129)*,
    Implementing a Batch Scoring Solution*.
  prefs: []
  type: TYPE_NORMAL
- en: However, running an ML pipeline only generates and stores predictions. It is
    not the same as delivering results. Consulting the business directly is the best
    way to determine where your data should ultimately land. Sometimes, they will
    want you to store your predictions in a SQL database to which they have direct
    access. Other times, they will want to receive an Excel file in their email. Often,
    they will ask you to push the results to a web application accessible by their
    mobile device.
  prefs: []
  type: TYPE_NORMAL
- en: AMLS itself can write results directly to Azure Data Lake Storage accounts (Gen
    1 and Gen 2), Azure SQL databases, Azure Blob storage, Azure file shares, Azure
    PostGreSQL, Azure Database for MySQL, and Databricks filesystems. ML pipelines
    can move data directly into these types of storage. That's all, though; you cannot
    directly move data out of Azure with an ML pipeline alone.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, however, business people will request that you land the results of
    your AutoML models someplace else, such as an on-premises database or file share.
    ADF is the perfect tool for moving data both in to and out of Azure. You will
    learn how to use ADF in [*Chapter 10*](B16595_10_ePub.xhtml#_idTextAnchor151)*,
    Creating End-to-End AutoML Solutions*, to solve this common task.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the process is the first step. The next is to understand when
    and why you should use batch scoring solutions over their real-time counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing batch over real time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Real-time solutions score new data points as they come in. Unlike with batching,
    there is no waiting for a compute cluster to spin up; real-time scoring clusters
    never spin down. As soon as new data comes in, new predictions are automatically
    generated. While this seems like an attractive alternative to batch scoring, there
    are two major reasons why you should prioritize batch inferencing over real-time
    inferencing: cost and complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: When using cloud computing, you are only paying for resources when you need
    them. With batch inferencing, when your job runs, a compute cluster spins up,
    and as soon as it finishes running the job, it spins down. With real-time inferencing,
    your cluster will be up and running 24 hours a day, 7 days a week. This means
    that real-time inferencing solutions are orders of magnitude more costly than
    their batch inferencing equivalents.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity is also a key issue. With batch solutions, all you need to do is
    move new data into your Azure datastore, score it, and send it off to another
    database for final delivery. This is an easy, repeatable pattern applicable to
    a wide array of problems.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time solutions, on the other hand, are never as straightforward. At the
    heart of any real-time solution is a **scoring endpoint**. These endpoints can
    be used anywhere in any piece of code. Sometimes, you'll want to integrate them
    with a web application; other times, you'll want to integrate them with an application
    that supports streaming data that never stops flowing in. Whereas batch scoring
    solutions follow a cookie cutter template, real-time solutions are usually more
    complex and unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, batch scoring solutions have the following advantages over real-time
    solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: They are cheaper to run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are less complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are easy to replicate as they follow a boilerplate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you have a handle on what batch inferencing solutions are and why you
    should use them, it's time to look at real-time solutions. Real-time solutions
    aren't as prevalent as batch ones, but they do exclusively support plenty of use
    cases. They're also powerful, require thought, and are fun to create.
  prefs: []
  type: TYPE_NORMAL
- en: Architecting real-time scoring solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Real-time inferencing** refers to scoring new data points as they arrive
    instead of on a time-based schedule. New data flows in, new predictions come out.
    While not as common as batch inferencing, real-time inferencing is used by companies
    in a number of scenarios such as credit card fraud detection, anomaly detection
    on the factory floor, and recommending products when you''re online shopping.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will learn how to architect a complete, end-to-end real-time
    scoring solution using Azure AutoML-trained models. You will also learn why, and
    in what situations, you should prioritize real-time scoring over batch scoring
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the four-step real-time scoring process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Real-time scoring solutions follow a slightly different process than batch
    scoring solutions. There are only four steps. Like batch solutions, the process
    begins by training an ML model and registering it as you did in previous chapters.
    You can use any type of ML model, including regression, classification, and forecasting,
    all of which follow the same pattern. The four steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train a model**. When training a model for deployment in real time, pay extra
    attention to what data will be available to your model at the time of scoring.
    It''s easy to mistakenly include data that, realistically, won''t always be available
    to a real-time solution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Register your model**. You can register with either the AMLS GUI or with
    Python code running on a Jupyter notebook in a compute instance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Score data in real time**. This step is where you run your real-time inferencing
    code. In Azure, we use AKS to create a real-time scoring endpoint. You will learn
    how to create and use AKS and real-time scoring endpoints in [*Chapter 11*](B16595_11_ePub.xhtml#_idTextAnchor172)*,
    Implementing a Real-Time Scoring Solution*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deliver results**. Delivering results in real time is quite different from
    delivering results in batches, and is problem dependent. Usually, the results
    will be displayed on some user-facing app. In other cases, your results will be
    funneled to a database that triggers an alert if a condition is met. Think about
    how you may receive a text message if an algorithm detects fraudulent credit card
    usage on your account.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 8.2* shows the whole end-to-end process. While you will implement a
    real-time inference solution for your AutoML-trained models, you can use this
    same process to deploy any ML model in real time. The key to ensuring the success
    of your solution lies in aligning your real-time solution with the needs of your
    business:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Real-time scoring process](img/Figure_8.2_B16595.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Real-time scoring process
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know the process, it's time to dive deeper into each of the steps.
    First, you need to review the unique considerations of training a model for real-time
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model for real-time deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you train a model for deployment in real time using AutoML or a custom
    ML model, the single most important consideration is the availability of your
    data. Batch scoring solutions do not run that often; data builds up, and you pass
    it in all at once. With a real-time solution, data is constantly being generated
    and constantly being scored. Thus, you need to ask yourself if your solution will
    always have timely access to data.
  prefs: []
  type: TYPE_NORMAL
- en: A great example of this is real-time scoring for fast-food product demand on
    a minute-by-minute basis. One of the greatest predictors of demand is how many
    cars are currently lined up in the drive-thru line. If you have reliable video
    technology that can record the cars, count them, and deliver them to your real-time
    scoring endpoint on a minute-by-minute basis, by all means, you should use that
    data. If, however, the video feed takes 3-5 minutes to deliver that piece of data,
    you shouldn't use it for minute-by-minute scoring.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, at the onset of every project, you should spend a lot of time figuring
    out what data will be available to you and when. This is another type of data
    availability problem. If your data will be available to match the cadence at which
    it will be scored, use it. If not, discard it. Furthermore, if data is only sometimes
    available due to reliability issues, such as a weather API that often returns
    null values, discard it.
  prefs: []
  type: TYPE_NORMAL
- en: You should now have a firm understanding of the importance of data availability
    for real-time scoring. It's time to think about how you should score data and
    deliver results.
  prefs: []
  type: TYPE_NORMAL
- en: Delivering results in real time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating a real-time solution with AMLS means creating an AKS-hosted real-time
    scoring endpoint. You will learn more on this topic in [*Chapter 11*](B16595_11_ePub.xhtml#_idTextAnchor172)*,
    Implementing a Real-Time Scoring Solution*. A **scoring endpoint** is a web service
    into which you pass data to generate predictions. Once created, you can place
    these endpoints anywhere, in any piece of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most often, the results of a real-time scoring solution are going to be embedded
    within an app. You should always consider three things when thinking about delivery:'
  prefs: []
  type: TYPE_NORMAL
- en: Is a human or an automated system receiving your predictions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will a human receive your results?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What will an automated system do with your predictions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case where a human being receives your predictions, you need to determine
    how they expect to receive them. Usually, it is through some sort of application
    that they can access on their PC or mobile device. Predictions in a restaurant
    are likely to be displayed on an in-store app facing the workers. Predictions
    on a factory floor are likely to be sent to mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: In the case where your predictions are being sent to an automated system, you
    need to figure out if the system will send an alert to humans based on some events,
    or if the predictions will merely be used to control some process. If it is the
    latter, you only need to write code to move predictions to the appropriate database
    used by that process.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if the automated system needs to alert humans, you need to
    decide which events humans will be alerted to. Think of credit card fraud detection.
    You would only want to alert users of a fraudulent transaction; you wouldn't want
    to inundate them with alerts for every transaction.
  prefs: []
  type: TYPE_NORMAL
- en: With the importance of delivery in mind, you next need to develop a strong grasp
    of when to use real-time scoring solutions over their batch equivalents.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing when to use real-time scoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Batch inferencing is the default type of scoring that data scientists use. This
    is because it's cheap, reliable, and easy to replicate. However, many situations
    mandate real-time inferencing. The key question to ask yourself is, "*Once the
    data becomes available, how much time do I have to give the business a prediction?*"
    This is really the only consideration. From the moment new data is available and
    ready to be scored, if users expect to have results in a period of time that cannot
    be achieved with batch scoring, you must use real-time scoring.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the case where users send data to a web-based application and a prediction
    is returned on the screen. In this scenario, if you use batch processing, the
    prediction may take between 5 and 15 minutes to show up on the screen. This is
    because the compute cluster needs time to spin up, the environment takes a little
    bit of time to create, and your code requires time to run.
  prefs: []
  type: TYPE_NORMAL
- en: If you use real-time scoring, however, only your code needs to run, vastly reducing
    the total runtime. If your user expects near-instantaneous results, you need to
    build a real-time solution. If they are willing to wait, then you should build
    a cheaper batch solution instead.
  prefs: []
  type: TYPE_NORMAL
- en: Important tip
  prefs: []
  type: TYPE_NORMAL
- en: When choosing a CPU for a real-time solution, always go with the cheapest available
    that will score your data in a timely manner. While more powerful CPUs may save
    you time, they will also cost hundreds of extra dollars per month, every month
    your solution is in existence.
  prefs: []
  type: TYPE_NORMAL
- en: Other common scenarios that demand a real-time solution include **transactional
    fraud detection**, **anomaly detection** in a factory setting, and **recommendation
    engines**. This is because these situations require instant predictions as soon
    as the data becomes available. Once again, this is the key consideration. However,
    there are similar situations where batch processing would be more appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the case of anomaly detection. If you have numerous machines running
    on a factory floor and need to know when one breaks so you can *immediately* replace
    it with a backup machine, that situation requires a real-time solution; the key
    word is *immediately*. The faster you replace the machine with a backup, the better
    your factory performs, the more money you save.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, imagine that those machines also show signs of wear and tear
    and, a few days in advance, you can schedule an engineer to come and perform maintenance
    to keep them up and running. In this case, you would still use an anomaly detection
    ML solution to detect when a machine needs maintenance, but you should score the
    data in batches. This is because there is no pressing need to immediately fix
    the machine. Thus, you should score the data once a day, at the end of the day,
    and order an engineer as needed.
  prefs: []
  type: TYPE_NORMAL
- en: The more experience you gain in creating real-time scoring solutions, the more
    you will be able to tell when it is a requirement. Always ask yourself, *"How
    long can my customers wait for a prediction once my data becomes available?"*
    If they can wait, build a batch solution. If they cannot, build a real-time solution.
  prefs: []
  type: TYPE_NORMAL
- en: There are also some other considerations to take into account when deciding
    which type of solution to use.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing real-time over batch solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To recap, batch solutions should always be prioritized over real-time solutions
    if there is no pressing need to score data as soon it arrives. This is because
    real-time scoring solutions are inherently more expensive than batch scoring solutions,
    as your compute clusters never spin down. On the other hand, if there is a pressing
    need to score data as soon as it becomes available, you need to use a real-time
    solution to avoid the lag inherent with batch processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Complexity is also an issue. Batch scoring solutions always follow the same
    template: collect data, score data, send the results to some file or database.
    This is not the case with real-time solutions; they need to be thoughtfully integrated
    into an application. compares real-time and batch scoring solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Comparing batch and real-time scoring ](img/Figure_8.3_B16595.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Comparing batch and real-time scoring
  prefs: []
  type: TYPE_NORMAL
- en: Being able to understand the differences between batch and real-time scoring
    solutions is one thing; being able to use that knowledge is another. In the next
    section, you will put your knowledge to the test.
  prefs: []
  type: TYPE_NORMAL
- en: Determining batch versus real-time scoring scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When confronted with real business use cases, it is often difficult to distinguish
    how you should deploy your ML model. Many data scientists make the mistake of
    implementing a batch solution when a real-time solution is required, while others
    implement real-time solutions even when a cheaper batch solution would be sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, you will look at different problem scenarios and
    solutions. Read each of the six scenarios and determine whether you should implement
    a real-time or batch inferencing solution. First, you will look at every scenario.
    Then, you will read each answer along with an explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Scenarios for real-time or batch scoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you are presented with six scenarios. Read each carefully and
    decide whether a batch or real-time scoring solution is most appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 1 – Demand forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A fast-food company is trying to determine how many bags of frozen French fries
    it needs to have on hand on any given day. The predictions generated by your ML
    regression algorithm will be used to determine how many bags of fries are delivered
    to each location. Once a week, fleets of trucks will deliver the French fries
    from centrally located warehouses to every store. Should your scoring solution
    be real-time or batch? If the solution is batch, how often should you score new
    data?
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 2 – Web-based supply chain optimization application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A chemical company is trying to optimize its supply chain and has operators
    located onsite at each of their warehouses. Once a day, they input data into a
    web-based application that will be used to determine delivery routes for the next
    day.
  prefs: []
  type: TYPE_NORMAL
- en: The predictions generated by your ML regression algorithm will predict total
    profitability for each of the possible routes and generate the best route. Operators
    are expected to manually enter data into the application once a day. Should this
    solution be batch or real-time? If the solution is real time, what benefits would
    it bring?
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 3 – Fraud detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A credit card company is implementing a fraud detection algorithm. Recently,
    customers have been reporting many suspicious transactions and are leaving as
    a result. As soon as fraudulent activity is detected, the company would like to
    block the transaction and notify customers that their transaction was blocked.
  prefs: []
  type: TYPE_NORMAL
- en: The predictions generated by your ML classification algorithm will block any
    suspicious transactions and an application will send a text message to the customer's
    mobile phone. Should this solution be batch or real-time? If batch, how often
    should you score new data?
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 4 – Predictive maintenance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An automotive company is having difficulty with its machines breaking down.
    Every time a machine breaks down, it costs the company tens of thousands of dollars
    as the entire assembly line shuts down waiting for the machine to be fixed. About
    once a month, engineers perform maintenance on the machines, but there are only
    enough engineers to repair 20% of the machines.
  prefs: []
  type: TYPE_NORMAL
- en: Your ML classification algorithm will tell these engineers which machines to
    repair and order the repairs by priority. Should this solution be batch or real-time?
    If batch, how often should you score new data?
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 5 – Web-based product cost planning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An aerospace company is trying to predict the profitability of a new line of
    aircraft, and would like to predict the cost of raw materials and labor based
    on specifications. Pricing managers will enter relevant data manually into a web-based
    application and would like to see the projected price immediately from your ML
    regression algorithm. There is no set time that they are expected to do this,
    and they may run the application many times in a single session.
  prefs: []
  type: TYPE_NORMAL
- en: Should you design this solution to be batch or real-time? If batch, how often
    should you score new data?
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 6 – Recommendation engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A retail store is building a website. As customers browse and add new items
    to their shopping cart, the store would like to recommend other items for those
    customers to purchase. Your ML regression algorithm will assign scores to items
    on the fly based on what customers browse, and the highest-scoring items will
    be automatically displayed to them. Should this solution be designed as a real-time
    or batch solution? If batch, how often should you score new data?
  prefs: []
  type: TYPE_NORMAL
- en: Think deeply about each scenario and then proceed to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Answers for the type of solution appropriate for each scenario
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will review answers for each of the six scenarios. Read
    each explanation and then review the original scenario for clarity.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 1 – Batch inferencing solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scenario 1 features a typical case for ML. The fast-food company wishes to forecast
    demand for French fries on a weekly basis. Data is collected from all of the stores
    and should be scored using a batch inferencing process once a week. Once the predictions
    are generated, French fries can be loaded onto trucks and delivered to all of
    their locations.
  prefs: []
  type: TYPE_NORMAL
- en: This is a *batch solution* because the data only needs to be scored once a week,
    and there's no pressing need to generate predictions as soon as new data becomes
    available.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 2 – Batch inferencing solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scenario 2 could use either a real-time or batch inferencing solution. When
    operators manually pass data into the web-based application, it's simply a matter
    of how long they are willing to wait for results. Since this is a once-a-day operation
    and the results will not be used until the next day, it's best to trade time for
    money and go with a *batch inferencing process*. However, if you built a *real-time
    solution* in this scenario, there is a benefit. Operators would be able to see
    results right away instead of having to wait 10 to 15 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 3 – Real-time inferencing solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scenario 3 is a classic real-time inferencing use case. Fraudulent transactions
    must be detected immediately, that is, as soon as data on the transaction becomes
    available. This speed enables both the transaction to be blocked and the customer
    to receive a notification as soon as possible.
  prefs: []
  type: TYPE_NORMAL
- en: There's no way that a batch inferencing solution could provide the necessary
    speed. Furthermore, each data point must be scored individually as it arrives.
    There's no time to lump data together in batches, or to spin up a compute cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 4 – Batch inferencing solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scenario 4 is tricky. Predictive maintenance can sometimes require real-time
    inferencing, particularly in cases where an anomalous detection indicates imminent
    failure and there are emergency staff on standby ready to fix the problem.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, however, engineers are only available to service machines once
    a month. As a result, there is no pressing need for immediate results. A *once-a-month
    batch processing solution* is most appropriate for this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 5 – Real-time inferencing solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Scenario 5 is similar to `Scenario` `2` with three major exceptions:'
  prefs: []
  type: TYPE_NORMAL
- en: The pricing managers do not run the scoring solution on a set schedule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pricing managers may run the solution many times in a single setting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pricing managers expect immediate results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Immediate results should indicate to you a *real-time scoring solution*. If
    you designed this solution with a batch process, every single time the pricing
    managers ran their numbers, they would experience a long wait. One way you could
    alter the application to be more batch friendly is if it allowed the pricing managers
    to enter data for all of their scenarios at once, instead of one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 6 – Real-time inferencing solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scenario 6 is another classic real-time inferencing scenario. Recommendation
    engines need to be extremely fast and change based on what users click, view,
    and add to their cart. They also need to change based on what customers purchase.
    Each time the screen changes, new data must be scored so that the appropriate
    items can be advertised to the user. Performance is also extremely important,
    as the algorithm must keep up with user actions. Every recommendation engine powered
    by ML should only be used in *real time*.
  prefs: []
  type: TYPE_NORMAL
- en: 'How did you do on the six scenarios? Were you able to achieve 100% accuracy?
    In either case, have a look at *Figure 8.4* to see how common scenarios map to
    real-time or batch scoring solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – How common scenarios map to real-time or batch ](img/Figure_8.4_B16595.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – How common scenarios map to real-time or batch
  prefs: []
  type: TYPE_NORMAL
- en: If you were able to accurately decide which type of scoring solution fits each
    scenario on your first try, you have passed with flying colors. You now have a
    deep understanding of which situations warrant a real-time inferencing solution
    and which situations warrant a batch inferencing solution. If you made a mistake
    on one or more of the scenarios, reread all of them until you intuitively understand
    the differences.
  prefs: []
  type: TYPE_NORMAL
- en: Remember the most important factor in deciding which type of solution to use
    is how fast you need the prediction relative to when the data becomes available.
    If the end user can wait, use batch. If the application demands an immediate response,
    use real-time. Understanding the difference will not only make you a great data
    scientist, but it will also save you and your organization time, money, and work.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You now have a firm understanding of batch and real-time inferencing, and when
    to use which type of scoring solution. This is important, as even seasoned data
    scientists occasionally make mistakes when designing end-to-end ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, most ML courses focus on training models instead of deploying them,
    but to be an effective data scientist, you must be proficient at both. In the
    upcoming chapters, you will learn how to code each of these inferencing methods
    in AMLS.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 9*](B16595_09_ePub.xhtml#_idTextAnchor129)*, Implementing a Batch
    Scoring Solution*, you will learn step by step how to use the ML models you've
    already built in batch scoring scenarios. You will create ML pipelines in AMLS
    and learn how to schedule them to run on a timer. This will allow you to easily
    productionalize your ML models and become a valuable asset to your company or
    organization.
  prefs: []
  type: TYPE_NORMAL
