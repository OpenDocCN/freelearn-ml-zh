- en: Features and scikit-learn Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The datasets we have used so far have been described in terms of *features*.
    In the previous chapter, we used a transaction-centric dataset. However, ultimately
    this was just a different format for representing feature-based data.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other types of datasets, including text, images, sounds, movies,
    or even real objects. Most data mining algorithms rely on having numerical or
    categorical features. This means we need a way to represent these types before
    we input them into the data mining algorithm. We call this representation a **model**.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how to extract numerical and categorical features,
    and choose the best features when we do have them. We will discuss some common
    patterns and techniques for extracting features. Choosing your model appropriately
    is critically important to the outcome of the data mining exercise, more so than
    the choice of classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key concepts introduced in this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features from datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating models for your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating new features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting good features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating your own transformer for custom datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extracting features is one of the most critical tasks in data mining, and it
    generally affects your end result more than the choice of data mining algorithm.
    Unfortunately, there are no hard and fast rules for choosing features that will
    result in high-performance data mining. The choice of features determines the
    model that you are using to represent your data.
  prefs: []
  type: TYPE_NORMAL
- en: Model creation is where the science of data mining becomes more of an art and
    why automated methods of performing data mining (there are several methods of
    this type) focus on algorithm choice and not model creation. Creating good models
    relies on intuition, domain expertise, data mining experience, trial and error,
    and sometimes a little luck.
  prefs: []
  type: TYPE_NORMAL
- en: Representing reality in models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given what we have done so far in the book, it is easy to forget that the reason
    we are performing data mining is to affect real world objects, not just manipulating
    a matrix of values. Not all datasets are presented in terms of features. Sometimes,
    a dataset consists of nothing more than all of the books that have been written
    by a given author. Sometimes, it is the film of each of the movies released in
    1979\. At other times, it is a library collection of interesting historical artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: From these datasets, we may want to perform a data mining task. For the books,
    we may want to know the different categories that the author writes. In the films,
    we may wish to see how women are portrayed. In the historical artifacts, we may
    want to know whether they are from one country or another. It isn't possible to
    just pass these raw datasets into a decision tree and see what the result is.
  prefs: []
  type: TYPE_NORMAL
- en: For a data mining algorithm to assist us here, we need to represent these as
    **features**. Features are a way to create a model and the model provides an approximation
    of reality in a way that data mining algorithms can understand. Therefore, a model
    is just a simplified version of some aspect of the real world. As an example,
    the game of chess is a simplified model (in game form) for historical warfare.
  prefs: []
  type: TYPE_NORMAL
- en: 'Selecting features has another advantage: they reduce the complexity of the
    real world into a more manageable model.'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine how much information it would take to properly, accurately, and fully
    describe a real-world object to someone that has no background knowledge of the
    item. You would need to describe the size, weight, texture, composition, age,
    flaws, purpose, origin, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: As the complexity of real objects is too much for current algorithms, we use
    these simpler models instead.
  prefs: []
  type: TYPE_NORMAL
- en: This simplification also focuses our intent in the data mining application.
    In later chapters, we will look at clustering and where it is critically important.
    If you put random features in, you will get random results out.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a downside as this simplification reduces the detail, or may
    remove good indicators of the things we wish to perform data mining on.
  prefs: []
  type: TYPE_NORMAL
- en: Thought should always be given to how to represent reality in the form of a
    model. Rather than just using what has been used in the past, you need to consider
    the goal of the data mining exercise. What are you trying to achieve? In [Chapter
    3](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml), *Predicting Sports Winners with
    Decision Trees*, we created features by thinking about the goal (predicting winners)
    and used a little domain knowledge to come up with ideas for new features.
  prefs: []
  type: TYPE_NORMAL
- en: Not all features need to be numeric or categorical. Algorithms have been developed
    that work directly on text, graphs, and other data structures. Unfortunately,
    those algorithms are outside the scope of this book. In this book, and normally
    in your data mining career, we mainly use numeric or categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: The *Adult* dataset is a great example of taking a complex reality and attempting
    to model it using features. In this dataset, the aim is to estimate if someone
    earns more than $50,000 per year.
  prefs: []
  type: TYPE_NORMAL
- en: To download the dataset, navigate to [http://archive.ics.uci.edu/ml/datasets/Adult](http://archive.ics.uci.edu/ml/datasets/Adult) and
    click on the Data Folder link. Download the `adult.data` and `adult.names` into
    a directory named Adult in your data folder.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset takes a complex task and describes it in features. These features
    describe the person, their environment, their background, and their life status.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Jupyter Notebook for this chapter, set the data filename and load
    the data with pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Most of the code is the same as in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Don''t want to type those heading names? Don''t forget you can download the
    code from Packt Publishing, or alternatively from the author''s GitHub repository
    for this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/dataPipelineAU/LearningDataMiningWithPython2](https://github.com/dataPipelineAU/LearningDataMiningWithPython2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The adult file itself contains two blank lines at the end of the file. By default,
    pandas will interpret the penultimate new line to be an empty (but valid) row.
    To remove this, we remove any line with invalid numbers (the use of `inplace`
    just makes sure the same Dataframe is affected, rather than creating a new one):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Having a look at the dataset, we can see a variety of features from `adult.columns`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The results show each of the feature names that are stored inside an Index
    object from pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Common feature patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While there are millions of ways to create models, there are some common patterns
    that are employed across different disciplines. However, choosing appropriate
    features is tricky and it is worth considering how a feature might correlate to
    the end result. As a well known adage goes, *don't judge a book by its cover*—it
    is probably not worth considering the size of a book if you are interested in
    the message contained within.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some commonly used features focus on the physical properties of the real world
    objects being studied, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Spatial properties such as the length, width, and height of an object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight and/or density of the object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Age of an object or its components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The type of the object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The quality of the object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other features might rely on the usage or history of the object:'
  prefs: []
  type: TYPE_NORMAL
- en: The producer, publisher, or creator of the object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The year of manufacturing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other features describe a dataset in terms of its components:'
  prefs: []
  type: TYPE_NORMAL
- en: Frequency of a given subcomponent, such as a word in a book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of subcomponents and/or the number of different subcomponents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average size of the subcomponents, such as the average sentence length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ordinal features allow us to perform ranking, sorting, and grouping of similar
    values. As we have seen in previous chapters, features can be numerical or categorical.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical features are often described as being ordinal. For example, three
    people, Alice, Bob, and Charlie, may have heights of 1.5 m, 1.6 m, and 1.7 m.
    We would say that Alice and Bob are more similar in height than Alice and Charlie.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Adult dataset that we loaded in the last section contains examples of continuous,
    ordinal features. For example, the Hours-per-week feature tracks how many hours
    per week people work. Certain operations make sense on a feature like this. They
    include computing the mean, standard deviation, minimum, and maximum. There is
    a function in pandas for giving some basic summary stats of this type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The result tells us a little about this feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Some of these operations do not make sense for other features. For example,
    it doesn't make sense to compute the sum of the education statuses of these people.
    In contrast, it would make sense to compute the sum of the number of orders by
    each customer on an online store.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also features that are not numerical, but still ordinal. The Education
    feature in the Adult dataset is an example of this. For example, a Bachelor''s
    degree is a higher education status than finishing high school, which is a higher
    status than not completing high school. It doesn''t quite make sense to compute
    the mean of these values, but we can create an approximation by taking the median
    value. The dataset gives a helpful feature, `Education-Num`, which assigns a number
    that is basically equivalent to the number of years of education completed. This
    allows us to quickly compute the median:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The result is 10, or finishing one year past high school. If we didn't have
    this, we could compute the median by creating an ordering over the education values.
  prefs: []
  type: TYPE_NORMAL
- en: Features can also be categorical. For instance, a ball can be a tennis ball,
    cricket ball, football, or any other type of ball. Categorical features are also
    referred to as nominal features. For nominal features, the values are either the
    same or they are different. While we could rank balls by size or weight, just
    the category alone isn't enough to compare things. A tennis ball is not a cricket
    ball, and it is also not a football. We could argue that a tennis ball is more
    similar to a cricket ball (say, in size), but the category alone doesn't differentiate
    this—they are the same, or they are not.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can convert categorical features to numerical features using the one-hot
    encoding, as we saw in [Chapter 3](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml),
    *Predicting Sports Winners with Decision Trees*. For the aforementioned categories
    of balls, we can create three new binary features: is a tennis ball, is a cricket
    ball, and is a football. This process is the one-hot encoding we used in [Chapter
    3](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml), *Predicting Sports Winners with
    Decision Trees*. For a tennis ball, the vector would be `[1, 0, 0]`. A cricket
    ball has the values `[0, 1, 0]`, while a football has the value `[0, 0, 1]`. These
    are binary features but can be used as continuous features by many algorithms.
    One key reason for doing this is that it easily allows for direct numerical comparison
    (such as computing the distance between samples).'
  prefs: []
  type: TYPE_NORMAL
- en: The Adult dataset contains several categorical features, with Work-Class being
    one example. While we could argue that some values are of higher rank than others
    (for instance, a person with a job is likely to have a better income than a person
    without), it doesn't make sense for all values. For example, a person working
    for the state government is not more or less likely to have a higher income than
    someone working in the private sector.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can view the unique values for this feature in the dataset using the `unique()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The result shows the unique values in this column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: There are some missing values in the preceding data, but they won't affect our
    computations in this example. You can also use the `adult.value_counts()` function
    to see how frequently each value appears.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another really useful step to take with a new dataset is to visualise it. The
    following code will create a swarm plot, giving a view of how education and hours-worked
    relate to the final classification (identified by colour):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B06162_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above code, we sample the dataset to show every 50 rows, using the `adult[::50]`
    dataset indexing. Setting this to just `adult` will result in all samples being
    shown, but that may also make the graph hard to read.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can convert numerical features to categorical features through
    a process called **discretization**, as we saw in Chapter 1, *Getting Started
    With Data Mining*. We can call any person who is taller than 1.7 m tall, and any
    person shorter than 1.7 m short. This gives us a categorical feature (although
    still an ordinal one). We do lose some data here. For instance, two people, one
    1.69 m tall and one 1.71 m, will be in two different categories and considered
    drastically different from each other by our algorithm. In contrast, a person
    1.2 m tall will be considered of roughly the same height as the person 1.69 m
    tall! This loss of detail is a side effect of discretization, and it is an issue
    that we deal with when creating models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Adult dataset, we can create a `LongHours` feature, which tells us if
    a person works more than 40 hours per week. This turns our continuous feature
    (`Hours-per-week`) into a categorical one that is True if the number of hours
    is more than 40, False otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '`adult["LongHours"] = adult["Hours-per-week"] > 40`'
  prefs: []
  type: TYPE_NORMAL
- en: Creating good features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simplification due to modeling is a key reason we do not have data mining methods
    that can just simply be applied to any dataset. A good data mining practitioner
    will need, or obtain, domain knowledge in the area they are applying data mining.
    They will look at the problem, the available data, and come up with a model that
    represents what they are trying to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a person's height feature may describe one component of a person,
    such as their ability to play basketball, but may not describe their academic
    performance well. If we were attempting to predict a person's grade, we may not
    bother measuring each person's height.
  prefs: []
  type: TYPE_NORMAL
- en: This is where data mining becomes more art than science. Extracting good features
    is difficult and is the topic of significant and ongoing research. Choosing better
    classification algorithms can improve the performance of a data mining application,
    but choosing better features is often a better option.
  prefs: []
  type: TYPE_NORMAL
- en: In all data mining applications, you should first outline what you are looking
    for before you start designing the methodology that will find it. This will dictate
    the types of features you are aiming for, the types of algorithms that you can
    use, and the expectations in the final result.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After initial modeling, we will often have a large number of features to choose
    from, but we wish to select only a small subset. There are many possible reasons
    for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reducing complexity**: Many data mining algorithms need significantly more
    time and resources when the number of features increase. Reducing the number of
    features is a great way to make an algorithm run faster or with fewer resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducing noise**: Adding extra features doesn''t always lead to better performance.
    Extra features may confuse the algorithm, finding correlations and patterns in
    training data that do not have any actual meaning. This is common in both smaller
    and larger datasets. Choosing only appropriate features is a good way to reduce
    the chance of random correlations that have no real meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creating readable models**: While many data mining algorithms will happily
    compute an answer for models with thousands of features, the results may be difficult
    to interpret for a human. In these cases, it may be worth using fewer features
    and creating a model that a human can understand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some classification algorithms can handle data with issues such as those described
    before. Getting the data right and getting the features to effectively describe
    the dataset you are modeling can still assist algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: There are some basic tests we can perform, such as ensuring that the features
    are at least different. If a feature's values are all the same, it can't give
    us extra information to perform our data mining.
  prefs: []
  type: TYPE_NORMAL
- en: 'The VarianceThreshold transformer in `scikit-learn`, for instance, will remove
    any feature that doesn''t have at least a minimum level of variance in the values.
    To show how this works, we first create a simple matrix using NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the numbers 0 to 29, in three columns and 10 rows. This represents
    a synthetic dataset with 10 samples and three features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we set the entire second column/feature to the value 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The result has lots of variance in the first and third rows, but no variance
    in the second row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create a `VarianceThreshold` transformer and apply it to our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the result `Xt` does not have the second column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can observe the variances for each column by printing the `vt.variances_
    attribute:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The result shows that while the first and third column contains at least some
    information, the second column had no variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: A simple and obvious test like this is always good to run when seeing data for
    the first time. Features with no variance do not add any value to a data mining
    application; however, they can slow down the performance of the algorithm and
    reduce the efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the best individual features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we have a number of features, the problem of finding the best subset is a
    difficult task. It relates to solving the data mining problem itself, multiple
    times. As we saw in [Chapter 4](7d14d664-fb65-47ed-af12-446f4e182f03.xhtml), *Recommending
    Movi**es Using Affinity Analysis*, subset-based tasks increase exponentially as
    the number of features increase. This exponential growth in the time needed is
    also true for finding the best subset of features.
  prefs: []
  type: TYPE_NORMAL
- en: One basic workaround to this problem is not to look for a subset that works
    well together, rather than just finding the best individual features. This univariate
    feature selection gives us a score based on how well a feature performs by itself.
    This is usually done for classification tasks, and we generally measure some type
    of association between a variable and the target class.
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn package has a number of transformers for performing univariate
    feature selection. They include SelectKBest, which returns the k-best-performing
    features, and SelectPercentile, which returns the top R% of features. In both
    cases, there are a number of methods of computing the quality of a feature.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different methods to compute how effectively a single feature
    correlates with a class value. A commonly used method is the chi-squared (*χ2*)
    test. Other methods include mutual information and entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can observe single-feature tests in action using our Adult dataset. First,
    we extract a dataset and class values from our pandas DataFrame. We get a selection
    of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also create a target class array by testing whether the Earnings-Raw
    value is above $50,000 or not. If it is, the class will be True. Otherwise, it
    will be False. Let''s look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create our transformer using the chi2 function and a SelectKBest transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Running `fit_transform` will call fit and then transform with the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The result will create a new dataset, choosing only the best three features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The resulting matrix now only contains three features. We can also get the scores
  prefs: []
  type: TYPE_NORMAL
- en: for each column, allowing us to find out which features were used. Let's look
    at
  prefs: []
  type: TYPE_NORMAL
- en: 'the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The printed results give us these scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The highest values are for the first, third, and fourth columns Correlates to
    the Age, Capital-Gain, and Capital-Loss features. Based on a univariate feature
    selection, these are the best features to choose.
  prefs: []
  type: TYPE_NORMAL
- en: If you'd like to find out more about the features in the Adult dataset, take
    a look at the adult.names file that comes with the dataset and the academic paper
    it references.
  prefs: []
  type: TYPE_NORMAL
- en: We could also implement other correlations, such as the Pearson's correlation
    coefficient. This is implemented in SciPy, a library used for scientific computing
    (scikit-learn uses it as a base).
  prefs: []
  type: TYPE_NORMAL
- en: If scikit-learn is working on your computer, so is SciPy. You do not need to
    install anything further to get this sample working.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the `pearsonr` function from SciPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding function almost fits the interface needed to be used in scikit-learn's
    univariate transformers. The function needs to accept two arrays (x and y in our
    example) as parameters and returns two arrays, the scores for each feature and
    the corresponding p-values. The chi2 function we used earlier only uses the required
    interface, which allowed us to just pass it directly to SelectKBest.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pearsonr function in SciPy accepts two arrays; however, the X array it
    accepts is only one dimension. We will write a wrapper function that allows us
    to use this for multivariate arrays like the one we have. Let''s look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The Pearson value could be between -1 and 1\. A value of 1 implies a perfect
    correlation between two variables, while a value of -1 implies a perfect negative
    correlation, that is, high values in one variable give low values in the other
    and vice versa. Such features are really useful to have. For this reason, we have
    stored the absolute value in the scores array, rather than the original, signed
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can use the transformer class as before to rank the features using
    the Pearson correlation coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns a different set of features! The features chosen this way are
    the first, second, and fifth columns: the Age, Education, and Hours-per-week worked.
    This shows that there is not a definitive answer to what the best features are—
    it depends on the metric used and the process undertaken.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see which feature set is better by running them through a classifier.
    Keep in mind that the results only indicate which subset is better for a particular
    classifier and/or feature combination—there is rarely a case in data mining where
    one method is strictly better than another in all cases! Let''s look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The chi2 average here is 0.83, while the Pearson score is lower at 0.77\. For
    this combination, chi2 returns better results!
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth remembering the goal of this particular data mining activity: predicting
    wealth. Using a combination of good features and feature selection, we can achieve
    83 percent accuracy using just three features of a person!'
  prefs: []
  type: TYPE_NORMAL
- en: Feature creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, just selecting features from what we have isn't enough. We can create
    features in different ways from features we already have. The one-hot encoding
    method we saw previously is an example of this. Instead of having category features
    with options A, B, and C, we would create three new features *Is it A?*, *Is it
    B?*, *Is it C?*.
  prefs: []
  type: TYPE_NORMAL
- en: Creating new features may seem unnecessary and to have no clear benefit—after
    all, the information is already in the dataset and we just need to use it. However,
    some algorithms struggle when features correlate significantly, or if there are
    redundant features. They may also struggle if there are redundant features. For
    this reason, there are various ways to create new features from the features we
    already have.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to load a new dataset, so now is a good time to start a new Jupyter Notebook.
    Download the Advertisements dataset from [http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements](http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements)
    and save it to your Data folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to load the dataset with pandas. First, we set the data''s filename
    as always:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: There are a couple of issues with this dataset that stop us from loading it
    easily. You can see these issues by trying to load the dataset with `pd.read_csv`. First,
    the first few features are numerical, but pandas will load them as strings. To
    fix this, we need to write a converting function that will convert strings to
    numbers if possible. Otherwise, we will get a **Not a Number** (NaN) - an invalid
    value, which is a special value that indicates that the value could not be interpreted
    as a number. It is similar to none or null in other programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue with this dataset is that some values are missing. These are represented
    in the dataset using the string ?. Luckily, the question mark doesn't convert
    to a float, so we can convert those to NaNs using the same concept. In further
    chapters, we will look at other ways of dealing with missing values like this.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a function that will do this conversion for us. It attempts
    to convert the number to a float, and if that fails, it returns NumPy''s special
    NaN value that can be stored in place of a float:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create a dictionary for the conversion. We want to convert all of the
    features to floats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we want to set the final column, the class, (column index #1558) to a
    binary feature. In the Adult dataset, we created a new feature for this. In the
    dataset, we will convert the feature while we load it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can load the dataset using `read_csv`. We use the converters parameter
    to pass our custom conversion into pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting dataset is quite large, with 1,559 features and more than 3,000
    rows. Here are some of the feature values, the first five, printed by inserting
    `ads.head()` into a new cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: This dataset describes images on websites, with the goal of determining whether
    a given image is an advertisement or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'The features in this dataset are not described well by their headings. There
    are two files accompanying the ad.data file that have more information: `ad.DOCUMENTATION`
    and `ad.names`. The first three features are the height, width, and ratio of the
    image size. The final feature is 1 if it is an advertisement and 0 if it is not.'
  prefs: []
  type: TYPE_NORMAL
- en: The other features are 1 for the presence of certain words in the URL, alt text,
    or caption of the image. These words, such as the word sponsor, are used to determine
    if the image is likely to be an advertisement. Many of the features overlap considerably,
    as they are combinations of other features. Therefore, this dataset has a lot
    of redundant information.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our dataset loaded in `pandas`, we will now extract the `x` and `y` data
    for our classification algorithms. The `x` matrix will be all of the columns in
    our Dataframe, except for the last column. In contrast, the `y` array will be
    only that last column, feature `1558.` Before that though, we simplify our dataset
    (just for this chapter''s sake) by dropping any row with a NaN value. Let''s look
    at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: More than 1000 rows are dropped due to this command, which is fine for our exercise.
    For real-world applications, you don't want to discard data if you can help it--instead,
    you can use interpolation or value replacing to fill the NaN values. As an example,
    you can replace any missing value with the average for that column.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some datasets, features heavily correlate with each other. For example, the
    speed and the fuel consumption would be heavily correlated in a go-kart with a
    single gear. While it can be useful to find these correlations for some applications,
    data mining algorithms typically do not need the redundant information.
  prefs: []
  type: TYPE_NORMAL
- en: The ads dataset has heavily correlated features, as many of the keywords are
    repeated across the alt text and caption.
  prefs: []
  type: TYPE_NORMAL
- en: The Principal Component Analysis (PCA) algorithm aims to find combinations of
    features that describe the dataset in less information. It aims to discover *principal
    components*, which are features that do not correlate with each other and explain
    the information—specifically the variance—of the dataset. What this means is that
    we can often capture most of the information in a dataset in fewer features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply PCA just like any other transformer. It has one key parameter, which
    is the number of components to find. By default, it will result in as many features
    as you have in the original dataset. However, these principal components are ranked—the
    first feature explains the largest amount of the variance in the dataset, the
    second a little less, and so on. Therefore, finding just the first few features
    is often enough to explain much of the dataset. Let''s look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting matrix, Xd, has just five features. However, let''s look at the
    amount of variance that is explained by each of these features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The result, `array([ 0.854, 0.145, 0.001, 0\. , 0\. ])`, shows us that the first
    feature accounts for 85.4 percent of the variance in the dataset, the second accounts
    for 14.5 percent, and so on. By the fourth feature, less than one-tenth of a percent
    of the variance is contained in the feature. The other 1,553 features explain
    even less (this is an ordered array).
  prefs: []
  type: TYPE_NORMAL
- en: The downside to transforming data with PCA is that these features are often
    complex combinations of the other features. For example, the first feature of
    the preceding code starts with `[-0.092, -0.995, -0.024],` that is, multiply the
    first feature in the original dataset by -0.092, the second by -0.995, the third
    by -0.024\. This feature has 1,558 values of this form, one for each of the original
    datasets (although many are zeros). Such features are indistinguishable by humans
    and it is hard to glean much relevant information from without a lot of experience
    working with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using PCA can result in models that not only approximate the original dataset, but
    can also improve the performance in classification tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The resulting score is 0.9356,  which is (slightly) higher than our original
    model's score. PCA won't always give a benefit like this, but it does more often
    than not.
  prefs: []
  type: TYPE_NORMAL
- en: We are using PCA here to reduce the number of features in our dataset. As a
    general rule, you shouldn't use it to reduce overfitting in your data mining experiments.
    The reason for this is that PCA doesn't take classes into account. A better solution
    is to use regularization. An introduction, with code, is available at [http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/](http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/)
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage is that PCA allows you to plot datasets that you otherwise
    couldn't easily visualize. For example, we can plot the first two features returned
    by PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we tell our Notebook to display plots inline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we get all of the distinct classes in our dataset (there are only two:
    is ad or not ad):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We also assign colors to each of these classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We use zip to iterate over both lists at the same time, then extract all samples
    from that class, and plot them with the color appropriate to the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, outside the loop, we create a legend and show the graph, showing where
    the samples from each class appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B06162_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating your own transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the complexity and type of dataset changes, you might find that you can't
    find an existing feature extraction transformer that fits your needs. We will
    see an example of this in [Chapter 7](67684c84-b6f7-4f09-b0ce-cabe2d2c373d.xhtml),
    *Follow Recommendations Using Graph Mining*, where we create new features from
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: A transformer is akin to a converting function. It takes data of one form as
    input and returns data of another form as output. Transformers can be trained
    using some training dataset, and these trained parameters can be used to convert
    testing data.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer API is quite simple. It takes data of a specific format as input
    and returns data of another format (either the same as the input or different)
    as output. Not much else is required of the programmer.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Transformers have two key functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fit():` This takes a training set of data as input and sets internal parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transform():` This performs the transformation itself. This can take either
    the training dataset, or a new dataset of the same format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both `fit()` and `transform()` functions should take the same data type as input,
    but `transform()` can return data of a different type while `fit()` always returns
    self.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to create a trivial transformer to show the API in action. The
    transformer will take a NumPy array as input, and discretize it based on the mean.
    Any value higher than the mean (of the training data) will be given the value
    1 and any value lower or equal to the mean will be given the value 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'We did a similar transformation with the Adult dataset using pandas: we took
    the Hours-per-week feature and created a LongHours feature if the value was more
    than 40 hours per week. This transformer is different for two reasons. First,
    the code will conform to the scikit-learn API, allowing us to use it in a pipeline.
    Second, the code will learn the mean, rather than taking it as a fixed value (such
    as 40 in the LongHours example).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start, open up the Jupyter Notebook that we used for the Adult dataset. Then,
    click on the Cell menu item and choose Run All. This will rerun all of the cells
    and ensure that the notebook is up to date.
  prefs: []
  type: TYPE_NORMAL
- en: First, we import the TransformerMixin, which sets the API for us. While Python
    doesn't have strict interfaces (as opposed to languages like Java), using a mixin like
    this allows scikit-learn to determine that the class is actually a transformer.
    We also need to import a function that checks the input is of a valid type. We
    will use that soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at our class in entirety, and then we will revisit some
    of the details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Our class will learn the mean for each feature in the fit method, by computing `X.mean(axis=0)`,
    which is then stored as an object attribute. After that, the fit function returns
    self, conforming to the API (scikit-learn uses this to allow for chaining function
    calls).
  prefs: []
  type: TYPE_NORMAL
- en: After fitting, the transform function takes a matrix with the same number of
    features (confirmed by the `assert` statement), and simply returns which values
    are more than the mean for a given feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our class is built, we can now create an instance of this class and
    use it to transform our X array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Take a shot at implementing this Transformer into a workflow, both using a Pipeline
    and without. You'll see that by conforming to the Transformer API, it is quite
    simple to use in place of a built-in scikit-learn Transformer object.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When creating your own functions and classes, it is always a good idea to do
    unit testing. Unit testing aims to test a single unit of your code. In this case,
    we want to test that our transformer does as it needs to do.
  prefs: []
  type: TYPE_NORMAL
- en: Good tests should be independently verifiable. A good way to confirm the legitimacy
    of your tests is by using another computer language or method to perform the calculations.
    In this case, I used Excel to create a dataset, and then computed the mean for
    each cell. Those values were then transferred to the unit test.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests should also, generally, be small and quick to run. Therefore, any
    data used should be of a small size. The dataset I used for creating the tests
    is stored in the Xt variable from earlier, which we will recreate in our test.
    The mean of these two features is 13.5 and 15.5, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our unit test, we import the `assert_array_equal` function from NumPy''s
    testing, which checks whether two arrays are equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from numpy.testing import assert_array_equal`'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create our function. It is important that the test's name starts with
    test_,
  prefs: []
  type: TYPE_NORMAL
- en: 'as this nomenclature is used for tools that automatically find and run tests.
    We also set up our testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run the test by simply running the function itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: If there was no error, then the test ran without an issue! You can verify this
    by changing some of the tests to deliberately make values incorrect, and confirming
    that the test fails. Remember to change them back so that the test passes!
  prefs: []
  type: TYPE_NORMAL
- en: If we had multiple tests, it would be worth using a testing framework, like py.test
    or nose to run our tests. Using a framework like this is beyond the scope of this
    book, but they manage running tests, recording failures, and providing feedback
    to you, as a programmer, to help you improve your code.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a tested transformer, it is time to put it into action. Using
    what we have learned so far, we create a Pipeline, set the first step to the MeanDiscrete
    transformer, and the second step to a Decision Tree Classifier. We then run a
    cross-validation and print out the result. Let''s look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The result is 0.917, which is not as good as before, but very good for a simple
    binary feature model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at features and transformers and how they can be
    used in the data mining pipeline. We discussed what makes a good feature and how
    to algorithmically choose good features from a standard set. However, creating
    good features is more art than science and often requires domain knowledge and
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: We then created our own transformer using an interface that allows us to use
    it in scikit-learn's helper functions. We will be creating more transformers in
    later chapters so that we can perform effective testing using existing functions.
  prefs: []
  type: TYPE_NORMAL
- en: To take the lessons learned in this chapter further, I recommend signing up
    to the online data mining competition website [Kaggle.com](http://Kaggle.com)
    and trying some of the competitions. Their recommended starting place is the Titanic
    dataset, which allows you to practice the feature creation aspects of this chapter.
    Many of the features are not numerical, requiring you to convert them to numerical
    features before applying a data mining algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we use feature extraction on a corpus of text documents.
    There are many transformers and feature types for text, each with their advantages
    and disadvantages.
  prefs: []
  type: TYPE_NORMAL
