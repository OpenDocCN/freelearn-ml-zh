- en: Features and scikit-learn Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征和scikit-learn转换器
- en: The datasets we have used so far have been described in terms of *features*.
    In the previous chapter, we used a transaction-centric dataset. However, ultimately
    this was just a different format for representing feature-based data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止所使用的数据集都是以*特征*的形式描述的。在前一章中，我们使用了一个以事务为中心的数据集。然而，这最终只是以不同格式表示基于特征的数据的另一种方式。
- en: There are many other types of datasets, including text, images, sounds, movies,
    or even real objects. Most data mining algorithms rely on having numerical or
    categorical features. This means we need a way to represent these types before
    we input them into the data mining algorithm. We call this representation a **model**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他类型的数据集，包括文本、图像、声音、电影，甚至是真实物体。大多数数据挖掘算法都依赖于具有数值或分类特征。这意味着在我们将这些类型输入数据挖掘算法之前，我们需要一种方法来表示它们。我们称这种表示为**模型**。
- en: In this chapter, we will discuss how to extract numerical and categorical features,
    and choose the best features when we do have them. We will discuss some common
    patterns and techniques for extracting features. Choosing your model appropriately
    is critically important to the outcome of the data mining exercise, more so than
    the choice of classification algorithm.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何提取数值和分类特征，并在我们有这些特征时选择最佳特征。我们将讨论一些常见的特征提取模式和技巧。适当地选择你的模型对于数据挖掘练习的结果至关重要，比分类算法的选择更为重要。
- en: 'The key concepts introduced in this chapter include:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的关键概念包括：
- en: Extracting features from datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中提取特征
- en: Creating models for your data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的数据创建模型
- en: Creating new features
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建新特征
- en: Selecting good features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择好的特征
- en: Creating your own transformer for custom datasets
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为自定义数据集创建自己的转换器
- en: Feature extraction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取
- en: Extracting features is one of the most critical tasks in data mining, and it
    generally affects your end result more than the choice of data mining algorithm.
    Unfortunately, there are no hard and fast rules for choosing features that will
    result in high-performance data mining. The choice of features determines the
    model that you are using to represent your data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 提取特征是数据挖掘中最关键的任务之一，它通常比数据挖掘算法的选择对最终结果的影响更大。不幸的是，没有一成不变的规则来选择能够导致高性能数据挖掘的特征。特征的选择决定了你用来表示数据的模型。
- en: Model creation is where the science of data mining becomes more of an art and
    why automated methods of performing data mining (there are several methods of
    this type) focus on algorithm choice and not model creation. Creating good models
    relies on intuition, domain expertise, data mining experience, trial and error,
    and sometimes a little luck.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 模型创建是数据挖掘科学变得更加像艺术的地方，这也是为什么执行数据挖掘的自动化方法（有几种此类方法）专注于算法选择而不是模型创建。创建好的模型依赖于直觉、领域专业知识、数据挖掘经验、试错，有时还需要一点运气。
- en: Representing reality in models
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在模型中呈现现实
- en: Given what we have done so far in the book, it is easy to forget that the reason
    we are performing data mining is to affect real world objects, not just manipulating
    a matrix of values. Not all datasets are presented in terms of features. Sometimes,
    a dataset consists of nothing more than all of the books that have been written
    by a given author. Sometimes, it is the film of each of the movies released in
    1979\. At other times, it is a library collection of interesting historical artifacts.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在本书中迄今为止所做的工作，很容易忘记我们进行数据挖掘的原因是影响现实世界中的对象，而不仅仅是操作一个值矩阵。并非所有数据集都是以特征的形式呈现的。有时，一个数据集可能仅仅是由某个作者所写的所有书籍组成。有时，它可能是1979年发布的每部电影的影片。在其他时候，它可能是一个有趣的历史文物的图书馆收藏。
- en: From these datasets, we may want to perform a data mining task. For the books,
    we may want to know the different categories that the author writes. In the films,
    we may wish to see how women are portrayed. In the historical artifacts, we may
    want to know whether they are from one country or another. It isn't possible to
    just pass these raw datasets into a decision tree and see what the result is.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些数据集中，我们可能想要执行数据挖掘任务。对于书籍，我们可能想知道作者写了哪些不同的类别。在电影中，我们可能希望看到女性是如何被描绘的。在历史文物中，我们可能想知道它们是来自一个国家还是另一个国家。仅仅将这些原始数据集输入决策树并查看结果是不可能的。
- en: For a data mining algorithm to assist us here, we need to represent these as
    **features**. Features are a way to create a model and the model provides an approximation
    of reality in a way that data mining algorithms can understand. Therefore, a model
    is just a simplified version of some aspect of the real world. As an example,
    the game of chess is a simplified model (in game form) for historical warfare.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让数据挖掘算法在这里帮助我们，我们需要将这些数据表示为**特征**。特征是创建模型的一种方式，而模型以数据挖掘算法能够理解的方式提供对现实的一种近似。因此，模型只是现实世界某个方面的简化版本。例如，象棋游戏就是历史战争的一种简化模型（以游戏形式）。
- en: 'Selecting features has another advantage: they reduce the complexity of the
    real world into a more manageable model.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 选择特征还有另一个优点：它们将现实世界的复杂性简化为更易于管理的模型。
- en: Imagine how much information it would take to properly, accurately, and fully
    describe a real-world object to someone that has no background knowledge of the
    item. You would need to describe the size, weight, texture, composition, age,
    flaws, purpose, origin, and so on.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，要向一个对物品没有任何背景知识的人准确、全面地描述一个现实世界对象需要多少信息。你需要描述其大小、重量、质地、成分、年龄、缺陷、用途、起源等等。
- en: As the complexity of real objects is too much for current algorithms, we use
    these simpler models instead.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现实对象的复杂性超出了当前算法的处理能力，我们使用这些更简单的模型来代替。
- en: This simplification also focuses our intent in the data mining application.
    In later chapters, we will look at clustering and where it is critically important.
    If you put random features in, you will get random results out.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简化也使我们在数据挖掘应用中的意图更加集中。在后面的章节中，我们将探讨聚类及其至关重要的应用。如果你输入随机特征，你将得到随机的结果。
- en: However, there is a downside as this simplification reduces the detail, or may
    remove good indicators of the things we wish to perform data mining on.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种简化也有缺点，因为它减少了细节，或者可能移除了我们希望进行数据挖掘的某些事物的良好指标。
- en: Thought should always be given to how to represent reality in the form of a
    model. Rather than just using what has been used in the past, you need to consider
    the goal of the data mining exercise. What are you trying to achieve? In [Chapter
    3](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml), *Predicting Sports Winners with
    Decision Trees*, we created features by thinking about the goal (predicting winners)
    and used a little domain knowledge to come up with ideas for new features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该始终思考如何以模型的形式表示现实。而不仅仅是使用过去使用过的方法，你需要考虑数据挖掘活动的目标。你试图实现什么？在[第3章](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml)《使用决策树预测体育比赛胜者》中，我们通过思考目标（预测胜者）并使用一些领域知识来提出新特征的想法来创建特征。
- en: Not all features need to be numeric or categorical. Algorithms have been developed
    that work directly on text, graphs, and other data structures. Unfortunately,
    those algorithms are outside the scope of this book. In this book, and normally
    in your data mining career, we mainly use numeric or categorical features.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有特征都需要是数值或分类的。已经开发出可以直接在文本、图和其他数据结构上工作的算法。不幸的是，这些算法超出了本书的范围。在本书中，以及在你的数据挖掘生涯中，我们主要使用数值或分类特征。
- en: The *Adult* dataset is a great example of taking a complex reality and attempting
    to model it using features. In this dataset, the aim is to estimate if someone
    earns more than $50,000 per year.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*Adult*数据集是使用特征来尝试对复杂现实进行建模的一个很好的例子。在这个数据集中，目标是估计某人每年是否赚超过$50,000。'
- en: To download the dataset, navigate to [http://archive.ics.uci.edu/ml/datasets/Adult](http://archive.ics.uci.edu/ml/datasets/Adult) and
    click on the Data Folder link. Download the `adult.data` and `adult.names` into
    a directory named Adult in your data folder.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载数据集，请导航到[http://archive.ics.uci.edu/ml/datasets/Adult](http://archive.ics.uci.edu/ml/datasets/Adult)并点击数据文件夹链接。将`adult.data`和`adult.names`下载到你的数据文件夹中名为Adult的目录下。
- en: This dataset takes a complex task and describes it in features. These features
    describe the person, their environment, their background, and their life status.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集将一个复杂任务描述为特征。这些特征描述了个人、他们的环境、他们的背景以及他们的生活状况。
- en: 'Open a new Jupyter Notebook for this chapter, set the data filename and load
    the data with pandas:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为本章打开一个新的Jupyter Notebook，设置数据文件名，并使用pandas加载数据：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Most of the code is the same as in the previous chapters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码与前面的章节相同。
- en: 'Don''t want to type those heading names? Don''t forget you can download the
    code from Packt Publishing, or alternatively from the author''s GitHub repository
    for this book:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不想输入那些标题名称？别忘了你可以从 Packt Publishing 下载代码，或者从本书作者的 GitHub 仓库下载：
- en: '[https://github.com/dataPipelineAU/LearningDataMiningWithPython2](https://github.com/dataPipelineAU/LearningDataMiningWithPython2)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/dataPipelineAU/LearningDataMiningWithPython2](https://github.com/dataPipelineAU/LearningDataMiningWithPython2)'
- en: 'The adult file itself contains two blank lines at the end of the file. By default,
    pandas will interpret the penultimate new line to be an empty (but valid) row.
    To remove this, we remove any line with invalid numbers (the use of `inplace`
    just makes sure the same Dataframe is affected, rather than creating a new one):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 成年文件本身在文件末尾包含两个空白行。默认情况下，pandas 将倒数第二个换行符解释为一个空行（但有效）。为了删除它，我们删除任何包含无效数字的行（使用
    `inplace` 只确保影响相同的 Dataframe，而不是创建一个新的 Dataframe）：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Having a look at the dataset, we can see a variety of features from `adult.columns`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 查看数据集，我们可以从 `adult.columns` 中看到各种特征：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The results show each of the feature names that are stored inside an Index
    object from pandas:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示了存储在 pandas Index 对象内的每个特征名称：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Common feature patterns
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的特征模式
- en: While there are millions of ways to create models, there are some common patterns
    that are employed across different disciplines. However, choosing appropriate
    features is tricky and it is worth considering how a feature might correlate to
    the end result. As a well known adage goes, *don't judge a book by its cover*—it
    is probably not worth considering the size of a book if you are interested in
    the message contained within.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有数百万种创建模型的方法，但不同学科中都有一些常见的模式。然而，选择合适的特征是棘手的，值得考虑一个特征可能如何与最终结果相关。正如一句著名的谚语所说，*不要以貌取人*——如果你对书中的信息感兴趣，考虑书的尺寸可能并不值得。
- en: 'Some commonly used features focus on the physical properties of the real world
    objects being studied, for example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常用的特征专注于研究现实世界对象的物理属性，例如：
- en: Spatial properties such as the length, width, and height of an object
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体的空间属性，如长度、宽度和高度
- en: Weight and/or density of the object
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体的重量和/或密度
- en: Age of an object or its components
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体或其组件的年龄
- en: The type of the object
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体的类型
- en: The quality of the object
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体的质量
- en: 'Other features might rely on the usage or history of the object:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其他特征可能依赖于对象的使用或历史：
- en: The producer, publisher, or creator of the object
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体的生产者、出版商或创作者
- en: The year of manufacturing
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制造年份
- en: 'Other features describe a dataset in terms of its components:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其他特征以对象的部分来描述数据集：
- en: Frequency of a given subcomponent, such as a word in a book
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定子组件的频率，例如一本书中的单词
- en: Number of subcomponents and/or the number of different subcomponents
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子组件的数量和/或不同子组件的数量
- en: Average size of the subcomponents, such as the average sentence length
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子组件的平均大小，例如平均句子长度
- en: Ordinal features allow us to perform ranking, sorting, and grouping of similar
    values. As we have seen in previous chapters, features can be numerical or categorical.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有序特征使我们能够对相似值进行排序、排序和分组。正如我们在前面的章节中看到的，特征可以是数值的或分类的。
- en: Numerical features are often described as being ordinal. For example, three
    people, Alice, Bob, and Charlie, may have heights of 1.5 m, 1.6 m, and 1.7 m.
    We would say that Alice and Bob are more similar in height than Alice and Charlie.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数值特征通常被描述为有序的。例如，三个人，Alice、Bob 和 Charlie，可能有 1.5 米、1.6 米和 1.7 米的身高。我们会说 Alice
    和 Bob 在身高上比 Alice 和 Charlie 更相似。
- en: 'The Adult dataset that we loaded in the last section contains examples of continuous,
    ordinal features. For example, the Hours-per-week feature tracks how many hours
    per week people work. Certain operations make sense on a feature like this. They
    include computing the mean, standard deviation, minimum, and maximum. There is
    a function in pandas for giving some basic summary stats of this type:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一个部分加载的 Adult 数据集包含连续的有序特征的例子。例如，每周工作小时数特征跟踪人们每周工作多少小时。某些操作适用于此类特征。包括计算平均值、标准差、最小值和最大值。pandas
    中有一个函数可以提供此类类型的一些基本摘要统计信息：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The result tells us a little about this feature:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结果告诉我们关于这个特征的一些信息：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Some of these operations do not make sense for other features. For example,
    it doesn't make sense to compute the sum of the education statuses of these people.
    In contrast, it would make sense to compute the sum of the number of orders by
    each customer on an online store.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作中的一些对于其他特征来说没有意义。例如，计算这些人的教育状态总和是没有意义的。相比之下，计算每个在线商店顾客的订单总数是有意义的。
- en: 'There are also features that are not numerical, but still ordinal. The Education
    feature in the Adult dataset is an example of this. For example, a Bachelor''s
    degree is a higher education status than finishing high school, which is a higher
    status than not completing high school. It doesn''t quite make sense to compute
    the mean of these values, but we can create an approximation by taking the median
    value. The dataset gives a helpful feature, `Education-Num`, which assigns a number
    that is basically equivalent to the number of years of education completed. This
    allows us to quickly compute the median:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些特征不是数值的，但仍然是序数的。成年数据集中的教育特征就是这样一个例子。例如，学士学位比完成高中教育有更高的教育地位，而完成高中教育比没有完成高中教育有更高的地位。对这些值计算平均值并不完全合理，但我们可以通过取中位数来创建一个近似值。数据集提供了一个有用的特征，`Education-Num`，它分配一个基本上等同于完成教育年数的数字。这使得我们可以快速计算中位数：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The result is 10, or finishing one year past high school. If we didn't have
    this, we could compute the median by creating an ordering over the education values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是10，即完成高中后的一年。如果我们没有这个，我们可以通过在教育值上创建一个排序来计算中位数。
- en: Features can also be categorical. For instance, a ball can be a tennis ball,
    cricket ball, football, or any other type of ball. Categorical features are also
    referred to as nominal features. For nominal features, the values are either the
    same or they are different. While we could rank balls by size or weight, just
    the category alone isn't enough to compare things. A tennis ball is not a cricket
    ball, and it is also not a football. We could argue that a tennis ball is more
    similar to a cricket ball (say, in size), but the category alone doesn't differentiate
    this—they are the same, or they are not.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 特征也可以是分类的。例如，一个球可以是网球、板球、足球或其他任何类型的球。分类特征也被称为名义特征。对于名义特征，其值要么相同，要么不同。虽然我们可以根据大小或重量对球进行排序，但仅仅类别本身并不足以比较事物。网球不是板球，它也不是足球。我们可以争论网球在大小上可能更接近板球（比如说），但仅仅类别本身并不能区分这一点——它们要么相同，要么不同。
- en: 'We can convert categorical features to numerical features using the one-hot
    encoding, as we saw in [Chapter 3](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml),
    *Predicting Sports Winners with Decision Trees*. For the aforementioned categories
    of balls, we can create three new binary features: is a tennis ball, is a cricket
    ball, and is a football. This process is the one-hot encoding we used in [Chapter
    3](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml), *Predicting Sports Winners with
    Decision Trees*. For a tennis ball, the vector would be `[1, 0, 0]`. A cricket
    ball has the values `[0, 1, 0]`, while a football has the value `[0, 0, 1]`. These
    are binary features but can be used as continuous features by many algorithms.
    One key reason for doing this is that it easily allows for direct numerical comparison
    (such as computing the distance between samples).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用独热编码将分类特征转换为数值特征，正如我们在[第3章](e3e085cc-17ce-429b-8a71-b4f09a12b672.xhtml)中看到的，即*使用决策树预测体育比赛胜者*。对于上述球类的类别，我们可以创建三个新的二元特征：是否是网球、是否是板球和是否是足球。这个过程就是我们第3章中使用的独热编码，即*使用决策树预测体育比赛胜者*。对于一个网球，向量将是
    `[1, 0, 0]`。板球的值是 `[0, 1, 0]`，而足球的值是 `[0, 0, 1]`。这些是二元特征，但许多算法可以将它们用作连续特征。这样做的一个关键原因是可以轻松地进行直接的数值比较（例如计算样本之间的距离）。
- en: The Adult dataset contains several categorical features, with Work-Class being
    one example. While we could argue that some values are of higher rank than others
    (for instance, a person with a job is likely to have a better income than a person
    without), it doesn't make sense for all values. For example, a person working
    for the state government is not more or less likely to have a higher income than
    someone working in the private sector.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 成年数据集包含几个分类特征，其中工作类别就是一个例子。虽然我们可以争论某些值可能比其他值有更高的等级（例如，有工作的人可能比没有工作的人有更好的收入），但对于所有值来说这并不合理。例如，为国家政府工作的人并不比在私营部门工作的人更有可能或更不可能有更高的收入。
- en: 'We can view the unique values for this feature in the dataset using the `unique()`
    function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`unique()`函数在数据集中查看该特征的唯一值：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The result shows the unique values in this column:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示了该列的唯一值：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There are some missing values in the preceding data, but they won't affect our
    computations in this example. You can also use the `adult.value_counts()` function
    to see how frequently each value appears.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的数据中存在一些缺失值，但它们不会影响本例中的计算。您也可以使用`adult.value_counts()`函数查看每个值出现的频率。
- en: 'Another really useful step to take with a new dataset is to visualise it. The
    following code will create a swarm plot, giving a view of how education and hours-worked
    relate to the final classification (identified by colour):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用新的数据集时，另一个非常有用的步骤是可视化它。以下代码将创建一个群组图，展示教育和工作时间与最终分类（通过颜色标识）之间的关系：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/B06162_05_03.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_05_03.png)'
- en: In the above code, we sample the dataset to show every 50 rows, using the `adult[::50]`
    dataset indexing. Setting this to just `adult` will result in all samples being
    shown, but that may also make the graph hard to read.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们通过使用`adult[::50]`数据集索引来采样数据集，以显示每50行，设置此为`adult`将导致显示所有样本，但这可能会使图表难以阅读。
- en: Similarly, we can convert numerical features to categorical features through
    a process called **discretization**, as we saw in Chapter 1, *Getting Started
    With Data Mining*. We can call any person who is taller than 1.7 m tall, and any
    person shorter than 1.7 m short. This gives us a categorical feature (although
    still an ordinal one). We do lose some data here. For instance, two people, one
    1.69 m tall and one 1.71 m, will be in two different categories and considered
    drastically different from each other by our algorithm. In contrast, a person
    1.2 m tall will be considered of roughly the same height as the person 1.69 m
    tall! This loss of detail is a side effect of discretization, and it is an issue
    that we deal with when creating models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以通过称为**离散化**的过程将数值特征转换为分类特征，正如我们在第1章“数据挖掘入门”中看到的。我们可以将身高超过1.7米的人称为高，身高低于1.7米的人称为矮。这给我们一个分类特征（尽管仍然是有序的）。我们在这里会丢失一些数据。例如，两个身高分别为1.69米和1.71米的人将属于两个不同的类别，并且被我们的算法认为差异很大。相比之下，一个身高1.2米的人将被认为与身高1.69米的人大致相同！这种细节的丢失是离散化的副作用，这是我们创建模型时需要处理的问题。
- en: 'In the Adult dataset, we can create a `LongHours` feature, which tells us if
    a person works more than 40 hours per week. This turns our continuous feature
    (`Hours-per-week`) into a categorical one that is True if the number of hours
    is more than 40, False otherwise:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在成人数据集中，我们可以创建一个`LongHours`特征，它告诉我们一个人是否每周工作超过40小时。这把我们的连续特征（每周小时数）转换为一个分类特征，如果小时数超过40则为True，否则为False：
- en: '`adult["LongHours"] = adult["Hours-per-week"] > 40`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`adult["LongHours"] = adult["Hours-per-week"] > 40`'
- en: Creating good features
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建良好的特征
- en: Simplification due to modeling is a key reason we do not have data mining methods
    that can just simply be applied to any dataset. A good data mining practitioner
    will need, or obtain, domain knowledge in the area they are applying data mining.
    They will look at the problem, the available data, and come up with a model that
    represents what they are trying to achieve.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于建模的简化，这是我们没有可以简单应用于任何数据集的数据挖掘方法的关键原因。一个优秀的数据挖掘从业者将需要或获得他们在应用数据挖掘领域的领域知识。他们将研究问题、可用数据，并提出一个代表他们试图实现的目标的模型。
- en: For instance, a person's height feature may describe one component of a person,
    such as their ability to play basketball, but may not describe their academic
    performance well. If we were attempting to predict a person's grade, we may not
    bother measuring each person's height.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个人的身高特征可能描述一个人的一个方面，比如他们打篮球的能力，但可能无法很好地描述他们的学术表现。如果我们试图预测一个人的成绩，我们可能不会麻烦去测量每个人的身高。
- en: This is where data mining becomes more art than science. Extracting good features
    is difficult and is the topic of significant and ongoing research. Choosing better
    classification algorithms can improve the performance of a data mining application,
    but choosing better features is often a better option.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是数据挖掘比科学更具艺术性的地方。提取良好的特征是困难的，这也是一个重要且持续的研究课题。选择更好的分类算法可以提高数据挖掘应用的表现，但选择更好的特征通常是一个更好的选择。
- en: In all data mining applications, you should first outline what you are looking
    for before you start designing the methodology that will find it. This will dictate
    the types of features you are aiming for, the types of algorithms that you can
    use, and the expectations in the final result.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有数据挖掘应用中，你应该在开始设计寻找目标的方法之前，首先概述你正在寻找的内容。这将决定你希望达到的特征类型，你可以使用的算法类型，以及最终结果中的期望。
- en: Feature selection
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: 'After initial modeling, we will often have a large number of features to choose
    from, but we wish to select only a small subset. There are many possible reasons
    for this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步建模之后，我们通常会有一大批特征可供选择，但我们希望只选择一小部分。有许多可能的原因：
- en: '**Reducing complexity**: Many data mining algorithms need significantly more
    time and resources when the number of features increase. Reducing the number of
    features is a great way to make an algorithm run faster or with fewer resources.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降低复杂性**：当特征数量增加时，许多数据挖掘算法需要显著更多的时间和资源。减少特征数量是使算法运行更快或使用更少资源的好方法。'
- en: '**Reducing noise**: Adding extra features doesn''t always lead to better performance.
    Extra features may confuse the algorithm, finding correlations and patterns in
    training data that do not have any actual meaning. This is common in both smaller
    and larger datasets. Choosing only appropriate features is a good way to reduce
    the chance of random correlations that have no real meaning.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降低噪声**：添加额外的特征并不总是导致更好的性能。额外的特征可能会使算法混淆，在训练数据中找到没有实际意义的关联和模式。这在较小的和较大的数据集中都很常见。只选择合适的特征是减少没有实际意义的随机关联的好方法。'
- en: '**Creating readable models**: While many data mining algorithms will happily
    compute an answer for models with thousands of features, the results may be difficult
    to interpret for a human. In these cases, it may be worth using fewer features
    and creating a model that a human can understand.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建可读的模型**：虽然许多数据挖掘算法乐于为具有数千个特征的模型计算答案，但结果可能对人类来说难以解释。在这些情况下，使用较少的特征并创建一个人类可以理解的模式可能是有价值的。'
- en: Some classification algorithms can handle data with issues such as those described
    before. Getting the data right and getting the features to effectively describe
    the dataset you are modeling can still assist algorithms.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一些分类算法可以处理前面描述的问题。确保数据正确，并确保特征能够有效地描述你正在建模的数据集，这仍然可以帮助算法。
- en: There are some basic tests we can perform, such as ensuring that the features
    are at least different. If a feature's values are all the same, it can't give
    us extra information to perform our data mining.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进行一些基本的测试，例如确保特征至少是不同的。如果一个特征的所有值都相同，它就不能为我们提供额外的信息来执行我们的数据挖掘。
- en: 'The VarianceThreshold transformer in `scikit-learn`, for instance, will remove
    any feature that doesn''t have at least a minimum level of variance in the values.
    To show how this works, we first create a simple matrix using NumPy:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`scikit-learn`中的`VarianceThreshold`转换器将删除任何在值中至少没有最小变异水平的特征。为了展示这是如何工作的，我们首先使用NumPy创建一个简单的矩阵：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The result is the numbers 0 to 29, in three columns and 10 rows. This represents
    a synthetic dataset with 10 samples and three features:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是0到29的数字，分为三列和10行。这代表了一个包含10个样本和三个特征的合成数据集：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we set the entire second column/feature to the value 1:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将整个第二列/特征设置为值1：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The result has lots of variance in the first and third rows, but no variance
    in the second row:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在第一行和第三行有很多变异，但在第二行没有变异：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can now create a `VarianceThreshold` transformer and apply it to our dataset:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建一个`VarianceThreshold`转换器并将其应用于我们的数据集：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, the result `Xt` does not have the second column:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，结果`Xt`没有第二列：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can observe the variances for each column by printing the `vt.variances_
    attribute:`
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过打印`vt.variances_`属性来观察每列的变异：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The result shows that while the first and third column contains at least some
    information, the second column had no variance:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，虽然第一列和第三列至少包含一些信息，但第二列没有变异：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: A simple and obvious test like this is always good to run when seeing data for
    the first time. Features with no variance do not add any value to a data mining
    application; however, they can slow down the performance of the algorithm and
    reduce the efficacy.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当第一次看到数据时，运行这样一个简单明了的测试总是好的。没有变异的特征不会为数据挖掘应用增加任何价值；然而，它们可能会减慢算法的性能并降低其有效性。
- en: Selecting the best individual features
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择最佳单个特征
- en: If we have a number of features, the problem of finding the best subset is a
    difficult task. It relates to solving the data mining problem itself, multiple
    times. As we saw in [Chapter 4](7d14d664-fb65-47ed-af12-446f4e182f03.xhtml), *Recommending
    Movi**es Using Affinity Analysis*, subset-based tasks increase exponentially as
    the number of features increase. This exponential growth in the time needed is
    also true for finding the best subset of features.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有多个特征，找到最佳子集的问题是一个困难任务。它与解决数据挖掘问题本身相关，需要多次解决。正如我们在[第 4 章](7d14d664-fb65-47ed-af12-446f4e182f03.xhtml)中看到的，*使用亲和分析推荐电影*，随着特征数量的增加，基于子集的任务呈指数增长。这种所需时间的指数增长也适用于找到最佳特征子集。
- en: One basic workaround to this problem is not to look for a subset that works
    well together, rather than just finding the best individual features. This univariate
    feature selection gives us a score based on how well a feature performs by itself.
    This is usually done for classification tasks, and we generally measure some type
    of association between a variable and the target class.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的基本方法不是寻找能够良好协同工作的子集，而是仅仅找到最佳的单个特征。这种单变量特征选择根据特征单独表现的好坏给出一个分数。这通常用于分类任务，我们通常测量变量和目标类别之间的某种关联。
- en: The scikit-learn package has a number of transformers for performing univariate
    feature selection. They include SelectKBest, which returns the k-best-performing
    features, and SelectPercentile, which returns the top R% of features. In both
    cases, there are a number of methods of computing the quality of a feature.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 包提供了一系列用于执行单变量特征选择的转换器。它们包括 SelectKBest，它返回性能最好的 k 个特征，以及 SelectPercentile，它返回前
    R% 的特征。在这两种情况下，都有多种计算特征质量的方法。
- en: There are many different methods to compute how effectively a single feature
    correlates with a class value. A commonly used method is the chi-squared (*χ2*)
    test. Other methods include mutual information and entropy.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的方法来计算单个特征与类别值的相关性有多有效。常用的方法之一是卡方 (*χ2*) 测试。其他方法包括互信息和熵。
- en: 'We can observe single-feature tests in action using our Adult dataset. First,
    we extract a dataset and class values from our pandas DataFrame. We get a selection
    of the features:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 Adult 数据集来观察单变量测试的实际操作。首先，我们从 pandas DataFrame 中提取数据集和类值。我们得到特征的选择：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will also create a target class array by testing whether the Earnings-Raw
    value is above $50,000 or not. If it is, the class will be True. Otherwise, it
    will be False. Let''s look at the code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将通过测试 Earnings-Raw 值是否超过 $50,000 来创建一个目标类数组。如果是，则类为 True。否则，为 False。让我们看看代码：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we create our transformer using the chi2 function and a SelectKBest transformer:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 chi2 函数和 SelectKBest 转换器创建我们的转换器：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Running `fit_transform` will call fit and then transform with the same dataset.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `fit_transform` 将调用 fit 然后使用相同的数据集进行转换。
- en: The result will create a new dataset, choosing only the best three features.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将创建一个新的数据集，只选择最好的三个特征。
- en: 'Let''s look at the code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The resulting matrix now only contains three features. We can also get the scores
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 结果矩阵现在只包含三个特征。我们还可以获取分数
- en: for each column, allowing us to find out which features were used. Let's look
    at
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一列，这使我们能够找出哪些特征被使用了。让我们看看
- en: 'the code:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The printed results give us these scores:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的结果给出了这些分数：
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The highest values are for the first, third, and fourth columns Correlates to
    the Age, Capital-Gain, and Capital-Loss features. Based on a univariate feature
    selection, these are the best features to choose.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的值是第一列、第三列和第四列，分别对应年龄、资本收益和资本损失特征。基于单变量特征选择，这些是最好的选择特征。
- en: If you'd like to find out more about the features in the Adult dataset, take
    a look at the adult.names file that comes with the dataset and the academic paper
    it references.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于 Adult 数据集中的特征的信息，请查看随数据集一起提供的 adult.names 文件以及它引用的学术论文。
- en: We could also implement other correlations, such as the Pearson's correlation
    coefficient. This is implemented in SciPy, a library used for scientific computing
    (scikit-learn uses it as a base).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以实现其他相关性，例如皮尔逊相关系数。这在 SciPy 中实现，SciPy 是一个用于科学计算的库（scikit-learn 使用它作为基础）。
- en: If scikit-learn is working on your computer, so is SciPy. You do not need to
    install anything further to get this sample working.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 scikit-learn 在您的计算机上运行，SciPy 也在运行。为了使这个示例工作，您不需要安装任何其他东西。
- en: 'First, we import the `pearsonr` function from SciPy:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从 SciPy 中导入 `pearsonr` 函数：
- en: '[PRE24]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding function almost fits the interface needed to be used in scikit-learn's
    univariate transformers. The function needs to accept two arrays (x and y in our
    example) as parameters and returns two arrays, the scores for each feature and
    the corresponding p-values. The chi2 function we used earlier only uses the required
    interface, which allowed us to just pass it directly to SelectKBest.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数几乎符合在scikit-learn的单变量转换器中使用的接口。该函数需要接受两个数组（在我们的例子中是x和y）作为参数，并返回两个数组，每个特征的得分和相应的p值。我们之前使用的chi2函数只使用了所需的接口，这使得我们可以直接将其传递给SelectKBest。
- en: 'The pearsonr function in SciPy accepts two arrays; however, the X array it
    accepts is only one dimension. We will write a wrapper function that allows us
    to use this for multivariate arrays like the one we have. Let''s look at the code:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: SciPy中的pearsonr函数接受两个数组；然而，它接受的X数组只有一个维度。我们将编写一个包装函数，使我们能够使用这个函数处理像我们这样的多元数组。让我们看看代码：
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The Pearson value could be between -1 and 1\. A value of 1 implies a perfect
    correlation between two variables, while a value of -1 implies a perfect negative
    correlation, that is, high values in one variable give low values in the other
    and vice versa. Such features are really useful to have. For this reason, we have
    stored the absolute value in the scores array, rather than the original, signed
    value.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 皮尔逊值可能在-1和1之间。1的值意味着两个变量之间有完美的相关性，而-1的值意味着完美的负相关性，即一个变量的高值对应另一个变量的低值，反之亦然。这样的特征非常有用。因此，我们在得分数组中存储了绝对值，而不是原始的有符号值。
- en: 'Now, we can use the transformer class as before to rank the features using
    the Pearson correlation coefficient:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像以前一样使用transformer类，通过皮尔逊相关系数来对特征进行排序：
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This returns a different set of features! The features chosen this way are
    the first, second, and fifth columns: the Age, Education, and Hours-per-week worked.
    This shows that there is not a definitive answer to what the best features are—
    it depends on the metric used and the process undertaken.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这会返回一组不同的特征！这样选择的特征是第一列、第二列和第五列：年龄、教育和每周工作小时数。这表明，并没有一个明确的答案来决定哪些是最好的特征——它取决于所使用的指标和所进行的过程。
- en: 'We can see which feature set is better by running them through a classifier.
    Keep in mind that the results only indicate which subset is better for a particular
    classifier and/or feature combination—there is rarely a case in data mining where
    one method is strictly better than another in all cases! Let''s look at the code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行它们通过分类器来查看哪个特征集更好。请注意，结果仅表明对于特定的分类器或特征组合，哪个子集更好——在数据挖掘中，很少有一种方法在所有情况下都严格优于另一种方法！让我们看看代码：
- en: '[PRE27]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The chi2 average here is 0.83, while the Pearson score is lower at 0.77\. For
    this combination, chi2 returns better results!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的chi2平均值为0.83，而皮尔逊分数较低，为0.77。对于这个组合，chi2返回更好的结果！
- en: 'It is worth remembering the goal of this particular data mining activity: predicting
    wealth. Using a combination of good features and feature selection, we can achieve
    83 percent accuracy using just three features of a person!'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这个特定数据挖掘活动的目标是值得的：预测财富。通过结合良好的特征和特征选择，我们只需使用一个人的三个特征就能达到83%的准确率！
- en: Feature creation
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征创建
- en: Sometimes, just selecting features from what we have isn't enough. We can create
    features in different ways from features we already have. The one-hot encoding
    method we saw previously is an example of this. Instead of having category features
    with options A, B, and C, we would create three new features *Is it A?*, *Is it
    B?*, *Is it C?*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，仅仅从我们所拥有的特征中选择特征是不够的。我们可以从已有的特征以不同的方式创建特征。我们之前看到的独热编码方法就是这样一个例子。而不是有选项A、B和C的类别特征，我们会创建三个新的特征：*它是A吗？*、*它是B吗？*、*它是C吗？*。
- en: Creating new features may seem unnecessary and to have no clear benefit—after
    all, the information is already in the dataset and we just need to use it. However,
    some algorithms struggle when features correlate significantly, or if there are
    redundant features. They may also struggle if there are redundant features. For
    this reason, there are various ways to create new features from the features we
    already have.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新特征可能看起来是不必要的，并且没有明显的益处——毕竟，信息已经在数据集中，我们只需要使用它。然而，一些算法在特征高度相关或存在冗余特征时可能会遇到困难。它们也可能在存在冗余特征时遇到困难。因此，有各种方法可以从我们已有的特征中创建新特征。
- en: We are going to load a new dataset, so now is a good time to start a new Jupyter Notebook.
    Download the Advertisements dataset from [http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements](http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements)
    and save it to your Data folder.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to load the dataset with pandas. First, we set the data''s filename
    as always:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: There are a couple of issues with this dataset that stop us from loading it
    easily. You can see these issues by trying to load the dataset with `pd.read_csv`. First,
    the first few features are numerical, but pandas will load them as strings. To
    fix this, we need to write a converting function that will convert strings to
    numbers if possible. Otherwise, we will get a **Not a Number** (NaN) - an invalid
    value, which is a special value that indicates that the value could not be interpreted
    as a number. It is similar to none or null in other programming languages.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Another issue with this dataset is that some values are missing. These are represented
    in the dataset using the string ?. Luckily, the question mark doesn't convert
    to a float, so we can convert those to NaNs using the same concept. In further
    chapters, we will look at other ways of dealing with missing values like this.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a function that will do this conversion for us. It attempts
    to convert the number to a float, and if that fails, it returns NumPy''s special
    NaN value that can be stored in place of a float:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we create a dictionary for the conversion. We want to convert all of the
    features to floats:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Also, we want to set the final column, the class, (column index #1558) to a
    binary feature. In the Adult dataset, we created a new feature for this. In the
    dataset, we will convert the feature while we load it:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now we can load the dataset using `read_csv`. We use the converters parameter
    to pass our custom conversion into pandas:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The resulting dataset is quite large, with 1,559 features and more than 3,000
    rows. Here are some of the feature values, the first five, printed by inserting
    `ads.head()` into a new cell:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_05_01.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: This dataset describes images on websites, with the goal of determining whether
    a given image is an advertisement or not.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'The features in this dataset are not described well by their headings. There
    are two files accompanying the ad.data file that have more information: `ad.DOCUMENTATION`
    and `ad.names`. The first three features are the height, width, and ratio of the
    image size. The final feature is 1 if it is an advertisement and 0 if it is not.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The other features are 1 for the presence of certain words in the URL, alt text,
    or caption of the image. These words, such as the word sponsor, are used to determine
    if the image is likely to be an advertisement. Many of the features overlap considerably,
    as they are combinations of other features. Therefore, this dataset has a lot
    of redundant information.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'With our dataset loaded in `pandas`, we will now extract the `x` and `y` data
    for our classification algorithms. The `x` matrix will be all of the columns in
    our Dataframe, except for the last column. In contrast, the `y` array will be
    only that last column, feature `1558.` Before that though, we simplify our dataset
    (just for this chapter''s sake) by dropping any row with a NaN value. Let''s look
    at the code:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: More than 1000 rows are dropped due to this command, which is fine for our exercise.
    For real-world applications, you don't want to discard data if you can help it--instead,
    you can use interpolation or value replacing to fill the NaN values. As an example,
    you can replace any missing value with the average for that column.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some datasets, features heavily correlate with each other. For example, the
    speed and the fuel consumption would be heavily correlated in a go-kart with a
    single gear. While it can be useful to find these correlations for some applications,
    data mining algorithms typically do not need the redundant information.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: The ads dataset has heavily correlated features, as many of the keywords are
    repeated across the alt text and caption.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The Principal Component Analysis (PCA) algorithm aims to find combinations of
    features that describe the dataset in less information. It aims to discover *principal
    components*, which are features that do not correlate with each other and explain
    the information—specifically the variance—of the dataset. What this means is that
    we can often capture most of the information in a dataset in fewer features.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply PCA just like any other transformer. It has one key parameter, which
    is the number of components to find. By default, it will result in as many features
    as you have in the original dataset. However, these principal components are ranked—the
    first feature explains the largest amount of the variance in the dataset, the
    second a little less, and so on. Therefore, finding just the first few features
    is often enough to explain much of the dataset. Let''s look at the code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The resulting matrix, Xd, has just five features. However, let''s look at the
    amount of variance that is explained by each of these features:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The result, `array([ 0.854, 0.145, 0.001, 0\. , 0\. ])`, shows us that the first
    feature accounts for 85.4 percent of the variance in the dataset, the second accounts
    for 14.5 percent, and so on. By the fourth feature, less than one-tenth of a percent
    of the variance is contained in the feature. The other 1,553 features explain
    even less (this is an ordered array).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: The downside to transforming data with PCA is that these features are often
    complex combinations of the other features. For example, the first feature of
    the preceding code starts with `[-0.092, -0.995, -0.024],` that is, multiply the
    first feature in the original dataset by -0.092, the second by -0.995, the third
    by -0.024\. This feature has 1,558 values of this form, one for each of the original
    datasets (although many are zeros). Such features are indistinguishable by humans
    and it is hard to glean much relevant information from without a lot of experience
    working with them.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Using PCA can result in models that not only approximate the original dataset, but
    can also improve the performance in classification tasks:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The resulting score is 0.9356,  which is (slightly) higher than our original
    model's score. PCA won't always give a benefit like this, but it does more often
    than not.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: We are using PCA here to reduce the number of features in our dataset. As a
    general rule, you shouldn't use it to reduce overfitting in your data mining experiments.
    The reason for this is that PCA doesn't take classes into account. A better solution
    is to use regularization. An introduction, with code, is available at [http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/](http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage is that PCA allows you to plot datasets that you otherwise
    couldn't easily visualize. For example, we can plot the first two features returned
    by PCA.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we tell our Notebook to display plots inline:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we get all of the distinct classes in our dataset (there are only two:
    is ad or not ad):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We also assign colors to each of these classes:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We use zip to iterate over both lists at the same time, then extract all samples
    from that class, and plot them with the color appropriate to the class:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Finally, outside the loop, we create a legend and show the graph, showing where
    the samples from each class appear:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![](img/B06162_05_02.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Creating your own transformer
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the complexity and type of dataset changes, you might find that you can't
    find an existing feature extraction transformer that fits your needs. We will
    see an example of this in [Chapter 7](67684c84-b6f7-4f09-b0ce-cabe2d2c373d.xhtml),
    *Follow Recommendations Using Graph Mining*, where we create new features from
    graphs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: A transformer is akin to a converting function. It takes data of one form as
    input and returns data of another form as output. Transformers can be trained
    using some training dataset, and these trained parameters can be used to convert
    testing data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The transformer API is quite simple. It takes data of a specific format as input
    and returns data of another format (either the same as the input or different)
    as output. Not much else is required of the programmer.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The transformer API
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Transformers have two key functions:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '`fit():` This takes a training set of data as input and sets internal parameters'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transform():` This performs the transformation itself. This can take either
    the training dataset, or a new dataset of the same format'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both `fit()` and `transform()` functions should take the same data type as input,
    but `transform()` can return data of a different type while `fit()` always returns
    self.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: We are going to create a trivial transformer to show the API in action. The
    transformer will take a NumPy array as input, and discretize it based on the mean.
    Any value higher than the mean (of the training data) will be given the value
    1 and any value lower or equal to the mean will be given the value 0.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'We did a similar transformation with the Adult dataset using pandas: we took
    the Hours-per-week feature and created a LongHours feature if the value was more
    than 40 hours per week. This transformer is different for two reasons. First,
    the code will conform to the scikit-learn API, allowing us to use it in a pipeline.
    Second, the code will learn the mean, rather than taking it as a fixed value (such
    as 40 in the LongHours example).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Transformer
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start, open up the Jupyter Notebook that we used for the Adult dataset. Then,
    click on the Cell menu item and choose Run All. This will rerun all of the cells
    and ensure that the notebook is up to date.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: First, we import the TransformerMixin, which sets the API for us. While Python
    doesn't have strict interfaces (as opposed to languages like Java), using a mixin like
    this allows scikit-learn to determine that the class is actually a transformer.
    We also need to import a function that checks the input is of a valid type. We
    will use that soon.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the code:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s take a look at our class in entirety, and then we will revisit some
    of the details:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Our class will learn the mean for each feature in the fit method, by computing `X.mean(axis=0)`,
    which is then stored as an object attribute. After that, the fit function returns
    self, conforming to the API (scikit-learn uses this to allow for chaining function
    calls).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: After fitting, the transform function takes a matrix with the same number of
    features (confirmed by the `assert` statement), and simply returns which values
    are more than the mean for a given feature.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our class is built, we can now create an instance of this class and
    use it to transform our X array:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Take a shot at implementing this Transformer into a workflow, both using a Pipeline
    and without. You'll see that by conforming to the Transformer API, it is quite
    simple to use in place of a built-in scikit-learn Transformer object.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When creating your own functions and classes, it is always a good idea to do
    unit testing. Unit testing aims to test a single unit of your code. In this case,
    we want to test that our transformer does as it needs to do.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Good tests should be independently verifiable. A good way to confirm the legitimacy
    of your tests is by using another computer language or method to perform the calculations.
    In this case, I used Excel to create a dataset, and then computed the mean for
    each cell. Those values were then transferred to the unit test.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests should also, generally, be small and quick to run. Therefore, any
    data used should be of a small size. The dataset I used for creating the tests
    is stored in the Xt variable from earlier, which we will recreate in our test.
    The mean of these two features is 13.5 and 15.5, respectively.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our unit test, we import the `assert_array_equal` function from NumPy''s
    testing, which checks whether two arrays are equal:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '`from numpy.testing import assert_array_equal`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create our function. It is important that the test's name starts with
    test_,
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'as this nomenclature is used for tools that automatically find and run tests.
    We also set up our testing data:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can run the test by simply running the function itself:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: If there was no error, then the test ran without an issue! You can verify this
    by changing some of the tests to deliberately make values incorrect, and confirming
    that the test fails. Remember to change them back so that the test passes!
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: If we had multiple tests, it would be worth using a testing framework, like py.test
    or nose to run our tests. Using a framework like this is beyond the scope of this
    book, but they manage running tests, recording failures, and providing feedback
    to you, as a programmer, to help you improve your code.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a tested transformer, it is time to put it into action. Using
    what we have learned so far, we create a Pipeline, set the first step to the MeanDiscrete
    transformer, and the second step to a Decision Tree Classifier. We then run a
    cross-validation and print out the result. Let''s look at the code:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The result is 0.917, which is not as good as before, but very good for a simple
    binary feature model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at features and transformers and how they can be
    used in the data mining pipeline. We discussed what makes a good feature and how
    to algorithmically choose good features from a standard set. However, creating
    good features is more art than science and often requires domain knowledge and
    experience.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: We then created our own transformer using an interface that allows us to use
    it in scikit-learn's helper functions. We will be creating more transformers in
    later chapters so that we can perform effective testing using existing functions.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: To take the lessons learned in this chapter further, I recommend signing up
    to the online data mining competition website [Kaggle.com](http://Kaggle.com)
    and trying some of the competitions. Their recommended starting place is the Titanic
    dataset, which allows you to practice the feature creation aspects of this chapter.
    Many of the features are not numerical, requiring you to convert them to numerical
    features before applying a data mining algorithm.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将本章学到的知识进一步深化，我建议您注册到在线数据挖掘竞赛网站 [Kaggle.com](http://Kaggle.com) 并尝试一些竞赛。他们推荐的起点是泰坦尼克号数据集，这可以让您练习本章中特征创建的方面。许多特征都不是数值型的，需要您在应用数据挖掘算法之前将它们转换为数值特征。
- en: In the next chapter, we use feature extraction on a corpus of text documents.
    There are many transformers and feature types for text, each with their advantages
    and disadvantages.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将在文本文档语料库上使用特征提取。文本有很多转换器和特征类型，每种都有其优缺点。
