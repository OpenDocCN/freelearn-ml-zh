- en: Winning the Casino Slot Machines with Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you have been following **machine learning** (**ML**) news, I am sure you
    will have encountered this kind of headline: *computers performing better than
    world champions in various games*. If you haven''t, the following are sample news
    snippets from my quick Google search that are worth spending time reading to understand
    the situation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check this out: [https://www.theverge.com/2017/10/18/16495548/deepmind-ai-go-alphago-zero-self-taught/](https://www.theverge.com/2017/10/18/16495548/deepmind-ai-go-alphago-zero-self-taught/):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c22df343-7a83-4023-914e-93d75586eafa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'See this: [https://www.makeuseof.com/tag/ais-winning-5-times-computers-beat-humans/](https://www.makeuseof.com/tag/ais-winning-5-times-computers-beat-humans/):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b0739d12-24c3-46f6-9062-02488a93f2be.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Reinforcement learning** (**RL**) is a subarea of **artificial intelligence**
    (**AI**) that powers computer systems who are able to demonstrate better performance
    in games such as Atari Breakout and Go than human players.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The multi-arm bandit problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods for solving the multi-arm bandit problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world applications of RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a project using RL techniques to maximize our chances of winning
    at a multi-arm bandit machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL is a very important area but is sometimes overlooked by practitioners for
    solving complex, real-world problems. It is unfortunate that even most ML textbooks
    focus only on supervised and unsupervised learning while totally ignorning RL.
  prefs: []
  type: TYPE_NORMAL
- en: RL as an area has picked up momentum in recent years; however, its origins date
    back to 1980\. It was invented by Rich Sutton and Andrew Barto, Rich's PhD thesis
    advisor. It was thought of as archaic, even back in the 1980s. Rich, however,
    believed in RL and its promise, maintaining that it would eventually be recognized.
  prefs: []
  type: TYPE_NORMAL
- en: A quick Google search with the term RL shows that RL methods are often used
    in games, such as checkers and chess. Gaming problems are problems that require
    taking actions over time to find a long-term optimal solution to a dynamic problem.
    They are dynamic in the sense that the conditions are constantly changing, sometimes
    in response to other agents, which can be adversarial.
  prefs: []
  type: TYPE_NORMAL
- en: Although the success of RL is proven in the area of games, it is also an emerging
    area that is increasingly applied in other fields, such as finance, economics,
    and other inter-disciplinary areas. There are a number of methods in the RL area
    that have grown independently within the AI and operations research communities.
    Therefore, it is key area for a ML practitioners to learn about.
  prefs: []
  type: TYPE_NORMAL
- en: 'In simple terms, RL is an area that mainly focuses on creating models that
    learn from mistakes. Imagine that a person is put in a new environment. At first,
    they will make mistakes, but they will learn from them, so that when the same
    situation should arise in future, they will not make the same mistake again. RL
    uses the same technique to train the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Environment ----------> Try and fail -----------> Learn from failures ---------->
    Reach goal
  prefs: []
  type: TYPE_NORMAL
- en: Historically, you couldn't use ML to get an algorithm learn how to become better
    than a human at performing a certain task. All that could be done was model the
    machine's behavior after a human's actions and, maybe, the computer would run
    through them faster. RL, however, makes it possible to create models that become
    better at performing certain tasks than humans.
  prefs: []
  type: TYPE_NORMAL
- en: Isaac Abhadu, CEO and co-founder at SYBBIO, had this wonderful explanation on
    Quora detailing the working of RL compared to supervised learning. He stated that
    an RL framework, in a nutshell, is very similar to that of supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we're trying to get an algorithm to excel at the game of Pong. We have
    input frames that we will run through a model to get it to produce some random
    output actions, just as we would in a supervised learning setting. The difference,
    however, is that in the case of RL, we ourselves do not know what the target labels
    are, and so we don't tell the machine what's better to do in every specific situation.
    Instead, we apply something called a **policy gradients** method.
  prefs: []
  type: TYPE_NORMAL
- en: So, we start with a random network and feed to it an input frame so it produces
    a random output action to react to that frame. This action is then sent back to
    the game engine, which makes it produce another frame. This loop continues over
    and over. The only feedback it will give is the game's scoreboard. Whenever our
    agent does something right – that is, it produces some successful sequence – it
    will get a point, generally termed as a **reward**. Whenever it produces a failing
    sequence, it will get a point removed—this is a **penalty**.
  prefs: []
  type: TYPE_NORMAL
- en: The ultimate goal the agent is pursuing is to keep updating its policy to get
    as much rewards as possible. So, over time, it will figure out how to beat a human
    at the game.
  prefs: []
  type: TYPE_NORMAL
- en: RL is not quick. The agent is going to lose a lot at first. But we will keep
    feeding it frames so it keeps producing random output actions, and it will stumble
    upon actions that are successful. It will keep accumulating knowledge about what
    moves are successful and, after a while, will become invincible.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of RL with other ML algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RL involves an **environment**, which is the problem set to be solved, and
    an **agent**, which is simply the AI algorithm. The agent will perform a certain
    action and the result of the action will be a change in the **state** of the agent.
    The change leads to the agent getting a reward, which is a positive reward, or
    a penalty, which is a negative reward for having performed an incorrect action.
    By repeating the action and reward process, the agent learns the environment.
    It understands the various states and the various actions that are desirable and
    undesirable. This process of performing actions and learning from the rewards
    is RL. The following diagram is an illustration showing the relationship between
    the agent and the environment in RL:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0363e4a-eefb-47da-aeff-5e949994c980.png)'
  prefs: []
  type: TYPE_IMG
- en: Relationship between the agent and environment in RL
  prefs: []
  type: TYPE_NORMAL
- en: RL, **deep learning** (**DL**), and ML all support automation in one way or
    another. All of them involve some kind of learning from the given data. However,
    what separates RL from the others is that RL learns the right actions by trail
    and error, whereas the others are focused on learning by finding patterns in the
    existing data. Another key difference is that for DL and ML algorithms to learn
    better, we will need to give them large labeled datasets, whereas this is not
    the case with RL.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand RL better by taking the analogy of training pets at home. Imagine
    we are teaching our pet dog, Santy, some new tricks. Santy, unfortunately, does
    not understand English; therefore, we need to find an alternative way to train
    him. We emulate a situation, and Santy tries to respond in many different ways.
    We reward Santy with a bone treat for any desirable responses. What this inculcates
    in the pet dog is that the next time he encounters a similar situation, he will
    perform the desired behavior as he knows that there is a reward. So, this is learning
    from positive responses; if he is treated with negative responses, such as frowning,
    he will be discouraged from undesirable behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Terminology of RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s understand the RL key terms—agent, environment, state, policy, reward,
    and penalty—with our pet dog training analogy:'
  prefs: []
  type: TYPE_NORMAL
- en: Our pet dog, Santy, is the agent that is exposed to the environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment is a house or play area, depending on what we want to teach
    to Santy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each situation encountered is called the state. For example, Santy crawling
    under the bed or running can be interpreted as states.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santy, the agent, reacts by performing actions to change from one state to another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After changes in states, we give the agent either a reward or a penalty, depending
    on the action that is performed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The policy refers to the strategy of choosing an action for finding better outcomes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we understand each of the RL terms, let''s define the terms more formally
    and visualize the agent''s behavior in the diagram that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**States**: The complete description of the world is known as the states. We
    do not abstract any information that is present in the world. States can be a
    position, a constant, or a dynamic. States are generally recorded in arrays, matrices,
    or higher order tensors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions**: The environment generally defines the possible actions; that is,
    different environments lead to different actions, based on the agent. The valid
    actions for an agent are recorded in a space called an action space. The possible
    valid actions in an environment are finite in number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment**: This is the space where the agent lives and with which the
    agent interacts. For different types of environments, we use different rewards
    and policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward and return**: The reward function is the one that must be kept track
    of at all times in RL. It plays a vital role in tuning, optimizing the algorithm,
    and stopping the training of the algorithm. The reward is computed based on the
    current state of the world, the action just taken, and the next state of the world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policies**: A policy in RL is a rule that''s used by an agent for choosing
    the next action; the policy is also known as the agent''s brain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at the following flowchart to understand the process better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bc9d744-0c4c-4e35-a4c6-d917cc652880.png)'
  prefs: []
  type: TYPE_IMG
- en: Agent behavior in RL
  prefs: []
  type: TYPE_NORMAL
- en: 'At each step, *t*, the agent performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Executes action *a[t]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Receives observation *s[t]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Receives scalar reward *r[t]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The environment implements the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Changes upon action *a[t]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Emits observation *s[t+1]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Emits scalar reward *r[t+1]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Time step *t* is incremented after each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The multi-arm bandit problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let me start with an analogy to understand this topic better. Do you like pizza?
    I like it a lot! My favorite restaurant in Bangalore serves delicious pizzas.
    I go to this place almost every time I feel like eating a pizza, and I am almost
    sure that I will get the best pizza. However, going to the same restaurant every
    time worries me that I am missing out on pizzas that are even better and served
    elsewhere in the town!
  prefs: []
  type: TYPE_NORMAL
- en: One alternative available is to try out restaurants one by one and sample the
    pizzas there, but this means that there is a very high probability that I will
    end up eating pizzas that aren't very nice. However, this is the one way for me
    to find a restaurant that serves better pizzas than the one I am currently aware
    of. I am aware you must be wondering why am I talking about pizzas when I am supposed
    to be talking about RL. Let me get to the point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dilemma with this task arises from incomplete information. In other words,
    to solve this task, it is essential to gather enough information to formulate
    the best overall strategy and then explore new actions. This will eventually lead
    to a minimization of overall bad experiences. This situation can otherwise be
    termed as the **exploration** versus **exploitation** dilemma:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edcd5dd5-b248-4187-a01c-ebeb54d077f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Exploration versus exploitation dilemma
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram aptly summarizes my best-pizza problem.
  prefs: []
  type: TYPE_NORMAL
- en: The **multi-arm bandit problem** (**MABP**) is a simplified form of the pizza
    analogy. It is used to represent similar kinds of problems, and finding a good
    strategy to solve them is already helping a lot of industries.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **bandit** is defined as someone who steals your money! A one-armed bandit
    is a simple slot machine. We find this sort of machine in a casino: you insert
    a coin into the slot machine, pull a lever, and pray to the luck god to get an
    immediate reward. But the million-dollar question is why is a slot machine called
    a bandit? It turns out that all casinos configure the slot machines in such a
    way that all gamblers end up losing money!'
  prefs: []
  type: TYPE_NORMAL
- en: 'A multi-arm bandit is a hypothetical but complicated slot machine where we
    have more than one slot machine lined up in a row. A gambler can pull several
    levers, with each lever giving a different return. The following diagram depicts
    the probability distribution for the corresponding reward that is different to
    each layer and unknown to the gambler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b18bda36-4411-4f1b-94bd-ac6ea8271f7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-arm bandit
  prefs: []
  type: TYPE_NORMAL
- en: 'Given these slot machines and after a set of initial trials, the task is to
    identify what lever to pull to get the maximum reward. In other words, pulling
    any one of the arms gives us a stochastic reward of either R=+1 for success, or
    R=0 for failure; this is called an **immediate reward**. A multi-arm bandit that
    issues a reward of 1 or 0 is called a **Bernoulli**. The objective is to pull
    the arms one-by-one in a sequence while gathering information to maximize the
    total payout over the long run. Formally, a Bernoulli MABP can be described as
    a tuple of (A,R), where the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: We have KK machines with reward probabilities, {θ1,…,θK}.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At each time step, *t*, we take an action, *a*, on one slot machine and receive
    a reward, *r*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A* is a set of actions, each referring to the interaction with one slot machine.
    The value of action *a* is the expected reward, [![](img/508d27c3-9820-4d02-8bda-d5b6ac02d4ef.png)].
    If action *a* at time step *t* is on the *i*-th machine, then [![](img/7b8aad37-eaa2-408a-81b6-4c361ebbf469.png)]. 
    Q(a) is generally referred to as the action-value function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R* is a reward function. In the case of the Bernoulli bandit, we observe a
    reward, *r*, in a stochastic fashion. At time step *t*, [![](img/7a7913f0-0999-4f43-a251-161a5ddd8fc6.png)]
    may return reward 1 with a probability of [![](img/84f9c0d3-29d8-44d3-9fcf-e0813cabb2e5.png),] or
    0 otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can solve the MABP with multiple strategies. We will review some of the strategies
    shortly in this section. To decide on the best strategy and to compare the different
    strategies, we need a quantitative method. One method is to directly compute the
    cumulative rewards after a certain predefined number of trials. Comparing the
    cumulative rewards from each of the strategies gives us an opportunity to identify
    the best strategies for the problem.
  prefs: []
  type: TYPE_NORMAL
- en: At times, we may already know the best action for the given bandit problem.
    In those cases, it may be interesting to look at the concept of regret.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine that we know of the details of the best arm to pull for the
    given bandit problem. Assume that by repeatedly pulling this best arm, we get
    a maximum expected reward, which is shown as a horizontal line in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1997e98-45a3-4ac4-8558-5b81dbbab53f.png)'
  prefs: []
  type: TYPE_IMG
- en: Maximum reward obtained by pulling the best arm for a MABP
  prefs: []
  type: TYPE_NORMAL
- en: As per the problem statement, we need to make repeated trials by pulling different
    arms of the multi-arm bandit until we are approximately sure of the arm to pull
    for the maximum average return at time *t*. There are a number of rounds involved
    while we explore and decide upon the best arm. The number of rounds, otherwise
    called **trials**, also incurs some loss, and this is called **regret**. In other
    words, we want to maximize the reward even during the learning phase. Regret can
    be summarized as a quantification of exactly how much we regret not picking the
    optimal arm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an illustration showing the regret due to trials done
    to find the best arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c7058b4-f051-4e70-84da-58a1531f8c4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Concept of regret in MAB
  prefs: []
  type: TYPE_NORMAL
- en: Strategies for solving MABP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on how the exploration is done, the strategies to solve the MABP can
    be classified into the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: No exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploration at random
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploration smartly with preference to uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's delve into the details of some of the algorithms that fall under each
    of the strategy types.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider one very naive approach that involves playing just one slot machine
    for a long time. Here, we do no exploration at all and just randomly pick one
    arm to repeatedly pull to maximize the long-term rewards. You must be wondering
    how this works! Let's explore.
  prefs: []
  type: TYPE_NORMAL
- en: In probability theory, the law of large numbers is a theorem that describes
    the result of performing the same experiment a large number of times. According
    to this law, the average of the results obtained from a large number of trials
    should be close to the expected value, and will tend to become closer as more
    trials are performed.
  prefs: []
  type: TYPE_NORMAL
- en: We can just play with one machine for a large number of rounds so as to eventually
    estimate the true reward probability according to the law of large numbers.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are some problems with this strategy. First and foremost, we
    do not know the value of a large number of rounds. Second, it is super resource
    intensive to play the same slot repeatedly for large number of times. And, most
    importantly, there is no guarantee that we will obtain the best long-term reward
    with this strategy.
  prefs: []
  type: TYPE_NORMAL
- en: The epsilon-greedy algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The greedy algorithm in RL is a complete exploitation algorithm, which does
    not care for exploration. Greedy algorithms always select the action with the
    highest estimated action value. The action value is estimated according to past
    experience by averaging the rewards associated with the target action that have
    been observed so far.
  prefs: []
  type: TYPE_NORMAL
- en: However, use of a greedy algorithm can be a smart approach if we are able to
    successfully estimate the action value to the expected action value; if we know
    the true distribution, we can just select the best actions. An epsilon-greedy
    algorithm is a simple combination of the greedy and random approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Epsilon helps to do this estimate. It adds exploration as part of the greedy
    algorithm. In order to counter the logic of always selecting the best action,
    as per the estimated action value, occasionally, the epsilon probability selects
    a random action for the sake of exploration; the rest of the time, it behaves
    as the original greedy algorithm and select the best known action.
  prefs: []
  type: TYPE_NORMAL
- en: The epsilon in this algorithm is an adjustable parameter that determines the
    probability of taking a random, rather than principled, action. It is also possible
    to adjust the epsilon value during training. Generally, at the start of the training
    process, the epsilon value is often initialized to a large probability. As the
    environment is unknown, the large epsilon value encourages exploration. The value
    is then gradually reduced to a small constant (often set to 0.1). This will increase
    the rate of exploitation selection.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the simplicity of the algorithm, the approach has become the de facto
    technique for most recent RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the common usage that the algorithm enjoys, this method is far from
    optimal, since it takes into account only whether actions are most rewarding or
    not.
  prefs: []
  type: TYPE_NORMAL
- en: Boltzmann or softmax exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boltzmann exploration is also called **softmax exploration**. As opposed to
    either taking the optimal action all the time or taking a random action all the
    time, this exploration favors both through weighted probabilities. This is done
    through a softmax over the network's estimates of values for each action. In this
    case, although not guaranteed, the action that the agent estimates to be optimal
    is most likely to be chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Boltzmann exploration has the biggest advantage over epsilon greedy. This method
    has information about the likely values of the other actions. In other words,
    let's imagine that there are five actions available to an agent. Generally, in
    the epsilon-greedy method, four actions are estimated as non-optimal and they
    are all considered equally. However, in Boltzmann exploration, the four sub-optimal
    choices are weighed by their relative value. This enables the agent to ignore
    actions that are estimated to be largely sub-optimal and give more attention to
    potentially promising, but not necessarily ideal, actions.
  prefs: []
  type: TYPE_NORMAL
- en: The temperature parameter (*τ*) controls the spread of the softmax distribution,
    so that all actions are considered equally at the start of training, and actions
    are sparsely distributed by the end of training. The parameter is annealed over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Decayed epsilon greedy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The value of epsilon is key in determining how well the epsilon-greedy algorithm
    works for a given problem. Instead of setting this value at the start and then
    decreasing it, we can make epsilon dependent on time. For example, epsilon can
    be kept equal to 1 / log(t + 0.00001). As time passes, the epsilon value will
    keep reducing. This method works as over the time that epsilon is reduced, we
    become more confident of the optimal action and less exploring is required.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with the random selection of actions is that after sufficient time
    steps, even if we know that some arm is bad, this algorithm will keep choosing
    that with probability *epsilon/n*. Essentially, we are exploring a bad action,
    which does not sound very efficient. The approach to get around this could be
    to favor exploration of arms with strong potential in order to get an optimal
    value.
  prefs: []
  type: TYPE_NORMAL
- en: The upper confidence bound algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **upper confidence bound** (**UCB**) algorithm is the most popular and widely
    used solution for MABPs. This algorithm is based on the principle of optimism
    in the face of uncertainty. This essentially means, the less uncertain we are
    about an arm, the more important it becomes to explore that arm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we have two arms that can be tried out. If we have tried out the
    first arm 100 times but the second arm only once, then we are probably reasonably
    confident about the payoff of the first arm. However, we are very uncertain about
    the payoff of the second arm. This gives rise to the family of UCB algorithms.
    This can be further explained through the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08208bea-abf0-4d4d-b37e-886510e89950.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration to explain upper confident bound algorithm
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, each bar represents a different arm or an action.
    The red dot is the true expected reward and the center of the bar represents the
    observed average reward. The width of the bar represents the confidence interval.
    We are already aware that, by the law of large numbers, the more samples we have,
    the closer the observed average gets to the true average, and the more the bar
    shrinks.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind UCB algorithms is to always pick the arm or action with the
    highest upper bound, which is the sum of the observed average and the one-sided
    width of the confidence interval. This balances the exploration of arms that have
    not been tried many times with the exploitation of arms that have.
  prefs: []
  type: TYPE_NORMAL
- en: Thompson sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thompson sampling is one of the oldest heuristics for MABPs. It is a randomized
    algorithm based on Bayesian ideas, and has recently generated significant interest
    after several studies demonstrated it to have better empirical performance compared
    to other methods.
  prefs: []
  type: TYPE_NORMAL
- en: There is a beautiful explanation I found on [https://stats.stackexchange.com/questions/187059/could-anyone-explain-thompson-sampling-in-simplest-terms](https://stats.stackexchange.com/questions/187059/could-anyone-explain-thompson-sampling-in-simplest-terms).
    I do not think I can do  better job at explaining Thompson sampling than this.
    You can refer to this for further reference.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-arm bandit – real-world use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We encounter so many situations in the real world that are similar to that
    of the MABP we reviewed in this chapter. We could apply RL strategies to all these
    situations. The following are some of the real-world use cases similar to that
    of the MABP:'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the best medicine/s among many alternatives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the best product to launch among possible products
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding the amount of traffic (users) that we need to allocate for each website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the best marketing strategy for launching a product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the best stocks portfolio to maximize profit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding out the best stock to invest in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figuring out the shortest path in a given map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click-through rate prediction for ads and articles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the most beneficial content to be cached at a router based upon the
    content of articles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allocation of funding for different departments of an organization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Picking best-performing athletes out of a group of students given limited time
    and an arbitrary selection threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have covered almost all of the basic details that we need to know
    to progress to the practical implementation of RL to the MABP. Let's kick-start
    coding solutions to the MABP in our next section.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the MABP with UCB and Thompson sampling algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this project, we will use upper confidence limits and Thompson sampling
    algorithms to solve the MABP. We will compare their performance and strategy in
    three different situations—standard rewards, standard but more volatile rewards,
    and somewhat chaotic rewards. Let''s prepare the simulation data, and once the
    data is prepared, we will view the simulated data using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af7c8009-6639-486b-aa13-f7b31d4a198e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, create a melted dataset with an arm and reward combination, and then convert
    the arm column to the nominal type using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6773aaa3-4263-436a-abda-d6523137ce4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, plot the distributions of rewards from bandits using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98af8da3-8b26-4f28-8670-55ecd4975f42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s implement the UCB algorithm on the hypothesized arm with a normal
    distribution using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following as the resultant output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will implement the Thompson sampling algorithm using a **normal-gamma**
    prior and normal likelihood to estimate posterior distributions using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following as the resultant output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: From the results, we can infer that the UCB algorithm quickly identified that
    the 10^(th) arm yields the most reward. We also observe that Thompson sampling
    tried the worst arms a lot more times before finding the best one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s simulate the data of bandits with normally distributed rewards
    with large variance and plot the distributions of rewards by using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following graph as the resultant output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3110523-de32-48fd-8998-a20d758ca85e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Apply UCB on rewards with higher variance using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, apply Thompson sampling on rewards with higher variance by using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: From the results, we can infer that when the fluctuation of rewards is greater,
    the UCB algorithm is more susceptible to being stuck at a suboptimal choice and
    never finds the optimal bandit. Thompson sampling is generally more robust and
    is able to find the optimal bandit in all kinds of situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s simulate the more chaotic distribution bandit data and plot the
    distribution of rewards from bandits by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following graph as the resultant output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2dc6b43-3e5f-4697-a714-ac73f64ed9f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Apply UCB on rewards with different distributions by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, apply Thompson sampling on rewards with different distributions by using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following as the resultant output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding results, we see that the performance of the two algorithms
    is similar. A major reason for the Thompson sampling algorithm trying all bandits
    several times before choosing the one it considers best is because we chose a
    prior distribution with a relatively high mean in this project. With the prior
    having a larger mean, the algorithm favors **exploration over exploitation** at
    the beginning. Only when the algorithm becomes very confident that it has found
    the best choice does it value exploitation over exploration. If we decrease the
    mean of the prior, exploitation would have a higher value and the algorithm would
    stop exploring faster. By altering the prior distribution used, you can adjust
    the relative importance of exploration over exploitation to suit the specific
    problem at hand. This is more evidence highlighting the flexibility of the Thompson
    sampling algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about RL. We started the chapter by defining RL
    and its difference when compared with other ML techniques. We then reviewed the
    details of the MABP and looked at the various strategies that can be used to solve
    this problem. Use cases that are similar to the MABP were discussed. Finally,
    a project was implemented with UCB and Thompson sampling algorithms to solve the
    MABP using three different simulated datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We have almost reached the end of this book. The appendix of this book, *The
    Road Ahead*, as the name reflects, is a guidance chapter suggesting details on
    what's next from here to become a better R data scientist. I am super excited
    that I am at the last leg of this R projects journey. Are you with me on this
    as well?
  prefs: []
  type: TYPE_NORMAL
