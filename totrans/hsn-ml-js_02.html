<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Data Exploration</h1>
                </header>
            
            <article>
                
<p>The single most important thing for a beginner to know about <strong>machine learning</strong> (<strong>ML</strong>) is that <em>machine learning is not magic</em>. Taking a large dataset and naively applying a neural network to it will not automatically give you earth-shaking insights. ML is built on top of sound and familiar mathematical principles, such as probability, statistics, linear algebra, and vector calculus—voodoo not included (though some readers may liken vector calculus to voodoo)!</p>
<p>We will be covering the following topics in this chapter:</p>
<ul>
<li><span>An overview</span></li>
<li><span>Variable identification</span></li>
<li><span>Cleaning of data</span></li>
<li><span>Transformation</span></li>
<li><span>Types of analysis</span></li>
<li><span>Missing values treatment</span></li>
<li><span>Outlier treatment</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview</h1>
                </header>
            
            <article>
                
<p>One misconception I would like to dispel early on is that implementing the ML algorithm itself is the bulk of the work you'll need to do to accomplish some task. If you're new to this, you may be under the impression that 95% of your time should be spent on implementing a neural network, and that the neural network is solely responsible for the results you get. Build a neural network, put data in, magically get results out. What could be easier?</p>
<div class="packt_infobox">The reality of ML is that the algorithm you use is only as good as the data you put into it. Furthermore, the results you get are only as good as your ability to process and interpret them. The age-old computer science acronym <strong>GIGO</strong> fits well here: <em>Garbage In</em>, <em>Garbage Out</em>.</div>
<p>When implementing ML techniques, you must also pay close attention to their preprocessing and postprocessing of data. Data preprocessing is required for many reasons, and is the focus of this chapter. Postprocessing relates to your interpretation of the algorithm's output, whether your confidence in the algorithm's result is high enough to take action on it, and your ability to apply the results to your business problem. Since postprocessing of results strongly depends on the algorithm in question, we'll address postprocessing considerations as they come up in our specific examples throughout this book.</p>
<p>Preprocessing of data, like postprocessing of data, often depends on the algorithm used, as different algorithms have different requirements. One straightforward example is image processing with <strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>), covered in a later chapter. All images processed by a single CNN are expected to have the same dimensions, or at least the same number of pixels and the same number of color channels (RGB versus RGBA versus grayscale, and so on). The CNN was configured to expect a specific number of inputs, and so every image you give to it must be preprocessed to make sure it complies with the neural network's expectations. You may need to resize, scale, crop, or pad input images before feeding them to the network. You may need to convert color images to grayscale. You may need to detect and remove images that have been corrupted from your dataset.</p>
<p>Some algorithms simply won't work if you attempt to give them the wrong input. If a CNN expects 10,000 grayscale pixel intensity inputs (namely an image that's 100 x 100 pixels), there's no way you can give it an image that's sized 150 x 200. This is a best-case scenario for us: the algorithm fails loudly, and we are able to change our approach before attempting to use our network.</p>
<p>Other algorithms, however, will fail silently if you give them bad input. The algorithm will appear to be working, and even give you results that look reasonable but are actually wholly inaccurate. This is our worst-case scenario: we think the algorithm is working as expected, but in reality we're in a GIGO situation. Just think about how long it will take you to discover that the algorithm is actually giving you nonsensical results. How many bad business decisions have you made based on incorrect analysis or poor data? These are the types of situations we must avoid, and it all starts at the beginning: making sure the data we use is appropriate for the application.</p>
<p>Most ML algorithms make assumptions about the data they process. Some algorithms expect data to be of a given size and shape (as in neural networks), some algorithms expect data to be bucketed, some algorithms expect data to be normalized over a range (between 0 and 1 or between -1 and +1), some algorithms are resilient to missing values and others aren't. It is ultimately your responsibility to understand what assumptions the algorithm makes about your data, and also to align the data with the expectations of the algorithm.</p>
<p>For the most part, the aforementioned relates to the format, shape, and size of data. There is another consideration: the quality of the data. A data point may be perfectly formatted and aligned with the expectations of an algorithm, but still be <em>wrong.</em> Perhaps someone wrote down the wrong value for a measurement, maybe there was an instrumentation failure, or maybe some environmental effect has contaminated or tainted your data. In these cases the format, shape, and size may be correct, but the data itself may harm your model and prevent it from converging on a stable or accurate result. In many of these cases, the data point in question is an <strong>outlier</strong>, or a data point that doesn't seem to fit within the set.</p>
<p>Outliers exist in real life, and are often valid data. It's not always apparent by looking at the data by itself whether an outlier is valid or not, and we must also consider the context and algorithm when determining how to handle the data. For instance, let's say you're running a meta-analysis that relates patients' height to their heart performance and you've got 100 medical records available to analyze. One of the patients is listed with a height of 7'3" (221 cm). Is this a typo? Did the person who recorded the data actually mean 6'3" (190 cm)? What are the odds that, of only 100 random individuals, one of them is actually that tall? Should you still use this data point in your analysis, even though it will skew your otherwise very clean-looking results? What if the sample size were 1 million records instead of only 100? In that case, it's much more likely that you did actually select a very tall person. What if the sample size were only 100, but they were all NBA players?</p>
<p>As you can see, dealing with outliers is not straightforward. You should always be hesitant to discard data, especially if in doubt. By discarding data, you run the risk of creating a self-fulfilling prophecy by which you've consciously or subconsciously selected only the data that will support your hypothesis, even if your hypothesis is wrong. On the other hand, using legitimately bad data can ruin your results and prevent progress.</p>
<p>In this chapter, we will discuss a number of different considerations you must make when preprocessing data, including data transformations, handling missing data, selecting the correct parameters, handling outliers, and other forms of analysis that will be helpful in the data preprocessing stage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature identification</h1>
                </header>
            
            <article>
                
<p>Imagine that you are responsible for placing targeted product advertisements on an e-commerce store that you help run. The goal is to analyze a visitor's past shopping trends and select products to display that will increase the shopper's likelihood to make a purchase. Given then the gift of foresight, you've been collecting 50 different metrics on all of your shoppers for months: you've been recording past purchases, the product categories of those purchases, the price tag on each purchase, the time on site each user spent before making a purchase, and so on.</p>
<p>Believing that ML is a silver bullet, believing that more data is better, and believing that more training of your model is better, you load all 50 dimensions of data into an algorithm and train it for days on end. When testing your algorithm you find that its accuracy is very high when evaluating data points that you've trained the algorithm on, but also find that the algorithm fails spectacularly when evaluating against your validation set. Additionally, the model has taken a very long time to train. What went wrong here?</p>
<p>First, you've made the assumption that all of your 50 dimensions of data are relevant to the task at hand. It turns out that not all data is relevant. ML is great at finding patterns within data, but not all data actually contains patterns. Some data is random, and other data is not random but is also uninteresting. One example of uninteresting data that fits a pattern might be the time of day that the shopper is browsing your site on: users can only shop while they're awake, so most of your users shop between 7 a.m. and midnight. This data obviously follows a pattern, but may not actually affect the user's purchase intent. Of course, there may indeed be an interesting pattern: perhaps night owls tend to make late-night impulse purchases—but maybe not.</p>
<p>Second, using all 50 dimensions and training your model for a long period of time may cause overfitting of your model: instead of being able to generalize behavioral patterns and making shopping predictions, your overfitted model is now very good at identifying that a certain behavior represents Steve Johnson (one specific shopper), rather than generalizing Steve's behavior into a widely applicable trend. This overfit was caused by two factors: the long training time and the existence of irrelevant data in the training set. If one of the dimensions you've recorded is largely random and you spend a lot of time training a model on that data, the model may end up using that random data as an identifier for a user rather than filtering it out as a non-trend. The model may learn that, when the user's time on site is exactly 182 seconds, they will purchase a product worth $120, simply because you've trained the model on that data point many thousands of times in the training process.</p>
<p>Let's consider a different example: face identification. You've got thousands of photos of peoples' faces and want to be able to analyze a photo and determine who the subject is. You train a CNN on your data, and find that the accuracy of your algorithm is quite low, only being able to correctly identify the subject 60% of the time. The problem here may be that your CNN, working with raw pixel data, has not been able to automatically identify the features of a face that actually matter. For instance, Sarah Jane always takes her selfies in her kitchen, and her favorite spatula is always on display in the background. Any other user who also happens to have a spatula in the picture may be falsely identified as Sarah Jane, even if their faces are quite different. The data has overtrained the neural network to recognize spatulas as Sarah Jane, rather than actually looking at the user's face.</p>
<p>In both of these examples, the problem starts with insufficient preprocessing of data. In the e-commerce store example, you have not correctly identified the features of a shopper that actually matter, and so have trained your model with a lot of irrelevant data. The same problem exists in the face detection example: not every pixel in the photograph represents a person or their features, and in seeing a reliable pattern of spatulas the algorithm has learned that Sarah Jane is a spatula.</p>
<p>To solve both of these problems, you will need to make better selections of the features that you give to your ML model. In the e-commerce example, it may turn out that only 10 of your 50 recorded dimensions are relevant, and to fix the problem you must identify what those 10 dimensions are and only use those when training your model. In the face detection example, perhaps the neural network should not receive raw pixel intensity data but instead facial dimensions such as <em>nose bridge length</em>, <em>mouth width</em>, <em>distance between pupils</em>, <em>distance between pupil and eyebrow</em>, <em>distance between earlobes</em>, <em>distance from chin to hairline</em>, and so on. Both of these examples demonstrate the need to select the most relevant and appropriate features of your data. Making the appropriate selection of features will serve to improve both the speed and accuracy of your model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The curse of dimensionality</h1>
                </header>
            
            <article>
                
<p>In ML applications, we often have high-dimensional data. If we're recording 50 different metrics for each of our shoppers, we're working in a space with 50 dimensions. If we're analyzing grayscale images sized 100 x 100, we're working in a space with 10,000 dimensions. If the images are RGB-colored, the dimensionality increases to 30,000 dimensions (one dimension for each color channel in each pixel in the image)!</p>
<div class="packt_infobox">This problem is called the <strong>curse of dimensionality</strong>. On one hand, ML excels at analyzing data with many dimensions. Humans are not good at finding patterns that may be spread out across so many dimensions, especially if those dimensions are interrelated in counter-intuitive ways. On the other hand, as we add more dimensions we also increase the processing power we need to analyze the data, and we also increase the amount of training data required to make meaningful models.</div>
<p>One area that clearly demonstrates the curse of dimensionality is <strong>natural language processing</strong> (<strong>NLP</strong>). Imagine you are using a Bayesian classifier to perform sentiment analysis of tweets relating to brands or other topics. As you will learn in a later chapter, part of data preprocessing for NLP is tokenization of input strings into <strong>n-grams</strong>, or groups of words. Those n-grams are the features that are given to the Bayesian classifier algorithm.</p>
<p>Consider a few input strings: <kbd>I love cheese</kbd>, <kbd>I like cheese</kbd>, <kbd>I hate cheese</kbd>, <kbd>I don't love cheese</kbd>, <kbd>I don't really like cheese</kbd>. These examples are straightforward to us, since we've been using natural language our entire lives. How would an algorithm view these examples, though? If we are doing a 1-gram or <strong>unigram</strong> analysis—meaning that we split the input string into individual words—we see <kbd>love</kbd> in the first example, <kbd>like</kbd> in the second, <kbd>hate</kbd> in the third, <kbd>love</kbd> in the fourth, and <kbd>like</kbd> in the fifth. Our unigram analysis may be accurate for the first three examples, but it fails on the fourth and fifth because it does not learn that <kbd>don't love</kbd> and <kbd>don't really like</kbd> are coherent statements; the algorithm is only looking at the effects of individual words. This algorithm runs very quickly and requires little storage space, because in the preceding example there are only seven unique words used in the four phrases above (<kbd>I</kbd>, <kbd>love</kbd>, <kbd>cheese</kbd>, <kbd>like</kbd>, <kbd>hate</kbd>, <kbd>don't</kbd>, and <kbd>really</kbd>).</p>
<p>You may then modify the tokenization preprocessing to use <kbd>bigrams</kbd>, or 2-grams—or groups of two words at a time. This increases the dimensionality of our data, requiring more storage space and processing time, but also yields better results. The algorithm now sees dimensions like <kbd>I love</kbd> and <kbd>love cheese</kbd>, and can now also recognize that <kbd>don't love</kbd> is different from <kbd>I love</kbd>. Using the bigram approach the algorithm may correctly identify the sentiment of the first four examples but still fail for the fifth, which is parsed as <kbd>I don't</kbd>, <kbd>don't really</kbd>, <kbd>really like</kbd>, and <kbd>like cheese</kbd>. The classification algorithm will see <kbd>really like</kbd> and <kbd>like cheese</kbd>, and incorrectly relate that to the positive sentiment in the second example. Still, the bigram approach is working for 80% of our examples.</p>
<p>You might now be tempted to upgrade the tokenization once more to capture trigrams, or groups of three words at a time. Instead of getting an increase in accuracy, the algorithm takes a nosedive and is unable to correctly identify anything. We now have too many dimensions in our data. The algorithm learns what <kbd>I love cheese</kbd> means, but no other training example includes the phrase <kbd>I love cheese</kbd> so that knowledge can't be applied in any way. The fifth example parses into the trigrams <kbd>I don't really</kbd>, <kbd>don't really like</kbd>, and <kbd>really like cheese</kbd>—none of which have ever been encountered before! This algorithm ends up giving you a 50% sentiment for every example, because there simply isn't enough data in the training set to capture all of the relevant combinations of trigrams.</p>
<p>This is the curse of dimensionality at play: the <strong>trigram</strong> approach may indeed give you better accuracy than the bigram approach, but only if you have a huge training set that provides data on all the different possible combinations of three words at a time. You also now need a tremendous amount of storage space because there are a much larger number of combinations of three words than there are of two words. Choosing the preprocessing approach will therefore depend on the context of the problem, the computing resources available, and also the training data available to you. If you have a lot of training data and tons of resources, the trigram approach may be more accurate, but in more realistic conditions, the bigram approach may be better overall, even if it does misclassify some tweets.</p>
<p>The preceding discussion relates to the concepts of <strong>feature selection</strong>, <strong>feature extraction</strong>, and <strong>dimensionality</strong>. In general, our goal is to <em>select</em> only relevant features (ignore shopper trends that aren't interesting to us), <em>extract</em> or <em>derive</em> features that better represent our data (by using facial measurements rather than photograph pixels), and ultimately <em>reduce dimensionality</em> such that we use the fewest, most relevant dimensions we can.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature selection and feature extraction</h1>
                </header>
            
            <article>
                
<p>Both feature selection and feature extraction are techniques used to reduce dimensionality, though they are slightly different concepts. Feature selection is the practice of using only variables or features that are relevant to the problem at hand. In general, feature selection looks at individual features (such as <kbd>time on site</kbd>) and makes a determination of the relevance of that single feature. Feature extraction is similar, however feature extraction often looks at multiple correlated features and combines them into a single feature (like looking at hundreds of individual pixels and converting them into a <strong>distance between pupils </strong>measurement). In both cases, we are reducing the dimensionality of the problem, but the difference between the two is whether we are simply filtering out irrelevant dimensions (feature selection) or combining existing features in order to derive a new representative feature (feature extraction).</p>
<p>The goal of feature selection is to select the subset of features or dimensions of your data that optimizes the accuracy of your model. Let's take a look at the naive approach to solving this problem: an exhaustive, brute force search of all possible subsets of dimensions. This approach is not viable in real-world applications, but it serves to frame the problem for us. If we take the e-commerce store example, our goal is to find some subset of dimensions or features that gives us the best results from our model. We know we have 50 features to choose from, but we don't know how many are in the optimum set of features. Solving this problem by brute force, we would first pick only one feature at a time, and train and evaluate our model for each feature.</p>
<p>For instance, we would use only <kbd>time on site</kbd> as a data point, train the model on that data point, evaluate the model, and record the accuracy of the model. Then we move on to <kbd>total past purchase amount</kbd>, train the model, evaluate the model, and record results. We do this 48 more times for the remaining features and record the performance of each. Then we have to consider combinations of two features at a time, for instance by training and evaluating the model on <kbd>time on site</kbd> and <kbd>total past purchase amount</kbd>, and then training and evaluating on <kbd>time on site</kbd> and <kbd>last purchase date</kbd>, and so on. There are 1,225 unique pairs of features out of our set of 50, and we must repeat the procedure for each pair. Then we must consider groups of three features at a time, of which there are 19,600 combinations. Then we must consider groups of four features, of which there are 230,300 unique combinations. There are <span class="answer">2,118,760 combinations of five features, and nearly 16 million combinations of six features available to us, and so on. Obviously this exhaustive search for the optimal set of features to use cannot be done in a reasonable amount of time: we'd have to train our model billions of times just to find out what the best subset of features to use is! We must find a better approach.<br/></span></p>
<div class="packt_infobox">In general, feature selection techniques are split into three categories: filter methods, wrapper methods, and embedded methods. Each category has a number of techniques, and the technique you select will depend on the data, the context, and the algorithm of your specific situation.</div>
<p>Filter methods are the easiest to implement and typically have the best performance. Filter methods for feature selection analyze a single feature at a time and attempt to determine that feature's relevance to the data. Filter methods typically do not have any relation to the ML algorithm you use afterwards, and are more typically statistical methods that analyze the feature itself.</p>
<p>For instance, you may use the Pearson correlation coefficient to determine if a feature has a linear relationship with the output variable, and remove features with a correlation very close to zero. This family of approaches will be very fast in terms of computational time, but has the disadvantage of not being able to identify features that are cross-correlated with one another, and, depending on the filter algorithm you use, may not be able to identify nonlinear or complex relationships.</p>
<p>Wrapper methods are similar to the brute force approach described earlier, however with the goal of avoiding a full exhaustive search of every combination of features as we did previously. For instance, you may use a genetic algorithm to select subsets of features, train and evaluate the model, and then use the evaluation of the model as evolutionary pressure to find the next subset of features to test.</p>
<p>The genetic algorithm approach may not find the perfect subset of features, but will likely discover a very good subset of features to use. Depending on the actual machine learning model you use and the size of the dataset, this approach may still take a long time, but it will not take an intractably long amount of time like the exhaustive search would. The advantage of wrapper methods is that they interact with the actual model you're training and therefore serve to directly optimize your model, rather than simply attempting to independently statistically filter out individual features. The major disadvantage of these methods is the computational time it takes to achieve the desired results.</p>
<p>There is also a family of methods called <strong>embedded methods</strong>, however this family of techniques relies on algorithms that have their own feature selection algorithm built in and are therefore quite specialized; we will not discuss them here.</p>
<p>Feature extraction techniques focus on combining existing features into new, derived features that better represent your data while also eliminating extra or redundant dimensionality. Imagine that your e-commerce shopper data includes both <kbd>time on site</kbd> and <kbd>total pixel scrolling distance while browsing</kbd> as dimensions. Also imagine that both of these dimensions do strongly correlate to the amount of money a shopper spends on the site. Naturally, these two features are related to each other: the more time a user spends on the site, the more likely they are to have scrolled a farther distance. Using only feature selection techniques, such as the Pearson correlation analysis, you would find that you should keep both <kbd>time on site</kbd> and <kbd>total distance scrolled</kbd> as features. The feature selection technique, which analyzes these features independently, has determined that both are relevant to your problem, but has not understood that the two features are actually highly related to each other and therefore redundant.</p>
<p>A more sophisticated feature extraction technique, such as <strong>Principal Component Analysis</strong> (<strong>PCA</strong>), would be able to identify that time on site and scroll distance can actually be combined into a single, new feature (let's call it <kbd>site engagement</kbd>) that encapsulates the data represented by what used to be two separate features. In this case we have <em>extracted</em> a new feature from the time on site and scrolling distance measurements, and we are using that single feature instead of the two original features separately. This differs from feature selection; in feature selection we are simply choosing which of the original features to use when training our model, however in feature extraction we are creating brand new features from related combinations of original features. Both feature selection and feature extraction therefore reduce the dimensionality of our data, but do so in different ways.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pearson correlation example</h1>
                </header>
            
            <article>
                
<p>Let's return to our example of shoppers on the e-commerce store and consider how we might use the Pearson correlation coefficient to select data features. Consider the following example data, which records purchase amounts for shoppers given their time spent on site and the amount of money they had spent on purchases previously:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Purchase Amount</strong></p>
</td>
<td>
<p><strong>Time on Site (seconds)</strong></p>
</td>
<td>
<p><strong>Past Purchase Amount</strong></p>
</td>
</tr>
<tr>
<td>
<p>$10.00</p>
</td>
<td>
<p>53</p>
</td>
<td>
<p>$7.00</p>
</td>
</tr>
<tr>
<td>
<p>$14.00</p>
</td>
<td>
<p>220</p>
</td>
<td>
<p>$12.00</p>
</td>
</tr>
<tr>
<td>
<p>$18.00</p>
</td>
<td>
<p>252</p>
</td>
<td>
<p>$22.00</p>
</td>
</tr>
<tr>
<td>
<p>$20.00</p>
</td>
<td>
<p>571</p>
</td>
<td>
<p>$17.00</p>
</td>
</tr>
<tr>
<td>
<p>$22.00</p>
</td>
<td>
<p>397</p>
</td>
<td>
<p>$21.00</p>
</td>
</tr>
<tr>
<td>
<p>$34.00</p>
</td>
<td>
<p>220</p>
</td>
<td>
<p>$23.00</p>
</td>
</tr>
<tr>
<td>
<p>$38.00</p>
</td>
<td>
<p>776</p>
</td>
<td>
<p>$29.00</p>
</td>
</tr>
<tr>
<td>
<p>$50.00</p>
</td>
<td>
<p>462</p>
</td>
<td>
<p>$74.00</p>
</td>
</tr>
<tr>
<td>
<p>$52.00</p>
</td>
<td>
<p>354</p>
</td>
<td>
<p>$63.00</p>
</td>
</tr>
<tr>
<td>
<p>$56.00</p>
</td>
<td>
<p>23</p>
</td>
<td>
<p>$61.00</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root">Of course, in a real application of this problem you may have thousands or hundreds of thousands of rows, and dozens of columns, each representing a different dimension of data.</p>
<p>Let's now select features for this data manually. The <kbd>purchase amount</kbd> column is our output data, or the data that we want our algorithm to predict given other features. In this exercise, we can choose to train the model using both time on site and previous purchase amount, time on site alone, or previous purchase amount alone.</p>
<p>When using a filter method for feature selection we consider one feature at a time, so we must look at time on site's relation to purchase amount independently of past purchase amount's relation to purchase amount. One manual approach to this problem would be to chart each of our two candidate features against the <kbd>Purchase Amount</kbd> column, and calculate a correlation coefficient to determine how strongly each feature is related to the purchase amount data.</p>
<p>First, we'll chart time on site versus purchase amount, and use our spreadsheet tool to calculate the Pearson correlation coefficient:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/afddac71-1293-4b6b-a716-4c16ace48d71.png" style="width:36.33em;height:22.42em;"/></div>
<p>Even a simple visual inspection of the data hints to the fact that there is only a small relationship—if any at all—between time on site and purchase amount. Calculating the Pearson correlation coefficient yields a correlation of about +0.1, a very weak, essentially insignificant correlation between the two sets of data.</p>
<p>However, if we chart the past purchase amount versus current purchase amount, we see a very different relationship:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/42d01751-2f64-434d-b706-3ef5aff2c71f.png" style="width:37.83em;height:23.33em;"/></div>
<p>In this case, our visual inspection tells us that there is a linear but somewhat noisy relationship between the past purchase amount and the current purchase amount. Calculating the correlation coefficient gives us a correlation value of +0.9, quite a strong linear relationship!</p>
<p>This type of analysis tells us that we can ignore the time on site data when training our model, as there seems to be little to no statistical significance in that information. By ignoring time on site data, we can reduce the number of dimensions we need to train our model on by one, allowing our model to better generalize data and also improve performance.</p>
<p>If we had 48 other numerical dimensions to consider, we could simply calculate the correlation coefficient for each of them and discard each dimension whose correlation falls beneath some threshold. Not every feature can be analyzed using correlation coefficients, however, so you can only apply the Pearson algorithm to those features where such a statistical analysis makes sense; it would not make sense to use Pearson correlation to analyze a feature that lists <em>recently browsed product category</em>, for instance. You can, and should, use other types of feature selection filters for different dimensions representing different types of data. Over time, you will develop a toolkit of analysis techniques that can apply to different types of data.</p>
<p>Unfortunately, a thorough explanation of all the possible feature extraction and feature selection algorithms and tools is not possible here; you will have to research various techniques and determine which ones fit the shape and style of your features and data.</p>
<p>Some algorithms to consider for filter techniques are the Pearson and Spearman correlation coefficients, the chi-squared test, and information gain algorithms such as the Kullback–Leibler divergence.</p>
<p>Approaches to consider for wrapper techniques are optimization techniques such as genetic algorithms, tree-search algorithms such as best-first search, stochastic techniques such as random hill-climb algorithms, and heuristic techniques such as recursive feature elimination and simulated annealing. All of these techniques aim to select the best set of features that optimize the output of your model, so any optimization technique can be a candidate, however, genetic algorithms are quite effective and popular.</p>
<p>Feature extraction has many algorithms to consider, and generally focuses on cross-correlation of features in order to determine new features that minimize some error function; that is, how can two or more features be combined such that a minimum amount of data is lost. Relevant algorithms include PCA, partial least squares, and autoencoding. In NLP, latent semantic analysis is popular. Image processing has many specialized feature extraction algorithms, such as edge detection, corner detection, and thresholding, and further specializations based on problem domain such as face identification or motion detection.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cleaning and preparing data</h1>
                </header>
            
            <article>
                
<p>Feature selection is not the only consideration required when preprocessing your data. There are many other things that you may need to do to prepare your data for the algorithm that will ultimately analyze the data. Perhaps there are measurement errors that create significant outliers. There can also be instrumentation noise in the data that needs to be smoothed out. Your data may have missing values for some features. These are all issues that can either be ignored or addressed, depending, as always, on the context, the data, and the algorithm involved.</p>
<p>Additionally, the algorithm you use may require the data to be normalized to some range of values. Or perhaps your data is in a different format that the algorithm cannot use, as is often the case with neural networks which expect you to provide a vector of values, but you have JSON objects that come from a database. Sometimes you need to analyze only a specific subset of data from a larger source. If you're working with images you may need to resize, scale, pad, crop, or reduce the image to grayscale.</p>
<p>These tasks all fall into the realm of data preprocessing. Let's take a look at some specific scenarios and discuss possible approaches for each.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling missing data</h1>
                </header>
            
            <article>
                
<p>In many cases, several data points may have values missing from certain features. If you're looking at Yes/No responses to survey questions, several participants may have accidentally or purposefully skipped a given question. If you're looking at time series data, your measurement tool may have had an error for a given period or measurement. If you're looking at e-commerce shopping habits, some features may not be relevant to a user, for instance <kbd>last login date</kbd> for users that shop as an anonymous guest. The individual situation and scenario, as well as your algorithm's tolerance for missing data, determines the approach you must take to remediate missing data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Missing categorical data</h1>
                </header>
            
            <article>
                
<p>In the case of categorical data, such as Yes/No survey questions that may not have been responded to, or an image that has not yet been labeled with its category, often the best approach is to create a new category called <em>undefined</em>, <em>N/A</em>, <em>unknown</em>, or <em>similar</em>. Alternatively, you may be able to select a reasonable default category to use for these missing values, perhaps choosing the most frequent category from the set, or choosing a category that represents the data point's logical parent. If you're analyzing photographs uploaded by users and are missing the category tag for a given photograph, you may instead use the <em>user's</em> stated category in place of the photo's individual category. That is, if a user is tagged as a fashion photographer, you may use the <em>fashion</em> category for the photo, even though the user has also uploaded a number of <em>travel</em> photographs. This approach will add noise in the form of miscategorized data points to the system, but may in fact have a positive overall effect of forcing the algorithm to generalize its model; the model may eventually learn that fashion and travel photography are similar.</p>
<p>Using an <em>undefined</em> or <em>N/A</em> category is also a preferred approach, as the fact that a data point has no category may be significant in and of itself—<em>No category</em> can itself be a valid category. The size of the dataset, the algorithm used, and the relative size of the <em>N/A</em> category within the dataset will affect whether this is a reasonable approach to take. In a classification scenario, for instance, two effects are possible. If the uncategorized items <em>do</em> form a pattern (for instance, <em>fashion</em> photos are uncategorized more often than other photos), you may find that your classifier incorrectly learns that fashion photos should be categorized as N/A! In this scenario, it may be better to ignore uncategorized data points entirely.</p>
<p>However, if the uncategorized photos are comprised of photos from various categories equally, your classifier may end up identifying difficult-to-classify photos as N/A, which could actually be a desired effect. In this scenario, you can consider N/A as a class of its own, being comprised of difficult, broken, or unresolvable photos.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Missing numerical data</h1>
                </header>
            
            <article>
                
<p>Missing values for numerical data is trickier to handle than categorical data, as there is often no reasonable default for missing numerical values. Depending on the dataset, you may be able to use zeros as replacements, however in some cases using the mean or median value of that feature is more appropriate. In other scenarios, and depending on the algorithm used, it may be useful to fill in missing values with a very large value: if that data point needs to have an error calculation performed on it, using a large value will mark the data point with a large error, and discourage the algorithm from considering that point.</p>
<p>In other cases, you can use a linear interpolation to fill in missing data points. This makes sense in some time series applications. If your algorithm expects 31 data points representing the growth of some metric, and you're missing one value for day 12, you can use the average of day 11's and day 13's values to serve as an estimate for day 12's value.</p>
<p>Often the correct approach is to ignore and filter out data points with missing values, however, you must consider the effects of such an action. If the data points with missing values strongly represent a specific category of data, you may end up creating a strong selection bias as a side effect, as your analysis would have ignored a significant group. You must balance this type of side effect with the possible side effects caused by the other approaches: will zeroing out missing values significantly skew your distribution? Will using the mean or median as replacements taint the rest of the analysis? These questions can only be answered on a case-by-case basis.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling noise</h1>
                </header>
            
            <article>
                
<p>Noise in data can come from many sources, but is not often a significant issue as most machine learning techniques are resilient to noisy datasets. Noise can come from environmental factors (for instance, the air conditioner compressor turning on randomly and causing signal noise in a nearby sensor), it can come from transcription errors (somebody recorded the wrong data point, selected the wrong option in a survey, or an OCR algorithm read a <kbd>3</kbd> as an <kbd>8</kbd>), or it can be inherent to the data itself (such as fluctuations in temperature recordings, which will follow a seasonal pattern but have a noisy daily pattern).</p>
<p>Noise in categorical data can also be caused by category labels that aren't normalized, such as images that are tagged <kbd>fashion</kbd> or <kbd>fashions</kbd> when the category is supposed to be <kbd>Fashion</kbd>. In those scenarios, the best approach is to simply normalize the category label, perhaps by forcing all category labels to be made singular and fully lowercase—this will combine the <kbd>Fashion</kbd>, <kbd>fashion</kbd>, and <kbd>fashions</kbd> categories into one single <kbd>fashion</kbd> category.</p>
<p>Noise in time series data can be smoothed by taking a moving average of multiple values; however, first you should evaluate if smoothing the data is important to your algorithm and results in the first place. Often, the algorithm will still perform well enough for practical applications if there is a small amount of noise, and especially if the noise is random rather than systemic.</p>
<p>Consider the following example of daily measurements of some sensor:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Day</strong></p>
</td>
<td>
<p><strong>Value</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>0.1381426172</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>0.5678176776</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>0.3564009968</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>1.239499423</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>1.267606181</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>1.440843361</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>0.3322843208</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>0.4329166745</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>0.5499234277</p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>-0.4016070826</p>
</td>
</tr>
<tr>
<td>
<p>11</p>
</td>
<td>
<p>0.06216906816</p>
</td>
</tr>
<tr>
<td>
<p>12</p>
</td>
<td>
<p>-0.9689103112</p>
</td>
</tr>
<tr>
<td>
<p>13</p>
</td>
<td>
<p>-1.170421963</p>
</td>
</tr>
<tr>
<td>
<p>14</p>
</td>
<td>
<p>-0.784125647</p>
</td>
</tr>
<tr>
<td>
<p>15</p>
</td>
<td>
<p>-1.224217169</p>
</td>
</tr>
<tr>
<td>
<p>16</p>
</td>
<td>
<p>-0.4689120937</p>
</td>
</tr>
<tr>
<td>
<p>17</p>
</td>
<td>
<p>-0.7458561671</p>
</td>
</tr>
<tr>
<td>
<p>18</p>
</td>
<td>
<p>-0.6746415566</p>
</td>
</tr>
<tr>
<td>
<p>19</p>
</td>
<td>
<p>-0.0429460593</p>
</td>
</tr>
<tr>
<td>
<p>20</p>
</td>
<td>
<p>0.06757010626</p>
</td>
</tr>
<tr>
<td>
<p>21</p>
</td>
<td>
<p>0.480806698</p>
</td>
</tr>
<tr>
<td>
<p>22</p>
</td>
<td>
<p>0.2019759014</p>
</td>
</tr>
<tr>
<td>
<p>23</p>
</td>
<td>
<p>0.7857692899</p>
</td>
</tr>
<tr>
<td>
<p>24</p>
</td>
<td>
<p>0.725414402</p>
</td>
</tr>
<tr>
<td>
<p>25</p>
</td>
<td>
<p>1.188534085</p>
</td>
</tr>
<tr>
<td>
<p>26</p>
</td>
<td>
<p>0.458488458</p>
</td>
</tr>
<tr>
<td>
<p>27</p>
</td>
<td>
<p>0.3017212831</p>
</td>
</tr>
<tr>
<td>
<p>28</p>
</td>
<td>
<p>0.5249332545</p>
</td>
</tr>
<tr>
<td>
<p>29</p>
</td>
<td>
<p>0.3333153146</p>
</td>
</tr>
<tr>
<td>
<p>30</p>
</td>
<td>
<p>-0.3517342423</p>
</td>
</tr>
<tr>
<td>
<p>31</p>
</td>
<td>
<p>-0.721682062</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Graphing this data shows a noisy but periodic pattern:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/77ebb25f-47d8-4c98-84c4-c899370b9c64.png" style="width:47.58em;height:26.58em;"/></div>
<p>This may be acceptable in many scenarios, but other applications may require smoother data.</p>
<div class="packt_infobox">Also, note that several of the data points exceed +1 and -1, which may be of significance especially if your algorithm is expecting data between the -1 and +1 range.</div>
<p>We can apply a <kbd>5-Day Moving Average</kbd> to the data to generate a smoother curve. To perform a <kbd>5-Day Moving Average</kbd>, start with day <kbd>3</kbd>, sum the values for days <kbd>1</kbd> to <kbd>5</kbd>, and divide by 5. The result becomes the moving average for day <kbd>3</kbd>.</p>
<p>Note that in this approach, we lose days <kbd>1</kbd> and <kbd>2</kbd>, and also days <kbd>30</kbd> and <kbd>31</kbd>, because we cannot look two days before day <kbd>1</kbd> nor can we look two days after day <kbd>31</kbd>. However, if you require values for those days, you may use the raw values for days <kbd>1</kbd>, <kbd>2</kbd>, <kbd>30</kbd>, and <kbd>31</kbd>, or you may use <kbd>3-Day Moving Averages</kbd> for days <kbd>2</kbd> and <kbd>30</kbd> in addition to single values for days <kbd>1</kbd> and <kbd>31</kbd>. If you have more historical data, you can use data from the previous month calculating the <kbd>5-Day Moving Average</kbd> for days <kbd>1</kbd> and <kbd>2</kbd> (calculate day <kbd>1</kbd> by using the previous month's last two days). The approach to how you handle this moving average will depend on the data available to you and the importance of having 5-day averages for each data point versus combining 5-day averages with 3-day and 1-day averages at the boundaries.</p>
<p>If we calculate the <kbd>5-Day Moving Average</kbd> for our month, the data becomes the following:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Day</strong></p>
</td>
<td>
<p><strong>Value</strong></p>
</td>
<td>
<p><strong>5-Day Moving Average</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>0.1381426172</p>
</td>
<td/>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>0.5678176776</p>
</td>
<td/>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>0.3564009968</p>
</td>
<td>
<p>0.7138933792</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>1.239499423</p>
</td>
<td>
<p>0.974433528</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>1.267606181</p>
</td>
<td>
<p>0.9273268566</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>1.440843361</p>
</td>
<td>
<p>0.9426299922</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>0.3322843208</p>
</td>
<td>
<p>0.8047147931</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>0.4329166745</p>
</td>
<td>
<p>0.4708721403</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>0.5499234277</p>
</td>
<td>
<p>0.1951372817</p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>-0.4016070826</p>
</td>
<td>
<p>-0.06510164468</p>
</td>
</tr>
<tr>
<td>
<p>11</p>
</td>
<td>
<p>0.06216906816</p>
</td>
<td>
<p>-0.3857693722</p>
</td>
</tr>
<tr>
<td>
<p>12</p>
</td>
<td>
<p>-0.9689103112</p>
</td>
<td>
<p>-0.6525791871</p>
</td>
</tr>
<tr>
<td>
<p>13</p>
</td>
<td>
<p>-1.170421963</p>
</td>
<td>
<p>-0.8171012043</p>
</td>
</tr>
<tr>
<td>
<p>14</p>
</td>
<td>
<p>-0.784125647</p>
</td>
<td>
<p>-0.9233174367</p>
</td>
</tr>
<tr>
<td>
<p>15</p>
</td>
<td>
<p>-1.224217169</p>
</td>
<td>
<p>-0.8787066079</p>
</td>
</tr>
<tr>
<td>
<p>16</p>
</td>
<td>
<p>-0.4689120937</p>
</td>
<td>
<p>-0.7795505266</p>
</td>
</tr>
<tr>
<td>
<p>17</p>
</td>
<td>
<p>-0.7458561671</p>
</td>
<td>
<p>-0.631314609</p>
</td>
</tr>
<tr>
<td>
<p>18</p>
</td>
<td>
<p>-0.6746415566</p>
</td>
<td>
<p>-0.3729571541</p>
</td>
</tr>
<tr>
<td>
<p>19</p>
</td>
<td>
<p>-0.0429460593</p>
</td>
<td>
<p>-0.1830133958</p>
</td>
</tr>
<tr>
<td>
<p>20</p>
</td>
<td>
<p>0.06757010626</p>
</td>
<td>
<p>0.006553017948</p>
</td>
</tr>
<tr>
<td>
<p>21</p>
</td>
<td>
<p>0.480806698</p>
</td>
<td>
<p>0.2986351872</p>
</td>
</tr>
<tr>
<td>
<p>22</p>
</td>
<td>
<p>0.2019759014</p>
</td>
<td>
<p>0.4523072795</p>
</td>
</tr>
<tr>
<td>
<p>23</p>
</td>
<td>
<p>0.7857692899</p>
</td>
<td>
<p>0.6765000752</p>
</td>
</tr>
<tr>
<td>
<p>24</p>
</td>
<td>
<p>0.725414402</p>
</td>
<td>
<p>0.6720364272</p>
</td>
</tr>
<tr>
<td>
<p>25</p>
</td>
<td>
<p>1.188534085</p>
</td>
<td>
<p>0.6919855036</p>
</td>
</tr>
<tr>
<td>
<p>26</p>
</td>
<td>
<p>0.458488458</p>
</td>
<td>
<p>0.6398182965</p>
</td>
</tr>
<tr>
<td>
<p>27</p>
</td>
<td>
<p>0.3017212831</p>
</td>
<td>
<p>0.561398479</p>
</td>
</tr>
<tr>
<td>
<p>28</p>
</td>
<td>
<p>0.5249332545</p>
</td>
<td>
<p>0.2533448136</p>
</td>
</tr>
<tr>
<td>
<p>29</p>
</td>
<td>
<p>0.3333153146</p>
</td>
<td>
<p>0.0173107096</p>
</td>
</tr>
<tr>
<td>
<p>30</p>
</td>
<td>
<p>-0.3517342423</p>
</td>
<td/>
</tr>
<tr>
<td>
<p>31</p>
</td>
<td>
<p>-0.721682062</p>
</td>
<td/>
</tr>
</tbody>
</table>
<p> </p>
<p>In some cases, the moving average differs from the day's data point by a significant margin. On day <kbd>3</kbd>, for instance, the moving average is double that of the day's measurement. </p>
<p><span>This approach would not be appropriate in instances where you need to consider a given day's measurement in isolation, however, when we graph the moving average against the daily data points, we can see the value of this approach:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f5147b1d-5741-4d10-b9d4-b6c6f2f88682.png" style="width:48.33em;height:29.83em;"/></div>
<p>We can see that the moving average is much smoother than the daily measurements, and that the moving average better represents the periodic, sinusoidal nature of our data. An added bonus for us is that the moving average data no longer contains points that lie outside our [-1, +1] range; because the noise in this data was random, the random fluctuations have largely canceled each other out and brought our data back into range.</p>
<p>Increasing the window of the moving average will result in broader and broader averages, reducing resolution; if we were to take a <em>31-Day Moving Average</em>, we would simply have the average measurement for the entire month. If your application simply needs to smooth out data rather than reduce data to lower resolutions, you should start by applying the smallest moving average window that serves to clean the data enough, for instance, a 3-point moving average.</p>
<p>If you're dealing with measurements that are not time series, then a moving average approach may not be appropriate. For instance, if you're measuring the value of a sensor at arbitrary and random times where the time of measurement is not recorded, a moving average would not be appropriate because the dimension to average over is unknown (that is, we do not know the time period that the average moves over).</p>
<p>If you still need to eliminate noise from your data, you can try <em>binning</em> the measurements by creating a histogram of the data. This approach changes the nature of the data itself and does not apply to every situation, however, it can serve to obfuscate individual measurement fluctuations while still representing the relative frequency of different measurements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling outliers</h1>
                </header>
            
            <article>
                
<p>Your data will often have outlying values, or data points that are far away from the expected value for your dataset. Sometimes, outliers are caused by noise or errors (somebody recording a height of 7'3" rather than 6'3"), but other times, outliers are legitimate data points (one celebrity with a Twitter reach of 10 million followers joining your service where most of the users have 10,000 to 100,000 followers). In either case, you'll first want to identify outliers so that you can determine what to do with them.</p>
<p>One approach to identifying outliers is to calculate the mean and standard deviation of your dataset, and determine how many standard deviations away from the mean each data point is. The standard deviation of a dataset represents the overall variance or dispersion of the data. Consider the following data which represents the number of Twitter followers of accounts that you're analyzing:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Followers</strong></p>
</td>
</tr>
<tr>
<td>
<p>1075</p>
</td>
</tr>
<tr>
<td>
<p>1879</p>
</td>
</tr>
<tr>
<td>
<p>3794</p>
</td>
</tr>
<tr>
<td>
<p>4111</p>
</td>
</tr>
<tr>
<td>
<p>4243</p>
</td>
</tr>
<tr>
<td>
<p>4885</p>
</td>
</tr>
<tr>
<td>
<p>7617</p>
</td>
</tr>
<tr>
<td>
<p>8555</p>
</td>
</tr>
<tr>
<td>
<p>8755</p>
</td>
</tr>
<tr>
<td>
<p>19422</p>
</td>
</tr>
<tr>
<td>
<p>31914</p>
</td>
</tr>
<tr>
<td>
<p>36732</p>
</td>
</tr>
<tr>
<td>
<p>39570</p>
</td>
</tr>
<tr>
<td>
<p>1230324</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>As you can see, the last value is much larger than the other values in the set. However, this discrepancy may not be so obvious if you're analyzing millions of records with dozens of features each. To automate our outlier identification we should first calculate the mean average of all our users, which in this case is an average of <strong>100,205</strong> followers. Then, we should calculate the standard deviation of the dataset, which for this data is <strong>325,523</strong> followers. Finally, we can inspect each data point by determining how many standard deviations away from the mean that data point is: find the absolute difference between the data point and the mean, and then divide by the standard deviation:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Followers</strong></p>
</td>
<td>
<p><strong>Deviation</strong></p>
</td>
</tr>
<tr>
<td>
<p>1075</p>
</td>
<td>
<p>0.3045078726</p>
</td>
</tr>
<tr>
<td>
<p>1879</p>
</td>
<td>
<p>0.3020381533</p>
</td>
</tr>
<tr>
<td>
<p>3794</p>
</td>
<td>
<p>0.2961556752</p>
</td>
</tr>
<tr>
<td>
<p>4111</p>
</td>
<td>
<p>0.2951819177</p>
</td>
</tr>
<tr>
<td>
<p>4243</p>
</td>
<td>
<p>0.2947764414</p>
</td>
</tr>
<tr>
<td>
<p>4885</p>
</td>
<td>
<p>0.2928043522</p>
</td>
</tr>
<tr>
<td>
<p>7617</p>
</td>
<td>
<p>0.2844122215</p>
</td>
</tr>
<tr>
<td>
<p>8555</p>
</td>
<td>
<p>0.2815308824</p>
</td>
</tr>
<tr>
<td>
<p>8755</p>
</td>
<td>
<p>0.2809165243</p>
</td>
</tr>
<tr>
<td>
<p>19422</p>
</td>
<td>
<p>0.248149739</p>
</td>
</tr>
<tr>
<td>
<p>31914</p>
</td>
<td>
<p>0.2097769366</p>
</td>
</tr>
<tr>
<td>
<p>36732</p>
</td>
<td>
<p>0.1949770517</p>
</td>
</tr>
<tr>
<td>
<p>39570</p>
</td>
<td>
<p>0.1862593113</p>
</td>
</tr>
<tr>
<td>
<p>1230324</p>
</td>
<td>
<p>3.471487079</p>
</td>
</tr>
</tbody>
</table>
<p>The approach has yielded good results: all data points except one are found within one standard deviation of the mean, and our outlier is far away from the average with a distance of nearly 3.5 deviations. In general, you can consider data points more than two or three standard deviations away from the mean to be outliers.</p>
<p>If your dataset represents a normal distribution, then you can use the <strong>68-95-99.7</strong> rule: 68% of data points are expected to be within one standard deviation, 95% are expected to be within two deviations, and 99.7% of data points are expected to be within three standard deviations. In a normal distribution, therefore, only 0.3% of data is expected to be farther than three standard deviations from the mean.</p>
<div class="packt_infobox">Note that the preceding data presented is not a normal distribution, and much of your data will not follow normal distributions either, but the concept of standard deviation may still apply (the ratios of data points expected per standard deviation will differ based on the distribution).</div>
<p>Now that an outlier is identified, a determination must be made as to how to handle the outlying data point. In some cases, it's better to keep the outliers in your dataset and continue processing as usual; outliers that are based in real data are often important data points that can't be ignored, because they represent uncommon but possible values for your data.</p>
<p>For instance, if you're monitoring a server's CPU load average and find an average value of 2.0 with a standard deviation of 1.0, you would not want to ignore data points with load averages of 10.0—those data points still represent load averages that your CPU actually experienced, and for many types of analysis it would be self-defeating to ignore that data, even though those points are far away from the mean. Those points should be considered and accounted for in your analysis. However, in our Twitter followers example we may want to ignore the outlier, especially if our analysis is to determine behavioral patterns of Twitter users' audiences—our outlier most likely exhibits a completely separate class of behavioral patterns that may simply confuse our analysis.</p>
<p>There is another approach to handling outliers that works well when considering data that's expected to be linear, polynomial, exponential, or periodic—the types of datasets where a regression can be performed. Consider data that is expected to be linear, like the following:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Observation</strong></p>
</td>
<td>
<p><strong>Value</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>2</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>3</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>4</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>5</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>6</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>22</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>8</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>9</p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>10</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>When performing a linear regression on this data, we can see that the outlying data point skews the regression upwards:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a21613d5-db9b-4a1f-9de0-31722a5c158d.png" style="width:36.08em;height:22.25em;"/></div>
<p>For this small set of data points the error in the regression may not be significant, but if you're using the regression to extrapolate future values, for instance, for observation number 30, the predicted value will be far from the actual value as the small error introduced by the outlier compounds the further you extrapolate values. In this case, we would want to remove the outlier before performing the regression so that the regression's extrapolation is more accurate.</p>
<p>In order to identify the outlier, we can perform a linear regression as we have before, and then calculate the squared error from the trendline for each point. If the data point exceeds an error of, for instance, 25%, we can consider that point an outlier and remove it before performing the regression a second time. Once we've removed the outlier and re-performed the regression, the trendline fits the data much better:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/273c129c-5c13-4fec-b3ed-61054c42fb68.png" style="width:38.08em;height:23.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transforming and normalizing data</h1>
                </header>
            
            <article>
                
<p>The most common preprocessing task is to transform and/or normalize data into a representation that can be used by your algorithm. For instance, you may receive JSON objects from an API endpoint that you need to transform into vectors used by your algorithm. Consider the following JSON data:</p>
<pre>const users = [<br/>     {<br/>       "name": "Andrew",<br/>       "followers": 27304,<br/>       "posts": 34,<br/>       "images": 38,<br/>       "engagements": 2343,<br/>       "is_verified": false<br/>     },<br/>     {<br/>       "name": "Bailey",<br/>       "followers": 32102,<br/>       "posts": 54,<br/>       "images": 102,<br/>       "engagements": 9488,<br/>       "is_verified": true<br/>     },<br/>     {<br/>       "name": "Caroline",<br/>       "followers": 19932,<br/>       "posts": 12,<br/>       "images": 0,<br/>       "engagements": 19,<br/>       "is_verified": false<br/>     }<br/>];</pre>
<p>Your neural network that processes the data expects input data in vector form, like so:</p>
<pre>[followers, posts, images, engagements, is_verified]</pre>
<p>In JavaScript, the easiest way to transform our JSON data in this situation is to use the built-in <kbd>Array.map</kbd> function. The following code will generate an array of vectors (an array of arrays). This form of transformation will be very common throughout this book:</p>
<pre>const vectors = users.map(user =&gt; [<br/>     user.followers,<br/>     user.posts,<br/>     user.images,<br/>     user.engagements,<br/>     user.is_verified ? 1 : 0<br/>   ]);</pre>
<p>Note that we are using the shortest form of ES6 arrow functions, which doesn't require parentheses around the parameters nor an explicit return statement, since we return our array of features directly. An equivalent ES5 example would look like the following:</p>
<pre>var vectors = users.map(function(user) {<br/>     return [<br/>       user.followers,<br/>       user.posts,<br/>       user.images,<br/>       user.engagements,<br/>       user.is_verified ? 1 : 0<br/>     ];<br/>   });</pre>
<p>Also note that the <kbd>is_verified</kbd> field was converted to an integer using the ternary operator, <kbd>user.is_verified ? 1 : 0</kbd>. Neural networks can only work with numeric values, and so we must represent the Boolean value as an integer.</p>
<div class="packt_tip">We will discuss techniques for using natural language with neural networks in a later chapter.</div>
<p>Another common data transformation is to normalize data values into a given range, for instance between -1 and +1. Many algorithms depend on data values falling within this range, however, most real-world data does not. Let's revisit our noisy daily sensor data from earlier in the chapter, and let's assume that we have access to this data in a simple JavaScript array called <strong>measurements</strong> (detail-oriented readers will notice I changed the value of day <kbd>15</kbd> as compared with the earlier example):</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Day</strong></p>
</td>
<td>
<p><strong>Value</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>0.1381426172</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>0.5678176776</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>0.3564009968</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>1.239499423</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>1.267606181</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>1.440843361</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>0.3322843208</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>0.4329166745</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>0.5499234277</p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>-0.4016070826</p>
</td>
</tr>
<tr>
<td>
<p>11</p>
</td>
<td>
<p>0.06216906816</p>
</td>
</tr>
<tr>
<td>
<p>12</p>
</td>
<td>
<p>-0.9689103112</p>
</td>
</tr>
<tr>
<td>
<p>13</p>
</td>
<td>
<p>-1.170421963</p>
</td>
</tr>
<tr>
<td>
<p>14</p>
</td>
<td>
<p>-0.784125647</p>
</td>
</tr>
<tr>
<td>
<p>15</p>
</td>
<td>
<p>-1.524217169</p>
</td>
</tr>
<tr>
<td>
<p>16</p>
</td>
<td>
<p>-0.4689120937</p>
</td>
</tr>
<tr>
<td>
<p>17</p>
</td>
<td>
<p>-0.7458561671</p>
</td>
</tr>
<tr>
<td>
<p>18</p>
</td>
<td>
<p>-0.6746415566</p>
</td>
</tr>
<tr>
<td>
<p>19</p>
</td>
<td>
<p>-0.0429460593</p>
</td>
</tr>
<tr>
<td>
<p>20</p>
</td>
<td>
<p>0.06757010626</p>
</td>
</tr>
<tr>
<td>
<p>21</p>
</td>
<td>
<p>0.480806698</p>
</td>
</tr>
<tr>
<td>
<p>22</p>
</td>
<td>
<p>0.2019759014</p>
</td>
</tr>
<tr>
<td>
<p>23</p>
</td>
<td>
<p>0.7857692899</p>
</td>
</tr>
<tr>
<td>
<p>24</p>
</td>
<td>
<p>0.725414402</p>
</td>
</tr>
<tr>
<td>
<p>25</p>
</td>
<td>
<p>1.188534085</p>
</td>
</tr>
<tr>
<td>
<p>26</p>
</td>
<td>
<p>0.458488458</p>
</td>
</tr>
<tr>
<td>
<p>27</p>
</td>
<td>
<p>0.3017212831</p>
</td>
</tr>
<tr>
<td>
<p>28</p>
</td>
<td>
<p>0.5249332545</p>
</td>
</tr>
<tr>
<td>
<p>29</p>
</td>
<td>
<p>0.3333153146</p>
</td>
</tr>
<tr>
<td>
<p>30</p>
</td>
<td>
<p>-0.3517342423</p>
</td>
</tr>
<tr>
<td>
<p>31</p>
</td>
<td>
<p>-0.721682062</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>If we wish to normalize this data to the range [-1, +1], we must first discover the largest <em>absolute value</em> of all numbers in the set, which in this case is day 15's value of <kbd>-1.52</kbd>. If we were to simply use JavaScript's <kbd>Math.max</kbd> on this data, we would find the maximum value on the number line, which is day 6's value of <kbd>1.44</kbd>—however, day <kbd>15</kbd> is more negative than day <kbd>6</kbd> is positive.</p>
<p>Finding the maximum absolute value in a JavaScript array can be accomplished with the following:</p>
<pre>const absolute_max = Math.max.apply(null, measurements.map(Math.abs));</pre>
<p>The value of <kbd>absolute_max</kbd> will be +1.524217169—the number became positive when we called <kbd>Math.abs</kbd> using <kbd>measurements.map</kbd>. It is important that the absolute maximum value remains positive, because in the next step we will divide by the maximum and want to preserve the signs of all data points.</p>
<p>Given the absolute maximum value, we can normalize our data points like so:</p>
<pre>const normalized = measurements.map(value =&gt; value / absolute_max);</pre>
<p>By dividing each number by the maximum value in the set, we ensure that all values lie in the range [-1, +1]. The maximum value will be (in this case) -1, and all other numbers in the set will be closer to 0 than the maximum will. After normalizing, our data now looks like this:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Day</strong></p>
</td>
<td>
<p><strong>Value</strong></p>
</td>
<td>
<p><strong>Normalized</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>0.1381426172</p>
</td>
<td>
<p>0.09063184696</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>0.5678176776</p>
</td>
<td>
<p>0.3725306927</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>0.3564009968</p>
</td>
<td>
<p>0.2338256018</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>1.239499423</p>
</td>
<td>
<p>0.8132039508</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>1.267606181</p>
</td>
<td>
<p>0.8316440777</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>1.440843361</p>
</td>
<td>
<p>0.9453005718</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>0.3322843208</p>
</td>
<td>
<p>0.218003266</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>0.4329166745</p>
</td>
<td>
<p>0.284025586</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>0.5499234277</p>
</td>
<td>
<p>0.3607907319</p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>-0.4016070826</p>
</td>
<td>
<p>-0.2634841615</p>
</td>
</tr>
<tr>
<td>
<p>11</p>
</td>
<td>
<p>0.06216906816</p>
</td>
<td>
<p>0.04078753963</p>
</td>
</tr>
<tr>
<td>
<p>12</p>
</td>
<td>
<p>-0.9689103112</p>
</td>
<td>
<p>-0.6356773373</p>
</td>
</tr>
<tr>
<td>
<p>13</p>
</td>
<td>
<p>-1.170421963</p>
</td>
<td>
<p>-0.7678839913</p>
</td>
</tr>
<tr>
<td>
<p>14</p>
</td>
<td>
<p>-0.784125647</p>
</td>
<td>
<p>-0.5144448332</p>
</td>
</tr>
<tr>
<td>
<p>15</p>
</td>
<td>
<p>-1.524217169</p>
</td>
<td>
<p>-1</p>
</td>
</tr>
<tr>
<td>
<p>16</p>
</td>
<td>
<p>-0.4689120937</p>
</td>
<td>
<p>-0.3076412623</p>
</td>
</tr>
<tr>
<td>
<p>17</p>
</td>
<td>
<p>-0.7458561671</p>
</td>
<td>
<p>-0.4893372037</p>
</td>
</tr>
<tr>
<td>
<p>18</p>
</td>
<td>
<p>-0.6746415566</p>
</td>
<td>
<p>-0.4426151145</p>
</td>
</tr>
<tr>
<td>
<p>19</p>
</td>
<td>
<p>-0.0429460593</p>
</td>
<td>
<p>-0.02817581391</p>
</td>
</tr>
<tr>
<td>
<p>20</p>
</td>
<td>
<p>0.06757010626</p>
</td>
<td>
<p>0.04433102293</p>
</td>
</tr>
<tr>
<td>
<p>21</p>
</td>
<td>
<p>0.480806698</p>
</td>
<td>
<p>0.3154450087</p>
</td>
</tr>
<tr>
<td>
<p>22</p>
</td>
<td>
<p>0.2019759014</p>
</td>
<td>
<p>0.1325112363</p>
</td>
</tr>
<tr>
<td>
<p>23</p>
</td>
<td>
<p>0.7857692899</p>
</td>
<td>
<p>0.5155231854</p>
</td>
</tr>
<tr>
<td>
<p>24</p>
</td>
<td>
<p>0.725414402</p>
</td>
<td>
<p>0.4759258831</p>
</td>
</tr>
<tr>
<td>
<p>25</p>
</td>
<td>
<p>1.188534085</p>
</td>
<td>
<p>0.7797668924</p>
</td>
</tr>
<tr>
<td>
<p>26</p>
</td>
<td>
<p>0.458488458</p>
</td>
<td>
<p>0.3008025808</p>
</td>
</tr>
<tr>
<td>
<p>27</p>
</td>
<td>
<p>0.3017212831</p>
</td>
<td>
<p>0.1979516366</p>
</td>
</tr>
<tr>
<td>
<p>28</p>
</td>
<td>
<p>0.5249332545</p>
</td>
<td>
<p>0.3443953167</p>
</td>
</tr>
<tr>
<td>
<p>29</p>
</td>
<td>
<p>0.3333153146</p>
</td>
<td>
<p>0.2186796747</p>
</td>
</tr>
<tr>
<td>
<p>30</p>
</td>
<td>
<p>-0.3517342423</p>
</td>
<td>
<p>-0.2307638633</p>
</td>
</tr>
<tr>
<td>
<p>31</p>
</td>
<td>
<p>-0.721682062</p>
</td>
<td>
<p>-0.4734771901</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root">There are no data points outside of the [-1, +1] range, and you can also see that day <kbd>15</kbd>, with the maximum absolute value of the data, has been normalized as <kbd>-1</kbd>. Graphing the data shows the relationship between the original and normalized values:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-332 image-border" src="assets/4abf6168-227e-42e3-a946-48011ad77e60.png" style="width:50.58em;height:31.08em;"/></div>
<p>The shape of the data has been preserved, and the chart has simply been scaled by a constant factor. This data is now ready to use in algorithms that require normalized ranges, such as PCA, for instance.</p>
<p>Your data is likely much more complex than these preceding examples. Perhaps your JSON data is composed of complex objects with nested entities and arrays. You may need to run an analysis on only those items which have specific sub-elements, or you may need to generate dynamic subsets of data based on some user-provided query or filter.</p>
<p>For complex situations and datasets, you may want some help from a third-party library such as <kbd>DataCollection.js</kbd>, which is a library that adds SQL and NoSQL style query functionality to JavaScript arrays. Imagine that our preceding JSON data of <strong>users</strong> also contained an object called <strong>locale</strong> which gives the user's country and language:</p>
<pre>const users = [<br/>     {<br/>       "name": "Andrew",<br/>       "followers": 27304,<br/>       "posts": 34,<br/>       "images": 38,<br/>       "engagements": 2343,<br/>       "is_verified": false,<br/>       "locale": {<br/>         "country":"US",<br/>         "language":"en_US"<br/>       }<br/>     },<br/>     ...<br/> ];</pre>
<p>To find only users whose language is <kbd>en_US</kbd>, you could perform the following query using <kbd>DataCollection.js</kbd>:</p>
<pre>const collection = new DataCollection(users);<br/>   const english_lang_users = collection.query().filter({locale__language__is: "en_US"}).values();</pre>
<p>Of course, you can accomplish the aforementioned in pure JavaScript easily:</p>
<pre>const english_lang_users = users.filter(user =&gt; user.locale.language === 'en_US');</pre>
<p>However, the pure JavaScript version needs some tedious modifications to be resilient against undefined or null <kbd>locale</kbd> objects, and of course more complicated filters become ever more tedious to write in pure JavaScript. Most of the time, we will use pure JavaScript for the examples in this book, however, our examples will be contrived and much cleaner than real-world use cases; use a tool such as <kbd>DataCollection.js</kbd>, if you feel you need it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have discussed data preprocessing, or the art of delivering the most useful possible data to our machine learning algorithms. We discussed the importance of appropriate feature selection and the relevance of feature selection, both to overfitting and to the curse of dimensionality. We looked at correlation coefficients as a technique to help us determine the appropriate features to select, and also discussed more sophisticated wrapper methods for feature selection, such as using a genetic algorithm to determine the optimal set of features to choose. We then discussed the more advanced topic of feature extraction, which is a category of algorithms that can be used to combine multiple features into new individual features, further reducing the dimensionality of the data.</p>
<p>We then looked at some common scenarios you might face when dealing with real-world datasets, such as missing values, outliers, and measurement noise. We discussed various techniques you can use to correct for those issues. We also discussed common data transformations and normalizations you may need to perform, such as normalizing values to a range or vectorizing objects.</p>
<p>In the next chapter, we will look at machine learning in broad strokes and begin to introduce specific algorithms and their applications.</p>


            </article>

            
        </section>
    </body></html>