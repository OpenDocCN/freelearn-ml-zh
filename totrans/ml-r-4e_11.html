<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer526">
    <h1 class="chapterNumber">11</h1>
    <h1 class="chapterTitle" id="_idParaDest-254">Being Successful with Machine Learning</h1>
    <p class="normal">An all-too-common problem in the field of machine learning occurs when students, with fresh excitement from learning the methods, struggle to apply what they’ve learned to real-world projects. Much as the beauty of a forest trail feels sinister in the darkness of night, code and methods that initially seemed straightforward feel daunting in the absence of a step-by-step roadmap. Without such a guide, the learning curve feels so much steeper and pitfalls appear deeper.</p>
    <p class="normal">It is discouraging to think about the countless students that have been turned away from machine learning, due to the chasm between machine learning in theory and practice. Having worked in the field for over a decade, and having trained, interviewed, hired, and supervised numerous new practitioners, I have seen the challenges of this catch-22 firsthand. It is seemingly a paradox: gaining real-world experience in machine learning seems impossible without first having gained experience in machine learning!</p>
    <p class="normal">The purpose of this chapter, as well as those that follow, is to serve as a bridge between the simple teaching examples in prior chapters and the unyielding complexity of the real world. In this chapter, you will learn:</p>
    <ul>
      <li class="bulletList">The factors that contribute to the success and failure of machine learning models</li>
      <li class="bulletList">Strategies for designing projects that are likely to perform well</li>
      <li class="bulletList">How to perform data exploration to spot potential issues early</li>
      <li class="bulletList">Why data science and competition are relevant to machine learning</li>
    </ul>
    <p class="normal">Whether your definition of success involves finding a job in the field, building better machine learning models, or simply deepening your knowledge of the field’s tools and techniques, you will find something to learn in the coming pages. You may even find yourself with a newfound desire to join many others in online machine learning competitions, which stretch your skills and put your knowledge to the test.</p>
    <h1 class="heading-1" id="_idParaDest-255">What makes a successful machine learning practitioner?</h1>
    <p class="normal">To be clear, the<a id="_idIndexMarker1219"/> challenges of real-world machine learning are not due to the addition of more advanced or complex methods; after all, the first nine chapters of this book covered practical, real-world problems as diverse and challenging as identifying cancer cells, filtering spam messages, and predicting risky bank loans. Instead, the challenges of real-world machine learning have much to do with aspects of the field that are difficult to convey in a scripted setting, like a textbook or lecture. Machine learning<a id="_idIndexMarker1220"/> is as much art as it is science, and just as it would be challenging to learn to paint, dance, or speak a foreign language without real-world practice, it is equally difficult to apply machine learning methods to new, uncharted domains.</p>
    <p class="normal">Like pioneers exploring distant lands, you will encounter never-before-seen challenges requiring soft skills, including persistence and creativity. You will encounter large, messy, and complex datasets requiring in-depth exploration and documentation; graphs and visualizations are the field’s equivalent to the pioneers’ charts and maps. Your analytical and programming skills will be tested, and when you fail, as tends to happen early and often, you will need to iterate and improve upon your mistakes. Creating reproducible experiments using the scientific method will become the breadcrumbs that will prevent you from walking in circles. To become the machine learning equivalent of a rugged individualist involves being both nimble and adaptable, yet also having an insatiable curiosity like a dog with a bone—that is, once it bites down, it doesn’t let go.</p>
    <p class="normal">Although the idea of the rugged individualist is a fantastic metaphor for the solo elements of machine learning, the work is very much also a team sport. Exploring the tundra of Antarctica or climbing the peaks of Mount Everest is a grueling effort, and so are most real-world machine learning projects. It would be unwise or risky to go at such tasks alone—perhaps even dangerous, if the stakes are high. However, even within a team, failure can occur due to poor planning or poor communication. The handoff between the data scientists that develop the models and the data engineers that provide them with the data is particularly treacherous. If a team makes it this far, an even more challenging handoff occurs later when the model must be implemented into business practices and IT systems. For this reason, among many others, the majority of machine learning models are never deployed.</p>
    <p class="normal">If it seems<a id="_idIndexMarker1221"/> like real-world machine learning requires superhuman-like skillsets, this may not be far from the truth if a perusal of recent online job postings is any indication. One very specific job posting asks for experience building recommender systems and designing image recognition tools, as well as familiarity with graph representation learning and natural language processing. Another asks for experience building “performant inference pipelines on very large datasets.” Many want experience with deep neural networks, yet some are more general, requiring a “solid understanding of machine learning fundamentals” and the “ability to analyze a wide variety of data both structured and unstructured.” The diverse sets of skills can be somewhat intimidating to early-career practitioners and lead them to wonder where to begin.</p>
    <p class="normal"><em class="italic">Figure 11.1</em> lists some of the so-called “hard” technical skills commonly used in the field as well as some of the beneficial “soft” traits. By the end of this book, you will have been exposed to most of the tools and skills on the left side of the figure, and by completing the exercises, you will develop the characteristics on the right. Keep in mind that finding a person who has more than a superficial handle on every one of these skills is exceptionally rare. These are the fabled “unicorns” of the field, although even they would likely admit that there is still much to learn. It is always possible to go deeper and learn more about a particular topic. Given limited time and energy, most people must compromise on whether to go broader across many areas or deeper into just a few.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_11_01.png"/></figure>
    <p class="packt_figref">Figure 11.1: Real-world machine learning requires numerous technical skills (left) and soft skills (right)</p>
    <p class="normal">One of the <a id="_idIndexMarker1222"/>best ways to hone machine learning skills, and especially soft skills, is through competition. As depicted in <em class="italic">Figure 11.1</em>, competition improves machine learning results in more ways than one; it is a key component of the individual drive to innovate and improve one’s own performance, yet it also fosters strong teamwork toward meeting a common goal. For these reasons, competition has long been a part of machine learning training. </p>
    <p class="normal">For instance, academic computer scientists have competed<a id="_idIndexMarker1223"/> for over 25 years in an exercise called the <strong class="keyWord">Knowledge Discovery and Data Mining</strong> (<strong class="keyWord">KDD</strong>) Cup (<a href="https://www.kdd.org/kdd-cup"><span class="url">https://www.kdd.org/kdd-cup</span></a>), which chooses winners based on their performance on machine learning tasks that change annually. Similar competitions exist for specialized topics in image, text, and audio data, as well as many others.</p>
    <p class="normal">In one of the earliest widely known examples of machine learning competitions in the for-profit space, in 2006, the Netflix video streaming service began offering a $1M prize to make its movie recommendations 10 percent more accurate. The publicity around this event spurred a wave of additional corporate-sponsored challenges, including those listed on<a id="_idIndexMarker1224"/> Kaggle (<a href="https://www.kaggle.com"><span class="url">https://www.kaggle.com</span></a>), a website that hosts competitions offering cash rewards for advancing the state of the art on tough machine learning tasks across varied domains. Kaggle soon became so popular that it inspired a generation of machine learning practitioners, some of whom used their victories as a springboard for future work in consulting and tech companies. In <em class="chapterRef">Chapter 12</em>, <em class="italic">Advanced Data Preparation</em>, you will learn from the experience of some of these Kaggle champions.</p>
    <p class="normal">Not everybody enjoys head-to-head competition, but you can still compete against yourself or imagine your company in competition against other businesses. Some practitioners are content with challenging themselves to beat their own “high score” on a model performance statistic. Others are motivated by “survival of the fittest” and the thought that a business market rewards companies that outperform others—some of which eventually go extinct. In any case, the goal of competition is not to boost one’s ego, but rather to motivate innovation and continuous quality improvement, ensuring your skills stay up to date.</p>
    <p class="normal">The need to continually learn, and to continually apply your learning, may be the most important characteristics of a machine learning devotee. As described in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introducing Machine Learning</em>, the field evolved in an environment in which the volume and complexity of data grew alongside the computing power and statistical methods necessary to make sense of it. This evolution shows no signs of slowing down. Data, tools, and methods will change, but there will always be a need for people to apply them. Therefore, approach each project as an opportunity to learn something new. The iterative and sometimes addictive process of building better models is an apt place to begin this journey.</p>
    <h1 class="heading-1" id="_idParaDest-256">What makes a successful machine learning model?</h1>
    <p class="normal">Until now, we <a id="_idIndexMarker1225"/>have taken a largely <em class="italic">quantitative</em> perspective of what it means to be a successful machine learning model. Supervised learners were initially said to perform well if the accuracy was high. </p>
    <p class="normal">In <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, we expanded this definition to include other, more sophisticated performance measures, such as the Matthews correlation coefficient and the area under the ROC curve, to account for the fact that accuracy is misleading for unbalanced datasets and to consider performance trade-offs for potential use cases.</p>
    <p class="normal">So far, we have relegated <em class="italic">qualitative</em> measures of model performance to the realm of unsupervised learning, although there are certainly non-quantifiable considerations in the area of predictive modeling as well. Consider, for example, a credit scoring model that is so computationally expensive that it cannot be implemented in a real-time application, or so algorithmically complex that an explanation for its decisions cannot be provided to the applicants. With this in mind, we may favor a simpler, less accurate model, if the alternative is no model at all. After all, even a simple predictive model is usually better than nothing—the word “usually” being a key qualifier, given what we will examine shortly about models failing dramatically in the real world!</p>
    <p class="normal">There may be business costs, resource constraints, or human resource factors not easily incorporated into the model itself that impact the success of a modeling project. To illustrate this fact, imagine that you create a customer churn forecasting algorithm that can identify with a high degree of accuracy the most likely customers to stop purchasing a product. Unfortunately, upon deploying the model, you receive complaints from the sales representatives, who have the role of attempting to retain these customers:</p>
    <ul>
      <li class="bulletList">“I already know these people will churn. You’re telling me nothing new.”</li>
      <li class="bulletList">“That customer stopped buying 2 months ago. They already churned.”</li>
      <li class="bulletList">“We actually want these people to churn, as they are low-value customers.”</li>
      <li class="bulletList">“I’ve already spoken with that customer, and there’s nothing we can do.”</li>
      <li class="bulletList">“Your model’s predictions make no sense. I don’t trust it.”</li>
      <li class="bulletList">“What makes you think this customer will churn? They seem happy to me.”</li>
      <li class="bulletList">“We’ll lose money trying to retain that customer.”</li>
      <li class="bulletList">“The predictions don’t seem to be as good as before. What happened?”</li>
    </ul>
    <p class="normal">These<a id="_idIndexMarker1226"/> types of comments are typical in real-world machine learning, and represent common barriers to a project’s overall success, even for a model that, by conventional, statistical performance metrics, was deemed accurate or effective. The trouble with these barriers is that they are not easily navigated without deep knowledge of the business in which the model will be used. On the other hand, these types of issues tend to follow similar patterns, which can be foreseen with practice. </p>
    <p class="normal">The following table categorizes the problems into four groups, along with typical symptoms and potential workarounds:</p>
    <table class="table-container" id="table001-8">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Pitfall</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Symptoms</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Potential solutions</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Predicting the obvious</p>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">A simpler model (or a well-known rule-of-thumb) performs almost as well</li>
              <li class="bulletList">The model’s performance statistics seem “too good to be true”</li>
              <li class="bulletList">The model performs well on training and test sets but makes no impact when deployed</li>
              <li class="bulletList">The outcomes seem inevitable; having the predictions provides no actionable way to intervene</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Reformulate the problem to be more challenging to the learning algorithm</li>
              <li class="bulletList">Look out for rote memorization, circular logic, or target leakage (having a predictor that is essentially a proxy for the target) </li>
              <li class="bulletList">Recode the target variable, or limit access to certain predictors overly correlated with the target, such that the model finds new connections rather than the obvious ones</li>
              <li class="bulletList">Examine the mid-range of predicted probabilities, or filter out the most obvious or inevitable predictions</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Conducting unfair evaluations</p>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">The model performs much worse in the real world than during testing</li>
              <li class="bulletList">Determining the “best” model used a lot of iteration or tuning</li>
              <li class="bulletList">The correct or incorrect predictions are seemingly predictable; the model may do better or worse than expected on certain segments of data</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Ensure an appropriate evaluation dataset is used</li>
              <li class="bulletList">Use cross-validation correctly and understand its limitations</li>
              <li class="bulletList">Beware of common forms of internally correlated data and understand how to construct fair test sets in these cases</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Being inconsiderate of real-world impacts</p>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">The results are interesting but not very impactful</li>
              <li class="bulletList">An unclear business case to implement the model</li>
              <li class="bulletList">Important subsets of examples are neglected by the model</li>
              <li class="bulletList">Overreliance on simple, quantitative performance measures</li>
              <li class="bulletList">Ignoring predicted probabilities and treating all predictions with equal weight</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Forecast the impact of the project under various plausible scenarios using simulations and experiments</li>
              <li class="bulletList">Put filters on the output that reflect real-world constraints</li>
              <li class="bulletList">Create ROC curves and other cost-aware performance metrics</li>
              <li class="bulletList">Focus on the high-impact outcomes, not the “low-hanging fruit”</li>
            </ul>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Lack of trust</p>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Stakeholders refuse to act upon the data and rely on intuition instead</li>
              <li class="bulletList">A preference to do things the “old” way</li>
              <li class="bulletList">Little interest in working through predictions systematically</li>
              <li class="bulletList">Stakeholders cherry-picking results they agree/disagree with</li>
              <li class="bulletList">Repeatedly asking to justify the predictions</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Identify “champions” that will help promote the project</li>
              <li class="bulletList">Include stakeholders in the modeling process (especially data preparation) and iterate on their feedback</li>
              <li class="bulletList">Craft an “elevator pitch” and “road show” slide deck, to preemptively address FAQs</li>
              <li class="bulletList">Document cases where the model made an impact and tell these stories repeatedly</li>
              <li class="bulletList">Output predictions in an actionable form, such as a “stoplight” approach</li>
              <li class="bulletList">Use model explainability tools</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">It is likely that many pages could be spent describing a career’s worth of experience with each of these<a id="_idIndexMarker1227"/> three categories of pitfalls, but unfortunately, this would be no substitute for learning for oneself the hard way. That being said, there are some broad pointers that may help you steer clear of some of the bumps in the road.</p>
    <h2 class="heading-2" id="_idParaDest-257">Avoiding obvious predictions</h2>
    <p class="normal">When it<a id="_idIndexMarker1228"/> comes to <em class="italic">predicting the obvious</em>, it may not be at all obvious how or why this happens in the first place! The short answer to this question is that is often easier than one might think to accidentally construct a model that “cheats” by finding a way to simplify the problem, or “short-circuit” the problem without doing the deeper work necessary to truly understand the problem. </p>
    <p class="normal">This is especially true for projects that track features and outcomes over time, such as forecasting an event that will happen in the future. These types of projects typically begin<a id="_idIndexMarker1229"/> with <strong class="keyWord">time series data</strong>, which repeatedly measures the same attributes for examples over time.</p>
    <p class="normal">We will consider time series data from a data preparation perspective in <em class="chapterRef">Chapter 12</em>, <em class="italic">Advanced Data Preparation</em>, but for now, it suffices to say that data with a time dimension must be carefully coded in order to avoid using values from the future to predict the past. This problem falls under the broader <a id="_idIndexMarker1230"/>category of <strong class="keyWord">leakage</strong>, or more specifically, <strong class="keyWord">target leakage</strong>, which<a id="_idIndexMarker1231"/> describes a situation in which the learning algorithm knows something about the target to be predicted, which it would not have available in the real-world deployment environment. When there is a clear out-of-sequence issue, such as using current credit scores to predict past loan defaults, target leakage is quite obvious in hindsight. It still occurs surprisingly often, especially when analysts simply dump all potential predictors into a model without considering what they mean. Other times, target leakage is very, very subtle and more difficult to detect, only to be revealed upon deeper analysis when a stakeholder rejects the results as “too good to be true.”</p>
    <p class="normal">One of the<a id="_idIndexMarker1232"/> most subtle forms of leakage occurs when the target variable is defined in such a way that it creates tautological or definitional predictions. The relationship between the target and the predictors need not always be fully deterministic in this case, but merely unduly correlated, or linked together in some unclear way. For example, suppose we use a business definition that defines this month’s churn status using a 3-month rolling average in sales. Using last month and 2 months ago as predictors for the current month’s churn then gets the algorithm two-thirds of the way to the correct answer—customers with low sales in those months are very likely to churn, by definition. This type of mistake is easy to make when using complex survey data, in which the individual survey responses are used as predictors and a score computed from the set of survey responses is used as the target to be predicted. As a general rule, to avoid target leakage, it is best to use a target that is generated by a completely independent process, and is collected at a later time than the data used to make the prediction.</p>
    <div class="packt_tip">
      <p class="normal">Beware of using the future to predict the past! For a model that will be deployed in the real world, this is almost always a clear sign that target leakage is present.</p>
    </div>
    <p class="normal">Leakage can<a id="_idIndexMarker1233"/> also occur when the target variable is linked in a hidden manner to predictors by a business practice. For instance, imagine a scenario in which a manufacturer attempts to boost customer acquisition rates by building a model to predict which customers are most likely to purchase their brand’s car. </p>
    <p class="normal">It seems reasonable to create a predictor for whether the person has opened or clicked on marketing emails, but if the marketing emails were sent only to prior customers, then the model is likely to make the common-sense forecast that loyal customers tend to stay loyal customers, and the sales agents are unlikely to be impressed. Due to the strong connection between the target and the predictor, the model essentially ignores the group of people that have never purchased from the brand before, which is the group most likely to impact the company’s profits! Excluding this predictor from the model, or building the model only based on first-time car buyers, would focus the algorithm on the most impactful predictions, or the ones that are less inevitable.</p>
    <p class="normal">Another factor leading to obvious predictions is related<a id="_idIndexMarker1234"/> to <strong class="keyWord">autocorrelation</strong>, which describes the inertia-like phenomenon in which measurements that are close together in time tend to also be close together in value. Based on this observation, one can conclude that the best predictor of something today is often the value of that same thing yesterday. This<a id="_idIndexMarker1235"/> is almost universally true: today’s energy use, spending, calorie intake, happiness, and virtually anything else one can imagine are all closely linked to the state of these things the day prior. Stated differently, autocorrelation implies that the long-term variation across people or other units of analysis, such as households, businesses, stock values, and so on, tends to be greater than the variation within those same units over small periods of time.</p>
    <p class="normal">Machine learning algorithms can quickly identify instances of autocorrelation and will happily produce a model that uses yesterday’s values to predict today. It will even have high accuracy on the test set, which leads many inexperienced analysts to ignore the underlying issue. Specifically, this is merely an overly complex way to do list-sorting! If a business wanted to predict the customers most likely to spend large amounts tomorrow, they could simply query the database for the top-spending customers as of today and sort the list in a simple spreadsheet application. In fact, this sorting approach has worked quite well for many years under the name <strong class="keyWord">recency, frequency, monetary value</strong> (<strong class="keyWord">RFM</strong>) analysis, which<a id="_idIndexMarker1236"/> was introduced in <em class="chapterRef">Chapter 6</em>, <em class="italic">Forecasting Numeric Data – Regression Methods</em>, in contrast to a machine learning based approach. The RFM approach basically says that the customer who purchases more recently and frequently and spends more money is more likely to continue these trends. Unfortunately, this does little to forecast which new customers are most likely to become top spenders in the future.</p>
    <p class="normal">Forcing the learning algorithm to tackle this more actionable question involves redefining the target variable around the “action.” The target needs to be very specific and indicate the exact circumstances in which the model will make an impact. In the previous example, rather than modeling total spending, it would be more actionable to model the spending increase or decrease over time. </p>
    <p class="normal">This is <a id="_idIndexMarker1237"/>known as the <strong class="keyWord">delta</strong>, and forecasting the sales delta will allow sales agents to intervene before the predicted increase or decrease occurs. Alternatively, it is possible to model the intervention’s impact itself; for example, rather than predicting the customers most likely to churn, it is better to model the customers most likely to respond positively to the churn intervention. Of course, this requires historic data that records the attributes of previous anti-churn interventions. Often, this type of data is lacking in most businesses.</p>
    <h2 class="heading-2" id="_idParaDest-258">Conducting fair evaluations</h2>
    <p class="normal">It is not<a id="_idIndexMarker1238"/> uncommon for machine learning projects that performed well on paper to perform worse in the real world. This is sometimes related to the problem of <em class="italic">conducting unfair evaluations</em>, and whether it is due to honest oversight or intentional deception, this mistake should not exist. Given the results of case studies in previous chapters, we know not to assume that training performance is an unbiased estimate of future performance. Therefore, we’ve always constructed a holdout test set to simulate future unseen data and provide this fair estimate. In <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, we learned that to compare and choose between multiple candidate models, we should use a validation dataset so that the test set can be “kept in a vault” and remain an unbiased estimate of future performance. The underlying issue is that by cherry-picking a model that performs best on the test set, one essentially overfits to the test set, and the performance is inflated, just as it is when overfitting to the training set. Violating the “vault” rule will, unsurprisingly, lead to unexpectedly poor performance models in the real world.</p>
    <p class="normal">Perhaps more surprisingly, just as it is possible to overfit to training and testing, it is also possible to overfit to the validation set. This is especially true when numerous models are built iteratively, or the model is “tuned” extensively to identify the optimal parameter values, as will be discussed in <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>. The problem is that by repeatedly using the same data, some information about the data inevitably “leaks” out to the learning algorithm, and it can eventually become overfitted to the validation set.</p>
    <p class="normal">It may help to visualize the procedures of model building, model selection, and model evaluation as a sequence of steps, as shown in the following figure. During the training step, the algorithm identifies the optimal fit for the data; in doing so, it optimizes internal values, known<a id="_idIndexMarker1239"/> as <strong class="keyWord">parameters</strong>, which are the basis of the model abstraction. In some cases, like regression models, neural networks, and support vector machines, the parameters are easily visible to the end user as coefficients, weights, or support vectors. In other cases, such as k-NN, decision trees, and rule learners, the parameters are more conceptual; think of the parameters as the internal choices the algorithm made to fit the data. </p>
    <p class="normal">In any case, as the model has chosen a single “best” set of parameters to fit the training data, any performance estimate is likely to be optimistic, and it is likely to perform at least slightly more poorly upon generalization to the test set.</p>
    <figure class="mediaobject"><img alt="Diagram, logo, company name  Description automatically generated" src="../Images/B17290_11_02.png"/></figure>
    <p class="packt_figref">Figure 11.2: Because a “best” model is chosen in training and validation, the performance estimates tend to be optimistic</p>
    <p class="normal">The <a id="_idIndexMarker1240"/>validation dataset is used to select between various types of models, test multiple iterations of a single type of model, or both of these simultaneously. This can be understood as the process of identifying the <a id="_idIndexMarker1241"/>optimal <strong class="keyWord">hyperparameters</strong> for the learner, or any other parameters that are set outside the learner and not estimated by the algorithm itself. Having read the previous chapters, you will already be familiar with several hyperparameters, such as the value of <em class="italic">k</em> for the k-NN algorithm, the cost parameter <em class="italic">C</em> and the kernel for the SVM algorithm, and the learning rate and the number of hidden nodes for a neural network, among many others. Defined broadly, the notion of a hyperparameter does not only refer to choices that directly influence a specific algorithm but also the overall choice of algorithm itself, as well as how the algorithm can be combined with others, as you will learn later in <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>. For reasons that will soon become clear, it may be helpful to consider a “hyperparameter” to be any decision that is made outside the learning process and may impact the model’s fit.</p>
    <p class="normal">Now, suppose you have a validation dataset, and you systematically evaluate numerous approaches on this data. You may test a variety of algorithms, like neural networks versus decision trees and SVMs, and then test various iterations of each of these models using different hyperparameter values. Upon choosing the “best” performer out of all of these dozens or hundreds of possibilities, based upon the validation set performance, this model’s performance is likely to regress when applied to the testing dataset given the potential overfitting to the validation set, just as it did between training and validation. We are left wondering whether we truly selected the true best model and how robust the performance will be.</p>
    <p class="normal">The 10-fold CV approach, which was introduced in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, seems at first as if it may solve both problems. Indeed, the practice of computing the average and standard deviation of performance across the 10 folds does provide a measure of the robustness of model performance as the underlying training data changes. This provides a sense of how well a model will generalize to future, unseen data. Consequently, a common practice is to run 10-fold CV repeatedly on the same dataset to compare various hyperparameters and choose the winner. </p>
    <p class="normal">However, as <em class="italic">Figure 11.3</em> illustrates, 10-fold CV in its standard form (left) does not provide a<a id="_idIndexMarker1242"/> validation dataset, and thus can only estimate the generalization error that arises from the model’s internal search for optimal parameters. For example, suppose we compare 10-fold CV performance statistics to help make a decision about whether a neural network performs better than a decision tree, or we use 10-fold CV to determine which of 25 potential values of <em class="italic">C</em> is best for an SVM approach. In both of these cases, notice that we are again cherry-picking the winner, which has the consequence of inflating our performance estimate. Ultimately, if cross-validation is used with extensive hyperparameter turning, we may be overly optimistic about the model’s true future performance.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_11_03.png"/></figure>
    <p class="packt_figref">Figure 11.3: Nested 10-fold CV adds an “inner loop” of 10-fold CV for learning optimal hyperparameters on the validation set</p>
    <p class="normal">It is better to think of cross-validation as not merely estimating the ability of the model to fit the training data (learning the best parameters) but also the entire pipeline of decisions made in the course of choosing the final model (learning the best hyperparameters). Suppose we could write an R function that automates these decisions; it takes several candidate approaches and chooses the best performer among them. </p>
    <p class="normal">For lack of a more formal term, let’s call this an “assessor” function. In this case, we might consider modifying our standard 10-fold CV approach by dividing each of the 10 folds into training and validation sets, rather than training and test sets. Each model would be evaluated by the assessor for its performance on the validation dataset, and the winning model would be chosen by the assessor as the one with the best average performance across the 10 folds. At this point, we are still left trying to figure out how well the performance will generalize to new, unseen datasets in the future.</p>
    <p class="normal">As <a id="_idIndexMarker1243"/>depicted in <em class="italic">Figure 11.3</em>, the purpose <a id="_idIndexMarker1244"/>of <strong class="keyWord">nested cross-validation</strong> (typically, nested 10-fold CV) is to use cross-validation as an inner loop in which parameters are learned on each of the inner folds, and the best hyperparameters are determined by an assessor function at each of the folds of an outer loop (also typically 10-fold CV). Within each of the inner loop folds, any number of models can be evaluated by the assessor function on the validation dataset. Perhaps we are evaluating three different types of models, such as decision trees, k-NN, and SVMs, or we may be evaluating 25 different learning rates for a single neural network. In the end, whether we are assessing 3 or 25 models, only the single best one is nominated by the assessor function to be sent for evaluation in the outer loop, where the 10 best inner loop models’ performance values are averaged on the corresponding fold’s test set. This means that nested cross-validation does not measure the performance of a single model but the entire process of selecting models and learning the best parameters and hyperparameters.</p>
    <p class="normal">As noted in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>, given the complexity of nested cross-validation—both in implementation and interpretation—standard 10-fold CV is often good enough for most real-world applications of machine learning. On one hand, as the amount of information leak occurring in the validation process is relatively minor, the larger the dataset, the smaller the impact will be on the analysis. On the other hand, if the performance difference between two models is small, it is possible that the magnitude of the overfitting is enough to cause the wrong choice to be made. This is especially true as the amount of tuning, iteration, and hyperparameterization is increased.</p>
    <p class="normal">Overall, whether nested cross-validation is necessary or overkill for a fair evaluation depends much on how the results will be used. For an industry benchmark or an academic publication, the more complex nested technique may be justified. For lower-stakes work, it may be wiser to choose the simpler standard 10-fold cross-validation, and use the time saved to consider how the model will be deployed. As will become clear in the next section, it is not uncommon for a model that performs well in theory to fail in the real world. Thus, simpler approaches that can get to failure faster will allow you more time to course-correct.</p>
    <h2 class="heading-2" id="_idParaDest-259">Considering real-world impacts</h2>
    <p class="normal">The<a id="_idIndexMarker1245"/> adrenaline rush of creating a machine learning project that, by all objective measures, appears like it will perform well at its intended task often leads to another common pitfall: <em class="italic">being inconsiderate of real-world impacts</em>. Real-world machine learning projects are generally not exercises performed for fun; they are typically costly, time-consuming endeavors. The stakeholders that commission a machine learning project generally expect a <strong class="keyWord">return on investment</strong> (<strong class="keyWord">ROI</strong>), not<a id="_idIndexMarker1246"/> just for the time and money needed to produce the model but also for the resources that will be required to act upon the result. A model that doesn’t work at all is a one-time sunk cost, but a model that provides poor recommendations or squanders or misdirects the company’s future time and resources is one that throws good money after bad. Escalating commitment to a good idea that simply didn’t work as intended is the root of the <strong class="keyWord">sunk cost fallacy</strong>, in <a id="_idIndexMarker1247"/>which one believes that a failing project can be saved with more investment. A machine learning project used this way is worse than nothing at all.</p>
    <p class="normal">In addition to wasting the resources of the implementation team, it is also possible that a project can cause unexpected harm. This is true even if the machine learning algorithm is doing what is rational in light of its training. To provide one humorous personal example of this possibility, see the following image, which shows dozens of mail pieces I received from the same credit card company over the period of just a few months. Making matters worse, this isn’t even all of them, as I didn’t start collecting them until I realized what was happening! There were even some periods when I received a letter every day. </p>
    <p class="normal">Knowing that banks are generally not inclined to waste money on postage without good reason, my suspicion is that a customer acquisition machine learning model determined that I would be a valuable customer to acquire, even if it meant spending a lot to do so.</p>
    <figure class="mediaobject"><img alt="A picture containing text  Description automatically generated" src="../Images/B17290_11_04.png"/></figure>
    <p class="packt_figref">Figure 11.4: A solution that seems reasonable to an algorithm may have negative real-world impacts, such as “spamming” end users with solicitations</p>
    <p class="normal">Although I <a id="_idIndexMarker1248"/>should have been flattered that an algorithm considered me valuable enough to mail relentlessly, it may have ultimately tarnished my impression of the credit card company. While I cannot be certain that this was not caused by some sort of glitch, while it was happening, I couldn’t help but recall what I had heard during a presentation from Rayid Ghani, the Chief Data Scientist of the 2012 Barack Obama presidential campaign. Specifically, the campaign had run thousands of experiments on their email solicitations and, in doing so, discovered that the more emails they sent, the more money they made. This occurred with effectively no limit. </p>
    <p class="normal">At no point did the number of people unsubscribing from their mailing list outweigh the additional donations they received by being constantly atop of email inboxes. Perhaps a finding like this explains my flood of paper mail from the credit card company; it certainly explains the huge increase in email marketing that consumers now see in their inboxes. Only time will tell what the long-term consequences of this approach will be.</p>
    <div class="note">
      <p class="normal">For more interesting findings discovered during the Obama campaign’s analysis of email behavior data, visit <a href="https://www.wired.com/2013/06/dont-dismiss-email-a-case-study-from-the-obama-campaign/"><span class="url">https://www.wired.com/2013/06/dont-dismiss-email-a-case-study-from-the-obama-campaign/</span></a></p>
    </div>
    <p class="normal">Rather <a id="_idIndexMarker1249"/>than leaving it to chance, the single best way to be considerate of real-world impacts is to design the machine learning project as a close approximation of the ultimate deployment scenario. Simulations, experiments, and <a id="_idIndexMarker1250"/>small <strong class="keyWord">proof-of-concept</strong> (<strong class="keyWord">POC</strong>) trial runs are especially helpful tools to this end. Before the project is even considered for deployment, the machine learning practitioner should be able to answer typical stakeholder questions such as:</p>
    <ul>
      <li class="bulletList">How many dollars, lives, widgets, and so on, can be saved using this model?</li>
      <li class="bulletList">How many “misses” will happen for every successful prediction?</li>
      <li class="bulletList">Is the ROI dependent on high-risk, high-reward events, or does it accumulate smaller, slow-and-steady wins?</li>
      <li class="bulletList">Is the model’s performance markedly better on certain types of examples, or does it perform evenly well across the full set?</li>
      <li class="bulletList">Does the model systematically favor or ignore categories of interest, such as protected age, race, or ethnic groups, geographic regions, or customer segments?</li>
      <li class="bulletList">What are the potential unintended consequences? Can the algorithm cause harm, or be exploited by bad actors to do so?</li>
    </ul>
    <p class="normal">Cleverly designed projects that include a simulated deployment and an estimated ROI calculation will help provide data to answer these questions. In these projects, it is important to not simply calculate accuracy but also take a step further, and compute a number that describes how this accuracy translates into a real-world impact once the model has been deployed.</p>
    <p class="normal">Making a fair <a id="_idIndexMarker1251"/>comparison requires enough business knowledge to construct or identify an appropriate <strong class="keyWord">control group</strong>—the existing status quo or situation that serves as a benchmark or baseline frame of reference. </p>
    <p class="normal">For a cancer identification model, for example, one might estimate the number of lives saved using the model’s predictions, and then compare this to a baseline comprising the lives saved using traditional human diagnosis. In cases where the intervention of a machine learning model has no obvious frame of reference, the baseline may be a scenario using no model at all, one in which the outcome is essentially decided at random, or one decided using a “dumb” model that always picks the majority class or predicts the mean value. You may also use a simple model like the OneR rule learning algorithm discussed in <em class="chapterRef">Chapter 5</em>, <em class="italic">Divide and Conquer – Classification Using Decision Trees and Rules</em>, to simulate a simple rule-of-thumb heuristic.</p>
    <p class="normal">Naturally, the real world is extremely complex and constantly changing, and thus it is important to <a id="_idIndexMarker1252"/>not only estimate the impact of a model but also examine how robust its impact is under various constraints. Some of these constraints may be ethical, such as the need to ensure that it performs evenly well across subgroups of interest. In a business environment, these important subgroups may consist of categories like age, race, ethnicity, gender, and economic status; in medical contexts, these may be based additionally on health characteristics like body mass index and whether the subject smokes. It is up to the analyst to determine what real-world contexts are most important to evaluate for performance. Once these have been decided, the analyst can make predictions on each of the subgroups and compare the model’s performance across the groups, checking for bias reflected in systematically over-performing or underperforming groups.</p>
    <p class="normal">Aside from variation across subgroups, a model’s impact may also vary in the real world due to changes over time in the data used for training or evaluation. In practice, 10-fold CV and even the more sophisticated nested cross-validation variant oversimplify many aspects of how machine learning models are built and deployed in the real world. They do not reflect many potential external factors, beyond mere variations in the training data, which may influence a model’s future performance.</p>
    <p class="normal">To help understand these other factors, <em class="italic">Figure 11.5</em> provides a simplified view of how models are generally built and evaluated in most real-world environments. It imagines a data stream composed of the entities for which the model is to make its predictions; you might imagine this as a single-file line of potential cancer patients, customers, loan applicants, and so on. To forecast their future outcomes, we generally take a snapshot of this data stream today, at a single point in historical time, or record observations for a period of time, and then divide this data into separate sets for training, validation, and testing—possibly using 10-fold CV or similar methods. The performance estimates from models built and evaluated on this snapshot are assumed to be reasonable estimates of future performance, but we cannot be certain until the future eventually happens. </p>
    <p class="normal">Of course, in an ideal scenario, we would simply build our model on today’s data (or past data) and then wait some time for the “future” to occur and conduct the evaluation, but this is very rare in practice. Consider yourself fortunate if your business had the foresight to gather sufficient historical data or has the patience to wait while it is gathered; in many cases, business moves too quickly and resources are too scarce for this to be true.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_11_05.png"/></figure>
    <p class="packt_figref">Figure 11.5: The data used to train and evaluate a model is often from a snapshot in time much earlier than the time of deployment</p>
    <p class="normal">Even <a id="_idIndexMarker1253"/>when a model can be evaluated on truly never-before-seen future data, the relentlessness of time and the data stream often cause problems after deployment that are difficult to foresee ahead of time. Specifically, as the real world is constantly changing, deployed machine learning projects tend to be subject <a id="_idIndexMarker1254"/>to <strong class="keyWord">model decay</strong>, a term that describes the common phenomenon in which their performance deteriorates over time after implementation.</p>
    <p class="normal">Sometimes, this is due to systematic changes in the input data over time, known as <strong class="keyWord">data drift</strong>, which<a id="_idIndexMarker1255"/> can happen due to cyclical patterns like purchasing behavior or disease spread, which vary by season, as well as changes in the meaning or magnitude of the data itself. Data drift occurs after changing the scale on which something was measured, as in switching from a scale from 1 to 5 to 1 to 10, as well as inflating the values overall, which occurs with currency inflation. It can also occur subtly even without changes in the attribute values themselves if, for instance, a survey question’s wording changed. </p>
    <p class="normal">Perhaps the survey used a value of three to indicate “neither agree nor disagree” but was later translated into another language as “no opinion.” This drift in meaning over time may contribute to degraded performance over time, but it can be somewhat mitigated with strict maintenance of codes and definitions—easier said than done!</p>
    <p class="normal">Another contributor to model<a id="_idIndexMarker1256"/> decay is <strong class="keyWord">model drift</strong>, which occurs when the relationship between the target and the predictors changes over time, even if the meaning of the underlying data remains constant. This generally reflects a change external to the model, or an external force that would have been difficult to foresee. For example, there may be broad <a id="_idIndexMarker1257"/>changes to the economy or customer behavior and preferences, evolutionary changes in how a disease behaves, or other such factors that fundamentally disrupt the patterns the learning algorithm discovered during training. Fortunately, model drift can be remedied via more frequent training—the model simply learns the new patterns—but this leads to confusion and further complexity, as it is not clear how frequently or infrequently one should retrain the model.</p>
    <p class="normal">Although one might think that frequent or nearly-real-time training is always best, this substantially increases the complexity of deployment and can lead to increased variability in the model’s predictions over time, and thus can contribute to a lack of trust in the model’s output—a problem described in the next section. Perhaps the best approach is to experiment and identify a schedule to refresh models that works best for the specific use case, such as annually, seasonally, or upon suspicion of data drift. Then, closely monitor the results and refine as needed. As usual, there is no such thing as a free lunch!</p>
    <h2 class="heading-2" id="_idParaDest-260">Building trust in the model</h2>
    <p class="normal">The final <a id="_idIndexMarker1258"/>category of pitfall contributing to failed data science projects has little to do with the technical details of model implementation or the performance of the model itself; instead, it stems from a fundamental <em class="italic">lack of trust</em> in the project from key stakeholders. This pitfall is especially burnout-inducing because it occurs so late in the workflow. It is disheartening to invest countless hours in a project only to see it fail to gain traction with the stakeholders who asked for it, or the end users that would benefit most from its implementation. As frustrating as this sounds, it is even more troubling to hear the raw statistics about the problem’s magnitude; a quick web search reveals a variety of estimates—none of them good—of the proportion of machine learning projects that make it into production. Some firms estimate the number of failed machine learning projects at over 60 percent, while other estimates are as high as a staggering 85 percent.</p>
    <p class="normal">How can it be possible that only around 15 percent of projects are ever launched successfully? Even using the more optimistic estimate, less than half will be implemented! Can this possibly be true? Unfortunately, if this seems to be an impossibly low number, you are unlikely to have worked in the field of machine learning for very long, or are one of the lucky few to work for a company that has found a solution to this epidemic of failed projects. Instead, most practitioners follow a similar pattern in which their efforts continually fall flat in an organization, and thus they look for another workplace where they can make a bigger impact. As most organizations suffer from the same issue, the cycle of discontent inevitably begins again soon.</p>
    <p class="normal">The reasons<a id="_idIndexMarker1259"/> for this failure to launch are myriad, and it is tempting to place the blame solely on the stakeholders who often have unrealistic expectations of the upside of machine learning or the costs and resources needed to follow a project through to completion. Perhaps they bought into the hype surrounding artificial intelligence and expected it to be plug-and-play with minimal investment. Under-resourced information technology teams are, after all, not a recent phenomenon. This being said, there is much that machine learning practitioners can do to proactively build the stakeholders’ trust in the project and make it much more likely to take root. A key part of building trust in modeling projects is recognizing that machine learning is both an art and a science. With experience comes the understanding that while soft skills are not necessarily fundamental to performing the work, they are virtually essential to its ultimate success.</p>
    <p class="normal">There is a lot that machine learning practitioners can learn from the study of magicians or illusionists, particularly with respect to showmanship. Of course, that is not to say in any way that the work itself should be phony—you don’t want to be a snake oil salesman—rather, consider the fact that to the end user, machine learning and artificial intelligence are a black box that might as well be magic. If it works as intended, it undoubtedly stirs a magical feeling. Being heavily invested in building the tool, the practitioner may not even realize this.</p>
    <p class="normal">As noted by David Copperfield, the most commercially successful magician in world history, “<em class="italic">magicians lose the opportunity to experience a sense of wonder</em>.” Savvy practitioners will take advantage of the magical allure of machine learning and identify early-adopter “champions” who will promote the work and help it take root in the organization. These stakeholders will know more about business operations, and including such stakeholders in the process of building the model, particularly during the phases of gathering data and translating the model into action, will provide an end user perspective and help ensure that the project will eventually be useful rather than merely interesting.</p>
    <p class="normal">Most<a id="_idIndexMarker1260"/> successful magicians take their show on the road to audiences all over the world. In machine learning, this is beneficial not only to sell the project to ever-growing audiences of stakeholders but also to gather feedback on what will work in practice and what will not. During this road-show period, it is wise to craft an elevator pitch, or a short two-or three-sentence description of the project that could be given in a brief elevator ride. Honing this pitch by repeatedly practicing it in one-on-one settings with potential end users will not only improve your ability to deliver the pitch but also assist with identifying additional champions of the project and gathering success stories. The successes can later be peppered into future conversations to build even greater buzz around the project. Of course, be sure to use the appropriate level of detail for the audience. Paraphrasing famous sleight-of-hand magician Jerry Andrus, it’s our job to dazzle and “fool” the audience, but not to make them feel like fools.</p>
    <p class="normal">Questions, criticisms, and even outright negativity can be helpful; these can lead to improvements in the project or can be added to an FAQ document or a slide deck, which can be presented to larger audiences. During the first few large audience presentations, incorporating success stories from audience members that are known to be in attendance, or even planting a champion in the audience to ask predefined questions, can contribute to trust in the project, much like a magician often plants an associate in the audience to “volunteer” for the illusion. Over time, you will begin to recognize similar questions and criticisms and preemptively address them or, even better, provide just enough information to allow the audience members to discover the answer for themselves. The trick of leaving some of the project faults in plain sight, and revealing them to the audience seemingly by accident, can be especially effective on known detractors. This is especially powerful because, according to Teller, of the long-running Las Vegas performing duo Penn and Teller, “<em class="italic">when a magician lets you notice something on your own, his lie becomes impenetrable</em>.”</p>
    <p class="normal">Sometimes, rather than glitz and showmanship, the path to building trust is to use statistical tools and methods to provide a more intuitive means of acting upon the model’s predictions. For example, suppose a model has been built to predict whether someone will develop lung cancer. Since lung cancer is relatively rare in the overall population (roughly 1 in 16 Americans will develop it in their lifetime), many models will tend toward low predicted probabilities of cancer, even among those most likely to develop cancer. For a person who is eight times more likely than the average person to develop lung cancer, it is still only a coin flip overall on whether they will develop it in their lifetime, and thus a reasonable prediction for this person may still be “no cancer.” Given this person’s high relative risk, early intervention may be impactful, but a prediction that simply states “no cancer” is of no use to distinguish this person from the others with much lower risk; likewise, a predicted probability of 49.999 percent is of little use without knowledge that the baseline cancer probability is 6.25 percent.</p>
    <p class="normal">As many<a id="_idIndexMarker1261"/> machine learning projects focus on rare events, translating the raw predicted probabilities into categories of relative risk can help drive trust and action. For the lung cancer model described previously, this may mean creating a red, yellow, and green “stoplight” system in which red indicates the highest possible risk, yellow indicates a moderate risk, and green indicates a low risk of developing the disease. These red, yellow, and green labels can then be presented directly in a report or dashboard and understood intuitively by the agents that need to act upon this information.</p>
    <p class="normal">For models intended for other use cases, other formats may be more appropriate to drive action. One business might use a primary school A, B, C, D, or E letter grade system for loan applicants, another might transform raw predicted churn probabilities in to a percentile-based system that ranks customers on their likelihood to churn, and some may even warrant more complex presentations that include not only a prediction but also a confidence rating. Part of the benefit of the road show is that, hopefully, while dazzling audiences with the predictive power of the model, you have also gained a sense of what output format will drive them to use it.</p>
    <p class="normal">Even if end user adoption is high, or perhaps <em class="italic">especially</em> if end user adoption is high, there will be predictions that are either counterintuitive or appear to be nonsensical. Occasionally, end users are merely curious about why a specific prediction was made. These raise questions that can only be answered by a dive under the hood of the model itself, which is virtually impossible for black-box models like neural networks and challenging even for simpler approaches like regression and decision trees, especially when these models have been applied to large and complex datasets. These differences get at the notion <a id="_idIndexMarker1262"/>of <strong class="keyWord">model interpretability</strong>, which is the ability for a human to understand how a model works. If a model is simple and transparent, it will be readily understood, and stakeholders will tend to trust it.</p>
    <p class="normal">Closely related to interpretability, which describes generally <em class="italic">how</em> a model makes predictions, it is also important to understand <em class="italic">why</em> a specific prediction was made. The growing field <a id="_idIndexMarker1263"/>of <strong class="keyWord">model explainability</strong> involves the development of methods that can be used to probe models, to develop a simplified or intuitive understanding of the factors the prediction was based on. Model explainability may be currently one of the fastest-evolving subsections of machine learning and artificial intelligence due to the rapid adoption of deep learning models, which are notoriously impossible to interpret, yet are used in fields like finance and medicine, which have strict requirements for model explainability. Model explainability tools allow powerful but uninterpretable models to be used even for applications where it is crucial to justify the decisions.</p>
    <p class="normal">Model explainability is a<a id="_idIndexMarker1264"/> rapidly advancing field of study, and new <a id="_idIndexMarker1265"/>methods and best practices are being discovered regularly. One promising technique, called <strong class="keyWord">Shapley Additive Explanations</strong> (<strong class="keyWord">SHAP</strong>), uses principles from game theory to allocate credit for a prediction to individual features that are most responsible for the predicted value. This is a more challenging task than it may seem at first because, for complex models, a given feature might not have a simple, linear impact on the outcome. Instead, the feature’s impact may also depend on the value of the other features, which themselves may have different impacts depending on how they are combined. Because it is computationally expensive to compute all possible permutations, most SHAP implementations use heuristics that simplify this computation, and the average impact of each feature is measured across all possibilities.</p>
    <div class="note">
      <p class="normal">SHAP implementations<a id="_idIndexMarker1266"/> for R are available in the <code class="inlineCode">shapr</code> and <code class="inlineCode">shapper</code> packages, but the most active development work is in Python in the <code class="inlineCode">shap</code> package. The documentation for this package is an outstanding resource to learn the fundamentals of SHAP, even if you plan to work in R. It can be found on the web at <a href="https://shap.readthedocs.io"><span class="url">https://shap.readthedocs.io</span></a></p>
    </div>
    <p class="normal">Because machine learning, at its core, is about turning data into action, explainability tools help build trust in a model, which leads to increased adoption and greater impact. For instance, a model that identifies a hospital patient to have a high risk of mortality will cause more harm than good unless we know what factors are causing the risk to be elevated. Without an explanation, a patient will be in fear for no reason. On the other hand, knowing that the risk is due to preventable factors will lead to interventions that save lives. Making these connections between the model and the real world is up to the practitioner. Thus, much in the same way that explainability techniques can lead to more effective models, your own practices and storytelling skills can contribute to the project’s success, as you will see in the sections that follow.</p>
    <h1 class="heading-1" id="_idParaDest-261">Putting the “science” in data science</h1>
    <p class="normal">In the time since the first edition of <em class="italic">Machine Learning with R</em> was published, a new phrase has <a id="_idIndexMarker1267"/>become somewhat ubiquitous within the field of machine learning. That buzzword, of course, is <strong class="keyWord">data science</strong>—a term that has been defined by many but is generally agreed to describe a field of work or study encapsulating aspects of statistics, data preparation and visualization, subject-matter expertise, as well as machine learning.</p>
    <p class="normal">It is debatable whether data science is synonymous with what used to be called data mining, but it is safe to assume that there is a lot of overlap between the two. A reasonable outsider might observe that data science is simply a more formalized version of data mining. The methods and techniques in data mining were often learned informally on the job or passed between practitioners at industry events. This is in stark contrast to the field of data science, which offers countless opportunities to earn formal credentials and experience via online training courses and in-person degree programs.</p>
    <p class="normal">As depicted in <em class="italic">Figure 11.6</em>, a search of Google Trends data suggests that the term truly began to grow in popularity just as the first edition of this book was published. While I would love to take credit for popularizing the phrase, unfortunately, I cannot do so, as it barely appeared in the first edition at all! Of only two appearances of the phrase in the text, the most notable appearance was literally on the book’s final page, where I wrote briefly about the “burgeoning” data science community. If only I had known how true this would be! At least I was in good company; even the Wikipedia page for data science was just in its infancy in 2012 when the earliest pages of this book’s first edition were being written.</p>
    <figure class="mediaobject"><img alt="Chart  Description automatically generated with low confidence" src="../Images/B17290_11_06.png"/></figure>
    <p class="packt_figref">Figure 11.6: A Google Trends search shows the rapid rise of “data science” in the past decade</p>
    <p class="normal">Since then, my <a id="_idIndexMarker1268"/>role as a machine learning practitioner in the workforce, like the roles of many others around the world at the time, was transformed from the title of data analyst in to data scientist. Yet, despite the rapid change in title and perception, it seems that the work itself changed very little. Applied machine learning was essentially just data mining, historically, and today’s data scientists are expected to use statistics and machine learning, as well as a strong hacker or tinkerer’s work ethic, to find useful insights in data—just like the data miners of yore. What made this new field of data science different from what we previously did?</p>
    <p class="normal">Numerous blogs and news publications attempted to answer this question on the road to understanding the hype and why data science suddenly became one of the “hottest new career fields of the 21<sup class="superscript">st</sup> century,” as it was so often called. </p>
    <p class="normal">Early in this trend, a common theme was to use a Venn diagram to illustrate the requisite skills. As shown in the following Bing image search, this depiction of data science was and continues to be pervasive, and the Venn diagram visualization has practically reached meme-like status. There are subtle variations, but most share the same overall structure: data science is found at the intersection of computer science, statistics, and domain expertise.</p>
    <figure class="mediaobject"><img alt="Graphical user interface, chart  Description automatically generated" src="../Images/B17290_11_07.png"/></figure>
    <p class="packt_figref">Figure 11.7: The data science Venn diagram has reached meme-like status; most place data science at the intersection of programming, statistics, and domain expertise</p>
    <p class="normal">The <a id="_idIndexMarker1269"/>problem with this type of reductionist conception is that while it captures the broad strokes of data science, it misses the soul and key distinction: a scientific mindset. One can have the requisite skills of statistics, programming, and domain knowledge, but if the work is treated without scientific rigor, then it is no different than the data mining of years prior. To be clear, this is not to say that data mining as it was performed before was useless even then, but it is certainly doubtful that it was deemed scientific by anybody who knew how the work was performed in the trenches.</p>
    <p class="normal">So, how do we put the “science” into data science? Answering this question involves realizing why the science matters now when it may not have previously. In particular, the operationalization of data science occurred as organizations of all sizes and across many domains began rapidly staffing and resourcing business intelligence teams to find insights in the so-called “treasure trove” of big data. </p>
    <p class="normal">These growing layers of complexity necessitated more sophisticated tools and processes to coordinate efforts and link findings across parts of an organization. When one or two people were doing data mining in the dark recesses of a business, it was not essential to be very scientific, but as the teams and tools grew, a more methodical approach was warranted to avoid having the effort devolve into chaos.</p>
    <p class="normal">Knowing that <a id="_idIndexMarker1270"/>data science at its core is a team sport, it is also important to incorporate elements of the scientific method into solo machine learning projects. Small machine learning projects can quickly grow in complexity, even for relatively simple tasks, via the use of trial and error and iteration, which are essential to the scientific method. <em class="italic">Figure 11.8</em> is intended to illustrate some of the dead ends and tangents that one explores during a rigorous machine learning project. Hypotheses are generated and examined during data exploration, only some of which prove to be fruitful. These insights inform feature engineering, which itself may have multiple false starts. Several models are tested; some fail, while others can be used to springboard more sophisticated models. Ultimately, the most promising models are evaluated, and then tuned for better performance in concert with additional feature engineering before deployment. From start to finish, the entire sequence can take days, weeks, or even months and years for complex, real-world projects. Recognizing the fits and starts of this process as natural steps in the scientific method helps communicate to stakeholders that progress isn’t always a straight line forward, proportional to time invested.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_11_08.png"/></figure>
    <p class="packt_figref">Figure 11.8: Machine learning projects rarely proceed in a straight line from start to finish</p>
    <p class="normal">Likewise, it is important for you, the data science practitioner, to recognize that your work will not proceed in a linear fashion. Unlike machine learning tutorials in books and on the web, the messy complexity of real-world projects requires more careful attention to avoid getting lost in code or reinventing the wheel. No longer is it sufficient to create a single R code file, which is executed line by line by hand. Instead, we assemble our code and output in a single, well-organized place, in a form that we hope will also serve as an artifact of our investigation for future readers or our future, forgetful selves. Thankfully, R and RStudio make this work seamless.</p>
    <h2 class="heading-2" id="_idParaDest-262">Using R Notebooks and R Markdown</h2>
    <p class="normal">Upon completion <a id="_idIndexMarker1271"/>of a large machine learning project, after letting out a sigh of relief, you may find yourself looking back and wondering where<a id="_idIndexMarker1272"/> the time went. Dwelling on this question for too long may lead to insecurity, as inevitably, you may start to ask whether you might have avoided some of the more obvious mistakes, or perhaps made different design choices. “If only!” you may find yourself saying repeatedly. How is it possible that a project seemingly so simple in hindsight consumed so much time and effort?</p>
    <p class="normal">This question stems from a newfound perspective atop the summit of a difficult data analysis project, with a clear view of the outcome. Recall the meandering pathway of a typical machine learning project depicted in <em class="italic">Figure 11.8</em> in the previous section. At the project’s completion, we tend to forget the numerous time-consuming dead ends and false starts and simplify the journey as a straight line from start to finish, rather than recalling the convoluted pathway it actually took.</p>
    <p class="normal">Early in a data science career, people tend to assume that there is a way to avoid these detours and jump straight to the conclusion without “wasting” so much time chasing pointless leads. There isn’t. This work is not in vain but is an essential part of the machine learning process. The work is not wasted at all; as you become smarter about the data, the machine likewise will become smarter.</p>
    <p class="normal">Later in a data science career, you may recognize that these initial exploratory ventures are a necessary step in all projects. However, you may have a suspicion that the work is not as impactful as it could be. While data exploration may inform a single analysis, it doesn’t seem to make a lasting impression, and the same mistakes are often repeated. Part of this may relate to the fact that it is much easier to recall what worked than what didn’t work. Successes stick in one’s mind while failures are forgotten and, in many cases, literally deleted from the R code file. In this way, the exploratory work doesn’t seem to accumulate in the same way that other experiences do. Failures, hypotheses ruled out, and paths not taken are, therefore, not easily remembered and not easily transferred to others to build historical knowledge about a project’s roots. This is to the detriment of your future self, or others who may take over your code upon your retirement. Rather than deleting everything except the final, clean solution, it would be better to have a way to present the full investigation—dead ends, mistakes, and all.</p>
    <p class="normal">The RStudio development environment provides a solution to this problem in the form of <strong class="keyWord">R notebooks</strong>, which are a special type of R code file that combines both R code and explanatory free-form text. These notebooks can easily be compiled into HTML, PDF, or Microsoft Word formats, or even slideshows and books with a bit more effort. The resulting output document embeds code within a report’s text, or text within code, depending on your perspective. </p>
    <p class="normal">This provides an artifact that can be used to document the entire machine learning process from start to finish, yet it doesn’t feel tedious due to the fact that the code can still be run interactively line by line or block by block during development. By <a id="_idIndexMarker1273"/>spending a little extra time to add explanatory or contextual documentation to the R code file, the result is a report that can be shared with others or reviewed by your future self to refresh your memory.</p>
    <p class="normal">R notebooks <a id="_idIndexMarker1274"/>are simply plain text files, much like a standard R code file, but saved with the <code class="inlineCode">.Rmd</code> file extension. These notebooks allow code to be executed interactively within the notebook, and the output will be displayed inline with the surrounding text. In Rstudio, a new file can be created by using the <strong class="screenText">File</strong> menu, selecting <strong class="screenText">New File</strong>, and choosing the <strong class="screenText">R Notebook</strong> option. This will create a new R notebook using the default template, as shown in the following image:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_11_09.png"/></figure>
    <p class="packt_figref">Figure 11.9: An R notebook file open in RStudio allows code and output to be integrated within a report</p>
    <p class="normal">The top of the file between the <code class="inlineCode">---</code> dashes includes metadata about the notebook, such as the title and the intended output format. The “gear” icon to the right of the <strong class="screenText">Preview</strong> button in Rstudio provides settings to switch between the default HTML notebook format and PDF or Microsoft Word document if you do not want to edit this setting manually. These settings govern the output format when the R notebook is compiled upon completion of the project.</p>
    <p class="normal">Directly below the<a id="_idIndexMarker1275"/> header<a id="_idIndexMarker1276"/> metadata, we find a key distinction between an R notebook and traditional R code files. In particular, this section is not R code but<a id="_idIndexMarker1277"/> rather <strong class="keyWord">R Markdown</strong>, which is a simple specification for formatting reports within plain text files. Because R and RStudio were not designed to be word processors, the styles are not controlled via a graphical user interface but rather by simple formatting codes, such as <code class="inlineCode">*italics*</code> and <code class="inlineCode">**bold**</code>, which are translated into <em class="italic">italics</em> and <strong class="keyWord">bold</strong>, respectively, in the final output file.</p>
    <div class="note">
      <p class="normal">The notebook template provides examples of several of the basic formatting options but does not begin to describe the complete set of R Markdown formatting capabilities. Other formats, such as headers, lists, and even embedded mathematical equations, are also possible. Much more information, including a one-page cheat sheet, is available at the R Markdown website: <a href="https://rmarkdown.rstudio.com"><span class="url">https://rmarkdown.rstudio.com</span></a></p>
    </div>
    <p class="normal">Because the R notebook format defaults <a id="_idIndexMarker1278"/>to R Markdown, any R code must be embedded into the file using special indicators, denoting where the code begins and ends. The indicators are three backtick characters followed by the code language, surrounded by curly brackets. For example, a section of R code would begin using the <code class="inlineCode">```{r}</code> statement. The end of this section is denoted by three backtick characters as in a <code class="inlineCode">```</code> statement. Alternatively, these sections can be added to a notebook using the graphical user interface <strong class="screenText">Insert</strong> button, just above the editor window and to the right of the <strong class="screenText">Preview</strong> and “gear” buttons. The <strong class="screenText">Insert</strong> button provides a drop-down selection of the programming languages available to use in the notebook, but keep in mind that these other languages may not be able to take advantage of the objects in the R environment—at least not without some additional steps.</p>
    <p class="normal">Executing a code block, either by clicking the <strong class="screenText">Run</strong> (green triangle) button within the chunk or by pressing your environment’s key combination, displays the command’s output inline with the R Markdown text. Options to manage the output format for each code block can be found using the “gear” icon at the top-right of the block. Here, one can govern whether the code or results are hidden from the final document, and whether or not the code should be executed at all. These features may be useful to suppress extraneous output from the report or prevent long-running code from being run unnecessarily.</p>
    <p class="normal">Clicking<a id="_idIndexMarker1279"/> the <strong class="screenText">Preview</strong> button at the top of the notebook file generates a preview version of the final output report, using whatever R code output has been run interactively. For HTML notebooks, this<a id="_idIndexMarker1280"/> file will open in a simple viewer, as shown in the screenshot that follows, or it can be opened in a web browser. </p>
    <p class="normal">Because the file only uses output as it is generated in real time, the preview file is regenerated automatically by RStudio every time the notebook is saved. Leaving it open in the viewer window will allow you to see approximately how the final report will look at the end of the project.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_11_10.png"/></figure>
    <p class="packt_figref">Figure 11.10: The preview file for an HTML notebook embeds the output within the text documentation</p>
    <p class="normal">The drop-down menu button to the right of the <strong class="screenText">Preview</strong> button provides a means to compile or “knit” the document to its final output format. This runs the complete set of R code blocks from start to finish and uses the <code class="inlineCode">knitr</code> package to bring together the code and text into a <a id="_idIndexMarker1281"/>single report. Knitting to HTML is usually straightforward, but knitting to PDF or Microsoft Word may require installing additional packages. </p>
    <p class="normal">The<a id="_idIndexMarker1282"/> resulting files are inclusive of code, text, and images and without dependences, so they can be easily shared via email. This is true even of the final HTML notebook format, which is saved with the file extension <code class="inlineCode">.nb.html</code> and offers some simple interactivity when viewed in a web browser. This format also embeds the original <code class="inlineCode">.Rmd</code> R Markdown file so that a recipient can open the file and recreate the analysis if needed.</p>
    <h2 class="heading-2" id="_idParaDest-263">Performing advanced data exploration</h2>
    <p class="normal">Falling <a id="_idIndexMarker1283"/>squarely on the “art” side of data science, data exploration is a topic rarely given much coverage in academic textbooks. Tutorials may provide lip service to the practice, showing learners how to create graphs and visualizations, but rarely explaining how these are useful or why they may be necessary. Even this book is guilty of this; although the first few chapters performed simple data exploration, these exploratory analyses very rarely expanded beyond the five-number summary statistics described in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>. Based on the limited coverage of this topic, one might gain the impression that it is not very important in practice, but this couldn’t be further from the truth; in fact, data exploration is a key component of real-world data science, and it is especially important for large, complex, and unfamiliar datasets.</p>
    <p class="normal">Even though we have already performed simple exploratory analyses, we have not yet formally defined what it means to do so. The pioneering mathematician and statistician John W. Tukey, whose 1977 book on the subject brought the term into widespread awareness, noted<a id="_idIndexMarker1284"/> that <strong class="keyWord">exploratory data analysis</strong> (<strong class="keyWord">EDA</strong>) involves allowing a dataset to suggest hypotheses and reveal useful insights rather than simply answer predetermined questions. It is often aided by graphs and charts, which in Tukey’s view, force us “<em class="italic">to notice what we never expected to see</em>.” One might imagine Tukey’s perspective as the notion that presenting the data in clear yet surprising ways, and listening carefully to what these analyses tell us, is not a ritual performed to merely understand the data itself but to better understand how we might ask questions about the data. In short, a rigorous precursory exploratory analysis is likely to lead to a more accurate main analysis.</p>
    <div class="note">
      <p class="normal">Considering his wide contributions to the field, John Tukey might reasonably be considered the grandfather of EDA. You are already familiar with one of his most famous inventions: the box-and-whiskers plot. He also literally wrote the original textbook on EDA techniques, <em class="italic">Exploratory Data Analysis, Tukey, JW, Addison-Wesley; 1977</em>.</p>
    </div>
    <p class="normal">As the <a id="_idIndexMarker1285"/>goal of machine learning is not merely to answer predetermined questions, the form of exploratory data analysis that should be performed in concert with a machine learning project is very much in line with Tukey’s line of thinking. Advanced data exploration, conducted well, allows data to suggest insights that can be exploited to improve the performance of the machine learning task. Given the goal of improving machine learning models, it is best when data exploration is performed systematically and iteratively, but this is no easy task without prior experience. Without direction, one can explore countless dead ends. To counter this, the next few sections provide some ideas on how and where to begin this journey.</p>
    <h3 class="heading-3" id="_idParaDest-264">Constructing a data exploration roadmap</h3>
    <p class="normal">If we are to <a id="_idIndexMarker1286"/>follow Tukey’s conception of data exploration, we are to believe that data exploration is less like an<a id="_idIndexMarker1287"/> interrogation and more like a conversation, or perhaps even a one-sided listening session in which the data shares its nuggets of wisdom. Unfortunately, when this impression is combined with the superficial manner in which exploratory data analysis is depicted in many contexts, many new data explorers are left in a state of so-called “analysis paralysis” and unable to determine where to begin. It is as if the data scientist has been led into a dark room, told to conduct a séance, and wait for the data’s illuminating response. It’s no wonder this is a bit intimidating!</p>
    <p class="normal">No exploratory analysis is exactly like another, and each data scientist should develop the confidence to perform the work in their own way. However, while building your own experience and your own data exploration roadmap, you may find it helpful to learn by example. With that in mind, this section provides advice that may be of assistance in guiding exploratory analyses in general. It is not able to cover the exhaustive set of approaches, nor is it intended to imply a single best approach for data exploration. Again, the best approach is a systematic, iterative, and perhaps even intimate conversation with the dataset, and just as there is no textbook to completely prepare you for a human conversation, there is no single formula to converse with data.</p>
    <p class="normal">That being said, although<a id="_idIndexMarker1288"/> every verbal conversation may be unique, they tend to begin similarly with a greeting and an exchange of names and pleasantries. Likewise, your data exploration roadmap may also begin with you simply becoming familiar with the data. Obtain a data dictionary, or create one in a text file or spreadsheet, which describes each of the features available for use. You may also record additional metadata such as the number of rows, the data source, when and where it was collected, and whether there are any known problems with the data. Such details may prompt questions during the analysis, or they may help enlighten when unexpected results are encountered.</p>
    <p class="normal">You may <a id="_idIndexMarker1289"/>find it fruitful to print a paper copy of the data dictionary and work methodically row by row, exploring each feature one at a time. Although this work can certainly be performed in an electronic document, with large datasets having hundreds of predictors or more, the task somehow feels less daunting when it is performed with pen, paper, and highlighter pens—not to mention the satisfying feeling of making check marks and notes and crossing items off lists! The following figure illustrates the result of one such real-world data exploration process; rows indicate the available features, which have been annotated with stars, highlighting, and notes, indicating the perceived importance of each potential predictor, as well as any potential concerns found during the exploration.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_11_11.png"/></figure>
    <p class="packt_figref">Figure 11.11: When performing data exploration, it can be helpful to print a data dictionary (or list of available attributes) and write notes directly on the paper by hand</p>
    <p class="normal">Working <a id="_idIndexMarker1290"/>systematically down the list of features, you may first scout for any potential pitfalls. An attribute that at first appears to be incredibly useful may ultimately prove to be useless, due to newly discovered flaws or issues. For each variable, you may consider whether any <a id="_idIndexMarker1291"/>of the following potential issues are present:</p>
    <ul>
      <li class="bulletList">Missing or unexpected values</li>
      <li class="bulletList">Outliers or extreme or unusual values</li>
      <li class="bulletList">Numeric features with high skew</li>
      <li class="bulletList">Numeric features with multiple modes</li>
      <li class="bulletList">Numeric features with very high or very low variance</li>
      <li class="bulletList">Categorical features with very many<a id="_idIndexMarker1292"/> levels (known as high <strong class="keyWord">cardinality</strong>)</li>
      <li class="bulletList">Categorical features with levels that have very few observations (known as “sparse” data)</li>
      <li class="bulletList">Features strongly or weakly associated with the target or each other</li>
    </ul>
    <p class="normal">Keep in mind that even if these potential issues are encountered, they do not always indicate a problem. Many of them can be worked around, and in fact, we will explore (or have already explored) solutions for all of them in this book. For now, by focusing only on the exploration and not on the workarounds, it is likely that you are already familiar with analysis methods that may help to identify cases of each of the listed issues.</p>
    <p class="normal">Simple one-way tables or visualizations like histograms can provide a look into individual features and identify problematic values, but more complex visuals may be necessary to investigate the data more deeply. It is important not to merely perform univariate analyses, which consider features in isolation, but also consider each feature’s relationship to the others and the target. This will require bivariate analysis such as cross tables or visualizations, like stacked bar charts, heat maps, and scatterplots. Given that the number of potential bivariate analyses is large for large numbers of predictors, R’s sophisticated visualization capabilities, described later in this chapter, make data exploration less tedious than it otherwise might be.</p>
    <p class="normal">The power of data exploration is not simply to probe the data for anything of negative value but also to identify aspects of positive value. By working one by one down the list, you should ask whether each potential feature could provide any useful information about the outcome. Conversely, you might ask whether the feature is completely useless, or whether it might provide even a tiny bit of assistance toward the model’s goals. This is <a id="_idIndexMarker1293"/>where human intelligence and subject-matter expertise are helpful for having an insightful conversation with the data.</p>
    <p class="normal">As truly <a id="_idIndexMarker1294"/>useless data is extremely rare, you may turn this into a kind of game in which you act as a detective, trying to discover the hidden information encoded in the supposedly “useless” attribute. As the saying goes, “One man’s trash is another man’s treasure.” Data scientists that are very good at turning trash into treasure will have a strong edge over the competition, as they will develop models that use more and better data.</p>
    <div class="packt_tip">
      <p class="normal">Data exploration may differ for so-called “big” datasets, with many millions of rows or features so numerous and confusing that manual exploration is infeasible. Rather than manually exploring these datasets, a typical practice is to write programs to systematically determine which features are useful or, otherwise, reduce the complexity of the data. We will examine some of these methods in later chapters.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-265">Encountering outliers: a real-world pitfall</h3>
    <p class="normal">Just as how the <a id="_idIndexMarker1295"/>process of data exploration, which once seemed quaint, became much more intricate in light of real-world complexity, many of the seemingly simple concepts of data exploration are actually much more nuanced in reality than they may have at first appeared. We will experience this many times firsthand when working through more complex real-world examples throughout the remaining chapters of this book; however, the nature of outliers may be the epitome of this phenomenon.</p>
    <p class="normal">Thus far, we’ve taken our definition of outliers for granted; in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>, we simply said that an outlier is “atypically high or low relative to the majority of data.” We observed such outliers quite easily on a box-and-whiskers plot, denoted by circles that were 1.5 times above or below<a id="_idIndexMarker1296"/> the <strong class="keyWord">interquartile range</strong> (<strong class="keyWord">IQR</strong>) beyond the median. In fact, these are not merely outliers but, specifically, <strong class="keyWord">Tukey outliers</strong>, named<a id="_idIndexMarker1297"/> after—if you haven’t already guessed—John W. Tukey, our previously noted forebearer of exploratory data analysis. This outlier definition is by no means wrong, but it may be slightly narrow. It is likely safe to assume that Tukey himself would agree that his own definition is but one of many ways to conceive of an “outlier.”</p>
    <p class="normal">Let’s consider a slightly broadened definition of the term and <a id="_idIndexMarker1298"/>define an <strong class="keyWord">outlier</strong> as a value that is unusual compared to others in a dataset; it is not necessarily high or low, but simply “unusual.” Although this may seem only slightly different, technically speaking, from the prior definition, the word “unusual” has been precisely chosen to convey a very specific meaning. In particular, the word “unusual” does not imply a particular way to fix the data, whereas terms like “high” and “low” suggest that a data point is wrong in a specific way. You generally cannot easily correct “unusual” to “usual” without first having a firm grasp of what “usual” means. Unusual things are simply odd or curious; we should investigate them further.</p>
    <p class="normal">With this <a id="_idIndexMarker1299"/>mindset, study the following hypothetical dataset comprising images of road signs taken from a simple Bing image search. Which of these are outliers? Most stop signs are red, so it would seem that the yellow (middle) and blue (bottom left) stop signs, as well as the “stop ahead” signs, are clearly outliers, but there are some other oddities too. There’s a stop sign with a hand, some with additional text, and many with slight variations to the sign’s font and border. Moreover, what about stop signs on a plain white background versus a natural landscape? Or, perhaps if you are from another country, literally all of these would be unusual, and therefore, all would be considered outliers. If you are from Hawaii, where the picture of the blue stop sign was apparently taken, then even a blue stop sign may be completely within the ordinary!</p>
    <figure class="mediaobject"><img alt="A picture containing graphical user interface  Description automatically generated" src="../Images/B17290_11_12.png"/></figure>
    <p class="packt_figref">Figure 11.12: Which of the images in this hypothetical stop sign dataset are outliers?</p>
    <p class="normal">The takeaway <a id="_idIndexMarker1300"/>from this exercise, of course, is that an outlier is almost always a matter of perspective, and thus, detecting and fixing outliers becomes much more complicated. On one hand, it does become a bit easier to discern if an outlier is obviously the product of a data error, as in a “mistake” that was made when recording a value. For example, suppose a data entry error recorded someone’s wealth as 1 trillion dollars rather than 1 billion. The extremeness of this value even relative to other wealthy people makes the value easy to detect, and the fact that it is obviously wrong makes for an easy fix: simply input the correct value. On the other hand, outliers that are “real,” such as Elon Musk who, at the time of writing, is worth nearly $200 billion, are much less straightforward to handle. This distinction between “real” and “mistake” outliers is intended to illustrate the idea of whether or not an outlier is explainable. It is often but surely not always best to try to model the explainable outliers; this makes the model more robust. On the other hand, modeling the “mistake” outliers, which are essentially random variations, will usually just add noise and make for a weaker model.</p>
    <p class="normal">The most important question to consider when encountering outliers during data exploration is whether including the outlier in the training data will ultimately improve or detract from the learning algorithm’s ability to perform the desired task. This speaks to the <strong class="keyWord">generalizability</strong> of<a id="_idIndexMarker1301"/> the model, or its ability to perform well on data that it has not seen before. While doing a thorough job of data exploration, keep in mind the deployment scenario and whether the model will need to be robust against similar outliers in the future. For example, if the prior stop sign images were being used to train an autonomous vehicle driving algorithm, then one might remove outliers that are not expected to be encountered on public roadways. Yet, a real-world self-driving vehicle would be expected to encounter signs defaced with graffiti, concealed by darkness, or obscured by plants and weather conditions, so one might also argue that this dataset has <em class="italic">too few</em> outliers!</p>
    <p class="normal">As has been and will continue to be the theme for real-world machine learning, there is no single one-size-fits-all approach to handling this problem. Deleting outliers is likely the most common strategy, and is often taught in introductory statistics courses, but it is perhaps one of the worst. It is certainly easy, but this ease comes with a dark side: deleting outliers may discard very important details about the learning task. The practice precludes the data scientist from engaging in a deeper conversation with the dataset about whether the information is useful or useless.</p>
    <p class="normal">Other approaches require more effort but may be more likely to improve the model’s generalizability. In the case of events that present as outliers due to their rarity, it may be possible to collect more data on these rare events. Alternatively, it may be possible to group outliers into a single, more frequent category through binning or bucketing rare values, or capping values at a maximum level. Ideally, these groups will be based on an intuitive sense of how the learning algorithm will use the data, but in the absence of subject-matter expertise, it is often sufficient to group them into a top decile or create groups of values that have a similar impact on the target variable.</p>
    <p class="normal">Returning<a id="_idIndexMarker1302"/> to the question of what it means to be an outlier, we have already observed that context is key. Something that appears unusual in one context, such as a blue stop sign, may be ordinary in another context. Likewise, something that is completely ordinary in one context may be highly irregular in another. In short, not only can a reasonable value falsely appear to be an outlier but actual outliers can also be hidden in plain sight. Truly grasping this fact is central to rigorous data exploration. For example, consider a dataset with a typical population distribution. We’d expect to see a fair number of elderly women and a fair number of pregnant women, but observing a pregnant elderly woman would be highly irregular! Exploratory data analysis, performed well, helps identify these types of anomalies and ultimately leads to better-performing models.</p>
    <h3 class="heading-3" id="_idParaDest-266">Example – using ggplot2 for visual data exploration</h3>
    <p class="normal">As noted <a id="_idIndexMarker1303"/>previously, data exploration is at its best when aided by graphs and charts, which according to<a id="_idIndexMarker1304"/> John Tukey—himself a pioneer of innovative <a id="_idIndexMarker1305"/>data visualization techniques—help us “to notice what we never expected to see.” We’ve explored a variety of datasets in previous chapters, yet until now, we have only used R’s built-in graphing capabilities to create simple visualizations, like boxplots, histograms, and scatterplots.</p>
    <p class="normal">For a deeper, more thorough job of data exploration, we’ll need to build more complex visuals, and although we could do so using base R, a better option is available. That option comes in the form of the <code class="inlineCode">ggplot2</code> package, which provides a “grammar of graphics” that describes how the elements of a plot relate to each other and the visualization itself. The package has been widely used for over a decade and is highly popular. It can create professional, publication-ready images, and its output can be seen in many academic journals and on many common websites. Even if you didn’t know it at the time, you are likely to have seen its output before.</p>
    <div class="note">
      <p class="normal">Entire books have been dedicated to the <code class="inlineCode">ggplot2</code> package and the “grammar of graphics.” This section covers only the essentials necessary to get started using the package. For many free resources on this topic, visit the website at <a href="https://ggplot2.tidyverse.org"><span class="url">https://ggplot2.tidyverse.org</span></a>, where you can even download a single-page cheat sheet with the most used commands.</p>
    </div>
    <p class="normal">It would <a id="_idIndexMarker1306"/>fill an entire book to demonstrate the capabilities of the <code class="inlineCode">ggplot2</code> package, but the fundamentals can be illustrated with several basic recipes. To this end, we’ll use it to explore a dataset over 100 years in the making. The dataset describes the passengers of the Titanic ship, which sunk in the year 1912. The machine learning application is used to predict which of the 1,309 passengers were tragically killed in the disaster, and although a predictive model is of little use today, the dataset is well suited to practicing data exploration, due to having many hidden patterns, which visualizations can help reveal.</p>
    <div class="note">
      <p class="normal">The Titanic dataset is a widely popular teaching dataset and is available from numerous online sources. The original file and documentation are available via the Vanderbilt University Department of Biostatistics, located on the web at <a href="https://hbiostat.org/data/"><span class="url">https://hbiostat.org/data/</span></a>. This book uses a variant of the Titanic dataset that was created to introduce learners to the Kaggle competition format. To read more or join the competition, visit <a href="https://www.kaggle.com/c/titanic"><span class="url">https://www.kaggle.com/c/titanic</span></a></p>
    </div>
    <p class="normal">We’ll begin<a id="_idIndexMarker1307"/> by loading the Titanic model training dataset<a id="_idIndexMarker1308"/> and examining its features:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train &lt;- read.csv(<span class="hljs-string">"titanic_train.csv"</span>)
&gt; str(titanic_train)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">'data.frame':    891 obs. of  12 variables:
 $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
 $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
 $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
 $ Name       : chr  "Braund, Mr. Owen Harris" ...
 $ Sex        : chr  "male" "female" "female" "female" ...
 $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
 $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
 $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
 $ Ticket     : chr  "A/5 21171" "PC 17599" "STON/O2. 3101282" ...
 $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
 $ Cabin      : chr  "" "C85" "" "C123" ...
 $ Embarked   : chr  "S" "C" "S" "S" ...
</code></pre>
    <p class="normal">The output shows that the dataset includes 12 features for 891 of the Titanic’s 1,309 passengers; the remaining 418 passengers can be found in the <code class="inlineCode">titanic_test.csv</code> file, representing a roughly 70/30 split for training and testing. The binary target feature <code class="inlineCode">Survived</code> indicates whether the passenger survived the shipwreck, with <code class="inlineCode">1</code> indicating survival and <code class="inlineCode">0</code> indicating the less fortunate outcome. Note that in the test set, <code class="inlineCode">Survived</code> is left blank to simulate unseen future data for prediction.</p>
    <p class="normal">In the spirit<a id="_idIndexMarker1309"/> of building a data exploration roadmap, we can start thinking about each feature’s potential value for prediction. The <code class="inlineCode">Pclass</code> column indicates the passenger class, as in first-, second-, or third-class ticket status. This, as well as the <code class="inlineCode">Sex</code> and <code class="inlineCode">Age</code> attributes, seem like potentially useful predictors of survival. We’ll use the <code class="inlineCode">ggplot2</code> package to explore these potential relationships in more depth. If you haven’t already installed this package, do so using the <code class="inlineCode">install.packages("ggplot2")</code> before proceeding.</p>
    <p class="normal">Every ggplot2 visualization is <a id="_idIndexMarker1310"/>composed of layers, which place graphics upon a blank canvas. Executing the <code class="inlineCode">ggplot()</code> function alone creates an empty gray plot area with no data points:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; library(ggplot2)
&gt; p &lt;- ggplot(data = titanic_train)
&gt; p
</code></pre>
    <p class="normal">To <a id="_idIndexMarker1311"/>create something<a id="_idIndexMarker1312"/> more interesting than a blank gray coordinate system, we’ll need to add additional layers to the plot object stored in the <code class="inlineCode">p</code> object. Additional layers are <a id="_idIndexMarker1313"/>specified by a <strong class="keyWord">geom</strong> function, which determines the type of layer to be added. Each of the many <code class="inlineCode">geom</code> functions requires a <code class="inlineCode">mapping</code> parameter, which invokes the package’s aesthetic function, <code class="inlineCode">aes()</code>, to link the dataset’s features to their visual depiction. This series of steps can be somewhat confusing, so the best way to learn is by example.</p>
    <p class="normal">Let’s begin by creating a simple boxplot of the <code class="inlineCode">Age</code> feature. You’ll recall that in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>, we used R’s built-in <code class="inlineCode">boxplot()</code> feature to construct such visualizations as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; boxplot(titanic_train$Age)
</code></pre>
    <p class="normal">To accomplish the same in the ggplot2 environment, we simply add <code class="inlineCode">geom_boxplot()</code> to the blank coordinate system, with the <code class="inlineCode">aes()</code> aesthetic mapping function indicating that we would like the <code class="inlineCode">Age</code> feature to be mapped to the <code class="inlineCode">y</code> coordinate as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_boxplot(mapping = aes(y = Age))
</code></pre>
    <p class="normal">The resulting figures are largely similar, with only a few stylistic differences in how the data is presented. Even the use of Tukey outliers is the same across both plots:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" src="../Images/B17290_11_13.png"/></figure>
    <p class="packt_figref">Figure 11.13: R’s built-in boxplot function (left) compared to the ggplot2 version of the same (right). Both depict the distribution of Titanic passenger ages.</p>
    <p class="normal">Although<a id="_idIndexMarker1314"/> it may seem pointless to use the more complicated <code class="inlineCode">ggplot()</code> visualization when the simpler function will <a id="_idIndexMarker1315"/>suffice, the strength of the framework is its ability to visualize bivariate relationships with only small changes to the code. For <a id="_idIndexMarker1316"/>example, suppose we’d like to examine how age is related to survival status. We can do so using a simple modification of our previous code. Note that the <code class="inlineCode">Age</code> has been mapped to the <code class="inlineCode">x</code> dimension in order to create a horizontal boxplot rather than the vertical boxplot used previously. Supplying a factor-converted <code class="inlineCode">Survived</code> as the <code class="inlineCode">y</code> dimension creates a boxplot for each of the two levels of the factor. Using this plot, it appears that survivors tended to be a bit younger than non-survivors:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_boxplot(aes(x = Age, y = as.factor(Survived)))
</code></pre>
    <figure class="mediaobject"><img alt="Chart, box and whisker chart  Description automatically generated" src="../Images/B17290_11_14.png"/></figure>
    <p class="packt_figref">Figure 11.14: Side-by-side boxplots help compare the age distribution of Titanic’s survivors and non-survivors</p>
    <p class="normal">Sometimes<a id="_idIndexMarker1317"/> a slightly <a id="_idIndexMarker1318"/>different visualization can better tell a story. With this in mind, recall that in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>, we also used R’s <code class="inlineCode">hist()</code> function to examine the <a id="_idIndexMarker1319"/>distribution of numeric features. We’ll begin by replicating this in ggplot to compare the two side by side. The built-in function is quite simple:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; hist(titanic_train$Age)
</code></pre>
    <p class="normal">The <code class="inlineCode">ggplot</code> version uses the <code class="inlineCode">geom_histogram()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_histogram(aes(x = Age))
</code></pre>
    <p class="normal">The resulting figures are largely the same, aside from stylistic differences and the defaults regarding the number of bins:</p>
    <figure class="mediaobject"><img alt="Chart, histogram  Description automatically generated" src="../Images/B17290_11_15.png"/></figure>
    <p class="packt_figref">Figure 11.15: R’s built-in histogram (left) compared to the ggplot2 version of the same (right) examining the distribution of Titanic passenger ages</p>
    <p class="normal">Again, where the <code class="inlineCode">ggplot2</code> framework shines is the ability to make a few small tweaks and reveal<a id="_idIndexMarker1320"/> interesting <a id="_idIndexMarker1321"/>relationships in the data. Here, let’s examine three variants of the same comparison of age and survival.</p>
    <p class="normal">First, we <a id="_idIndexMarker1322"/>can construct overlapping histograms by adding a <code class="inlineCode">fill</code> parameter to the <code class="inlineCode">aes()</code> function. This colorizes the bars according to the levels of the factor provided. We’ll also use the <code class="inlineCode">ggtitle()</code> function to add an informative title to the figure:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_histogram(aes(x = Age, fill = as.factor(Survived))) +
      ggtitle(<span class="hljs-string">"Distribution of Age by Titanic Survival Status"</span>)
</code></pre>
    <p class="normal">Second, rather than having overlapping histograms, we can create a grid of side-by-side plots using the <code class="inlineCode">facet_grid()</code> function. This function takes <code class="inlineCode">rows</code> and <code class="inlineCode">cols</code> parameters to define the cells in the grid. In our case, to create side-by-side plots, we need to define the columns for survivors and non-survivors using the <code class="inlineCode">Survived</code> variable. This must be wrapped by the <code class="inlineCode">vars()</code> function to denote that it’s a feature from the accompanying dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_histogram(aes(x = Age)) +
      facet_grid(cols = vars(Survived)) +
      ggtitle(<span class="hljs-string">"Distribution of Age by Titanic Survival Status"</span>)
</code></pre>
    <p class="normal">Third, rather than using the histogram <code class="inlineCode">geom</code>, we can use <code class="inlineCode">geom_density()</code> to create a density plot. This type of visualization is like a histogram but uses a smoothed curve instead of individual bars to depict the proportion of records at each value of the <code class="inlineCode">x</code> dimension. We’ll set <a id="_idIndexMarker1323"/>the color of the line based on the levels of <code class="inlineCode">Survived</code> and fill the area beneath the curve with the same color. Because the areas overlap, the <code class="inlineCode">alpha</code> parameter allows us to control the level of transparency so that both may be seen at the same time. Note<a id="_idIndexMarker1324"/> that this is a <a id="_idIndexMarker1325"/>parameter of the <code class="inlineCode">geom</code> function and not the <code class="inlineCode">aes()</code> function. The complete command is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_density(aes(x = Age,
                       color = as.factor(Survived),
                       fill = as.factor(Survived)),
                   alpha = <span class="hljs-number">0.25</span>) +
      ggtitle(<span class="hljs-string">"Density of Age by Titanic Survival Status"</span>)
</code></pre>
    <p class="normal">The resulting three figures visualize the same data in different ways:</p>
    <figure class="mediaobject"><img alt="Graphical user interface, histogram  Description automatically generated" src="../Images/B17290_11_16.png"/></figure>
    <p class="packt_figref">Figure 11.16: Small changes in the ggplot() function call can create vastly different outputs</p>
    <p class="normal">These three visualizations demonstrate the fact that different visualizations of the same data can help tell different stories. For example, the top figure with overlapping histograms seems to highlight the fact that a relatively small proportion of people survived. In <a id="_idIndexMarker1326"/>contrast, the bottom figure clearly depicts <a id="_idIndexMarker1327"/>the spike in survival for travelers below 10 years of age; this provides evidence of a “women and children first” policy for the lifeboats—at least with respect to children.</p>
    <p class="normal">Let’s <a id="_idIndexMarker1328"/>examine a few more plots to see if we can uncover more details of the Titanic’s evacuation policies. We’ll begin by confirming the assumed differences in survival by gender. For this, we’ll create a simple bar chart using the <code class="inlineCode">geom_bar()</code> layer. By default, this simply counts the number of occurrences of the supplied dimension. The following command creates a bar chart, illustrating the fact that there were nearly twice as many males as females on board the Titanic:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_bar(aes(x = Sex)) +
    ggtitle(<span class="hljs-string">"Titanic Passenger Counts by Gender"</span>)
</code></pre>
    <figure class="mediaobject"><img alt="Chart, bar chart  Description automatically generated" src="../Images/B17290_11_17.png"/></figure>
    <p class="packt_figref">Figure 11.17: A simple bar chart of a single feature helps to put count data into perspective</p>
    <p class="normal">A more interesting visualization would be to compare the rate of survival by gender. To do this, we must not only supply the <code class="inlineCode">Survived</code> outcome as the <code class="inlineCode">y</code> parameter to the <code class="inlineCode">aes()</code> function but also tell the <code class="inlineCode">geom_bar()</code> function to compute a summary statistic of the data—in particular, using the <code class="inlineCode">mean</code> function—using the <code class="inlineCode">stat</code> and <code class="inlineCode">fun</code> parameters as shown:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_bar(aes(x = Sex, y = Survived),
               stat = <span class="hljs-string">"summary"</span>, fun = <span class="hljs-string">"mean"</span>) +
      ggtitle(<span class="hljs-string">"Titanic Survival Rate by Gender"</span>)
</code></pre>
    <p class="normal">The<a id="_idIndexMarker1329"/> resulting figures <a id="_idIndexMarker1330"/>confirm the<a id="_idIndexMarker1331"/> assumption of a “women and children first” lifeboat policy. Although there were almost twice as many men on board, women were three times more likely to survive:</p>
    <figure class="mediaobject"><img alt="Chart, bar chart  Description automatically generated" src="../Images/B17290_11_18.png"/></figure>
    <p class="packt_figref">Figure 11.18: More complex bar charts can illustrate disparities in survival rates by gender</p>
    <p class="normal">To once again demonstrate ggplot’s ability to create a large variety of visualizations and tell different stories about the data with relatively small changes to the code, we’ll examine the passenger class (<code class="inlineCode">Pclass</code>) feature in a few different ways. First, we’ll create a simple bar chart that depicts the survival rate using the <code class="inlineCode">stat</code> and <code class="inlineCode">fun</code> parameters, just as we did for survival rate by gender:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_bar(aes(x = Pclass, y = Survived),
               stat = <span class="hljs-string">"summary"</span>, fun = <span class="hljs-string">"mean"</span>) +
      ggtitle(<span class="hljs-string">"Titanic Survival Rate by Passenger Class"</span>)
</code></pre>
    <p class="normal">The resulting figure depicts a substantial decline in survival likelihood for second- and third-class passengers:</p>
    <figure class="mediaobject"><img alt="Chart, bar chart  Description automatically generated" src="../Images/B17290_11_19.png"/></figure>
    <p class="packt_figref">Figure 11.19: A bar chart shows a clear disparity in survival outcomes for Titanic’s lower passenger classes</p>
    <p class="normal">Color can be an effective tool to communicate additional dimensions. Using the <code class="inlineCode">fill</code> parameter, we’ll create a simple bar chart of passenger counts with bars that are filled with color according to survival status, which is converted to a factor:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_bar(aes(x = Pclass,
                   fill = factor(Survived,
                                 labels = c(<span class="hljs-string">"No"</span>, <span class="hljs-string">"Yes"</span>)))) +
      labs(fill = <span class="hljs-string">"Survived"</span>) +
      ylab(<span class="hljs-string">"Number of Passengers"</span>) +
      ggtitle(<span class="hljs-string">"Titanic Survival Counts by Passenger Class"</span>)
</code></pre>
    <p class="normal">The result highlights the fact that the overwhelming number of deceased came from the third-class section of the ship:</p>
    <figure class="mediaobject"><img alt="Chart, bar chart  Description automatically generated" src="../Images/B17290_11_20.png"/></figure>
    <p class="packt_figref">Figure 11.20: A bar chart emphasizing the number of third-class passengers that died on the Titanic</p>
    <p class="normal">Next, we’ll modify this plot using a <code class="inlineCode">position</code> parameter that informs <code class="inlineCode">ggplot()</code> how to arrange the colorized bars. In this case, we’ll set <code class="inlineCode">position = "fill"</code>, which creates a stacked bar chart that fills the vertical space—essentially giving each color in the stack a relative proportion out of 100 percent:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_bar(aes(x = Pclass,
                   fill = factor(Survived,
                                 labels = c(<span class="hljs-string">"No"</span>, <span class="hljs-string">"Yes"</span>))),
               position = <span class="hljs-string">"fill"</span>) +
    labs(fill = <span class="hljs-string">"Survived"</span>) +
    ylab(<span class="hljs-string">"Proportion of Passengers"</span>) +
    ggtitle(<span class="hljs-string">"Titanic Survival by Passenger Class"</span>)
</code></pre>
    <p class="normal">The resulting figure emphasizes the decreased odds of survival for the lower classes:</p>
    <figure class="mediaobject"><img alt="Chart, bar chart  Description automatically generated" src="../Images/B17290_11_21.png"/></figure>
    <p class="packt_figref">Figure 11.21: A bar chart contrasting the survival rates by passenger class</p>
    <p class="normal">Lastly, we’ll attempt to visualize the relationship between three dimensions: passenger class, gender, and survival. The <code class="inlineCode">Pclass</code> and Survived features define the <code class="inlineCode">x</code> and <code class="inlineCode">y</code> dimensions, leaving <code class="inlineCode">Sex</code> to define the bar colors via the <code class="inlineCode">fill</code> parameter. Setting the <code class="inlineCode">position = "dodge"</code> tells <code class="inlineCode">ggplot()</code> to place the colored bars side-by-side rather than stacked, while the <code class="inlineCode">stat</code> and <code class="inlineCode">fun</code> parameters compute the survival rate. The full command is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; p + geom_bar(aes(x = Pclass, y = Survived, fill = Sex),
             position = <span class="hljs-string">"dodge"</span>, stat = <span class="hljs-string">"summary"</span>, fun = <span class="hljs-string">"mean"</span>) +
    ylab(<span class="hljs-string">"Survival Proportion"</span>) +
    ggtitle(<span class="hljs-string">"Titanic Survival Rate by Class and Sex"</span>)
</code></pre>
    <p class="normal">This figure reveals the fact that nearly all first- and second-class female passengers survived, whereas men of all classes were more likely to perish:</p>
    <figure class="mediaobject"><img alt="Chart, bar chart  Description automatically generated" src="../Images/B17290_11_22.png"/></figure>
    <p class="packt_figref">Figure 11.22: A bar chart illustrating the low survival rate for males, regardless of passenger class</p>
    <p class="normal">Examining more facets of the Titanic data is an exercise best left to the reader. After all, data exploration may be best thought of as a <a id="_idIndexMarker1332"/>personal conversation between the data and the data scientist. Similarly, as mentioned before, it is beyond the scope of this book to cover every aspect of the <code class="inlineCode">ggplot2</code> package. Still, this section should have <a id="_idIndexMarker1333"/>demonstrated ways in which data visualization can help identify connections <a id="_idIndexMarker1334"/>between features, which is useful for developing a rich understanding of the data. Diving deeper into the capabilities of the <code class="inlineCode">ggplot()</code> function, perhaps by exploring a dataset of personal interest to you, will do much to improve your model building and storytelling skills—both of which are important elements of being successful with machine learning.</p>
    <div class="note">
      <p class="normal">The <em class="italic">R Graphics Cookbook</em> by Winston Chang (<a href="https://r-graphics.org"><span class="url">https://r-graphics.org</span></a>) is available online for free and provides a wealth of recipes, covering virtually every type of <code class="inlineCode">ggplot2</code> visualization.</p>
    </div>
    <h1 class="heading-1" id="_idParaDest-267">Summary</h1>
    <p class="normal">In this chapter, you learned the fundamentals of what it means to be a successful machine learning practitioner and the skills necessary to build successful machine learning models. These require not only a broad set of requisite knowledge and experience but also a thorough understanding of the learning algorithms, the training dataset, the real-world deployment scenario, and the myriad ways that the work can go wrong—either by accident or by design.</p>
    <p class="normal">The data science buzzword suggests a relationship between the data, the machine, and the people who guide the learning process. This is a team effort, and the growing emphasis on data science as a distinct outgrowth from the field of data mining that came before it, with numerous degree programs and online certifications, reflects its operationalization as a field of study concerned with not just statistics, data, and computer algorithms but also the technologic and bureaucratic infrastructure that enables applied machine learning to be successful.</p>
    <p class="normal">Applied machine learning and data science ask their practitioners to be compelling explorers and storytellers. The audacious use of data must be carefully balanced with what truly can be learned from the data, and what may reasonably be done with what is learned. This is certainly both an art and a science, and therefore, few can master the field in its entirety. Instead, striving to constantly improve, iterate, and compete will lead to an improvement of self that inevitably leads to models that better perform at their intended real-world applications. This contributes to the so-called “virtuous cycle” of artificial intelligence, in which a flywheel-like effect rapidly increases the productivity of organizations employing data science methods.</p>
    <p class="normal">Just as this chapter revisited familiar topics and revealed the newfound complexity in the real-world practice of machine learning, the next chapter revisits data preparation to consider solutions to common problems found in large and messy datasets. We’ll work our way back into the trenches by learning about a completely new way to program in R, which is not only more capable of handling such challenges but also, once the initial learning curve has been passed, perhaps even more fun and intuitive to use.</p>
    <h1 class="heading-1" id="_idParaDest-268">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 4000 people at:</p>
    <p class="normal"><a href="https://packt.link/r"><span class="url">https://packt.link/r</span></a></p>
    <p class="normal"><img alt="" src="../Images/r.jpg"/></p>
  </div>
</body></html>