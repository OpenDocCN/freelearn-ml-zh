- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Attributing Authorship and How to Evade It
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The internet has provided the impetus to the fundamental right of freedom of
    expression by providing a public platform for individuals to voice their opinions,
    thoughts, findings, and concerns. Any person can express their views through an
    article, a blog post, or a video and post it online, free of charge in some cases
    (such as on Blogspot, Facebook, or YouTube). However, this has also led to malicious
    actors being able to generate misinformation, slander, libel, and abusive content
    freely. Authorship attribution is a task where we identify the author of a text
    based on the contents. Attributing authorship can help law enforcement authorities
    trace hate speech and threats to the perpetrator, or help social media companies
    detect coordinated attacks and Sybil accounts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, individuals may wish to remain anonymous as authors. They
    may want to protect their identity to avoid scrutiny or public interest. This
    is where authorship obfuscation comes into play. Authorship obfuscation is the
    task of modifying the text so that the author cannot be identified with attribution
    techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Authorship attribution and obfuscation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for authorship attribution
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for authorship obfuscation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have an understanding of authorship attribution,
    the socio-technical aspects behind it, and methods to evade it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%207](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%207).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Authorship attribution and obfuscation
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss exactly what authorship attribution is and
    the incentives for designing attribution systems. While there are some very good
    reasons for doing so, there are some nefarious ones as well; we will therefore
    also discuss the importance of obfuscation to protect against attacks by nefarious
    attackers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What is authorship attribution?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Authorship attribution** is the task of identifying the author of a given
    text. The fundamental idea behind attribution is that different authors have different
    styles of writing that will reflect in the vocabulary, grammar, structure, and
    overall organization of the text. Attribution can be based on heuristic methods
    (such as similarity, common word analysis, or manual expert analysis). Recent
    advances in **machine learning** (**ML**) have also made it possible to build
    classifiers that can learn to detect the author of a given text.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Authorship attribution is not a new problem—the study of this field goes back
    to 1964\. A series of papers known as *The Federalist Papers* had been published,
    which contained over 140 political essays. While the work was jointly authored
    by 3 people, 12 of those essays were claimed by 2 authors. The study by Mosteller
    and Wallace involving Bayesian modeling and statistical analysis using *n*-grams,
    which produced statistically significant differences between the authors, is known
    to be the first actual work in authorship attribution.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作者归属不是一个新问题——这一领域的研究可以追溯到1964年。一系列被称为《联邦党人文集》的论文已经发表，其中包含140多篇政治论文。虽然这项工作是由3个人共同撰写的，但其中12篇论文被2位作者声称。Mosteller和Wallace的研究涉及贝叶斯模型和统计分析，使用*n*-gram产生了作者之间的统计显著差异，这是已知的第一项作者归属的实际工作。
- en: 'Authorship attribution is important for several reasons, as outlined here:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作者归属之所以重要，原因如下：
- en: '**Historical significance**: Scientists and researchers rely on historical
    documents and texts for evidence of certain events. At times, these may have immense
    political and cultural significance, and knowing the author would help place them
    in the proper context and determine their credibility. For example, if an account
    describing certain historical periods and projecting dictators or known malicious
    actors in a positive light were to be found, it would be important to ascertain
    who the author is, as that could change the credibility of the text. Authorship
    attribution would help in determining whether the text could be accepted as an
    authoritative source or not.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**历史意义**：科学家和研究人员依赖历史文件和文本作为某些事件的证据。有时，这些文件可能具有巨大的政治和文化意义，了解作者将有助于将它们置于适当的背景中并确定其可信度。例如，如果发现一个描述某些历史时期并正面描绘独裁者或已知恶意行为者的账户，确定作者身份就很重要，因为这可能会改变文本的可信度。作者归属有助于确定文本是否可以被视为权威来源。'
- en: '**Intellectual property**: As with *The Federalist Papers*, there is often
    contention on who the owner of certain creative or academic works is. This happens
    when multiple people claim ownership over the same book, article, or research
    paper. At other times, one individual may be accused of plagiarizing the work
    of another. In such cases, it is extremely important to trace who the author of
    a particular text is. Authorship attribution can help identify the author, match
    similarity in style and tone, and resolve issues of contended intellectual property.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识产权**：与《联邦党人文集》一样，关于某些创意或学术作品的归属常常存在争议。这发生在多人声称对同一本书、文章或研究论文拥有所有权时。在其他时候，某个人可能被指控剽窃他人的作品。在这种情况下，追踪特定文本的作者至关重要。作者归属可以帮助识别作者，匹配风格和语气上的相似性，并解决知识产权争议。'
- en: '**Criminal investigation**: Criminals often use text as a means of communicating
    with victims and law enforcement. This can be in the form of a ransom note or
    threats. If there is a significant amount of text, it may be possible that it
    reflects some of the stylistic habits of the author. Law enforcement officers
    use authorship attribution methods to determine whether the messages received
    fit the style of any known criminal.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**犯罪调查**：罪犯经常使用文本作为与受害者或执法部门沟通的手段。这可能表现为勒索信或威胁。如果文本量很大，可能反映出作者的一些风格习惯。执法官员使用作者归属方法来确定收到的信息是否符合任何已知罪犯的风格。'
- en: '**Abuse detection**: Sybil accounts are a growing challenge on the internet
    and social media. These are a group of accounts controlled by the same entity
    but masquerading as different people. Sybil accounts have nefarious purposes such
    as multiple Facebook accounts generating fake engagement, or multiple Amazon accounts
    to write fake product reviews. As they are controlled by the same entity, the
    content produced (posts, tweets, reviews) is generally similar. Authorship attribution
    can be used to identify groups of accounts that post content written by the same
    author.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滥用检测**：在互联网和社交媒体上，Sybil账户正成为一个日益严峻的挑战。这些账户是由同一实体控制，但伪装成不同的人。Sybil账户有恶意目的，例如多个Facebook账户生成虚假互动，或多个Amazon账户撰写虚假产品评论。由于它们由同一实体控制，产生的内容（帖子、推文、评论）通常相似。作者归属可以用来识别由同一作者撰写的帖子内容的账户组。'
- en: With the prevalence of the internet and social media platforms, cybercrime has
    been on the rise, and malicious actors are preying on unknowing victims. Authorship
    attribution, therefore, is also a cybersecurity problem. The next section will
    describe authorship obfuscation, a task that counters authorship attribution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随着互联网和社交媒体平台的普及，网络犯罪呈上升趋势，恶意行为者正在捕食不知情的受害者。因此，作者归属分析也是一个网络安全问题。下一节将描述作者身份混淆，这是一个与作者归属分析相反的任务。
- en: What is authorship obfuscation?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是作者身份混淆？
- en: In the previous section, we discussed authorship attribution, which is the task
    of identifying the author of a given text. Authorship obfuscation is a task that
    works exactly toward the opposite.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了作者归属分析，即识别给定文本作者的任务。作者身份混淆是一个与作者归属分析完全相反的任务。
- en: 'Given a text, authorship obfuscation aims to manipulate and modify the text
    in such a way that the end result is this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一段文本，作者身份混淆的目标是通过操纵和修改文本，使其最终结果如下：
- en: The meaning and key points in the text are left intact
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本中的意义和要点保持不变
- en: The style, structure, and vocabulary are suitably modified so that the text
    cannot be attributed to the original author (that is, authorship attribution techniques
    will be evaded)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风格、结构和词汇被适当地修改，使得文本无法归因于原始作者（即，避免作者归属分析技术）
- en: Individuals may use obfuscation techniques to hide their identity. Consider
    the sentence *“We have observed great corruption at the highest levels of government
    in this country.”* If this is re-written as *“Analysis has shown tremendous corrupt
    happenings in the uppermost echelons of this nation’s administration,”* the meaning
    is left intact. However, the style is clearly different and does not bear much
    resemblance to the original author. This is effective obfuscation. An analyst
    examining the text will not be easily able to map it to the same author.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 个人可能会使用混淆技术来隐藏他们的身份。考虑以下句子*“我们在这个国家的最高政府层级观察到了巨大的腐败。”*如果这句话被改写为*“分析显示，这个国家政府最高层发生了巨大的腐败事件，”*意义保持不变。然而，风格明显不同，与原始作者的风格相差甚远。这是一种有效的混淆。分析文本的分析师不太可能轻易地将它映射到同一作者。
- en: Note that both of the objectives in obfuscation (that is, retaining the original
    meaning and stripping off the style markers) are equally important and there is
    a trade-off between them. We can obtain high obfuscation by making extreme changes
    to the text, but at that point, the text may have lost its original meaning and
    intent. On the other hand, we can retain the meaning with extremely minor tweaks—but
    this may not lead to effective obfuscation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，混淆的两个目标（即，保留原始意义和去除风格标记）同样重要，它们之间存在权衡。我们可以通过对文本进行极端修改来获得高程度的混淆，但到那时，文本可能已经失去了其原始意义和意图。另一方面，我们可以通过极其微小的调整来保留意义——但这可能不会导致有效的混淆。
- en: 'Authorship obfuscation has both positive and negative use cases. Malicious
    actors can use obfuscation techniques in order to counter the attribution purposes
    discussed previously and avoid detection. For example, a criminal who wants to
    stay undetected and yet send ransom notes and emails may obfuscate their text
    by choosing a different vocabulary, grammatical structure, and organization. However,
    obfuscation has several important use cases in civil and human rights, as detailed
    here:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者身份混淆既有积极的应用，也有消极的应用。恶意行为者可以使用混淆技术来对抗之前讨论的归属目的，并避免被检测。例如，一个想要保持不被发现，同时发送勒索笔记和电子邮件的罪犯，可以通过选择不同的词汇、语法结构和组织来混淆他们的文本。然而，正如以下详细说明的那样，混淆在公民权和人权方面有几个重要的应用案例：
- en: '**Oppressive governments**: As discussed before, the internet has greatly facilitated
    the human right to freely express oneself. However, some governments may try to
    curtail these rights by targeting individuals who speak up against them. For example,
    an autocratic government may want to prohibit reporting on content that speaks
    against its agenda or expose corruption and malicious schemes. At such times,
    journalists and individuals may want to remain anonymous—their identity being
    detected could lead to them being captured. Obfuscation techniques will alter
    the text they write so that the matter they want to convey will be retained, but
    the writing style will be significantly different than their usual one.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**压迫性政府**：如前所述，互联网极大地促进了人类自由表达的权利。然而，一些政府可能会试图通过针对那些对他们提出批评的个体来限制这些权利。例如，一个专制政府可能希望禁止报道反对其议程的内容或揭露腐败和恶意的计划。在这样的时刻，记者和个人可能希望保持匿名——他们的身份被识别可能会导致他们被捕。混淆技术将改变他们所写的文本，以便他们想要传达的内容得以保留，但写作风格将与其通常的风格有显著不同。'
- en: '**Sensitive issues**: Even if the government is not oppressive by nature, certain
    issues may be sensitive to discuss and controversial. Examples of such issues
    include religion, racial discrimination, reports of sexual violence, homosexuality,
    and reproductive healthcare. Individuals who write about such issues may offend
    the public or certain other groups or sects. Authorship obfuscation allows such
    individuals to publish such content and yet remain anonymous (or, at least, makes
    it harder to discern the author of the text).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**敏感问题**：即使政府本质上不是压迫性的，某些问题也可能敏感且具有争议。这类问题的例子包括宗教、种族歧视、性暴力报告、同性恋和生殖健康保健。撰写关于这类问题的个人可能会冒犯公众或某些其他群体或教派。作者归属混淆允许这样的个人发布此类内容，同时保持匿名（或者至少使辨别文本作者的难度增加）。'
- en: '**Privacy and anonymity**: Many believe that privacy is a fundamental human
    right. Therefore, even if an issue is not sensitive or the government is not corrupt,
    users have the right to protect their identity if they want to. Every individual
    should be free to post what they want and hide their identity. Authorship obfuscation
    allows users to maintain their privacy while expressing themselves.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私和匿名性**：许多人认为隐私是一项基本的人权。因此，即使问题不敏感或政府不腐败，用户也有权在想要的时候保护他们的身份。每个人都应该自由地发布他们想要的内容并隐藏他们的身份。作者归属混淆允许用户在表达自己的同时保持隐私。'
- en: Now that you have a good understanding of authorship attribution and obfuscation
    and why it is actually needed, let us go into implementing it with Python.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经很好地理解了作者归属和混淆及其实际需求，让我们用Python来实现它。
- en: Techniques for authorship attribution
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作者归属技术
- en: The previous section described the importance of authorship attribution and
    obfuscation. This section will focus on the attribution aspect—how we can design
    and build models to pinpoint the author of a given text.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节描述了作者归属和混淆的重要性。本节将重点关注归属方面——我们如何设计和构建模型来确定给定文本的作者。
- en: Dataset
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: There has been prior research in the field of authorship attribution and obfuscation.
    The standard dataset for benchmarking on this task is the *Brennan-Greenstadt
    Corpus*. This dataset was collected through a survey at a university in the United
    States. 12 authors were recruited, and each author was required to submit a pre-written
    text that comprised at least 5,000 words.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在作者归属和混淆领域已有先前的研究。在此任务上进行基准测试的标准数据集是*Brennan-Greenstadt语料库*。这个数据集是通过在美国一所大学进行的调查收集的。招募了12位作者，每位作者都需要提交一篇至少包含5,000个单词的预先撰写的文本。
- en: A modified and improved version of this data—called the *Extended Brennan-Greenstadt
    Corpus*—was released later by the same authors. To generate this dataset, the
    authors conducted a large-scale survey by recruiting participants from Amazon
    **Mechanical Turk** (**MTurk**). MTurk is a platform that allows researchers and
    scientists to conduct human-subjects research. Users sign up for MTurk and fill
    out detailed questionnaires, which makes it easier for researchers to survey the
    segment or demographic (by gender, age, nationality) they want. Participants get
    paid for every **human interaction task** (**HIT**) they complete.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 同一作者后来发布了一个修改和改进的版本的数据——称为**扩展Brennan-Greenstadt语料库**。为了生成这个数据集，作者通过从Amazon
    Mechanical Turk（**MTurk**）招募参与者进行了一次大规模调查。MTurk是一个允许研究人员和科学家进行人类受试者研究的平台。用户注册MTurk并填写详细的问卷，这使得研究人员更容易调查他们想要的细分市场或人口统计特征（按性别、年龄、国籍）。参与者每完成一个**人类交互任务**（**HIT**）就能获得报酬。
- en: To create the extended corpus, MTurk was used so that the submissions would
    be diverse and varied and not limited to university students. Each piece of writing
    was scientific or scholarly (such as an essay, a research paper, or an opinion
    paper). The submission only contained text and no other information (such as references,
    citations, URLs, images, footnotes, endnotes, and section breaks). Quotations
    were to be kept to a minimum as most of the text was supposed to be author generated.
    Each sample had at least 500 words.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建扩展语料库，使用了MTurk，以确保提交的内容多样化且不限于大学生。每篇写作都是科学或学术性的（例如论文、研究报告或观点文章）。提交的内容仅包含文本，不包含其他信息（如参考文献、引用、URL、图片、脚注、尾注和章节分隔）。引用应尽量减少，因为大部分文本应该是作者原创的。每个样本至少有500个单词。
- en: Both the *Brennan-Greenstadt Corpus* and the *Extended Brennan-Greenstadt Corpus*
    are available online to the public for free. For simplicity, we will run our experiments
    with the *Brennan-Greenstadt Corpus* (which contains writing samples from university
    students). However, readers are encouraged to reproduce the results on the extended
    corpus, and tune models as required. The process and code would remain the same—you
    would have to just change the underlying dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**Brennan-Greenstadt语料库**和**扩展Brennan-Greenstadt语料库**均可免费向公众在线提供。为了简化，我们将使用**Brennan-Greenstadt语料库**（其中包含大学生的写作样本）进行实验。然而，鼓励读者在扩展语料库上重现结果，并根据需要调整模型。过程和代码将保持不变——你只需更改底层数据集。'
- en: For convenience, we have provided the dataset we're using ([https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/blob/main/Chapter%207/Chapter_7.ipynb](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/blob/main/Chapter%207/Chapter_7.ipynb)).
    The dataset consists of a root folder that has one subfolder for every author.
    Each subfolder contains writing samples for the author. You will need to unzip
    the data and place it into the folder you want (and change `data_root_dir` in
    the following code accordingly).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们提供了我们使用的数据集（[https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/blob/main/Chapter%207/Chapter_7.ipynb](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/blob/main/Chapter%207/Chapter_7.ipynb)）。该数据集包含一个根文件夹，每个作者都有一个子文件夹。每个子文件夹包含作者的写作样本。您需要解压数据并将其放置在您想要的文件夹中（并相应地更改以下代码中的`data_root_dir`）。
- en: 'Recall that for our experiments, we need to read the dataset such that the
    input (features) is in an array and the labels are in a separate array. The following
    code snippet parses the folder structure and produces data in this format:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，对于我们的实验，我们需要读取数据集，使得输入（特征）在一个数组中，标签在另一个数组中。以下代码片段解析文件夹结构，并生成这种格式的数据：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The dataset also contains some housekeeping files as well as some files that
    indicate the training, test, and validation data. We need a function to filter
    out these so that this information is not read in the data. Here’s what we’ll
    use:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集还包含一些维护文件以及一些指示训练、测试和验证数据的文件。我们需要一个函数来过滤掉这些文件，以便这些信息不会被读取到数据中。以下是我们将使用的方法：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our dataset has been read, and we can now extract features from it. For authorship
    attribution, most features are stylometric and hand-crafted. In the next section,
    we will explore some features that have shown success in prior work.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经读取了数据集，现在可以从中提取特征。对于作者归属，大多数特征是风格测量和手工制作的。在下文中，我们将探讨一些在先前工作中取得成功的特征。
- en: Feature extraction
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取
- en: We will now implement a series of functions, each of which extracts a particular
    feature from our data. Each function will take in the input text as a parameter,
    process it, and return the feature as output.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现一系列函数，每个函数都从我们的数据中提取特定的特征。每个函数都将输入文本作为参数，处理它，并将特征作为输出返回。
- en: 'We begin as usual by importing the required libraries:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像往常一样开始，导入所需的库：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As a first feature, we will use the number of characters in the input:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一个特征，我们将使用输入中的字符数：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we will design a feature that measures the average word length (number
    of characters per word). For this, we first split the text into an array of words
    and clean it up by removing any special characters such as braces, symbols, and
    punctuation. Then, we calculate the number of characters and the number of words
    separately. Their ratio is our desired feature:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将设计一个特征，该特征衡量平均单词长度（每个单词的字符数）。为此，我们首先将文本分割成单词数组，并通过删除任何特殊字符（如大括号、符号和标点符号）来清理它。然后，我们分别计算字符数和单词数。它们的比率是我们希望的特征：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we calculate the frequency of alphabets. We will first create a 26-element
    array where each element counts the number of times that alphabet appears in the
    text. The first element corresponds to A, the next to B, and so on. Note that
    as we are counting alphabets, we need to convert the text to lowercase. However,
    if this were our feature, it would depend heavily on the length of the text. Therefore,
    we normalize this by the total number of characters. Each element of the array,
    therefore, depicts the percentage of that particular alphabet in the text:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算字母的频率。我们首先创建一个26个元素的数组，其中每个元素计算该字母在文本中出现的次数。第一个元素对应于A，下一个对应于B，依此类推。请注意，由于我们在计算字母，我们需要将文本转换为小写。然而，如果这是我们的特征，它将严重依赖于文本的长度。因此，我们通过总字符数进行归一化。因此，数组的每个元素都描述了该特定字母在文本中的百分比：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we will calculate the frequency of common bigrams. Prior research in
    linguistics and phonetics has indicated which bigrams are common in English writing.
    We will first compile a list of such bigrams. Then, we will parse through the
    list and calculate the frequency of each bigram and compute a vector. Finally,
    we normalize this vector, and the result represents our feature:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算常见二元组的频率。语言学和语音学的先前研究已经表明哪些二元组在英语写作中是常见的。我们将首先编制这样一个二元组的列表。然后，我们将遍历列表并计算每个二元组的频率，并计算一个向量。最后，我们归一化这个向量，结果代表我们的特征：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Just as with the common bigrams, we also compute the frequency of common trigrams
    (sequences of three alphabets). The final feature represents a normalized vector,
    similar to what we had for bigrams:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 就像常见的二元组一样，我们也计算常见三元组的频率（由三个字母组成的序列）。最终的特征表示一个归一化向量，类似于我们之前对二元组所做的那样：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The next feature is the percentage of characters that are digits. First, we
    calculate the total number of characters in the text. Then, we parse through the
    text character by character and check whether each character is numeric. We count
    all such occurrences and divide them by the total number we computed earlier—this
    gives us our feature:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个特征是字符中数字的比例。首先，我们计算文本中的总字符数。然后，我们逐个字符解析文本并检查每个字符是否为数字。我们计算所有此类出现的次数，并将其除以之前计算的总数——这给我们提供了我们的特征：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Similarly, the next feature is the percentage of characters that are alphabets.
    We will first need to convert the text to lowercase. Just as with the previous
    feature, we parse character by character, now checking whether each character
    we encounter is in the range `[a-z]`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，下一个特征是字符中字母的比例。我们首先需要将文本转换为小写。就像先前的特征一样，我们逐个字符解析，现在检查我们遇到的每个字符是否在 `[a-z]`
    范围内：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Previously, we calculated the frequency of alphabets. On similar lines, we
    calculate the frequency of each digit from `0` to `9` and normalize it. The normalized
    vector is used as our feature:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们计算了字母的频率。按照类似的思路，我们计算从 `0` 到 `9` 的每个数字的频率并将其归一化。归一化向量被用作我们的特征：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will now calculate the percentage of characters that are uppercase. We follow
    a similar procedure as we did for counting the characters, but now we count for
    capital letters instead. The result is normalized, and the normalized value forms
    our feature:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将计算字符中是大写字母的比例。我们遵循与计数字符相同的程序，但现在我们计数的是大写字母。结果是归一化的，归一化的值形成我们的特征：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we will calculate the frequency of special characters in our text. We
    first compile a list of special characters of interest in a file. We parse the
    file and count the frequency of each character and form a vector. Finally, we
    normalize this vector by the total number of characters. Note that the following
    function uses a static file where the list of characters is stored—you will need
    to change this line of code to reflect the path where the file is stored on your
    system:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将计算文本中特殊字符的频率。我们首先在文件中编译一个感兴趣的特殊字符列表。我们解析文件并计算每个字符的频率并形成一个向量。最后，我们通过总字符数来归一化这个向量。请注意，以下函数使用一个静态文件，其中存储了字符列表——你需要更改这一行代码以反映你的系统上文件存储的路径：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we will count the number of short words in the text. We define a short
    word as one with fewer than or at most three characters. This is a rather heuristic
    definition; there is no globally accepted standard for a word being short. You
    can play around with different values here and see whether it affects the results:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算文本中短单词的数量。我们将短单词定义为少于或最多三个字符的单词。这是一个相当启发式定义；没有全球公认的短单词标准。你可以尝试不同的值来查看它是否会影响结果：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As a very simple feature, we compute the total number of words in the input.
    This involves splitting the text into an array of words (cleaning up special characters)
    and counting the length of the array:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个非常简单的特征，我们计算输入中的总单词数。这涉及到将文本分割成一个单词数组（清理特殊字符）并计算数组的长度：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we calculate the average word length. We simply calculate the length of
    each word in the text and use the mean of all such length values as the feature:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算平均单词长度。我们简单地计算文本中每个单词的长度，并使用所有这些长度值的平均值作为特征：
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We now have all of the functions to compute features in place. Each function
    will take in the text as a parameter and process it to produce the feature we
    designed. Now, we will write a wrapper function to put it all together. This function,
    on being passed the text, will run it through all of our feature extraction functions
    and compute each feature. Each feature will be appended to a vector. This forms
    our final feature vector:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经有了所有计算特征的函数。每个函数都将文本作为参数并对其进行处理，以产生我们设计的特征。现在，我们将编写一个包装函数来将这些功能组合在一起。这个函数在被传递文本时，将运行所有我们的特征提取函数并计算每个特征。每个特征都将附加到一个向量中。这形成了我们的最终特征向量：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, all that is to be done is to apply this function to our dataset:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有要做的事情就是将这个函数应用到我们的数据集上：
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: After this is executed, `X` will be an array containing features that we designed,
    and `Y` will contain the corresponding labels. The hard part is done! Next, we
    will turn to the modeling phase.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作后，`X`将包含我们设计的特征数组，而`Y`将包含相应的标签。困难的部分已经完成！接下来，我们将转向建模阶段。
- en: Training the attributor
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练属性器
- en: In the previous section, we processed our dataset, hand-crafted several features,
    and now have one feature vector per text and the ground-truth label corresponding
    to it. At this point, this is essentially a **supervised learning** (**SL**) problem;
    we have the features and labels and want to learn the association between them.
    We will approach this as we did with all other supervised problems we have seen
    so far.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们处理了我们的数据集，手工制作了几个特征，现在每个文本都有一个特征向量以及相应的真实标签。在这个时候，这本质上是一个**监督学习**（**SL**）问题；我们有特征和标签，并希望学习它们之间的关联。我们将像处理迄今为止看到的所有其他监督问题一样来处理这个问题。
- en: 'To recap, here are the steps we’ll take:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾，以下是我们将采取的步骤：
- en: Split the data into training and testing sets.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分割成训练集和测试集。
- en: Train a supervised classifier on the training set.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上训练一个监督分类器。
- en: Evaluate the performance of the trained model on the testing set.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估训练模型在测试集上的性能。
- en: First, we split the data as follows. Note that we have a mix of authors, therefore
    we have multiple labels. We must ensure that the distribution of labels in the
    training and test sets is roughly similar; otherwise, our model will be biased
    toward specific authors. If a particular author does not appear in the training
    set, the model will not be able to detect them at all.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们按照以下方式分割数据。请注意，我们有多种作者，因此我们有多个标签。我们必须确保训练集和测试集中标签的分布大致相似；否则，我们的模型将偏向于特定作者。如果某个作者没有出现在训练集中，模型将完全无法检测到他们。
- en: Then, we train our classification model (logistic regression, decision tree,
    random forest, **deep neural network** (**DNN**)) on the training set. We use
    this model to make predictions for the data in the test set and compare the predictions
    with the ground truth. As this procedure has been covered in preceding chapters,
    we will not go into detailed explanations here.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在训练集上训练我们的分类模型（逻辑回归、决策树、随机森林、**深度神经网络**（**DNN**））。我们使用这个模型对测试集中的数据进行预测，并将预测与真实值进行比较。由于这个程序在前面的章节中已经介绍过，我们这里不再进行详细解释。
- en: 'A sample code snippet that performs the previous steps with a random forest
    is shown next. Readers should repeat it with other models as well:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例代码片段展示了如何使用随机森林执行前面的步骤。读者应该用其他模型重复它：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you run this, you will notice that the confusion matrix now looks different.
    Whereas previously we had a 2x2 matrix, now we get a 6x6 matrix. This is because
    our dataset now contains six different labels (one for every author). Therefore,
    for every data point with a given class, there are six possible classes to be
    predicted.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个程序时，你会注意到混淆矩阵现在看起来不同了。之前我们有一个2x2的矩阵，现在我们得到一个6x6的矩阵。这是因为我们的数据集现在包含六个不同的标签（每个作者一个）。因此，对于给定类别的每个数据点，有六个可能的类别可以预测。
- en: 'Calculating accuracy is still the same; we need to find the fraction of examples
    that were predicted correctly. Here is a function that does this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 计算准确度仍然是相同的；我们需要找到预测正确的示例的比例。以下是一个执行此操作的函数：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In multi-class problems, the definitions of precision and recall are no longer
    as simple as computing false positives and negatives. Rather, these metrics are
    calculated per class. For example, if there are six labels (1-6), then for class
    2, we say the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在多类问题中，精确度和召回率的定义不再像计算假阳性和假阴性那样简单。相反，这些指标是按类别计算的。例如，如果有六个标签（1-6），那么对于类别2，我们可以说以下内容：
- en: True positives are those where the actual and predicted classes are both 2
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真正的阳性是指实际和预测的类别都是2的情况
- en: False positives are the ones where the predicted class is 2, but the actual
    class is anything other than 2
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阳性是指预测的类别是2，但实际类别不是2的情况
- en: True negatives are those where both the actual and predicted classes are anything
    other than 2
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真负例是指实际和预测的类别都不是2的情况
- en: False negatives are those where the predicted class is anything other than 2,
    but the actual class is 2
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阴性是指预测的类别不是2，但实际类别是2的情况
- en: Using these definitions and the usual expressions for calculating metrics, we
    can calculate per-class metrics. The per-class precision and recall may be averaged
    to compute the overall precision, recall, and F1 scores.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些定义和计算指标的标准表达式，我们可以计算每类的指标。每类的精确度和召回率可以平均计算整体精确度、召回率和F1分数。
- en: 'Fortunately, we do not need to manually implement this per-class metric calculation.
    `scikit-learn` has an inbuilt classification report that will compute and produce
    these metrics for you. This can be used as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不需要手动实现这个按类别的指标计算。`scikit-learn`有一个内置的分类报告，可以为你计算并生成这些指标。这可以按照以下方式使用：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This completes our implementation and analysis of authorship attribution. Next,
    we will suggest some experiments that readers can pursue to explore the topic
    more.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们关于作者归属的实现和分析。接下来，我们将建议一些读者可以追求的实验，以进一步探索这个主题。
- en: Improving authorship attribution
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进作者归属
- en: We have presented vanilla models and techniques for authorship attribution.
    However, there is a large scope for improvement here. As data scientists, we must
    be willing to explore new ideas and techniques and continuously improve our models.
    Here are a few suggestions that readers should try out to see whether they can
    obtain a better performance.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了用于作者归属的vanilla模型和技术。然而，这里有很大的改进空间。作为数据科学家，我们必须愿意探索新的想法和技术，并持续改进我们的模型。以下是一些建议，读者可以尝试看看是否能获得更好的性能。
- en: Additional features
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 额外特征
- en: 'We have used the feature set that is known as the Writeprints set of features.
    This has shown success in prior research. However, this is not an exhaustive list
    of features. Readers can explore more hand-crafted and automatic features to evaluate
    whether performance is improved. Examples of some features are set out here:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了被称为Writeprints特征集的特征集。这在先前的研究中已经显示出成功。然而，这并不是特征的全列表。读者可以探索更多手工制作和自动化的特征来评估性能是否有所提高。以下是一些特征的示例：
- en: Text sentiment
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本情感
- en: Text polarity
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本极性
- en: Number and fraction of function words
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数词的数量和分数
- en: '**Term Frequency – Inverse Document Frequency** (**TF-IDF**) features'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词频-逆文档频率**（**TF-IDF**）特征'
- en: Word embeddings derived from Word2vec
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由Word2vec生成的词嵌入
- en: Contextual word embeddings derived from **Bidirectional Encoder Representations
    from** **Transformers** (**BERT**)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由**双向编码器表示从** **Transformers** （**BERT**）生成的上下文词嵌入
- en: Data configurations
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据配置
- en: 'The experiment we ran was on a subset of six authors from the dataset. In the
    real world, the problem is much more open-ended and there may be several more
    authors. It is worth exploring how the model performance varies as the number
    of authors changes. In particular, readers should explore the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行的实验是在数据集的六个作者子集上进行的。在现实世界中，这个问题要开放得多，可能有更多的作者。值得探索随着作者数量的变化，模型性能如何变化。特别是，读者应该探索以下内容：
- en: What are the performance measures if we choose only 3 authors? What about if
    we choose 12?
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们只选择3位作者，性能指标会是什么？如果我们选择12位呢？
- en: How does the performance change if we model this problem as binary classification?
    Instead of predicting the author, we predict whether a particular text was written
    by a particular author or not. This would involve training a separate classifier
    per author. Does this show better predictive power and practical application than
    the multi-class approach?
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们将这个问题建模为二元分类，性能会如何变化？我们不是预测作者，而是预测特定文本是否由特定作者撰写。这将涉及为每位作者训练一个单独的分类器。这比多类方法显示出更好的预测能力和实际应用吗？
- en: Model improvements
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型改进
- en: 'For brevity and to avoid repetition, we showed only the example of a random
    forest. However, readers should experiment with more models, including but not
    limited to the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁和避免重复，我们只展示了随机森林的例子。然而，读者应该尝试更多模型，包括但不限于以下模型：
- en: '**Support vector** **machines** (**SVMs**)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量** **机**（**SVMs**）'
- en: Naïve-Bayes classifier
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: Logistic regression
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Decision tree
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: DNN
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络（DNN）
- en: The **neural network** (**NN**) algorithms will be particularly useful as the
    number of features increases. When the embeddings and TF-IDF scores are added,
    the features will not be easily interpretable anymore—NNs excel in such situations
    where they can discover high-dimensional features.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征数量增加时，**神经网络**（**NN**）算法将特别有用。当嵌入和TF-IDF分数被添加时，特征将不再容易解释——神经网络擅长在它们能够发现高维特征的情况下。
- en: This completes our discussion of authorship attribution. In the next section,
    we will discuss a problem that is the opposite of the attribution task.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对作者身份归因的讨论。在下一节中，我们将讨论一个与归因任务相反的问题。
- en: Techniques for authorship obfuscation
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作者身份混淆技术
- en: So far, we have seen how authorship can be attributed to the writer and how
    to build models to detect the author. In this section, we will turn to the authorship
    obfuscation problem. Authorship obfuscation, as discussed in the initial section
    of this chapter, is the art of purposefully manipulating the text to strip it
    of any stylistic features that might give away the author.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何将作者身份归因于作者，以及如何构建检测作者的模型。在本节中，我们将转向作者身份混淆问题。正如本章初始部分所讨论的，作者身份混淆是一种故意操纵文本的艺术，以去除可能泄露作者身份的任何风格特征。
- en: The code is inspired by an implementation that is freely available online ([https://github.com/asad1996172/Obfuscation-Systems](https://github.com/asad1996172/Obfuscation-Systems))
    with a few minor tweaks.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 代码灵感来源于一个免费在线可用的实现（[https://github.com/asad1996172/Obfuscation-Systems](https://github.com/asad1996172/Obfuscation-Systems)），并进行了一些小的调整。
- en: First, we will import the required libraries. The most important library here
    is the **Natural Language Toolkit** (**NLTK**) library ([https://www.nltk.org/](https://www.nltk.org/))
    developed by Stanford. This library contains standard off-the-shelf implementations
    for several **natural language processing** (**NLP**) tasks such as tokenization,
    **part-of-speech** (**POS**) tagging, **named entity recognition** (**NER**),
    and so on. It has a powerful set of functionalities that greatly simplify feature
    extraction in text data. You are encouraged to explore the library in detail.
    The **word-sense disambiguation** (**WSD**) implementation ([https://github.com/asad1996172/Obfuscation-Systems/blob/master/Document%20Simplification%20PAN17/WSD_with_UKB.py](https://github.com/asad1996172/Obfuscation-Systems/blob/master/Document%20Simplification%20PAN17/WSD_with_UKB.py))
    can be found online and should be downloaded locally.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入所需的库。在这里最重要的库是斯坦福大学开发的**自然语言工具包**（**NLTK**）库（[https://www.nltk.org/](https://www.nltk.org/））。这个库包含了几种标准现成的**自然语言处理**（**NLP**）任务，如分词、**词性标注**（**POS**）、**命名实体识别**（**NER**）等。它提供了一套强大的功能，极大地简化了文本数据中的特征提取。我们鼓励您详细探索这个库。**词义消歧**（**WSD**）的实现（[https://github.com/asad1996172/Obfuscation-Systems/blob/master/Document%20Simplification%20PAN17/WSD_with_UKB.py](https://github.com/asad1996172/Obfuscation-Systems/blob/master/Document%20Simplification%20PAN17/WSD_with_UKB.py)）可以在网上找到，并应下载到本地。
- en: 'The code to import the libraries is shown here:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库的代码如下所示：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'First, we will implement a function for the expansion and contraction replacement.
    We begin by reading the extraction-contraction list from the `pickle` file (you
    will have to change the path to it accordingly). The result is a dictionary where
    the keys are contractions and values associated are corresponding expansions.
    We parse through the sentence and count the expansions and contractions occurring.
    If there are mostly contractions, we replace them with expansions, and if there
    are mostly expansions, we replace them with contractions. If both are the same,
    we do nothing at all:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将实现一个用于扩展和收缩替换的函数。我们首先从`pickle`文件中读取提取收缩列表（您必须相应地更改路径）。结果是，键是缩写，与之关联的值是相应的扩展。我们通过句子进行解析，并计算出现的扩展和收缩的数量。如果主要是收缩，我们将它们替换为扩展，如果主要是扩展，我们将它们替换为收缩。如果两者相同，我们则什么都不做：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we will remove any parentheses occurring in the text. This means that
    we have to search for characters associated with brackets—`(, ), [, ], {, }`—and
    remove them from the text:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从文本中移除任何括号。这意味着我们必须搜索与括号相关的字符——`(, ), [, ], {, }`——并将它们从文本中移除：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will implement a function to purge discourse markers from the text. We will
    first read a list of discourse markers (you will need to change the filename and
    path, depending on how you have saved it locally). We then iterate through the
    list and remove each item from the text, if found:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个函数来从文本中清除语篇标记。我们首先读取一个语篇标记列表（您需要根据您在本地保存的方式更改文件名和路径）。然后，我们遍历列表，如果找到，则从文本中移除每个项目：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we will implement a function to remove appositions from the text. We
    will use **regular expression** (**regex**) matching for this:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个函数来从文本中移除同位语。我们将使用**正则表达式**（**regex**）匹配来完成这项工作：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will now implement a function to change expressions of possession. We will
    first use regex matching to find expressions of the form “X of Y.” We will then
    replace this with “Y’s X.” For example, “book of Jacob” will become “Jacob’s book.”
    Note that we are not making this replacement deterministically. We will randomly
    choose whether to replace or not (biased with the probability of replacement being
    2/3):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现一个函数来更改所有格的表达式。我们首先使用正则表达式匹配来找到形式为“X of Y”的表达式。然后，我们将这个表达式替换为“Y’s X”。例如，“Jacob的书”将变为“Jacob’s
    book”。请注意，我们不是确定性地进行这种替换。我们将随机选择是否替换（替换的概率为2/3）：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we will apply equation transformation where we will replace mathematical
    expressions with their textual representations. We will define a dictionary where
    common symbols and their text representations are defined (such as “+” translating
    to “plus” and "*" translating to “multiplied by”). Then, we will find occurrences
    of each symbol in the text and make the necessary replacements:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将应用方程转换，其中我们将用它们的文本表示替换数学表达式。我们将定义一个字典，其中定义了常见的符号及其文本表示（例如，“+”翻译为“加”，"*"翻译为“乘以”）。然后，我们将找到文本中每个符号的出现，并进行必要的替换：
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The next step is synonym replacement. However, as a helper function for synonym
    replacement, we need a function for *untokenization*. This is the exact opposite
    of tokenization and can be done with the following code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是同义词替换。然而，作为一个同义词替换的辅助函数，我们需要一个*去分词*函数。这是与分词相反的操作，可以使用以下代码实现：
- en: '[PRE28]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we will implement the actual synonym substitution:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现实际的同义词替换：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we will put all of this together in a wrapper function. This function
    will take in the text and apply all of our transformations (contraction-expansion
    replacement, parenthesis removal, discourse and apposition removal, synonym replacement,
    equation transformation, and possessive transformation) to each sentence of the
    text, and then join the sentences back to form the obfuscated text:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将所有这些整合到一个包装函数中。这个函数将接受文本，并将我们所有的转换（缩写替换、括号删除、话语和同位语删除、同义词替换、方程式转换和所有格转换）应用于文本的每一句话，然后将句子重新组合成混淆文本：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We now will test how effective this obfuscation is. We will train a vanilla
    model and then test it on the obfuscated data. This mirrors exactly the threat
    model that would occur in the real world; at the time of training, we would not
    have access to the obfuscated data. Here is the process we will follow in order
    to evaluate the model:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将测试这种混淆的有效性。我们将训练一个基础模型，然后将其应用于混淆后的数据。这正好反映了现实世界中的威胁模型；在训练时，我们无法访问混淆后的数据。以下是我们将遵循的评估模型的过程：
- en: Split the data into training and test sets.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集。
- en: Extract features from the training data.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据中提取特征。
- en: Train an authorship attribution ML model based on these features.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于这些特征训练一个作者归属的机器学习模型。
- en: Apply the obfuscator on the test data to transform the raw text into obfuscated
    text.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上应用混淆器，将原始文本转换为混淆文本。
- en: Extract features from the obfuscated text and use them to run inference on the
    previously trained model.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从混淆文本中提取特征，并使用它们在先前训练的模型上进行推理。
- en: 'We load the data and split it as before:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载数据，并像以前一样进行分割：
- en: '[PRE31]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we extract features and train a model. Note that we extract features
    only from the training data, not the test data (which we need to obfuscate):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们提取特征并训练一个模型。请注意，我们只从训练数据中提取特征，而不是测试数据（我们需要对其进行混淆）：
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we will obfuscate the test data using the functions we defined earlier,
    and then extract features from the obfuscated version of the data:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用我们之前定义的函数对测试数据进行混淆，然后从数据的混淆版本中提取特征：
- en: '[PRE33]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we can run inference on the trained model using the newly generated
    (obfuscated) data:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用新生成的（混淆的）数据在训练的模型上进行推理：
- en: '[PRE34]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Comparing the two values of accuracy should give you the performance degradation
    caused due to the obfuscation. The first calculated value represents the accuracy
    of the original data, and the second one represents the accuracy of the model
    when our obfuscation tactics are applied. When the second value is lower than
    the first, our obfuscation has been successful.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 比较两个准确率的值应该能给出由于混淆而引起的性能下降。第一个计算值代表原始数据的准确率，第二个值代表应用我们的混淆策略后模型的准确率。当第二个值低于第一个值时，我们的混淆是成功的。
- en: Next, we will provide an overview of some strategies to improve the performance
    of our obfuscators.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将概述一些提高我们混淆器性能的策略。
- en: Improving obfuscation techniques
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高混淆技术
- en: Here, we describe potential changes and improvements that can help us achieve
    a better performance of our obfuscator. Readers are highly encouraged to experiment
    with these to examine which ones show the best performance.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们描述了一些可能的变化和改进，可以帮助我们提高混淆器的性能。强烈鼓励读者尝试这些方法，以检验哪些方法能展现出最佳性能。
- en: Advanced manipulations
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级操作
- en: 'In our example obfuscator, we implemented basic obfuscation tactics such as
    the replacement of synonyms, changing of contractions, removing parentheses, and
    so on. There is a vast arena of features that can be manipulated here. A few possibilities
    are given next:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例混淆器中，我们实现了基本的混淆策略，如同义词替换、改变缩写、删除括号等。这里有大量的特征可以被操作。以下是一些可能性：
- en: '**Antonym replacement**: Replacing words with the negation of their antonyms.
    For example, *good* is replaced by *not bad*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反义词替换**：用反义词的否定来替换单词。例如，*good*被替换为*not bad*。'
- en: '**Function word manipulation**: Adding extra helper words at the beginning
    of sentences, or removing existing words that add no value. For example, *“Thus,
    we have shown that the plan works”* becomes *“We have shown that the* *plan works.”*'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功能词操作**：在句首添加额外的辅助词，或者移除没有价值的现有词。例如，“因此，我们已经证明这个计划是可行的”变为“我们已经证明，这个计划是可行的。”'
- en: '**Punctuation manipulation**: Adding punctuation symbols (two question marks,
    two exclamation marks, trailing periods) or removing existing ones. This may affect
    the grammar and structure of the sentence, which may or may not be acceptable
    depending on your use case.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标点符号操作**：添加标点符号（两个问号、两个感叹号、句尾的点号）或移除现有的标点符号。这可能会影响句子的语法和结构，这取决于你的使用情况，可能或可能不被接受。'
- en: Language models
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言模型
- en: Recent advances such as transformers and the attention mechanism have led to
    the development of several improved language models, which have excellent text-generation
    capabilities. Such models can be used to generate obfuscated text. An example
    is using a transformer-based document summarizer as an obfuscator. The summarizer
    aims to reproduce the text in the original document in a short and concise manner.
    The hope is that in doing so, it will strip off the stylistic features from the
    text. Readers are encouraged to experiment with various summarization models and
    compare the accuracy before and after obfuscation. Note that it is also important
    to check the similarity of the text against the original in terms of meaning.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的发展，如变压器和注意力机制，导致了几个改进的语言模型的发展，这些模型具有出色的文本生成能力。这些模型可以用来生成混淆文本。一个例子是使用基于变压器的文档摘要器作为混淆器。摘要器的目的是以简短和简洁的方式重现原始文档中的文本。希望这样做能够去除文本的风格特征。鼓励读者尝试各种摘要模型，并比较混淆前后的准确性。请注意，检查文本与原文在意义上的相似性也很重要。
- en: This completes our discussion of authorship obfuscation models!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对作者归属混淆模型的讨论！
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter focused on two important problems in security and privacy. We began
    by discussing authorship attribution, a task of identifying who wrote a particular
    piece of text. We designed a series of linguistic and text-based features and
    trained ML models for authorship attribution. Then, we turned to authorship obfuscation,
    a task that aims to evade the attribution models by making changes to the text
    such that author-identifying characteristics and style markers are removed. We
    looked at a series of obfuscation methods for this. For both tasks, we looked
    at the improvements that could be made to the performance.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点讨论了安全和隐私中的两个重要问题。我们首先讨论了作者归属，这是一个识别谁写了特定文本的任务。我们设计了一系列语言和基于文本的特征，并训练了机器学习模型进行作者归属。然后，我们转向作者归属混淆，这是一个通过改变文本以去除作者识别特征和风格标记来规避归属模型的任务。我们研究了这一系列混淆方法。对于这两个任务，我们研究了可以提高性能的改进。
- en: Both authorship attribution and obfuscation have important applications in cybersecurity.
    Attribution can be used to detect Sybil accounts, trace cybercriminals, and protect
    intellectual property rights. Similarly, obfuscation can help preserve the anonymity
    of individuals and provide privacy guarantees. This chapter enables ML practitioners
    in cybersecurity and privacy to effectively tackle these two tasks.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 作者归属和混淆在网络安全中都有重要的应用。归属可以用来检测Sybil账户、追踪网络犯罪分子、保护知识产权。同样，混淆可以帮助保护个人的匿名性并提供隐私保证。本章使网络安全和隐私领域的机器学习从业者能够有效地处理这两个任务。
- en: In the next one, we will change tracks slightly and look at how fake news can
    be detected using graph ML.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将稍微改变方向，探讨如何使用图机器学习检测假新闻。
