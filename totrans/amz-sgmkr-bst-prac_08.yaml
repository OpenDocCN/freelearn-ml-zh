- en: 'Chapter 6: Training and Tuning at Scale'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) practitioners face multiple challenges when training
    and tuning models at scale. **Scale challenges** come in the form of high volumes
    of training data and increased model size and model architecture complexity. Additional
    challenges come from having to run a large number of tuning jobs to identify the
    right set of hyperparameters and keeping track of multiple experiments conducted
    with varying algorithms for a specific ML objective. Scale challenges lead to
    long training times, resource constraints, and increased costs. This can reduce
    the productivity of teams, and potentially create a bottleneck for ML projects.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon SageMaker** provides managed distributed training and tuning capabilities
    to improve training efficiency, and capabilities to organize and track ML experiments
    at scale. SageMaker enables techniques such as streaming data into algorithms
    by using pipe mode for training with data at scale and Managed Spot Training for
    reduced training costs. Pipe mode and managed spot training are discussed in detail
    in *Learn Amazon SageMaker: A guide to building, training, and deploying machine
    learning models for developers and data scientists*, by Julien Simon.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss advanced topics of distributed training, best
    practices for hyperparameter tuning, and how to organize ML experiments at scale.
    By the end of this chapter, you will be able to use Amazon SageMaker's managed
    capabilities to train and tune at scale in a cost-effective manner and keep track
    of a large number of training experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: ML training at scale with SageMaker distributed libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated model tuning with SageMaker hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing and tracking training jobs with SageMaker Experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an **AWS** account to run the examples included in this chapter.
    If you have not set up the data science environment yet, please refer to [*Chapter
    2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Data Science Environments*,
    which walks you through the setup process.
  prefs: []
  type: TYPE_NORMAL
- en: Code examples included in the book are available on GitHub at https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter06\.
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: ML training at scale with SageMaker distributed libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two common scale challenges with ML projects are scaling training data and scaling
    model size. While increased training data volume, model size, and complexity can
    potentially result in a more accurate model, there is a limit to the data volume
    and the model size that you can use with a single compute node, CPU, or GPU. Increased
    training data volumes and model sizes typically result in more computations, and
    therefore training jobs take longer to finish, even when using powerful compute
    instances such as `p3` and `p4` instances.
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed training** is a commonly used technique to speed up training
    when dealing with scale challenges. Training load can be distributed either across
    multiple compute instances (nodes), or across multiple CPUs and GPUs (devices)
    on a single compute instance. There are two strategies for distributed training
    – **data parallelism** and **model parallelism**. Their names are a good indication
    of what is involved with each strategy. With data parallelism, the training data
    is split up across multiple nodes (or devices). With model parallelism, the model
    is split up across the nodes (or devices).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**Mixed-precision training** is a popular technique to handle training at scale
    and reduce training time. Typically used on compute instances equipped with NVIDIA
    GPUs, mixed-precision training converts network weights from FP32 representation
    to FP16, calculates the gradients, converts weights back to FP32, multiplies by
    the learning rate, and finally updates the optimizer weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the data parallelism distribution strategy, the ML algorithm or the neural
    network-based model is replicated on all devices, and each device processes a
    batch of data. Results from all devices are then combined. In the model parallelism
    distribution strategy, the model (which is the neural network) is split up across
    the devices. Batches of training data are sent to all devices so that the data
    can be processed by all parts of the model. The following diagram shows an overview
    of data and model parallelism:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Distribution strategies'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – Distribution strategies
  prefs: []
  type: TYPE_NORMAL
- en: Both data and model parallelism distribution strategies come with their own
    complexities. With data parallelism, each node (or device) is trained on a subset
    of data (called a mini-batch), and a mini-gradient is calculated. However, within
    each node, a mini-gradient average, with gradients coming from other nodes, should
    be calculated and communicated to all other nodes. This step is called **all reduce**,
    which is a communication overhead that grows as the training cluster is scaled
    up.
  prefs: []
  type: TYPE_NORMAL
- en: While model parallelism addresses the requirements of a model not fitting in
    a single device's memory by splitting it across devices, partitioning the model
    across multiple GPUs may lead to under-utilization. This is because training on
    GPUs is sequential in nature, where only one GPU is actively processing data while
    the other GPUs are waiting to be activated. To be effective, model parallelism
    should be coupled with a pipeline execution schedule to train the model across
    multiple nodes, and in turn, maximize GPU utilization. Now that you know two different
    distribution strategies, how do you choose between data and model parallelism?
  prefs: []
  type: TYPE_NORMAL
- en: Choosing between data and model parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When choosing a distributed strategy to implement, keep in mind the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Training on multiple nodes inherently causes inter-node communication overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, to meet security and regulatory requirements, you may choose to
    protect the data transmitted between the nodes by enabling inter-container encryption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling inter-container encryption will further increase the training time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to these reasons, use data parallelism if the trained model can fit in the
    memory of a single device or node. In situations where the model does not fit
    in the memory due to its size or complexity, you should experiment further with
    data parallelism before deciding on model parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can experiment with the following to improve data parallelism performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tuning the model''s hyperparameters**: Tuning parameters such as the number
    of layers of a neural network, or the optimizer to use, affects the model''s size
    considerably.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducing the batch size**: Experiment by incrementally reducing the batch
    size until the model fits in the memory. This experiment should balance out the
    model''s memory needs with optimal batch size. Make sure you do not end up with
    a suboptimal small batch size just because training with a large batch size takes
    up most of the device memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducing the model input size**: If the model input is tabular, consider
    embedding vectors of reduced dimensions. Similarly, for **natural language processing**
    (**NLP**) models, reduce the input NLP sequence length, and if the input is an
    image, reduce image resolution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using mixed-point precision**: Experiment with mixed-precision training,
    which uses FP16 representation of weights during gradient calculation, to reduce
    memory consumption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following flowchart shows the sequence of decisions and experiments to
    follow when choosing a distribution strategy to implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Choose a distribution strategy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – Choose a distribution strategy
  prefs: []
  type: TYPE_NORMAL
- en: 'While data parallelism addresses the challenge of training data scale, model
    parallelism addresses the challenge of increased model size and complexity. A
    hybrid distribution strategy can also be implemented to include both data and
    model parallelism. *Figure 6.3* walks you through a hybrid distribution strategy
    with two-way data parallelism and four-way model parallelism:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Hybrid distribution strategy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – Hybrid distribution strategy
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the compute resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both the distributed training strategies depend on a cluster of compute resources
    to spread the training load. When scaling the distributed cluster to meet the
    training demands, the recommended best practices are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, scale vertically. That is, scale from a single GPU to multiple GPUs on
    a single instance. For example, let's say you started with the instance type `p3.2xlarge`,
    which has a single GPU for training your model, and you find yourself needing
    a greater number of GPUs to increase the training time. Change the instance type
    to `p3.16xlarge`, which has eight GPUs. This will result in a nearly eight-times
    decrease in the training, a near-linear speedup. Keeping the training job on a
    single scaled-up instance results in better performance than using multiple instances
    while keeping the cost low.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, scale from a single instance to multiple instances. When you reach limits
    of the instance types offered and still need to scale your training even further,
    then use multiple instances of the same type, that is, scale from a single `p3.16xlarge`
    to two `p3.16xlarge` instances. This will give you double the compute capacity,
    going from 8 GPUs on a single instance, to 16 GPUs across two instances. Keep
    in mind that when you use multiple instances in the training cluster, all instances
    should be in the same `us-west-2` must all be in `us-west-2a` or all in `us-west-2b`.
    Your training data should also be in the same region, `us-west-2`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When moving from a single instance to multiple instances, it is recommended
    that you observe the model convergence and increase the batch size as necessary.
    Since the batch size you use is split across GPUs, each GPU is processing a lower
    batch size, which could lead to a high error rate and disrupt the model convergence.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's say you start with a single GPU on a `p3.2xlarge` instance
    using a batch size of 64, then scale up to four `p3dn.24xlarge`, which gives you
    32 GPUs. After this move, each GPU only processes a batch size of two, which is
    very likely to break the model convergence you observed with the original training.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker distributed libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For easy implementation of data and model parallelism in your training jobs,
    SageMaker provides two different distributed training libraries. The libraries
    address the issues of inter-node and inter-GPU communications overhead using a
    combination of software and hardware technologies. To implement the distributed
    libraries and take advantage of data and model parallelism, you will need to make
    minor code changes to your training scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of the book publication, the SageMaker distributed libraries support
    two frameworks—**TensorFlow** and **PyTorch**.
  prefs: []
  type: TYPE_NORMAL
- en: While in this chapter we are focusing on the SageMaker native libraries for
    distributed training, you can also choose to use **Horovod**, the most popular
    open source distributed training framework, or the native distributed training
    strategies in frameworks such as TensorFlow and PyTorch. Please see the blog link
    in the references section for details on using Horovod with TensorFlow on SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker distributed data parallel library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's first dive into the SageMaker distributed data parallel library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SageMaker distributed data parallel library provides the capabilities to
    achieve near-linear scaling efficiency and fast training times on deep learning
    models. The library addresses the challenge of communications overhead in a distributed
    cluster using two approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: It automatically performs the `AllReduce` operation responsible for the overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It optimizes node-to-communication by utilizing AWS's network infrastructure
    and Amazon EC2 instance topology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker data parallelism can be used with both single-node, multi-device setup,
    and with multi-node setup. However, its value is more apparent in training clusters
    with two or more nodes. In this multi-node cluster, the `AllReduce` operation
    implemented as part of the library gives you significant performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the distributed libraries with the SageMaker training jobs, first enable
    the strategy you want when you construct the `estimator` object. The following
    code block shows how to create an `estimator` object using a `PyTorch` container
    with the data parallel strategy enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, there are a few changes that are needed to the training script,
    `train_pytorch_dist`, in this example. The next few code blocks show the changes
    required to the training script:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import and initialize the SageMaker distributed library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, pin each GPU to a single SageMaker data parallel library process with
    `local_rank`, which is a relative rank of the process within a given node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, resize the batch size to be handled by each worker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, wrap the trained model artifact with the `DDP` class from the distributed
    library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, once all of the changes are in place, simply call the `fit()` method
    on the estimator to kick off training with the training script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To observe the benefits of the distributed training, we ran two different training
    jobs on the same dataset. Both the jobs were run on a single `ml.p3.16xlarge`,
    the first job without distributed training, and the second job with `smdistributed
    dataparallel` enabled. In this experiment, the first job was completed in 12041
    seconds, and the second job was completed in 4179 seconds, resulting in a 65.29%
    improvement in the training time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison of the two training jobs with and without `smdistributed dataparallel`
    enabled is captured in the notebook in the GitHub repo: [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker distributed model parallel library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let's look into the SageMaker distributed model parallel library. This
    provides the capability to train large, complex deep learning models that can
    potentially increase prediction accuracy. The library automatically and efficiently
    splits a model across multiple GPUs, providing an option for both manual and automatic
    partitioning. It further coordinates training through a pipelined execution by
    building an efficient computation schedule where different nodes can simultaneously
    work on forward and backward passes for different data samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block shows creating an `estimator` object using a `PyTorch`
    container with the model parallel strategy enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As with the data parallel strategy, there are a few code changes necessary
    to the training script. Important changes are discussed in the next few code blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import and initialize the SageMaker distributed library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, wrap the model artifact in the `DistributedModel` class from the distributed
    library, and wrap the optimizer in the `DistributedOptimizer` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, add the forward and backward logic to a function and decorate it with
    `smp.step`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, call the `fit()` method on the `estimator` object to kick off training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'An example notebook that provides a complete walk-through of using the `ModelParallel`
    distribution strategy with a PyTorch container is provided in the GitHub repository:
    [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'While the SageMaker distributed model parallel library makes it easy to implement
    model parallel distributed training, for optimal training results consider the
    following best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Using manual versus auto-partitioning**: You can partition the model onto
    multiple nodes (or devices) using either manual or auto-partitioning. While both
    of the options are supported, you should choose auto-partitioning over the manual
    approach. With auto-partitioning, training operations and modules that share the
    same parameters will automatically be placed on the same device for correctness.
    With a manual approach, you will have to take care of the details on how to split
    up the model parts, and which part should be placed on which device. This is a
    time-consuming and error-prone process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choosing the batch size**: The model parallel library is most efficient with
    large batch sizes. In case you start with a smaller batch size to fit the model
    into a single node, then decide to implement model parallelism across multiple
    nodes, you should increase the batch size accordingly. Model parallelism saves
    memory for large models, allowing training with large batch sizes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choosing the number and size of micro-batches**: The model parallel library
    executes each micro-batch sequentially in each node or device. So, the micro-batch
    size should be large enough to fully utilize each GPU. At the same time, pipeline
    efficiency increases with the number of micro-batches, so balancing the two is
    important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is best practice to start with two or four micro-batches and increase the
    batch size according to the available memory of the node/device. Then experiment
    with larger batch sizes and increase the number of micro-batches. As the number
    of micro-batches is increased, larger batch sizes might become feasible if an
    interleaved pipeline is used.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When huge volumes of data are available upfront before training your model,
    distributed training strategies should be used. But what happens when a trained
    model is deployed and then you collect new data that might improve the model predictions?
    In this situation, you can incrementally train a new model starting with artifacts
    from an existing model and using an expanded dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Incremental training can save training time, resources, and costs in the following
    situations:'
  prefs: []
  type: TYPE_NORMAL
- en: An existing model is under-performing and new data becomes available that can
    potentially improve model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to use publicly available models as a starting point for your model
    without having to train from scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to train multiple versions of a model, with either different hyperparameters
    or using different datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to restart a previously stopped training job, without having to start
    from scratch again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, to complement or substitute for loading existing model weights
    and incrementally training, you can retrain on a sliding window on the most recent
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to use SageMaker capabilities to train with
    large volumes of data and complex model architectures. Besides the training data
    and model architecture, a critical part of ML training is tuning hyperparameters
    of the ML algorithm. In the next section, you will learn the best practices for
    using SageMaker to handle model tuning at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Automated model tuning with SageMaker hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning** (**HPT**) helps you find the right parameters to
    use with your ML algorithm or the neural network to find an optimal version of
    the model. Amazon SageMaker supports managed hyperparameter tuning, also called
    **automatic model tuning**. In this section, we discuss the best practices to
    consider while configuring hyperparameter jobs on Amazon SageMaker.'
  prefs: []
  type: TYPE_NORMAL
- en: To execute a SageMaker hyperparameter tuning job, you specify a set of hyperparameters,
    a range of values to explore for each hyperparameter, and an objective metric
    to measure the model's performance. Automatic tuning executes multiple training
    jobs on your training dataset with the ML algorithm and the hyperparameter values
    to find the best-performing model as measured by the objective metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code blocks, we will see how to create an HPT job on SageMaker:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, initialize the hyperparameter names and range of values for each hyperparameter
    you want to explore:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, configure the SageMaker `estimator` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, configure the `HyperparameterTuner` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, call the `fit()` method on the `tuner` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the hyperparameter job is completed, you can view the different training
    jobs executed by SageMaker, along with the objective metric for each job, in *Figure
    6.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – SageMaker HPT results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – SageMaker HPT results
  prefs: []
  type: TYPE_NORMAL
- en: 'You can dive further into each of the training jobs to view the exact values
    of the hyperparameters used, as shown in *Figure 6.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Hyperparameter values for a specific training job'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – Hyperparameter values for a specific training job
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'An example notebook that provides a complete walk-through of using SageMaker
    HPT, along with analysis of results, is provided in the GitHub repository: [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/HPO.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/HPO.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know the basics, let''s discuss some of the best practices to
    consider while configuring hyperparameter jobs on Amazon SageMaker:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Selecting a small number of hyperparameters**: HPT is a computationally intensive
    task, the computational complexity being proportional to the number of hyperparameters
    you want to tune. SageMaker allows you to specify up to 20 hyperparameters to
    optimize for a tuning job but limiting your search to a smaller number is likely
    to give you better results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Selecting a small range for hyperparameters**: Along the same lines, the
    range of values for hyperparameters can significantly affect the success of hyperparameter
    optimization. Intuitively, you may want to specify a very large range to explore
    all possible values for a hyperparameter, but you will in fact get better results
    by limiting your search to a small range of values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_batch_size` hyperparameter, instead of exploring a range in a linear
    fashion, you might want only to evaluate the two values–128 and 256\. In this
    case, you treat the parameter as a categorical value. In contrast, if you want
    to explore the values for the `train_batch_size` hyperparameter in a range from
    a minimum threshold value of 128 to a maximum threshold value of 256, you will
    use the `Integer` type. The `Integer` type allows for greater exploration of the
    range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you search a range that spans several orders of magnitude, you can optimize
    the search by choosing a logarithmic scale for `Integer` hyperparameters. Finally,
    choose a continuous parameter if the range of all values to explore, from the
    lowest to the highest, is relatively small. For example, exploring the `learning_rate`
    hyperparameter in the range of `0.0001` and `0.0005` at a linear scale.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Enabling warm start**: SageMaker HPT supports warm start, which reuses results
    from one or more prior tuning jobs as a starting point. Configure your HPT job
    to use warm start to limit the combinations of hyperparameters to search over
    in the new tuning job. This results in a faster tuning job. Warm start is particularly
    useful when you want to change the HPT ranges from the previous job or add new
    hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enabling early stop to save tuning time and costs**: With early stop enabled,
    the individual training jobs launched by the HPT job will terminate early when
    the objective metric is not improving significantly. After each epoch of training,
    a running average of the objective metric for all the previous training jobs up
    to the same epoch is determined and the median of running averages is calculated.
    If the value of the objective metric for the current training job is worse than
    the median value, SageMaker stops the current training job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stopping jobs early reduces the overall compute time and thereby the cost of
    the job. An additional benefit is that early stopping helps prevent overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`MaxParallelTrainingJobs` parameter. On one hand, running more HPT jobs concurrently
    completes the tuning job quickly. On the other, a tuning job can only find better
    combinations of hyperparameters through successive rounds of experiments. In the
    long run, executing a single training job at a time gives the best results with
    minimum computation time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the case when the default **Bayesian** optimization tuning strategy
    is used by SageMaker HPO. However, if you have experience with your algorithm
    and dataset, you can also use the random search strategy natively supported by
    SageMaker, since it enables concurrency but doesn't require serial rounds of experiments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'While in this section we focused on a single algorithm for best practice. The
    `CreateHyperParameterTuningJob` API can also be used to tune multiple algorithms
    by providing multiple training job definitions pointing to the different algorithms.
    For a detailed explanation of this API, see the following article: [https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateHyperParameterTuningJob.html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateHyperParameterTuningJob.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how to keep track of all your ML experiments
    related to solving a specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: Organizing and tracking training jobs with SageMaker Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key challenge ML practitioners face is keeping track of the myriad ML experiments
    that need to be executed before a model achieves desired results. For a single
    ML project, it is not uncommon for data scientists to routinely train several
    different models looking for improved accuracy. HPT adds more training jobs to
    these experiments. Typically, there are many details to track for experiments
    such as hyperparameters, model architectures, training algorithms, custom scripts,
    metrics, result artifacts, and more.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss **Amazon SageMaker Experiments**, which allows
    you to organize, track, visualize, and compare ML models across all phases of
    the ML lifecycle, including feature engineering, model training, model tuning,
    and model deploying. SageMaker Experiments' capability tracks model lineage, allowing
    you to troubleshoot production issues and audit your models to meet compliance
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic components that make up Amazon SageMaker Experiments include an experiment,
    a trial, a trial component, and a tracker, as shown in *Figure 6.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Amazon SageMaker Experiments overview'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – Amazon SageMaker Experiments overview
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at each component:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment**: An experiment encapsulates all related components that represent
    the ML problem you are attempting to solve. Each experiment is a collection of
    trials, with the goal of determining the trial that produces the best model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trial**: A trial represents a single attempt at solving the ML problem that
    captures the end-to-end ML process within an experiment. Each trial is a collection
    consisting of several trial components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trial Component**: A trial component represents a specific step within a
    given trial. For example, the data preprocessing step could be one trial component,
    and model training could be another trial component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracker**: A tracker is used to track metadata of individual trial components,
    including all parameters, inputs, outputs, artifacts, and metrics. Since this
    metadata is tracked and persisted, you can link the final model artifact to its
    origin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following code blocks, we will see how to create a SageMaker experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create an experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a `Tracker` instance to track the `Training` stage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, define experiment variables to define what you want to change to see how
    your objective is affected. In this example, we will experiment with several values
    for the number of the `max_depth` hyperparameter of `XGBoostmodel`. We will create
    a trial to track each training job run.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will also create a `TrialComponent` instance from the `Tracker` instance
    we created earlier and add this to the `Trial` instance. This will allow you to
    capture metrics from the training step as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When running the training job with the `fit()` method, associate `estimator`
    with the experiment and trial:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, after the experiment is completed, let''s analyze the experiment results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Figure 6.7* shows a list of all the trial components that were created as
    part of the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Trial components from the experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – Trial components from the experiment
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this section, a SageMaker experiment gives you a way to
    organize your efforts toward an ML goal and allows visibility into several important
    aspects of those efforts. A best practice we recommend is that any time you launch
    a training or tuning job, wrap it in an experiment. This allows you to gain visibility
    into the training and tuning jobs without any additional cost.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'An example notebook that provides a complete walk-through of using SageMaker
    Experiments is provided in the GitHub repository: [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/Experiments.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/Experiments.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the advanced techniques required to train models
    at scale using different distribution strategies. You further reviewed best practices
    for hyperparameter tuning to find the best version of the model to meet your objectives.
    You learned how to organize and track multiple experiments conducted in a typical
    ML workflow and create comparison reports.
  prefs: []
  type: TYPE_NORMAL
- en: Using the SageMaker capabilities and best practices discussed in this chapter,
    you can tackle ML at scale, allowing your organization to move out of the experimentation
    phase. You can take advantage of large datasets collected over years, and move
    toward realizing the full benefits of ML. In the next chapter, you will continue
    to enhance ML training by profiling training jobs using **Amazon SageMaker** **Debugger**.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For additional reading material, please review these references:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Learn Amazon SageMaker: A guide to building, training, and deploying ML models
    for developers and data scientists*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`https://www.amazon.com/Learn-Amazon-SageMaker-developers-scientists/dp/180020891X/ref=sr_1_1?dchild=1&keywords
    =Learn+Amazon+SageMaker+%3A+A+guide+to+building%2C+training%2C+and+deploying+machine+learning+models+for
    +developers+and+data+scientists&qid=1624801601&sr=8-1`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Multi-GPU and distributed training using Horovod in Amazon SageMaker Pipe
    mode*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/](https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Streamline modeling with Amazon SageMaker Studio and the Amazon Experiments
    SDK*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: https://aws.amazon.com/blogs/machine-learning/streamline-modeling-with-amazon-sagemaker-studio-and-amazon-experiments-sdk
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
