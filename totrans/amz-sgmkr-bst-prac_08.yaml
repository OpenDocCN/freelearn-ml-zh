- en: 'Chapter 6: Training and Tuning at Scale'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) practitioners face multiple challenges when training
    and tuning models at scale. **Scale challenges** come in the form of high volumes
    of training data and increased model size and model architecture complexity. Additional
    challenges come from having to run a large number of tuning jobs to identify the
    right set of hyperparameters and keeping track of multiple experiments conducted
    with varying algorithms for a specific ML objective. Scale challenges lead to
    long training times, resource constraints, and increased costs. This can reduce
    the productivity of teams, and potentially create a bottleneck for ML projects.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon SageMaker** provides managed distributed training and tuning capabilities
    to improve training efficiency, and capabilities to organize and track ML experiments
    at scale. SageMaker enables techniques such as streaming data into algorithms
    by using pipe mode for training with data at scale and Managed Spot Training for
    reduced training costs. Pipe mode and managed spot training are discussed in detail
    in *Learn Amazon SageMaker: A guide to building, training, and deploying machine
    learning models for developers and data scientists*, by Julien Simon.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss advanced topics of distributed training, best
    practices for hyperparameter tuning, and how to organize ML experiments at scale.
    By the end of this chapter, you will be able to use Amazon SageMaker's managed
    capabilities to train and tune at scale in a cost-effective manner and keep track
    of a large number of training experiments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ML training at scale with SageMaker distributed libraries
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated model tuning with SageMaker hyperparameter tuning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing and tracking training jobs with SageMaker Experiments
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an **AWS** account to run the examples included in this chapter.
    If you have not set up the data science environment yet, please refer to [*Chapter
    2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Data Science Environments*,
    which walks you through the setup process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Code examples included in the book are available on GitHub at https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter06\.
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: ML training at scale with SageMaker distributed libraries
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two common scale challenges with ML projects are scaling training data and scaling
    model size. While increased training data volume, model size, and complexity can
    potentially result in a more accurate model, there is a limit to the data volume
    and the model size that you can use with a single compute node, CPU, or GPU. Increased
    training data volumes and model sizes typically result in more computations, and
    therefore training jobs take longer to finish, even when using powerful compute
    instances such as `p3` and `p4` instances.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习项目中常见的两个规模挑战是扩展训练数据和扩展模型大小。虽然增加训练数据量、模型大小和复杂性可能会使模型更准确，但单个计算节点、CPU 或 GPU
    可以使用的最大数据量和模型大小是有限的。增加训练数据量和模型大小通常会导致更多的计算，因此即使使用像 `p3` 和 `p4` 这样的强大计算实例，训练作业完成的时间也会更长。
- en: '**Distributed training** is a commonly used technique to speed up training
    when dealing with scale challenges. Training load can be distributed either across
    multiple compute instances (nodes), or across multiple CPUs and GPUs (devices)
    on a single compute instance. There are two strategies for distributed training
    – **data parallelism** and **model parallelism**. Their names are a good indication
    of what is involved with each strategy. With data parallelism, the training data
    is split up across multiple nodes (or devices). With model parallelism, the model
    is split up across the nodes (or devices).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**分布式训练**是一种常用的技术，用于处理规模挑战时加速训练。训练负载可以分布在多个计算实例（节点）上，或者分布在单个计算实例上的多个 CPU 和
    GPU（设备）上。分布式训练有两种策略——**数据并行**和**模型并行**。它们的名称很好地说明了每个策略所涉及的内容。在数据并行中，训练数据被分割到多个节点（或设备）上。在模型并行中，模型被分割到节点（或设备）上。'
- en: Note
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**Mixed-precision training** is a popular technique to handle training at scale
    and reduce training time. Typically used on compute instances equipped with NVIDIA
    GPUs, mixed-precision training converts network weights from FP32 representation
    to FP16, calculates the gradients, converts weights back to FP32, multiplies by
    the learning rate, and finally updates the optimizer weights.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合精度训练**是一种流行的技术，用于处理大规模训练并减少训练时间。通常用于配备 NVIDIA GPU 的计算实例，混合精度训练将网络权重从 FP32
    表示转换为 FP16，计算梯度，将权重转换回 FP32，乘以学习率，并最终更新优化器权重。'
- en: 'In the data parallelism distribution strategy, the ML algorithm or the neural
    network-based model is replicated on all devices, and each device processes a
    batch of data. Results from all devices are then combined. In the model parallelism
    distribution strategy, the model (which is the neural network) is split up across
    the devices. Batches of training data are sent to all devices so that the data
    can be processed by all parts of the model. The following diagram shows an overview
    of data and model parallelism:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据并行分布策略中，机器学习算法或基于神经网络的模型在所有设备上复制，每个设备处理一批数据。然后，将所有设备的结果合并。在模型并行分布策略中，模型（即神经网络）被分割到各个设备上。将训练数据批次发送到所有设备，以便模型的所有部分都能处理数据。以下图表显示了数据和模型并行的概述：
- en: '![Figure 6.1 – Distribution strategies'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.1 – 分布策略'
- en: '](img/B17249_06_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.1 – 分布策略](img/B17249_06_01.jpg)'
- en: Figure 6.1 – Distribution strategies
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 分布策略
- en: Both data and model parallelism distribution strategies come with their own
    complexities. With data parallelism, each node (or device) is trained on a subset
    of data (called a mini-batch), and a mini-gradient is calculated. However, within
    each node, a mini-gradient average, with gradients coming from other nodes, should
    be calculated and communicated to all other nodes. This step is called **all reduce**,
    which is a communication overhead that grows as the training cluster is scaled
    up.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行和模型并行分布策略都伴随着自己的复杂性。在数据并行中，每个节点（或设备）在数据子集（称为小批量）上训练，并计算小梯度。然而，在节点内部，应该计算并与其他节点通信的小梯度平均值，这些梯度来自其他节点。这一步称为**all
    reduce**，它是随着训练集群扩展而增长的通信开销。
- en: While model parallelism addresses the requirements of a model not fitting in
    a single device's memory by splitting it across devices, partitioning the model
    across multiple GPUs may lead to under-utilization. This is because training on
    GPUs is sequential in nature, where only one GPU is actively processing data while
    the other GPUs are waiting to be activated. To be effective, model parallelism
    should be coupled with a pipeline execution schedule to train the model across
    multiple nodes, and in turn, maximize GPU utilization. Now that you know two different
    distribution strategies, how do you choose between data and model parallelism?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型并行处理解决模型无法适应单个设备内存的需求时，通过跨设备分割模型，将模型分割到多个GPU上可能会导致资源利用率不足。这是因为GPU上的训练本质上是顺序的，只有一个GPU正在积极处理数据，而其他GPU正在等待被激活。为了有效，模型并行处理应与管道执行调度相结合，以跨多个节点训练模型，从而最大化GPU利用率。现在你已经知道了两种不同的分布策略，你是如何在这两种策略之间进行选择的？
- en: Choosing between data and model parallelism
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在数据并行处理和模型并行处理之间进行选择
- en: 'When choosing a distributed strategy to implement, keep in mind the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择分布式策略实施时，请记住以下几点：
- en: Training on multiple nodes inherently causes inter-node communication overhead.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个节点上训练固有的会导致节点间通信开销。
- en: Additionally, to meet security and regulatory requirements, you may choose to
    protect the data transmitted between the nodes by enabling inter-container encryption.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，为了满足安全和监管要求，你可以选择通过启用容器间加密来保护节点间传输的数据。
- en: Enabling inter-container encryption will further increase the training time.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用容器间加密将进一步增加训练时间。
- en: Due to these reasons, use data parallelism if the trained model can fit in the
    memory of a single device or node. In situations where the model does not fit
    in the memory due to its size or complexity, you should experiment further with
    data parallelism before deciding on model parallelism.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，如果训练模型可以适应单个设备或节点的内存，则使用数据并行处理。在模型由于大小或复杂性而无法适应内存的情况下，在决定模型并行处理之前，应进一步实验数据并行处理。
- en: 'You can experiment with the following to improve data parallelism performance:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方法来提高数据并行处理性能：
- en: '**Tuning the model''s hyperparameters**: Tuning parameters such as the number
    of layers of a neural network, or the optimizer to use, affects the model''s size
    considerably.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整模型的超参数**：调整诸如神经网络层数量或要使用的优化器等参数会显著影响模型的大小。'
- en: '**Reducing the batch size**: Experiment by incrementally reducing the batch
    size until the model fits in the memory. This experiment should balance out the
    model''s memory needs with optimal batch size. Make sure you do not end up with
    a suboptimal small batch size just because training with a large batch size takes
    up most of the device memory.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少批量大小**：通过逐步减少批量大小进行实验，直到模型适应内存。这个实验应该平衡模型的内存需求与最优批量大小。确保你不会因为使用大批量大小占用了大部分设备内存而最终得到一个次优的小批量大小。'
- en: '**Reducing the model input size**: If the model input is tabular, consider
    embedding vectors of reduced dimensions. Similarly, for **natural language processing**
    (**NLP**) models, reduce the input NLP sequence length, and if the input is an
    image, reduce image resolution.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少模型输入大小**：如果模型输入是表格形式，考虑嵌入降维的向量。同样，对于自然语言处理（NLP）模型，减少输入NLP序列长度，如果输入是图像，则降低图像分辨率。'
- en: '**Using mixed-point precision**: Experiment with mixed-precision training,
    which uses FP16 representation of weights during gradient calculation, to reduce
    memory consumption.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用混合精度**：实验混合精度训练，在梯度计算期间使用FP16表示权重，以减少内存消耗。'
- en: 'The following flowchart shows the sequence of decisions and experiments to
    follow when choosing a distribution strategy to implement:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下流程图显示了在选择实施分布策略时应遵循的决策和实验顺序：
- en: '![Figure 6.2 – Choose a distribution strategy'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2 – 选择分布策略'
- en: '](img/B17249_06_02.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_06_02.jpg)'
- en: Figure 6.2 – Choose a distribution strategy
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 选择分布策略
- en: 'While data parallelism addresses the challenge of training data scale, model
    parallelism addresses the challenge of increased model size and complexity. A
    hybrid distribution strategy can also be implemented to include both data and
    model parallelism. *Figure 6.3* walks you through a hybrid distribution strategy
    with two-way data parallelism and four-way model parallelism:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Hybrid distribution strategy'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_03.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – Hybrid distribution strategy
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the compute resources
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both the distributed training strategies depend on a cluster of compute resources
    to spread the training load. When scaling the distributed cluster to meet the
    training demands, the recommended best practices are as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: First, scale vertically. That is, scale from a single GPU to multiple GPUs on
    a single instance. For example, let's say you started with the instance type `p3.2xlarge`,
    which has a single GPU for training your model, and you find yourself needing
    a greater number of GPUs to increase the training time. Change the instance type
    to `p3.16xlarge`, which has eight GPUs. This will result in a nearly eight-times
    decrease in the training, a near-linear speedup. Keeping the training job on a
    single scaled-up instance results in better performance than using multiple instances
    while keeping the cost low.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, scale from a single instance to multiple instances. When you reach limits
    of the instance types offered and still need to scale your training even further,
    then use multiple instances of the same type, that is, scale from a single `p3.16xlarge`
    to two `p3.16xlarge` instances. This will give you double the compute capacity,
    going from 8 GPUs on a single instance, to 16 GPUs across two instances. Keep
    in mind that when you use multiple instances in the training cluster, all instances
    should be in the same `us-west-2` must all be in `us-west-2a` or all in `us-west-2b`.
    Your training data should also be in the same region, `us-west-2`.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When moving from a single instance to multiple instances, it is recommended
    that you observe the model convergence and increase the batch size as necessary.
    Since the batch size you use is split across GPUs, each GPU is processing a lower
    batch size, which could lead to a high error rate and disrupt the model convergence.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's say you start with a single GPU on a `p3.2xlarge` instance
    using a batch size of 64, then scale up to four `p3dn.24xlarge`, which gives you
    32 GPUs. After this move, each GPU only processes a batch size of two, which is
    very likely to break the model convergence you observed with the original training.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker distributed libraries
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For easy implementation of data and model parallelism in your training jobs,
    SageMaker provides two different distributed training libraries. The libraries
    address the issues of inter-node and inter-GPU communications overhead using a
    combination of software and hardware technologies. To implement the distributed
    libraries and take advantage of data and model parallelism, you will need to make
    minor code changes to your training scripts.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: At the time of the book publication, the SageMaker distributed libraries support
    two frameworks—**TensorFlow** and **PyTorch**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: While in this chapter we are focusing on the SageMaker native libraries for
    distributed training, you can also choose to use **Horovod**, the most popular
    open source distributed training framework, or the native distributed training
    strategies in frameworks such as TensorFlow and PyTorch. Please see the blog link
    in the references section for details on using Horovod with TensorFlow on SageMaker.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker distributed data parallel library
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's first dive into the SageMaker distributed data parallel library.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'The SageMaker distributed data parallel library provides the capabilities to
    achieve near-linear scaling efficiency and fast training times on deep learning
    models. The library addresses the challenge of communications overhead in a distributed
    cluster using two approaches:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: It automatically performs the `AllReduce` operation responsible for the overhead.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It optimizes node-to-communication by utilizing AWS's network infrastructure
    and Amazon EC2 instance topology.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker data parallelism can be used with both single-node, multi-device setup,
    and with multi-node setup. However, its value is more apparent in training clusters
    with two or more nodes. In this multi-node cluster, the `AllReduce` operation
    implemented as part of the library gives you significant performance improvement.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the distributed libraries with the SageMaker training jobs, first enable
    the strategy you want when you construct the `estimator` object. The following
    code block shows how to create an `estimator` object using a `PyTorch` container
    with the data parallel strategy enabled:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Additionally, there are a few changes that are needed to the training script,
    `train_pytorch_dist`, in this example. The next few code blocks show the changes
    required to the training script:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import and initialize the SageMaker distributed library:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, pin each GPU to a single SageMaker data parallel library process with
    `local_rank`, which is a relative rank of the process within a given node:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, resize the batch size to be handled by each worker:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, wrap the trained model artifact with the `DDP` class from the distributed
    library:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, once all of the changes are in place, simply call the `fit()` method
    on the estimator to kick off training with the training script:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To observe the benefits of the distributed training, we ran two different training
    jobs on the same dataset. Both the jobs were run on a single `ml.p3.16xlarge`,
    the first job without distributed training, and the second job with `smdistributed
    dataparallel` enabled. In this experiment, the first job was completed in 12041
    seconds, and the second job was completed in 4179 seconds, resulting in a 65.29%
    improvement in the training time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察分布式训练的好处，我们在同一数据集上运行了两个不同的训练任务。这两个任务都在单个`ml.p3.16xlarge`上运行，第一个任务没有启用分布式训练，第二个任务启用了`smdistributed
    dataparallel`。在这个实验中，第一个任务耗时12041秒完成，第二个任务耗时4179秒完成，从而在训练时间上提高了65.29%。
- en: Note
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Comparison of the two training jobs with and without `smdistributed dataparallel`
    enabled is captured in the notebook in the GitHub repo: [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub仓库中的笔记本中记录了启用和未启用`smdistributed dataparallel`的两种训练任务的比较：[https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb)。
- en: SageMaker distributed model parallel library
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SageMaker分布式模型并行库
- en: Next, let's look into the SageMaker distributed model parallel library. This
    provides the capability to train large, complex deep learning models that can
    potentially increase prediction accuracy. The library automatically and efficiently
    splits a model across multiple GPUs, providing an option for both manual and automatic
    partitioning. It further coordinates training through a pipelined execution by
    building an efficient computation schedule where different nodes can simultaneously
    work on forward and backward passes for different data samples.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看SageMaker分布式模型并行库。这个库提供了训练大型、复杂的深度学习模型的能力，这可能会提高预测精度。该库自动且高效地将模型分割到多个GPU上，提供了手动和自动分区选项。它进一步通过构建高效的计算调度来协调训练，使得不同的节点可以同时为不同的数据样本执行前向和反向传递。
- en: 'The following code block shows creating an `estimator` object using a `PyTorch`
    container with the model parallel strategy enabled:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块展示了使用启用模型并行的`PyTorch`容器创建一个`estimator`对象：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As with the data parallel strategy, there are a few code changes necessary
    to the training script. Important changes are discussed in the next few code blocks:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据并行策略一样，训练脚本需要进行一些代码更改。重要的更改将在接下来的几个代码块中讨论：
- en: 'First, import and initialize the SageMaker distributed library:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入并初始化SageMaker分布式库：
- en: '[PRE7]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, wrap the model artifact in the `DistributedModel` class from the distributed
    library, and wrap the optimizer in the `DistributedOptimizer` class:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将模型工件包装在分布式库中的`DistributedModel`类中，并将优化器包装在`DistributedOptimizer`类中：
- en: '[PRE8]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, add the forward and backward logic to a function and decorate it with
    `smp.step`:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将前向和反向逻辑添加到一个函数中，并用`smp.step`进行装饰：
- en: '[PRE9]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, call the `fit()` method on the `estimator` object to kick off training:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在`estimator`对象上调用`fit()`方法以启动训练：
- en: '[PRE10]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Important Note
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'An example notebook that provides a complete walk-through of using the `ModelParallel`
    distribution strategy with a PyTorch container is provided in the GitHub repository:
    [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb).'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在GitHub仓库中提供了一个示例笔记本，它详细介绍了如何使用`ModelParallel`分布策略与PyTorch容器一起使用：[https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb)。
- en: 'While the SageMaker distributed model parallel library makes it easy to implement
    model parallel distributed training, for optimal training results consider the
    following best practices:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SageMaker分布式模型并行库使得实现模型并行分布式训练变得容易，但要获得最佳的训练结果，请考虑以下最佳实践：
- en: '**Using manual versus auto-partitioning**: You can partition the model onto
    multiple nodes (or devices) using either manual or auto-partitioning. While both
    of the options are supported, you should choose auto-partitioning over the manual
    approach. With auto-partitioning, training operations and modules that share the
    same parameters will automatically be placed on the same device for correctness.
    With a manual approach, you will have to take care of the details on how to split
    up the model parts, and which part should be placed on which device. This is a
    time-consuming and error-prone process.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choosing the batch size**: The model parallel library is most efficient with
    large batch sizes. In case you start with a smaller batch size to fit the model
    into a single node, then decide to implement model parallelism across multiple
    nodes, you should increase the batch size accordingly. Model parallelism saves
    memory for large models, allowing training with large batch sizes.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choosing the number and size of micro-batches**: The model parallel library
    executes each micro-batch sequentially in each node or device. So, the micro-batch
    size should be large enough to fully utilize each GPU. At the same time, pipeline
    efficiency increases with the number of micro-batches, so balancing the two is
    important.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is best practice to start with two or four micro-batches and increase the
    batch size according to the available memory of the node/device. Then experiment
    with larger batch sizes and increase the number of micro-batches. As the number
    of micro-batches is increased, larger batch sizes might become feasible if an
    interleaved pipeline is used.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Incremental training
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When huge volumes of data are available upfront before training your model,
    distributed training strategies should be used. But what happens when a trained
    model is deployed and then you collect new data that might improve the model predictions?
    In this situation, you can incrementally train a new model starting with artifacts
    from an existing model and using an expanded dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Incremental training can save training time, resources, and costs in the following
    situations:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: An existing model is under-performing and new data becomes available that can
    potentially improve model performance.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to use publicly available models as a starting point for your model
    without having to train from scratch.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to train multiple versions of a model, with either different hyperparameters
    or using different datasets.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to restart a previously stopped training job, without having to start
    from scratch again.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, to complement or substitute for loading existing model weights
    and incrementally training, you can retrain on a sliding window on the most recent
    data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to use SageMaker capabilities to train with
    large volumes of data and complex model architectures. Besides the training data
    and model architecture, a critical part of ML training is tuning hyperparameters
    of the ML algorithm. In the next section, you will learn the best practices for
    using SageMaker to handle model tuning at scale.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Automated model tuning with SageMaker hyperparameter tuning
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning** (**HPT**) helps you find the right parameters to
    use with your ML algorithm or the neural network to find an optimal version of
    the model. Amazon SageMaker supports managed hyperparameter tuning, also called
    **automatic model tuning**. In this section, we discuss the best practices to
    consider while configuring hyperparameter jobs on Amazon SageMaker.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: To execute a SageMaker hyperparameter tuning job, you specify a set of hyperparameters,
    a range of values to explore for each hyperparameter, and an objective metric
    to measure the model's performance. Automatic tuning executes multiple training
    jobs on your training dataset with the ML algorithm and the hyperparameter values
    to find the best-performing model as measured by the objective metric.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code blocks, we will see how to create an HPT job on SageMaker:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'First, initialize the hyperparameter names and range of values for each hyperparameter
    you want to explore:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, configure the SageMaker `estimator` object:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, configure the `HyperparameterTuner` object:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, call the `fit()` method on the `tuner` object:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once the hyperparameter job is completed, you can view the different training
    jobs executed by SageMaker, along with the objective metric for each job, in *Figure
    6.4*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – SageMaker HPT results'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_04.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – SageMaker HPT results
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'You can dive further into each of the training jobs to view the exact values
    of the hyperparameters used, as shown in *Figure 6.5*:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Hyperparameter values for a specific training job'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_05.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – Hyperparameter values for a specific training job
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'An example notebook that provides a complete walk-through of using SageMaker
    HPT, along with analysis of results, is provided in the GitHub repository: [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/HPO.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/HPO.ipynb).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know the basics, let''s discuss some of the best practices to
    consider while configuring hyperparameter jobs on Amazon SageMaker:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '**Selecting a small number of hyperparameters**: HPT is a computationally intensive
    task, the computational complexity being proportional to the number of hyperparameters
    you want to tune. SageMaker allows you to specify up to 20 hyperparameters to
    optimize for a tuning job but limiting your search to a smaller number is likely
    to give you better results.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Selecting a small range for hyperparameters**: Along the same lines, the
    range of values for hyperparameters can significantly affect the success of hyperparameter
    optimization. Intuitively, you may want to specify a very large range to explore
    all possible values for a hyperparameter, but you will in fact get better results
    by limiting your search to a small range of values.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_batch_size` hyperparameter, instead of exploring a range in a linear
    fashion, you might want only to evaluate the two values–128 and 256\. In this
    case, you treat the parameter as a categorical value. In contrast, if you want
    to explore the values for the `train_batch_size` hyperparameter in a range from
    a minimum threshold value of 128 to a maximum threshold value of 256, you will
    use the `Integer` type. The `Integer` type allows for greater exploration of the
    range.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you search a range that spans several orders of magnitude, you can optimize
    the search by choosing a logarithmic scale for `Integer` hyperparameters. Finally,
    choose a continuous parameter if the range of all values to explore, from the
    lowest to the highest, is relatively small. For example, exploring the `learning_rate`
    hyperparameter in the range of `0.0001` and `0.0005` at a linear scale.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Enabling warm start**: SageMaker HPT supports warm start, which reuses results
    from one or more prior tuning jobs as a starting point. Configure your HPT job
    to use warm start to limit the combinations of hyperparameters to search over
    in the new tuning job. This results in a faster tuning job. Warm start is particularly
    useful when you want to change the HPT ranges from the previous job or add new
    hyperparameters.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enabling early stop to save tuning time and costs**: With early stop enabled,
    the individual training jobs launched by the HPT job will terminate early when
    the objective metric is not improving significantly. After each epoch of training,
    a running average of the objective metric for all the previous training jobs up
    to the same epoch is determined and the median of running averages is calculated.
    If the value of the objective metric for the current training job is worse than
    the median value, SageMaker stops the current training job.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stopping jobs early reduces the overall compute time and thereby the cost of
    the job. An additional benefit is that early stopping helps prevent overfitting.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`MaxParallelTrainingJobs` parameter. On one hand, running more HPT jobs concurrently
    completes the tuning job quickly. On the other, a tuning job can only find better
    combinations of hyperparameters through successive rounds of experiments. In the
    long run, executing a single training job at a time gives the best results with
    minimum computation time.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the case when the default **Bayesian** optimization tuning strategy
    is used by SageMaker HPO. However, if you have experience with your algorithm
    and dataset, you can also use the random search strategy natively supported by
    SageMaker, since it enables concurrency but doesn't require serial rounds of experiments.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'While in this section we focused on a single algorithm for best practice. The
    `CreateHyperParameterTuningJob` API can also be used to tune multiple algorithms
    by providing multiple training job definitions pointing to the different algorithms.
    For a detailed explanation of this API, see the following article: [https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateHyperParameterTuningJob.html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateHyperParameterTuningJob.html).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how to keep track of all your ML experiments
    related to solving a specific problem.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Organizing and tracking training jobs with SageMaker Experiments
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key challenge ML practitioners face is keeping track of the myriad ML experiments
    that need to be executed before a model achieves desired results. For a single
    ML project, it is not uncommon for data scientists to routinely train several
    different models looking for improved accuracy. HPT adds more training jobs to
    these experiments. Typically, there are many details to track for experiments
    such as hyperparameters, model architectures, training algorithms, custom scripts,
    metrics, result artifacts, and more.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss **Amazon SageMaker Experiments**, which allows
    you to organize, track, visualize, and compare ML models across all phases of
    the ML lifecycle, including feature engineering, model training, model tuning,
    and model deploying. SageMaker Experiments' capability tracks model lineage, allowing
    you to troubleshoot production issues and audit your models to meet compliance
    requirements.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic components that make up Amazon SageMaker Experiments include an experiment,
    a trial, a trial component, and a tracker, as shown in *Figure 6.6*:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Amazon SageMaker Experiments overview'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_06.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – Amazon SageMaker Experiments overview
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at each component:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment**: An experiment encapsulates all related components that represent
    the ML problem you are attempting to solve. Each experiment is a collection of
    trials, with the goal of determining the trial that produces the best model.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trial**: A trial represents a single attempt at solving the ML problem that
    captures the end-to-end ML process within an experiment. Each trial is a collection
    consisting of several trial components.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trial Component**: A trial component represents a specific step within a
    given trial. For example, the data preprocessing step could be one trial component,
    and model training could be another trial component.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracker**: A tracker is used to track metadata of individual trial components,
    including all parameters, inputs, outputs, artifacts, and metrics. Since this
    metadata is tracked and persisted, you can link the final model artifact to its
    origin.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following code blocks, we will see how to create a SageMaker experiment:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create an experiment:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, create a `Tracker` instance to track the `Training` stage:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, define experiment variables to define what you want to change to see how
    your objective is affected. In this example, we will experiment with several values
    for the number of the `max_depth` hyperparameter of `XGBoostmodel`. We will create
    a trial to track each training job run.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will also create a `TrialComponent` instance from the `Tracker` instance
    we created earlier and add this to the `Trial` instance. This will allow you to
    capture metrics from the training step as follows:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When running the training job with the `fit()` method, associate `estimator`
    with the experiment and trial:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, after the experiment is completed, let''s analyze the experiment results:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Figure 6.7* shows a list of all the trial components that were created as
    part of the experiment:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Trial components from the experiment'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_06_07.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – Trial components from the experiment
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this section, a SageMaker experiment gives you a way to
    organize your efforts toward an ML goal and allows visibility into several important
    aspects of those efforts. A best practice we recommend is that any time you launch
    a training or tuning job, wrap it in an experiment. This allows you to gain visibility
    into the training and tuning jobs without any additional cost.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'An example notebook that provides a complete walk-through of using SageMaker
    Experiments is provided in the GitHub repository: [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/Experiments.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/Experiments.ipynb).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the advanced techniques required to train models
    at scale using different distribution strategies. You further reviewed best practices
    for hyperparameter tuning to find the best version of the model to meet your objectives.
    You learned how to organize and track multiple experiments conducted in a typical
    ML workflow and create comparison reports.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Using the SageMaker capabilities and best practices discussed in this chapter,
    you can tackle ML at scale, allowing your organization to move out of the experimentation
    phase. You can take advantage of large datasets collected over years, and move
    toward realizing the full benefits of ML. In the next chapter, you will continue
    to enhance ML training by profiling training jobs using **Amazon SageMaker** **Debugger**.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For additional reading material, please review these references:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '*Learn Amazon SageMaker: A guide to building, training, and deploying ML models
    for developers and data scientists*:'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`https://www.amazon.com/Learn-Amazon-SageMaker-developers-scientists/dp/180020891X/ref=sr_1_1?dchild=1&keywords
    =Learn+Amazon+SageMaker+%3A+A+guide+to+building%2C+training%2C+and+deploying+machine+learning+models+for
    +developers+and+data+scientists&qid=1624801601&sr=8-1`'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Multi-GPU and distributed training using Horovod in Amazon SageMaker Pipe
    mode*:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/](https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/)'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Streamline modeling with Amazon SageMaker Studio and the Amazon Experiments
    SDK*:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: https://aws.amazon.com/blogs/machine-learning/streamline-modeling-with-amazon-sagemaker-studio-and-amazon-experiments-sdk
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
