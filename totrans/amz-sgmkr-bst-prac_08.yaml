- en: 'Chapter 6: Training and Tuning at Scale'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章：大规模训练和调整
- en: '**Machine learning** (**ML**) practitioners face multiple challenges when training
    and tuning models at scale. **Scale challenges** come in the form of high volumes
    of training data and increased model size and model architecture complexity. Additional
    challenges come from having to run a large number of tuning jobs to identify the
    right set of hyperparameters and keeping track of multiple experiments conducted
    with varying algorithms for a specific ML objective. Scale challenges lead to
    long training times, resource constraints, and increased costs. This can reduce
    the productivity of teams, and potentially create a bottleneck for ML projects.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）实践者在进行大规模模型训练和调整时面临多重挑战。**规模挑战**以大量训练数据、增加的模型大小和模型架构复杂性为形式出现。此外，还需要运行大量调整作业以确定正确的超参数集，并跟踪针对特定机器学习目标使用不同算法进行的多个实验。规模挑战导致训练时间延长、资源受限和成本增加。这可能会降低团队的效率，并可能成为机器学习项目的瓶颈。'
- en: '**Amazon SageMaker** provides managed distributed training and tuning capabilities
    to improve training efficiency, and capabilities to organize and track ML experiments
    at scale. SageMaker enables techniques such as streaming data into algorithms
    by using pipe mode for training with data at scale and Managed Spot Training for
    reduced training costs. Pipe mode and managed spot training are discussed in detail
    in *Learn Amazon SageMaker: A guide to building, training, and deploying machine
    learning models for developers and data scientists*, by Julien Simon.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon SageMaker**提供了托管分布式训练和调整功能，以提高训练效率，并提供了在大规模下组织和跟踪机器学习实验的能力。SageMaker通过使用管道模式将数据流式传输到算法中，以及通过托管Spot
    Training来降低训练成本，实现了这些技术。管道模式和托管Spot Training在Julien Simon所著的*《学习Amazon SageMaker：开发者与数据科学家构建、训练和部署机器学习模型的指南》*中进行了详细讨论。'
- en: In this chapter, we will discuss advanced topics of distributed training, best
    practices for hyperparameter tuning, and how to organize ML experiments at scale.
    By the end of this chapter, you will be able to use Amazon SageMaker's managed
    capabilities to train and tune at scale in a cost-effective manner and keep track
    of a large number of training experiments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论分布式训练的高级主题、超参数调整的最佳实践以及如何在大规模下组织机器学习实验。到本章结束时，您将能够使用Amazon SageMaker的托管功能以经济高效的方式在大规模下进行训练和调整，并跟踪大量训练实验。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: ML training at scale with SageMaker distributed libraries
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker分布式库进行大规模机器学习训练
- en: Automated model tuning with SageMaker hyperparameter tuning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker超参数调整进行自动化模型调整
- en: Organizing and tracking training jobs with SageMaker Experiments
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker Experiments组织和跟踪训练作业
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an **AWS** account to run the examples included in this chapter.
    If you have not set up the data science environment yet, please refer to [*Chapter
    2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Data Science Environments*,
    which walks you through the setup process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要AWS账户才能运行本章包含的示例。如果您尚未设置数据科学环境，请参阅[*第2章*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*，数据科学环境*，其中将指导您完成设置过程。
- en: Code examples included in the book are available on GitHub at https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter06\.
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 书中包含的代码示例可在GitHub上找到，网址为https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter06。您需要安装Git客户端才能访问它们（[https://git-scm.com/](https://git-scm.com/)）。
- en: ML training at scale with SageMaker distributed libraries
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker分布式库进行大规模机器学习训练
- en: Two common scale challenges with ML projects are scaling training data and scaling
    model size. While increased training data volume, model size, and complexity can
    potentially result in a more accurate model, there is a limit to the data volume
    and the model size that you can use with a single compute node, CPU, or GPU. Increased
    training data volumes and model sizes typically result in more computations, and
    therefore training jobs take longer to finish, even when using powerful compute
    instances such as `p3` and `p4` instances.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习项目中常见的两个规模挑战是扩展训练数据和扩展模型大小。虽然增加训练数据量、模型大小和复杂性可能会使模型更准确，但单个计算节点、CPU 或 GPU
    可以使用的最大数据量和模型大小是有限的。增加训练数据量和模型大小通常会导致更多的计算，因此即使使用像 `p3` 和 `p4` 这样的强大计算实例，训练作业完成的时间也会更长。
- en: '**Distributed training** is a commonly used technique to speed up training
    when dealing with scale challenges. Training load can be distributed either across
    multiple compute instances (nodes), or across multiple CPUs and GPUs (devices)
    on a single compute instance. There are two strategies for distributed training
    – **data parallelism** and **model parallelism**. Their names are a good indication
    of what is involved with each strategy. With data parallelism, the training data
    is split up across multiple nodes (or devices). With model parallelism, the model
    is split up across the nodes (or devices).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**分布式训练**是一种常用的技术，用于处理规模挑战时加速训练。训练负载可以分布在多个计算实例（节点）上，或者分布在单个计算实例上的多个 CPU 和
    GPU（设备）上。分布式训练有两种策略——**数据并行**和**模型并行**。它们的名称很好地说明了每个策略所涉及的内容。在数据并行中，训练数据被分割到多个节点（或设备）上。在模型并行中，模型被分割到节点（或设备）上。'
- en: Note
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**Mixed-precision training** is a popular technique to handle training at scale
    and reduce training time. Typically used on compute instances equipped with NVIDIA
    GPUs, mixed-precision training converts network weights from FP32 representation
    to FP16, calculates the gradients, converts weights back to FP32, multiplies by
    the learning rate, and finally updates the optimizer weights.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合精度训练**是一种流行的技术，用于处理大规模训练并减少训练时间。通常用于配备 NVIDIA GPU 的计算实例，混合精度训练将网络权重从 FP32
    表示转换为 FP16，计算梯度，将权重转换回 FP32，乘以学习率，并最终更新优化器权重。'
- en: 'In the data parallelism distribution strategy, the ML algorithm or the neural
    network-based model is replicated on all devices, and each device processes a
    batch of data. Results from all devices are then combined. In the model parallelism
    distribution strategy, the model (which is the neural network) is split up across
    the devices. Batches of training data are sent to all devices so that the data
    can be processed by all parts of the model. The following diagram shows an overview
    of data and model parallelism:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据并行分布策略中，机器学习算法或基于神经网络的模型在所有设备上复制，每个设备处理一批数据。然后，将所有设备的结果合并。在模型并行分布策略中，模型（即神经网络）被分割到各个设备上。将训练数据批次发送到所有设备，以便模型的所有部分都能处理数据。以下图表显示了数据和模型并行的概述：
- en: '![Figure 6.1 – Distribution strategies'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.1 – 分布策略'
- en: '](img/B17249_06_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.1 – 分布策略](img/B17249_06_01.jpg)'
- en: Figure 6.1 – Distribution strategies
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 分布策略
- en: Both data and model parallelism distribution strategies come with their own
    complexities. With data parallelism, each node (or device) is trained on a subset
    of data (called a mini-batch), and a mini-gradient is calculated. However, within
    each node, a mini-gradient average, with gradients coming from other nodes, should
    be calculated and communicated to all other nodes. This step is called **all reduce**,
    which is a communication overhead that grows as the training cluster is scaled
    up.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行和模型并行分布策略都伴随着自己的复杂性。在数据并行中，每个节点（或设备）在数据子集（称为小批量）上训练，并计算小梯度。然而，在节点内部，应该计算并与其他节点通信的小梯度平均值，这些梯度来自其他节点。这一步称为**all
    reduce**，它是随着训练集群扩展而增长的通信开销。
- en: While model parallelism addresses the requirements of a model not fitting in
    a single device's memory by splitting it across devices, partitioning the model
    across multiple GPUs may lead to under-utilization. This is because training on
    GPUs is sequential in nature, where only one GPU is actively processing data while
    the other GPUs are waiting to be activated. To be effective, model parallelism
    should be coupled with a pipeline execution schedule to train the model across
    multiple nodes, and in turn, maximize GPU utilization. Now that you know two different
    distribution strategies, how do you choose between data and model parallelism?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型并行处理解决模型无法适应单个设备内存的需求时，通过跨设备分割模型，将模型分割到多个GPU上可能会导致资源利用率不足。这是因为GPU上的训练本质上是顺序的，只有一个GPU正在积极处理数据，而其他GPU正在等待被激活。为了有效，模型并行处理应与管道执行调度相结合，以跨多个节点训练模型，从而最大化GPU利用率。现在你已经知道了两种不同的分布策略，你是如何在这两种策略之间进行选择的？
- en: Choosing between data and model parallelism
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在数据并行处理和模型并行处理之间进行选择
- en: 'When choosing a distributed strategy to implement, keep in mind the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择分布式策略实施时，请记住以下几点：
- en: Training on multiple nodes inherently causes inter-node communication overhead.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个节点上训练固有的会导致节点间通信开销。
- en: Additionally, to meet security and regulatory requirements, you may choose to
    protect the data transmitted between the nodes by enabling inter-container encryption.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，为了满足安全和监管要求，你可以选择通过启用容器间加密来保护节点间传输的数据。
- en: Enabling inter-container encryption will further increase the training time.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用容器间加密将进一步增加训练时间。
- en: Due to these reasons, use data parallelism if the trained model can fit in the
    memory of a single device or node. In situations where the model does not fit
    in the memory due to its size or complexity, you should experiment further with
    data parallelism before deciding on model parallelism.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，如果训练模型可以适应单个设备或节点的内存，则使用数据并行处理。在模型由于大小或复杂性而无法适应内存的情况下，在决定模型并行处理之前，应进一步实验数据并行处理。
- en: 'You can experiment with the following to improve data parallelism performance:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方法来提高数据并行处理性能：
- en: '**Tuning the model''s hyperparameters**: Tuning parameters such as the number
    of layers of a neural network, or the optimizer to use, affects the model''s size
    considerably.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整模型的超参数**：调整诸如神经网络层数量或要使用的优化器等参数会显著影响模型的大小。'
- en: '**Reducing the batch size**: Experiment by incrementally reducing the batch
    size until the model fits in the memory. This experiment should balance out the
    model''s memory needs with optimal batch size. Make sure you do not end up with
    a suboptimal small batch size just because training with a large batch size takes
    up most of the device memory.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少批量大小**：通过逐步减少批量大小进行实验，直到模型适应内存。这个实验应该平衡模型的内存需求与最优批量大小。确保你不会因为使用大批量大小占用了大部分设备内存而最终得到一个次优的小批量大小。'
- en: '**Reducing the model input size**: If the model input is tabular, consider
    embedding vectors of reduced dimensions. Similarly, for **natural language processing**
    (**NLP**) models, reduce the input NLP sequence length, and if the input is an
    image, reduce image resolution.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少模型输入大小**：如果模型输入是表格形式，考虑嵌入降维的向量。同样，对于自然语言处理（NLP）模型，减少输入NLP序列长度，如果输入是图像，则降低图像分辨率。'
- en: '**Using mixed-point precision**: Experiment with mixed-precision training,
    which uses FP16 representation of weights during gradient calculation, to reduce
    memory consumption.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用混合精度**：实验混合精度训练，在梯度计算期间使用FP16表示权重，以减少内存消耗。'
- en: 'The following flowchart shows the sequence of decisions and experiments to
    follow when choosing a distribution strategy to implement:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下流程图显示了在选择实施分布策略时应遵循的决策和实验顺序：
- en: '![Figure 6.2 – Choose a distribution strategy'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2 – 选择分布策略'
- en: '](img/B17249_06_02.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_06_02.jpg)'
- en: Figure 6.2 – Choose a distribution strategy
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 选择分布策略
- en: 'While data parallelism addresses the challenge of training data scale, model
    parallelism addresses the challenge of increased model size and complexity. A
    hybrid distribution strategy can also be implemented to include both data and
    model parallelism. *Figure 6.3* walks you through a hybrid distribution strategy
    with two-way data parallelism and four-way model parallelism:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据并行处理解决训练数据规模挑战时，模型并行处理则应对模型规模和复杂性的增加挑战。可以实施混合分布策略，包括数据和模型并行。*图6.3* 将向您展示一个具有双向数据并行和四向模型并行的混合分布策略：
- en: '![Figure 6.3 – Hybrid distribution strategy'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.3 – Hybrid distribution strategy'
- en: '](img/B17249_06_03.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17249_06_03.jpg]'
- en: Figure 6.3 – Hybrid distribution strategy
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 混合分布策略
- en: Scaling the compute resources
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展计算资源
- en: 'Both the distributed training strategies depend on a cluster of compute resources
    to spread the training load. When scaling the distributed cluster to meet the
    training demands, the recommended best practices are as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种分布式训练策略都依赖于计算资源集群来分散训练负载。当扩展分布式集群以满足训练需求时，以下为推荐的最佳实践：
- en: First, scale vertically. That is, scale from a single GPU to multiple GPUs on
    a single instance. For example, let's say you started with the instance type `p3.2xlarge`,
    which has a single GPU for training your model, and you find yourself needing
    a greater number of GPUs to increase the training time. Change the instance type
    to `p3.16xlarge`, which has eight GPUs. This will result in a nearly eight-times
    decrease in the training, a near-linear speedup. Keeping the training job on a
    single scaled-up instance results in better performance than using multiple instances
    while keeping the cost low.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，进行垂直扩展。也就是说，在单个实例上从单个GPU扩展到多个GPU。例如，假设您从一个具有单个GPU用于训练模型的 `p3.2xlarge` 实例开始，并发现自己需要更多的GPU来增加训练时间。将实例类型更改为具有八个GPU的
    `p3.16xlarge`。这将导致训练时间几乎减少八倍，实现近线性的加速。将训练作业保持在单个扩展后的实例上，比使用多个实例同时保持低成本获得更好的性能。
- en: Next, scale from a single instance to multiple instances. When you reach limits
    of the instance types offered and still need to scale your training even further,
    then use multiple instances of the same type, that is, scale from a single `p3.16xlarge`
    to two `p3.16xlarge` instances. This will give you double the compute capacity,
    going from 8 GPUs on a single instance, to 16 GPUs across two instances. Keep
    in mind that when you use multiple instances in the training cluster, all instances
    should be in the same `us-west-2` must all be in `us-west-2a` or all in `us-west-2b`.
    Your training data should also be in the same region, `us-west-2`.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，从单个实例扩展到多个实例。当达到提供的实例类型极限，但仍需要进一步扩展训练时，则使用相同类型的多个实例，即从单个 `p3.16xlarge` 实例扩展到两个
    `p3.16xlarge` 实例。这将使您获得双倍的计算能力，从单个实例上的8个GPU增加到两个实例上的16个GPU。请注意，当您在训练集群中使用多个实例时，所有实例应位于相同的
    `us-west-2` 区域，必须全部位于 `us-west-2a` 或全部位于 `us-west-2b`。您的训练数据也应位于同一区域，`us-west-2`。
- en: When moving from a single instance to multiple instances, it is recommended
    that you observe the model convergence and increase the batch size as necessary.
    Since the batch size you use is split across GPUs, each GPU is processing a lower
    batch size, which could lead to a high error rate and disrupt the model convergence.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当从单个实例迁移到多个实例时，建议您观察模型收敛情况，并在必要时增加批大小。由于您使用的批大小是在GPU之间分割的，每个GPU处理的批大小较低，这可能导致高错误率并破坏模型收敛。
- en: For example, let's say you start with a single GPU on a `p3.2xlarge` instance
    using a batch size of 64, then scale up to four `p3dn.24xlarge`, which gives you
    32 GPUs. After this move, each GPU only processes a batch size of two, which is
    very likely to break the model convergence you observed with the original training.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您从一个 `p3.2xlarge` 实例上的单个GPU开始，使用64的批大小，然后扩展到四个 `p3dn.24xlarge` 实例，这为您提供了32个GPU。在此迁移之后，每个GPU只处理2个批次的任务，这很可能破坏您在原始训练中观察到的模型收敛。
- en: SageMaker distributed libraries
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker分布式库
- en: For easy implementation of data and model parallelism in your training jobs,
    SageMaker provides two different distributed training libraries. The libraries
    address the issues of inter-node and inter-GPU communications overhead using a
    combination of software and hardware technologies. To implement the distributed
    libraries and take advantage of data and model parallelism, you will need to make
    minor code changes to your training scripts.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在训练作业中轻松实现数据和模型并行，SageMaker提供了两个不同的分布式训练库。这些库通过软件和硬件技术的组合解决了节点间和GPU间通信开销的问题。为了实现分布式库并利用数据和模型并行，您需要修改训练脚本中的少量代码。
- en: Important note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: At the time of the book publication, the SageMaker distributed libraries support
    two frameworks—**TensorFlow** and **PyTorch**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书出版时，SageMaker分布式库支持两个框架——**TensorFlow**和**PyTorch**。
- en: While in this chapter we are focusing on the SageMaker native libraries for
    distributed training, you can also choose to use **Horovod**, the most popular
    open source distributed training framework, or the native distributed training
    strategies in frameworks such as TensorFlow and PyTorch. Please see the blog link
    in the references section for details on using Horovod with TensorFlow on SageMaker.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在本章中我们专注于SageMaker原生的分布式训练库，您也可以选择使用最受欢迎的开源分布式训练框架**Horovod**，或者使用TensorFlow和PyTorch等框架的本地分布式训练策略。请参阅参考文献部分的博客链接，了解如何在SageMaker上使用Horovod与TensorFlow的详细信息。
- en: SageMaker distributed data parallel library
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SageMaker分布式数据并行库
- en: Let's first dive into the SageMaker distributed data parallel library.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先深入了解SageMaker分布式数据并行库。
- en: 'The SageMaker distributed data parallel library provides the capabilities to
    achieve near-linear scaling efficiency and fast training times on deep learning
    models. The library addresses the challenge of communications overhead in a distributed
    cluster using two approaches:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker分布式数据并行库提供了在深度学习模型上实现接近线性扩展效率和快速训练时间的能力。该库通过两种方法解决了分布式集群中通信开销的挑战：
- en: It automatically performs the `AllReduce` operation responsible for the overhead.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它自动执行负责开销的`AllReduce`操作。
- en: It optimizes node-to-communication by utilizing AWS's network infrastructure
    and Amazon EC2 instance topology.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过利用AWS的网络基础设施和Amazon EC2实例拓扑结构来优化节点间的通信。
- en: SageMaker data parallelism can be used with both single-node, multi-device setup,
    and with multi-node setup. However, its value is more apparent in training clusters
    with two or more nodes. In this multi-node cluster, the `AllReduce` operation
    implemented as part of the library gives you significant performance improvement.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker数据并行可以在单节点多设备设置和多节点设置中使用。然而，它在训练包含两个或更多节点的集群时价值更为明显。在这个多节点集群中，作为库一部分实现的`AllReduce`操作为您提供了显著的性能提升。
- en: 'To use the distributed libraries with the SageMaker training jobs, first enable
    the strategy you want when you construct the `estimator` object. The following
    code block shows how to create an `estimator` object using a `PyTorch` container
    with the data parallel strategy enabled:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用SageMaker训练作业中的分布式库，首先在构建`estimator`对象时启用您想要的策略。以下代码块展示了如何使用启用数据并行策略的`PyTorch`容器创建`estimator`对象：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Additionally, there are a few changes that are needed to the training script,
    `train_pytorch_dist`, in this example. The next few code blocks show the changes
    required to the training script:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在这个例子中，训练脚本`train_pytorch_dist`还需要进行一些修改。接下来的几个代码块展示了需要修改的训练脚本：
- en: 'First, import and initialize the SageMaker distributed library:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入并初始化SageMaker分布式库：
- en: '[PRE1]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, pin each GPU to a single SageMaker data parallel library process with
    `local_rank`, which is a relative rank of the process within a given node:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，使用`local_rank`将每个GPU固定到单个SageMaker数据并行库进程，`local_rank`是在给定节点内进程的相对排名：
- en: '[PRE2]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, resize the batch size to be handled by each worker:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，调整每个工作器处理的批大小：
- en: '[PRE3]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, wrap the trained model artifact with the `DDP` class from the distributed
    library:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，使用分布式库中的`DDP`类包装训练好的模型工件：
- en: '[PRE4]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, once all of the changes are in place, simply call the `fit()` method
    on the estimator to kick off training with the training script:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，一旦所有修改都到位，只需在估计器上调用`fit()`方法即可启动训练，使用训练脚本：
- en: '[PRE5]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To observe the benefits of the distributed training, we ran two different training
    jobs on the same dataset. Both the jobs were run on a single `ml.p3.16xlarge`,
    the first job without distributed training, and the second job with `smdistributed
    dataparallel` enabled. In this experiment, the first job was completed in 12041
    seconds, and the second job was completed in 4179 seconds, resulting in a 65.29%
    improvement in the training time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察分布式训练的好处，我们在同一数据集上运行了两个不同的训练任务。这两个任务都在单个`ml.p3.16xlarge`上运行，第一个任务没有启用分布式训练，第二个任务启用了`smdistributed
    dataparallel`。在这个实验中，第一个任务耗时12041秒完成，第二个任务耗时4179秒完成，从而在训练时间上提高了65.29%。
- en: Note
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Comparison of the two training jobs with and without `smdistributed dataparallel`
    enabled is captured in the notebook in the GitHub repo: [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub仓库中的笔记本中记录了启用和未启用`smdistributed dataparallel`的两种训练任务的比较：[https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb)。
- en: SageMaker distributed model parallel library
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SageMaker分布式模型并行库
- en: Next, let's look into the SageMaker distributed model parallel library. This
    provides the capability to train large, complex deep learning models that can
    potentially increase prediction accuracy. The library automatically and efficiently
    splits a model across multiple GPUs, providing an option for both manual and automatic
    partitioning. It further coordinates training through a pipelined execution by
    building an efficient computation schedule where different nodes can simultaneously
    work on forward and backward passes for different data samples.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看SageMaker分布式模型并行库。这个库提供了训练大型、复杂的深度学习模型的能力，这可能会提高预测精度。该库自动且高效地将模型分割到多个GPU上，提供了手动和自动分区选项。它进一步通过构建高效的计算调度来协调训练，使得不同的节点可以同时为不同的数据样本执行前向和反向传递。
- en: 'The following code block shows creating an `estimator` object using a `PyTorch`
    container with the model parallel strategy enabled:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块展示了使用启用模型并行的`PyTorch`容器创建一个`estimator`对象：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As with the data parallel strategy, there are a few code changes necessary
    to the training script. Important changes are discussed in the next few code blocks:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据并行策略一样，训练脚本需要进行一些代码更改。重要的更改将在接下来的几个代码块中讨论：
- en: 'First, import and initialize the SageMaker distributed library:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入并初始化SageMaker分布式库：
- en: '[PRE7]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, wrap the model artifact in the `DistributedModel` class from the distributed
    library, and wrap the optimizer in the `DistributedOptimizer` class:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将模型工件包装在分布式库中的`DistributedModel`类中，并将优化器包装在`DistributedOptimizer`类中：
- en: '[PRE8]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, add the forward and backward logic to a function and decorate it with
    `smp.step`:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将前向和反向逻辑添加到一个函数中，并用`smp.step`进行装饰：
- en: '[PRE9]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, call the `fit()` method on the `estimator` object to kick off training:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在`estimator`对象上调用`fit()`方法以启动训练：
- en: '[PRE10]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Important Note
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'An example notebook that provides a complete walk-through of using the `ModelParallel`
    distribution strategy with a PyTorch container is provided in the GitHub repository:
    [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb).'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在GitHub仓库中提供了一个示例笔记本，它详细介绍了如何使用`ModelParallel`分布策略与PyTorch容器一起使用：[https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb)。
- en: 'While the SageMaker distributed model parallel library makes it easy to implement
    model parallel distributed training, for optimal training results consider the
    following best practices:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SageMaker分布式模型并行库使得实现模型并行分布式训练变得容易，但要获得最佳的训练结果，请考虑以下最佳实践：
- en: '**Using manual versus auto-partitioning**: You can partition the model onto
    multiple nodes (or devices) using either manual or auto-partitioning. While both
    of the options are supported, you should choose auto-partitioning over the manual
    approach. With auto-partitioning, training operations and modules that share the
    same parameters will automatically be placed on the same device for correctness.
    With a manual approach, you will have to take care of the details on how to split
    up the model parts, and which part should be placed on which device. This is a
    time-consuming and error-prone process.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用手动分区与自动分区**：你可以使用手动或自动分区将模型分区到多个节点（或设备）。虽然两种方法都受支持，但你应该选择自动分区而不是手动方法。使用自动分区时，训练操作和共享相同参数的模块将自动放置在相同的设备上以确保正确性。使用手动方法时，你必须注意如何分割模型部分以及哪个部分应该放置在哪个设备上。这是一个耗时且容易出错的过程。'
- en: '**Choosing the batch size**: The model parallel library is most efficient with
    large batch sizes. In case you start with a smaller batch size to fit the model
    into a single node, then decide to implement model parallelism across multiple
    nodes, you should increase the batch size accordingly. Model parallelism saves
    memory for large models, allowing training with large batch sizes.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择批次大小**：模型并行库在大型批次大小下效率最高。如果你开始时使用较小的批次大小以将模型放入单个节点，然后决定在多个节点上实现模型并行，你应该相应地增加批次大小。模型并行可以节省大型模型的内存，允许使用大型批次大小进行训练。'
- en: '**Choosing the number and size of micro-batches**: The model parallel library
    executes each micro-batch sequentially in each node or device. So, the micro-batch
    size should be large enough to fully utilize each GPU. At the same time, pipeline
    efficiency increases with the number of micro-batches, so balancing the two is
    important.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择微批次的数量和大小**：模型并行库在每个节点或设备上顺序执行每个微批次。因此，微批次的大小应该足够大，以便充分利用每个GPU。同时，随着微批次数量的增加，管道效率也会提高，因此平衡两者很重要。'
- en: It is best practice to start with two or four micro-batches and increase the
    batch size according to the available memory of the node/device. Then experiment
    with larger batch sizes and increase the number of micro-batches. As the number
    of micro-batches is increased, larger batch sizes might become feasible if an
    interleaved pipeline is used.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是从两个或四个微批次开始，并根据节点/设备的可用内存增加批次大小。然后尝试更大的批次大小，并增加微批次数量。随着微批次数量的增加，如果使用交错管道，更大的批次大小可能变得可行。
- en: Incremental training
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逐步训练
- en: When huge volumes of data are available upfront before training your model,
    distributed training strategies should be used. But what happens when a trained
    model is deployed and then you collect new data that might improve the model predictions?
    In this situation, you can incrementally train a new model starting with artifacts
    from an existing model and using an expanded dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当在训练模型之前就已经有大量数据可用时，应该使用分布式训练策略。但是，当模型部署后，你收集了可能改进模型预测的新数据时会发生什么？在这种情况下，你可以从现有模型的工件开始，使用扩展的数据集逐步训练一个新的模型。
- en: 'Incremental training can save training time, resources, and costs in the following
    situations:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步训练可以在以下情况下节省训练时间、资源和成本：
- en: An existing model is under-performing and new data becomes available that can
    potentially improve model performance.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个现有模型表现不佳，并且有新的数据可用，这些数据有可能提高模型性能。
- en: You want to use publicly available models as a starting point for your model
    without having to train from scratch.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望将公开可用的模型作为你模型的起点，而无需从头开始训练。
- en: You want to train multiple versions of a model, with either different hyperparameters
    or using different datasets.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望训练多个版本的模型，这些模型要么具有不同的超参数，要么使用不同的数据集。
- en: You want to restart a previously stopped training job, without having to start
    from scratch again.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望重新启动之前停止的训练作业，而无需再次从头开始。
- en: Additionally, to complement or substitute for loading existing model weights
    and incrementally training, you can retrain on a sliding window on the most recent
    data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了补充或替代加载现有模型权重和逐步训练，你可以在最近的数据上滑动窗口重新训练。
- en: In this section, you learned how to use SageMaker capabilities to train with
    large volumes of data and complex model architectures. Besides the training data
    and model architecture, a critical part of ML training is tuning hyperparameters
    of the ML algorithm. In the next section, you will learn the best practices for
    using SageMaker to handle model tuning at scale.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您学习了如何使用 SageMaker 功能进行大量数据和复杂模型架构的训练。除了训练数据和模型架构之外，ML 训练的一个关键部分是调整 ML
    算法的超参数。在下一节中，您将学习使用 SageMaker 进行大规模模型调优的最佳实践。
- en: Automated model tuning with SageMaker hyperparameter tuning
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker 超参数调优进行自动化模型调优
- en: '**Hyperparameter tuning** (**HPT**) helps you find the right parameters to
    use with your ML algorithm or the neural network to find an optimal version of
    the model. Amazon SageMaker supports managed hyperparameter tuning, also called
    **automatic model tuning**. In this section, we discuss the best practices to
    consider while configuring hyperparameter jobs on Amazon SageMaker.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数调优**（**HPT**）帮助您找到与您的 ML 算法或神经网络一起使用的正确参数，以找到模型的最佳版本。Amazon SageMaker
    支持托管超参数调优，也称为 **自动模型调优**。在本节中，我们将讨论在 Amazon SageMaker 上配置超参数作业时应考虑的最佳实践。'
- en: To execute a SageMaker hyperparameter tuning job, you specify a set of hyperparameters,
    a range of values to explore for each hyperparameter, and an objective metric
    to measure the model's performance. Automatic tuning executes multiple training
    jobs on your training dataset with the ML algorithm and the hyperparameter values
    to find the best-performing model as measured by the objective metric.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行 SageMaker 超参数调优作业，您需要指定一组超参数、每个超参数要探索的值范围以及用于衡量模型性能的目标指标。自动调优会在您的训练数据集上执行多个训练作业，使用
    ML 算法和超参数值，以目标指标衡量找到最佳性能的模型。
- en: 'In the following code blocks, we will see how to create an HPT job on SageMaker:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们将看到如何在 SageMaker 上创建一个 HPT 作业：
- en: 'First, initialize the hyperparameter names and range of values for each hyperparameter
    you want to explore:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，初始化您想要探索的每个超参数的名称和值范围：
- en: '[PRE11]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, configure the SageMaker `estimator` object:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，配置 SageMaker `estimator` 对象：
- en: '[PRE12]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, configure the `HyperparameterTuner` object:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，配置 `HyperparameterTuner` 对象：
- en: '[PRE13]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, call the `fit()` method on the `tuner` object:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在 `tuner` 对象上调用 `fit()` 方法：
- en: '[PRE14]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once the hyperparameter job is completed, you can view the different training
    jobs executed by SageMaker, along with the objective metric for each job, in *Figure
    6.4*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦超参数作业完成，您可以在 *图 6.4* 中查看 SageMaker 执行的不同训练作业，以及每个作业的目标指标：
- en: '![Figure 6.4 – SageMaker HPT results'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4 – SageMaker HPT 结果'
- en: '](img/B17249_06_04.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_06_04.jpg)'
- en: Figure 6.4 – SageMaker HPT results
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – SageMaker HPT 结果
- en: 'You can dive further into each of the training jobs to view the exact values
    of the hyperparameters used, as shown in *Figure 6.5*:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以进一步深入了解每个训练作业，查看使用的超参数的确切值，如图 6.5 所示：
- en: '![Figure 6.5 – Hyperparameter values for a specific training job'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5 – 特定训练作业的超参数值'
- en: '](img/B17249_06_05.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_06_05.jpg)'
- en: Figure 6.5 – Hyperparameter values for a specific training job
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 特定训练作业的超参数值
- en: Important Note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'An example notebook that provides a complete walk-through of using SageMaker
    HPT, along with analysis of results, is provided in the GitHub repository: [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/HPO.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/HPO.ipynb).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GitHub 仓库中提供了一个示例笔记本，它提供了使用 SageMaker HPT 的完整教程，以及结果分析：[https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/HPO.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/HPO.ipynb)。
- en: 'Now that you know the basics, let''s discuss some of the best practices to
    consider while configuring hyperparameter jobs on Amazon SageMaker:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了基础知识，让我们讨论在 Amazon SageMaker 上配置超参数作业时应考虑的一些最佳实践：
- en: '**Selecting a small number of hyperparameters**: HPT is a computationally intensive
    task, the computational complexity being proportional to the number of hyperparameters
    you want to tune. SageMaker allows you to specify up to 20 hyperparameters to
    optimize for a tuning job but limiting your search to a smaller number is likely
    to give you better results.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择少量超参数**：HPT 是一个计算密集型任务，其计算复杂性与您想要调整的超参数数量成正比。SageMaker 允许您为调优作业指定最多 20
    个超参数进行优化，但将搜索范围限制在更小的数量可能会给您带来更好的结果。'
- en: '**Selecting a small range for hyperparameters**: Along the same lines, the
    range of values for hyperparameters can significantly affect the success of hyperparameter
    optimization. Intuitively, you may want to specify a very large range to explore
    all possible values for a hyperparameter, but you will in fact get better results
    by limiting your search to a small range of values.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为超参数选择小范围**：同样，超参数的值范围可以显著影响超参数优化的成功。直观上，你可能想指定一个非常大的范围来探索超参数的所有可能值，但实际上，通过限制搜索到小范围的值，你会得到更好的结果。'
- en: '`train_batch_size` hyperparameter, instead of exploring a range in a linear
    fashion, you might want only to evaluate the two values–128 and 256\. In this
    case, you treat the parameter as a categorical value. In contrast, if you want
    to explore the values for the `train_batch_size` hyperparameter in a range from
    a minimum threshold value of 128 to a maximum threshold value of 256, you will
    use the `Integer` type. The `Integer` type allows for greater exploration of the
    range.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_batch_size` 超参数，而不是以线性方式探索一个范围，你可能只想评估两个值——128 和 256。在这种情况下，你将参数视为一个分类值。相比之下，如果你想探索
    `train_batch_size` 超参数的范围，从最小阈值值 128 到最大阈值值 256，你将使用 `Integer` 类型。`Integer` 类型允许对范围进行更广泛的探索。'
- en: If you search a range that spans several orders of magnitude, you can optimize
    the search by choosing a logarithmic scale for `Integer` hyperparameters. Finally,
    choose a continuous parameter if the range of all values to explore, from the
    lowest to the highest, is relatively small. For example, exploring the `learning_rate`
    hyperparameter in the range of `0.0001` and `0.0005` at a linear scale.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你搜索的范围跨越几个数量级，你可以通过为 `Integer` 超参数选择对数尺度来优化搜索。最后，如果所有要探索的值的范围（从最低到最高）相对较小，请选择连续参数。例如，在
    `0.0001` 和 `0.0005` 的范围内以线性尺度探索 `learning_rate` 超参数。
- en: '**Enabling warm start**: SageMaker HPT supports warm start, which reuses results
    from one or more prior tuning jobs as a starting point. Configure your HPT job
    to use warm start to limit the combinations of hyperparameters to search over
    in the new tuning job. This results in a faster tuning job. Warm start is particularly
    useful when you want to change the HPT ranges from the previous job or add new
    hyperparameters.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启用预热启动**：SageMaker HPT 支持预热启动，它将重用先前调优作业的结果作为起点。配置你的 HPT 作业使用预热启动以限制新调优作业中要搜索的超参数组合。这导致调优作业更快。预热启动在你想更改先前作业的
    HPT 范围或添加新超参数时特别有用。'
- en: '**Enabling early stop to save tuning time and costs**: With early stop enabled,
    the individual training jobs launched by the HPT job will terminate early when
    the objective metric is not improving significantly. After each epoch of training,
    a running average of the objective metric for all the previous training jobs up
    to the same epoch is determined and the median of running averages is calculated.
    If the value of the objective metric for the current training job is worse than
    the median value, SageMaker stops the current training job.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启用早期停止以节省调优时间和成本**：启用早期停止后，由 HPT 作业启动的个别训练作业将在目标指标没有显著改善时提前终止。在每个训练周期后，确定所有之前训练作业到同一周期的目标指标的运行平均值，并计算运行平均值的中间值。如果当前训练作业的目标指标值比中间值差，SageMaker
    将停止当前训练作业。'
- en: Stopping jobs early reduces the overall compute time and thereby the cost of
    the job. An additional benefit is that early stopping helps prevent overfitting.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提前停止作业可以减少总体计算时间，从而降低作业成本。另一个好处是，早期停止有助于防止过拟合。
- en: '`MaxParallelTrainingJobs` parameter. On one hand, running more HPT jobs concurrently
    completes the tuning job quickly. On the other, a tuning job can only find better
    combinations of hyperparameters through successive rounds of experiments. In the
    long run, executing a single training job at a time gives the best results with
    minimum computation time.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxParallelTrainingJobs` 参数。一方面，同时运行更多的 HPT 作业可以快速完成调优作业。另一方面，调优作业只能通过连续的实验轮次找到更好的超参数组合。从长远来看，一次执行一个训练作业可以获得最佳结果，同时计算时间最短。'
- en: This is the case when the default **Bayesian** optimization tuning strategy
    is used by SageMaker HPO. However, if you have experience with your algorithm
    and dataset, you can also use the random search strategy natively supported by
    SageMaker, since it enables concurrency but doesn't require serial rounds of experiments.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当SageMaker HPO使用默认的**贝叶斯**优化调优策略时，情况就是这样。然而，如果您对您的算法和数据集有经验，您也可以使用SageMaker原生支持的随机搜索策略，因为它可以实现并发，但不需要进行实验的串行轮次。
- en: 'While in this section we focused on a single algorithm for best practice. The
    `CreateHyperParameterTuningJob` API can also be used to tune multiple algorithms
    by providing multiple training job definitions pointing to the different algorithms.
    For a detailed explanation of this API, see the following article: [https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateHyperParameterTuningJob.html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateHyperParameterTuningJob.html).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在本节中我们专注于单个算法的最佳实践，但`CreateHyperParameterTuningJob` API也可以通过提供指向不同算法的多个训练作业定义来调整多个算法。有关此API的详细说明，请参阅以下文章：[https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateHyperParameterTuningJob.html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateHyperParameterTuningJob.html)。
- en: In the next section, you will learn how to keep track of all your ML experiments
    related to solving a specific problem.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习如何跟踪与解决特定问题相关的所有机器学习实验。
- en: Organizing and tracking training jobs with SageMaker Experiments
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker Experiments组织和跟踪训练作业
- en: A key challenge ML practitioners face is keeping track of the myriad ML experiments
    that need to be executed before a model achieves desired results. For a single
    ML project, it is not uncommon for data scientists to routinely train several
    different models looking for improved accuracy. HPT adds more training jobs to
    these experiments. Typically, there are many details to track for experiments
    such as hyperparameters, model architectures, training algorithms, custom scripts,
    metrics, result artifacts, and more.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习从业者面临的一个关键挑战是跟踪在模型达到预期结果之前需要执行的众多机器学习实验。对于单个机器学习项目来说，数据科学家通常需要定期训练几个不同的模型以寻找提高准确度的方法并不罕见。HPT向这些实验添加了更多的训练任务。通常，实验有许多细节需要跟踪，如超参数、模型架构、训练算法、自定义脚本、指标、结果工件等。
- en: In this section, we will discuss **Amazon SageMaker Experiments**, which allows
    you to organize, track, visualize, and compare ML models across all phases of
    the ML lifecycle, including feature engineering, model training, model tuning,
    and model deploying. SageMaker Experiments' capability tracks model lineage, allowing
    you to troubleshoot production issues and audit your models to meet compliance
    requirements.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论**Amazon SageMaker Experiments**，它允许您在机器学习生命周期的所有阶段组织、跟踪、可视化和比较机器学习模型，包括特征工程、模型训练、模型调优和模型部署。SageMaker
    Experiments的功能可以跟踪模型血缘，让您能够排查生产问题并审计您的模型以满足合规性要求。
- en: 'Basic components that make up Amazon SageMaker Experiments include an experiment,
    a trial, a trial component, and a tracker, as shown in *Figure 6.6*:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 构成Amazon SageMaker Experiments的基本组件包括一个实验、一个试验、一个试验组件和一个跟踪器，如图6.6所示：
- en: '![Figure 6.6 – Amazon SageMaker Experiments overview'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 – Amazon SageMaker Experiments概览'
- en: '](img/B17249_06_06.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_06_06.jpg)'
- en: Figure 6.6 – Amazon SageMaker Experiments overview
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – Amazon SageMaker Experiments概览
- en: 'Let''s look at each component:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每个组件：
- en: '**Experiment**: An experiment encapsulates all related components that represent
    the ML problem you are attempting to solve. Each experiment is a collection of
    trials, with the goal of determining the trial that produces the best model.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实验**: 实验封装了代表您试图解决的机器学习问题的所有相关组件。每个实验是一系列试验的集合，目标是确定产生最佳模型的试验。'
- en: '**Trial**: A trial represents a single attempt at solving the ML problem that
    captures the end-to-end ML process within an experiment. Each trial is a collection
    consisting of several trial components.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**试验**: 试验代表了解决机器学习问题的一次尝试，它在一个实验中捕捉了端到端的机器学习过程。每个试验由几个试验组件组成。'
- en: '**Trial Component**: A trial component represents a specific step within a
    given trial. For example, the data preprocessing step could be one trial component,
    and model training could be another trial component.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**试验组件**: 试验组件代表给定试验中的特定步骤。例如，数据预处理步骤可以是其中一个试验组件，而模型训练可以是另一个试验组件。'
- en: '**Tracker**: A tracker is used to track metadata of individual trial components,
    including all parameters, inputs, outputs, artifacts, and metrics. Since this
    metadata is tracked and persisted, you can link the final model artifact to its
    origin.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tracker**：Tracker用于跟踪单个试验组件的元数据，包括所有参数、输入、输出、工件和指标。由于这些元数据被跟踪和持久化，你可以将最终模型工件与其来源链接起来。'
- en: 'In the following code blocks, we will see how to create a SageMaker experiment:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们将看到如何创建SageMaker实验：
- en: 'First, create an experiment:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个实验：
- en: '[PRE15]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, create a `Tracker` instance to track the `Training` stage:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个`Tracker`实例以跟踪`训练`阶段：
- en: '[PRE16]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, define experiment variables to define what you want to change to see how
    your objective is affected. In this example, we will experiment with several values
    for the number of the `max_depth` hyperparameter of `XGBoostmodel`. We will create
    a trial to track each training job run.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，定义实验变量以定义你想要更改的内容，以了解你的目标是如何受到影响的。在这个例子中，我们将对`XGBoostmodel`的`max_depth`超参数的几个值进行实验。我们将创建一个试验来跟踪每个训练作业的运行。
- en: 'We will also create a `TrialComponent` instance from the `Tracker` instance
    we created earlier and add this to the `Trial` instance. This will allow you to
    capture metrics from the training step as follows:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还将从之前创建的`Tracker`实例创建一个`TrialComponent`实例，并将其添加到`Trial`实例中。这将允许你捕获训练步骤的指标，如下所示：
- en: '[PRE17]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When running the training job with the `fit()` method, associate `estimator`
    with the experiment and trial:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当使用`fit()`方法运行训练作业时，将`estimator`与实验和试验关联起来：
- en: '[PRE18]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, after the experiment is completed, let''s analyze the experiment results:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在实验完成后，让我们分析实验结果：
- en: '[PRE19]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Figure 6.7* shows a list of all the trial components that were created as
    part of the experiment:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.7* 显示了作为实验一部分创建的所有试验组件列表：'
- en: '![Figure 6.7 – Trial components from the experiment'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7 – 实验中的试验组件'
- en: '](img/B17249_06_07.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17249_06_07.jpg](img/B17249_06_07.jpg)'
- en: Figure 6.7 – Trial components from the experiment
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 实验中的试验组件
- en: As you can see from this section, a SageMaker experiment gives you a way to
    organize your efforts toward an ML goal and allows visibility into several important
    aspects of those efforts. A best practice we recommend is that any time you launch
    a training or tuning job, wrap it in an experiment. This allows you to gain visibility
    into the training and tuning jobs without any additional cost.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，SageMaker实验为你提供了一种组织你向机器学习目标努力的方法，并允许你了解这些努力的几个重要方面。我们推荐的一个最佳实践是，每次你启动训练或调优作业时，都将其包裹在一个实验中。这样，你可以在没有任何额外成本的情况下获得对训练和调优作业的可见性。
- en: Important note
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: 'An example notebook that provides a complete walk-through of using SageMaker
    Experiments is provided in the GitHub repository: [https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/Experiments.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/Experiments.ipynb).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub仓库中提供了一个示例笔记本，提供了使用SageMaker Experiments的完整教程：[https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/Experiments.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/Experiments.ipynb)。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned the advanced techniques required to train models
    at scale using different distribution strategies. You further reviewed best practices
    for hyperparameter tuning to find the best version of the model to meet your objectives.
    You learned how to organize and track multiple experiments conducted in a typical
    ML workflow and create comparison reports.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了使用不同分布策略在规模上训练模型所需的高级技术。你进一步了解了超参数调优的最佳实践，以找到满足你目标的最佳模型版本。你学习了如何在典型的机器学习工作流程中组织和管理多个实验，并创建比较报告。
- en: Using the SageMaker capabilities and best practices discussed in this chapter,
    you can tackle ML at scale, allowing your organization to move out of the experimentation
    phase. You can take advantage of large datasets collected over years, and move
    toward realizing the full benefits of ML. In the next chapter, you will continue
    to enhance ML training by profiling training jobs using **Amazon SageMaker** **Debugger**.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本章中讨论的SageMaker功能和最佳实践，你可以处理大规模机器学习，使你的组织摆脱实验阶段。你可以利用多年来收集的大量数据集，并朝着实现机器学习的全部好处迈进。在下一章中，你将继续通过使用**Amazon
    SageMaker** **调试器**来分析训练作业来增强机器学习训练。
- en: References
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For additional reading material, please review these references:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如需阅读更多参考资料，请查阅以下内容：
- en: '*Learn Amazon SageMaker: A guide to building, training, and deploying ML models
    for developers and data scientists*:'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习 Amazon SageMaker：开发者与数据科学家构建、训练和部署机器学习模型的指南*:'
- en: '`https://www.amazon.com/Learn-Amazon-SageMaker-developers-scientists/dp/180020891X/ref=sr_1_1?dchild=1&keywords
    =Learn+Amazon+SageMaker+%3A+A+guide+to+building%2C+training%2C+and+deploying+machine+learning+models+for
    +developers+and+data+scientists&qid=1624801601&sr=8-1`'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`https://www.amazon.com/Learn-Amazon-SageMaker-developers-scientists/dp/180020891X/ref=sr_1_1?dchild=1&keywords
    =Learn+Amazon+SageMaker+%3A+A+guide+to+building%2C+training%2C+and+deploying+machine+learning+models+for
    +developers+and+data+scientists&qid=1624801601&sr=8-1`'
- en: '*Multi-GPU and distributed training using Horovod in Amazon SageMaker Pipe
    mode*:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在 Amazon SageMaker Pipe 模式下使用 Horovod 进行多 GPU 和分布式训练*:'
- en: '[https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/](https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/)'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/](https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/)'
- en: '*Streamline modeling with Amazon SageMaker Studio and the Amazon Experiments
    SDK*:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 Amazon SageMaker Studio 和 Amazon Experiments SDK 简化建模*:'
- en: https://aws.amazon.com/blogs/machine-learning/streamline-modeling-with-amazon-sagemaker-studio-and-amazon-experiments-sdk
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: https://aws.amazon.com/blogs/machine-learning/streamline-modeling-with-amazon-sagemaker-studio-and-amazon-experiments-sdk
