- en: Deep Learning Using TensorFlow on the IBM Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will provide an introduction to the concepts of deep learning
    and neural networks on the IBM Cloud. An overview of how to use the TensorFlow
    framework to implement deep learning models on the cloud will be provided as well.
    This chapter is designed to be a balance between theory and practical implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow and image classifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning (also known as **deep structured learning** or **hierarchical
    learning**) is part of a larger group of machine learning approaches based on
    learning data representations, as opposed to task-specific algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Learning can be supervised (which we covered in [Chapter 3](b2822c69-13f0-4943-9e66-f9ef04898b60.xhtml),
    *Supervised Machine Learning Models and Your Data*), semi-supervised, or unsupervised
    (covered in [Chapter 4](f131f753-1d77-478c-9c0d-1e799330eed8.xhtml), *Implementing
    Unsupervised Algorithms*).
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning algorithms are at work in exciting areas such as image classification
    (categorizing every pixels in a digital image into one of several land cover classes,
    or themes), object detection (the process of finding instances of real-world objects
    such as faces, cars, and buildings in images or videos), image restoration (to
    compensate for, or undo, defects caused by motion blur, noise, and camera misfocus,
    which degrade an image) and image segmentation (the process of partitioning a
    digital image into multiple segments of pixels, also known as **super-pixels**,
    to simplify and/or change the representation of an image into something that is
    more meaningful and easier to analyze).
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning using enormous neural networks is teaching machines to automate
    the tasks performed by human visual systems.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning models are vaguely inspired by information processing and communication
    patterns in biological nervous systems, yet they do differ from the structural
    and functional properties of biological brains, which make them incompatible with
    neuroscience evidences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enough theory. While the preceding explanation of machine/deep learning may
    be high-level, it is sufficient enough for us to move on to the next section,
    where we start thinking about the means of deep learning implementation, specifically
    using a toolset developed by the Google Brain team for internal Google use, under
    the Apache 2.0 open source license on November 9, 2015: TensorFlow.'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tensors can be thought to be generalized matrixes or, more specifically, mathematical
    entities living in structures and interacting with other mathematical entities.
    If the other entities in the structure are transformed in any way, then the tensor
    must also be transformed by that transformation rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does the preceding definition mean? Perhaps thinking of a **Tensor** as
    multidimensional array is easier to grasp, or consider the following, comparing
    **Scalar**, **Vector**, **Matrix**, and **Tensor**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6aa4e189-4fb1-4e5d-84b3-2ad53319d2b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Building on the topic of TensorFlow, TensorFlow is an open source software library
    (also called a **framework**) originally created by Google for creating deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: You can visit [https://www.tensorflow.org/](https://www.tensorflow.org/) for
    more details on TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will talk about the connection between deep learning,
    neural networks, and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks and TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning models typically employ algorithms known as **neural networks**,
    which are said to be inspired by the way actual biological nervous systems (such
    as the brain) process information. It enables computers to recognize all data
    points as to what each represents and learn patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Today, the principal software tool for deep learning models is TensorFlow as
    it permits developers to create large-scale neural networks with numerous layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow is mainly used for the following purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As noted in the Watson documentation, the challenge with deploying complex machine
    learning models such as a TensorFlow model is that these models are very computationally
    expensive and time-consuming to train. Some solutions (to this challenge) include
    GPU acceleration, distributed computing, or a combination of both. The IBM Cloud
    platform and Watson Studio offers both of these.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also points: IBM Watson Studio permits one to leverage the computational
    power available on the cloud to speed up the training time of the more complex
    machine learning models, and thus reduce the time from hours or days, down to
    minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will explore several exercises demonstrating various
    ways of using TensorFlow with IBM Watson Studio.
  prefs: []
  type: TYPE_NORMAL
- en: An example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will start by stepping through a Watson Community ([https://dataplatform.cloud.ibm.com/community](https://dataplatform.cloud.ibm.com/community))
    tutorial, designed to demonstrate how easy it is to deploy a deep neural network
    using the TensorFlow libraries on IBM Watson Studio.
  prefs: []
  type: TYPE_NORMAL
- en: The tutorial is available on GitHub for download, but we won't provide the URL
    here because we will demonstrate how easy it is to simply import content from
    external sources (such as GitHub) from directly within a IBM Watson Studio project.
  prefs: []
  type: TYPE_NORMAL
- en: This exercise's key point is that complex machine learning models can be computationally
    thirsty, but IBM Watson Studio gives you the opportunity to easily and efficiently
    (pay as you go) use the computational power available on the cloud to speed up
    processing time and reduce the time it takes to learn from hours, or days, down
    to minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, IBM Watson Studio provides all the tools essential to develop
    a data-centric solution in the cloud. It makes use of Apache Spark clusters (for
    computational power) and lets you create assets in Python, Scala, and R, and leverage
    open source frameworks (such as TensorFlow), all of which are already installed
    on Watson Studio.
  prefs: []
  type: TYPE_NORMAL
- en: If you take the time to read through the details of the tutorial, you will see
    that it explains how to create a new IBM Cloud account and sign up for IBM Watson
    Studio (which we already covered in [Chapter 1](07c92a06-635f-41ef-b2be-3654ba90b790.xhtml),
    *Introduction to IBM Cloud*).
  prefs: []
  type: TYPE_NORMAL
- en: The tutorial then goes on to show how to navigate to IBM's Watson Studio (once
    on the IBM Cloud platform), create a new project, and then import a notebook to
    the project.
  prefs: []
  type: TYPE_NORMAL
- en: Although, in earlier chapters, we showed how to create a new project in Watson
    Studio and create new notebooks, this will be the first time we do a notebook
    import (directly from an external URL), so the next sections will focus on how
    that process works.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook that is to be imported will already contain the TensorFlow libraries
    and example code, so this exercise should be both quick and super easy for us,
    so let's not waste any more time!
  prefs: []
  type: TYPE_NORMAL
- en: Creating the new project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Taking the same steps that we followed in earlier chapters, we can create a
    new deep learning IBM Watson Studio project (see the following screenshot) and
    give it a name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab396f17-1f49-4b85-aa27-166db71fb636.png)'
  prefs: []
  type: TYPE_IMG
- en: Notebook asset type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the new project is created, from the project dashboard, you can click
    the Add to project option and select Notebook, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4ae4435-6b3f-47e4-a4f5-ef618a9f6793.png)'
  prefs: []
  type: TYPE_IMG
- en: As we have done in preceding chapters, we could create a new, empty notebook.
    But for this exercise we want to import an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: IBM Watson Studio allows you to import notebooks from either a file or directly
    from a known and accessible URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we will choose to import from a URL. To do that, you select the
    From URL option and type or paste in the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/aounlutfi/building-first-dl-model/blob/master/first-dl-model.ipynb](https://github.com/aounlutfi/building-first-dl-model/blob/master/first-dl-model.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding link will be the (external) URL of the notebook to be imported
    into your Watson Studio project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, click on Create Notebook to begin the import (it should only take a few
    seconds, providing you have access to the URL), as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0793073b-1eea-4389-b611-fe74e691b61f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After a few seconds, the notebook opens and is ready for review and execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01d88983-414f-4ffd-9100-de56f3c23d4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the imported notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the imported notebook, click on Cell from the commands ribbon, and then
    click Run All:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e080b2d-d4f3-44a1-b2b7-e0c7c1e3cec4.png)'
  prefs: []
  type: TYPE_IMG
- en: After clicking Run All, IBM Watson Studio will then run all the cells within
    the notebook, which should take (about) fifteen minutes or so to complete, since
    the data set is made up of 20,000 images.
  prefs: []
  type: TYPE_NORMAL
- en: You can also run each cell individually, if you want to better understand what
    is going on.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you take the time (and you should) to look through the cells contained in
    the notebook, you will notice that there are plenty of markdown cells that explicitly
    describe the steps within the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you should take note of the markdown cell labeled Imports (as
    shown in the following screenshot) where it clearly states In order to be able
    to build, test, and run a NN in TensorFlow, the following imports have to be used.
    This also imports the MNIST data set (each point in the data set is a handwritten
    representation of the digits 0-9 in 784 pixels):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/338b3dd5-be69-473b-9969-6823826befeb.png)'
  prefs: []
  type: TYPE_IMG
- en: The tutorial also makes a point by challenging you to attempt to set up Python
    and TensorFlow on a local computer (not on the cloud using IBM Watson Studio)
    and run the example so that you can compare the results, noting that it may take
    hours, maybe even days to train, depending on the performance of the machine,
    and that is after you have assembled the required environment!
  prefs: []
  type: TYPE_NORMAL
- en: In the next example, we will cover using IBM Watson Studio with Watson services
    and the TensorFlow API to perform image classifications and object detection.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow and image classifications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The IBM Watson Visual Recognition service uses deep learning algorithms to identify
    features like scenes, objects, and faces within images you upload to the service.
    You can also create and train custom classifiers to identify subjects that meet
    your requirements, using the Visual Recognition service, IBM Watson Studio, and
    related Python modules.
  prefs: []
  type: TYPE_NORMAL
- en: To get started with Visual Recognition, we'll need to use the usual procedure
    to create a Watson Studio project and define a new Python 3.5 Notebook, but we
    will also need to associate an IBM Watson Visual Recognition service instance
    with the project (a pretty easy thing to do as it turns out).
  prefs: []
  type: TYPE_NORMAL
- en: Adding the service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To add the Watson Visual Recognition service, you need to go to the IBM Cloud
    Dashboard and select Watson Services, then Browse Services, where you can then
    find and select the Visual Recognition service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f3c736d-6d73-4ee5-9dd4-925f4fa61f6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, from the Visual Recognition page (which is shown in the following screenshot),
    you can choose a location and resource group for the service instance and then
    click on Create to actually create the instance of the service that you can use
    in your project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fac7444-0a3b-4002-acee-a22511d01131.png)'
  prefs: []
  type: TYPE_IMG
- en: A Visual Recognition service instance may only be associated with one project
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have created the Visual Recognition service instance, it will be listed
    on your cloud dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76a53786-2282-468a-8ca4-61171883a0a6.png)'
  prefs: []
  type: TYPE_IMG
- en: After creating the service instance, you should be able to create what is referred
    to as Service Credentials (or the API key) by clicking on New Credentials or View
    Credentials in the Service credentials section.
  prefs: []
  type: TYPE_NORMAL
- en: You will need this API key to refer to and use the instance within the project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Service credentials page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e779fd2e-0867-413b-8977-8719feab630c.png)'
  prefs: []
  type: TYPE_IMG
- en: We will need to refer to this API key in the next few sections of this chapter,
    so keep it handy!
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to get going with the actual Visual Recognition project. In
    this example (a version is available on GitHub), the Watson Visual Recognition
    service is used to perform object detection using the TensorFlow object detection
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Required modules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although many pre-installed libraries and modules are already included as part
    of an IBM Watson Notebook environment (depending upon the type of notebook selected),
    various other modules and frameworks may still need to be installed for a particular
    project to work correctly, so the first thing we should do in our notebook is
    to type the following command to see the list of pre-installed libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It is a very good idea to always begin a new project by using this command to
    gain an understanding of your notebook's environment; it may save time later.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then use the Python `!pip install` command to install each of this
    project''s required modules line by line, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'An alternative and more efficient way to install the required modules with
    one command is to reference the provided `requirements.txt` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Using the API key in code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The IBM Watson Visual Recognition service comes with built-in models that you
    can use to analyze images for scenes, objects, faces, and many other categories
    without any training. We have already created an instance of the Visual Recognition
    service, so it is available to our project. For this to work, you need to have
    that valid key (our actual API key).
  prefs: []
  type: TYPE_NORMAL
- en: Even though you have working Python code in the Watson Notebook, you need to
    now use your established API key with it, so it will validate with the Watson
    service (and actually work).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look through the Python project code (which we will load in the next
    section), you will find the following code statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This line of code is where the Watson Visual Recognition service is initialized
    as an object we can use within our code. You will replace the `API_KEY` phrase
    with your actual API key (the one you created in the previous section).
  prefs: []
  type: TYPE_NORMAL
- en: Additional preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will take care of a bit more housekeeping that is required
    before we can successfully run the project.
  prefs: []
  type: TYPE_NORMAL
- en: Pillow is a **Python Imaging Library** (**PIL**), which provides support for
    opening, manipulating, and saving images. The current version identifies and reads
    a large number of formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'This project utilizes `pillow` and requires at least version 5.3.0 to be installed.
    To ensure that the notebook uses this version, we need to run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The code uninstalls the currently installed version of Pillow, installs version
    5.3.0, imports it into the project, and then prints the (now) currently installed
    version.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the final line of code indicates, if the output of the `print` command does
    not indicate that the `pillow` version installed is 5.3.0, you will need to stop
    and then restart your kernel (click on Kernel, then restart within your notebook),
    then again execute the `print` command, and you should be ready to go:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1874dbc-70a5-4042-b240-6dfb3913ccf3.png)'
  prefs: []
  type: TYPE_IMG
- en: Upgrading Watson
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When I first started experimenting with this project I ran into several difficulties
    with using the Visual Recognition service. After much debugging and some much
    appreciated help from IBM Cloud support, it was determined that the project code
    that I was using was using an older version of the cloud API.
  prefs: []
  type: TYPE_NORMAL
- en: 'To resolve the issues I was seeing, it was necessary to upgrade the Watson
    service to the latest version using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9cbeb948-5a03-4c36-a9b0-8d8f44a23ecb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To validate that the service was upgraded, you should see the following message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once the service was upgraded, all of the issues I was previously experiencing
    were resolved.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, 2.8.0 is the latest release. It is advisable to always
    check for and use the latest available version.
  prefs: []
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One last setup task to perform is to provide an image file for our solution
    to detect objects within.
  prefs: []
  type: TYPE_NORMAL
- en: The sample project code offers a picture of four dogs that you can use, but
    it's more fun to provide one or more of your own. The README notes of this project
    indicate that the code will expect the file to be located in `test_image/image1.jpg`,
    but you can simply upload it as a data asset using the same steps we did in previous
    chapters and then update the code, so it finds the file (you can change the filename
    as well).
  prefs: []
  type: TYPE_NORMAL
- en: 'I chose to use the following three different images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2450e5f-fefe-4018-991d-603cdc52aad6.png)'
  prefs: []
  type: TYPE_IMG
- en: Code examination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, our environment should be ready for the main code section. Let's
    now look at each section of that code to understand its purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first section performs various additional imports. Take specific note of
    the line of code that imports the Visual Recognition service (`VisualRecognitionV3`)
    from the (now upgraded) `watson_developer_cloud`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The next line of code uses our previously mentioned API key (you''ll use your
    own):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The next section contains variables that you can experiment with when you run
    the notebook. Look at the results and adjust the variables to see the effects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding commands, let''s explore each of the three variables defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MAX_NUMBER_OF_BOXES`: This variable represents the maximum number of objects
    to locate within you test image; I used `9` because it can get ugly if there are
    a lot of them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MINIMUM_CONFIDENCE`: This variable represents the minimum confidence score
    that a box can have. If this value is too low, you may end up with boxes around
    nothing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COLORS`: This variable sets the resulting identification boxes'' attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next sections of the code will download the Visual Recognition model to
    be used and then load it into memory. Downloading the model may take a few minutes
    the first time, but it only needs to be downloaded on the first run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next part of the code runs the image using the TensorFlow object detection
    API. It will provide the coordinates of the box as an array of the edge positions
    (top, left, bottom, and right). It will then crop and save the images based on
    the boxes. In order for us to crop the correct area we need to transform the coordinates
    from percentages to pixels by multiplying the values by the width and height:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: After we have saved the image portions, we can pass each of them to Watson for
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the use of the variables we set previously (`MAX_NUMBER_OF_BOXES` and
    `MINIMUM_CONFIDENCE`) in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Classification and output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assuming you perform the previously outlined environmental checks and setups
    outlined earlier in this chapter, all of the code up to this point should run
    flawlessly without producing any errors. The next section of code was updated
    from the original version of the project offered on GitHub due to changes IBM
    made to the newer version of the API.
  prefs: []
  type: TYPE_NORMAL
- en: If you use the notebook code provided with this book, the updates have already
    been made for you.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the next code section receives the results or the classifications back
    from the Watson Visual Recognition service into an object named `results`.
  prefs: []
  type: TYPE_NORMAL
- en: 'From that information, a label is then constructed, and a rectangle shape is
    drawn around each object that was detected within the source image file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If you examine the `results` object more closely (try using the Python `type`
    command on it), you will see that the results object is a Python dictionary object.
  prefs: []
  type: TYPE_NORMAL
- en: A Python dictionary object is similar to a list in that it is a collection of
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: Now try adding `print(results)` to the notebook code and you'll get a glimpse
    of the raw output returned in results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `print(results)` command, the actual output is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7b06b27-feff-47a6-82bc-e7ae5082476b.png)'
  prefs: []
  type: TYPE_IMG
- en: Objects detected
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally we are ready to use, `matplotlib`, `plt.show()` to display the current
    image that we are working on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now finally see the output of the project. In this example, our image
    was of a horse, and we can see that the Watson Visual Recognition service correctly
    detected and labeled the object as a pinto horse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71d2361b-c83c-4c62-add9-cee100a4e2ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Now the fun part
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now comes the fun part. You can download any number of files from almost anywhere
    to test the application (or create your own). I used several images and found
    that Watson was pretty accurate.
  prefs: []
  type: TYPE_NORMAL
- en: The first image used was correctly detected as a motorcycle, the second (an
    image showing two vintage cars) was close in that Watson detected one of the cars
    as a car but the other was detected as a light truck.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of the final image we already mentioned: Watson not only correctly
    detected a horse but also labeled its breed: pinto.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the three different images and their respective
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11d3605d-8eed-4e7f-af0e-79b35559d38c.png)'
  prefs: []
  type: TYPE_IMG
- en: Save and share your work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As always, once you have a working project notebook, you can click on File
    then Save to save your work. The next step is to share your notebook by clicking
    on the Share icon, as shown in the following screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/175a785c-318a-4faa-9a0a-8f43d52c38ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From there, you can select the way you want to share your notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/426faef0-dc94-41ef-90b1-1cde9e209233.png)'
  prefs: []
  type: TYPE_IMG
- en: You should get into the habit of documenting your notebooks with Markdown cell
    content before sharing it. You will be surprised how much better your work will
    be received if you add commentary and perspective to each cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, when you view your project notebooks (see the following screenshot),
    note the SHARED and STATUS icons. You can publish your notebook to various target
    environments, such as GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d53062b6-b401-4d3e-ba45-7f0b8e9f37d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with an introduction to the concepts of deep learning
    and looked at the basics of using TensorFlow libraries and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: We then walked through two IBM Watson Studio projects to illustrate how to build
    both a neural network and an object detection project using the tools and services
    provided on IBM Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will create a facial expression platform on IBM Cloud.
  prefs: []
  type: TYPE_NORMAL
