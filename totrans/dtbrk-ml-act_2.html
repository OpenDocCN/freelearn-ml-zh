<html><head></head><body>
		<div id="_idContainer060">
			<h1 class="chapter-number" id="_idParaDest-34"><a id="_idTextAnchor073"/>2</h1>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor074"/>Designing Databricks: Day One</h1>
			<p class="author-quote">“Design is not just what it looks like and feels like. Design is how it works.”</p>
			<p class="author-quote">- Steve Jobs</p>
			<p>This chapter will introduce concepts and topics that engineers, data scientists, and people in similar roles should know to set themselves up for success in the Databricks Data Intelligence Platform. When setting up your data and AI platform, in our case, Databricks, there are always best practices to follow. We share those in this chapter to give you a better understanding of the setup options and their impacts; these can be strategic decisions that impact the entire data product workflow, as well as simply matters of preference. We start by explaining Databrick’s general architecture and key terminology, then cover the most important decisions to be made during platform setup, and conclude with code examples and configurations to download the data for our example projects. We also introduce a variety of platform features and components throughout the chapter, which we will cover in more detail throughout the rest of this book. Here is what you will learn as part of <span class="No-Break">this chapter:</span></p>
			<p>Here is what you will learn about as part of <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Planning <span class="No-Break">your platform</span></li>
				<li>Defining <span class="No-Break">a workspace</span></li>
				<li>Selecting <span class="No-Break">the metastore</span></li>
				<li>Discussing <span class="No-Break">data preparation</span></li>
				<li>Planning to <span class="No-Break">create features</span></li>
				<li>Modeling <span class="No-Break">in Databricks</span></li>
				<li><span class="No-Break">Applying learning</span></li>
			</ul>
			<h1><a id="_idTextAnchor075"/></h1>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor076"/>Planning your platform</h1>
			<p>This section covers topics for <a id="_idIndexMarker043"/>discussion before and during the DI Platform setup process. The role of the data team often determines the platform setup. One of Databricks’ ideal attributes is that the technology stack is unified, making the setup and collaboration between teams more straightforward. The data team reporting structure frequently determines the border where one role ends and another begins, rather than the actual data product workflow. Luckily, we do not have to worry because the DI Platform serves data engineers, scientists, and <span class="No-Break">analysts alike.</span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em>, you can see an end-to-end lakehouse architecture and the components <span class="No-Break">in Databricks.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer023">
					<img alt="Figure 2.1 – Overview of a lakehouse architecture and how Databricks DI Platform fits this paradigm" src="image/B16865_02_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Overview of a lakehouse architecture and how Databricks DI Platform fits this paradigm</p>
			<p>The DI Platform consists of one or more Databricks accounts. Most of the time, companies only have one. However, there are situations where companies require extra environment isolation, and having separate accounts for development, staging, and production is an option. Discussion about multiple accounts for levels of isolation is outside of this book's scope, but if you have questions or want to know more, please check out the resources in <span class="No-Break"><em class="italic">Further reading</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer024">
					<img alt="Figure 2.2 – Visual representation of the environment isolation options" src="image/B16865_02_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Visual representation of the environment isolation options</p>
			<p>We separate our environments using different catalogs. Most of this book’s project work occurs using the <strong class="source-inline">ml_in_action</strong> catalog. For the production version of some models, we use the <strong class="source-inline">ml_in_prod</strong> catalog. Setting up multiple workspaces is another way to separate environments. We recommend using documentation and your company policies to guide your isolation setup. Let’s move on to what precisely a workspace is in the context <span class="No-Break">of Databricks.</span></p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor077"/>Defining a workspace</h1>
			<p>It’s important to know that Databricks uses the word <strong class="bold">workspace</strong> to refer to two distinct components: an instance of Databricks (meaning your hosted Databricks deployment that you access via <a id="_idIndexMarker044"/>your unique URL address) and the folder environment for accessing your work products, like notebooks, queries, <span class="No-Break">and dashboards.</span></p>
			<p>Let’s go through the <span class="No-Break">two components:</span></p>
			<ul>
				<li><strong class="bold">Workspace as an instance</strong>: A Databricks account can have multiple workspaces attached to it, meaning<a id="_idIndexMarker045"/> instances of the DI Platform are deployed and often accessible from a browser, as mentioned previously, but are also accessible via an SDK or a <span class="No-Break">REST API.</span></li>
				<li><strong class="bold">Workspace as a folder</strong>: Workspace <a id="_idIndexMarker046"/>also refers to the folder that contains your user’s home folder, repositories, projects, and a shared folder that is visible to all users on the workspace instance. Often, users set their main system path to their <strong class="source-inline">Workspace</strong> folder to store their MLFlow experiments or Terraform states for pipeline deployment. You can also create and store notebooks outside source control in your home and <span class="No-Break">project folders.</span></li>
			</ul>
			<p>We now have a clearer understanding of a workspace. Now let’s discuss why we choose <strong class="bold">Unity Catalog</strong> (<strong class="bold">UC</strong>) as our <a id="_idIndexMarker047"/><span class="No-Break">preferred metasto<a id="_idTextAnchor078"/>re.</span></p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor079"/>Selecting the metastore</h1>
			<p>A metastore is a system that stores metadata for a data platform and can be thought of as the top-level container of objects. It registers a variety of information about databases, tables, views, <strong class="bold">User-Defined Functions</strong> (<strong class="bold">UDFs</strong>), and other data assets. Metadata includes details such as storage location and the permissions that govern access to <span class="No-Break">each asset.</span></p>
			<p>Two types of metastores are natively available in the DI Platform: <strong class="bold">Unity Catalog</strong> (<strong class="bold">UC</strong>) and the <strong class="bold">Hive Metastore</strong> (<strong class="bold">HMS</strong>). UC has a three-level namespace consisting of a catalog, a database (also called a schema), and a table name. In contrast, the HMS only uses a two-level namespace containing just a database and table name. A metastore is required for your Databricks Workspace instance, as this is the component that organizes and governs data access. Deciding on the right metastore is an early decision in your DI Platform journey, and we recommend Unity Catalog. Let’s talk <span class="No-Break">about why.</span></p>
			<p>Notice in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.3</em> that you can have multiple workspaces assigned to the same metastore. <strong class="bold">Access Controls</strong> and <strong class="bold">User Management</strong> are scoped to the account level, as shown in the figure. A UC <strong class="bold">Metastore</strong>, a group of catalogs, is scoped to a region with precisely one metastore per region. Within the region, you can easily share <strong class="bold">Data</strong>, <strong class="bold">Features</strong>, <strong class="bold">Volumes</strong> access, <strong class="bold">Functions</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">Models</strong></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer025">
					<img alt="Figure 2.3 – The Design of Unity Catalog with multiple workspaces" src="image/B16865_02_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – The Design of Unity Catalog with multiple workspaces</p>
			<p>Unity Catalog is more than a group of data assets. UC also tracks who has accessed assets, which makes auditing a simple exercise. Using UC allows companies to administer privileges and secure data and objects easily while being able to share them between various workspaces. Securely sharing between environments is one of the reasons why we recommend using the <span class="No-Break">UC metastore.</span></p>
			<p>The HMS design is less<a id="_idIndexMarker048"/> centralized than that of UC. For example, historically, workspaces have been created as data and code isolation, meaning there is a separate workspace for separate isolation levels. This design often required a centralized model registry workspace in addition to development, staging, and production workspaces. If not using UC, each workspace requires its own HMS and user and group management. In contrast, UC governs all assets at the account level rather than the individual workspace level; see <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.3</em>, <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.4</em>, and <em class="italic">Further reading</em>. The centralized governance model provides the ability to integrate multiple workspaces <span class="No-Break">more seamlessly.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer026">
					<img alt="Figure 2.4 – UC governs all assets under catalogs, including databases, tables, volumes, functions, and models" src="image/B16865_02_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – UC governs all assets under catalogs, including databases, tables, volumes, functions, and models</p>
			<p>Deciding on your metastore does not have to be a permanent choice. However, migrating later could become a headache. UC is continually improving and integrating with new Databricks features. The list of reasons to choose UC over HMS continues to grow, and our recommendation is to begin with and stick <span class="No-Break">with UC.</span></p>
			<p>To determine whether<a id="_idIndexMarker049"/> Unity Catalog is the right choice for you and your company, you can check out the shrinking list of limitations for choosing UC in <em class="italic">Further reading</em>. As UC continues to expand in capability, it is the path of the future. Specifically, for machine learning, there is an integration with the Feature Engineering client and the new Model Registry. Using the UC Model Registry for model sharing and governing is simpler. We will cover more about the Model Registry in Unity Catalog and Databricks Feature Engineering Client in <em class="italic">Chapters 5, 6</em>, and <em class="italic">7</em>, but if you’re curious and eager to learn more now, you can check out Manage model lifecycle in Unity Catalog in the Further reading section. Given the ever-growing number of reasons to use UC, all project code in this book will <span class="No-Break">use UC.</span></p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor080"/>Defining where the data lives, and cloud object storage</h2>
			<p>All data products start<a id="_idIndexMarker050"/> with data, and so how we make the data accessible for data teams is another important early design choice. Databricks is a cloud-based<a id="_idIndexMarker051"/> platform that connects to cloud object storage – <strong class="bold">Azure Data Lake Storage</strong> (<strong class="bold">ADLS</strong>), Amazon <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>), or <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>). There is a separation of compute and <a id="_idIndexMarker052"/>storage. Databricks orchestrates compute; the data is in cloud <a id="_idIndexMarker053"/><span class="No-Break">object storage.</span></p>
			<p>Originally, data had to be in cloud object storage before being utilized on the DI Platform. Now, <strong class="bold">Query Federation</strong> allows <a id="_idIndexMarker054"/>customers to query their data no matter where it <a id="_idIndexMarker055"/>resides (see the documentation for any possible limitations) without first worrying about ingestion and data engineering from that remote system. However, that is data in memory, not persistent data. You can land your data in cloud storage in various ways. There are many documentation sources and external tools for the actual landing of data in cloud storage. These may depend on your cloud service provider of choice. Despite best practices of storing data in<a id="_idIndexMarker056"/> your cloud storage, using the <strong class="bold">Databricks File System</strong> (<strong class="bold">DBFS</strong>) to store the<a id="_idIndexMarker057"/> data for this book’s example projects is <span class="No-Break">also possible.</span></p>
			<p>DBFS is a shared filesystem provided by Databricks that all users of a given workspace can access. Any data stored in DBFS is potentially accessible to all users, regardless of their group, role, or permissions. Therefore, only non-sensitive and non-production data you are willing to share openly across your organization should be in DBFS. An example of non-sensitive data would be the publicly available <em class="italic">Kaggle</em> datasets. This lack of governance is why we recommend storing data in Databricks volumes, where you can apply governance. We will cover more on volumes in the last section of <span class="No-Break">this chapter.</span></p>
			<p>When it comes to cloud storage, the optimal format for structured data is almost always Delta, which we talked about in detail in <a href="B16865_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. When a table is stored in the Delta format, we refer to it as a Delta table. You can choose tables to be “managed” or “external” tables. We use both types of tables in this book (the choice is justified when required). Please see the resources in <em class="italic">Further reading</em> for more information on the tw<a id="_idTextAnchor081"/>o types <span class="No-Break">of tables.</span></p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor082"/>Discussing source control</h2>
			<p>Whether or not to use a <a id="_idIndexMarker058"/>source control is usually not the question. The question is how should someone use it? Databricks has a few features that can aid in <span class="No-Break">source control.</span></p>
			<p>The first is the version history that is built into notebooks. The history of changes for each notebook is tracked even before submitting it to a remote repository using Git. Version tracking is beneficial as we are not always ready to make a Git commit but still want to track progress and collaborate. It’s also a game changer if you accidentally pull someone’s code into your working remote branch and forget to push your code before it. The notebook’s history will keep your edited copy so you can simply roll back in time and restore all <span class="No-Break">your work!</span></p>
			<p>The second feature is the ease of connecting notebooks and files in your workspace to a remote Git repository. Historically, saving Jupyter notebooks to remote repositories was a technical nightmare for code reviews, sharing, and diffs. The Databricks code repository integration allows Databricks notebooks to contain multiple languages (Python, Scala, SQL) and track them as nicely as a typical Python file. This ability to track notebooks in source as a standard file is an improvement for data engineers and scientists wanting to review notebooks compared to previously converting files to Python and losing all output and images. The days of setting up hooks to automatically save your notebook as a regular Python file every time you save your notebook <span class="No-Break">are over.</span></p>
			<p>Of course, you can store standard file formats such as markdown, delimiter separated, JSON, or YML for a whole reproducibility approach. Note that we do not recommend keeping data under repos unless it’s a data sample <span class="No-Break">for testing.</span></p>
			<p>Within a repository, how a team defines the expected folder structure for each project is generally less important than the consistent use of that structure. However, defining your project structure is still <a id="_idIndexMarker059"/>important. We recommend reading through <em class="italic">Big Book of MLOps</em> (Joseph Bradley, Rafi Kurlansik, Matthew Thomson, and Niall Turbitt, 2023, <em class="italic">Big Book of MLOps, second edition</em>, <a href="https://www.databricks.com/resources/ebook/the-big-book-of-mlops">https://www.databricks.com/resources/ebook/the-big-book-of-mlops</a>) to determine the best structure for your team or organization. As we will see in future chapters, MLflow and repositories are essential for <strong class="bold">reproducible research</strong>. In the world of data science and machine learning specifically, we want to ensure the reproduction of mod<a id="_idTextAnchor083"/>els <span class="No-Break">and experiments.</span></p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor084"/>Discussing data preparation</h1>
			<p>Generally, the first step for any data science project is to explore and prepare the data. We will refer to this process as<a id="_idIndexMarker060"/> moving the data from “Bronze” to “Silver” layers in reference to the Medallion architecture methodology. You might think of this type of data transformation exclusively as a data engineering task, but it’s also essential for data science and <span class="No-Break">machine learning.</span></p>
			<p>If you aren’t familiar with this <a id="_idIndexMarker061"/>architecture terminology, the <strong class="bold">Medallion architecture</strong> is a data design pattern used to organize<a id="_idIndexMarker062"/> data logically in a warehouse. This architecture is also commonly called “multi-hop” architecture. It aims to incrementally and progressively improve the structure and quality of data as it flows through each layer. The Medallion architecture has three layers: Bronze, Silver, and Gold, listed <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The <strong class="bold">Bronze</strong> layer is the raw <a id="_idIndexMarker063"/>data layer. It contains all the raw, unprocessed data ingested from the source systems. This data still needs to be cleaned <span class="No-Break">or transformed.</span></li>
				<li>The <strong class="bold">Silver</strong> layer is the validated data layer. It contains data that has been cleaned and is subject to <a id="_idIndexMarker064"/>various validation and transformation steps. This data is ready to be used for analysis <span class="No-Break">and modeling.</span></li>
				<li>The <strong class="bold">Gold</strong> layer is the enriched <a id="_idIndexMarker065"/>data layer. It is the highest level and contains data enriched with additional information, such as business intelligence metrics and key performance indicators, to meet the requirements of the <span class="No-Break">business users.</span></li>
			</ul>
			<p>The Medallion architecture is a flexible and customizable architecture that can meet the specific needs of each organization. The Medallion architecture is compatible with the concept of Data Mesh. Data Mesh is an architectural and organizational paradigm to ensure value from data. Lakehouse and Data Mesh are complementary, paradigms. See <em class="italic">Further reading</em> for blog posts on leveraging a data mesh with the DI Platform. This distributed data architecture enables organizations to unlock the value of their data by making it accessible and usable by everyone in <span class="No-Break">the organization.</span></p>
			<p>Communication and collaboration are vital for the data preparation process. This step involves identifying and correcting errors, filling in missing values, and resolving inconsistencies in the data. The actions you take should be discussed as a team <a id="_idIndexMarker066"/>and documented. This is especially important when working collaboratively across data teams because data engineers and data scientists often have different perspectives on how data should be prepared. For example, we have seen situations where an engineer imputed all the missing values in a column with a zero. The rationalization made sense; many zeros were already in the column, making the KPIs’ values come out correctly. However, from a modeling perspective, missing data differs from zeros, especially if there are already zeros in the dataset. The approach of replacing nulls with zeros is not necessarily incorrect; it simply needs to be discussed with the downstream consumers of the data. One helpful communication tool is the column tagging functionality from the Databricks Catalog UI. See <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.5</em> for <span class="No-Break">an example</span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer027">
					<img alt="Figure 2.5 – Example of how to use tagging in a catalog to communicate the transformation performed on a column to all table users" src="image/B16865_02_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Example of how to use tagging in a catalog to communicate the transformation performed on a column to all table users</p>
			<p>This implementation of an incorrect imputation method also serves as an example of wanting to go back and reprocess history. Luckily, using the Medallion architecture methodology is a saving grace. In the situation mentioned previously, the chosen imputation would only be present in the Silver and Gold data layers. Meanwhile, the Bronze layer still contains the original raw data, so the source of truth is not lost, and reprocessing <span class="No-Break">is possible.</span></p>
			<p>One of the ways that the Databricks Platform boosts productivity and collaboration is the feature of real-time collaboration support for notebooks. This feature allows two or more people to simultaneously see and edit a notebook. The ability to pair-program virtually during the pandemic was a lifesaver for many. We’re big fans of people who have worked remotely for much of our careers. Collaborative editing of a notebook is much easier than sharing code via video call. While there are many options for reviewing code, historically, reviewing code in notebooks has been difficult, particularly when committing notebooks to <span class="No-Break">source control.</span></p>
			<p>After completing<a id="_idIndexMarker067"/> transformations, documentation is easy using markdown in notebooks. Even if the resulting notebook is not not itself designated for production ETL, documenting the how and why of your data transformations is important for all downstream users. To read more about Databricks notebooks, see the docume<a id="_idTextAnchor085"/>ntation in <span class="No-Break"><em class="italic">Further reading</em></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor086"/>Planning to create features</h1>
			<p>A data engineer may build Gold tables from Silver tables for consumption by the business. At the same time, a data scientist is building features from the same Silver tables for models. If we aren’t careful, two<a id="_idIndexMarker068"/> people working separately without communication can create different versions of the same metrics. When architecting your unified DI Platform, be sure to think about reusability and maintainability. For this reason, with features specifically, the features-as-code approach is our recommendation. Features-as-code refers to the software development practice <em class="italic">everything is code</em>, with a focus on creating a repository of reusable code to define features rather than features stored <span class="No-Break">in tables.</span></p>
			<p>You can implement features-as-code in various ways. Initially, we mainly focus on function reusability. You can place functions you execute in multiple notebooks or scripts in a folder within the repository root directory. In the <em class="italic">Applying our learning</em> section, you will see this is where we store functions even when not calculating a feature per se. We call these the utils. You will be referencing the <strong class="source-inline">mlia_utils</strong> notebook throughout the <span class="No-Break">example</span><span class="No-Break"> projects.</span></p>
			<p>You can find the <strong class="source-inline">mlia_utils</strong> functions in the root folder of the GitHub repository (<a href="https://github.com/PacktPublishing/Databricks-Lakehouse-ML-In-Action">https://github.com/PacktPublishing/Databricks-Lakehouse-ML-In-Action</a>). We walk through pulling the GitHub repository into Databricks in the <em class="italic">Applying our learning</em> section. In it, you will find Python files containing useful functions we will use in the projects. It is best practice to save, share, and track functions so that the metrics and features we calculate are consistent. Note the empty <strong class="source-inline">__init__.py</strong> file is also in the <strong class="source-inline">utils</strong> folder. Having an <strong class="source-inline">__init__.py</strong> file is required. With this structure, we can use all functions as imports, for example, from <strong class="source-inline">mlia_utils.rag_funcs</strong> <span class="No-Break">import </span><span class="No-Break"><strong class="source-inline">extract_doc_text</strong></span><span class="No-Break">.</span></p>
			<p>Features-as-code is not only a way to reduce duplicative work by reusing functions. It is also a great way to ensure consistent business metrics. For example, online advertising often has complex calculations<a id="_idIndexMarker069"/> for the different types of revenue. Therefore, if other people or teams calculate business-critical metrics differently, it will be hard to establish the true metric value. Instead, you can often avoid this confusion by providing executive-approved functions for use. We will talk about this again in <em class="italic">Chapters 5</em> and <em class="italic">6</em>. In addition to features being centrally located and thus easier to find, Databricks offers easy ways to document your <span class="No-Break">data assets.</span></p>
			<p>Creating a business process requiring the team to document tables and functions with descriptions makes current and previous efforts more discoverable. In the Databricks Catalog UI, you should see <em class="italic">AI-generated</em> suggestions to fill in your table and column descriptions, so you don’t have to start from scratch. Another great way to document transformations performed on a data table would be using tags. Tags can help with documentation and communication. Recall the example of missing data being imputed (<span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">).</span></p>
			<p>Focusing on ML features more specifically, you will learn how to store and serve feature functions to simplify your final deployment process. You will create an on-demand feature function and use it in your model. We also will show you how to leverage saved feature tables to create training sets. If you want to jump ahead right away, see <a href="B16865_05.xhtml#_idTextAnchor244"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, where we cover topics such as on-demand feature functions, feature lookups, syncing to the online store, and the Databricks<a id="_idTextAnchor087"/> Feature <span class="No-Break">Engineering client.</span></p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor088"/>Modeling in Databricks</h1>
			<p>After features have been created and stored as feature tables, we create training sets and focus on model training. We will cover modeling in terms of leveraging Databricks to facilitate the model lifecycle in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>. In <a href="B16865_07.xhtml#_idTextAnchor325"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, we’ll discuss the Unity Catalog Registry and how to use it to track an enormous amount of information from the associated experiments, in addition to details such as model lineage. You can register multiple versions of a model at every stage and can give these different versions aliases, such as <strong class="bold">champion</strong> and <strong class="bold">challenger</strong>, or a more specific alias referring to versions A and B in an A/B test. See aliasing in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer028">
					<img alt="Figure 2.6 – A user can alias a model with specific names for A/B testing or multi-armed bandits" src="image/B16865_02_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – A user can alias a model with specific names for A/B testing or multi-armed bandits</p>
			<p>In <a href="B16865_07.xhtml#_idTextAnchor325"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, we demonstrate how to trigger a testing script to test every model before having a human review it. Testing models is an efficient practice to reduce the time to production when used consistently and with intention. We suggest defining the criteria for successfully transitioning models/code through isolation environments (from development to stage to production). Clearly <a id="_idIndexMarker070"/>defined environments are one of the practices that enable you to create clear and consistent model quality expectations across all models. Be sure to consult <em class="italic">The Big Book of MLOps</em> on best practices for isolation and model promotion. No matter where your environment is, it is beneficial to incorporate logging into <span class="No-Break">model experimentation.</span></p>
			<p>We discuss logging in the ML context rather than the software development sense. Logging for ML is focused on reproducible<a id="_idIndexMarker071"/> research and is also known as experiment tracking. It is common practice to track <span class="No-Break">the following:</span></p>
			<ul>
				<li>The input data used to train <span class="No-Break">a model</span></li>
				<li>The parameters used to train <span class="No-Break">a model</span></li>
				<li>The accuracy and speed performance of a model during training <span class="No-Break">and inference</span></li>
				<li>The errors that occur during training <span class="No-Break">and inference</span></li>
				<li>The runtime environment of <span class="No-Break">a model</span></li>
			</ul>
			<p>When using MLflow, you have access to a powerful feature called automatic logging, or autologging. Autologging is excellent because it makes it easy to track the parameters, metrics, and artifacts of your machine learning experiments without <span class="No-Break">explicit instructions.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Auto logging only tracks flavors supported by MLflow. Custom <em class="italic">pyfunc</em> models are not supported. For more information, check <span class="No-Break"><em class="italic">Further reading</em></span><span class="No-Break">.</span></p>
			<p>MLflow auto logging logs parameter values and models for each run in a single experiment. Every time you train and evaluate your model, MLflow logs your standard metrics and parameters. If you have custom metrics to track with your models, you can also easily add them. We demonstrate tracking custom metrics in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, when we<a id="_idIndexMarker072"/> log parameters for the streaming <span class="No-Break">transactions model.</span></p>
			<p>An admin can enable auto logging for all notebooks attached to interactive clusters at the workspace level. At the cluster level, you can add <strong class="source-inline">spark.databricks.mlflow.autologging.enabled=true</strong> to the advanced section of your cluster configuration to turn on auto logging with a cluster scope. It is less common but possible to enable auto logging within a notebook scope by adding <strong class="source-inline">mlflow.autolog()</strong> to a Python cell in a notebook. Be sure to check the list of modeling flavors supported <span class="No-Break">by autolog.</span></p>
			<p>By default, MLflow saves the tracked items in the managed folder in DBFS (which will be in UC in the future). You can also set <strong class="source-inline">artifact_location</strong> to point to a volume path, which is what we do in the example projects. You also have the option to set the location to another cloud storage location, although doing so eliminates the ability to see your experiments in the <span class="No-Break">MLflow UI.</span></p>
			<p>The MLflow UI makes it incredibly easy to compare each trail; see <em class="italic">Figures 2.7 </em><span class="No-Break">and</span><span class="No-Break"><em class="italic"> 2.8</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer029">
					<img alt="Figure 2.7 – Comparison of experiment runs using the MLflow UI" src="image/B16865_02_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – Comparison of experiment runs using the MLflow UI</p>
			<p>There are numerous options for<a id="_idIndexMarker073"/> visualizing the results of experiments tracked <span class="No-Break">using MLflow.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer030">
					<img alt="Figure 2.8 – Graphically comparing parameters and model performance in the MLflow UI" src="image/B16865_02_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Graphically comparing parameters and model performance in the MLflow UI</p>
			<p>We’ve examined how to compare parameters and model performance in your experiment using the MLflow UI. Next, we’ll look at how to use model monitoring (Lakehouse monitoring) to keep track of your<a id="_idTextAnchor089"/> model’s performance <span class="No-Break">over time.</span></p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor090"/>Monitoring data and models</h2>
			<p>When we think about model <a id="_idIndexMarker074"/>monitoring and how to implement it, it becomes less about the actual model itself and more about the model’s input and output. For this reason, Databricks Lakehouse Monitoring focuses on monitoring a model’s input and output, which is simply data. The computation of table metrics occurs in the background using serverless compute, or as we like to call it, managed compute. Fully managed compute abstracts away the complexities and optimization so users focus <a id="_idIndexMarker075"/>on which tables to monitor, known as primary <a id="_idIndexMarker076"/>tables, rather than how. Lakehouse Monitoring is currently in public preview, meaning not all information is ready for release. For the latest on this feature, check out the Lakehouse Monitoring product page. We demonstrate how to use Lakehouse Monitoring in <em class="italic">Chapters 4 </em><span class="No-Break"><em class="italic">and 7</em></span><span class="No-Break">.</span></p>
			<p>We’ve touched on a wide variety of topics so far, from the earliest design decisions when setting up your Databricks Data Intelligence Platform to the key topics we’ll cover throughout the rest of this book. Now let’s dive into the example projects. Get ready to follow along in your own Databricks workspace as you work <a id="_idTextAnchor091"/>through setting up <span class="No-Break">your workspace.</span></p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor092"/>Applying our learning</h1>
			<p>This chapter’s <em class="italic">Applying our learning</em> section focuses on getting your Databricks workspace set up and ready for each project we’ll be working through. We’ll also go over getting set up in Kaggle so that you can download the datasets we will use throughout the res<a id="_idTextAnchor093"/>t of this book. Let’s <span class="No-Break">get started!</span></p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor094"/>Technical requirements</h2>
			<p>Before we begin setting up a workspace, please review the technical requirements needed to complete the hands-on work in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>We utilize a Python package, <strong class="source-inline">opendatasets</strong>, to download the data we need from the Kaggle <span class="No-Break">API easily.</span></li>
				<li>We use the Databricks Labs Python library, <strong class="source-inline">dbldatagen</strong>, to generate <span class="No-Break">synthetic data.</span></li>
				<li>To use the Kaggle API, you must download your credential <span class="No-Break">file, </span><span class="No-Break"><strong class="source-inline">kaggle.json</strong></span><span class="No-Break">.</span></li>
				<li>A GitHub account is beneficial for connecting Databricks and the code repository for the book (<a href="https://github.com/PacktPublishing/Databricks-ML-In-Action">https://github.com/PacktPublishing/Databricks-ML-In-Action</a>). In addition to a GitHub account, it is ideal to fork the book repository into your GitHub account. You will see that each chapter has a folder, and each project has a folder under the chapters. We will refer to the notebooks by name throughout the <span class="No-Break">project work.</span></li>
				<li>We will use the Databricks Secrets API to save both Kaggle and OpenAI credentials. The Secrets API requires the Databricks CLI. We will walk through this setup. However, you will<a id="_idIndexMarker077"/> need to create a <strong class="bold">personal access token</strong> (<strong class="bold">PAT</strong>) on your<a id="_idIndexMarker078"/> own for the configuration <span class="No-Break">step: </span><a href="https://docs.databricks.com/en/dev-tools/auth/pat.html"><span class="No-Break">https://docs.databricks.com/en/dev-tools/auth/pat.html</span></a></li>
				<li>The compute clusters we use are as follows (they vary slightly depending on your <span class="No-Break">data cloud):</span><ul><li>Single-node <span class="No-Break">CPU configuration</span></li></ul></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer031">
					<img alt="Figure 2.9 – Single-node CPU cluster configuration, DBR ML 14.2" src="image/B16865_02_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Single-node CPU cluster configuration, DBR ML 14.2</p>
			<p>This will w<a id="_idTextAnchor095"/>ork for most workloads in <span class="No-Break">this book.</span></p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor096"/>Setting up your workspace</h1>
			<p>As defined previously, the workspace discussed in this section refers to the deployment instance. Here, we will discuss <a id="_idIndexMarker079"/>workspace setup recommendations, project setup files, and download instructions for each dataset used throughout <span class="No-Break">this book.</span></p>
			<p>There is comprehensive documentation on deploying your workspace for the first time. If you do not already have a Databricks account and deployed workspace, then you have a couple of places to start from. One method is going into your cloud account and activating Databricks through the marketplaces. Another method is to begin on the Databricks website. For more advanced users, consider using Terraform. Given the amount of documentation and the ever-changing world of technology, we leave the exercise of activa<a id="_idTextAnchor097"/>ting a workspace up <span class="No-Break">to you.</span></p>
			<p>Once we have a workspace deployed, we can begin setting it up. Generally, we start with user groups and <a id="_idIndexMarker080"/>governance. The experience of setting up Unity Catalog is frequently updated for simplicity. Therefore, we recommend you watch the latest video documentation on how to do so (see <em class="italic">Further reading</em>). The process is the same, regardless of the data persona using the platform. Please be sure to complete metastore and governance setup before <span class="No-Break">going forward.</span></p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor098"/>Kaggle setup</h2>
			<p>You will need a Kaggle account to<a id="_idIndexMarker081"/> download the Kaggle datasets we’ll be using, which require an API token for authentication. There is an official Kaggle API, but, there are numerous other ways to connect to Kaggle to download data and interact with the Kaggle site as well. All methods require downloading your API credentials file, <strong class="source-inline">kaggle.json</strong>, from the Kaggle website. Before downloading data, you need to make your credentials accessible. Here are three<a id="_idIndexMarker082"/> methods for <span class="No-Break">accomplishing this:</span></p>
			<ul>
				<li><strong class="bold">Option 1</strong>: Upload the <strong class="source-inline">kaggle.json</strong> file to your project folder. If you choose to do this, be aware that your credentials are viewable to others, even if only admins. Also, add <strong class="source-inline">kaggle.json</strong> to your <strong class="source-inline">.gitignore</strong> file to prevent committing your credentials to the repository and ensure you do not commit your credentials to a <span class="No-Break">Git repository.</span></li>
				<li><strong class="bold">Option 2</strong>: Paste your credentials into a notebook with the same concerns as option one. Keeping secret API keys in your Databricks notebook is far from best practice, but it is the simplest option and you can lock down your notebook access and add a configuration notebook to your <strong class="source-inline">.gitignore</strong> file to prevent committing your credentials to the repository. However, the ability to remove other users’ access may not be in your control, depending on your role. Furthermore, in general, admins can see <span class="No-Break">all files.</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer032">
					<img alt="Figure 2.10 – Passing user credentials to the notebook" src="image/B16865_02_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – Passing user credentials to the notebook</p>
			<ul>
				<li><strong class="bold">Option 3</strong>: Use Databricks secrets to store and retrieve your username and token, for optimal security, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.11</em>. This is the method we use for <span class="No-Break">downloading images.</span></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer033">
					<img alt="Figure 2.11 – Using secrets to store and retrieve your username and token" src="image/B16865_02_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Using secrets to store and retrieve your username and token</p>
			<p class="list-inset">This code is in global_setup.py, but you could also put it in the <span class="No-Break">notebook itself</span></p>
			<ul>
				<li><strong class="bold">Option 4</strong>: Use the <strong class="source-inline">o</strong><strong class="source-inline">pendatasets</strong> library to paste your credentials in at the time of download. This is a safe way to download data, so we demonstrate this with the Favorita <span class="No-Break">Sales data.</span></li>
			</ul>
			<p>We’ll will walk through <strong class="source-inline">global-setup.py</strong> later. The last section of the file is setting your Kaggle credentials. We recommend <a id="_idIndexMarker083"/>setting up a secret scope with your credentials. We will show you how to set this up once you have a cluster running, so there is no need to jump around. Simply dow<a id="_idTextAnchor099"/>nload your Kaggle credentials <span class="No-Break">for now.</span></p>
			<h3>Setting up our GitHub repository</h3>
			<p>The first thing is to pull the <a id="_idIndexMarker084"/>code you will work with throughout the book from the <span class="No-Break">GitHub repository.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer034">
					<img alt="Figure 2.12 – Setting up a Git repository: Workspace &gt; Repos &gt; Home folder &gt; Add &gt; Repo&#13;&#10;" src="image/B16865_02_12.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer035">
					<img alt="Figure 2.12 – Setting up a Git repository: Workspace &gt; Repos &gt; Home folder &gt; Add &gt; Repo" src="image/B16865_02_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Setting up a Git repository: Workspace &gt; Repos &gt; Home folder &gt; Add &gt; Repo</p>
			<p>Navigate to your fork of the Book’s GitHub <a id="_idIndexMarker085"/>repository, as mentioned in the <em class="italic">Technical requirements</em> section under the <em class="italic">Applying our learning</em> section. You can copy and paste the HTTPS link into the <strong class="bold">Add Repo</strong> screen’s URL section, shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.12</em>. Next, you will link your GitHub account. If you are unsure how to do this, follow the documentation linked in <em class="italic">Further reading</em> titled <em class="italic">About personal access tokens</em> and <em class="italic">Set up Databricks Repos</em>. Once your repository is ready, you can<a id="_idIndexMarker086"/> <a id="_idTextAnchor100"/>create a cluster if no one has done <span class="No-Break">so yet</span><span class="No-Break">.</span></p>
			<h3>Creating compute</h3>
			<p>We provide the cluster configurations we use for this project in the <em class="italic">Technical requirements</em> section. You can <a id="_idIndexMarker087"/>use the same configuration if you like. There are several options to choose from when creating a new cluster configuration. It might seem complicated for new users, but stay calm when trying to choose the right one. We highly recommend the <em class="italic">Best Practices for Cluster Configuration</em> linked in <em class="italic">Further reading</em> for guidance, especially if you are responsible for setting up compute for one or more teams. Let’s talk about some of the compute options as they relate to ML and <span class="No-Break">this book:</span></p>
			<ul>
				<li><strong class="bold">Multi-node versus single-node</strong>: Multi-node is excellent for distributed projects (think Spark). Single-node is suitable for projects or workloads that are performed on the driver (think scikit-learn <span class="No-Break">or pandas).</span></li>
				<li><strong class="bold">Access mode</strong>: Some cluster configurations support Unity Catalog, and some don’t. Choose a cluster that supports UC for the projects in <span class="No-Break">this book.</span></li>
				<li><strong class="bold">Databricks Runtime (DBR) and Python packages</strong>: The DBR is part of the managed service of Databricks. A DBR is a group of commonly used packages bundled and pinned to a release version and then versioned as a DBR. There are two significant variations of a DBR: standard and ML. Data engineering workloads often utilize the standard DBR. The ML DBR, as the name implies, is for DS and ML workloads. Think of the greatest hits of ML Packages – sklearn, pandas, numpy, and seaborn, as well as the latest CUDA libraries for GPU clusters, and so on, these are included in the ML runtime, which means they are pre-installed and ready to be used in your code. DBRs provide the benefit of reproducibility as well as reducing the engineering development time of packaging and pinning versions. When adding additional libraries, you can add them to your notebook, cluster, <span class="No-Break">or workspace.</span><p class="list-inset">The best practice for production is to install cluster-scoped libraries rather than notebook-scoped libraries. However, notebook-scoped libraries are great for interactive use and development (please note that if you opt for notebook-scoped libraries, every notebook will require you to re-install non-default<a id="_idIndexMarker088"/> libraries each time, as one notebook represents a unique <strong class="source-inline">pyenv</strong>). You will install libraries both at the cluster level and in a couple of project notebooks. You can see the <strong class="bold">Libraries</strong> tab in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.13</em>. Simply click <strong class="bold">Install new</strong> to install a <span class="No-Break">new library.</span></p></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer036">
					<img alt="Figure 2.13 – Adding libraries to the Machine Learning in Action (MLIA) cluster configuration" src="image/B16865_02_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13 – Adding libraries to the Machine Learning in Action (MLIA) cluster configuration</p>
			<p class="list-inset">This is the ideal time to install the libraries you will need. Install via <strong class="source-inline">PyPI opendatasets</strong>, <strong class="source-inline">dbldatagen</strong>, <strong class="source-inline">databricks-feature-engineering</strong>, and <strong class="source-inline">mlflow-skinny[databricks]&gt;=2.5.0</strong>. These libraries are used across multiple notebooks throughout <span class="No-Break">the book.</span></p>
			<ul>
				<li><strong class="bold">Photon acceleration</strong>: Photon is an acceleration engine that speeds up ETL and SQL workloads. Photon currently is not advantageous for standard <span class="No-Break">ML modeling.</span></li>
				<li><strong class="bold">VM types</strong>: There are many VMs to pick from. You can choose VMs by family from the drop-down list. You can start with a VM in the <em class="italic">General Purpose</em> group if you need more clarification or are <span class="No-Break">just starting.</span></li>
				<li><strong class="bold">Min and max workers</strong>: The rule of thumb is to start with a few max workers and increase the number of workers as your workload increases. Keep in mind that your cluster will autoscale for you. However, we still recommend starting smaller and growing out for only the more compute-heavy examples, such as certain notebooks in the Multilabel Image Classification deep <span class="No-Break">learning project.</span></li>
			</ul>
			<p>You now have your development<a id="_idIndexMarker089"/> environment set up. You are ready to lock down your credentials for safe use in <span class="No-Break">your code.</span></p>
			<h3>Setting up the Databricks CLI and secrets</h3>
			<p>The Databricks CLI is the command line interface for Databricks. We recommend using the web terminal from your cluster to install Databricks CLI as well as create Databricks secrets. As we mentioned earlier in the chapter, there are other options to get access to Kaggle datasets, but we walk you through the steps to set up secrets here. Please see the documentation in <em class="italic">Further reading</em> for more details on the CLI, installation, usage, <span class="No-Break">and secrets.</span></p>
			<ol>
				<li>Go to the <strong class="bold">Apps</strong> tab of the compute cluster you set up in the last section. You can refer to <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.13</em> to see the location of the <strong class="bold">Apps</strong> tab. Apps are only available while the cluster is, so it may be greyed out initially. You will have to start your cluster <span class="No-Break">to proceed.</span></li>
				<li>Select the <span class="No-Break">web terminal.</span></li>
				<li>Install the latest version of the CLI. <strong class="source-inline">curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | </strong><span class="No-Break"><strong class="source-inline">sudo sh</strong></span></li>
				<li>Check your Databricks version to be sure it’s greater than <strong class="source-inline">0.2</strong>. We had to point to the updated version in the location installed by <span class="No-Break">curl. </span><span class="No-Break"><strong class="source-inline">/usr/local/bin/databricks-v</strong></span><span class="No-Break">.</span></li>
				<li>Next, you need to configure the connection. You need your PAT <span class="No-Break">for this:</span><pre class="source-code">
/usr/local/bin/databricks configure</pre></li>				<li>Create a secret scope for storing credentials related to <span class="No-Break">this book:</span><pre class="source-code">
/usr/local/bin/databricks secrets create-scope "machine-learning-in-action"</pre></li>				<li>Create a secret for storing your <span class="No-Break">Kaggle username:</span><pre class="source-code">
/usr/local/bin/databricks secrets put-secret --json '{"scope": "machine-learning-in-action",
"key": "kaggle_username",
"string_value": "readers-username"
}'</pre></li>				<li>Create a secret for <a id="_idIndexMarker090"/>storing your Kaggle <span class="No-Break">API key:</span><pre class="source-code">
/usr/local/bin/databricks secrets put-secret --json '{
"scope": "machine-learning-in-action",
"key": "kaggle_key",
"string_value": "readers-api-key"
}'</pre></li>				<li>Last, list your secrets to make sure everything works <span class="No-Break">as expected:</span><pre class="source-code">
/usr/local/bin/databricks secrets list-secrets "machine-learning-in-action"</pre></li>			</ol>
			<p>In <a href="B16865_08.xhtml#_idTextAnchor384"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, we’ll create another scope to hold an OpenAI API key, but for now we just need the Kaggle credentials. Now that we have our secrets set up, let's get our <span class="No-Break">codebase ready!</span></p>
			<h3>Setting up your code base</h3>
			<p>We use a setup file to help keep variables <a id="_idIndexMarker091"/><a id="_idTextAnchor101"/>consistent across multiple project notebooks. You will run the setup file each time you run the project notebooks using a magic command, <strong class="source-inline">%run</strong>. This command brings everything into the memory of your notebook session. The <strong class="source-inline">global-setup.py</strong> file has numerous components to it. Let’s walk through each section. Feel free to edit the file to fit <span class="No-Break">your needs.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">It’s possible you’ll receive an error message: <strong class="source-inline">py4j.security.Py4JSecurityException: Method public scala.collection.immutable.Map com.databricks.backend.common.rpc.CommandContext.tags() is not whitelisted on class </strong><span class="No-Break"><strong class="source-inline">class com.databricks.backend.common.rpc.CommandContext</strong></span></p>
			<p class="callout">This is because you are on a shared compute cluster. You can <a id="_idTextAnchor102"/>simply hardcode <strong class="source-inline">current_user</strong> to <span class="No-Break">your username.</span></p>
			<h3>Passing variables via widgets</h3>
			<p>Widgets pass variables to notebooks<a id="_idIndexMarker092"/> similarly to how command-line arguments pass variables to Python scripts. The code block in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.14</em> creates the widgets needed to pass variables from the <strong class="source-inline">Run</strong> command to the <strong class="source-inline">global-setup.py</strong> file using Databricks Utilities or <strong class="source-inline">dbutils</strong>. You can read more about the <strong class="source-inline">dbutils</strong> capabilities in the Databricks Utilities documentation in <em class="italic">Further reading</em>. These widgets create, pass, and access parameters. The arguments are the variable name, default value, and verbose name in <span class="No-Break">respective order.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer037">
					<img alt="" role="presentation" src="image/B16865_02_15.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer038">
					<img alt="" role="presentation" src="image/B16865_02_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14 – Creating widgets for accepting notebook-specific variables</p>
			<p>You can pass each variable while running the file by adding a single line cell with appropriate parameters at the top of the notebook, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break"><em class="italic">5</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer039">
					<img alt="Figure 2.15 – Running project-specific variables in our global setup file" src="image/B16865_02_17.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.15 – Running project-specific variables in our global setup file</p>
			<p>Running <strong class="source-inline">global-setup.py</strong> saves <a id="_idIndexMarker093"/>all the variables defined<a id="_idTextAnchor103"/> in the script in memory for <span class="No-Break">easy reference.</span></p>
			<h3>Checking for compatibility</h3>
			<p>Next, in <strong class="source-inline">global-setup.py</strong>, we run checks for compatibility between the code base and the cluster attached to <span class="No-Break">the </span><span class="No-Break"><a id="_idIndexMarker094"/></span><span class="No-Break">notebook.</span></p>
			<p>The compatibility code block checks <span class="No-Break">the following:</span></p>
			<ul>
				<li>A project name was submitted as <span class="No-Break">a variable.</span></li>
				<li>The cluster is configured with an ML runtime and meets the minimum version. To be sure all features in the code are available in the runtime used, we set <span class="No-Break">a minimum.</span></li>
			</ul>
			<p>Once<a id="_idTextAnchor104"/><a id="_idTextAnchor105"/> all checks pass, we assign a user <span class="No-Break">and paths.</span></p>
			<h3>Setting a default catalog and project-specific database</h3>
			<p>This book provides code that<a id="_idIndexMarker095"/> uses the Unity Catalog catalog. Your default catalog is set based <a id="_idIndexMarker096"/>on your environment. If you do not set the environment or you set it to <strong class="source-inline">dev</strong>, then the catalog is named <strong class="source-inline">ml_in_action</strong>. When the environment is <strong class="source-inline">prod</strong>, the catalog is <strong class="source-inline">ml_in_prod</strong>. The default name for the database is always the project name. However, you can provide a different <a id="_idIndexMarker097"/>name if you desire by entering a project variable for the <span class="No-Break">database name.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer040">
					<img alt="Figure 2.16 – Using the defined variables to set the default with retries" src="image/B16865_02_18.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.16 – Using the defined variables to set the default with retries</p>
			<p>We want to be sure that the catalog <a id="_idIndexMarker098"/>and database are set to the notebooks’ defaults. Occasionally, with parallel execution, this command can fail during initialization; therefore, we add a few retries to<a id="_idTextAnchor106"/> <a id="_idTextAnchor107"/>work around this issue, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">.</span></p>
			<h3>Granting permissions</h3>
			<p>Now that we’ve set our catalog and <a id="_idIndexMarker099"/>database defaults, we can <span class="No-Break">grant permissions.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer041">
					<img alt="" role="presentation" src="image/B16865_02_19.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer042">
					<img alt="" role="presentation" src="image/B16865_02_19_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.17 – Granting permissions to the catalog and database</p>
			<p>We grant the group <strong class="source-inline">account users</strong> permission. If you do not want to make your assets available to others, remove this or comment <span class="No-Break">it out.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Be sure to use tick marks around your group name or email address when granting permission. If you use single quotes instead, you will get an <span class="No-Break">error message.</span></p>
			<p>The catalog and database are<a id="_idIndexMarker100"/> ready for tables. However, not all data we use in machine learning goes into a table. For <a id="_idTextAnchor108"/>other data, files, and objects, we <span class="No-Break">have volumes.</span></p>
			<h3>Setting up volumes</h3>
			<p>Volumes are views of cloud object storage. We create project-specific volumes. Use them for path-based <a id="_idIndexMarker101"/>access to structured or unstructured data. Volumes sit under a database in a catalog and are used to manage and provide access to data files. You can govern access to volumes using <strong class="source-inline">GRANT</strong> statements. Volumes provide scalable file-based storage without sacrificing governance. Often, we use unstructured, semi-structured, or non-tabular data in machine learning. Images are a good example of unstructured, non-tabular data that we will use for the Multilabel Image Classification project. To work with these images, the Multilabel Image Classification project <span class="No-Break">uses volumes.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer043">
					<img alt="" role="presentation" src="image/B16865_02_20.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer044">
					<img alt="Figure 2.18 – EndpointApiClient class" src="image/B16865_02_20_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.18 – EndpointApiClient class</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor109"/>Starting the projects</h2>
			<p>We have planned our platform and set up our workspace environment. Next, let’s work through each project. In <a id="_idIndexMarker102"/>GitHub, you will see that each chapter has a folder containing folders corresponding to each project. When we refer to the notebooks by name, we assume you are in the appropriate chapter and project folder. For example, this chapter has the <span class="No-Break">first notebook:</span></p>
			<pre class="source-code">
Chapter 2: Designing Databricks: Day One/
  Project: Favorita Store Sales - TimeSeries Forecasting/
CH2-01-Downloading_Sales_Forecast_Data</pre>			<p>We refer to the notebook <a id="_idIndexMarker103"/>by only the filename itself, <strong class="source-inline">CH2-01-Downloading_Sales<a id="_idTextAnchor110"/>_Forecast_Data</strong>. Let’s jump into the <span class="No-Break">first project.</span></p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor111"/>Project: Favorita store sales – time series forecasting</h2>
			<p>Recall from <a href="B16865_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> that we use<a id="_idIndexMarker104"/> a Kaggle-provided dataset to forecast sales. In this chapter, we download our data from the Kaggle website. To follow along in your workspace, please open the <span class="No-Break">following notebook:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">CH2-01-Downloading_Sales_Forecast_Data</strong></span></li>
			</ul>
			<p>In the notebook, and as well as the code here in <em class="italic">Figures 2.19 and 2.20</em>, we set our path and download our data <span class="No-Break">from Kaggle.</span></p>
			<p>First, we designate <strong class="source-inline">raw_data_path</strong> to store <span class="No-Break">the files.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer045">
					<img alt="Figure 2.19 – Setting the path for our volume" src="image/B16865_02_21.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.19 – Setting the path for our volume</p>
			<p>In the following code block (<span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.20</em>), we use the Python package <strong class="source-inline">opendatasets</strong>, a library specifically created to download data from the Kaggle API. You can find more information in the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer046">
					<img alt="Figure 2.20 – Downloading Favorita data from opendatasets" src="image/B16865_02_22.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.20 – Downloading Favorita data from opendatasets</p>
			<p>That is all for the <em class="italic">Favorita Store Sales</em> project in this<a id="_idIndexMarker105"/> chapter! Now, we can focus on gener<a id="_idTextAnchor112"/>ating data for our <strong class="source-inline">Streaming </strong><span class="No-Break"><strong class="source-inline">Transactions</strong></span><span class="No-Break"> project.</span></p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor113"/>Project: Streaming Transactions</h2>
			<p>Your goal with the Streaming <a id="_idIndexMarker106"/>Transactions project is to build a model to classify transactions. The dataset consists of JSON-formatted transactions with <strong class="source-inline">Transaction</strong>, <strong class="source-inline">timestamp</strong>, <strong class="source-inline">Label</strong>, <strong class="source-inline">Amount</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">CustomerID</strong></span><span class="No-Break">.</span></p>
			<p>In later chapters, you will add a product column to demonstrate schema evolution. In this chapter, you’ll create the first version of transaction data used throughout the rest of the book. To follow along in your workspace, please open the <span class="No-Break">following notebook:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">CH2-01-Generating_Records_Using_DBKS_Labs_Datagen</strong></span></li>
			</ul>
			<p>You can run each cell of the notebook as we work through them or run them all at once. After the setup commands, we set notebook variables to establish the number of rows generated per batch of transactions (<strong class="source-inline">nRows</strong>), the number of positively labeled rows per batch (<strong class="source-inline">nPositiveRows</strong>), the path to the volume where you will store the JSON dataset (<strong class="source-inline">destination_path</strong>), a temporary path (<strong class="source-inline">temp_path</strong>), and the number of seconds between each batch of data you <span class="No-Break">generate (</span><span class="No-Break"><strong class="source-inline">sleepIntervalSeconds</strong></span><span class="No-Break">).</span></p>
			<p>The following code block accesses the value of the <strong class="source-inline">Reset</strong> widget. Any data already written to the volume will be deleted if the widget is set to <strong class="source-inline">True</strong> (its <span class="No-Break">default value).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer047">
					<img alt="Figure 2.21 – Checking the Reset widget" src="image/B16865_02_23.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.21 – Checking the Reset widget</p>
			<p>Next, we set the parameter values used in the data generator to create the transactions. We set the minimum and maximum values for each <strong class="source-inline">CustomerID</strong>. We also create a dictionary of product types and set <strong class="source-inline">min</strong>, <strong class="source-inline">max</strong>, <strong class="source-inline">mean</strong>, <strong class="source-inline">alpha</strong>, and <strong class="source-inline">beta</strong> variables, which you use to generate random transaction amounts according to <span class="No-Break">a distribution.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer048">
					<img alt="" role="presentation" src="image/B16865_02_24.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer049">
					<img alt="" role="presentation" src="image/B16865_02_25.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.22 – Dictionaries to hold variables for use within the define_specs function</p>
			<p>With the variables set, we build out the functions to create the transaction data, starting with the <strong class="source-inline">define_specs</strong> function. The function <a id="_idIndexMarker107"/>accepts as input a product type (defined in the dictionary in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.22</em>), a positive or negative label, and a timestamp; it returns a dollar amount for the transaction. <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.23</em> shows a portion of the code; the rest is in the <span class="No-Break">accompanying notebook.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer050">
					<img alt=" Figure 2.23 – Defining the define_specs function to generate transaction records" src="image/B16865_02_26.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 2.23 – Defining the define_specs function to generate transaction records</p>
			<p>Next, we write a function to generate a single record by calling <strong class="source-inline">define_specs</strong> and including the <span class="No-Break">current timestamp.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer051">
					<img alt="Figure 2.24 – Defining a function to generate a single transaction record" src="image/B16865_02_27.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.24 – Defining a function to generate a single transaction record</p>
			<p>We then build <strong class="source-inline">generateRecordSet</strong> to generate the <strong class="source-inline">recordCount</strong> number of records in each batch. Notice <a id="_idIndexMarker108"/>that in this notebook, we’re using the <strong class="source-inline">None</strong> product type, so the records generated will only have four features: <strong class="source-inline">CustomerID</strong>, <strong class="source-inline">TransactionTimestamp</strong>, <strong class="source-inline">Amount</strong>, and <strong class="source-inline">Label</strong> (this will be important in the <span class="No-Break">next chapter!).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer052">
					<img alt=" Figure 2.25 – The generateRecordSet function creates a record for each product and each label. Each record contains nRows transactions" src="image/B16865_02_28.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 2.25 – The generateRecordSet function creates a record for each product and each label. Each record contains nRows transactions</p>
			<p>Finally, we write a function to generate a set of data, convert the data to a DataFrame, and write it out as one JSON file to a temporary path. Then, we move that file to the final <span class="No-Break">volume destination.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer053">
					<img alt="Figure 2.26 – The writeJsonFile function generates a set of records" src="image/B16865_02_29.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.26 – The writeJsonFile function generates a set of records</p>
			<p>The set contains amounts generated as integers, so we divide by 100 to turn the amounts into dollars and type float. The function writes out the JSON file to a <strong class="source-inline">temp</strong> directory and then moves the single file to the <span class="No-Break">final directory.</span></p>
			<p>With everything set up, create<a id="_idIndexMarker109"/> the dataset with the code provided. Feel free to increase the iterations to build a la<a id="_idTextAnchor114"/>rger dataset. Then, move on to the <span class="No-Break">next project!</span></p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor115"/>Project: Retrieval-Augmented Generation Chatbot</h2>
			<p>The RAG Chatbot project will ingest PDF <a id="_idIndexMarker110"/>documents to build the knowledge base for the chatbot. We use a volume to store the PDFs. To follow along in your workspace, please open the <span class="No-Break">following notebook:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">CH2-01-Downloading_PDF_Documents</strong></span></li>
			</ul>
			<p>Files can be uploaded to a volume directly in the Databricks console via the user interface, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.27</em>; however, this project uses the code provided in the notebook to download and save the data to a <span class="No-Break">volume programmatically.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer054">
					<img alt="Figure 2.27 – Manually uploading documents into a volume" src="image/B16865_02_30.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.27 – Manually uploading documents into a volume</p>
			<p>The code for this chapter begins with setup cells and helper functions, and in <span class="No-Break"><em class="italic">Figure</em></span><span class="No-Break"> </span><span class="No-Break"><em class="italic">2</em></span><em class="italic">.28</em> we designate <strong class="source-inline">library_folder</strong> where we will save the PDFs <span class="No-Break">we download.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer055">
					<img alt="Figure 2.28 – Designating the library folder to hold the files for this project" src="image/B16865_02_31.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.28 – Designating the library folder to hold the files for this project</p>
			<p>We are using open articles <a id="_idIndexMarker111"/>published on the <strong class="bold">Arxiv</strong> page that relate to <strong class="bold">Generative AI</strong> (<strong class="bold">GenAI</strong>) and how it can impact human labor markets and economics. We <a id="_idIndexMarker112"/>pass the URLs to be used as documents for our chatbot and load these files into <span class="No-Break">our volume.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer056">
					<img alt="Figure 2.29 – Download PDF files and save them to our volume" src="image/B16865_02_32.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.29 – Download PDF files and save them to our volume</p>
			<p>Now that we have the documents downloaded and, they are ready to be processed for our chatbot. With that completed, we can <a id="_idIndexMarker113"/>move on to our<a id="_idTextAnchor116"/> final project: <strong class="bold">Multilabel </strong><span class="No-Break"><strong class="bold">Image Classification</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor117"/>Project: Multilabel Image Classification</h2>
			<p>The MIC project ingests images into <a id="_idIndexMarker114"/>Delta tables to fine-tune a pre-trained model from the <em class="italic">Restnet</em> family to improve its accuracy. We will programmatically download the images from Kaggle and save the data to a volume. To follow along in your workspace, please open the <span class="No-Break"><strong class="source-inline">CH2-01-Downloading_Images</strong></span><span class="No-Break"> notebook:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer057">
					<img alt="" role="presentation" src="image/B16865_02_33.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer058">
					<img alt="" role="presentation" src="image/B16865_02_34.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.30 – Downloading data from Kaggle using Databricks magic commands</p>
			<p>Now we create the volume folder and<a id="_idIndexMarker115"/> unzip the images for our classification project into our volumes. It will take around one hour (as it contains 80K images!) to extract the images from ZIP <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">Volumes</strong></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer059">
					<img alt="Figure 2.31 – Unzipping images into the volumes for this project" src="image/B16865_02_35.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.31 – Unzipping images into the volumes for this project</p>
			<p>We have downloaded or generated all four datasets, and they are ready to be bro<a id="_idTextAnchor118"/>ught into our Bronze layer in the <span class="No-Break">next chapter.</span></p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor119"/>Summary</h1>
			<p>This chapter covered a wide range of setup decisions, options, and processes for planning your Data Intelligence Platform. We took you through an overview of the main components of the DI Platform, from early design choices to important features that we will dive into further in upcoming chapters. You also learned how to set up your workspace and project code base. We hope you feel more comfortable with the basics of the platform. With Databricks ready and the project data downloaded, we are now ready to get into what it means to build out the Bronze <span class="No-Break">data layer.</span></p>
			<p>In <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, we cover the essentials of building out the Bronze data layer within the Databricks Intelligence Platform. We will format our data into the most optimized format, learn about schema evolution, change data capture using Delta, <span class="No-Break">and more.</span></p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor120"/>Questions</h1>
			<p>The following questions are meant to solidify key points to remember and tie the content back to your <span class="No-Break">own experience.</span></p>
			<ol>
				<li>How do Databricks runtimes <span class="No-Break">enable stability?</span></li>
				<li>How can we make our data <span class="No-Break">more discoverable?</span></li>
				<li>What are some common steps needed to set up a <span class="No-Break">Databricks workspace?</span></li>
			</ol>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor121"/>Answers</h1>
			<p>After putting thought into the questions, compare your answers <span class="No-Break">to ours.</span></p>
			<ol>
				<li>Databricks runtimes enable stability by providing a consistent set <span class="No-Break">of libraries.</span></li>
				<li>Utilizing the built-in functionality for metadata, such as table and column descriptions, makes our data <span class="No-Break">more discoverable.</span></li>
				<li>Some common steps for setting up a workspace are activating Databricks through the marketplace and setting up user groups <span class="No-Break">and governance.</span></li>
			</ol>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor122"/>Further reading</h1>
			<p>In this chapter, we identified specific libraries, technical features, and options. Please take a look at these resources to delve deeper into the areas that interest <span class="No-Break">you most:</span></p>
			<ul>
				<li><em class="italic">What is Unity </em><span class="No-Break"><em class="italic">Catalog?</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/data-governance/unity-catalog/index.html"><span class="No-Break">https://docs.databricks.com/data-governance/unity-catalog/index.html</span></a></li>
				<li><em class="italic">Lakehouse Monitoring </em><span class="No-Break"><em class="italic">demo</em></span><span class="No-Break">: </span><a href="https://youtu.be/3TLBZSKeYTk?t=560"><span class="No-Break">https://youtu.be/3TLBZSKeYTk?t=560</span></a></li>
				<li><em class="italic">UC has a more centralized method of managing the model lifecycle than </em><span class="No-Break"><em class="italic">HMS</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html"><span class="No-Break">https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html</span></a></li>
				<li><em class="italic">Share Models across </em><span class="No-Break"><em class="italic">workspaces</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/multiple-workspaces.html"><span class="No-Break">https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/multiple-workspaces.html</span></a></li>
				<li><em class="italic">In-depth UC setup on </em><span class="No-Break"><em class="italic">Azure</em></span><span class="No-Break">: </span><a href="https://youtu.be/itGKRVHdNPo"><span class="No-Break">https://youtu.be/itGKRVHdNPo</span></a></li>
				<li><em class="italic">Connecting external HMS to </em><span class="No-Break"><em class="italic">UC</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api"><span class="No-Break">https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api</span></a></li>
				<li><em class="italic">Unity Catalog </em><span class="No-Break"><em class="italic">limitations</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/data-governance/unity-catalog/index.html#unity-catalog-limitations"><span class="No-Break">https://docs.databricks.com/en/data-governance/unity-catalog/index.html#unity-catalog-limitations</span></a></li>
				<li><em class="italic">Best practices: Cluster configuration | Select Cloud in the </em><span class="No-Break"><em class="italic">dropdown</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/clusters/cluster-config-best-practices.html"><span class="No-Break">https://docs.databricks.com/clusters/cluster-config-best-practices.html</span></a></li>
				<li><em class="italic">Databricks </em><span class="No-Break"><em class="italic">Notebooks</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/notebooks/index.html"><span class="No-Break">https://docs.databricks.com/en/notebooks/index.html</span></a></li>
				<li><em class="italic">Databricks Autologging | Select Cloud in the </em><span class="No-Break"><em class="italic">dropdown</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/mlflow/databricks-autologging.html#security-and-data-management"><span class="No-Break">https://docs.databricks.com/mlflow/databricks-autologging.html#security-and-data-management</span></a></li>
				<li><em class="italic">Kaggle API </em><span class="No-Break"><em class="italic">GitHub</em></span><span class="No-Break">: </span><a href="https://github.com/Kaggle/kaggle-api"><span class="No-Break">https://github.com/Kaggle/kaggle-api</span></a></li>
				<li><em class="italic">Lakehouse Monitoring product </em><span class="No-Break"><em class="italic">page</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/product/machine-learning/lakehouse-monitoring"><span class="No-Break">https://www.databricks.com/product/machine-learning/lakehouse-monitoring</span></a></li>
				<li><em class="italic">System </em><span class="No-Break"><em class="italic">Tables</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/resources/demos/tutorials/governance/system-tables"><span class="No-Break">https://www.databricks.com/resources/demos/tutorials/governance/system-tables</span></a></li>
				<li><em class="italic">Opendatasets Python </em><span class="No-Break"><em class="italic">package</em></span><span class="No-Break">: </span><a href="https://pypi.org/project/opendatasets/"><span class="No-Break">https://pypi.org/project/opendatasets/</span></a></li>
				<li><em class="italic">Kaggle </em><span class="No-Break"><em class="italic">API</em></span><span class="No-Break">: </span><a href="https://www.kaggle.com/docs/api"><span class="No-Break">https://www.kaggle.com/docs/api</span></a></li>
				<li><span class="No-Break"><em class="italic">GitHub</em></span><span class="No-Break">: </span><a href="https://github.com/"><span class="No-Break">https://github.com/</span></a></li>
				<li><em class="italic">Databricks ML in Action GitHub </em><span class="No-Break"><em class="italic">Repository</em></span><span class="No-Break">: </span><a href="https://github.com/PacktPublishing/Databricks-ML-In-Action"><span class="No-Break">https://github.com/PacktPublishing/Databricks-ML-In-Action</span></a></li>
				<li><em class="italic">Databricks Secrets </em><span class="No-Break"><em class="italic">API</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/security/secrets/secrets.html"><span class="No-Break">https://docs.databricks.com/en/security/secrets/secrets.html</span></a></li>
				<li><em class="italic">Databricks </em><span class="No-Break"><em class="italic">CLI</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/dev-tools/cli/index.html"><span class="No-Break">https://docs.databricks.com/en/dev-tools/cli/index.html</span></a></li>
				<li><em class="italic">Databricks </em><span class="No-Break"><em class="italic">Utilities</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/dev-tools/databricks-utils.html"><span class="No-Break">https://docs.databricks.com/en/dev-tools/databricks-utils.html</span></a></li>
				<li><em class="italic">Workspace </em><span class="No-Break"><em class="italic">libraries</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/libraries/workspace-libraries.html"><span class="No-Break">https://docs.databricks.com/en/libraries/workspace-libraries.html</span></a></li>
				<li><em class="italic">Data Mesh and the DI Platforms Blog Posts</em>: <a href="https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html"><span class="No-Break">https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html</span></a><span class="No-Break">, </span><a href="https://www.databricks.com/blog/2022/10/19/building-data-mesh-based-databricks-lakehouse-part-2.html"><span class="No-Break">https://www.databricks.com/blog/2022/10/19/building-data-mesh-based-databricks-lakehouse-part-2.html</span></a></li>
				<li><em class="italic">Short YouTube video on managed vs external tables in </em><span class="No-Break"><em class="italic">UC</em></span><span class="No-Break">: </span><a href="https://youtu.be/yt9vax_PH58?si=dVJRZHAOnrEUBdkA"><span class="No-Break">https://youtu.be/yt9vax_PH58?si=dVJRZHAOnrEUBdkA</span></a></li>
				<li><em class="italic">Query </em><span class="No-Break"><em class="italic">Federation</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/query-federation/index.html"><span class="No-Break">https://docs.databricks.com/en/query-federation/index.html</span></a></li>
				<li><em class="italic">Centralized model registry workspace for </em><span class="No-Break"><em class="italic">HMS</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/applications/machine-learning/manage-model-lifecycle/multiple-workspaces.html"><span class="No-Break">https://docs.databricks.com/applications/machine-learning/manage-model-lifecycle/multiple-workspaces.html</span></a></li>
				<li><em class="italic">Manage model lifecycle in Unity </em><span class="No-Break"><em class="italic">Catalog</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html"><span class="No-Break">https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html</span></a></li>
				<li><em class="italic">Terraform </em><span class="No-Break"><em class="italic">https</em></span><span class="No-Break">: </span><a href="https://github.com/databricks/terraform-provider-databricks"><span class="No-Break">https://github.com/databricks/terraform-provider-databricks</span></a></li>
				<li><span class="No-Break"><em class="italic">Widgets</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/notebooks/widgets.html"><span class="No-Break">https://docs.databricks.com/notebooks/widgets.html</span></a></li>
				<li><em class="italic">Kaggle API </em><span class="No-Break"><em class="italic">GitHub</em></span><span class="No-Break">: </span><a href="https://github.com/Kaggle/kaggle-api"><span class="No-Break">https://github.com/Kaggle/kaggle-api</span></a><span class="No-Break">.</span></li>
			</ul>
		</div>
	</body></html>