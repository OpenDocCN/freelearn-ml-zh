<html><head></head><body>
		<div id="_idContainer547">
			<h1 id="_idParaDest-131"><a id="_idTextAnchor141"/>Chapter 9: Building a Data-Driven Graph-Powered Application</h1>
			<p>So far, we have provided you with both theoretical and practical ideas to allow you to design and implement machine learning models that leverage graph structures. Besides designing the algorithm, it is often very important to embed the modeling/analytical pipeline into a robust and reliable end-to-end application. This is especially true in industrial applications, where the end goal is usually to design and implement production systems that support data-driven decisions and/or provide users with timely information. However, creating a data-driven application that resorts to graph representation/modeling is indeed a challenging task that requires a proper design that is a lot more complicated than simply importing <strong class="source-inline">networkx</strong>. This chapter aims to provide you with a general overview of the key concepts and frameworks that are used when building graph-based, scalable, data-driven applications.</p>
			<p>We will start by providing an overview of the so-called <strong class="bold">Lambda architectures</strong>, which provide a framework to structure scalable applications that require large-scale processing and real-time updates. We will then continue by applying this framework in the context of <em class="italic">graph-powered applications</em>, that is, applications that leverage graph structures using techniques such as the ones described in this book. We will describe their two main analytical components: <strong class="bold">graph processing engines</strong> and <strong class="bold">graph querying engines</strong>. We'll present some of the technologies used, both in shared memory machines and distributed memory machines, outlining similarities and differences. The following topics will be covered in this chapter:</p>
			<ul>
				<li>Overview of Lambda architectures</li>
				<li>Lambda architectures for graph-powered applications</li>
				<li>Technologies and examples of graph processing engines</li>
				<li>Graph querying engines and graph databases</li>
			</ul>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor142"/>Technical requirements</h1>
			<p>We will be using Python 3.8 for all of our exercises. In the following code block, you can find a list of the Python libraries that need to be installed for this chapter using <strong class="source-inline">pip</strong>. For example, run <strong class="source-inline">pip install networkx==2.5</strong> on the command line, and so on:</p>
			<p class="source-code">networkx==2.5 </p>
			<p class="source-code">neo4j==4.2.0 </p>
			<p class="source-code">gremlinpython==3.4.6</p>
			<p>All the code files relevant to this chapter are available at <a href="https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter09">https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter09</a>.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor143"/>Overview of Lambda architectures</h1>
			<p>In<a id="_idIndexMarker885"/> recent years, great focus has been given to designing scalable architectures that will allow, on the one hand, the <em class="italic">processing of a large amount of data</em>, and, on the other, <em class="italic">providing answers/alerts/actions in real time, using the latest available information</em>.</p>
			<p>Besides, these systems need to also be able to scale out seamlessly to a larger number of users or a larger amount of data by increasing resources horizontally (adding more servers) or vertically (using servers that are more powerful). <strong class="bold">Lambda architecture</strong> is a particular data-processing architecture that is designed to process massive quantities of data and ensure large throughput in a very efficient manner, preserving reduced latency and ensuring fault tolerance and negligible errors.</p>
			<p>The Lambda architecture is composed of three different layers:</p>
			<ul>
				<li><strong class="bold">The batch layer</strong>: This<a id="_idIndexMarker886"/> layer sits on top of the (possibly distributed and scalable) storage system, and can handle and store all historical data, as well as performing <strong class="bold">Online Analytical Processing</strong> (<strong class="bold">OLAP</strong>) computation <a id="_idIndexMarker887"/>on the entire dataset. New data is continuously ingested and stored, as it would be traditionally done in data warehouse systems. Large-scale processing is generally achieved via massively parallel jobs, which aim at producing aggregation, structuring, and computation of relevant information. In the context of machine learning, model training that relies on historic information is generally done in this layer, thus producing a trained model to be used either in a batch prediction job or in <a id="_idIndexMarker888"/>real-time execution. </li>
				<li><strong class="bold">The speed layer</strong>: This is a <a id="_idIndexMarker889"/>low-latency layer that allows the real-time processing of the information to provide timely updates and information. It is generally fed by a streaming process, usually involving fast computation that does not require long computational time or load. It produces an output that is integrated with the data generated by the batch layer in (near) real time, providing <a id="_idIndexMarker890"/>support for <strong class="bold">Online Transaction Processing</strong> (<strong class="bold">OLTP</strong>) operations. The speed layer might also very well use some outputs of the OLAP computations, such as a trained model. Oftentimes, applications that use machine learning modeling in real time (for example, fraud detection engines used in credit card transactions) embed in their speed layers trained models that provide prompt predictions and trigger real-time alerts of potential fraud. Libraries may operate at an event level (such as Apache Storm) or over mini-batches (such as Spark Streaming), providing, depending on the use case, slightly different requirements for latency, fault tolerance, and computational speed. </li>
				<li><strong class="bold">The serving layer</strong>: The serving layer<a id="_idIndexMarker891"/> has the duty of organizing, structuring, and indexing information in order to allow the fast retrieval of data coming from the batch and speed layers. The serving layer thus integrates the outputs of the batch layer with the most updated and real-time information of the speed layer in order to deliver to the user a unified and coherent view of the data. A serving layer can be composed of a persistence layer that integrates both historical aggregation and real-time updates. This component may be based on some kind of database, which can be relational or not, conveniently indexed in order to reduce latency and allow the fast retrieval of relevant data. The information is generally exposed to the user via either a direct connection to the database and is accessible using a specific domain query language, such as SQL, or also via dedicated services, such as RESTful API servers (which in Python can be easily implemented using several frameworks, such as <strong class="source-inline">flask</strong>, <strong class="source-inline">fastapi</strong>, or <strong class="source-inline">turbogear</strong>), which provide the data via specifically<a id="_idIndexMarker892"/> designed endpoints:</li>
			</ul>
			<div>
				<div id="_idContainer543" class="IMG---Figure">
					<img src="image/B16069_09_01.jpg" alt="Figure 9.1 – Functional diagram for an application based on Lambda architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Functional diagram for an application based on Lambda architecture</p>
			<p>Lambda architectures have several benefits that have motivated and promoted their use, especially in the context of <em class="italic">big data</em> applications. In the following bullet points, we list some of the main<a id="_idIndexMarker893"/> pros of Lambda architectures:</p>
			<ul>
				<li><strong class="bold">No server management</strong>: As the Lambda architectural design pattern typically abstracts the functional layers and does not require installing, maintaining, or administering any software/infrastructure</li>
				<li><strong class="bold">Flexible scaling</strong>: As the application can be either automatically scaled or scaled by controlling the number of processing units that are used in batch layers (for example, computing nodes) and/or in speed layers (for example, Kafka brokers) separately</li>
				<li><strong class="bold">Automated high availability</strong>: Due to the fact that it represents a serverless design for which we already have built-in availability and fault tolerance</li>
				<li><strong class="bold">Business agility</strong>: Reacts in <a id="_idIndexMarker894"/>real time to changing business/market scenarios</li>
			</ul>
			<p>Although very powerful and flexible, Lambda architectures<a id="_idIndexMarker895"/> come with some limitations mainly due to the presence of two interconnected processing flows: the <strong class="bold">batch layer</strong> and the <strong class="bold">speed layer</strong>. This may require developers to build and maintain separate code bases for batch and stream processes, resulting in more complexity and code overhead, which may lead to harder debugging, possible misalignment, and bug promotion. </p>
			<p>Here, we have provided a short overview of Lambda architectures and their basic building blocks. For more details on how to design scalable architectures and the most commonly used architectural patterns, please refer to the book <em class="italic">Data Lake for Enterprises</em>, 2017, by Tomcy John and Pankaj Misra.</p>
			<p>In the next section, we will show you how to implement a Lambda architecture for graph-powered applications. In particular, we will describe the main components and review the most common technologies.</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor144"/>Lambda architectures for graph-powered applications</h1>
			<p>When <a id="_idIndexMarker896"/>dealing with scalable, graph-powered, data-driven applications, the design of Lambda architectures is also reflected in the <a id="_idIndexMarker897"/>separation of functionalities between two crucial components of the analytical pipeline, as shown in <em class="italic">Figure 9.2</em>: </p>
			<ul>
				<li>The <strong class="bold">graph processing engine</strong> executes <a id="_idIndexMarker898"/>computations on the graph structure in order to extract features (such as embeddings), compute statistics (such as degree distributions, the number of edges, and cliques), compute <a id="_idIndexMarker899"/>metrics and <strong class="bold">Key Performance Indicators</strong> (<strong class="bold">KPIs</strong>) (such as centrality measures and clustering coefficients), and identify relevant subgraphs (for example, communities) that often require OLAP.</li>
				<li>The <strong class="bold">graph querying engine</strong> allows <a id="_idIndexMarker900"/>us to persist network data (usually done via a graph database) and provides fast information retrieval and efficient querying and graph traversal (usually via graph querying languages). All of the information is already persisted in some data <a id="_idIndexMarker901"/>storage (that may or may not be in memory) and no further computation is required apart from (possibly) some final aggregation results, for which indexing is crucial to achieving high performance<a id="_idIndexMarker902"/> and low latency:</li>
			</ul>
			<div>
				<div id="_idContainer544" class="IMG---Figure">
					<img src="image/B16069_09_02.jpg" alt="FigurFigure 9.2 – Graph-based architecture, with the main components &#13;&#10;also reflected in a Lambda architectural pattern"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Graph-based architecture, with the main components also reflected in a Lambda architectural pattern</p>
			<p>Graph processing engines sit on top of batch layers and produce outputs that may be stored and indexed in appropriate graph databases. These databases are the backend of graph querying engines, which allow relevant information to be easily and quickly retrieved, representing the operational views used by the serving layer. Depending on the use cases and/or the size of the graph, it often makes sense to run both the graph processing engine and the graph query engine on top of the same infrastructure.</p>
			<p>Instead of storing the graph on a low-level storage layer (for example, the filesystem, HDFS, or S3), there are graph database options that could support both OLAP and OLTP. These provide, at <a id="_idIndexMarker903"/>the same time, a backend<a id="_idIndexMarker904"/> persistence layer where historical information processed by batch layers, together with real-time updates from the speed layer, is stored, and information to be queried efficiently by the serving layer.</p>
			<p>As compared to other use cases, this condition is indeed quite peculiar for graph-powered, data-driven applications. Historical data often provides a topology on top of which new, real-time updates and OLAP outputs (KPIs, data aggregations, embeddings, communities, and so on) can be stored. This data structure also represents the information that is later queried by the serving layer that traverses the enriched graph.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor145"/>Graph processing engines</h2>
			<p>To select the right technology <a id="_idIndexMarker905"/>for a <strong class="bold">graph processing engine</strong>, it is crucial to estimate the size in memory of the network compared to the capacity of the target architecture. You can start by using simpler frameworks that allow fast prototyping during the first phases of a project when the goal is to<a id="_idIndexMarker906"/> quickly build a <strong class="bold">Minimum Viable Product</strong> (<strong class="bold">MVP</strong>).</p>
			<p>Such frameworks can then be substituted by more advanced tools later on when performance and scalability become more crucial. A microservice modular approach and proper structuring of these components will allow the switching of technologies/libraries independently from the rest of the application to target specific issues, which will also guide the choice of the backend stack.</p>
			<p>Graph processing engines require information on the whole graphs to be accessed quickly, that is, having all of the graph in memory, and depending on the context, you might or might not need <em class="italic">distributed architectures</em>. As we saw in <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs</em>, <strong class="source-inline">networkx</strong> is a great example of a library to build a graph processing engine when dealing with reasonably small datasets. When datasets get larger, but they can still fit in single servers or shared memory machines, other libraries may help to reduce computational time. As seen in <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs</em>, using libraries other than <strong class="source-inline">networkx</strong> where graph algorithms are implemented in more performant languages, such as C++ or Julia, may dramatically speed up the computation by more than two orders of magnitude.</p>
			<p>However, there are cases where datasets grow so much that it is no longer technologically or economically viable to use shared memory machines of increasing capacity (fat nodes). In such cases, it is rather necessary to distribute the data on clusters of tens or hundreds of computing nodes, allowing horizontal scaling. The two most popular frameworks that can support a graph processing engine in these cases are the following:</p>
			<ul>
				<li><strong class="bold">Apache Spark GraphX</strong>, which is the <a id="_idIndexMarker907"/>module of the Spark library that <a id="_idIndexMarker908"/>deals with graph structures (<a href="https://spark.apache.org/graphx">https://spark.apache.org/graphx</a>). It involves a distributed representation of the graph <a id="_idIndexMarker909"/>using <strong class="bold">Resilient Distributed Datasets</strong> (<strong class="bold">RDDs</strong>) for both vertices and edges. The graph repartition throughout the computing nodes can be done either with an <em class="italic">edge-cut</em> strategy, which logically corresponds to dividing the nodes among multiple machines, or a <em class="italic">vertex-cut</em> strategy, which logically corresponds to assigning edges to different machines and allowing vertices to span multiple machines. Although written in Scala, GraphX features wrappers with both R and Python. GraphX already comes with some algorithms implemented, such as <em class="italic">PageRank</em>, <em class="italic">connected components</em>, and <em class="italic">triangle counting</em>. There are also other libraries that can be used on top of GraphX for other algorithms, such as <strong class="bold">SparklingGraph</strong>, which <a id="_idIndexMarker910"/>implements more centrality measures.</li>
				<li><strong class="bold">Apache Giraph</strong>, which is an<a id="_idIndexMarker911"/> iterative graph processing system built for high <a id="_idIndexMarker912"/>scalability (<a href="https://giraph.apache.org/">https://giraph.apache.org/</a>). It was developed, and is currently used, by Facebook to analyze the social graph formed by users and their connections and is built on top of the Hadoop ecosystem for unleashing the potential of structured datasets at a massive scale. Giraph is natively written in Java and, similarly to GraphX, also provides a scalable implementation for some basic graph algorithms, such as <em class="italic">PageRank</em> and <em class="italic">shortest path</em>.</li>
			</ul>
			<p>When we consider scale-out to a distributed ecosystem, we should always keep in mind that the available choice for algorithms is significantly smaller than in a shared machine context. This is generally due to two reasons:</p>
			<ul>
				<li>First, implementing algorithms in a distributed way is a lot more complex than in a shared machine due to communication among nodes, which also reduces the overall efficiency.</li>
				<li>Secondly, and more importantly, one fundamental mantra of big data analytics is that only algorithms that (nearly) scale linearly with the number of data points should be implemented in order to ensure horizontal scalability of the solution, by increasing the computational nodes as the dataset increases.</li>
			</ul>
			<p>In this respect, both Giraph and GraphX allow you to define scalable, vertex-centric, iterative algorithms using standard interfaces<a id="_idIndexMarker913"/> based on <strong class="bold">Pregel</strong>, which can be seen as a sort of equivalent of iterative map-reduce operations for graphs (actually, iterative map-reduce operations applied to triplet node-edge-node instances). A Pregel computation is composed of a sequence of iterations, each called a <strong class="bold">superstep</strong>, each<a id="_idIndexMarker914"/> involving a node and its neighbors.</p>
			<p>During the superstep, <em class="italic">S</em>, a user-defined function is applied for each vertex, <em class="italic">V</em>. This function takes the messages sent to <em class="italic">V</em> in superstep <em class="italic">S – 1</em> as input and modifies the state of <em class="italic">V</em> and its outgoing edges. This function represents the mapping stage, which can be easily parallelized. Besides computing the new states of <em class="italic">V</em>, the function also sends messages to other vertices connected to <em class="italic">V</em>, which will receive this information at superstep <em class="italic">S + 1</em>. Messages are typically sent along outgoing edges, but a message may be sent to any vertex whose identifier is known. In <em class="italic">Figure 9.3</em>, we show a sketch of what a Pregel algorithm would look like when computing the maximum value over a network. For further details on this algorithm, please refer to the original paper, <em class="italic">Pregel: A System for Large-Scale Graph Processing</em>, written by Malewicz et al. in 2010:</p>
			<div>
				<div id="_idContainer545" class="IMG---Figure">
					<img src="image/B16069_09_03.jpg" alt="Figure 9.3 – Example of calculating a maximum value over a node property using Pregel &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – Example of calculating a maximum value over a node property using Pregel </p>
			<p>By using Pregel, you can easily implement other algorithms, such as <em class="italic">PageRank</em> or <em class="italic">connected components</em>, in a very efficient and general way, or even implement node embeddings' parallel variants (for an example, see <em class="italic">Distributed-Memory Vertex-Centric Network Embedding for Large-Scale Graphs</em>, Riazi and Norris, 2020).</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor146"/>Graph querying layer</h2>
			<p>In the last decade, due <a id="_idIndexMarker915"/>to the large diffusion of non-structured data, NoSQL databases have started to gain considerable attention and importance. Among them, <strong class="bold">graph databases</strong> are<a id="_idIndexMarker916"/> indeed extremely powerful to store information based on a relation between entities. Indeed, in many applications, data can naturally be seen as entities, associated with metadata in the form of node properties, connected by edges that also have properties that further describe the relationship between entities.</p>
			<p>Examples of graph databases are libraries or tools such as Neo4j, OrientDB, ArangoDB, Amazon Neptune, Cassandra, and JanusGraph (previously named TitanDB). In the following sections, we will briefly describe some of them, together with the languages that allow us to query and traverse the <a id="_idIndexMarker917"/>underlying graphs, which are called <strong class="bold">graph querying languages</strong>.</p>
			<h3>Neo4j</h3>
			<p>At <a id="_idIndexMarker918"/>the time of writing, <strong class="bold">Neo4J</strong> (<a href="https://neo4j.com/">https://neo4j.com/</a>) is surely the most common graph database around, with<a id="_idIndexMarker919"/> a large community supporting its use and adoption. It features two editions:</p>
			<ul>
				<li><em class="italic">Community Edition</em>, released<a id="_idIndexMarker920"/> under a GPL v3 license, which allows users/developers to openly include Neo4j in their applications</li>
				<li><em class="italic">Enterprise Edition</em>, designed <a id="_idIndexMarker921"/>for commercial deployments where scale and availability are crucial</li>
			</ul>
			<p>Neo4j can scale out to fairly large datasets via <strong class="bold">sharding</strong>, that is, distributing data over multiple nodes and parallelizing queries and <a id="_idIndexMarker922"/>aggregation over multiple instances of the database. Besides, the Neo4j federation also allows querying smaller separated graphs (sometimes even with a different schema) as if they were one large graph.</p>
			<p>Some of Neo4j's strong points are its flexibility (which allows the schema to be evolved) and its user-friendliness. In particular, many operations in Neo4j can be done through its query language, which is very intuitive and easy to learn: <strong class="bold">Cypher</strong>. Cypher can just be seen as the<a id="_idIndexMarker923"/> counterpart of SQL for graph databases.</p>
			<p>Testing out Neo4j and Cypher is extremely easy. You could install the Community Edition (via Docker; see the next section) or play around with an online sandbox version (<a href="https://neo4j.com/sandbox/">https://neo4j.com/sandbox/</a>).</p>
			<p>By using the latter, you can import some built-in datasets, such as the Movie dataset, and start querying it using the Cypher query language. The Movie dataset is made up of 38 movies and 133 people that acted in, directed, wrote, reviewed, and produced them. Both the on-premises version and the online version are equipped with a user-friendly UI that allows the user to query and visualize the data (see <em class="italic">Figure 9.4</em>). We start by listing 10 actors in the Movie dataset, by simply querying the following:</p>
			<p class="source-code">MATCH (p: Person) RETURN p LIMIT 10</p>
			<p>But let's<a id="_idIndexMarker924"/> now leverage the information about relations between data points. We see that one of the actors that appears in the database is Keanu Reeves. We may wonder who all the actors that he has acted with in the listed movies are. This information can be easily retrieved using the following query:</p>
			<p class="source-code">MATCH (k: Person {name:"Keanu Reeves"})-[:ACTED_IN]-(m: Movie)-[:ACTED_IN]-(a: Person) RETURN k, m, a</p>
			<p>As shown in the following figure, the query intuitively and graphically indicates in its syntax how to traverse the graph by declaring the path we are interested in: </p>
			<div>
				<div id="_idContainer546" class="IMG---Figure">
					<img src="image/B16069_09_04.jpg" alt="Figure 9.4 – Example of the Neo4j UI with the Cypher query to retrieve the co-actors of Keanu Reeves in the Movie dataset"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Example of the Neo4j UI with the Cypher query to retrieve the co-actors of Keanu Reeves in the Movie dataset</p>
			<p>Besides Cypher, data can also be queried using<a id="_idIndexMarker925"/> Gremlin. This will be described shortly as a common interface for graph databases.</p>
			<p>Neo4j<a id="_idIndexMarker926"/> also provides bindings with several programming languages, such as Python, JavaScript, Java, Go, Spring, and .NET. For Python in particular, there are several libraries that implement connections with Neo4j, such as <strong class="source-inline">neo4j</strong>, <strong class="source-inline">py2neo</strong>, and <strong class="source-inline">neomodel</strong>, of which <strong class="source-inline">neo4j</strong> is the official and supported one and provides direct connections to the database via a binary protocol. Creating a connection to the database and running a query is just a matter of a few lines of code:</p>
			<p class="source-code">from neo4j import GraphDatabase</p>
			<p class="source-code">driver = GraphDatabase("bolt://localhost:7687", "my-user", "my-password")</p>
			<p class="source-code">def run_query(tx, query):</p>
			<p class="source-code">    return tx.run(query)</p>
			<p class="source-code">with driver.session() as session:</p>
			<p class="source-code">    session.write_transaction(run_query, query)</p>
			<p>A query could be any Cypher query, for instance, the one written previously to retrieve the co-actors of Keanu Reeves.</p>
			<h3>JanusGraph – a graph database to scale out to very large datasets</h3>
			<p>Neo4j is an extremely great piece of software, unbeatable when you want to get things done quickly, thanks to its intuitive interface and query language. Neo4j is indeed a graph database suitable for production, but especially good in MVPs when agility is crucial. However, as data increases, its scalability based on sharding and breaking down large graphs into smaller subgraphs may not be the best option.</p>
			<p>When the volume of the data increases substantially, you should probably start to consider other graph database options. Once again, this should be done only when the use case requirements start to hit the scalability limitation of Neo4j, as needs evolve from the MVP initial requirements.</p>
			<p>In such cases, there are several options. Some of them are commercial products, such as Amazon Neptune or Cassandra. However, open source options are also available. Among them, we believe it is worth<a id="_idIndexMarker927"/> mentioning <strong class="bold">JanusGraph</strong> (<a href="https://janusgraph.org/">https://janusgraph.org/</a>), which is a particularly interesting piece of software. JanusGraph is the evolution of a previously open source project that was called <strong class="bold">TitanDB</strong> and is <a id="_idIndexMarker928"/>now an official project under the Linux Foundation, also featuring support from top players in the tech landscape, such as IBM, Google, Hortonworks, Amazon, Expero, and Grakn Labs.</p>
			<p>JanusGraph<a id="_idIndexMarker929"/> is a scalable graph database designed for storing and querying graphs distributed across a multi-machine cluster with hundreds of billions of vertices and edges. As a matter of fact, JanusGraph does not have a storage layer on its own, but it is rather a component, written in Java, that sits on top of other data storage layers, such as the following:</p>
			<ul>
				<li><strong class="bold">Google Cloud Bigtable</strong> (<a href="https://cloud.google.com/bigtable">https://cloud.google.com/bigtable</a>), which is the cloud version of the proprietary<a id="_idIndexMarker930"/> data storage system built on Google File System, designed to scale a massive amount of data distributed across data centers (<em class="italic">Bigtable: A Distributed Storage System for Structured Data</em>, Fay Chang et al., 2006).</li>
				<li><strong class="bold">Apache HBase</strong> (<a href="https://hbase.apache.org/">https://hbase.apache.org/</a>), which is a non-relational database that <a id="_idIndexMarker931"/>features Bigtable capabilities on top of Hadoop and HDFS, thus ensuring similar scalability and fault tolerance.</li>
				<li><strong class="bold">Apache Cassandra</strong> (<a href="https://cassandra.apache.org/">https://cassandra.apache.org/</a>), which is an open source distributed NoSQL<a id="_idIndexMarker932"/> database that allows handling a large amount of data, spanning multiple data centers.</li>
				<li><strong class="bold">ScyllaDB</strong> (<a href="https://www.scylladb.com/">https://www.scylladb.com/</a>), which is specifically designed for real-time applications, is <a id="_idIndexMarker933"/>compatible with Apache Cassandra while achieving significantly higher throughputs and lower latencies.</li>
			</ul>
			<p>Thus, JanusGraph inherits all the good features, such as scalability, high availability, and fault tolerance, from scalable solutions, abstracting a graph view on top of them.</p>
			<p>With its integration with ScyllaDB, Janu<a id="_idTextAnchor147"/>sGraph handles extremely fast, scalable, and high-throughput applications. Besides, JanusGraph also integrates indexing layers that can be based on Apache Lucene, Apache Solr, and Elasticsearch in order to allow even faster information retrieval and search functionalities within the graph.</p>
			<p>The usage of highly distributed backends together with indexing layers allows JanusGraph to scale to enormous<a id="_idIndexMarker934"/> graphs, with hundreds of billions of nodes and edges, efficiently handling the so-called <strong class="bold">supernodes</strong>—in other words, nodes that have an extremely large degree, which often arise in real-world applications (remember that a very famous model for real networks is<a id="_idIndexMarker935"/> the <em class="italic">Barabasi-Albert</em> model, based on preferential attachments, which makes hubs naturally emerge within the graph).</p>
			<p>In large graphs, supernodes are often potential bottlenecks of the application, especially when the business logic requires traversing the graph passing through them. Having properties that can help with rapidly filtering only the relevant edges during a graph traversal can dramatically speed up the process and achieve better performance.</p>
			<p>JanusGraph exposes a standard API to <a id="_idIndexMarker936"/>query and traverse the graph via the <strong class="bold">Apache TinkerPop</strong> library (<a href="https://tinkerpop.apache.org/">https://tinkerpop.apache.org/</a>), which is an open source, vendor-agnostic graph computing framework. TinkerPop provides a standard interface for querying and analyzing the underlying graph using <a id="_idIndexMarker937"/>the <strong class="bold">Gremlin</strong> graph traversal language. All TinkerPop-compatible graph database systems can therefore integrate seamlessly with one another. TinkerPop thus allows you to build "standard" serving layers that do not depend on the backend technology, giving you the freedom to choose/change the appropriate graph technology for your application depending on your actual needs. As a matter of fact, most of the graph databases (even Neo4j as we have seen previously) nowadays feature integration with TinkerPop, making switching between backend graph databases seamless and avoiding any vendor lock-in. </p>
			<p>Besides Java connectors, Gremlin also has direct Python bindings thanks to the <strong class="source-inline">gremlinpython</strong> library, which allows Python applications to connect to and traverse graphs. In order to query the graph structure, we first need to connect to the database, using the following:</p>
			<p class="source-code">from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection</p>
			<p class="source-code">connection = DriverRemoteConnection(</p>
			<p class="source-code">    'ws://localhost:8182/gremlin', 'g'</p>
			<p class="source-code">)</p>
			<p>Once the connection is created, we can then instantiate <strong class="source-inline">GraphTraversalSource</strong>, which is the basis for all Gremlin traversals, and bind it to the connection we just created:</p>
			<p class="source-code">from gremlin_python.structure.graph import Graph</p>
			<p class="source-code">from gremlin_python.process.graph_traversal import __ </p>
			<p class="source-code">graph = Graph()</p>
			<p class="source-code">g = graph.traversal().withRemote(connection)</p>
			<p>Once <strong class="source-inline">GraphTraversalSource</strong> is instantiated, we can reuse it across the application to query the graph database. Imagine that we have imported the Movie graph database we described previously into JanusGraph; we can re-write the Cypher query we used previously to find all the co-actors of Keanu Reeves using Gremlin:</p>
			<p class="source-code">co_actors = g.V().has('Person', 'name', 'Keanu Reeves').out("ACTED_IN").in("ACTED_IN").values("name")</p>
			<p>As can be seen from the preceding code lines, Gremlin is a functional language whereby operators are grouped together to form path-like expressions.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor148"/>Selecting between Neo4j and GraphX</h2>
			<p>Neo4j or GraphX? This is a question that often gets asked. However, as we have described briefly, the two pieces of software are not really competitors, but they rather target different needs. Neo4j allows us to store information in a graph-like structure and query the data, whereas <a id="_idIndexMarker938"/>GraphX makes <a id="_idIndexMarker939"/>it possible to analytically process a graph (especially for large graph dimensions). Although you could also use Neo4j as a processing engine (and indeed the Neo4j ecosystem features a Graph Data Science library, which is an actual processing engine) and GraphX could also be used as an in-memory stored graph, such approaches should be discouraged.</p>
			<p>Graph processing engines usually compute KPIs that get stored in the graph database layers (potentially indexed such that querying and sorting become efficient) for later use. Thus, technologies such as GraphX are not competing with graph databases such as Neo4j, and they can very well co-exist within the same application to serve different purposes. As we stressed in the introduction, even in MVPs and at early stages, it is best to separate the two components, the graph processing engine and the graph querying engine, and use appropriate technologies for each of them.</p>
			<p>Simple and easy-to-use libraries and tools do exist in both cases and we strongly encourage you to use them wisely in order to build a solid and reliable application that can be scaled out seamlessly.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor149"/>Summary</h1>
			<p>In this section, we have provided you with the basic concepts of how to design, implement, and deploy data-driven applications that resort to graph modeling and leverage graph structures. We have highlighted the importance of a modular approach, which is usually the key to seamlessly scaling any data-driven use case from early-stage MVPs to production systems that can handle a large amount of data and large computational performances.</p>
			<p>We have outlined the main architectural pattern, which should provide you with a guide when designing the backbone structure of your data-driven applications. We then continued by describing the main components that are the basis of graph-powered applications: <em class="italic">graph processing engines</em>, <em class="italic">graph databases</em>, and <em class="italic">graph querying languages</em>. For each component, we have provided an overview of the most common tools and libraries, with practical examples that will help you to build and implement your solutions. You should thus have by now a good overview of what the main technologies out there are and what they should be used for.</p>
			<p>In the next chapter, we will turn to some recent developments and the latest research that trends in machine learning that has been applied to graphs. In particular, we will describe some of the latest techniques (such as generative neural networks) and applications (such as graph theory applied in neuroscience) available in the scientific literature, providing some practical examples and possible applications.</p>
		</div>
	</body></html>