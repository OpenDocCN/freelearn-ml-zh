<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">What's Next?</h1>
                
            
            <article>
                
<p class="calibre2">We have come a long way. From the basics and steps for building <strong class="calibre4">machine learning</strong> (<strong class="calibre4">ML</strong>) models to actually developing numerous ML models for various real-world projects, we have covered a lot so far. After a brief introductory chapter, where we learned the basics of ML and the essential steps that go into building ML models, we started building ML models. In <a target="_blank" href="part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 2</a>, <em class="calibre13">Spam Email Filtering</em> and <a target="_blank" href="part0036.html#12AK80-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 3</a>, <em class="calibre13">Twitter Sentiment Analysis</em>, we discussed building classification models using text datasets. In <a target="_blank" href="part0045.html#1AT9A0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 4</a>, <em class="calibre13">Foreign Exchange Rate Forecast</em> and <a target="_blank" href="part0056.html#1LCVG0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 5</a>, <em class="calibre13">Fair Value of House and Property</em>, we used financial and real estate property data to build regression models. Then in <a target="_blank" href="part0073.html#25JP20-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 6</a>, <em class="calibre13">Customer Segmentation</em>, we covered how to use clustering algorithms to draw intuitive insights into customer behavior using the e-commerce dataset. In <a target="_blank" href="part0082.html#2E6E40-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 7</a>, <em class="calibre13">Music Genre Recommendation</em> and <a target="_blank" href="part0097.html#2SG6I0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 8</a>, <em class="calibre13">Handwritten Digit Recognition</em>, we expanded our knowledge of building ML models to build music recommendation and image recognition models using music records and handwritten digit image data. In <a target="_blank" href="part0116.html#3EK180-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 9</a>, <em class="calibre13">Cyber Attack Detection</em> and <a target="_blank" href="part0132.html#3TSA80-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 10</a>, <em class="calibre13">Credit Card Fraud Detection</em> we built anomaly detection models for cyber attack detection and credit card fraud detection.</p>
<p class="calibre2">In this chapter, we are going to review the types of ML model we have built, the projects we have worked on so far, and code snippets for training various ML models using the Accod.NET framework. We will also discuss some of the challenges when using and applying ML in real-life projects and situations. Lastly, we are going to cover some of the other software packages that can be used for future ML projects, as well as other common technologies that are frequently used by data scientists.</p>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre10">
<li class="calibre11">A review of what we have learned so far</li>
<li class="calibre11">Real-life challenges in building ML models</li>
<li class="calibre11">Other common technologies used by data scientists</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Review</h1>
                
            
            <article>
                
<p class="calibre2">From the first chapter onward, we have discussed and covered a large amount of material. From discussing the basics of ML to building classification, regression, and clustering models, it is worth reviewing what we have done so far before we end this book. Let's review some of the essential concepts and code that will be helpful for your future C# ML projects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Steps for building ML models</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">As discussed in <a target="_blank" href="part0020.html#J2B80-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 1</a>, <em class="calibre13">Basics of Machine Learning Modeling</em></span>, it <span class="calibre5">can be challenging for aspiring data scientists and ML engineers to understand the flow and approaches to building real-world ML models that will be used in production systems. We have discussed the steps for building machine learning models in detail in <a target="_blank" href="part0020.html#J2B80-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 1</a>, <em class="calibre13">Basics of Machine Learning Modeling</em></span>, <span class="calibre5">and we have followed those steps in each of the projects that we have worked on so far. The following diagram should be a good recap of the essential steps in building real-world ML models:</span></p>
<div class="mce-root"><img src="../images/00189.jpeg" class="calibre138"/></div>
<p class="calibre2">As you should already, we always start a ML project with the problem definition. In this step, we define the problems that we are going to solve with ML and why we need ML models to solve such problems. This is also the step where we brainstorm our ideas and the prerequisites, such as the types of data required, as well as the types of learning algorithms that we are going to experiment with. Lastly, this is where we need to clearly define the success criteria for the project. We can define some evaluation metrics not only for the prediction performance of ML models, but also the execution performance of your models, especially if the models need to be run in a real-time system, and output the prediction results within a given time window.</p>
<p class="calibre2">From the problem definition phase, we move on to the data collection step. For those projects that we have worked on in this book, we used publicly available data that was already compiled and labeled. However, in real-world situations, data might not be available to start with. In this case, we will have to come up with approaches to collect the data. For example, if we are planning to build ML models for user behavior predictions for users on our website or application, then we can collect user activities on the website or application. On the other hand, if we are building a credit model to score the credit worthiness of potential borrowers, most likely we will not be able to collect data ourselves. In this case, we will have to resort to third-party data vendors who sell credit-related data.</p>
<p class="calibre2">Once we have gathered all of our data, the next thing we will have to do is prepare and analyze the data. During the data preparation step, we will need to validate the dataset by looking at the formats of the data fields, the existence of duplicate records, or the number of missing values. With these criteria checked, we can then start analyzing the data to see if there is any noticeable pattern in the dataset. If you recall, we typically analyzed the target variable distribution first and then we started analyzing the distributions of the features for each of the target classes to identify any noticeable patterns that could separate the target classes from each other. During the data analysis step, we focused on gaining some insights into the patterns in the data, as well as the structure of the data itself.</p>
<p class="calibre2">With insight and understanding of the data from the data analysis step, we can then start building features that will be used for our ML models. As Andrew Ng mentioned, applied ML is basically feature engineering. This is one of the most critical steps in building ML models and in determining the performance of our prediction models. If you recall, we discussed how to use one-hot encoding to transform text features into an encoded matrix of 1s and 0s for our text classification problems. We also discussed building time series features, such as moving averages and Bollinger Bands and using log transformations for highly skewed features, when we were building regression models. This feature engineering step is where we need to be creative.</p>
<p class="calibre2">Once we have all the features ready, we can then move on to training and testing various learning algorithms. Depending on whether the target variable is continuous or categorical, we can decide whether to build a classification model or regression model. If you recall from previous projects, we trained and tested our models by using k-fold cross-validation or by splitting the dataset into two subsets and training with one group and testing with another hold-out group. Until we find the model that we are satisfied with, we will have to repeat the the previous steps. If we do not have enough data, we will have to go back to the data collection phase and try to collect more data for more accurate models. If we handled duplicate records or missing values poorly, we will have to go back to the data preparation step to clean up the data. If we can build more and better features, then repeating the feature engineering step can help by improving the performance of our ML models.</p>
<p class="calibre2">The last step in building ML models is to deploy them to production systems. All the models should have been fully tested and validated by this point. It will be beneficial to have some monitoring tools in place before the deployment, so that the performance of the models can be monitored.</p>
<p class="calibre2">We have followed these steps quite thoroughly throughout the chapters, so you will realize how comfortable and familiar with these steps you are when you start working on your future ML projects. However, there are a couple of essential steps that we could not fully cover in this book, such as the data collection and model deployment steps, so you should always keep in mind the importance and goals of those steps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Classification models</h1>
                
            
            <article>
                
<p class="calibre2">This first two ML models we built in <a target="_blank" href="part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 2</a>, <em class="calibre13">Spam Email Filtering</em> and <a target="_blank" href="part0036.html#12AK80-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 3</a>, <em class="calibre13">Twitter Sentiment Analysis</em>, were classification models. In <a target="_blank" href="part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 2</a>, <em class="calibre13">Spam Email Filtering</em>, we built a classification model to classify emails into spam and ham (non-spam emails). In <a target="_blank" href="part0036.html#12AK80-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 3</a>, <em class="calibre13">Twitter Sentiment Analysis</em>, we built a classification model for Twitter sentiment analysis, where the model classified each tweet into one of the three emotions—positive, negative, and neutral. Classification problems are common among ML projects. Building a model to predict whether a customer will buy an item in an online store is a classification problem. Building a model to predict whether a borrower will pay back his/her loan is also a classification problem. </p>
<p class="calibre2">If there are only two classes in the target variable, typically a positive outcome and a negative outcome, then we call it a binary classification. A good example of a binary classification problem is the spam email filtering project that we did in <a target="_blank" href="part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 2</a>, <em class="calibre13">Spam Email Filtering</em>. If there are more than two classes in the target variable, then we call it a multi-class or multinomial classification. We had a case of having to classify a record into three different classes in the Twitter sentiment analysis project in <a target="_blank" href="part0036.html#12AK80-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 3</a>, <em class="calibre13">Twitter Sentiment Analysis</em>; this was a good example of a multinomial classification problem. We had two more classification projects in this book. If you recall, we had eight different genres or classes in our target variable for the Music Genre Recommendation project in <a target="_blank" href="part0082.html#2E6E40-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 7</a>, <em class="calibre13">Music Genre Recommendation</em>, and we had 10 different digits in our target variable for the handwritten digit recognition project in <a target="_blank" href="part0097.html#2SG6I0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 8</a>, <em class="calibre13">Handwritten Digit Recognition</em>.</p>
<p class="calibre2">We experimented with numerous learning algorithms<span class="calibre5">, such as logistic regression, Naive Bayes, <strong class="calibre4">Support Vector Machine</strong> (<strong class="calibre4">SVM</strong>), random forest, and neural network,</span> for the aforementioned classification projects. To remind you how to train these learning algorithms in C#, we will reiterate how we initialized some of those learning algorithms in C# using the Accord.NET framework.</p>
<p class="calibre2">The following code snippet shows how we can train a binary logistic regression classifier:</p>
<pre class="calibre19">var learner = new IterativeReweightedLeastSquares&lt;LogisticRegression&gt;()<br class="title-page-name"/>{<br class="title-page-name"/>    MaxIterations = 100<br class="title-page-name"/>};<br class="title-page-name"/>var model = learner.Learn(inputs, outputs);</pre>
<p class="calibre2">For multinomial classification problems, we trained a logistic regression classifier using the following code:</p>
<pre class="calibre19">var learner = new MultinomialLogisticLearning&lt;GradientDescent&gt;()<br class="title-page-name"/>{<br class="title-page-name"/>    MiniBatchSize = 500<br class="title-page-name"/>};<br class="title-page-name"/>var model = learner.Learn(inputs, outputs);</pre>
<p class="calibre2">When building a Naive Bayes classifier, we used the following code:</p>
<pre class="calibre19">var learner = new NaiveBayesLearning&lt;NormalDistribution&gt;();<br class="title-page-name"/>var model = learner.Learn(inputs, outputs);</pre>
<p class="calibre2">If you recall, we used <kbd class="calibre12">NormalDistribution</kbd> when the features had continuous variables, as in the case of the Music Genre Recommendation project, where all the features were audio spectrum features and had continuous values. One the other hand, we used <span class="calibre5"><kbd class="calibre12">BernoulliDistribution</kbd>, where the features can only take binary values (0 versus 1). In the case of the Twitter sentiment analysis project in <a target="_blank" href="part0036.html#12AK80-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 3</a>, <em class="calibre13">Twitter Sentiment Analysis</em></span>, <span class="calibre5">all the features we had could only take 0s or 1s.</span></p>
<p class="calibre2">The following code shows how we could train a <kbd class="calibre12">RandomForestLearning</kbd> classifier:</p>
<pre class="calibre19">var learner = new RandomForestLearning()<br class="title-page-name"/>{<br class="title-page-name"/>    NumberOfTrees = 100,<br class="title-page-name"/><br class="title-page-name"/>    CoverageRatio = 0.5,<br class="title-page-name"/><br class="title-page-name"/>    SampleRatio = 0.7<br class="title-page-name"/><br class="title-page-name"/>};<br class="title-page-name"/>var model = learner.Learn(inputs, outputs);</pre>
<p class="calibre2">As you might already be known, we could tune hyperparameters, such as the number of trees in the random forest (<kbd class="calibre12">NumberOfTrees</kbd>), the proportion of variables that can be used at maximum by each tree (<kbd class="calibre12">CoverageRatio</kbd>), and the proportion of samples used to train each of the trees (<kbd class="calibre12">SampleRatio</kbd>), to find better performing random forest models.</p>
<p class="calibre2">We used the following code to train a SVM model:</p>
<pre class="calibre19">var learner = new SequentialMinimalOptimization&lt;Gaussian&gt;();<br class="title-page-name"/>var model = learner.Learn(inputs, outputs);</pre>
<p class="calibre2">If you recall, we could use different kernels for SVMs. On top of the <kbd class="calibre12">Gaussian</kbd> kernel, we could use <kbd class="calibre12">Linear</kbd> and <kbd class="calibre12">Polynomial</kbd> kernels as well. Depending on the type of dataset you have, one kernel works better than the others and various kernels should be tried to find the best performing SVM model.</p>
<p class="calibre2">Lastly, we could train a neural network using the following code:</p>
<pre class="calibre19">var network = new ActivationNetwork(<br class="title-page-name"/>    new BipolarSigmoidFunction(2), <br class="title-page-name"/>    91, <br class="title-page-name"/>    20,<br class="title-page-name"/>    10<br class="title-page-name"/>);<br class="title-page-name"/><br class="title-page-name"/>var teacher = new LevenbergMarquardtLearning(network);<br class="title-page-name"/><br class="title-page-name"/>Console.WriteLine("\n-- Training Neural Network");<br class="title-page-name"/>int numEpoch = 10;<br class="title-page-name"/>double error = Double.PositiveInfinity;<br class="title-page-name"/>for (int i = 0; i &lt; numEpoch; i++)<br class="title-page-name"/>{<br class="title-page-name"/>    error = teacher.RunEpoch(trainInput, outputs);<br class="title-page-name"/>    Console.WriteLine("* Epoch {0} - error: {1:0.0000}", i + 1, error);<br class="title-page-name"/>}</pre>
<p class="calibre2">As you might recall from <a target="_blank" href="part0097.html#2SG6I0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 8</a>, <em class="calibre13">Handwritten Digit Recognition</em>, we trained a neural network model by running it through the dataset multiple times (epochs). After each iteration or epoch, we noticed the error rate decreased, as the neural network learned more and more from the dataset. We also noticed that in each epoch, the rate of improvements in the error rate was in diminishing return, so after enough epochs there would be no significant improvement in the performance of a neural network model.</p>
<p class="calibre2">You can view the code samples at the following link: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/ClassificationModelReview.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/ClassificationModelReview.cs</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Regression models</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">We have also developed multiple regression ML models. In <a target="_blank" href="part0045.html#1AT9A0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 4</a>, <em class="calibre13">Foreign Exchange Rate Forecast</em></span>, <span class="calibre5">we worked on the Foreign Exchange Rate Forecast project, where we built models that could predict future exchange rates between Euros and US dollars. In <a target="_blank" href="part0056.html#1LCVG0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 5</a>, <em class="calibre13">Fair Value of House and Property</em></span>, <span class="calibre5">we trained different ML models that could predict house prices for the Fair Value of House and Property project. </span>Regression problems are also common in real-world ML projects. Building a model that predicts the lifetime value of a customer is a regression problem. Building a model that predicts the maximum amount of money that a potential borrower can borrow without going bankrupt is another regression problem.</p>
<p class="calibre2">We have explored numerous machine learning algorithms for regression projects in this book. We have experimented with linear regression and linear SVM models in <a target="_blank" href="part0045.html#1AT9A0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 4</a>, <em class="calibre13">Foreign Exchange Rate Forecast</em> for the Foreign Exchange Rate Forecast project. We have also tried using different kernels, such as <kbd class="calibre12">Polynomial</kbd> and <kbd class="calibre12">Guassian</kbd> kernels, for SVM models in <a target="_blank" href="part0056.html#1LCVG0-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 5</a>, <em class="calibre13">Fair Value of House and Property</em> for the Fair Value of House and Property project. To remind you how to train these regression models in C#, we will reiterate how we could use C# and the Accord.NET framework to build these models.</p>
<p class="calibre2">The following code snippet shows how we can train a linear regression model:</p>
<pre class="calibre19">var learner = new OrdinaryLeastSquares()<br class="title-page-name"/>{<br class="title-page-name"/>    UseIntercept = true<br class="title-page-name"/>};<br class="title-page-name"/>var model = learner.Learn(inputs, outputs);</pre>
<p class="calibre2">When building a SVM with the linear kernel, we used the following code:</p>
<pre class="calibre19">var learner = new LinearRegressionNewtonMethod()<br class="title-page-name"/>{<br class="title-page-name"/>    Epsilon = 2.1,<br class="title-page-name"/>    Tolerance = 1e-5,<br class="title-page-name"/>    UseComplexityHeuristic = true<br class="title-page-name"/>};<br class="title-page-name"/>var model = learner.Learn(inputs, outputs);</pre>
<p class="calibre2">As you might recall, <kbd class="calibre12">Epsilon</kbd>, <kbd class="calibre12">Tolerance</kbd>, and <kbd class="calibre12">UseComplexityHeuristic</kbd> are hyperparameters that can be tuned further for better model performance. When building a SVM model, we recommend you try various combinations of the hyperparameters to find the best performing model for your business case.</p>
<p class="calibre2">When we want to use a polynomial kernel for a SVM, we can use the following code:</p>
<pre class="calibre19">var learner = new FanChenLinSupportVectorRegression&lt;Polynomial&gt;()<br class="title-page-name"/>{<br class="title-page-name"/>    Kernel = new Polynomial(3)<br class="title-page-name"/>};<br class="title-page-name"/>var model = learner.Learn(inputs, outputs);</pre>
<p class="calibre2">For a <kbd class="calibre12">Polynomial</kbd> kernel, you can tune the degree of a polynomial function. For example, for a second degree polynomial (quadratic) kernel, you can initialize the kernel with <kbd class="calibre12">new Polynomial(2)</kbd>. Similarly, for a fourth degree polynomial kernel, <span class="calibre5">you can initialize the kernel with </span><kbd class="calibre12">new Polynomial(4)</kbd><span class="calibre5">. However, increasing the complexity of a kernel can result in overfitting, so you will need to take care when using a high-degree polynomial kernel for a SVM.</span></p>
<p class="calibre2">When we want to build a SVM with a Gaussian kernel, we can use the following code:</p>
<pre class="calibre19">var learner = new FanChenLinSupportVectorRegression&lt;Gaussian&gt;()<br class="title-page-name"/>{<br class="title-page-name"/>    Kernel = new Gaussian()<br class="title-page-name"/>};<br class="title-page-name"/>var model = learner.Learn(inputs, outputs);</pre>
<p class="calibre2">You can find <span class="calibre5">the code samples for the aforementioned regression models at</span> the following link: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/RegressionModelReview.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/RegressionModelReview.cs</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Clustering algorithms</h1>
                
            
            <article>
                
<p class="calibre2">We discussed one unsupervised learning algorithm, k-means clustering, and how it can be used to draw insights from an unlabeled dataset. In <a target="_blank" href="part0073.html#25JP20-5ebdf09927b7492888e31e8436526470" class="calibre9">Chapter 6</a>, <em class="calibre13">Customer Segmentation</em>, we used the k-means clustering algorithm on an e-commerce dataset and we learned about different customer behaviors from the dataset. We have covered how to use clustering algorithms to build different customer segments, based on their purchase history, but there are many other applications of clustering algorithms. For example, clustering algorithms can also be used in image analysis, for example in partitioning images into sub-sections, and in bioinformatics, such as discovering groups of closely related genes (gene clustering).</p>
<p class="calibre2">We used the following code to build a k-means clustering algorithm using C# and the Accord.NET framework:</p>
<pre class="calibre19">KMeans kmeans = new KMeans(numClusters);<br class="title-page-name"/>KMeansClusterCollection clusters = kmeans.Learn(sampleSet);</pre>
<p class="calibre2">As you might recall, we need to give the number of clusters we want to build to the <kbd class="calibre12">KMeans</kbd> class. One way to programmatically decide the best number of clusters that we discussed was to look at the Silhouette score, which measures how similar a data point is to its own cluster. Using this Silhouette score, you can iterate through different numbers for the number of clusters and then decide which one works the best for the given dataset.</p>
<p class="calibre2"><span class="calibre5">You can find the code samples for the k-means clustering algorithm at the following link: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/ClusteringAlgorithmReview.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.11/ClusteringAlgorithmReview.cs</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Real-life challenges</h1>
                
            
            <article>
                
<p class="calibre2">It would be great if we could just build ML models for all of our business problems. However, that is normally not the case. Often, there are more challenges in getting to the model development phase than in actually building working models. We will discuss the following frequently appearing data science challenges when we are working on ML projects<span class="calibre5"><span class="calibre5">:</span></span></p>
<ul class="calibre10">
<li class="calibre11">Data issues</li>
<li class="calibre11">Infrastructural issues</li>
<li class="calibre11">Explainability versus accuracy</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data issues</h1>
                
            
            <article>
                
<p class="calibre2">Having the right data and enough data is the most important prerequisite for building a working ML model. However, often, this is the most difficult part in developing ML models for a few different reasons. We will discuss a few common challenges that many data scientists face in terms of issues related to data. </p>
<p class="calibre2">First, the data needed might simply not exist. For example, think of a recently formed online retail store wanting to apply ML to understand or predict their customers' spending patterns. Since they are a new business with a small customer base, with not much historical purchase data, they will not have enough data for data scientists to work with. In this case, all they can do is wait for a better time to embark on ML projects, even if they have data scientists on their team. Their data scientists will simply not be able to build anything meaningful with a limited amount of data.</p>
<p class="calibre2">Second, the dataset exists, but it is not accessible. This kind of problem happens often in big corporations. Due to security issues, accessing the data might have been restricted to certain subgroups of an organization. In this case, data scientists might have to go through multiple levels of approval from different departments or business entities or they might have to build a separate data pipeline, through which they can ingest the data that they need. This kind of issue typically means it takes a long time before data scientists can start working on the ML project that they wanted to work on.</p>
<p class="calibre2">Lastly, the data is segmented or too messy. Almost all of the time, the raw datasets that data scientists get include messy data and come from different data sources. There might be too many missing values or too many duplicate records in the data and data scientists will have to spend lots of time cleaning up the raw dataset. The data might be too unstructured. This typically happens when you work with text-heavy datasets. In this case, you might have to apply various text mining and <strong class="calibre4">natural language processing</strong> (<strong class="calibre4">NLP</strong>) techniques to clean up the data and make it usable for building ML models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Infrastructure issues</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Training a ML model on a large dataset requires a large amount of memory and CPU resources. As we get bigger and bigger data, it is inevitable that we run into infrastructural issues. If you do not have enough memory resources for training ML models, you might end up getting <em class="calibre13">Out of Memory</em> exceptions after many hours or days of training models. If you do not have enough processing power, then training a complex ML model can take weeks and even months. Getting the right amount of computational resources is a real challenge in building ML models. As the data that is being used for ML grows faster than ever, the amount of computational resources required also grows significantly year after year.</span></p>
<p class="calibre2">With the emerging popularity of cloud computing service providers, such as AWS, Google, and Microsoft Azure, it became easier to get the required computational resources. On any of those cloud computing platforms, you can easily request and use the amount of memory and CPUs that you need. However, as everything comes with a price, running ML jobs on those cloud platforms can cost lots of money. Depending on your budget, such costs can restrict how much computational resources you can use for your ML tasks, and it needs to be planned cleverly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Explainability versus accuracy</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">The last common real-life challenge in ML is the trade-off between the explainability and accuracy of ML models. More traditional and linear models, such as logistic regression and linear regression models, are easy to explain in terms of the prediction output. We can extract the intercept and the coefficients of those linear models and we can get the prediction output using simple arithmetic operations. However, more complex models, such as random forest and SVM, are more difficult to use in terms of explaining the prediction output. Unlike logistic regression or linear regression models, we cannot deduce the prediction output from simple arithmetic operations. Those complex models work more like a black box. We know the input and the output, but what goes in between is a black box to us.</span></p>
<p class="calibre2">This kind of explainability issue among complex learning algorithms becomes a problem when users or auditors request explanations about the model behavior. If there is such a requirement for explainability, we will have to resort to more traditional linear models, even if more complex models perform better than those linear models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Other common technologies</h1>
                
            
            <article>
                
<p class="calibre2">As the field of ML and data science is evolving faster than ever, the number of new technologies being built is also growing at a fast pace. There are many resources and tools that help in building ML solutions and applications more easily and quickly. We are going to discuss a few technologies and tools that we recommend you get acquainted with for your future ML projects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Other ML libraries</h1>
                
            
            <article>
                
<p class="calibre2">The Accord.NET framework that we have used throughout this book is one of the most frequently used and well documented frameworks for ML. However, other libraries that are built for ML in C# are worth mentioning and taking a look at for your future ML projects.</p>
<p class="calibre2"><strong class="calibre4">Encog</strong> is a ML framework that can be used in Java and C#. It is very similar to the Accord.NET framework that we have been using, in the sense that is has a wide range of numerous ML algorithms available within the framework. This framework is well documented and has lots of sample code that can be referenced for your future machine learning projects. More information and documentation about the <strong class="calibre4">Encog</strong> framework can be found at the following link: <a href="https://www.heatonresearch.com/encog/" target="_blank" class="calibre9">https://www.heatonresearch.com/encog/</a>.</p>
<p class="calibre2"><strong class="calibre4">Weka</strong> is another ML framework, but it is different from the Accord.NET framework in the sense that the <strong class="calibre4">Weka</strong> framework is specifically engineered for data mining. It is broadly used by many researchers and has good documentation and even a book that explains how to use <strong class="calibre4">Weka</strong> for data mining. Weka is written in Java, but it can also be used in C#. More information about the <strong class="calibre4">Weka</strong> framework can be found at the following link: <a href="https://www.cs.waikato.ac.nz/~ml/index.html" target="_blank" class="calibre9">https://www.cs.waikato.ac.nz/~ml/index.html</a>. Also, information about how to use the <strong class="calibre4">Weka</strong> framework in C# can be found at the following link: <a href="https://weka.wikispaces.com/Use%20WEKA%20with%20the%20Microsoft%20.NET%20Framework" target="_blank" class="calibre9">https://weka.wikispaces.com/Use%20WEKA%20with%20the%20Microsoft%20.NET%20Framework</a>.</p>
<p class="calibre2">Lastly, you can always search in NuGet, the package manager for .NET, for any other machine learning frameworks for C#. Any library or package that is available on NuGet can easily be downloaded and referenced in your development environment. It is a good practice to search the following link for any packages you might need or that might be helpful for your future machine learning projects: <a href="https://www.nuget.org/" target="_blank" class="calibre9">https://www.nuget.org/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data visualization libraries and tools</h1>
                
            
            <article>
                
<p class="calibre2">The next set of tools and packages that we are going to discuss is about data visualizations. ML and data visualization are an inseparable combination for data science. For any ML models that you build, you should be able to present your findings, model performance, and model results to users or business partners. Furthermore, for continuous model performance monitoring purposes, data visualization techniques are often used to identify any issues with the models in production systems or any potential deterioration in the model performance. As a result, many data visualization libraries were built to make data visualization tasks easier.</p>
<p class="calibre2"><strong class="calibre4">LiveCharts</strong> is a .NET library for data visualization. We have used the Accord.NET framework's charting libraries throughout this book, but for more complex plots, we recommend using <strong class="calibre4">LiveCharts</strong>. From basic charts, such as line and bar charts, to complex interactive charts, you can build various visualizations in C# relatively easily. The <strong class="calibre4">LiveCharts</strong> library has thorough documentation and lots of examples along with sample code. You can find more information about how to use <strong class="calibre4">LiveCharts</strong> for data visualizations at the following link: <a href="https://lvcharts.net/" class="calibre9">https://lvcharts.net/</a>.</p>
<p class="calibre2">Aside from the C#.NET library for data visualization tasks, there are two more data visualization tools that are frequently used in the data science community: <strong class="calibre4">D3.js</strong> and <strong class="calibre4">Tableau</strong>. <strong class="calibre4">D3.js</strong> is a JavaScript library for building and presenting charts on web pages. Often, this JavaScript library is used to create a dashboard for various data science and data visualization tasks. <strong class="calibre4">Tableau</strong> is a business intelligence tool, with which you can drag and drop to create various visualizations. This tool is frequently used to create a dashboard not only by data scientists, but also by non-data professionals. For more information about the <strong class="calibre4">D3.js</strong> library, you can follow this link: <a href="https://d3js.org/" target="_blank" class="calibre9">https://d3js.org/</a>. For more information about Tableau, you can follow this link: <a href="https://www.tableau.com/" target="_blank" class="calibre9">https://www.tableau.com/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Technologies for data processing</h1>
                
            
            <article>
                
<p class="calibre2">Lastly, we are going to discuss some commonly used technologies and tools for processing data. Throughout this book, we have mostly used CSV files as input for our ML modeling projects. We have used the Deedle framework to load, manipulate, and aggregate the data. However, often, the type of input data for ML projects varies. For some projects, the data might be stored in SQL databases. For other projects, the data might be stored across distributed filesystems. Furthermore, the source of the input data can even be from real-time streaming services. We will briefly discuss a few commonly used technologies for such cases and where to look for more detailed information in order for you to do further research.</p>
<p class="calibre2">SQL databases, such as SQL Server or PostgreSQL, are the most commonly used technologies for data storage and data processing. Using the SQL language, data scientists can easily retrieve, manipulate, and aggregate data to process and prepare the data for their ML projects. As an aspiring data scientist, it will be beneficial for you to become more comfortable with using the SQL language for processing the data.</p>
<p class="calibre2">Another technology that is often used within the data science community is <strong class="calibre4">Spark</strong>, which is a cluster-computing framework. With <strong class="calibre4">Spark</strong>, you can process a large amount of data at scale. Using clusters of machines and distributing heavy computations across those machines, <strong class="calibre4">Spark</strong> helps in building scalable big data solutions. This technology is widely used among numerous organizations and companies, such as Netflix, Yahoo, and eBay, which have lots of data to process every day.</p>
<p class="calibre2">Lastly, there are numerous stream-processing technologies for real-time ML applications. One of the most popular ones is <strong class="calibre4">Kafka</strong>. This technology is often used when building real-time applications or data pipelines that need to continuously stream the data<span class="calibre5">. In the case of building real-time ML applications, using a stream-processing technology, such as <strong class="calibre4">Kafka</strong>, will be essential for the successful delivery of a real-time ML product.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we reviewed what we have discussed so far in this book. We briefly went over the essential steps in building ML models. Then, we summarized and compiled the code to build various ML models in C# using the Accord.NET framework for classification, regression, and clustering problems. We have also discussed the real-life challenges that we could not cover in this book, but that you will most likely face when you start working on your future ML projects. We discussed challenges in accessing and compiling the data to build ML models, infrastructural challenges that will occur for big data, and the trade-offs between the explainability and accuracy of the ML models. Lastly, we covered some commonly used technologies that we recommend you get acquainted with for your future ML projects. The code libraries and tools that were mentioned in this chapter are only a subset of the tools that are available, and the commonly used tools and technologies are going to evolve year on year. We recommend you consistently research upcoming technologies for ML and data science.</p>
<p class="calibre2">We have covered various ML techniques, tools, and concepts throughout this book. As you have worked through this book from building basic classification and regression models to complex recommendation and image recognition systems, as well as anomaly detection models for real-world problems, I hope you have gained more confidence in building ML models for your future ML projects. I hope your journey throughout this book was worthwhile and meaningful, and that you have learned and gained many new and useful skills.</p>


            </article>

            
        </section>
    </body></html>