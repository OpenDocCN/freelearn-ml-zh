- en: 'Chapter 12: Machine Learning Automated Workflows'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For **machine learning** (**ML**) models that are deployed to production environments,
    it's important to establish a consistent and repeatable process to retrain, deploy,
    and operate these models. This becomes increasingly important as you scale the
    number of ML models running in production. The **machine learning development
    lifecycle** (**ML Lifecycle)** brings with it some unique challenges in operationalizing
    ML workflows. This will be discussed in this chapter. We will also discuss common
    patterns to not only automate your ML workflows, but also implement **continuous
    integration** (**CI**) and **continuous delivery**/**deployment** (**CD**) practices
    for your ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Although we will cover various options for automating your ML workflows and
    building CI/CD pipelines for ML in this chapter, we will focus particularly on
    detailed implementation patterns using Amazon SageMaker Pipelines and Amazon SageMaker
    projects. SageMaker Pipelines is purpose-built for activities that include the
    automation of the steps needed to build a model, such as **data preparation**,
    **model training**, and **model evaluation** tasks. SageMaker projects build on
    SageMaker Pipelines by incorporating CI/CD practices into your ML pipelines. SageMaker
    projects utilize SageMaker Pipelines in combination with the SageMaker model registry
    to build out end-to-end ML pipelines that also incorporate CI/CD practices such
    as source control, version management, and automated deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for automating your SageMaker ML workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building ML workflows with Amazon SageMaker Pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating CI/CD ML pipelines using Amazon SageMaker projects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations for automating your SageMaker ML workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll review a typical ML workflow that includes the basic
    steps for model building and deploy activities. Understanding the key SageMaker
    inputs and artifacts for each step is important in building automated workflows,
    regardless of the automation or workflow tooling you choose to employ.
  prefs: []
  type: TYPE_NORMAL
- en: This information was covered in [*Chapter 8*](B17249_08_Final_JM_ePub.xhtml#_idTextAnchor151),
    *Manage Models at Scale Using a Model Registry*. Therefore, if you have not yet
    read that chapter it's recommended to do so prior to continuing with this chapter.
    We'll build on that information and cover high-level considerations and guidance
    for building out automated workflows and CI/CD pipelines for SageMaker workflows.
    We'll also briefly cover the common AWS native service options when building automated
    workflows and CI/CD ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Typical ML workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An ML workflow contains all the steps required to build an ML model for an ML
    use case, followed by the steps needed to deploy and operate the model in production.
    *Figure 12.1* shows a typical ML workflow that includes model build and model
    deploy steps. Each step within the workflow often has a number of associated tasks.
    As an example, data preparation can include multiple tasks needed to transform
    data into a format that is consistent with your ML algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: When we look at automating the end-to-end ML workflow, we look to automate the
    tasks included within a step, as well as how to orchestrate the sequence and timing
    of steps into an end-to-end pipeline. As a result, knowing the key inputs for
    each step, as well as the expected output or artifact of a step, is key in building
    end-to-end pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, model development is an iterative process. It may therefore take
    many experiments until you're able to find a candidate model that meets your model
    performance criteria. As a result, it's common to continue to experiment in a
    data science sandbox environment until you find a candidate model to register
    into a model registry. This would indicate that the model is ready to deploy to
    one or more target environments for additional testing, followed by deployment
    to a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following figure for an example of a typical workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Typical ML workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_12_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – Typical ML workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'After the model is deployed, there may also be additional tasks required to
    integrate the model with existing client applications. There may also be tasks
    required to create a more complex inference workflow that includes multiple models
    and tasks required for inference. Finally, there would still be tasks required
    to operate that model. Although the *Operate* step comes at the end, the activities
    that need to be performed for the ongoing operation of that model need to be considered
    early on in the process. This is in order to include all necessary tasks within
    your automated workflow, as well as ensure key metrics are captured, and available
    for key personas. In addition, this allows you to set up alerts as needed. This
    includes activities such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model monitoring**: This includes the tasks required to ensure your model
    performance does not degrade over time. This topic is covered in detail in [*Chapter
    11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210), *Monitoring Production Models
    with Amazon SageMaker Model Monitor and Clarify.* However, when building your
    automated deployment workflows, it''s important to consider the additional tasks
    that may need to be included and automated within your pipeline. As an example,
    SageMaker Model Monitor for data drift requires tasks such as baselining of your
    training data, enabling data capture on your endpoints, and scheduling a SageMaker
    monitoring job. All of these tasks should be automated and included in your automated
    workflow. You can also utilize *Human in the Loop* reviews with **Amazon Augmented
    AI** (**Amazon A2I**) to check low-confidence predictions that can be implemented
    along with, or complementary to, SageMaker Model Monitor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System monitoring**: System monitoring includes capturing and alerting on
    metrics that are key to the resources hosting your model, as well as the other
    resources supporting the deployed ML solution. As an example, Amazon SageMaker
    will automatically capture key metrics about an endpoint, such as CPU/GPU utilization
    or the number of invocations. Setting thresholds and creating alerts in Amazon
    CloudWatch helps ensure the overall health of resources hosting models, as well
    as other solution components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model retraining**: To set up automatic model retraining, the tasks that
    are performed across your model build steps should be captured as code that can
    be executed as part of a model build pipeline. This pipeline would include automation
    of all of the tasks within each step, as well as orchestration of those steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline monitoring**: If you have automated pipelines set up for your model
    build and model deploy activities, it''s key to also have monitoring in place
    on your pipeline to ensure you are notified in the event of a step failure in
    your pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have covered the general steps in an ML workflow. However, each automated
    workflow and CI/CD pipeline can vary due to a number of factors. In the next section,
    we'll cover some of the considerations that are common across ML use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations and guidance for building SageMaker workflows and CI/CD pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps and tasks performed as part of an ML workflow can vary depending
    on the use case; however, the following high-level practices are recommended when
    building an automated workflow for your ML use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implement a model registry**: A **model registry** helps bridge the steps
    between the phases of model building experimentation and deploying your models
    to higher-level environments. A model registry captures key metadata, such as
    **model metrics**. It also ensures you''re able to track key inputs and artifacts
    for traceability, as well as manage multiple model versions across environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version inputs and artifacts**: The ability to roll back or recreate a specific
    model version or deployable artifact is dependent on knowing the specific versions
    of inputs and artifacts used to create that resource. As an example, to recreate
    a SageMaker endpoint, you need to know key version information, such as the model
    artifact and the inference container image. These inputs and artifacts should
    be protected from inadvertent deletion. They should also be tracked through an
    end-to-end pipeline to be able to confidently recreate resources as part of an
    automated workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS-native options for automated workflow and CI/CD pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we focus primarily on the SageMaker-native options for creating
    automated workflows, as well as layering on CI/CD practices in end-to-end pipelines.
    However, there are other options that can also be used for creating automated
    workflows that contain SageMaker tasks for model building and model deployment.
    There are also third-party options that contain operators or integrations with
    SageMaker. However, they are not covered in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll cover a few of the AWS services and features that can be used
    to build automated workflows that include SageMaker tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Step Functions**: AWS Step Functions ([https://aws.amazon.com/step-functions/?step-functions.sort-by=item.additionalFields.postDateTime&step-functions.sort-order=desc](https://aws.amazon.com/step-functions/?step-functions.sort-by=item.additionalFields.postDateTime&step-functions.sort-order=desc))
    allows you to create automated serverless workflows that include integration with
    a number of AWS services, as well as giving you the capability to integrate third-party
    tasks into your workflows. AWS Step Functions also has native support for SageMaker
    tasks, such as SageMaker processing jobs, SageMaker training jobs, and SageMaker
    hosting options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, ML builders can choose to take advantage of the AWS Step Functions
    Data Science SDK ([https://docs.aws.amazon.com/step-functions/latest/dg/concepts-python-sdk.html](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-python-sdk.html))
    to create ML workflows using Python instead of through Amazon States Language.
    Amazon States Language is the native pipeline syntax for AWS Step Functions. AWS
    Step Functions offers extensibility across AWS services with native integrations
    for the AWS services most commonly used in ML workflows, such as AWS Lambda, Amazon
    EMR, or AWS Glue.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Amazon Managed Workflows for Apache Airflow**: Amazon Managed Workflows for
    Apache Airflow ([https://aws.amazon.com/managed-workflows-for-apache-airflow/](https://aws.amazon.com/managed-workflows-for-apache-airflow/))
    allows you to create automated ML workflows by using native integration with SageMaker
    among other AWS services that are commonly used. Many organizations and teams
    already use or have invested in Airflow, so this service provides a way to take
    advantage of those existing investments using a managed service that includes
    native integrations with SageMaker for model building and deployment steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon SageMaker Operators for Kubernetes**: SageMaker Operators for Kubernetes
    ([https://docs.aws.amazon.com/sagemaker/latest/dg/amazon-sagemaker-operators-for-kubernetes.html](https://docs.aws.amazon.com/sagemaker/latest/dg/amazon-sagemaker-operators-for-kubernetes.html))
    allows teams to create SageMaker tasks natively using the Kubernetes API and command-line
    Kubernetes tools, such as **kubectl**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon SageMaker Components for Kubeflow Pipelines**: SageMaker Components
    for Kubeflow Pipelines allows teams to still utilize Kubeflow for workflow orchestration,
    while providing integrations with SageMaker so that you can create and run SageMaker
    jobs in managed environments without running them directly on your Kubernetes
    clusters. This is useful for taking advantage of end-to-end managed SageMaker
    features, but also for cases where you do not want to perform those tasks directly
    on your cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we''ll cover a few of the AWS services and features that can be used
    to incorporate CI/CD practices into your ML pipelines. These services are not
    unique to ML and can also be substituted for third-party tools offering similar
    capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS CodeCommit**: AWS CodeCommit ([https://aws.amazon.com/codecommit/](https://aws.amazon.com/codecommit/))
    is a private Git-based source code repository. For ML pipelines, AWS CodeCommit
    can store any related source code, such as **infrastructure as code** (**IaC**)/**configuration
    as code** (**CaC**), data processing code, training code, model evaluation code,
    pipeline code, and model deployment code. The structure of your repositories may
    vary, but in general, it''s recommended to at least separate your model build
    and model deploy code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS CodeBuild**: AWS CodeBuild ([https://aws.amazon.com/codebuild/](https://aws.amazon.com/codebuild/))
    is a fully managed build service that can be used for multiple purposes. These
    include compiling source code, running tests, and running custom scripts as part
    of a pipeline. For ML pipelines, AWS CodeBuild can be used for tasks such as testing
    through custom scripts and packaging AWS CloudFormation templates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS CodePipeline**: AWS CodePipeline ([https://aws.amazon.com/codepipeline/](https://aws.amazon.com/codepipeline/))
    is a fully managed CD service that can be used to orchestrate the steps of your
    ML pipeline. AWS CodePipeline can be used to orchestrate the steps for model build
    tasks, as well as model deploy tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding list of AWS services can be used to incorporate CI/CD practices
    for your ML pipelines. You can also optionally substitute the services above for
    third-party options, such as GitHub, BitBucket, or Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered a high-level ML workflow in the context of automating
    the tasks within key steps, as well as providing overall orchestration to automate
    those steps. We also discussed some of the key considerations when building your
    ML workflows. We reviewed the AWS-native options for creating automated ML workflows.
    We then looked at the AWS services that can be used to incorporate CI/CD practices.
  prefs: []
  type: TYPE_NORMAL
- en: All of these, as well as many third-party options, are valid options when selecting
    the right tooling for automating your SageMaker workflows. The decision to custom
    build workflows using the services mentioned in the preceding list, or the decision
    to substitute the services above with third-party options, typically comes from
    either personal preference or having organizational standards or requirements
    to utilize existing tooling.
  prefs: []
  type: TYPE_NORMAL
- en: For the remainder of this chapter, we'll focus on the SageMaker-native capabilities
    for automating your ML workflows and incorporating CI/CD practices.
  prefs: []
  type: TYPE_NORMAL
- en: Building ML workflows with Amazon SageMaker Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model build workflows cover all of the steps performed when developing your
    model, including data preparation, model training, model tuning, and model deployment.
    In this case, model deployment can include the tasks necessary to evaluate your
    model, as well as batch use cases that do not need to be deployed to higher environments.
    SageMaker Pipelines is a fully managed service that allows you to create automated
    model build workflows using the SageMaker Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Pipelines includes built-in step types ([https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html))
    for executing SageMaker tasks, such as SageMaker Processing for data pre-processing,
    and SageMaker Training for model training. Pipelines also include steps for controlling
    how your pipeline works. For example, the pipeline could include conditional steps
    that could be used to evaluate the output of a previous step to determine whether
    to proceed to the next step in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To include steps that perform tasks using other AWS services or non-AWS tasks,
    you must use the **callback step**. This is useful if you are using another AWS
    service for a task in your pipeline. One example could be if you are using AWS
    Glue for data preprocessing. *Figure 12.2* builds on the previous workflow illustration
    to indicate where SageMaker Pipelines fits into the end-to-end workflow, as well
    as providing examples of the supported SageMaker features for each model build
    workflow step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – SageMaker Pipelines model building workflows'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_12_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 – SageMaker Pipelines model building workflows
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you''ll build out a SageMaker pipeline for your ML use case.
    The pipeline will include all of the steps necessary for data preparation, model
    training, and model evaluation. Because we don''t need every SageMaker feature
    to build our pipeline, you''ll only be using the features noted in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – SageMaker Pipelines example pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_12_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – SageMaker Pipelines example pipeline
  prefs: []
  type: TYPE_NORMAL
- en: For each step in your SageMaker pipeline, you first need to configure the task
    that you will execute (for example, a training job) and then configure the SageMaker
    Pipelines step for that task. After all, steps have been configured, you chain
    the steps together and then execute the pipeline. The following sections will
    walk you through the steps in building your SageMaker pipeline for your example
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: Building your SageMaker pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we''ll walk through the steps needed to configure each step
    in your SageMaker pipeline, as well as how to chain those steps together and finally
    execute your model build pipeline. For each step in your pipeline, there are two
    steps to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure the SageMaker job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the SageMaker Pipelines step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 12.4* illustrates the steps that we will use to build the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Pipeline use case with SageMaker steps'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_12_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.4 – Pipeline use case with SageMaker steps
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with the data preparation step, where we'll use SageMaker Processing
    to transform our raw data into the format expected by the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this step, you''ll configure the SageMaker processing job that will be used
    to transform your data into a format expected by the algorithm. For this, we''ll
    use the same configuration from [*Chapter 4*](B17249_04_Final_JM_ePub.xhtml#_idTextAnchor072),
    *Data Preparation at Scale Using Amazon SageMaker Data Wrangler and Processing*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll configure the SageMaker processing job, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll configure the SageMaker Pipelines step that will be used to execute
    your data preparation tasks. For this, we''ll use the built-in processing step
    ([https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing))
    that tells Pipelines this step will be a SageMaker processing job. *Figure 12.5*
    shows the high-level inputs and outputs/artifacts that `ProcessingStep` used for
    data preprocessing will expect:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Data preparation pipeline step'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_12_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 – Data preparation pipeline step
  prefs: []
  type: TYPE_NORMAL
- en: 'We previously configured the processor, so we will now use that processor (combined
    with the other inputs shown in *Figure 12.4*) to set up our Pipelines step, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll enable **step caching**. Step caching tells SageMaker to check
    for a previous execution of a step that was called with the same arguments. This
    is so that it can use the previous step values of a successful run instead of
    re-executing a step with the exact same arguments. You should consider using step
    caching to avoid unnecessary tasks and costs. As an example, if the second step
    (model training) in your pipeline fails, you can start the pipeline again without
    re-executing the data preparation step if that step has not changed, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll define the runtime arguments using the `get_run_args` method.
    In this case, we are passing the Spark processor that was previously configured,
    in combination with the parameters identifying the inputs (raw weather data),
    the outputs (train, test, and validation datasets), and additional arguments the
    data processing script accepts as input. The data processing script, `preprocess.py`,
    is a slightly modified version of the processing script used in [*Chapter 4*](B17249_04_Final_JM_ePub.xhtml#_idTextAnchor072),
    *Data Preparation at Scale Using Amazon SageMaker Data Wrangler and Processing*.
    Refer to the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll use the runtime parameters to configure the actual SageMaker Pipelines
    step for our data preprocessing tasks. You''ll notice we''re using all of the
    parameters we configured previously to build the step that will execute as part
    of the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Model build step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, you'll configure the SageMaker training job that will be used
    to train your model. You'll use the training data produced from the data preparation
    step, in combination with your training code and configuration parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Although we do not cover it in this chapter specifically, it is important to
    note that SageMaker Pipelines now integrates with SageMaker Experiments, allowing
    you to capture extra metrics, as well as view corresponding plots in SageMaker
    Pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we''ll use the same configuration from [*Chapter 6*](B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117),
    *Training and Tuning at Scale*. Refer to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll configure the SageMaker training job, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll configure the SageMaker Pipelines step that will be used to execute
    your model training task. For this, we''ll use the built-in `training step` ([https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training)).
    This tells Pipelines this step will be a SageMaker training job. *Figure 12.6*
    shows the high-level inputs and outputs/artifacts that a **Training step** will
    expect:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Model build pipeline step'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_12_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.6 – Model build pipeline step
  prefs: []
  type: TYPE_NORMAL
- en: 'We previously configured the estimator, so we will now use that estimator combined
    with the other inputs shown in *Figure 12.6* to set up our Pipelines step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Model evaluation step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this step, you''ll configure a SageMaker processing job that will be used
    to evaluate your trained model using the model artifact produced from the training
    step in combination with your processing code and configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll configure the SageMaker processing job starting with `ScriptProcessor`.
    We will use this to execute a simple evaluation script, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll configure the SageMaker Pipelines step that will be used to execute
    your model evaluation tasks. For this, we''ll use the built-in Processing step
    ([https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing)).
    This tells Pipelines this step will be a SageMaker processing job. *Figure 12.7*
    shows the high-level inputs and outputs/artifacts that a Processing step used
    for model evaluation will expect:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Model evaluation pipeline step'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_12_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.7 – Model evaluation pipeline step
  prefs: []
  type: TYPE_NORMAL
- en: 'We previously configured the processor, so we will now use that processor combined
    with the other inputs shown in *Figure 12.7* to set up our Pipelines step. To
    do this, we''ll first set up the property file that will be used to store the
    output, in this case, model evaluation metrics, of our processing job. Then, we''ll
    configure the `ProcessingStep` definition as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Conditional step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this step, you''ll configure a built-in conditional step that will determine
    whether to proceed to the next step in the pipeline based on the results of your
    previous model evaluation step. Setting up a conditional step requires a list
    of conditions or items that must be true. This is in combination with instructions
    on the list of steps to execute based on that condition. *Figure 12.8* illustrates
    the inputs and outputs required for a conditional step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Conditional pipeline step'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_12_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.8 – Conditional pipeline step
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we''re going to set up a condition using the `if_steps` parameter.
    In this case, the next steps if the condition were true would be to register the
    model and then create the model that packages your model for deployment. You can
    optionally specify `else_steps` to indicate the next steps to perform if the condition
    is not true. In this case, we will simply terminate the pipeline if the condition
    is not true:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Register model step(s)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this final step, you''ll package the model and configure a built-in register
    model ([https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-register-model](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-register-model))
    step that will register your model to a model package group in SageMaker model
    registry. As seen in *Figure 12.9*, the inputs we''ll use to register the model
    contain information about the packaged model, such as the model version, estimator,
    and S3 location of the model artifact. This information, when combined with additional
    information such as model metrics and inference specifications, is used to register
    the model version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Conditional pipeline step'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_12_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.9 – Conditional pipeline step
  prefs: []
  type: TYPE_NORMAL
- en: 'This step will use data from the prior steps in the pipeline to register the
    model and centrally store key metadata about this specific model version. In addition,
    you''ll see an `approval_status parameter`. This parameter can be used to trigger
    downstream deployment processes (these will be discussed in more detail under
    SageMaker Projects):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Creating the pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding steps, we configured the tasks and steps that will be used
    as part of the model build pipeline. We now need to chain those steps together
    to create the SageMaker Pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: When configuring pipeline steps and creating a SageMaker pipeline, it is important
    to identify the parameters that could vary per pipeline execution and may be more
    dynamic. For example, the instance type for processing or training may be something
    you want to be able to change with each execution of your pipeline without directly
    modifying your pipeline code. This is where parameters become important in being
    able to dynamically pass in parameters at execution time. This allows you to change
    configurations (such as changing the instance type parameters) with each execution
    of your pipeline, based on different environments or as your data grows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the chaining together of our previously configured
    pipeline steps, as well as identifying the parameters we want to be able to pass
    in on each execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Executing the pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we''ve defined and configured our steps and the pipeline itself, we
    want to be able to execute the pipeline. To do this, you''ll need to perform a
    few steps. These steps need to be performed for each pipeline execution. A pipeline
    can be started in multiple ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Programmatically within a notebook (as shown in the example notebook for this
    chapter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under Pipelines in the SageMaker Studio UI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programmatically via another resource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through an EventBridge source triggered by an event or schedule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we'll focus on the steps required to execute your pipeline
    from your example notebook. First, you need to submit the pipeline definition
    to the SageMaker Pipelines service. This is done through an `upsert` that passes
    in the IAM role as an argument. Keep in mind that an `upsert` will create a pipeline
    definition if it doesn't exist or update the pipeline if it does. Also, the role
    that is passed is used by SageMaker Pipelines to create and launch all of the
    tasks defined in the steps. Therefore, you need to ensure that the role is scoped
    to the API permissions you need for your pipeline. It's a best practice to only
    include the API permissions that are actually needed so as to avoid overly permissive
    roles.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, you need to load the pipeline definition and then submit
    that definition through `upsert`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once your pipeline definition is submitted, you''re ready to start the pipeline
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There are multiple ways to check the status and progress of your pipeline steps.
    You can view your pipeline in the Studio console and click on each step to get
    metadata about each step, including the step logs. In addition, you can programmatically
    check the status of your pipeline execution. To do this, you can run `execution.describe()`
    to view the pipeline execution status, or `execution.list_steps()` to view the
    execution status and each step.
  prefs: []
  type: TYPE_NORMAL
- en: Running your pipelines ad hoc from a notebook is often acceptable during your
    model-building activities. However, when you're ready to move your models to production,
    it's common at that stage to find the most consistent and repeatable ways to trigger
    or schedule your model-building pipelines for model retraining.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, you can utilize the integration between SageMaker Pipelines and
    Amazon EventBridge ([https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html](https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html)).
    This integration allows you to trigger the execution of your SageMaker pipeline
    through event rules. These rules can be based on an event, such as the completion
    of an AWS Glue job, or they can be scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline recommended practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we covered how to set up a SageMaker pipeline using your example
    weather use case. As you build your own pipelines, they will likely vary in terms
    of the configuration required and the steps that should be included. However,
    the following general recommendations apply across use cases (unique considerations
    are highlighted where applicable):'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Pipelines has built-in steps supporting a variety of SageMaker jobs
    and the ability to utilize callback for custom steps. The built-in integrations
    with SageMaker steps simplify building and managing the pipeline. It is therefore
    recommended to **utilize SageMaker-native steps for the tasks in your pipeline**
    when possible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Utilize runtime parameters** for job arguments that are more likely to change
    between executions or environments, such as the size or number of ML instances
    running your training or processing jobs. This allows you to pass values in when
    you start the execution of the pipeline, as opposed to modifying your pipeline
    code every time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Enable step caching** to take advantage of eliminating unnecessary execution
    of steps in your pipeline. This will reduce costs, as well as reducing pipeline
    time when a previous pipeline step has already been successfully executed with
    the same parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we covered automating your model build ML workflows using SageMaker
    Pipelines. In the next section, we'll cover creating an end-to-end ML pipeline
    that goes beyond automation and incorporates CI/CD practices.
  prefs: []
  type: TYPE_NORMAL
- en: Creating CI/CD pipelines using Amazon SageMaker Projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: "In this section, we'll discuss using Amazon SageMaker Projects to incorporate\
    \ CI/CD practices into your ML pipelines. SageMaker Projects is a service that\
    \ uses SageMaker Pipelines and the SageMaker model registry, in combination with\
    \ CI/CD tools, to automatically provision and configure CI/CD pipelines for ML.\
    \ *Figure 12.10* illustrates \Lthe core components of SageMaker Projects. With\
    \ Projects, you have the advantage of a CD pipeline, source code versioning, and\
    \ automatic triggers for pipeline execution:"
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 12.10 – SageMaker Projects'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_12_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.10 – SageMaker Projects
  prefs: []
  type: TYPE_NORMAL
- en: Projects are made available through built-in SageMaker MLOps project templates
    or by creating your own organization's MLOps templates. The underlying templates
    are offered through AWS Service Catalog, via SageMaker Studio, and contain CloudFormation
    templates that preconfigure CI/CD pipelines for the selected template. Because
    projects rely on CloudFormation to provision pipelines, this ensures the practice
    of IaC/CaC to be able to consistently and reliably create CI/CD ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three core types of built-in SageMaker MLOps project templates. *Figure
    12.11* shows the three primary types: 1\. **Build and Train Pipeline**, 2\. **Deploy
    Pipeline**, 3\. **Build, Train, and Deploy Pipeline**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – SageMaker Projects'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_12_11_new.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.11 – SageMaker Projects
  prefs: []
  type: TYPE_NORMAL
- en: First, there is a build and train template. This covers the tasks required in
    data preparation, feature engineering, model training, and evaluation. This template
    is useful when you are performing model build activities on SageMaker but deploying
    your model somewhere else. It is also useful if you have batch-only use cases.
    In this case, Projects will automatically provision and seed a source code repository
    for a model build pipeline, set up pipeline triggers for changes to that code
    repository, and create a model group in the model registry. You are then responsible
    for going in and modifying that pipeline code to match your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Second, there is a model deployment template. This template is useful when you
    are looking to standardize SageMaker for hosting. In this case, Projects will
    automatically provision and seed a source code repository for a model deploy pipeline
    that deploys to a SageMaker endpoint based on triggers and information pulled
    from the model registry.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there are end-to-end templates that cover all phases, including build,
    train, and deploy. These templates cover AWS Developer Services (AWS CodePipeline,
    AWS CodeCommit, AWS CodeBuild), or allow the option to utilize third-party source
    code repositories (GitHub, GitHub Enterprise, BitBucket, or Jenkins) for orchestration.
    In this case, Projects will automatically provision and seed source code for both
    model build and model deploy activities. Projects will also set up the triggers
    for both model build, and model deploy activities. Again, you are then responsible
    for going in and modifying seed code to meet your use case.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we examined SageMaker projects. We concluded that it is a service
    that can be used to incorporate CI/CD practices into your ML pipelines. We'll
    now cover some of the recommended practices when using SageMaker projects.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker projects recommended practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding section, we covered SageMaker projects as a way to incorporate
    CI/CD practices into your ML pipelines by using a managed AWS service that will
    automatically provision and configure the integrations that are required. We'll
    now cover some of the general recommended practices when using SageMaker projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you use SageMaker projects, the customizations for your use case can vary
    between customizing the code within the built-in MLOps project templates or creating
    your own fully custom MLOps project templates. As a result, there can be a lot
    of variance between pipelines in order to meet the requirements of your organization
    and use case. However, there are some general recommendations that apply across
    use cases, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Utilize built-in MLOps project templates when they meet your requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you have unique requirements, such as additional deployment quality gates,
    create custom MLOps project templates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When creating custom MLOps project templates, it is often easier to use the
    AWS CloudFormation templates used for the built-in MLOps project templates as
    a starting point and then modify accordingly. All of the built-in MLOps project
    templates are available and visible in AWS Service Catalog.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we covered adding CI/CD practices to your automated workflows
    using SageMaker projects. We also discussed the MLOps project template options
    that are available. Finally, we discussed additional considerations and best practices
    when using SageMaker projects.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first covered general considerations for automating your
    SageMaker workflows. We then discussed automating your SageMaker model build workflows,
    specifically through using SageMaker Pipelines. The steps required to build out
    a pipeline for your weather use case were highlighted in order to illustrate SageMaker
    Pipeline usage. Finally, we discussed how you can enhance that automated model
    build workflow by using SageMaker projects to incorporate CI/CD practices, in
    addition to the automation offered by SageMaker Pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll discuss the AWS Well-Architected Framework, specifically
    looking at how best practices across each Well-Architected pillar map to SageMaker
    workloads.
  prefs: []
  type: TYPE_NORMAL
