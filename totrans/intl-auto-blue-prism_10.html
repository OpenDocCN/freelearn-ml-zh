<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer266">
			<h1 id="_idParaDest-153" class="chapter-number"><a id="_idTextAnchor157"/>10</h1>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor158"/>IA’s Impact on the Robotic Operating Model</h1>
			<p>The <strong class="bold">Robotic Operating Model</strong> (<strong class="bold">ROM</strong>) is what I call the <em class="italic">management</em> side of RPA. It distills <a id="_idIndexMarker775"/>BP’s 20+ years of automation experience and best practices into a framework that can be applied to all types of firms. This framework’s purpose is to help ensure the continued success and growth of automation in a company. This includes areas that may not be immediately obvious, such as securing executive sponsorship, developing career paths, choosing an appropriate organizational structure, and facilitating <span class="No-Break">cultural adoption.</span></p>
			<p>Although the ROM isn’t a technology product, it’s still a key differentiator between BP and its competitors. Anyone who works in RPA should become familiar with the ROM, especially since its guidance is vendor-agnostic. As technologies and the regulatory environment change, so must our management frameworks; the ROM is constantly undergoing revisions and is currently on version 2.0. More information about the ROM can be found <span class="No-Break">here: </span><a href="https://community.blueprism.com/content/rom-hub"><span class="No-Break">https://community.blueprism.com/content/rom-hub</span></a><span class="No-Break">.</span></p>
			<p>This new version of the ROM is organized into five foundations: <strong class="bold">Strategy</strong>, <strong class="bold">Workforce</strong>, <strong class="bold">Design</strong>, <strong class="bold">Development</strong>, and <strong class="bold">Operations</strong>. Each foundation is further subdivided into six subtopics, leading to 30 topics overall. While IA is certainly mentioned in the ROM, it isn’t one of the primary focuses. As a framework, the ROM is meant to provide general guidance, leaving us to fill in specifics. Trying to fill in these specifics was one of the main goals of my IA research, and you’ll find a number of my research results here. In this chapter, we’ll discuss how IA impacts the five ROM foundations so that you’re better prepared to tackle IA implementation in <span class="No-Break">your organization:</span></p>
			<ul>
				<li><span class="No-Break">Strategy</span></li>
				<li><span class="No-Break">Workforce</span></li>
				<li><span class="No-Break">Design</span></li>
				<li><span class="No-Break">Development</span></li>
				<li><span class="No-Break">Operations</span></li>
			</ul>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor159"/>Strategy</h1>
			<p>Three <a id="_idIndexMarker776"/>of the subtopics within the Strategy foundation are more heavily affected by IA. These are <em class="italic">Future of Work Vision</em>, <em class="italic">Business Case and Value</em>, and <em class="italic">Governance, Risk, and Controls</em>. The Future of Work Vision subtopic discusses the vision statement, mission statement, and objectives of the<a id="_idIndexMarker777"/> overall automation program. It also discusses the importance of having a communication plan in place to ensure that the IA vision is spread throughout the organization. Business Case and Value is about ensuring that IA is aligned with corporate strategy and that there are measurable KPIs to support that argument. Finally, Governance, Risk, and Controls discusses the governance board and the management <span class="No-Break">of risk.</span></p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor160"/>Future of Work Vision</h2>
			<p>If we’re looking <a id="_idIndexMarker778"/>to transition from RPA to IA, it’s important that the head of automation and executive sponsors explicitly update the vision to include this. For the more specific portions of the vision (mission statement and objectives), we can specify whether the focus should be placed on integrating pre-built ML services, such as API-based OCR, publicly available LLMs, and so on, or developing in-house expertise for building custom <span class="No-Break">ML models.</span></p>
			<p>Setting up the vision to include IA would be best done for RPA teams that have been in operation for at least 1 or 2 years. This would be a minimum of level 3 in the ROM Maturity <span class="No-Break">Model: </span><a href="https://community.blueprism.com/content/rom-hub/rom2-maturity-model"><span class="No-Break">https://community.blueprism.com/content/rom-hub/rom2-maturity-model</span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor161"/>Business case and value</h2>
			<p>As ML is a<a id="_idIndexMarker779"/> trending technology, no shortage of executive management wants to see it being used across their business. Despite executive support, we should still work out how IA can be measured as contributing to the overall corporate strategy. Commonly used KPIs, such as <strong class="bold">full-time employee</strong> (<strong class="bold">FTE</strong>) savings and <a id="_idIndexMarker780"/>hours returned to business, rely on quantifying how much time is saved by Digital Workers versus human workers. But for IA, the amount of time saved can be negligible – for instance, if we’re replacing an expert’s decision-making process that takes only minutes to perform. We may need to shift from time-based and dollar savings-based KPI measurements to <em class="italic">business-value-based</em> assessments. Examples of these can be found in the ROM 2 <span class="No-Break">training materials.</span></p>
			<p>The <strong class="bold">total cost of ownership</strong> (<strong class="bold">TCO</strong>) also <a id="_idIndexMarker781"/>changes significantly with IA. We still have the traditional RPA costs, but we also have the costs of ML on top of that. Three primary scenarios lead to different ML costs on an IA project. The first case is if ML development is outsourced to a third party. This will likely have fixed and variable development costs. There are also possible ongoing service costs if the model is hosted by a third party. If the model is deployed back on-prem<span class="Annotation-reference">i</span>ses and managed completely by internal teams, there will still be internal ongoing costs. The second is consuming ML through an API service, which incurs per-transaction costs. These costs can be forecasted based on expected work volumes and the <span class="No-Break">API pricing.</span></p>
			<p>The third <a id="_idIndexMarker782"/>scenario is if ML is developed in-house. Through cursory research, I found that deploying an in-house ML solution into production for the first time costs roughly 100,000 USD, excluding RPA costs. IA is similar to introducing any new technology, in that there are high costs for the first project, but the marginal costs decrease as more IA projects are deployed into production. Deciding to build ML in-house should be thought of as a multi-year endeavor, not as a one-off or pilot. The costs of in-house ML are mostly from salaries and <span class="No-Break">renting hardware.</span></p>
			<p>Outside of ML which is completely consumed through API calls, models will have ongoing costs as data and model performance need to be actively monitored, and models must be rebuilt. There may be costs related to hiring ML prediction reviewers <span class="No-Break">as well.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor162"/>Governance, Risk, and Controls</h2>
			<p>One way to <a id="_idIndexMarker783"/>think about IA governance is to take the governance concerns of RPA and add them to the governance of ML. On its own, ML governance is already a huge topic, so the <em class="italic">governance board</em> must have someone very familiar with productionizing ML and can keep up to date with the shifting regulatory environment <span class="No-Break">surrounding it.</span></p>
			<p>A starting point for the IA team to develop proper governance is to look internally for existing data and AI policies around data privacy, data retention, and security. We also need to consider whether the use of an ML model is advisable from a fairness and ethical perspective. Currently, very few companies have AI ethics policies in place. Some potential starting points for developing internal ethical guidelines are the <em class="italic">IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems</em> and the <em class="italic">Montreal Declaration for </em><span class="No-Break"><em class="italic">Responsible AI</em></span><span class="No-Break">.</span></p>
			<h3>Risks</h3>
			<p>IA has numerous<a id="_idIndexMarker784"/> risks associated with it that aren’t present in RPA. First, we need to <em class="italic">comply</em> with existing and upcoming laws around AI. As discussed in <a href="B18416_04.xhtml#_idTextAnchor062"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, nations are already developing laws to govern the use of AI <span class="No-Break">in business.</span></p>
			<p>Another category of risks is <em class="italic">model-based</em>, with the primary one being low prediction accuracy. The performance of ML models is also known to degrade over time and certain types of ML algorithms are prone to <span class="No-Break">adversarial attacks.</span></p>
			<p>There are also <em class="italic">data-based</em> risks, which include data bias, data quality, and data drift. Data bias refers to data that has “undesirable properties,” such as when the collected data underrepresents certain populations when it shouldn’t. Data quality refers to the presence of “desirable properties,” such as having a sufficient number of samples and having features that are highly relevant to the prediction that we’re trying to make. Data drift refers to changes in the underlying input data distributions that occur naturally over time. An example of data drift can be seen in consumer purchasing behavior. Even among similar age and salary ranges, the types of items that people purchase today are different from what people purchased 10 <span class="No-Break">years ago.</span></p>
			<p>The next class of risks that can be reduced through ML governance has to do with <em class="italic">security and data access</em>. The introduction of new servers that host models, and new staff that must interact with data and models, increases the potential areas of attack that can target an <span class="No-Break">IA solution.</span></p>
			<h3>Addressing risks through governance</h3>
			<p>Governance<a id="_idIndexMarker785"/> can help address all of the risks that were mentioned in the previous section. For <em class="italic">compliance risks</em>, governance can mandate that all IA solutions have a way to disable ML predictions (the kill switch we designed in <a href="B18416_06.xhtml#_idTextAnchor093"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>), a preference for inherently interpretable algorithms, and a channel for customers to request that ML isn’t used when handling <span class="No-Break">their cases.</span></p>
			<p>ML governance can help reduce <em class="italic">model-based risks</em>. Governance can require that we implement formalized review and testing processes to ensure that a minimum proportion of predictions are correct before allowing a model to reach production. We might also mandate that general ML explainability methods, such as LIME and SHAP, are used to assess models before they’re put <span class="No-Break">into production.</span></p>
			<p>Governance <a id="_idIndexMarker786"/>should define requirements around the continuous monitoring of model performance since models are known to lose predictive power over time. This includes collecting human-reviewed predictions for regular review and understanding how the frequencies of predicted labels for classification change <span class="No-Break">over time.</span></p>
			<p>Governance can address <em class="italic">data risks</em> by implementing standards for data, and training requirements for data scientists. An example of establishing data quality might be to mandate that data must have a minimum of 1,000 samples per label, and that each row must have fewer than 5% missing columns. Governance can request data scientists to complete training around identifying data biases before allowing them to develop models for <span class="No-Break">IA projects.</span></p>
			<p>Governance can also set standards around the need to monitor data regularly. First, baseline statistics need to be gathered, usually from the training data. Some common statistics that are calculated include the mean, maximum, minimum, and standard deviations of data columns. Then, new statistics are regularly recalculated based on the input data used in production. Governance might mandate that this be done monthly and that any changes in data columns with more than one standard deviation be manually assessed or trigger the model to be rebuilt and deployed <span class="No-Break">into production.</span></p>
			<p>For <em class="italic">security</em>, we need to amend the existing governance policies to define who can make changes to these IA components and the procedures for doing so. This can make use of the <em class="italic">ML Deployer</em> and <em class="italic">ML Reviewer</em> roles that were developed in <a href="B18416_08.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor163"/>Workforce</h1>
			<p>For the<a id="_idIndexMarker787"/> workforce foundation, we<a id="_idIndexMarker788"/> will dive deeper into <em class="italic">Building Your Organizational Model</em>, <em class="italic">Adopting New Ways of Thinking and Working</em>, and <em class="italic">Roles and Career Paths</em>. Building Your Organizational Model discusses different ways that the IA function can be structured in a company. Some examples include IA as a central unit that serves everyone, individual IA teams within different business units, or IA being completely outsourced to a vendor. <em class="italic">Adopting New Ways of Thinking and </em>Working is about influencing the organization and <a id="_idIndexMarker789"/>overcoming resistance to get buy-in into IA. Finally, <em class="italic">Roles and Career Paths </em>is about which skillsets and roles are needed to create successful IA outcomes and ensure career progression within the <span class="No-Break">IA team.</span></p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor164"/>Building your organizational model</h2>
			<p>Some<a id="_idIndexMarker790"/> of the main things to consider regarding the organization model are where ML expertise resides in the organization, how to engage with it, and whether ML expertise needs to be brought directly into the IA team. If the firm already has a centralized data science team, we have to consider their organizational model as well, and how they expect to provide ML expertise to the rest of <span class="No-Break">the company.</span></p>
			<p>If the company is serious about IA, it’s expected that the automation team will have at least a few <a id="_idIndexMarker791"/>members who can perform data science work without relying on external teams. For the <strong class="bold">Center of Excellence</strong> (<strong class="bold">COE</strong>), <em class="italic">Franchise</em>, or <em class="italic">Hub and Spoke</em> organizational models, the central team should have in-house expertise that can productionize ML models. For <em class="italic">Divisional</em>, <em class="italic">Divisional Alliance</em>, and <em class="italic">Outsourced Managed Service</em> models, ML expertise can sit in those <span class="No-Break">teams instead.</span></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor165"/>Adopting new ways of thinking and working</h2>
			<p>It’s important to <a id="_idIndexMarker792"/>understand ways in which IA can be opposed by both <em class="italic">individual staff</em> and <em class="italic">management</em>. The adoption of IA will likely be more difficult to manage than RPA due to the widened scope of what can potentially be automated. During my research, I identified some important ways in which employees and management oppose IA adoption and some ways to counteract <span class="No-Break">this opposition.</span></p>
			<h3>IA resistance at the employee level</h3>
			<p>IA can<a id="_idIndexMarker793"/> lead to a <em class="italic">loss of job meaning</em> if work that’s deemed meaningful to the employee is automated. A real-life example of this is replacing social workers’ elderly benefit screening phone calls with chatbots and RPA. Interviews with the affected employees showed that they had lower job satisfaction post-automation. A questionnaire called the <strong class="bold">Work and Meaning Inventory</strong> (<strong class="bold">WAMI</strong>) can<a id="_idIndexMarker794"/> be used to establish whether employees have experienced a loss of job meaning after IA <span class="No-Break">is implemented.</span></p>
			<p>Job meaningfulness<a id="_idIndexMarker795"/> can be broken into four components: the <em class="italic">individual</em>, the <em class="italic">job</em>, the <em class="italic">organization</em>, and <em class="italic">society</em>. If we think that IA will reduce job meaningfulness for someone, we can try to counteract this by targeting improvements at these different levels. For instance, at an <em class="italic">individual</em> level, we could provide more flexibility and autonomy to affected staff by allowing work-from-home days. At the <em class="italic">job</em> level, we could change the significance, visibility, and scope of the work of the affected people. At the <em class="italic">organizational</em> level, we could increase the number of corporate social responsibility activities. A more practical approach would be to ask affected staff whether the proposed automation has the potential to reduce their job satisfaction and to avoid automating <span class="No-Break">those areas.</span></p>
			<p>Another risk that employees can encounter is <em class="italic">reduced work preparedness</em>. In one financial services firm, documents were digitized and automatically input into internal systems through IA, instead of being input by the case workers themselves. Removing the manual digitization and entry tasks meant that less time was spent looking at customer’s details. The affected staff felt more anxious and less prepared to directly interact with customers. Counteracting reduced work preparedness requires making access to data simpler or improving the presentation of the data through <span class="No-Break">summary dashboards.</span></p>
			<p>IA potentially affects employees’ sense of overall <em class="italic">job security</em>. There are two parts to measuring job security. First, we can measure how “stable” an employee thinks their job is through a <em class="italic">Job Security Index</em> questionnaire. Next, we can measure the employee’s attitude, given how they view their level of job security, through the <em class="italic">Job Security Satisfaction scale</em>. If practical, we can engage HR to measure these before and after IA has been implemented, to determine whether there are job security concerns that need to <span class="No-Break">be addressed.</span></p>
			<p>If the goal of IA isn’t to reduce headcount, make “no job loss” an explicit message and publicize which measurable metrics will be used to evaluate the success of the IA program. One message that SS&amp;C has put forward since implementing IA is not to cut jobs but to slow hiring down. If the goal is to cut jobs, training plans should be prepared to educate the staff that will be retained on how to work together with the digital workers. The job role definitions for staff should also be revised. These two actions can help reduce the perception of <span class="No-Break">job insecurity.</span></p>
			<p>A key to counteracting the potential negative impact on employees more broadly is developing an understanding of AI sentiment across the organization. In all companies, there will be<a id="_idIndexMarker796"/> people who support or want to work with AI technologies, and there will be those who don’t. The first rollouts of IA should target individuals or departments that view AI positively as this improves the odds of initial positive results, which can help convince other areas of the business of <span class="No-Break">IA’s benefits.</span></p>
			<h3>IA resistance at the management level</h3>
			<p>IA may face <em class="italic">managerial resistance</em> as it’s often presented as a way to reduce headcount and <a id="_idIndexMarker797"/>costs. It’s natural for managers to be concerned about their budget, sphere of influence, and ability to meet KPIs if their headcount stays the same or is reduced. IA can lead managers to passively resist, stall, and even actively sabotage IA efforts. IA metrics that are gathered by uncooperative management teams will often be underreported to <span class="No-Break">undermine IA.</span></p>
			<p>Studies have been performed to understand management resistance to automation. <em class="italic">Senior management</em> opposition is mostly due to a lack of knowledge. <em class="italic">Middle management</em>, who directly supervise staff or processes that will be replaced by IA, have more nuanced reasons for opposition. In order of importance, these are being unconvinced, not having enough training, and worrying about their job importance. The fear of losing headcount was also directly mentioned as a major <span class="No-Break">management concern.</span></p>
			<p>The proposed ways of addressing managerial resistance in research are quite vague from a practical point of view. Suggestions to reduce resistance include education, using data to convince management, implementing change gradually, and training. One practical way of reducing the fear of headcount loss (assuming that it isn’t the primary goal of IA) is to explicitly ask managers to submit plans on what other value-added tasks staff will be performing post-IA implementation. Going through the motions of planning how their staff will be used after freeing up their time can help <span class="No-Break">alleviate fears.</span></p>
			<p>Managers also worry that IA can trigger <em class="italic">employee turnover</em> and lower employee retention rates. There have been studies specifically about AI adoption, and its impact on employees. As expected, staff with negative attitudes toward AI are more likely to quit if AI is adopted. Again, we should understand how staff perceive AI and target use cases that affect employees with positive AI perceptions. Turnover intention is weakened when employees feel they have support from the company. This includes many things, including team building exercises, career planning, employee development, education, <span class="No-Break">and more.</span></p>
			<p>Another key <a id="_idIndexMarker798"/>management fear is that of <em class="italic">financial loss</em>. This has been expressed in two main ways. The first is financial loss through litigation – for instance if someone belonging to a protected class feels they’ve been treated unfairly due to biases in an ML model and decides to pursue legal action. The second type of financial loss is from incorrect predictions, which lead to further incorrect processing. We can manage these fears by allowing management to review the governance that’s in place to select models for production use, select IA use cases, and review <span class="No-Break">specific predictions.</span></p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor166"/>Roles and career paths</h2>
			<p>If the <a id="_idIndexMarker799"/>overall strategy of the organization is to consume ML through APIs or other pre-built services, no changes need to be made to the roles that are needed on the team. Existing senior developers or team leads should already be equipped to handle these types of integrations. The exception to this is needing <em class="italic">ML reviewers</em>, but this role likely sits outside of the IA team, on the <span class="No-Break">business side.</span></p>
			<p>However, if the strategy is to develop ML capability internally, a few new roles emerge. The first is the <em class="italic">IA data scientist</em>. This person will be responsible for analyzing data, building models, testing models, and evaluating them. I’d recommend hiring a person with these skills or borrowing this person from elsewhere in the organization instead of developing someone internally as the first data scientist. The main reason for this is that the time investment needed to become a data scientist is <span class="No-Break">extremely long.</span></p>
			<p>This data scientist can then build up a team of <em class="italic">IA developers</em>, which is a hybrid role between the data scientist and the automation developer. IA developers will start with a strong RPA base, and slowly build up their data science skills over time. IA developers should also keep up to date on what commercial ML services are available on the market. IA developers with enough experience can become full-fledged data scientists as a lateral <span class="No-Break">career move.</span></p>
			<p>An important question is who manages the ML infrastructure. As the IA initiative grows, we may need a dedicated <em class="italic">IA technical architect</em> to define the appropriate deployment method, manage deployments and rollbacks, monitor data, and manage other infrastructure concerns that support IA. Unlike the “traditional” RPA technical architect role, which often isn’t a full-time position, the IA technical architect will likely have enough work <a id="_idIndexMarker800"/>to eventually become full-time as the IA program grows, due to the ongoing management needs of <span class="No-Break">ML models.</span></p>
			<p>It’s unrealistic to expect that <em class="italic">citizen developers</em> will ever be able to build and deploy ML models. Citizen developers will eventually be able to use AutoML services (such as <strong class="bold">Decision</strong>, as discussed in <a href="B18416_13.xhtml#_idTextAnchor212"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>), and <strong class="bold">LLMs</strong>, although the use of LLMs has numerous challenges to overcome, such as dealing with hallucinated results, hallucinated confidence scores, and parsing an unstructured response. The <em class="italic">governance board</em> and <em class="italic">design authority</em> will need to set guidelines on how citizen developers can <span class="No-Break">use AI.</span></p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor167"/>Design</h1>
			<p>In this<a id="_idIndexMarker801"/> section, we will dive <a id="_idIndexMarker802"/>deeper into the subtopics of <em class="italic">Assessment and Prioritization</em>, which discusses how to select IA processes for development, and <em class="italic">Requirements Design</em>, which discusses capturing the process steps and functional requirements of the <span class="No-Break">business process.</span></p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor168"/>Assessment and Prioritization</h2>
			<p>As part of <a id="_idIndexMarker803"/>the <em class="italic">assessment phase</em> of process discovery, a use case should have endorsement from an experienced data scientist as to whether the ML portion is even feasible or not, before moving on to process analysis. Part of the <em class="italic">analysis phase</em> should include either the POC-style development of an ML model or testing ready-to-use APIs to ensure that there’s sufficient accuracy (or other desired metrics) to warrant continuing with the IA use case. Again, there must be someone on the <em class="italic">governance board</em> who is experienced with ML to greenlight IA processes <span class="No-Break">for development.</span></p>
			<p>One important area to assess is how IA will <em class="italic">affect existing SLAs</em>. While we’d expect IA to increase the overall throughput of work that can be completed, the SLA might also include <em class="italic">quality-based targets</em>, which would depend on the accuracy of the model that’s used. An example of a quality-based target would be to maintain a customer satisfaction score <span class="No-Break">above 3/5.</span></p>
			<p>The introduction<a id="_idIndexMarker804"/> of new infrastructure to deploy ML also affects SLAs since it’s a new <em class="italic">source of downtime</em>. Many ML models are cloud-hosted on major cloud platforms, such as GCP, AWS, and Azure. In December 2021, AWS had multiple outages during the workday. Azure also had a major outage in January 2023. If your model was hosted there at that time, there’s a chance that you’d have an SLA breach due to the ML model <span class="No-Break">being unavailable.</span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor169"/>Requirements Design</h2>
			<p>Under IA, we <a id="_idIndexMarker805"/>need to help the business define requirements that didn’t exist before. For example, what are the <em class="italic">criteria to trigger the HITL review</em> of predictions? As seen in <a href="B18416_04.xhtml#_idTextAnchor062"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, this can include random sampling, different threshold values for different labels, or formula-based methods. The specific threshold values themselves don’t need to be picked yet as they should be chosen based on experimentation on the <span class="No-Break">candidate models.</span></p>
			<p>The business also needs to define <em class="italic">how prediction data will be presented to reviewers</em>, such as through Excel, a custom-developed website, or a database. Once a prediction is made, we need to know whether there’s a maximum allowable <em class="italic">delay between a completed prediction and human review</em> for <span class="No-Break">SLA purposes.</span></p>
			<p>There can also be requirements around the data and model itself. In some use cases, some data fields, such as age and gender, are protected and cannot be used in the model. If data is deemed <em class="italic">sensitive</em>, it likely can’t be sent outside the organization, removing online ML APIs from being considered in our <span class="No-Break">solution design.</span></p>
			<p>If models need to be <em class="italic">explainable</em>, this implies that certain types of regression and tree models should be favored. We also might have <em class="italic">desired algorithms</em> already in mind, such as deep learning, or LLMs, which impose hardware requirements (GPUs) on solutions. We also need to understand whether predictions are <em class="italic">time-sensitive</em> as this also informs the data scientists on which algorithms are possible and roughly what hardware requirements are needed to deploy <span class="No-Break">a solution.</span></p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor170"/>Development</h1>
			<p>The <em class="italic">Methodology and Teamwork</em> subsection is affected by IA as there’s a separate development<a id="_idIndexMarker806"/> cycle for ML models that runs in <a id="_idIndexMarker807"/>parallel to the traditional RPA development. In terms of <em class="italic">Delivery Controls</em>, having a high-enough quality model also often acts as a go-no-go decision toward continuing the development of the IA solution. Under IA, <em class="italic">Testing and Quality Assurance</em> never really stop as the ML model needs constant monitoring to ensure that quality levels <span class="No-Break">are maintained.</span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor171"/>Methodology and Teamwork</h2>
			<p>ML model <a id="_idIndexMarker808"/>development is normally independent of RPA development. While BP has a six-phase delivery methodology (Define, Design, Build, Test, UAT, and Deploy), this isn’t exactly applicable to ML model building. For example, many ML model development methodologies have data analysis and refinement phases, which aren’t needed <span class="No-Break">in RPA.</span></p>
			<p>If the ML model is built by a different team, we don’t need to think too much about what particular ML methodology is being used. However, if ML is being built internally, the IA team should look to standardize their ML model development approach. There are many examples of this online – for example, from AWS (<a href="https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning-lifecycle.html">https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning-lifecycle.html</a>) and <span class="No-Break">GCP (</span><a href="https://cloud.google.com/blog/products/ai-machine-learning/making-the-machine-the-machine-learning-lifecycle"><span class="No-Break">https://cloud.google.com/blog/products/ai-machine-learning/making-the-machine-the-machine-learning-lifecycle</span></a><span class="No-Break">).</span></p>
			<p>It’s normal for the ML model development life cycle to begin before the <em class="italic">Process Definition Document</em> is finalized, especially for gathering and analyzing training data. During the <em class="italic">Design</em> phase of development, we need to work with the data scientists to choose the interface between the ML model and BP, whether that be Web API calls, executables, programmatic scripts, or Code Stages. We also need the data scientists to provide different examples of prediction responses so that they can be mocked in BP. This allows RPA developers to test their work locally, even if the ML program isn’t ready yet. The functional requirements should also be discussed, such as required SLAs, response times, whether we can send prediction requests in batch, and so on. This should all be documented in the <em class="italic">Solution </em><span class="No-Break"><em class="italic">Design Document</em></span><span class="No-Break">.</span></p>
			<p>It’s often the case that for the first few initial IA projects, the RPA team needs to borrow expertise from<a id="_idIndexMarker809"/> elsewhere in the organization. If data scientists aren’t fully dedicated to the project, there’s a risk that work on the IA project will be deprioritized and delivery timelines get <span class="No-Break">pushed back.</span></p>
			<p>The <em class="italic">Design Authority</em> should have someone who has hands-on experience developing models. If models are developed internally, thought needs to be put into whether a model can be designed to be reused across multiple IA use cases, or whether models must remain separate. Part of the Design Authority’s work will be extended to keep track of what ML models are being used <span class="No-Break">and where.</span></p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor172"/>Delivery Controls</h2>
			<p>During the <em class="italic">Build</em> phase<a id="_idIndexMarker810"/> of RPA or the equivalent ML development stage, we need to keep in mind that insufficient prediction accuracy can lead to a no-go decision in terms of continuing development on the overall IA solution. In a sense, the progress of ML model development should be slightly ahead of RPA development to minimize the risk of wasted <span class="No-Break">development work.</span></p>
			<p>The UAT of the ML model is likely to be done completely separately from RPA. Completion of ML model UAT should be part of the entrance criteria before starting the <span class="No-Break">RPA UAT.</span></p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor173"/>Testing and Quality Assurance</h2>
			<p>During <a id="_idIndexMarker811"/>the UAT of the ML model, it’s important to capture the predicted result and the confidence scores. This data is needed to help us calibrate the appropriate confidence scores needed for different labels, in case thresholding is used to determine whether human review <span class="No-Break">is needed.</span></p>
			<p>One of the main differences between IA and RPA is that ML model QA never really stops. An ML operations team should monitor the results of ML predictions regularly to detect <span class="No-Break">potential drift.</span></p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor174"/>Operations</h1>
			<p>IA affects <em class="italic">Deploy and Release</em>, as ML deployments are expected to be done even if there are no<a id="_idIndexMarker812"/> changes to the business logic (Process) or applications (Objects). The <em class="italic">Support Model</em> has numerous changes as the addition of ML<a id="_idIndexMarker813"/> brings in numerous ongoing operations that need to be performed, including monitoring models, monitoring data, exporting HITL review results, <span class="No-Break">and more.</span></p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor175"/>Deploy and Release</h2>
			<p>As we saw in <a id="_idIndexMarker814"/>the previous chapter, ML models are expected to be deployed regularly into production, on a schedule that’s independent from Process and Object updates. The deployment methodology that’s used for the ML model informs the Control Room operators of how the overall IA solution must be updated, to maintain auditability, and also how to roll back in case there’s an issue with the newly deployed model. It’s recommended to choose a deployment method that doesn’t require downtime to roll back and for the prediction response of the model to return the model version that was used to make the prediction, for auditing purposes. Deploying new ML models should also require a formal change request, which <span class="No-Break">requires approval.</span></p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor176"/>Support model</h2>
			<p>IA brings<a id="_idIndexMarker815"/> significant additions to the support model. We now have to worry about the uptime of the ML model (business continuity), ensuring the accuracy of the ML model (this can be assessed through random sampling), and potentially the SLAs for human review. Each ML model will have their own ML reviewers, who potentially also need training and access to BP as they might be manually changing the Statuses of Work Queue Items or editing Session Variables to recreate <span class="No-Break">review data.</span></p>
			<p>If the ML models are maintained within the IA team, the models and their data will need constant monitoring to prevent performance degradation and data drifts. This is usually done through web-based dashboards that read ML server logs. This works if ML is deployed through Web APIs, but not if models are called through the CLI or Code Stages. In this case, the support model needs to include exporting ML results, from the Work Queue or Session Logs, to data scientists for analysis, and ingestion into monitoring systems. Regardless of the method used by BP to make the ML prediction, we still need to feedback information regarding reviewed predictions to the data scientists as that data isn’t present in ML API <span class="No-Break">server logs.</span></p>
			<p>Monitoring data and models can lead to <em class="italic">regular updates and deployments of ML models</em>. Other concerns, such as updates to the ML libraries, the addition of new training data received by the HITL reviews, or experimentation with new algorithms, can also lead to model updates. Changes in business logic can also trigger changes to the ML model. The IA support team needs to become very familiar with deploying new models <span class="No-Break">into production.</span></p>
			<p>Another <a id="_idIndexMarker816"/>major concern (related to referral handling and exception handling, although it isn’t directly one of the two) is something called <em class="italic">time lag effects</em>. Imagine<a id="_idIndexMarker817"/> that we’re a bank that’s using ML to determine whether an account opening application is fraudulent or not. If the ML model is flawed, many fraudulent account creation requests will be processed further, with the data being sent to many other systems. Thousands of account opening applications might have been processed before the model flaw was detected. The amount of work that needs to be undone or corrected between making an incorrect ML prediction, and discovering it, is a time <span class="No-Break">lag effect.</span></p>
			<p>Issues with time lag are inevitable unless the ML model is perfect. The support model needs to consider how to deal with individual cases – for example, if a business user flags a case where an incorrect ML prediction has led to incorrect processing. The support model also needs to deal with batch cases, which can be caused by flawed ML models. It might even be necessary to create a separate automated process specifically to undo the steps that were performed by the IA <span class="No-Break">solution post-prediction.</span></p>
			<p>The support model should try to address <em class="italic">how liability should be assigned</em> to a prediction that has led to losses of some sort. At some point, a person or a team will need to be held responsible for the incorrect processing caused by an incorrect ML prediction. Is it someone on the IA team or the data science team? Is it the business users who have signed off on the model testing results? Is it the reviewers (assuming that the prediction in question has been reviewed)? This is something that needs to be agreed on by all parties before <span class="No-Break">finger-pointing occurs.</span></p>
			<p>ML liability is an active area of legal research. I’ve found that legal experts believe that the company that makes use of the prediction will be held liable, even if the ML model development is completely outsourced. While the company that makes use of the prediction can try to seek legal damages against the third party, it’s almost impossible to prove that the algorithm <span class="No-Break">is defective.</span></p>
			<p>An<a id="_idIndexMarker818"/> intuitive assignment of liability would be to a person or team that maintains the model or has reviewed the prediction, but both of these can potentially be outsourced. If those functions have been outsourced, the next intuitive liability assignment would go to the business users who have signed off on the model <span class="No-Break">after UAT.</span></p>
			<p>Finally, if the IA practice is advanced, it’s worth looking into implementing some generic <em class="italic">ML interpretability methods</em>, such as LIME and SHAP. While these interpretability algorithms are most often thought of as an auditing step before deploying a model into production, they can also find some use in helping the IA team investigate problematic predictions for the business users and find ways to tweak the model to avoid them in <span class="No-Break">the future.</span></p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor177"/>Summary</h1>
			<p>The BP ROM is a framework and methodology that guides the entire automation program. Having a good understanding of the ROM significantly improves the odds of reaching your automation goals and scaling your Digital Workforce more quickly. While the ROM is an invaluable resource, it doesn’t provide specific guidance on how to add ML predictions to your <span class="No-Break">RPA processes.</span></p>
			<p>In this chapter, I went through the five ROM 2 foundations and discussed the subtopics that are the most heavily affected by moving from RPA to IA. The discussion was informed by my IA research findings and my experience in ML. IA mainly affects the ROM by adding a separate set of ML concerns on top of existing <span class="No-Break">RPA ones.</span></p>
			<p>That concludes this chapter on the management aspects of BP. In the next chapter, we’ll consolidate what we’ve learned in this book by examining two real-life <span class="No-Break">use cases.</span></p>
		</div>
	</div>
</div>


<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer267" class="Content">
			<h1 id="_idParaDest-174" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor178"/>Part 4:Real-Life Scenarios and Other Blue Prism Products</h1>
			<p>In <em class="italic">Part 4</em>, we’ll consolidate the contents of <em class="italic">Part 2</em> and <em class="italic">Part 3</em> of this book by examining two scenarios that are modeled on real-life use cases. <a href="B18416_11.xhtml#_idTextAnchor179"><span class="No-Break"><em class="italic">Chapter 11</em></span></a> describes an IA use case with three different ML models. Here, we’ll analyze the ML model requirements to select a proper solution design and implement the solution structure by using the IA process templates developed in <a href="B18416_07.xhtml#_idTextAnchor114"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
			<p><a href="B18416_12.xhtml#_idTextAnchor196"><span class="No-Break"><em class="italic">Chapter 12</em></span></a> describes a different use case, where ML auditability is of primary concern. Here, we’ll go through examples of how to deploy a new version of the ML model and roll back. We’ll also look at how to extract ML auditing data through the <span class="No-Break">BP database.</span></p>
			<p>Throughout the book, we’ve only discussed the main BP product. There are numerous other products in the BP ecosystem that are also directly or indirectly related to IA. <a href="B18416_13.xhtml#_idTextAnchor212"><span class="No-Break"><em class="italic">Chapter 13</em></span></a> describes these additional products, their purposes, and how they can be used for IA. Finally, we conclude the book by discussing three future <span class="No-Break">IA trends.</span></p>
			<p>This part contains the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B18416_11.xhtml#_idTextAnchor179"><em class="italic">Chapter 11</em></a>, <em class="italic">Processing Refunds</em></li>
				<li><a href="B18416_12.xhtml#_idTextAnchor196"><em class="italic">Chapter 12</em></a>, <em class="italic">Power Service Interruptions</em></li>
				<li><a href="B18416_13.xhtml#_idTextAnchor212"><em class="italic">Chapter 13</em></a>, <em class="italic">Other Intelligent Blue Prism Products</em></li>
				<li><em class="italic">Appendix, IA Risk Management</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer268">
			</div>
		</div>
		<div>
			<div id="_idContainer269" class="Basic-Graphics-Frame">
			</div>
		</div>
	</div>
</div>
</body></html>