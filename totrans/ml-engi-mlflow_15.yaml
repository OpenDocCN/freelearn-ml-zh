- en: '*Chapter 11*: Performance Monitoring'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the important and relevant area of **Machine
    Learning (ML)** operations and how to ensure a smooth ride in the production systems
    developed so far in this book using best practices in the area and known operational
    patterns. We will understand the concept of operations in ML, and look at metrics
    for monitoring data quality in ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will look at the following sections in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of performance monitoring for ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring data drift and model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring target drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure monitoring and alerting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will address some practical reference tools for performance and reliability
    monitoring of ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of Docker installed on your machine. If you don't already
    have it installed, please follow the instructions at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of `docker-compose` installed. To do this, please follow
    the instructions at https://docs.docker.com/compose/install/.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to Git in the command line, which can be installed as described at [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a Bash terminal (Linux or Windows).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.8+ installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of your ML platform installed locally as described in [*Chapter
    3*](B16783_03_Final_SB_epub.xhtml#_idTextAnchor066)*, Your Data Science Workbench*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AWS account configured to run the MLflow model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of performance monitoring for machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring is at the cornerstone of reliable ML systems able to consistently
    unlock the value of data and provide critical feedback for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the monitoring side of ML models, there are multiple interested parties,
    and we should take the requirements for monitoring from the different stakeholders
    involved. One example of a typical set of stakeholders is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data scientists**: Their focus regarding monitoring is evaluating model performance
    and data drift that might negatively affect that performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software engineers**: These stakeholders want to ensure that they have metrics
    that assess whether their products have reliable and correct access to the APIs
    that are serving models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data engineers**: They want to ensure that the data pipelines are reliable
    and pushing data reliably, at the right velocity, and in line with the correct
    schemas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business/product stakeholders**: These stakeholders are interested in the
    core impact of the overall solution on their customer base. For instance, in a
    trading platform, they might be most concerned with the profit-to-risk ratio that
    the overall solution brings to the company. A circuit breaker might be added to
    the algorithm if the market is in a day of very high volatility or in an atypical
    situation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The most widely used dimensions of monitoring in the ML industry are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data drift**: This corresponds to significant changes in the input data used
    either for training or inference in a model. It might indicate a change of the
    modeled premise in the real world, which will require the model to be retrained,
    redeveloped, or even archived if it''s no longer suitable. This can be easily
    detected by monitoring the distributions of data used for training the model versus
    the data used for scoring or inference over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target drift**: In line with the change of regimens in input data, we often
    see the same change in the distribution of outcomes of the model over a period
    of time. The common periods are months, weeks, or days, and might indicate a significant
    change in the environment that would require model redevelopment and tweaking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance drift**: This involves looking at whether the performance metrics
    such as accuracy for classification problems, or root mean square error, start
    suffering a gradually worsening over time. This is an indication of an issue with
    the model requiring investigation and action from the model developer or maintainer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Platform and infrastructure metrics**: This type of metrics is not directly
    related to modeling, but with the systems infrastructure that encloses the model.
    It implies abnormal CPU, memory, network, or disk usage that will certainly affect
    the ability of the model to deliver value to the business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business metrics**: Very critical business metrics, such as the profitability
    of the models, in some circumstances should be added to the model operations in
    order to ensure that the team responsible for the model can monitor the ability
    of the model to deliver on its business premise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will look at using a tool that we can integrate with
    **MLflow** to monitor for data drift and check the performance of models.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring data drift and model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will run through an example that you can follow in the notebook
    available in the **GitHub** repository (at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter11/model_performance_drifts)
    of the code of the package. We will run through the process of calculating different
    types of drift and exploring its integration with MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: One emergent open source tool in the space of monitoring model performance is
    called `pandas`, JSON, and CSV. It allows us to monitor multiple drifts in ML
    models and their performance. The GitHub repository for Evidently is available
    at [https://github.com/evidentlyai/evidently/](https://github.com/evidentlyai/evidently/).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore the combination of Evidently with MLflow, in
    order to monitor data drift and model performances in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring data drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will set up **Evidently** in our environment and understand
    how to integrate it. Follow these steps in the GitHub repository (refer to the
    *Technical requirements* section for more details):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `evidently`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the relevant libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get a reference dataset, basically a training dataset. We will add a set of
    features to the `pandas` DataFrame so `evidently` will be able to use the feature
    names in the drift reports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following *Figure 11.1* represents the data structure of the training data
    that we will be using as the reference dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Sample of the dataset to be used](img/image0017.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 11.1 – Sample of the dataset to be used
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this step, we load the `to_score_input_data.csv` file. This is the file
    to be scored. Our intention later in this exercise is to calculate the distribution
    difference between the data in the reference training set and the data to be scored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the data drift report generation and log into an MLflow run. Basically,
    what happens in the following code excerpt is the generation of an Evidently dashboard
    with the reference data and the latest input data. A drift report is calculated
    and loaded into an MLflow run so it can be actioned and reviewed in further steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can run now the notebook code (on the `monitoring_data_drift_performance.ipynb`
    file) of the previous cells and explore your data drift reports in the MLflow
    UI over the Artifacts component of the MLflow run. *Figure 11.2* shows that the
    tool didn''t detect any drift among the 14 features, and the distributions are
    presented accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Sample of the dataset to be used](img/image0028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Sample of the dataset to be used
  prefs: []
  type: TYPE_NORMAL
- en: In a similar fashion to data drift, we will now look in the next subsection
    at target drift to uncover other possible issues in our model.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring target drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now compare the scored output with the reference training output to
    look for possible target drift:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the recently scored dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the data drift report generation and log the results in MLflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Explore the target drift reports on your target. As can be seen in *Figure 11.3*,
    no statistically significant figure on this run was found for target drift. In
    detecting drift, Evidently does statistical tests using the probability of the
    data being from a different distribution represented by the **p-value** (more
    details on this can be found at [https://en.wikipedia.org/wiki/P-value](https://en.wikipedia.org/wiki/P-value)).
    It compares the results between the reference and the current data:![](img/image0037.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 11.3 – Target data drift for target
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As shown in *Figure 11.4*, you can drill down further into target drift on
    a specific feature; in this case, a specific previous **day8** to predict the
    stock price:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Target data drift for our target](img/image0048.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Target data drift for our target
  prefs: []
  type: TYPE_NORMAL
- en: After having learned how to detect drift in the input data, we will now look
    at how to use Evidently to monitor drift in models.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring model drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring model drift is extremely important to ensure that your model is still
    delivering at its optimal performance level. From this analysis, you can make
    a decision on whether to retrain your model or even develop a new one from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now monitor model drift. To do this, you need to execute the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get a reference dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train your model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a reference prediction and training predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate and attach the performance reports to your execution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Explore your MLflow performance metrics report. By looking at the reports generated,
    you can check on the **Reference** metrics that **Accuracy**, **Precision**, **Recall**,
    and **F1 metrics**, which are considered the reference metrics based on the training
    data, have maximum values of **1**. The current status on the row below is definitely
    degraded when we test the subset of testing data. This can help you make the call
    on whether it is sensible for the model to still be in production with the current
    **F1** value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Target data drift for target](img/image0056.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Target data drift for target
  prefs: []
  type: TYPE_NORMAL
- en: After having delved into the details of data drift, target drift, and model
    performance monitoring, along with how to integrate these functionalities with
    MLflow, we will now look at the basic principles of monitoring infrastructure,
    including monitoring and alerting.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure monitoring and alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main dimensions of monitoring in ML systems from an infrastructure perspective
    do not differ from those in traditional software systems.
  prefs: []
  type: TYPE_NORMAL
- en: In order to illustrate this exact issue, we will leverage the monitoring and
    alerting tools available in **AWS CloudWatch** and **SageMaker** to illustrate
    an example of setting up monitoring and alerting infrastructure. This same mechanism
    can be set up with tools such as Grafana/Prometheus for on-premises and cloud
    deployments alike. These monitoring tools achieve similar goals and provide comparable
    features, so you should choose the most appropriate depending on your environment
    and cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS CloudWatch** provides a monitoring and observability solution. It allows
    you to monitor your applications, respond to system-wide performance changes,
    optimize resource use, and receive a single view of operational health.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At a higher level, we can split the infrastructure monitoring and alerting
    components into the following three items:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource metrics**: This refers to metrics regarding the hardware infrastructure
    where the system is deployed. The main metrics in this case would be the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a. **CPU utilization**: This is basically a unit of utilization of your processor
    as a percentage value. This is the general metric available and should be monitored.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b. **Memory utilization**: The percentage of memory in use at the moment by
    your computing system.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c. **Network data transfer**: Network data transfer refers to the amount of
    traffic in and out of a specific compute node. It is generally measured in Mb/s.
    An anomaly might mean that you need to add more nodes to your system or increase
    capacity.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd. **Disk I/O**: This is measured in the throughput of writes and reads from
    the disk; it might point to a system under stress that needs to be either scaled
    or have its performance investigated:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.6 – SageMaker infrastructure metric examples'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0065.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.6 – SageMaker infrastructure metric examples
  prefs: []
  type: TYPE_NORMAL
- en: '**System metrics**: The second pillar of infrastructure monitoring and alerting
    components refers to metrics regarding the system infrastructure where the system
    is deployed. The main metrics in this case would be the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a. **Request throughput**: The number of predictions served over a second'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b. **Error rate**: The number of errors per prediction'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c. **Request latencies**: The end-to-end time taken to serve a prediction'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd. **Validation metrics**: Error metrics on input data for the request'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A production system such as SageMaker pushes system metrics into AWS CloudWatch
    to provide real-time system metrics monitoring. AWS CloudWatch has a complete
    feature set of features to manage, store, and monitor metrics and dashboards:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Specify an alarm in AWS CloudWatch](img/image0074.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Specify an alarm in AWS CloudWatch
  prefs: []
  type: TYPE_NORMAL
- en: '**Alerting**: For alerting, we use any of the metrics calculated in the previous
    section and set up a threshold that we consider acceptable. The AWS CloudWatch
    interface allows you to easily set up alerts on the default service metrics and
    custom metrics. The team responsible for reliability is alerted by CloudWatch
    sending messages to a corporate chat/Slack, email address, or mobile phone to
    allow the team to address or mitigate the incident:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Specify an alarm in AWS CloudWatch](img/image0084.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Specify an alarm in AWS CloudWatch
  prefs: []
  type: TYPE_NORMAL
- en: You can use the same monitoring tools to log and monitor all the other metrics
    that are interrelated with your ML systems. For instance, having an alert for
    the weekly profit of a ML model is a business metric that should be deployed alongside
    the core systems metrics of your system.
  prefs: []
  type: TYPE_NORMAL
- en: After being exposed to an overview of AWS CloudWatch as an example of a tool
    to implement metrics monitoring and alerting for your ML systems in production,
    we will explore advanced concepts of MLflow in the last chapter of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the concepts of data drift and target drift,
    and examined different approaches to performance monitoring in ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: We started by introducing important concepts in the realm of performance and
    monitoring, different types of drift and business metrics to monitor, and the
    use of AWS CloudWatch as a tool to implement monitoring and alerting in real-time
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and monitoring is an important component of our architecture, and
    it will allow us to conclude an important layer of our ML system's architecture.
    Now let's delve into the next chapter on advanced topics in MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to further your knowledge, you can consult the documentation at the
    following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.mlflow.org/docs/latest/projects.html](https://www.mlflow.org/docs/latest/projects.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://evidentlyai.com/](https://evidentlyai.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/cloudwatch/](https://aws.amazon.com/cloudwatch/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
