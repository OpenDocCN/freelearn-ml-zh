- en: '*Chapter 4*: Ingesting Data and Managing Datasets'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we set up and explored the Azure Machine Learning workspace,
    performed data experimentation, and scheduled scripts to run on remote compute
    targets in Azure Machine Learning. In this chapter, we will learn how to connect
    datastores and create, explore, access, and track data in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will take a look at how data is managed in Azure Machine Learning
    by understanding the concepts of **datastores and datasets**. We will see different
    types of datastores and learn best practices for organizing and storing data for
    **machine learning** (**ML**) in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create an **Azure Blob storage** account and connect it as a datastore
    to Azure Machine Learning. We will cover best practices for ingesting data into
    Azure using popular CLI tools as well as **Azure Data Factory** and **Azure Synapse
    Spark** services.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will learn how to create datasets from data in
    Azure, access and explore these datasets, and pass data efficiently to compute
    environments in your Azure Machine Learning workspace. Finally, we will discuss
    how to access Azure Open Datasets to improve your model's performance through
    third-party data sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the topics that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing data storage solutions for Azure Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a datastore and ingesting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using datasets in Azure Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the following Python libraries and versions to
    create and manage datastores and datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '`azureml-core 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`azureml-sdk 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to previous chapters, you can run this code using either a local Python
    interpreter or a notebook environment hosted in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter04](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter04).'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing data storage solutions for Azure Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When running ML experiments or training scripts on your local development machine,
    you often don't think about managing your datasets. You probably store your training
    data on your local hard drive, external storage device, or file share. In such
    a case, accessing the data for experimentation or training is not a problem, and
    you don't have to worry about the data location, access permissions, maximal throughput,
    parallel access, storage and egress cost, data versioning, and such.
  prefs: []
  type: TYPE_NORMAL
- en: However, as soon as you start training an ML model on remote compute targets,
    such as a VM in the cloud or within Azure Machine Learning, you must make sure
    that all your executables can access the training data efficiently. This is even
    more relevant if you collaborate with other people who also need to access the
    data in parallel for experimentation, labeling, and training from multiple environments
    and multiple machines. And if you deploy a model that requires access to this
    data as well – for example, looking up labels for categorical results, scoring
    recommendations based on a user's history of ratings, and the like – then this
    environment needs to access the data as well.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to manage data for different use cases in
    Azure. We will first see the abstractions Azure Machine Learning provides to facilitate
    data access for ML experimentation, training, and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Organizing data in Azure Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Azure Machine Learning, data is managed as **datasets** and data storage
    as **datastores**. This abstraction hides the details of location, data format,
    data transport protocol, and access permissions behind the dataset and datastore
    objects and hence lets Azure Machine Learning users focus on exploring, transforming,
    and managing data without worrying about the underlying storage system.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **datastore** is an abstraction of a physical data storage system that is
    used to link the existing storage system to an Azure Machine Learning workspace.
    In order to connect the existing storage to the workspace – by creating a datastore
    – you need to provide the connection and authentication details of the storage
    system. Once created, the data storage can be accessed by users through the datastore
    object, which will automatically use the provided credentials of the datastore
    definition. This makes it easy to provide access to data storage to your developers,
    data engineers, and scientists who are collaborating in an Azure Machine Learning
    workspace. Currently, the following services can be connected as datastores to
    a workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: Azure Blob containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure file share
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Data Lake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Data Lake Gen2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure SQL Database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Database for PostgreSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks File System
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Database for MySQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While datastores are abstractions of data storage systems, a **dataset** is
    an abstraction of data in general – for example, data in the form of a file on
    a remote server accessible through a public URL or files and tables within a datastore.
    Azure Machine Learning supports two types of abstraction on data formats, namely
    **tabular datasets** and **file datasets**. The former is used to define *tabular*
    data – for example, from comma- or delimiter-separated files, from Parquet and
    JSON files, or from SQL queries – whereas the latter is used to specify *any binary*
    data from files and folders, such as images, audio, and video data.
  prefs: []
  type: TYPE_NORMAL
- en: Tabular datasets can also be defined and used directly from their publicly available
    URL, which is called a `pandas` and `requests`. Both tabular and file datasets
    can be registered in your workspace. We will refer to these datasets as **registered
    datasets**. Registered datasets will show up in your Azure Machine Learning Studio
    under **Datasets**.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the default storage accounts of Azure Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There exists one special datastore in Azure Machine Learning that is used internally
    to store all snapshots, logs, figures, models, and more when executing experiment
    runs. This is called the **default datastore**, is an Azure Blob storage account,
    and is created automatically with Azure Machine Learning when you set up the initial
    workspace. You can select your own Blob storage as the default datastore during
    the workspace creation or connect your storage account and mark it as default
    in Azure Machine Learning Studio.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.1* shows you the list of datastores in Azure Machine Learning Studio.
    The default datastore is marked as **Default** and generated automatically when
    setting up an Azure Machine Learning workspace. To go to this view, simply click
    on **Datastores** under the **Manage** category in the left menu in Azure Machine
    Learning Studio. To view existing datasets, click on **Datasets** in the **Assets**
    category:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Default datastore in Azure Machine Learning ](img/B17928_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Default datastore in Azure Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: 'The default datastore is used by Azure Machine Learning internally to store
    all assets and artifacts when no other datastore is defined. You can access and
    use the default datastore in your workspace identically to your custom datastores
    by creating a datastore reference. The following code snippet shows how to get
    a reference to the default datastore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The default datastore is used internally by Azure Machine Learning to store
    all assets and artifacts during the ML life cycle. Using the previous code snippet,
    you can access the default datastore to store custom datasets and files.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have accessed the default datastore and connected custom datastores,
    we need to think about a strategy for efficiently storing data for different ML
    use cases. Let's tackle this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring options for storing training data in Azure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure supports a myriad of different data storage solutions and technologies
    to store data in the cloud – and as we saw in the previous section, many of these
    are supported datastores in Azure Machine Learning. In this section, we will explore
    some of these services and technologies to understand which ones can be used for
    machine learning use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Database systems can be broadly categorized by the type of *data* and *data
    access* into the following two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Relational database management systems** (**RDBMSs**) are often used to store
    normalized transactional data using B-tree-based ordered indices. Typical queries
    filter, group, and aggregate results by joining multiple rows from multiple tables.
    Azure supports different RDBMSs, such as Azure SQL Database, as well as Azure
    Database for PostgreSQL and MySQL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NoSQL**: Key-value-based storage systems are often used to store de-normalized
    data with hash-based or ordered indices. Typical queries access a single record
    from a collection distributed based on a partition key. Azure supports different
    NoSQL-based services such as Azure Cosmos DB and Azure Table storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, depending on your use cases, you can use both database technologies
    to store data for machine learning. While RDBMSs are great technologies to store
    training data for machine learning, NoSQL systems are great to store lookup data
    – such as training labels – or ML results such as recommendations, predictions,
    or feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of choosing a database service, another popular choice for machine learning
    is to use data storage systems. On disk, most database services persist as data
    pages on **file** or **blob storage systems**. Blob storage systems are a very
    popular choice for storing all kinds of data and assets for machine learning due
    to their scalability, performance, throughput, and cost. Azure Machine Learning
    makes extensive use of blob storage systems, especially for storing all operational
    assets and logs.
  prefs: []
  type: TYPE_NORMAL
- en: Popular Azure blob storage services are Azure Blob storage and Azure Data Lake
    Storage, which provide great flexibility to implement efficient data storage and
    access solutions through different choices of data formats. While Azure Blob storage
    supports most common blob-based filesystem operations, Azure Data Lake Storage
    implements efficient directory services, which makes it a popular general-purpose
    storage solution for horizontally scalable filesystems. It is a popular choice
    for storing large machine learning training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: While tabular data can be stored efficiently in RDBMS systems, similar properties
    can be achieved by choosing the correct data formats and embedded clustered indices
    while storing data on blob storage systems. Choosing the right data format will
    allow your filesystem to efficiently store, read, parse, and aggregate information.
  prefs: []
  type: TYPE_NORMAL
- en: Common data format choices can be categorized into textual (CSV, JSON, and more)
    as well as binary formats (images, audio, video, and more). Binary formats for
    storing tabular data are broadly categorized into row-compressed (Protobuf, Avro,
    SequenceFiles, and more) or column-compressed (Parquet, ORC, and more) formats.
    A popular choice is also to compress the whole file using Gzip, Snappy, or other
    compression algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: One structure that most data storage systems have in common is a hierarchical
    path or directory structure to organize data blobs. A popular choice for storing
    training data for machine learning is to implement a partitioning strategy for
    your data. This means that data is organized in multiple directories where each
    directory contains all the data for a specific key, also called the partitioning
    key.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud providers offer a variety of different storage solutions, which can be
    customized further by choosing different indexing, partitioning, format, and compression
    techniques. A common choice for storing tabular training data for machine learning
    is a column-compressed binary format such as Parquet, partitioned by ingestion
    date, stored on Azure Data Lake Storage, for efficient management operations and
    scalable access.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a datastore and ingesting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After having a look through the options for storing data in Azure for ML processing,
    we will now create a storage account, which we will use throughout the book for
    our raw data and ML datasets. In addition, we will have a look at how to transfer
    some data into our storage account manually and how to perform this task automatically
    by utilizing integration engines available in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Blob Storage and connecting it with the Azure Machine Learning workspace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by creating a storage account. Any storage account will come with
    a file share, a queue, and table storage for you to utilize in other scenarios.
    In addition to those three, you can either end up with Blob Storage or a Data
    Lake, depending on the settings you provide at creation time. By default, a Blob
    storage account will be created. If we instead want a Data Lake account, we must
    set the `enable-hierarchical-namespace` setting to `True`, as Data Lake offers
    an actual hierarchical folder structure and not a flat namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Blob Storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Keeping that in mind, let''s create a Blob Storage account:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to a terminal of your choosing, log in to Azure, and check that you
    are working in the correct subscription as we learned in [*Chapter 3*](B17928_03_ePub.xhtml#_idTextAnchor054),
    *Preparing the Azure Machine Learning Workspace*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we want to create a storage account, let''s have a look at the options and
    required settings for doing so by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Looking through the result, you will see a very long list of possible arguments,
    but the only required ones are `name` and `resource-group`. Still, we should look
    further through this, as a lot of the other settings are still set to certain
    default values, which might be incorrect for our case.
  prefs: []
  type: TYPE_NORMAL
- en: Going through the list, you will find a lot of options concerning network or
    security settings. The default for most of them is to at least allow access from
    everywhere. At this moment, we are not too concerned about virtual network integration
    or handling our own managed keys in Azure Key Vault.
  prefs: []
  type: TYPE_NORMAL
- en: Besides all these options, there are a few that define the type of storage account
    we set, namely `enable-hierarchical-namespace`, `kind`, `location`, and `sku`.
  prefs: []
  type: TYPE_NORMAL
- en: We already discussed the first option and as the default is `False`, we can
    ignore it.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at `kind`, you see a list of storage types. You might think we need
    to choose `BlobStorage`, but unfortunately, that is a legacy setting left there
    for any storage account still running on the first version, V1\. The default (`StorageV2`)
    is the best option for our scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at `location`, we see that we apparently can set a default location
    for all deployments, therefore it is not flagged as required. As we did not do
    that so far, we will just provide it when deploying the storage account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, looking at `sku`, we see a combined setting of an option concerning
    the type of disk technology used (`Standard`/`Premium`), where `Standard` denotes
    HDD storage and `Premium` denotes SSD storage, and an option defining the data
    redundancy scheme (LRS/ZRS/GRS/RAGRS/GZRS). If you want to learn more about the
    redundancy options, follow this link: [https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy](https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy).
    As both increase costs, feel free to either stick with the default (`Standard_RAGRS`)
    or go with local redundancy (`Standard_LRS`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create our storage account. Please be aware that the name you choose
    must be globally unique, therefore you cannot choose the one you will read in
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output this creates will show you the detailed settings for the created
    storage account.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final step, let''s create a container in our new blob storage. For that,
    run the following command with the appropriate account name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will show `True` at the end, but will give you some warnings beforehand,
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The command worked because it automatically pulled the account key of the storage
    account through our session. Normally, to access a storage account, we either
    need an AD identity, a key to access the whole account (`account-key`), or a shared-access
    key (`sas-token`) to access only a specific subset of folders or containers. We
    will come back to this when connecting from the ML workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the result, run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our storage, let's connect it to our Azure Machine Learning
    workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a datastore in Azure Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to not bother with the storage account itself anymore when working
    with our ML scripts, we will now create a permanent connection to a container
    in a storage account and define it as one of our datastores in the Azure Machine
    Learning workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will guide you through this process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s understand what is required to create a datastore by running
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Looking through the output,, we understand that the name of the resource group,
    the name of the ML workspace, and a YAML file is needed. We have two of those
    three things. Therefore, let's understand what the YAML file has to look like.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to [https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-datastore-blob](https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-datastore-blob),
    where you will find the required schema of our file and some examples. Going through
    the examples, you will see that they mainly differ concerning the way to authenticate
    to the storage account. The most secure of them is limited access via a SAS token
    and therefore we will pick that route.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Please either download the `blobdatastore.yml` file from the files for [*Chapter
    4*](B17928_04_ePub.xhtml#_idTextAnchor071), *Ingesting Data and Managing Datasets,*
    from the GitHub repository or create a file with the same name and the following
    content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Please enter the appropriate account name for your case. The only thing missing
    now is the SAS token, which we need to create for our `mlfiles` container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to create a SAS token for our container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command generates a SAS token with an expiration date of 01/01/2023 and
    permissions to `mlfiles` container. Choose an expiration date that is far enough
    in the future for you to work with this book. In normal circumstances, you would
    choose a much shorter expiration date and rotate this key accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result should be in this kind of format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Take this result (without quotations) and enter it in the `sas_token` field
    in the YAML file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the directory the YAML file is in so that we can finally create
    the datastore in the Azure Machine Learning workspace by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: With these steps, we have registered a datastore connected to our blob storage
    using a SAS token.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can follow the same steps when connecting to a Data Lake Storage, but be
    aware that to access a data lake, you will need to create a **service p****rincipal**.
    A detailed description of this can be found here: [https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal).'
  prefs: []
  type: TYPE_NORMAL
- en: As discussed before, we could have created a blob storage by navigating to the
    wizard in the Azure portal, creating a SAS token for the container there, and
    entering it in the datastore creation wizard in Azure Machine Learning Studio.
    We used the Azure CLI so that you can get comfortable with this, as this is required
    to automate such steps in the future, especially when we talk about infrastructure-as-code
    and DevOps environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case, feel free to navigate to the **Datastores** tab in Azure Machine
    Learning Studio. *Figure 4.2* shows our newly created workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Created datastore ](img/B17928_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Created datastore
  prefs: []
  type: TYPE_NORMAL
- en: Keep this tab open, so we can verify later via the `mlfiles` container, which
    we will start doing in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting data into Azure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created an Azure Blob storage account and learned how to organize and format
    files and tabular data for common ML use cases. However, one often-neglected step
    is how to efficiently ingest data into these datastores, or into Azure in general.
    There are different solutions for different datasets and use cases, from ad hoc,
    automated, parallelized solutions, and more. In this section, we will have a look
    at methods to upload and transform data either in a manual or an automated fashion
    to a relational database (SQL, MySQL, or PostgreSQL) or a storage account in Azure.
    Finally, we will upload a dataset file to the previously created blob storage.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding tooling for the manual ingestion of data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you work with a small number of datasets and files and you do not need to
    transfer data from other existing sources, a manual upload of data is the go-to
    option.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list shows possible options to bring data into your datastores
    or directly into your ML pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Azure Storage Explorer**: Storage Explorer is an interactive application
    that allows you to upload data to and control datastores, such as storage accounts
    and managed disks. This is the easiest tool to use for managing storage accounts
    and can be found here: [https://azure.microsoft.com/en-us/features/storage-explorer/#overview](https://azure.microsoft.com/en-us/features/storage-explorer/#overview).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure CLI**: As we saw before, we basically can do anything with the CLI,
    including creating and uploading blobs to a storage account. You can find the
    appropriate commands to upload blobs in the storage extension described here:
    [https://docs.microsoft.com/en-us/cli/azure/storage/blob](https://docs.microsoft.com/en-us/cli/azure/storage/blob).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AzCopy**: This is another command-line tool specifically designed to copy
    blobs or files to a storage account. Whether you use Azure CLI packages or AzCopy
    comes down to personal preference, as there are no clear performance differences
    between these two options. You can find the download link and the description
    here: [https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Azure portal**: For any service, you will always find a web interface
    directly in the Azure portal to upload or change data. If you navigate to a storage
    account, you can use the inbuilt storage browser to upload blobs and files directly
    through the web interface. The same is true for any of the database technologies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RDBMS management tooling**: You can use any typical management tool to configure,
    create, and change tables and schemas in a relational database. For a SQL database
    and Synapse, this would be **SQL Server Management Studio** ([https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15));
    for PostgreSQL, this would be **pgAdmin** ([https://www.pgadmin.org/](https://www.pgadmin.org/));
    and for MySQL, this would be **MySQL Workbench** ([https://docs.microsoft.com/en-us/azure/mysql/connect-workbench](https://docs.microsoft.com/en-us/azure/mysql/connect-workbench)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Data Studio**: Data Studio allows you to connect to any Microsoft SQL
    database, to Synapse, to a PostgreSQL database in Azure, and to Azure Data Explorer.
    It is a multiplatform tool very similar to the typical management tooling mentioned
    in the last point but in one platform. You can download this tool here: [https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Machine Learning designer (Import Data)**: If you do not want to use
    an Azure Machine Learning datastore, you can use the **Import Data** component
    in the Machine Learning designer to add data ad hoc to your pipelines. This is
    not the cleanest way to operate, but an option nonetheless. You can find all information
    about this method here: [https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/import-data](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/import-data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we test out some of these options, let's have a look at the options to
    create automated data flows and transform data in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding tooling for automated ingestion and transformation of data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Copying data manually is completely fine for small tests and probably even for
    most of the tasks we will perform in this book, but in a real-world scenario,
    we will need to not only integrate with a lot of different sources but will also
    need a process that does not include a person manually moving data from A to B.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we will now have a look at services that allow us to transform and
    move data in an automated fashion and that integrate very well with pipelines
    and MLOps in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Data Factory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Azure Data Factory is the enterprise-ready solution for moving and transforming
    data in Azure. It offers the ability to connect to hundreds of different sources
    and to create pipelines to transform the integrated data, calling multiple other
    services in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to create a data factory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Please be aware that the name, once again, has to be globally unique. In addition,
    before deployment, the CLI will ask you to install the `datafactory` extension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are done, navigate to the resource in the Azure portal, and on the
    **Overview** tab, click on **Open Azure Data Factory Studio**, which will lead
    you to the workbench for your data factory instance. You should see a view as
    shown in *Figure 4.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Data Factory resource view ](img/B17928_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Data Factory resource view
  prefs: []
  type: TYPE_NORMAL
- en: 'From this view, you can create pipelines, datasets, data flows, and power queries.
    Let''s briefly discuss what they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipelines**: Pipelines are the main star of Azure Data Factory. You can create
    complex pipelines calling multiple services to pull data from a source, transform
    it, and store it in a sink.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Datasets**: Datasets are used in a pipeline as a source or a sink. Therefore,
    before building a pipeline, you can define a connection to specific data in a
    datastore that you want to read from or write to in the end.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data flows**: Data flows allows you to do the actual processing or transformation
    of data within Data Factory itself, instead of calling a different service to
    do the heavy lifting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power Query**: Power Query allows you to do data exploration with DAX inside
    Data Factory, which is typically only possible with Power BI or Excel otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you click on the three dots next to **Pipeline**, you can create a new one,
    which will result in the following view shown in *Figure 4.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Creating a Data Factory pipeline ](img/B17928_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Creating a Data Factory pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Having a look through the possible activities, you will find a way to copy data
    (**Copy Data**) from A to B, to execute a script in Azure Functions (**Azure Function**),
    to call a stored procedure in a SQL database (**Stored Procedure**), to execute
    a notebook in Databricks (**Notebook**), and to execute an ML pipeline (**Machine
    Learning Execute Pipeline**), among other things. With these activities and the
    control tools you will find in **General** and **Iteration & conditionals**, you
    can build very complex data pipelines to move and transform your data.
  prefs: []
  type: TYPE_NORMAL
- en: As you might have noticed, Azure Synapse is missing from the list of activities.
    The reason for that is that Synapse has its own version of Data Factory integrated
    into the platform. Therefore, if you are using a SQL pool or a Spark pool in Synapse,
    you can use the integration tool of Synapse instead, which will give you access
    to running a notebook in the Synapse Spark pool or a stored procedure on the SQL
    pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are looking for an in-depth overview of Azure Data Factory, have a look
    at Catherine Wilhelmsen''s *Beginners Guide to Azure Data Factory*: [https://www.cathrinewilhelmsen.net/series/beginners-guide-azure-data-factory/](https://www.cathrinewilhelmsen.net/series/beginners-guide-azure-data-factory/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what we need to understand is that there are two ways to integrate this
    Data Factory pipeline into Azure Machine Learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Read results from a storage account**: We can just run the transformation
    pipeline in Data Factory, transforming our data, and then store the result in
    a storage account. We then access the data as we learned via an ML datastore.
    In this scenario, any pipeline we have in Azure Machine Learning is disconnected
    from the transformation pipelines in Data Factory, which might not be the optimal
    way for MLOps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invoke Azure Machine Learning from Data Factory**: We can create a transformation
    pipeline and invoke the actual Azure Machine Learning pipeline as part of the
    Data Factory pipeline. This is the preferred way if we are starting to build an
    end-to-end MLOps workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For further information on this, have a read through the following article:
    [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf).'
  prefs: []
  type: TYPE_NORMAL
- en: Azure Synapse Spark pools
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 2*](B17928_02_ePub.xhtml#_idTextAnchor034), *Choosing
    the Right Machine Learning Service in Azure*, Azure Databricks and Azure Synapse
    give you the option to run Spark jobs in a Spark pool. Apache Spark can help you
    transform and preprocess extremely large datasets by utilizing the distributive
    nature of the node pool underneath. Therefore, this tool can be helpful to take
    apart and filter out datasets before starting the actual machine learning process.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that we can run notebooks from either Azure Data Factory or from
    the integration engine in Azure Synapse and therefore already have access to these
    services. On top of that, we have the option to add a Synapse Spark pool as a
    so-called **linked service** in the Azure Machine Learning workspace (see the
    **Linked Services** tab in Azure Machine Learning Studio). Doing this step gives
    us the opportunity to access not only the ML compute targets but also the Spark
    pool as a target for computations via the Azure Machine Learning SDK.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create this link either via Azure Machine Learning Studio or via the
    Azure Machine Learning Python SDK, both of which are described in the following
    article: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-link-synapse-ml-workspaces](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-link-synapse-ml-workspaces).'
  prefs: []
  type: TYPE_NORMAL
- en: Through this direct integration, we can run transformation steps in our ML pipelines
    through a Spark cluster and therefore get another good option for building a clean
    end-to-end MLOps workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Copying data to Blob storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, that we have a good understanding of most of the options to move and transform
    data, let's upload a dataset to our storage account.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B17928_05_ePub.xhtml#_idTextAnchor085), *Performing Data Analysis
    and Visualization*, we will start analyzing and preprocessing data. To prepare
    for this, let's upload the dataset we will work with in that chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will work with the **Melbourne Housing dataset**, created by Anthony Pino,
    which you can find here: [https://www.kaggle.com/anthonypino/melbourne-housing-market](https://www.kaggle.com/anthonypino/melbourne-housing-market).
    The reason to work with this dataset is the domain it covers, as everyone understands
    housing, and the reasonable cleanliness of the data. If you continue your journey
    through working with data, you will find out that there are a lot of datasets
    out there, but only a few that are clean and educational.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, to make our lives a bit easier when analyzing the dataset in the
    next chapter, we will actually work with a subset of this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the next steps so that we can make this file available in our `mldemoblob`
    datastore:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the `melb_data.csv` file from [https://www.kaggle.com/dansbecker/melbourne-housing-snapshot](https://www.kaggle.com/dansbecker/melbourne-housing-snapshot)
    and store it in a suitable folder on your device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to that folder and run the following command in the CLI, replacing
    the storage account name with your own:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To verify this, let''s have a look at another option to move this file. Install
    Azure Storage Explorer and log in to your Azure account in that application. Navigate
    to your storage account and open the `mlfiles` container. It should show you a
    view as seen in *Figure 4.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Azure Storage Explorer ](img/B17928_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Azure Storage Explorer
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, our file is where it is supposed to be. We could have also just
    dragged and dropped the file directly here, creating a blob file automatically.
    From here on out, feel free to use what feels more comfortable to you.
  prefs: []
  type: TYPE_NORMAL
- en: To finish this up, have a look at the application itself. For example, if you
    right-click on the container, you can choose an option called **Get Shared Access
    Signature**, which opens a wizard allowing you to create a SAS token directly
    here, instead of as we did via the command line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the previous steps, we made our raw dataset file available in our storage
    account and therefore in our ML datastore. In the next section, we will have a
    look at how to create an Azure Machine Learning dataset from these raw files and
    what features they offer to support us in our ongoing ML journey.
  prefs: []
  type: TYPE_NORMAL
- en: Using datasets in Azure Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections of this chapter, we discussed how to get data into
    the cloud, store the data in a datastore, and connect the data via a **datastore
    and d****ataset** to an Azure Machine Learning workspace. We did all this effort
    of managing the data and data access centrally in order to use the data across
    all compute environments, either for experimentation, training, or inferencing.
    In this section, we will focus on how to create, explore, and access these datasets
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data is managed as datasets, we can track the data that was used for
    each experimentation or training run in Azure Machine Learning. This will give
    us visibility of the data used for a specific training run and for the trained
    model – an essential step in creating reproducible end-to-end machine learning
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of organizing your data into datasets is that you can easily
    pass a managed dataset to your experimentation or training scripts via **direct
    access**, **download**, or **mount**. The direct access method is useful for publicly
    available data sources, the *download* method is convenient for small datasets,
    and the *mount* method is useful for large datasets. In Azure Machine Learning
    training clusters, this is completely transparent, and the data will be provided
    automatically. However, we can use the same technique to access the data in any
    other Python environment, by simply having access to the dataset object.
  prefs: []
  type: TYPE_NORMAL
- en: In the last part of this section, we will explore Azure Open Datasets – a collection
    of curated Azure Machine Learning datasets you can consume directly from within
    your Azure Machine Learning workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Creating new datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are multiple ways to create new datasets, but most of them differentiate
    between tabular and file datasets. You need to use different constructors based
    on the type of dataset you want to create:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Dataset.Tabular.from_*` for tabular datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataset.File.from_*` for file-based datasets (for example, image, audio, and
    more)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For tabular datasets, we also differentiate between the data being accessed
    from the original location through a public URL – called a *direct dataset* –
    or stored on either the default or a custom *datastore*.
  prefs: []
  type: TYPE_NORMAL
- en: A `Dataset` object can be accessed or passed around in the current environment
    through its object reference. However, a dataset can also be registered (and versioned),
    and hence accessed through the dataset name (and version) – this is called a *registered
    dataset*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see a simple example of a direct dataset, which is defined as a tabular
    dataset, and a publicly available URL containing a delimiter-separated file with
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the code, we can create a *direct dataset* by passing the
    URL to a publicly accessible delimiter-separated file. When passing this dataset
    internally, every consumer will attempt to fetch the dataset from its URL.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Direct dataset ](img/B17928_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Direct dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a reference to a datastore, we can access data within it. In the
    following example, we create a file dataset from files stored in a directory of
    the `mldata` datastore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the example, we can register data from within the datastore
    as datasets. In this example, we defined all files in a folder as a file dataset,
    but we could also define a delimiter-separated file in Blob storage as a tabular
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – File dataset ](img/B17928_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – File dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we register this dataset in the workspace using the following
    code snippet to create a *registered dataset*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The previous code will register the direct dataset in your workspace and return
    a registered dataset. Registered datasets are listed in Azure Machine Learning
    Studio, and can be accessed via the dataset name instead of the `Dataset` Python
    object.
  prefs: []
  type: TYPE_NORMAL
- en: The `create_new_version` argument controls whether we want to create a new version
    of an existing dataset. Once a new dataset version is created, the dataset can
    be accessed through the dataset name – which will implicitly access the latest
    version – or through its name and a specific version. Dataset versions are useful
    to manage different iterations of the dataset within your workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data in datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are multiple options to explore registered datasets in Azure Machine
    Learning. For tabular datasets, the most convenient way is to load and analyze
    a dataset programmatically in an Azure Machine Learning workspace. To do so, you
    can simply reference a dataset by its name and version as shown in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have a reference to the dataset, you can convert a dataset reference
    to an actual in-memory **pandas** DataFrame or a lazy-loaded **Spark** or **Dask**
    DataFrame. To do so, you can call one of the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`to_pandas_dataframe()` to create an in-memory pandas DataFrame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`to_spark_dataframe()` to create a lazily loaded Spark DataFrame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`to_dask_dataframe()` to create a lazily loaded Dask DataFrame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see the three commands in action, starting with the in-memory pandas
    DataFrame. The following code snippet will load all the data into a pandas DataFrame
    and then return the first five rows of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: After loading the DataFrame, you can run your favorite pandas methods to explore
    the datasets. For example, good commands to get started are `info()` to see columns
    and datatypes and `describe()` to see statistics of the numerical values of the
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy datasets are datasets that only load some data to memory when explicitly
    needed, for example, when a result of a computation is required. Non-lazy datasets
    load all the data into memory and hence are limited by the available memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are more familiar with PySpark, you can also transform a dataset into
    a Spark DataFrame with the following code snippet. In contrast to the previous
    example, this code won''t actually load all data into memory but only fetches
    the data required for executing the `show()` command – this makes it a great choice
    for analyzing large datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Another alternative is to return a Dask DataFrame of the dataset. Dask is a
    Python library for parallel computing that supports lazy datasets with a pandas-
    and NumPy-like API. Hence you can run the following code to return the first five
    rows of the DataFrame lazily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Once you have programmatic access to the data in your favorite numeric or statistical
    libraries, you can slice and dice your dataset as much as needed. While programmatic
    access is great for reproducibility and customization, users often just want to
    understand how the data is structured and see a few example records. Azure Machine
    Learning also offers the possibility to explore the dataset in the Data Studio
    UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get to this view, go to **Datasets**, select a dataset, and click on the
    **Explore** tab. The first page shows you a preview of your data, including the
    first *n* rows as well as some basic information about the data – such as the
    number of rows and columns. The following screenshot shows an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Dataset with data preview ](img/B17928_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Dataset with data preview
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the second tab, you can generate and view a data profile. This
    profile is similar to calling `describe()` on the pandas DataFrame – a statistical
    analysis of each column in the dataset, but with support for categorical data
    and some more useful information. As you can see in *Figure 4.9*, it also shows
    a figure with the data distribution for each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Dataset with data profile ](img/B17928_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Dataset with data profile
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the previous figure, this is a very useful summary of the
    dataset. The insights from this view are important for everyone working with this
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw multiple ways to access and analyze data stored in Azure
    Machine Learning datasets – programmatically via Python and your favorite numerical
    libraries or via the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking datasets in Azure Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: End-to-end tracking of all assets that go into your final production model is
    essential for reproducibility and interpretability but also auditing and tracking.
    A machine learning model is a function that minimizes a loss function by iterating
    and sampling experiments from your training data. Therefore, the training data
    itself should be treated as being a part of the model itself, and hence should
    be managed, versioned, and tracked through the end-to-end machine learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to take advantage of datasets to add data tracking to our experiments.
    A good way to understand the differences between data tracking capabilities is
    to look at two examples: first, loading a CSV dataset from a URL, and then loading
    the same data from the same URL but through a dataset abstraction in Azure Machine
    Learning. However, we don''t only want to load the data, but also pass it from
    the authoring script to the training script as an argument.'
  prefs: []
  type: TYPE_NORMAL
- en: We will first use `pandas` to load a CSV file directly from the URL and pass
    it to the training script as a URL. In the next step, we will enhance this method
    by using a direct dataset instead, allowing us to conveniently pass the dataset
    configuration to the training script and track the dataset for the experiment
    run in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Passing external data as a URL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start our example using data that is available as a CSV file from a remote
    URL, a common way to distribute public datasets. In the first example without
    Azure Machine Learning dataset tracking, we will use the `pandas` library to fetch
    and parse the CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started with the first code snippet using pandas'' `read_csv()`
    method as an example to fetch data via a public URL from a remote server. However,
    this is just an example – you could replace it with any other method to fetch
    data from a remote location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our goal is to pass the data from the authoring script to the training script,
    so it can be tracked and updated easily in the future. To achieve this, we can''t
    send the DataFrame directly, but have to pass the URL to the CSV file and use
    the same method to fetch the data in the training script. Let''s write a small
    training script whose only job is to parse the command-line arguments and fetch
    the data from the URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '**code/access_data_from_path.py**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As we see in the preceding code, we pass the data path from the command-line
    `--input` argument and then load the data from the location using pandas' `read_csv()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a `ScriptRunConfig` constructor to submit an experiment run
    to Azure Machine Learning that executes the training script from *step 1*. For
    now, we are not performing any training but only want to understand what data
    is passed between the authoring and execution runtime:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Access_data_from_path.ipynb**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s execute the run configuration to run the experiment and track the run
    details in Azure Machine Learning. Once the experiment run has finished, we navigate
    to Azure Machine Learning and check the details of this run. As we can see in
    *Figure 4.10*, Azure Machine Learning will track the `script` argument as expected
    but cannot associate the argument to a dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Run details of the experiment ](img/B17928_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Run details of the experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s summarize the downsides of this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: We can't pass the pandas DataFrame or a DataFrame identifier to the training
    script; we have to pass the data through the URL to its location. If the file
    path changes, we have to update the argument for the training script.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training script doesn't know that the input path refers to the input data
    for the training script, it's simply a string argument to the training script.
    While we can track the argument in Azure Machine Learning, we can't automatically
    track the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passing external data as a direct dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As promised, we will now enhance the previous example using a dataset in Azure
    Machine Learning. This will allow us to pass the dataset as a named configuration
    – abstracting the URL and access to the physical location of the data. It also
    automatically enables dataset tracking for the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start in the authoring script, and load the data from the path – only this
    time, using Azure Machine Learning''s `TabularDataset`, created through the `from_delimited_files()`
    factory method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will output the same set of rows as the previous example in pandas – so
    there is almost no difference other than using a different method to create the
    DataFrame. However, now that we have created a *direct dataset*, we can easily
    pass the dataset to the training script as a named dataset configuration – which
    will use the dataset ID under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the pandas example, we write a simplified training script that will access
    the dataset and print the first few records by parsing the input dataset from
    the command-line arguments. In the training script, we can use the `Dataset.get_by_id()`
    method to fetch the dataset by its ID from a workspace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**code/access_data_from_dataset.py**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, we modified the previous code slightly
    and added code to retrieve the current run context, experiment, and the workspace.
    This lets us access the direct dataset from the workspace by passing the dataset
    ID to the `Dataset.get_by_id()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we write a run configuration to submit the preceding code as an experiment
    to Azure Machine Learning. First, we need to convert the dataset into a command-line
    argument and pass it to the training script so it can be automatically retrieved
    in the execution runtime. We can achieve this by using the `as_named_input(name)`
    method on the dataset instance, which will convert the dataset into a named `DatasetConsumptionConfig`
    argument that allows the dataset to be passed to other environments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, the dataset will be passed in direct mode and provided as the
    `name` environment variable in the runtime environment or as the dataset ID in
    the command-line arguments. The dataset will also get tracked in Azure Machine
    Learning as an input argument to the training script.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as we saw in the previous code snippet, we use the `Dataset.get_by_id()`
    method to retrieve the dataset in the training script from the dataset ID. We
    don''t need to manually create or access the dataset ID, because the `DatasetConsumptionConfig`
    argument will be automatically expanded into the dataset ID when the training
    script is called by Azure Machine Learning with a direct dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Access_data_from_dataset.ipynb**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in the preceding code, the dataset is converted to a configuration
    that can simply be passed to the training script through the `as_named_input(name)`
    method. If we submit the experiment and check the logs of the run, we can see
    that Azure Machine Learning passed the dataset ID to the training script:'
  prefs: []
  type: TYPE_NORMAL
- en: 70_driver_log.txt
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The run details for this experiment are shown in *Figure 4.11*. If you look
    at the input arguments, you can see that we passed the `DatasetConsumptionConfig`
    object to the script, which was then converted automatically to the dataset ID.
    Not only is the input argument passed without any information about the location
    of the underlying data, but the input dataset is also recognized as an input to
    the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Run details of the experiment ](img/B17928_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Run details of the experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'By passing a dataset to a training script, Azure Machine Learning automatically
    tracks the dataset with the experiment run. As you can see in *Figure 4.11*, the
    dataset ID is a link to the tracked dataset. When clicking on the dataset ID in
    Azure Machine Learning, it will open a page showing details about the tracked
    dataset, such as description, URL, size, and type of dataset, as shown in *Figure
    4.12*. Like registered datasets, you can also explore the raw data and look at
    dataset column statistics – called the profile – or see any registered models
    derived from this data. Tracked datasets can easily be registered – and hence
    versioned and managed – by clicking on the **Register** action or from code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Direct dataset tracked in Azure Machine Learning ](img/B17928_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Direct dataset tracked in Azure Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in this section, there are important benefits to passing the input
    data to your training script as a dataset argument. This will automatically track
    the dataset in your workspace and connect the dataset with the experimentation
    run.
  prefs: []
  type: TYPE_NORMAL
- en: In the code snippets of this section, we passed the data as a *direct dataset*,
    which means that the training script has to fetch the data again from the external
    URL. This is not always optimal, especially when dealing with large amounts of
    data or when data should be managed in Azure Machine Learning. In the next section,
    we will explore different ways to pass data to the training script.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing data during training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we implicitly passed the URL of the original dataset
    to the training script. While this is a practical and fast solution for small
    public datasets, it's often not the preferred approach for private or larger datasets.
    Imagine your data is stored on a SQL server, Blob storage, or file share instead,
    and password protected. Imagine your dataset contains many gigabytes of files.
    In this section, we will see techniques that work well for both cases.
  prefs: []
  type: TYPE_NORMAL
- en: While external public data reachable through a URL is created and passed as
    a *direct dataset*, all other datasets can be accessed either as a **download**
    or as a **mount**. For big data datasets, Azure Machine Learning also provides
    an option to mount a dataset as a **Hadoop Distributed File System** (**HDFS**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will see authoring scripts that will pass datasets both
    as a download and as a mount. Let''s first create a reference in the authoring
    script to the `cifar10` dataset, which we registered in the previous section.
    The following snippet retrieves a dataset by name from the Azure Machine Learning
    workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Next, we want to pass the dataset to the training script so that we can access
    the training data from the script. The benefit of using datasets is not only tracking
    but the fact that we can simply choose the appropriate data consumption configuration
    that is appropriate for each dataset. It will also help us to separate the training
    script from the training data, making it easy to pass new, updated, or enriched
    data to the same training script without needing to update the training script.
  prefs: []
  type: TYPE_NORMAL
- en: Independently of the consumption method, the training script can always load
    the data from a directory path where it will be either downloaded or mounted.
    Under the hood, Azure Machine Learning inspects the command-line arguments of
    `ScriptRunConfig`, detects the dataset reference, delivers the data to the compute
    environment, and replaces the argument with the path of the dataset in the local
    filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure Machine Learning uses parameter expansion to replace the dataset reference
    with the path to the actual data on disk. To make this more obvious, we will write
    a single training file that will simply list all training files that were passed
    to it. The following code snippet implements this training script:'
  prefs: []
  type: TYPE_NORMAL
- en: code/access_dataset.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In the previous script, we define a single `--input` argument that we will use
    to pass the training data. Then we will output this argument and list all files
    from the directory. We will use this script to pass data with different mounting
    techniques and will see that the data will always be available in the folder.
  prefs: []
  type: TYPE_NORMAL
- en: Having the dataset reference and a simple training script, we can now look at
    a different `ScriptRunConfig` to pass the `cifar10` dataset using the different
    data consumption configurations. While the code is downloaded or mounted by Azure
    Machine Learning before the training script is invoked, we will also explore what
    happens under the hood – so we can apply the same technique to load the training
    data outside of Azure Machine Learning-managed compute environments.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing data as a download
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will first look at downloading the data to the training instance. To do
    so, we will first create a `ScriptRunConfig` constructor in the authoring environment
    where we pass the data to `as_download()`. We will schedule a code snippet that
    will access and output the files passed to the script:'
  prefs: []
  type: TYPE_NORMAL
- en: Access_dataset_as_download.ipynb
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Azure will interpolate the dataset passed by the `input` parameter and replace
    it with the location of the dataset on disk. The data will be automatically downloaded
    to the training environment if the dataset is passed with the `Dataset.as_download()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run this script configuration, the `access_dataset.py` script will output
    the temporary location of the dataset, which was automatically downloaded to disk.
    You can replicate the exact same process in your authoring environment that Azure
    Machine Learning does under the hood. To do so, you can simply call the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Passing data as a download is convenient for small datasets or when using a
    large number of consumers that require a high throughput on the data. However,
    if you are dealing with large datasets, you can also pass them as a *mount* instead.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing data as a mount
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we will mount the data on the training environment. To do
    so, we will again create a `ScriptRunConfig` constructor in the authoring environment
    and this time we invoke the `as_mount()`. We will schedule a code snippet that
    will access and output the files passed to the script:'
  prefs: []
  type: TYPE_NORMAL
- en: Access_dataset_as_mount.ipynb
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the preceding example is very similar to the previous example
    where data was downloaded to disk. In fact, we are reusing the exact same scheduled
    script, `access_dataset.py`, which will output the location of the data on disk.
    However, in this example, the data is not downloaded to this location but mounted
    to the file path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure Machine Learning will interpolate the dataset passed through the input
    argument with the mounted path on disk. Similar to the previous example, you can
    replicate what happens under the hood in Azure Machine Learning and mount the
    data from within your authoring environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the previous snippet, the dataset is mounted and released
    using the mount context''s `start` and `stop` methods. You can also simplify the
    code snippet using Python''s `with` statement to automatically mount and unmount
    the data as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Hence, depending on the use case, we have different options to pass a dataset
    reference to a scheduled script. Independent of the data transport, Azure Machine
    Learning will implement the correct method under the hood and interpolate the
    input arguments so that the training script doesn't need to know how a dataset
    was configured. For the executed script, the data is simply made available through
    a path in the filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Using external datasets with open datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most effective methods to improve the prediction performance of any
    ML model is to add additional information to your training data. A common way
    to achieve this is by joining external datasets to the training data. A good indication
    to join external data is the availability of popular joining keys in your dataset,
    such as dates, locations, countries, and more.
  prefs: []
  type: TYPE_NORMAL
- en: When you work with transactional data that contains dates, you can easily join
    external data to create additional features for the training dataset and hence
    improve prediction performance. Common derived features for dates are weekdays,
    weekends, time to or since weekends, holidays, time to or since holidays, sports
    events, concerts, and more. When dealing with country information, you can often
    join additional country-specific data, such as population data, economic data,
    sociological data, health data, labor data, and more. When dealing with geolocation,
    you can join distance to points of interest, weather data, traffic data, and more.
    Each of these additional datasets gives you additional insights and hence can
    boost your model's performance significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Open Datasets is a service that provides access to curated datasets for the
    transportation, health and genomics, labor and economics, population, and safety,
    categories and common datasets that you can use to boost your model's performance.
    Let's look into three examples.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Before using a specific dataset for a commercial service, please make sure that
    your application is covered by the license. If in doubt, reach out to Microsoft
    via [aod@microsoft.com](mailto:aod@microsoft.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first example, we will investigate the dataset for *worldwide public
    holidays*. The data covers holidays in almost 40 countries or regions from 1970
    to 2099\. It is curated from Wikipedia and the `holidays` Python package. You
    can import them into your environment and access these holidays using the `opendatasets`
    library as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As we see in the code, we can access the dataset from the `azureml-opendatasets`
    package and use it as an Azure Machine Learning dataset. This means we can return
    the pandas or Spark DataFrame for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular dataset is the *US population* by county for the years 2000
    and 2010\. It is broken down by gender and race and sourced from the United States
    Census Bureau:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Another example open dataset is the *Current Employment Statistics* of the
    United States, published by the US **Bureau of Labor Statistics** (**BLS**). It
    contains estimates of employment, hours, and earnings of workers on payrolls in
    the US:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As you saw in this section, Azure Open Datasets gives you a convenient option
    to access curated datasets in the form of Azure Machine Learning datasets right
    from within your Azure Machine Learning workspace. While the number of available
    datasets is still manageable, you can expect the number of available datasets
    to grow over time.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to manage data in Azure Machine Learning using
    datastores and datasets. We saw how to configure the default datastore that is
    responsible for storing all assets, logs, models, and more in Azure Machine Learning,
    as well as other services that can be used as datastores for different types of
    data.
  prefs: []
  type: TYPE_NORMAL
- en: After creating an Azure Blob storage account and configuring it as a datastore
    in Azure Machine Learning, we saw different tools to ingest data into Azure, such
    as Azure Storage Explorer, Azure CLI, and AzCopy, as well as services optimized
    for data ingestion and transformation, Azure Data Factory and Azure Synapse Spark.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent section, we got our hands on datasets. We created file and
    tabular datasets and learned about direct and registered datasets. Datasets can
    be passed as a download or a mount to executed scripts, which will automatically
    track datasets in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned how to improve predication performance by joining third-party
    datasets from Azure Open Datasets to our machine learning process. In the next
    chapter, we will learn how to explore data by performing data analysis and visualization.
  prefs: []
  type: TYPE_NORMAL
