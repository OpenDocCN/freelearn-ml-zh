- en: '*Chapter 4*: Ingesting Data and Managing Datasets'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第四章*：导入数据和管理数据集'
- en: In the previous chapter, we set up and explored the Azure Machine Learning workspace,
    performed data experimentation, and scheduled scripts to run on remote compute
    targets in Azure Machine Learning. In this chapter, we will learn how to connect
    datastores and create, explore, access, and track data in Azure Machine Learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们设置了Azure机器学习工作区，进行了数据实验，并安排了在Azure机器学习中远程计算目标上运行的脚本。在本章中，我们将学习如何连接数据存储，创建、探索、访问和跟踪Azure机器学习中的数据。
- en: First, we will take a look at how data is managed in Azure Machine Learning
    by understanding the concepts of **datastores and datasets**. We will see different
    types of datastores and learn best practices for organizing and storing data for
    **machine learning** (**ML**) in Azure.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将通过了解**数据存储和数据集**的概念来探讨Azure机器学习中数据是如何管理的。我们将查看不同类型的数据存储，并学习在Azure中组织和管理数据以用于**机器学习（ML**）的最佳实践。
- en: Next, we will create an **Azure Blob storage** account and connect it as a datastore
    to Azure Machine Learning. We will cover best practices for ingesting data into
    Azure using popular CLI tools as well as **Azure Data Factory** and **Azure Synapse
    Spark** services.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个**Azure Blob存储**账户，并将其作为数据存储连接到Azure机器学习。我们将介绍使用流行的CLI工具以及**Azure
    Data Factory**和**Azure Synapse Spark**服务将数据导入Azure的最佳实践。
- en: In the following section, we will learn how to create datasets from data in
    Azure, access and explore these datasets, and pass data efficiently to compute
    environments in your Azure Machine Learning workspace. Finally, we will discuss
    how to access Azure Open Datasets to improve your model's performance through
    third-party data sources.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何从Azure中的数据创建数据集，访问和探索这些数据集，并将数据有效地传递到Azure机器学习工作区中的计算环境中。最后，我们将讨论如何通过第三方数据源访问Azure开放数据集，以改善模型性能。
- en: 'The following are the topics that will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Choosing data storage solutions for Azure Machine Learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择Azure机器学习的数据存储解决方案
- en: Creating a datastore and ingesting data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建数据存储和导入数据
- en: Using datasets in Azure Machine Learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Azure机器学习中使用数据集
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use the following Python libraries and versions to
    create and manage datastores and datasets:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下Python库和版本来创建和管理数据存储和数据集：
- en: '`azureml-core 1.34.0`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-core 1.34.0`'
- en: '`azureml-sdk 1.34.0`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-sdk 1.34.0`'
- en: Similar to previous chapters, you can run this code using either a local Python
    interpreter or a notebook environment hosted in Azure Machine Learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，您可以使用本地Python解释器或Azure机器学习中的笔记本环境运行此代码。
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter04](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter04).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有的代码示例都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter04](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter04)。
- en: Choosing data storage solutions for Azure Machine Learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择Azure机器学习的数据存储解决方案
- en: When running ML experiments or training scripts on your local development machine,
    you often don't think about managing your datasets. You probably store your training
    data on your local hard drive, external storage device, or file share. In such
    a case, accessing the data for experimentation or training is not a problem, and
    you don't have to worry about the data location, access permissions, maximal throughput,
    parallel access, storage and egress cost, data versioning, and such.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当在本地开发机器上运行ML实验或训练脚本时，您通常不会考虑管理您的数据集。您可能将训练数据存储在本地硬盘、外部存储设备或文件共享中。在这种情况下，访问实验或训练数据不会成为问题，您也不必担心数据位置、访问权限、最大吞吐量、并行访问、存储和出口成本、数据版本等问题。
- en: However, as soon as you start training an ML model on remote compute targets,
    such as a VM in the cloud or within Azure Machine Learning, you must make sure
    that all your executables can access the training data efficiently. This is even
    more relevant if you collaborate with other people who also need to access the
    data in parallel for experimentation, labeling, and training from multiple environments
    and multiple machines. And if you deploy a model that requires access to this
    data as well – for example, looking up labels for categorical results, scoring
    recommendations based on a user's history of ratings, and the like – then this
    environment needs to access the data as well.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to manage data for different use cases in
    Azure. We will first see the abstractions Azure Machine Learning provides to facilitate
    data access for ML experimentation, training, and deployment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Organizing data in Azure Machine Learning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Azure Machine Learning, data is managed as **datasets** and data storage
    as **datastores**. This abstraction hides the details of location, data format,
    data transport protocol, and access permissions behind the dataset and datastore
    objects and hence lets Azure Machine Learning users focus on exploring, transforming,
    and managing data without worrying about the underlying storage system.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'A **datastore** is an abstraction of a physical data storage system that is
    used to link the existing storage system to an Azure Machine Learning workspace.
    In order to connect the existing storage to the workspace – by creating a datastore
    – you need to provide the connection and authentication details of the storage
    system. Once created, the data storage can be accessed by users through the datastore
    object, which will automatically use the provided credentials of the datastore
    definition. This makes it easy to provide access to data storage to your developers,
    data engineers, and scientists who are collaborating in an Azure Machine Learning
    workspace. Currently, the following services can be connected as datastores to
    a workspace:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Azure Blob containers
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure file share
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Data Lake
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Data Lake Gen2
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure SQL Database
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Database for PostgreSQL
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks File System
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Database for MySQL
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While datastores are abstractions of data storage systems, a **dataset** is
    an abstraction of data in general – for example, data in the form of a file on
    a remote server accessible through a public URL or files and tables within a datastore.
    Azure Machine Learning supports two types of abstraction on data formats, namely
    **tabular datasets** and **file datasets**. The former is used to define *tabular*
    data – for example, from comma- or delimiter-separated files, from Parquet and
    JSON files, or from SQL queries – whereas the latter is used to specify *any binary*
    data from files and folders, such as images, audio, and video data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Tabular datasets can also be defined and used directly from their publicly available
    URL, which is called a `pandas` and `requests`. Both tabular and file datasets
    can be registered in your workspace. We will refer to these datasets as **registered
    datasets**. Registered datasets will show up in your Azure Machine Learning Studio
    under **Datasets**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the default storage accounts of Azure Machine Learning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There exists one special datastore in Azure Machine Learning that is used internally
    to store all snapshots, logs, figures, models, and more when executing experiment
    runs. This is called the **default datastore**, is an Azure Blob storage account,
    and is created automatically with Azure Machine Learning when you set up the initial
    workspace. You can select your own Blob storage as the default datastore during
    the workspace creation or connect your storage account and mark it as default
    in Azure Machine Learning Studio.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.1* shows you the list of datastores in Azure Machine Learning Studio.
    The default datastore is marked as **Default** and generated automatically when
    setting up an Azure Machine Learning workspace. To go to this view, simply click
    on **Datastores** under the **Manage** category in the left menu in Azure Machine
    Learning Studio. To view existing datasets, click on **Datasets** in the **Assets**
    category:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Default datastore in Azure Machine Learning ](img/B17928_04_01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Default datastore in Azure Machine Learning
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'The default datastore is used by Azure Machine Learning internally to store
    all assets and artifacts when no other datastore is defined. You can access and
    use the default datastore in your workspace identically to your custom datastores
    by creating a datastore reference. The following code snippet shows how to get
    a reference to the default datastore:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The default datastore is used internally by Azure Machine Learning to store
    all assets and artifacts during the ML life cycle. Using the previous code snippet,
    you can access the default datastore to store custom datasets and files.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Once we have accessed the default datastore and connected custom datastores,
    we need to think about a strategy for efficiently storing data for different ML
    use cases. Let's tackle this in the next section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Exploring options for storing training data in Azure
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure supports a myriad of different data storage solutions and technologies
    to store data in the cloud – and as we saw in the previous section, many of these
    are supported datastores in Azure Machine Learning. In this section, we will explore
    some of these services and technologies to understand which ones can be used for
    machine learning use cases.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Database systems can be broadly categorized by the type of *data* and *data
    access* into the following two categories:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '**Relational database management systems** (**RDBMSs**) are often used to store
    normalized transactional data using B-tree-based ordered indices. Typical queries
    filter, group, and aggregate results by joining multiple rows from multiple tables.
    Azure supports different RDBMSs, such as Azure SQL Database, as well as Azure
    Database for PostgreSQL and MySQL.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关系数据库管理系统**（**RDBMSs**）通常用于存储使用基于 B 树的有序索引的规范化事务数据。典型的查询通过连接多个表中的多行来过滤、分组和聚合结果。Azure
    支持不同的 RDBMS，例如 Azure SQL 数据库、Azure Database for PostgreSQL 和 MySQL。'
- en: '**NoSQL**: Key-value-based storage systems are often used to store de-normalized
    data with hash-based or ordered indices. Typical queries access a single record
    from a collection distributed based on a partition key. Azure supports different
    NoSQL-based services such as Azure Cosmos DB and Azure Table storage.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NoSQL**：基于键值存储的系统通常用于存储使用基于哈希或有序索引的非规范化数据。典型的查询通过基于分区键分布的集合访问单个记录。Azure 支持不同的基于
    NoSQL 的服务，如 Azure Cosmos DB 和 Azure 表存储。'
- en: As you can see, depending on your use cases, you can use both database technologies
    to store data for machine learning. While RDBMSs are great technologies to store
    training data for machine learning, NoSQL systems are great to store lookup data
    – such as training labels – or ML results such as recommendations, predictions,
    or feature vectors.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，根据您的用例，您可以使用这两种数据库技术来存储机器学习数据。虽然 RDBMS 是存储机器学习训练数据的优秀技术，但 NoSQL 系统非常适合存储查找数据，例如训练标签，或机器学习结果，如推荐、预测或特征向量。
- en: Instead of choosing a database service, another popular choice for machine learning
    is to use data storage systems. On disk, most database services persist as data
    pages on **file** or **blob storage systems**. Blob storage systems are a very
    popular choice for storing all kinds of data and assets for machine learning due
    to their scalability, performance, throughput, and cost. Azure Machine Learning
    makes extensive use of blob storage systems, especially for storing all operational
    assets and logs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择数据库服务之外，机器学习的另一种流行选择是使用数据存储系统。在磁盘上，大多数数据库服务以数据页的形式持久化在 **文件** 或 **blob 存储系统**
    上。由于它们的可扩展性、性能、吞吐量和成本，blob 存储系统是存储各种数据和资产以供机器学习使用的非常流行的选择。Azure 机器学习广泛使用 blob
    存储系统，特别是用于存储所有操作资产和日志。
- en: Popular Azure blob storage services are Azure Blob storage and Azure Data Lake
    Storage, which provide great flexibility to implement efficient data storage and
    access solutions through different choices of data formats. While Azure Blob storage
    supports most common blob-based filesystem operations, Azure Data Lake Storage
    implements efficient directory services, which makes it a popular general-purpose
    storage solution for horizontally scalable filesystems. It is a popular choice
    for storing large machine learning training datasets.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的 Azure Blob 存储服务包括 Azure Blob 存储和 Azure Data Lake Storage，它们通过不同的数据格式选择提供了极大的灵活性，以实现高效的数据存储和访问解决方案。虽然
    Azure Blob 存储支持大多数基于 blob 的文件系统操作，但 Azure Data Lake Storage 实现了高效的目录服务，这使得它成为横向可扩展文件系统的流行通用存储解决方案。它是存储大型机器学习训练数据集的流行选择。
- en: While tabular data can be stored efficiently in RDBMS systems, similar properties
    can be achieved by choosing the correct data formats and embedded clustered indices
    while storing data on blob storage systems. Choosing the right data format will
    allow your filesystem to efficiently store, read, parse, and aggregate information.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然表格数据可以在 RDBMS 系统中有效地存储，但在 blob 存储系统上存储数据时，通过选择正确的数据格式和嵌入的聚类索引也可以实现类似的功能。选择正确的数据格式将允许您的文件系统有效地存储、读取、解析和聚合信息。
- en: Common data format choices can be categorized into textual (CSV, JSON, and more)
    as well as binary formats (images, audio, video, and more). Binary formats for
    storing tabular data are broadly categorized into row-compressed (Protobuf, Avro,
    SequenceFiles, and more) or column-compressed (Parquet, ORC, and more) formats.
    A popular choice is also to compress the whole file using Gzip, Snappy, or other
    compression algorithms.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的数据格式选择可以分为文本格式（CSV、JSON 等）以及二进制格式（图像、音频、视频等）。用于存储表格数据的二进制格式通常分为行压缩格式（Protobuf、Avro、SequenceFiles
    等）或列压缩格式（Parquet、ORC 等）。另一种流行的选择是使用 Gzip、Snappy 或其他压缩算法压缩整个文件。
- en: One structure that most data storage systems have in common is a hierarchical
    path or directory structure to organize data blobs. A popular choice for storing
    training data for machine learning is to implement a partitioning strategy for
    your data. This means that data is organized in multiple directories where each
    directory contains all the data for a specific key, also called the partitioning
    key.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据存储系统都共同拥有一种结构，即分层路径或目录结构来组织数据块。对于存储机器学习训练数据的一个流行选择是实施数据分区策略。这意味着数据被组织在多个目录中，每个目录包含特定键的所有数据，也称为分区键。
- en: Cloud providers offer a variety of different storage solutions, which can be
    customized further by choosing different indexing, partitioning, format, and compression
    techniques. A common choice for storing tabular training data for machine learning
    is a column-compressed binary format such as Parquet, partitioned by ingestion
    date, stored on Azure Data Lake Storage, for efficient management operations and
    scalable access.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商提供各种不同的存储解决方案，可以通过选择不同的索引、分区、格式和压缩技术进一步定制。对于存储机器学习表格训练数据的一个常见选择是使用列压缩的二进制格式，如Parquet，按摄取日期分区，存储在Azure数据湖存储上，以实现高效的管理操作和可扩展的访问。
- en: Creating a datastore and ingesting data
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据存储和导入数据
- en: After having a look through the options for storing data in Azure for ML processing,
    we will now create a storage account, which we will use throughout the book for
    our raw data and ML datasets. In addition, we will have a look at how to transfer
    some data into our storage account manually and how to perform this task automatically
    by utilizing integration engines available in Azure.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看Azure中用于ML处理的数据存储选项之后，我们现在将创建一个存储账户，我们将在整个书中使用它来存储原始数据和ML数据集。此外，我们还将探讨如何手动将一些数据传输到我们的存储账户，以及如何通过利用Azure中可用的集成引擎自动执行此任务。
- en: Creating Blob Storage and connecting it with the Azure Machine Learning workspace
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Blob存储并将其与Azure机器学习工作区连接
- en: Let's start by creating a storage account. Any storage account will come with
    a file share, a queue, and table storage for you to utilize in other scenarios.
    In addition to those three, you can either end up with Blob Storage or a Data
    Lake, depending on the settings you provide at creation time. By default, a Blob
    storage account will be created. If we instead want a Data Lake account, we must
    set the `enable-hierarchical-namespace` setting to `True`, as Data Lake offers
    an actual hierarchical folder structure and not a flat namespace.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个存储账户。任何存储账户都将附带一个文件共享、一个队列以及表格存储，以便您在其他场景中利用。除了这三个之外，根据您在创建时提供的设置，您最终可能会得到Blob存储或数据湖。默认情况下，将创建Blob存储账户。如果我们想创建数据湖账户，我们必须将`enable-hierarchical-namespace`设置设置为`True`，因为数据湖提供了一个实际的分层文件夹结构，而不是一个扁平的命名空间。
- en: Creating Blob Storage
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Blob存储
- en: 'Keeping that in mind, let''s create a Blob Storage account:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这一点，让我们创建一个Blob存储账户：
- en: Navigate to a terminal of your choosing, log in to Azure, and check that you
    are working in the correct subscription as we learned in [*Chapter 3*](B17928_03_ePub.xhtml#_idTextAnchor054),
    *Preparing the Azure Machine Learning Workspace*.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到您选择的终端，登录Azure，并确认您正在正确的订阅中工作，正如我们在[*第3章*](B17928_03_ePub.xhtml#_idTextAnchor054)，“准备Azure机器学习工作区”中学到的。
- en: 'As we want to create a storage account, let''s have a look at the options and
    required settings for doing so by running the following command:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们要创建一个存储账户，让我们通过运行以下命令来查看创建账户的选项和所需设置：
- en: '[PRE1]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Looking through the result, you will see a very long list of possible arguments,
    but the only required ones are `name` and `resource-group`. Still, we should look
    further through this, as a lot of the other settings are still set to certain
    default values, which might be incorrect for our case.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 查看结果，您将看到一个非常长的可能参数列表，但唯一必需的是`name`和`resource-group`。尽管如此，我们仍然应该进一步查看，因为许多其他设置仍然设置为某些默认值，这些值可能不适合我们的情况。
- en: Going through the list, you will find a lot of options concerning network or
    security settings. The default for most of them is to at least allow access from
    everywhere. At this moment, we are not too concerned about virtual network integration
    or handling our own managed keys in Azure Key Vault.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看列表时，您会发现许多关于网络或安全设置的选项。大多数选项的默认设置是至少允许从任何地方访问。在这个时候，我们不太关心虚拟网络集成或处理Azure
    Key Vault中的自己的管理密钥。
- en: Besides all these options, there are a few that define the type of storage account
    we set, namely `enable-hierarchical-namespace`, `kind`, `location`, and `sku`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: We already discussed the first option and as the default is `False`, we can
    ignore it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Looking at `kind`, you see a list of storage types. You might think we need
    to choose `BlobStorage`, but unfortunately, that is a legacy setting left there
    for any storage account still running on the first version, V1\. The default (`StorageV2`)
    is the best option for our scenario.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Looking at `location`, we see that we apparently can set a default location
    for all deployments, therefore it is not flagged as required. As we did not do
    that so far, we will just provide it when deploying the storage account.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, looking at `sku`, we see a combined setting of an option concerning
    the type of disk technology used (`Standard`/`Premium`), where `Standard` denotes
    HDD storage and `Premium` denotes SSD storage, and an option defining the data
    redundancy scheme (LRS/ZRS/GRS/RAGRS/GZRS). If you want to learn more about the
    redundancy options, follow this link: [https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy](https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy).
    As both increase costs, feel free to either stick with the default (`Standard_RAGRS`)
    or go with local redundancy (`Standard_LRS`).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create our storage account. Please be aware that the name you choose
    must be globally unique, therefore you cannot choose the one you will read in
    the following command:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output this creates will show you the detailed settings for the created
    storage account.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final step, let''s create a container in our new blob storage. For that,
    run the following command with the appropriate account name:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result will show `True` at the end, but will give you some warnings beforehand,
    something like this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The command worked because it automatically pulled the account key of the storage
    account through our session. Normally, to access a storage account, we either
    need an AD identity, a key to access the whole account (`account-key`), or a shared-access
    key (`sas-token`) to access only a specific subset of folders or containers. We
    will come back to this when connecting from the ML workspace.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the result, run this command:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have our storage, let's connect it to our Azure Machine Learning
    workspace.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Creating a datastore in Azure Machine Learning
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to not bother with the storage account itself anymore when working
    with our ML scripts, we will now create a permanent connection to a container
    in a storage account and define it as one of our datastores in the Azure Machine
    Learning workspace.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will guide you through this process:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s understand what is required to create a datastore by running
    the following command:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Looking through the output,, we understand that the name of the resource group,
    the name of the ML workspace, and a YAML file is needed. We have two of those
    three things. Therefore, let's understand what the YAML file has to look like.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to [https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-datastore-blob](https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-datastore-blob),
    where you will find the required schema of our file and some examples. Going through
    the examples, you will see that they mainly differ concerning the way to authenticate
    to the storage account. The most secure of them is limited access via a SAS token
    and therefore we will pick that route.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Please either download the `blobdatastore.yml` file from the files for [*Chapter
    4*](B17928_04_ePub.xhtml#_idTextAnchor071), *Ingesting Data and Managing Datasets,*
    from the GitHub repository or create a file with the same name and the following
    content:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Please enter the appropriate account name for your case. The only thing missing
    now is the SAS token, which we need to create for our `mlfiles` container.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to create a SAS token for our container:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This command generates a SAS token with an expiration date of 01/01/2023 and
    permissions to `mlfiles` container. Choose an expiration date that is far enough
    in the future for you to work with this book. In normal circumstances, you would
    choose a much shorter expiration date and rotate this key accordingly.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'The result should be in this kind of format:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Take this result (without quotations) and enter it in the `sas_token` field
    in the YAML file.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the directory the YAML file is in so that we can finally create
    the datastore in the Azure Machine Learning workspace by running the following
    command:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The result should look like the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With these steps, we have registered a datastore connected to our blob storage
    using a SAS token.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'You can follow the same steps when connecting to a Data Lake Storage, but be
    aware that to access a data lake, you will need to create a **service p****rincipal**.
    A detailed description of this can be found here: [https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: As discussed before, we could have created a blob storage by navigating to the
    wizard in the Azure portal, creating a SAS token for the container there, and
    entering it in the datastore creation wizard in Azure Machine Learning Studio.
    We used the Azure CLI so that you can get comfortable with this, as this is required
    to automate such steps in the future, especially when we talk about infrastructure-as-code
    and DevOps environments.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case, feel free to navigate to the **Datastores** tab in Azure Machine
    Learning Studio. *Figure 4.2* shows our newly created workspace:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Created datastore ](img/B17928_04_02.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 创建的数据存储](img/B17928_04_02.jpg)'
- en: Figure 4.2 – Created datastore
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 创建的数据存储
- en: Keep this tab open, so we can verify later via the `mlfiles` container, which
    we will start doing in the following section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 保持此标签页打开，以便我们可以在下一节中通过 `mlfiles` 容器进行验证。
- en: Ingesting data into Azure
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据导入 Azure
- en: We created an Azure Blob storage account and learned how to organize and format
    files and tabular data for common ML use cases. However, one often-neglected step
    is how to efficiently ingest data into these datastores, or into Azure in general.
    There are different solutions for different datasets and use cases, from ad hoc,
    automated, parallelized solutions, and more. In this section, we will have a look
    at methods to upload and transform data either in a manual or an automated fashion
    to a relational database (SQL, MySQL, or PostgreSQL) or a storage account in Azure.
    Finally, we will upload a dataset file to the previously created blob storage.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个 Azure Blob 存储帐户，并学习了如何为常见的机器学习用例组织和格式化文件和表格数据。然而，一个常被忽视的步骤是如何将这些数据存储或
    Azure 中的数据有效地导入。针对不同的数据集和用例有不同的解决方案，从临时、自动化、并行化解决方案等。在本节中，我们将探讨将数据手动或自动上传到关系数据库（SQL、MySQL
    或 PostgreSQL）或 Azure 中的存储帐户的方法。最后，我们将上传一个数据集文件到之前创建的 blob 存储。
- en: Understanding tooling for the manual ingestion of data
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解手动导入数据的相关工具
- en: If you work with a small number of datasets and files and you do not need to
    transfer data from other existing sources, a manual upload of data is the go-to
    option.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您处理少量数据集和文件，并且不需要从其他现有源传输数据，手动上传数据是首选选项。
- en: 'The following list shows possible options to bring data into your datastores
    or directly into your ML pipelines:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了将数据带入您的数据存储或直接带入您的机器学习管道的可能选项：
- en: '**Azure Storage Explorer**: Storage Explorer is an interactive application
    that allows you to upload data to and control datastores, such as storage accounts
    and managed disks. This is the easiest tool to use for managing storage accounts
    and can be found here: [https://azure.microsoft.com/en-us/features/storage-explorer/#overview](https://azure.microsoft.com/en-us/features/storage-explorer/#overview).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure 存储资源管理器**：存储资源管理器是一个交互式应用程序，允许您上传数据到并控制数据存储，例如存储帐户和管理磁盘。这是管理存储帐户最容易使用的工具，您可以在以下位置找到它：[https://azure.microsoft.com/en-us/features/storage-explorer/#overview](https://azure.microsoft.com/en-us/features/storage-explorer/#overview)。'
- en: '**Azure CLI**: As we saw before, we basically can do anything with the CLI,
    including creating and uploading blobs to a storage account. You can find the
    appropriate commands to upload blobs in the storage extension described here:
    [https://docs.microsoft.com/en-us/cli/azure/storage/blob](https://docs.microsoft.com/en-us/cli/azure/storage/blob).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure CLI**：正如我们之前看到的，我们基本上可以使用 CLI 做任何事情，包括在存储帐户中创建和上传 blob。您可以在以下存储扩展中找到上传
    blob 的适当命令：[https://docs.microsoft.com/en-us/cli/azure/storage/blob](https://docs.microsoft.com/en-us/cli/azure/storage/blob)。'
- en: '**AzCopy**: This is another command-line tool specifically designed to copy
    blobs or files to a storage account. Whether you use Azure CLI packages or AzCopy
    comes down to personal preference, as there are no clear performance differences
    between these two options. You can find the download link and the description
    here: [https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AzCopy**：这是另一个专门设计用于将 blob 或文件复制到存储帐户的命令行工具。您使用 Azure CLI 软件包还是 AzCopy 取决于个人喜好，因为这两种选项之间没有明显的性能差异。您可以在以下位置找到下载链接和描述：[https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10)。'
- en: '**The Azure portal**: For any service, you will always find a web interface
    directly in the Azure portal to upload or change data. If you navigate to a storage
    account, you can use the inbuilt storage browser to upload blobs and files directly
    through the web interface. The same is true for any of the database technologies.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure 门户**：对于任何服务，您都会在 Azure 门户中直接找到一个用于上传或更改数据的网络界面。如果您导航到存储帐户，您可以使用内置的存储浏览器通过网络界面直接上传
    blob 和文件。对于任何数据库技术也是如此。'
- en: '**RDBMS management tooling**: You can use any typical management tool to configure,
    create, and change tables and schemas in a relational database. For a SQL database
    and Synapse, this would be **SQL Server Management Studio** ([https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15));
    for PostgreSQL, this would be **pgAdmin** ([https://www.pgadmin.org/](https://www.pgadmin.org/));
    and for MySQL, this would be **MySQL Workbench** ([https://docs.microsoft.com/en-us/azure/mysql/connect-workbench](https://docs.microsoft.com/en-us/azure/mysql/connect-workbench)).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Data Studio**: Data Studio allows you to connect to any Microsoft SQL
    database, to Synapse, to a PostgreSQL database in Azure, and to Azure Data Explorer.
    It is a multiplatform tool very similar to the typical management tooling mentioned
    in the last point but in one platform. You can download this tool here: [https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Machine Learning designer (Import Data)**: If you do not want to use
    an Azure Machine Learning datastore, you can use the **Import Data** component
    in the Machine Learning designer to add data ad hoc to your pipelines. This is
    not the cleanest way to operate, but an option nonetheless. You can find all information
    about this method here: [https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/import-data](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/import-data).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we test out some of these options, let's have a look at the options to
    create automated data flows and transform data in Azure.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Understanding tooling for automated ingestion and transformation of data
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Copying data manually is completely fine for small tests and probably even for
    most of the tasks we will perform in this book, but in a real-world scenario,
    we will need to not only integrate with a lot of different sources but will also
    need a process that does not include a person manually moving data from A to B.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we will now have a look at services that allow us to transform and
    move data in an automated fashion and that integrate very well with pipelines
    and MLOps in Azure Machine Learning.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Azure Data Factory
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Azure Data Factory is the enterprise-ready solution for moving and transforming
    data in Azure. It offers the ability to connect to hundreds of different sources
    and to create pipelines to transform the integrated data, calling multiple other
    services in Azure.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to create a data factory:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Please be aware that the name, once again, has to be globally unique. In addition,
    before deployment, the CLI will ask you to install the `datafactory` extension.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are done, navigate to the resource in the Azure portal, and on the
    **Overview** tab, click on **Open Azure Data Factory Studio**, which will lead
    you to the workbench for your data factory instance. You should see a view as
    shown in *Figure 4.3*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Data Factory resource view ](img/B17928_04_03.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Data Factory resource view
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'From this view, you can create pipelines, datasets, data flows, and power queries.
    Let''s briefly discuss what they are:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipelines**: Pipelines are the main star of Azure Data Factory. You can create
    complex pipelines calling multiple services to pull data from a source, transform
    it, and store it in a sink.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Datasets**: Datasets are used in a pipeline as a source or a sink. Therefore,
    before building a pipeline, you can define a connection to specific data in a
    datastore that you want to read from or write to in the end.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data flows**: Data flows allows you to do the actual processing or transformation
    of data within Data Factory itself, instead of calling a different service to
    do the heavy lifting.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power Query**: Power Query allows you to do data exploration with DAX inside
    Data Factory, which is typically only possible with Power BI or Excel otherwise.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you click on the three dots next to **Pipeline**, you can create a new one,
    which will result in the following view shown in *Figure 4.4*:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Creating a Data Factory pipeline ](img/B17928_04_04.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Creating a Data Factory pipeline
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Having a look through the possible activities, you will find a way to copy data
    (**Copy Data**) from A to B, to execute a script in Azure Functions (**Azure Function**),
    to call a stored procedure in a SQL database (**Stored Procedure**), to execute
    a notebook in Databricks (**Notebook**), and to execute an ML pipeline (**Machine
    Learning Execute Pipeline**), among other things. With these activities and the
    control tools you will find in **General** and **Iteration & conditionals**, you
    can build very complex data pipelines to move and transform your data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: As you might have noticed, Azure Synapse is missing from the list of activities.
    The reason for that is that Synapse has its own version of Data Factory integrated
    into the platform. Therefore, if you are using a SQL pool or a Spark pool in Synapse,
    you can use the integration tool of Synapse instead, which will give you access
    to running a notebook in the Synapse Spark pool or a stored procedure on the SQL
    pool.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are looking for an in-depth overview of Azure Data Factory, have a look
    at Catherine Wilhelmsen''s *Beginners Guide to Azure Data Factory*: [https://www.cathrinewilhelmsen.net/series/beginners-guide-azure-data-factory/](https://www.cathrinewilhelmsen.net/series/beginners-guide-azure-data-factory/).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what we need to understand is that there are two ways to integrate this
    Data Factory pipeline into Azure Machine Learning:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '**Read results from a storage account**: We can just run the transformation
    pipeline in Data Factory, transforming our data, and then store the result in
    a storage account. We then access the data as we learned via an ML datastore.
    In this scenario, any pipeline we have in Azure Machine Learning is disconnected
    from the transformation pipelines in Data Factory, which might not be the optimal
    way for MLOps.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invoke Azure Machine Learning from Data Factory**: We can create a transformation
    pipeline and invoke the actual Azure Machine Learning pipeline as part of the
    Data Factory pipeline. This is the preferred way if we are starting to build an
    end-to-end MLOps workflow.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For further information on this, have a read through the following article:
    [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Azure Synapse Spark pools
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 2*](B17928_02_ePub.xhtml#_idTextAnchor034), *Choosing
    the Right Machine Learning Service in Azure*, Azure Databricks and Azure Synapse
    give you the option to run Spark jobs in a Spark pool. Apache Spark can help you
    transform and preprocess extremely large datasets by utilizing the distributive
    nature of the node pool underneath. Therefore, this tool can be helpful to take
    apart and filter out datasets before starting the actual machine learning process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that we can run notebooks from either Azure Data Factory or from
    the integration engine in Azure Synapse and therefore already have access to these
    services. On top of that, we have the option to add a Synapse Spark pool as a
    so-called **linked service** in the Azure Machine Learning workspace (see the
    **Linked Services** tab in Azure Machine Learning Studio). Doing this step gives
    us the opportunity to access not only the ML compute targets but also the Spark
    pool as a target for computations via the Azure Machine Learning SDK.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create this link either via Azure Machine Learning Studio or via the
    Azure Machine Learning Python SDK, both of which are described in the following
    article: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-link-synapse-ml-workspaces](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-link-synapse-ml-workspaces).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Through this direct integration, we can run transformation steps in our ML pipelines
    through a Spark cluster and therefore get another good option for building a clean
    end-to-end MLOps workflow.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Copying data to Blob storage
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, that we have a good understanding of most of the options to move and transform
    data, let's upload a dataset to our storage account.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B17928_05_ePub.xhtml#_idTextAnchor085), *Performing Data Analysis
    and Visualization*, we will start analyzing and preprocessing data. To prepare
    for this, let's upload the dataset we will work with in that chapter.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'We will work with the **Melbourne Housing dataset**, created by Anthony Pino,
    which you can find here: [https://www.kaggle.com/anthonypino/melbourne-housing-market](https://www.kaggle.com/anthonypino/melbourne-housing-market).
    The reason to work with this dataset is the domain it covers, as everyone understands
    housing, and the reasonable cleanliness of the data. If you continue your journey
    through working with data, you will find out that there are a lot of datasets
    out there, but only a few that are clean and educational.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: In addition, to make our lives a bit easier when analyzing the dataset in the
    next chapter, we will actually work with a subset of this dataset.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the next steps so that we can make this file available in our `mldemoblob`
    datastore:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Download the `melb_data.csv` file from [https://www.kaggle.com/dansbecker/melbourne-housing-snapshot](https://www.kaggle.com/dansbecker/melbourne-housing-snapshot)
    and store it in a suitable folder on your device.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to that folder and run the following command in the CLI, replacing
    the storage account name with your own:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To verify this, let''s have a look at another option to move this file. Install
    Azure Storage Explorer and log in to your Azure account in that application. Navigate
    to your storage account and open the `mlfiles` container. It should show you a
    view as seen in *Figure 4.5*:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Azure Storage Explorer ](img/B17928_04_05.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Azure Storage Explorer
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, our file is where it is supposed to be. We could have also just
    dragged and dropped the file directly here, creating a blob file automatically.
    From here on out, feel free to use what feels more comfortable to you.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: To finish this up, have a look at the application itself. For example, if you
    right-click on the container, you can choose an option called **Get Shared Access
    Signature**, which opens a wizard allowing you to create a SAS token directly
    here, instead of as we did via the command line.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the previous steps, we made our raw dataset file available in our storage
    account and therefore in our ML datastore. In the next section, we will have a
    look at how to create an Azure Machine Learning dataset from these raw files and
    what features they offer to support us in our ongoing ML journey.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Using datasets in Azure Machine Learning
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections of this chapter, we discussed how to get data into
    the cloud, store the data in a datastore, and connect the data via a **datastore
    and d****ataset** to an Azure Machine Learning workspace. We did all this effort
    of managing the data and data access centrally in order to use the data across
    all compute environments, either for experimentation, training, or inferencing.
    In this section, we will focus on how to create, explore, and access these datasets
    during training.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Once the data is managed as datasets, we can track the data that was used for
    each experimentation or training run in Azure Machine Learning. This will give
    us visibility of the data used for a specific training run and for the trained
    model – an essential step in creating reproducible end-to-end machine learning
    workflows.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of organizing your data into datasets is that you can easily
    pass a managed dataset to your experimentation or training scripts via **direct
    access**, **download**, or **mount**. The direct access method is useful for publicly
    available data sources, the *download* method is convenient for small datasets,
    and the *mount* method is useful for large datasets. In Azure Machine Learning
    training clusters, this is completely transparent, and the data will be provided
    automatically. However, we can use the same technique to access the data in any
    other Python environment, by simply having access to the dataset object.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: In the last part of this section, we will explore Azure Open Datasets – a collection
    of curated Azure Machine Learning datasets you can consume directly from within
    your Azure Machine Learning workspace.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Creating new datasets
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are multiple ways to create new datasets, but most of them differentiate
    between tabular and file datasets. You need to use different constructors based
    on the type of dataset you want to create:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '`Dataset.Tabular.from_*` for tabular datasets'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataset.File.from_*` for file-based datasets (for example, image, audio, and
    more)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For tabular datasets, we also differentiate between the data being accessed
    from the original location through a public URL – called a *direct dataset* –
    or stored on either the default or a custom *datastore*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: A `Dataset` object can be accessed or passed around in the current environment
    through its object reference. However, a dataset can also be registered (and versioned),
    and hence accessed through the dataset name (and version) – this is called a *registered
    dataset*.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see a simple example of a direct dataset, which is defined as a tabular
    dataset, and a publicly available URL containing a delimiter-separated file with
    the data:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see in the code, we can create a *direct dataset* by passing the
    URL to a publicly accessible delimiter-separated file. When passing this dataset
    internally, every consumer will attempt to fetch the dataset from its URL.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Direct dataset ](img/B17928_04_06.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Direct dataset
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a reference to a datastore, we can access data within it. In the
    following example, we create a file dataset from files stored in a directory of
    the `mldata` datastore:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see in the example, we can register data from within the datastore
    as datasets. In this example, we defined all files in a folder as a file dataset,
    but we could also define a delimiter-separated file in Blob storage as a tabular
    dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – File dataset ](img/B17928_04_07.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – File dataset
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we register this dataset in the workspace using the following
    code snippet to create a *registered dataset*:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The previous code will register the direct dataset in your workspace and return
    a registered dataset. Registered datasets are listed in Azure Machine Learning
    Studio, and can be accessed via the dataset name instead of the `Dataset` Python
    object.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The `create_new_version` argument controls whether we want to create a new version
    of an existing dataset. Once a new dataset version is created, the dataset can
    be accessed through the dataset name – which will implicitly access the latest
    version – or through its name and a specific version. Dataset versions are useful
    to manage different iterations of the dataset within your workspace.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data in datasets
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are multiple options to explore registered datasets in Azure Machine
    Learning. For tabular datasets, the most convenient way is to load and analyze
    a dataset programmatically in an Azure Machine Learning workspace. To do so, you
    can simply reference a dataset by its name and version as shown in the following
    snippet:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once you have a reference to the dataset, you can convert a dataset reference
    to an actual in-memory **pandas** DataFrame or a lazy-loaded **Spark** or **Dask**
    DataFrame. To do so, you can call one of the following methods:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '`to_pandas_dataframe()` to create an in-memory pandas DataFrame'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`to_spark_dataframe()` to create a lazily loaded Spark DataFrame'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`to_dask_dataframe()` to create a lazily loaded Dask DataFrame'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see the three commands in action, starting with the in-memory pandas
    DataFrame. The following code snippet will load all the data into a pandas DataFrame
    and then return the first five rows of the DataFrame:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After loading the DataFrame, you can run your favorite pandas methods to explore
    the datasets. For example, good commands to get started are `info()` to see columns
    and datatypes and `describe()` to see statistics of the numerical values of the
    DataFrame.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Lazy datasets are datasets that only load some data to memory when explicitly
    needed, for example, when a result of a computation is required. Non-lazy datasets
    load all the data into memory and hence are limited by the available memory.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are more familiar with PySpark, you can also transform a dataset into
    a Spark DataFrame with the following code snippet. In contrast to the previous
    example, this code won''t actually load all data into memory but only fetches
    the data required for executing the `show()` command – this makes it a great choice
    for analyzing large datasets:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Another alternative is to return a Dask DataFrame of the dataset. Dask is a
    Python library for parallel computing that supports lazy datasets with a pandas-
    and NumPy-like API. Hence you can run the following code to return the first five
    rows of the DataFrame lazily:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Once you have programmatic access to the data in your favorite numeric or statistical
    libraries, you can slice and dice your dataset as much as needed. While programmatic
    access is great for reproducibility and customization, users often just want to
    understand how the data is structured and see a few example records. Azure Machine
    Learning also offers the possibility to explore the dataset in the Data Studio
    UI.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'To get to this view, go to **Datasets**, select a dataset, and click on the
    **Explore** tab. The first page shows you a preview of your data, including the
    first *n* rows as well as some basic information about the data – such as the
    number of rows and columns. The following screenshot shows an example:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Dataset with data preview ](img/B17928_04_08.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Dataset with data preview
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the second tab, you can generate and view a data profile. This
    profile is similar to calling `describe()` on the pandas DataFrame – a statistical
    analysis of each column in the dataset, but with support for categorical data
    and some more useful information. As you can see in *Figure 4.9*, it also shows
    a figure with the data distribution for each column:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Dataset with data profile ](img/B17928_04_09.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Dataset with data profile
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the previous figure, this is a very useful summary of the
    dataset. The insights from this view are important for everyone working with this
    dataset.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw multiple ways to access and analyze data stored in Azure
    Machine Learning datasets – programmatically via Python and your favorite numerical
    libraries or via the UI.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Tracking datasets in Azure Machine Learning
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: End-to-end tracking of all assets that go into your final production model is
    essential for reproducibility and interpretability but also auditing and tracking.
    A machine learning model is a function that minimizes a loss function by iterating
    and sampling experiments from your training data. Therefore, the training data
    itself should be treated as being a part of the model itself, and hence should
    be managed, versioned, and tracked through the end-to-end machine learning process.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to take advantage of datasets to add data tracking to our experiments.
    A good way to understand the differences between data tracking capabilities is
    to look at two examples: first, loading a CSV dataset from a URL, and then loading
    the same data from the same URL but through a dataset abstraction in Azure Machine
    Learning. However, we don''t only want to load the data, but also pass it from
    the authoring script to the training script as an argument.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: We will first use `pandas` to load a CSV file directly from the URL and pass
    it to the training script as a URL. In the next step, we will enhance this method
    by using a direct dataset instead, allowing us to conveniently pass the dataset
    configuration to the training script and track the dataset for the experiment
    run in Azure Machine Learning.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Passing external data as a URL
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start our example using data that is available as a CSV file from a remote
    URL, a common way to distribute public datasets. In the first example without
    Azure Machine Learning dataset tracking, we will use the `pandas` library to fetch
    and parse the CSV file:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started with the first code snippet using pandas'' `read_csv()`
    method as an example to fetch data via a public URL from a remote server. However,
    this is just an example – you could replace it with any other method to fetch
    data from a remote location:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Our goal is to pass the data from the authoring script to the training script,
    so it can be tracked and updated easily in the future. To achieve this, we can''t
    send the DataFrame directly, but have to pass the URL to the CSV file and use
    the same method to fetch the data in the training script. Let''s write a small
    training script whose only job is to parse the command-line arguments and fetch
    the data from the URL:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '**code/access_data_from_path.py**'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As we see in the preceding code, we pass the data path from the command-line
    `--input` argument and then load the data from the location using pandas' `read_csv()`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a `ScriptRunConfig` constructor to submit an experiment run
    to Azure Machine Learning that executes the training script from *step 1*. For
    now, we are not performing any training but only want to understand what data
    is passed between the authoring and execution runtime:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Access_data_from_path.ipynb**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s execute the run configuration to run the experiment and track the run
    details in Azure Machine Learning. Once the experiment run has finished, we navigate
    to Azure Machine Learning and check the details of this run. As we can see in
    *Figure 4.10*, Azure Machine Learning will track the `script` argument as expected
    but cannot associate the argument to a dataset:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Run details of the experiment ](img/B17928_04_10.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Run details of the experiment
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s summarize the downsides of this approach:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: We can't pass the pandas DataFrame or a DataFrame identifier to the training
    script; we have to pass the data through the URL to its location. If the file
    path changes, we have to update the argument for the training script.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training script doesn't know that the input path refers to the input data
    for the training script, it's simply a string argument to the training script.
    While we can track the argument in Azure Machine Learning, we can't automatically
    track the data.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passing external data as a direct dataset
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As promised, we will now enhance the previous example using a dataset in Azure
    Machine Learning. This will allow us to pass the dataset as a named configuration
    – abstracting the URL and access to the physical location of the data. It also
    automatically enables dataset tracking for the experiment:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'We start in the authoring script, and load the data from the path – only this
    time, using Azure Machine Learning''s `TabularDataset`, created through the `from_delimited_files()`
    factory method:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This will output the same set of rows as the previous example in pandas – so
    there is almost no difference other than using a different method to create the
    DataFrame. However, now that we have created a *direct dataset*, we can easily
    pass the dataset to the training script as a named dataset configuration – which
    will use the dataset ID under the hood.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the pandas example, we write a simplified training script that will access
    the dataset and print the first few records by parsing the input dataset from
    the command-line arguments. In the training script, we can use the `Dataset.get_by_id()`
    method to fetch the dataset by its ID from a workspace:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**code/access_data_from_dataset.py**'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see in the preceding code, we modified the previous code slightly
    and added code to retrieve the current run context, experiment, and the workspace.
    This lets us access the direct dataset from the workspace by passing the dataset
    ID to the `Dataset.get_by_id()` method.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Next, we write a run configuration to submit the preceding code as an experiment
    to Azure Machine Learning. First, we need to convert the dataset into a command-line
    argument and pass it to the training script so it can be automatically retrieved
    in the execution runtime. We can achieve this by using the `as_named_input(name)`
    method on the dataset instance, which will convert the dataset into a named `DatasetConsumptionConfig`
    argument that allows the dataset to be passed to other environments.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, the dataset will be passed in direct mode and provided as the
    `name` environment variable in the runtime environment or as the dataset ID in
    the command-line arguments. The dataset will also get tracked in Azure Machine
    Learning as an input argument to the training script.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as we saw in the previous code snippet, we use the `Dataset.get_by_id()`
    method to retrieve the dataset in the training script from the dataset ID. We
    don''t need to manually create or access the dataset ID, because the `DatasetConsumptionConfig`
    argument will be automatically expanded into the dataset ID when the training
    script is called by Azure Machine Learning with a direct dataset:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '**Access_data_from_dataset.ipynb**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As we can see in the preceding code, the dataset is converted to a configuration
    that can simply be passed to the training script through the `as_named_input(name)`
    method. If we submit the experiment and check the logs of the run, we can see
    that Azure Machine Learning passed the dataset ID to the training script:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 70_driver_log.txt
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The run details for this experiment are shown in *Figure 4.11*. If you look
    at the input arguments, you can see that we passed the `DatasetConsumptionConfig`
    object to the script, which was then converted automatically to the dataset ID.
    Not only is the input argument passed without any information about the location
    of the underlying data, but the input dataset is also recognized as an input to
    the training data:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Run details of the experiment ](img/B17928_04_11.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Run details of the experiment
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'By passing a dataset to a training script, Azure Machine Learning automatically
    tracks the dataset with the experiment run. As you can see in *Figure 4.11*, the
    dataset ID is a link to the tracked dataset. When clicking on the dataset ID in
    Azure Machine Learning, it will open a page showing details about the tracked
    dataset, such as description, URL, size, and type of dataset, as shown in *Figure
    4.12*. Like registered datasets, you can also explore the raw data and look at
    dataset column statistics – called the profile – or see any registered models
    derived from this data. Tracked datasets can easily be registered – and hence
    versioned and managed – by clicking on the **Register** action or from code:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Direct dataset tracked in Azure Machine Learning ](img/B17928_04_12.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Direct dataset tracked in Azure Machine Learning
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in this section, there are important benefits to passing the input
    data to your training script as a dataset argument. This will automatically track
    the dataset in your workspace and connect the dataset with the experimentation
    run.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: In the code snippets of this section, we passed the data as a *direct dataset*,
    which means that the training script has to fetch the data again from the external
    URL. This is not always optimal, especially when dealing with large amounts of
    data or when data should be managed in Azure Machine Learning. In the next section,
    we will explore different ways to pass data to the training script.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Accessing data during training
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we implicitly passed the URL of the original dataset
    to the training script. While this is a practical and fast solution for small
    public datasets, it's often not the preferred approach for private or larger datasets.
    Imagine your data is stored on a SQL server, Blob storage, or file share instead,
    and password protected. Imagine your dataset contains many gigabytes of files.
    In this section, we will see techniques that work well for both cases.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: While external public data reachable through a URL is created and passed as
    a *direct dataset*, all other datasets can be accessed either as a **download**
    or as a **mount**. For big data datasets, Azure Machine Learning also provides
    an option to mount a dataset as a **Hadoop Distributed File System** (**HDFS**).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will see authoring scripts that will pass datasets both
    as a download and as a mount. Let''s first create a reference in the authoring
    script to the `cifar10` dataset, which we registered in the previous section.
    The following snippet retrieves a dataset by name from the Azure Machine Learning
    workspace:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Next, we want to pass the dataset to the training script so that we can access
    the training data from the script. The benefit of using datasets is not only tracking
    but the fact that we can simply choose the appropriate data consumption configuration
    that is appropriate for each dataset. It will also help us to separate the training
    script from the training data, making it easy to pass new, updated, or enriched
    data to the same training script without needing to update the training script.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Independently of the consumption method, the training script can always load
    the data from a directory path where it will be either downloaded or mounted.
    Under the hood, Azure Machine Learning inspects the command-line arguments of
    `ScriptRunConfig`, detects the dataset reference, delivers the data to the compute
    environment, and replaces the argument with the path of the dataset in the local
    filesystem.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure Machine Learning uses parameter expansion to replace the dataset reference
    with the path to the actual data on disk. To make this more obvious, we will write
    a single training file that will simply list all training files that were passed
    to it. The following code snippet implements this training script:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: code/access_dataset.py
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the previous script, we define a single `--input` argument that we will use
    to pass the training data. Then we will output this argument and list all files
    from the directory. We will use this script to pass data with different mounting
    techniques and will see that the data will always be available in the folder.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Having the dataset reference and a simple training script, we can now look at
    a different `ScriptRunConfig` to pass the `cifar10` dataset using the different
    data consumption configurations. While the code is downloaded or mounted by Azure
    Machine Learning before the training script is invoked, we will also explore what
    happens under the hood – so we can apply the same technique to load the training
    data outside of Azure Machine Learning-managed compute environments.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Accessing data as a download
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will first look at downloading the data to the training instance. To do
    so, we will first create a `ScriptRunConfig` constructor in the authoring environment
    where we pass the data to `as_download()`. We will schedule a code snippet that
    will access and output the files passed to the script:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Access_dataset_as_download.ipynb
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Azure will interpolate the dataset passed by the `input` parameter and replace
    it with the location of the dataset on disk. The data will be automatically downloaded
    to the training environment if the dataset is passed with the `Dataset.as_download()`
    method.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run this script configuration, the `access_dataset.py` script will output
    the temporary location of the dataset, which was automatically downloaded to disk.
    You can replicate the exact same process in your authoring environment that Azure
    Machine Learning does under the hood. To do so, you can simply call the following:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Passing data as a download is convenient for small datasets or when using a
    large number of consumers that require a high throughput on the data. However,
    if you are dealing with large datasets, you can also pass them as a *mount* instead.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Accessing data as a mount
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we will mount the data on the training environment. To do
    so, we will again create a `ScriptRunConfig` constructor in the authoring environment
    and this time we invoke the `as_mount()`. We will schedule a code snippet that
    will access and output the files passed to the script:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Access_dataset_as_mount.ipynb
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you can see, the preceding example is very similar to the previous example
    where data was downloaded to disk. In fact, we are reusing the exact same scheduled
    script, `access_dataset.py`, which will output the location of the data on disk.
    However, in this example, the data is not downloaded to this location but mounted
    to the file path.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure Machine Learning will interpolate the dataset passed through the input
    argument with the mounted path on disk. Similar to the previous example, you can
    replicate what happens under the hood in Azure Machine Learning and mount the
    data from within your authoring environment:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As you can see in the previous snippet, the dataset is mounted and released
    using the mount context''s `start` and `stop` methods. You can also simplify the
    code snippet using Python''s `with` statement to automatically mount and unmount
    the data as shown in the following snippet:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Hence, depending on the use case, we have different options to pass a dataset
    reference to a scheduled script. Independent of the data transport, Azure Machine
    Learning will implement the correct method under the hood and interpolate the
    input arguments so that the training script doesn't need to know how a dataset
    was configured. For the executed script, the data is simply made available through
    a path in the filesystem.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Using external datasets with open datasets
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most effective methods to improve the prediction performance of any
    ML model is to add additional information to your training data. A common way
    to achieve this is by joining external datasets to the training data. A good indication
    to join external data is the availability of popular joining keys in your dataset,
    such as dates, locations, countries, and more.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: When you work with transactional data that contains dates, you can easily join
    external data to create additional features for the training dataset and hence
    improve prediction performance. Common derived features for dates are weekdays,
    weekends, time to or since weekends, holidays, time to or since holidays, sports
    events, concerts, and more. When dealing with country information, you can often
    join additional country-specific data, such as population data, economic data,
    sociological data, health data, labor data, and more. When dealing with geolocation,
    you can join distance to points of interest, weather data, traffic data, and more.
    Each of these additional datasets gives you additional insights and hence can
    boost your model's performance significantly.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Open Datasets is a service that provides access to curated datasets for the
    transportation, health and genomics, labor and economics, population, and safety,
    categories and common datasets that you can use to boost your model's performance.
    Let's look into three examples.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Before using a specific dataset for a commercial service, please make sure that
    your application is covered by the license. If in doubt, reach out to Microsoft
    via [aod@microsoft.com](mailto:aod@microsoft.com).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first example, we will investigate the dataset for *worldwide public
    holidays*. The data covers holidays in almost 40 countries or regions from 1970
    to 2099\. It is curated from Wikipedia and the `holidays` Python package. You
    can import them into your environment and access these holidays using the `opendatasets`
    library as shown in the following example:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As we see in the code, we can access the dataset from the `azureml-opendatasets`
    package and use it as an Azure Machine Learning dataset. This means we can return
    the pandas or Spark DataFrame for further processing.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular dataset is the *US population* by county for the years 2000
    and 2010\. It is broken down by gender and race and sourced from the United States
    Census Bureau:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Another example open dataset is the *Current Employment Statistics* of the
    United States, published by the US **Bureau of Labor Statistics** (**BLS**). It
    contains estimates of employment, hours, and earnings of workers on payrolls in
    the US:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As you saw in this section, Azure Open Datasets gives you a convenient option
    to access curated datasets in the form of Azure Machine Learning datasets right
    from within your Azure Machine Learning workspace. While the number of available
    datasets is still manageable, you can expect the number of available datasets
    to grow over time.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to manage data in Azure Machine Learning using
    datastores and datasets. We saw how to configure the default datastore that is
    responsible for storing all assets, logs, models, and more in Azure Machine Learning,
    as well as other services that can be used as datastores for different types of
    data.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: After creating an Azure Blob storage account and configuring it as a datastore
    in Azure Machine Learning, we saw different tools to ingest data into Azure, such
    as Azure Storage Explorer, Azure CLI, and AzCopy, as well as services optimized
    for data ingestion and transformation, Azure Data Factory and Azure Synapse Spark.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent section, we got our hands on datasets. We created file and
    tabular datasets and learned about direct and registered datasets. Datasets can
    be passed as a download or a mount to executed scripts, which will automatically
    track datasets in Azure Machine Learning.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned how to improve predication performance by joining third-party
    datasets from Azure Open Datasets to our machine learning process. In the next
    chapter, we will learn how to explore data by performing data analysis and visualization.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
