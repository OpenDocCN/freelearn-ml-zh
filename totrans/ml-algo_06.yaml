- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naive Bayes are a family of powerful and easy-to-train classifiers that determine
    the probability of an outcome given a set of conditions using Bayes' theorem.
    In other words, the conditional probabilities are inverted, so that the query
    can be expressed as a function of measurable quantities. The approach is simple,
    and the adjective "naive" has been attributed not because these algorithms are
    limited or less efficient, but because of a fundamental assumption about the causal
    factors that we're going to discuss. Naive Bayes are multi-purpose classifiers
    and it's easy to find their application in many different contexts; however, their
    performance is particularly good in all those situations where the probability
    of a class is determined by the probabilities of some causal factors. A good example
    is natural language processing, where a piece of text can be considered as a particular
    instance of a dictionary and the relative frequencies of all terms provide enough
    information to infer a belonging class. We're going to discuss these concepts
    in later chapters. In this one, our examples will be always generic to let the
    reader understand how to apply naive Bayes in various contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider two probabilistic events A and B. We can correlate the marginal
    probabilities *P(A)* and *P(B)* with the conditional probabilities *P(A|B)* and
    *P(B|A)* using the product rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5dc0d26e-1f50-4e36-810e-86d776378850.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering that the intersection is commutative, the first members are equal;
    so we can derive **Bayes'' theorem**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34364511-0ab8-4a58-b0ce-437b5d6a0c05.png)'
  prefs: []
  type: TYPE_IMG
- en: This formula has very deep philosophical implications and it's a fundamental
    element of statistical learning. First of all, let's consider the marginal probability
    *P(A)*; this is normally a value that determines how probable a target event is,
    such as *P(Spam)* or *P(Rain)*. As there are no other elements, this kind of probability
    is called **Apriori**, because it's often determined by mathematical considerations
    or simply by a frequency count. For example, imagine we want to implement a very
    simple spam filter and we've collected 100 emails. We know that 30 are spam and
    70 are regular. So we can say that *P(Spam)* = 0.3.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we''d like to evaluate using some criteria (for simplicity, let''s
    consider a single one), for example, email text is shorter than 50 characters*. *Therefore,
    our query becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92428044-51de-4f08-be62-88a4c4c67b25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first term is similar to *P(Spam)* because it''s the probability of spam
    given a certain condition. For this reason, it''s called **a posteriori** (in
    other words, it''s a probability that we can estimate after knowing some additional
    elements). On the right-hand side, we need to calculate the missing values, but
    it''s simple. Let''s suppose that 35 emails have text shorter than 50 characters,
    so *P(Text < 50 chars)* *= 0.35*. Looking only into our spam folder, we discover
    that only 25 spam emails have short text, so that *P(Text < 50 chars|Spam) = 25/30
    = 0.83*. The result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2e0edfe-4670-4c3d-a1e8-d583a1127ae8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, after receiving a very short email, there is a 71% probability that it''s
    spam. Now, we can understand the role of *P(Text < 50 chars|Spam)*; as we have
    actual data, we can measure how probable is our hypothesis given the query. In
    other words, we have defined a likelihood (compare this with logistic regression),
    which is a weight between the Apriori probability and the a posteriori one (the
    term in the denominator is less important because it works as a normalizing factor):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84a1e8b9-ae4e-4091-8ba2-a8de4241f2f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The normalization factor is often represented by the Greek letter alpha, so
    the formula becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af320552-6e08-4cf4-b2ff-230fd4d455b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The last step is considering the case when there are more concurrent conditions
    (this is more realistic in real-life problems):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1cd46b0a-20c6-4eb7-952c-de350e0b0d30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A common assumption is called **conditional independence** (in other words,
    the effects produced by every cause are independent of each other) and this allows
    us to write a simplified expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/422c5d68-3418-419b-a452-b67d76c1d3be.png)'
  prefs: []
  type: TYPE_IMG
- en: Naive Bayes classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A naive Bayes classifier is called so because it''s based on a naive condition,
    which implies the conditional independence of causes. This can seem very difficult
    to accept in many contexts where the probability of a particular feature is strictly
    correlated to another one. For example, in spam filtering, a text shorter than
    50 characters can increase the probability of the presence of an image, or if
    the domain has been already blacklisted for sending the same spam emails to million
    users, it''s likely to find particular keywords. In other words, the presence
    of a cause isn''t normally independent from the presence of other ones. However,
    in Zhang H., *The Optimality of Naive Bayes*, AAAI 1, no. 2 (2004): 3, the author
    showed that under particular conditions (not so rare to happen), different dependencies
    clears one another, and a naive Bayes classifier succeeds in achieving very high
    performances even if its naiveness is violated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f771e2c-796e-46bc-8850-19489ca0cbfb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Every feature vector, for simplicity, will be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2d6f8c8-9514-4d42-9cab-37316828c236.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need also a target dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c5cb9f4-8da1-4b55-8e4a-a48ec24e38ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, each *y* can belong to one of *P* different classes. Considering Bayes''
    theorem under conditional independence, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a9e8a86-54c2-4daa-a963-14fdbf7869cf.png)'
  prefs: []
  type: TYPE_IMG
- en: The values of the marginal Apriori probability *P(y)* and of the conditional
    probabilities *P(x[i]|y)* is obtained through a frequency count; therefore, given
    an input vector *x*, the predicted class is the one for which the a posteriori
    probability is maximum.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'scikit-learn implements three naive Bayes variants based on the same number
    of different probabilistic distributions: Bernoulli, multinomial, and Gaussian.
    The first one is a binary distribution, useful when a feature can be present or
    absent. The second one is a discrete distribution and is used whenever a feature
    must be represented by a whole number (for example, in natural language processing,
    it can be the frequency of a term), while the third is a continuous distribution
    characterized by its mean and variance.'
  prefs: []
  type: TYPE_NORMAL
- en: Bernoulli naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If `X` is random variable and is Bernoulli-distributed, it can assume only
    two values (for simplicity, let''s call them 0 and 1) and their probability is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/846f4f5d-fcc5-4a35-9343-9f31856c4cfb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To try this algorithm with scikit-learn, we''re going to generate a dummy dataset.
    Bernoulli naive Bayes expects binary feature vectors; however, the class `BernoulliNB` has
    a `binarize` parameter, which allows us to specify a threshold that will be used
    internally to transform the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We have generated the bidimensional dataset shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16dea9d7-5994-4309-9a14-e8b63c13ea42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have decided to use 0.0 as a binary threshold, so each point can be characterized
    by the quadrant where it''s located. Of course, this is a rational choice for
    our dataset, but Bernoulli naive Bayes is envisaged for binary feature vectors
    or continuous values, which can be precisely split with a predefined threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The score is rather good, but if we want to understand how the binary classifier
    worked, it''s useful to see how the data has been internally binarized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/012d8f2b-dcd6-47c6-ae6d-b28da747bb40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, checking the naive Bayes predictions, we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is exactly what we expected.
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A multinomial distribution is useful to model feature vectors where each value
    represents, for example, the number of occurrences of a term or its relative frequency.
    If the feature vectors have *n* elements and each of them can assume *k* different
    values with probability *p[k]*, then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a56c6bb0-c27d-48fb-877d-5c172b42624c.png)'
  prefs: []
  type: TYPE_IMG
- en: The conditional probabilities *P(x[i]|y)* are computed with a frequency count
    (which corresponds to applying a maximum likelihood approach), but in this case,
    it's important to consider the **alpha parameter** (called **Laplace smoothing
    factor**). Its default value is 1.0 and it prevents the model from setting null
    probabilities when the frequency is zero. It's possible to assign all non-negative
    values; however, larger values will assign higher probabilities to the missing
    features and this choice could alter the stability of the model. In our example,
    we're going to consider the default value of 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes, we''re going to use `DictVectorizer`, already analyzed in
    [Chapter 2 - Important Elements in Machine Learning](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml).
    There are automatic instruments to compute the frequencies of terms, but we''re
    going to discuss them later. Let''s consider only two records: the first one representing
    a city, and the second one, the countryside. Our dictionary contains hypothetical
    frequencies, as if the terms were extracted from a text description:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the term `''river''` is missing from the first set, so it''s useful
    to keep alpha equal to 1.0 to give it a small probability. The output classes
    are 1 for city and 0 for the countryside. Now we can train a `MultinomialNB` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To test the model, we create a dummy city with a river and a dummy countryside
    place without any river:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the prediction is correct. Later on, when discussing some elements
    of natural language processing, we're going to use multinomial naive Bayes for
    text classification with larger corpora. Even if a multinomial distribution is
    based on the number of occurrences, it can be used successfully with frequencies
    or more complex functions.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gaussian naive Bayes is useful when working with continuous values whose probabilities
    can be modeled using a Gaussian distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41e8549a-4c7b-4a3d-940b-db47c3a446a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The conditional probabilities *P(x[i]|y)* are also Gaussian distributed; therefore,
    it''s necessary to estimate the mean and variance of each of them using the maximum
    likelihood approach. This quite easy; in fact, considering the property of a Gaussian,
    we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9a2a040-43aa-4710-98ee-3954fa808d69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the *k* index refers to the samples in our dataset and *P(x[i]|y)*  is
    a Gaussian itself. By minimizing the inverse of this expression (in Russel S.,
    Norvig P., *Artificial Intelligence: A Modern Approach*, Pearson, there''s a complete
    analytical explanation), we get the mean and variance for each Gaussian associated
    with *P(x[i]|y)*, and the model is hence trained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, we compare Gaussian naive Bayes with logistic regression using
    the ROC curves. The dataset has 300 samples with two features. Each sample belongs
    to a single class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'A plot of the dataset is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71c833f1-52e5-4534-99b2-fd01f9f2fdbb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can train both models and generate the ROC curves (the `Y` scores for
    naive Bayes are obtained through the `predict_proba` method):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting ROC curves (generated in the same way shown in the previous chapter)
    are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/156d6f86-7911-4da7-8591-0909c279f5f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Naive Bayes'' performance is slightly better than logistic regression; however,
    the two classifiers have similar accuracy and **Area Under the Curve** (**AUC**).
    It''s interesting to compare the performances of Gaussian and multinomial naive
    Bayes with the MNIST digit dataset. Each sample (belonging to 10 classes) is an
    8 x 8 image encoded as an unsigned integer (0-255); therefore, even if each feature
    doesn''t represent an actual count, it can be considered as a sort of magnitude
    or frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Multinomial naive Bayes performs better than the Gaussian variant and the result
    is not really surprising. In fact, each sample can be thought of as a feature
    vector derived from a dictionary of 64 symbols. The value can be the count of
    each occurrence, so a multinomial distribution can better fit the data, while
    a Gaussian is slightly more limited by its mean and variance.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Russel S., Norvig P., *Artificial Intelligence: A Modern Approach*, Pearson'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang H., *The Optimality of Naive Bayes, AAAI 1*, no. 2 (2004): 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papoulis A., *Probability, Random Variables and Stochastic Processes*, McGraw-Hill
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we exposed the generic naive Bayes approach, starting from
    the Bayes' theorem and its intrinsic philosophy. The naiveness of such algorithms
    is due to the choice to assume all the causes to be conditional independent. This
    means that each contribution is the same in every combination and the presence
    of a specific cause cannot alter the probability of the other ones. This is not
    so often realistic; however, under some assumptions, it's possible to show that
    internal dependencies clear each other so that the resulting probability appears
    unaffected by their relations.
  prefs: []
  type: TYPE_NORMAL
- en: 'scikit-learn provides three naive Bayes implementations: Bernoulli, multinomial
    and Gaussian. The only difference between them is in the probability distribution
    adopted. The first one is a binary algorithm, particularly useful when a feature
    can be present or not. Multinomial assumes having feature vectors, where each
    element represents the number of times it appears (or, very often, its frequency).
    This technique is very efficient in natural language processing or whenever the
    samples are composed starting from a common dictionary. Gaussian, instead, is
    based on a continuous distribution and it''s suitable for more generic classification
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to introduce a new classification technique
    called **support vector machines**. These algorithms are very powerful for solving
    both linear and non-linear problems. They're often the first choice for more complex
    scenarios because, despite their efficiency, the internal dynamics are very simple
    and they can be trained in a very short time.
  prefs: []
  type: TYPE_NORMAL
