- en: Naive Bayes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Naive Bayes are a family of powerful and easy-to-train classifiers that determine
    the probability of an outcome given a set of conditions using Bayes' theorem.
    In other words, the conditional probabilities are inverted, so that the query
    can be expressed as a function of measurable quantities. The approach is simple,
    and the adjective "naive" has been attributed not because these algorithms are
    limited or less efficient, but because of a fundamental assumption about the causal
    factors that we're going to discuss. Naive Bayes are multi-purpose classifiers
    and it's easy to find their application in many different contexts; however, their
    performance is particularly good in all those situations where the probability
    of a class is determined by the probabilities of some causal factors. A good example
    is natural language processing, where a piece of text can be considered as a particular
    instance of a dictionary and the relative frequencies of all terms provide enough
    information to infer a belonging class. We're going to discuss these concepts
    in later chapters. In this one, our examples will be always generic to let the
    reader understand how to apply naive Bayes in various contexts.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一系列强大且易于训练的分类器，它们使用贝叶斯定理根据一组条件确定结果的可能性。换句话说，条件概率被反转，因此查询可以表示为可测量量的函数。这种方法很简单，形容词“朴素”之所以被赋予，并不是因为这些算法有限或效率较低，而是因为我们对因果因素的假设是基本的。朴素贝叶斯是多用途分类器，很容易在不同的环境中找到它们的应用；然而，它们的性能在所有那些由某些因果因素的概率确定类别的情境中尤其出色。一个很好的例子是自然语言处理，其中一段文本可以被视为字典的一个特定实例，所有术语的相对频率提供了足够的信息来推断所属类别。我们将在后面的章节中讨论这些概念。在这一章中，我们的例子将始终是通用的，以便让读者了解如何在各种环境中应用朴素贝叶斯。
- en: Bayes' theorem
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: 'Let''s consider two probabilistic events A and B. We can correlate the marginal
    probabilities *P(A)* and *P(B)* with the conditional probabilities *P(A|B)* and
    *P(B|A)* using the product rule:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑两个概率事件A和B。我们可以使用乘法规则将边缘概率 *P(A)* 和 *P(B)* 与条件概率 *P(A|B)* 和 *P(B|A)* 相关联：
- en: '![](img/5dc0d26e-1f50-4e36-810e-86d776378850.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5dc0d26e-1f50-4e36-810e-86d776378850.png)'
- en: 'Considering that the intersection is commutative, the first members are equal;
    so we can derive **Bayes'' theorem**:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到交集是可交换的，第一成员是相等的；因此，我们可以推导出**贝叶斯定理**：
- en: '![](img/34364511-0ab8-4a58-b0ce-437b5d6a0c05.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/34364511-0ab8-4a58-b0ce-437b5d6a0c05.png)'
- en: This formula has very deep philosophical implications and it's a fundamental
    element of statistical learning. First of all, let's consider the marginal probability
    *P(A)*; this is normally a value that determines how probable a target event is,
    such as *P(Spam)* or *P(Rain)*. As there are no other elements, this kind of probability
    is called **Apriori**, because it's often determined by mathematical considerations
    or simply by a frequency count. For example, imagine we want to implement a very
    simple spam filter and we've collected 100 emails. We know that 30 are spam and
    70 are regular. So we can say that *P(Spam)* = 0.3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式具有非常深刻的哲学意义，它是统计学习的基本元素。首先，让我们考虑边缘概率 *P(A)*；这通常是一个确定目标事件可能性的值，例如 *P(Spam)*
    或 *P(Rain)*。由于没有其他元素，这种概率被称为**先验概率**，因为它通常由数学考虑或简单地由频率计数确定。例如，想象我们想要实现一个非常简单的垃圾邮件过滤器，我们已经收集了100封电子邮件。我们知道其中30封是垃圾邮件，70封是常规邮件。因此，我们可以说
    *P(Spam)* = 0.3。
- en: 'However, we''d like to evaluate using some criteria (for simplicity, let''s
    consider a single one), for example, email text is shorter than 50 characters*. *Therefore,
    our query becomes:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们希望使用一些标准来评估（为了简单起见，让我们考虑一个），例如，电子邮件文本的长度少于50个字符*。因此，我们的查询变为：
- en: '![](img/92428044-51de-4f08-be62-88a4c4c67b25.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/92428044-51de-4f08-be62-88a4c4c67b25.png)'
- en: 'The first term is similar to *P(Spam)* because it''s the probability of spam
    given a certain condition. For this reason, it''s called **a posteriori** (in
    other words, it''s a probability that we can estimate after knowing some additional
    elements). On the right-hand side, we need to calculate the missing values, but
    it''s simple. Let''s suppose that 35 emails have text shorter than 50 characters,
    so *P(Text < 50 chars)* *= 0.35*. Looking only into our spam folder, we discover
    that only 25 spam emails have short text, so that *P(Text < 50 chars|Spam) = 25/30
    = 0.83*. The result is:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项类似于*P(Spam)*，因为它是在特定条件下垃圾邮件的概率。因此，它被称为**后验概率**（换句话说，它是在知道一些额外元素后我们可以估计的概率）。在等式的右侧，我们需要计算缺失的值，但这很简单。假设有35封邮件的文本长度小于50个字符，所以*P(Text
    < 50 chars)* = 0.35。仅查看我们的垃圾邮件文件夹，我们发现只有25封垃圾邮件的文本较短，因此*P(Text < 50 chars|Spam)
    = 25/30 = 0.83*。结果是：
- en: '![](img/d2e0edfe-4670-4c3d-a1e8-d583a1127ae8.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d2e0edfe-4670-4c3d-a1e8-d583a1127ae8.png)'
- en: 'So, after receiving a very short email, there is a 71% probability that it''s
    spam. Now, we can understand the role of *P(Text < 50 chars|Spam)*; as we have
    actual data, we can measure how probable is our hypothesis given the query. In
    other words, we have defined a likelihood (compare this with logistic regression),
    which is a weight between the Apriori probability and the a posteriori one (the
    term in the denominator is less important because it works as a normalizing factor):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，收到一封非常短的电子邮件后，有71%的概率它是垃圾邮件。现在，我们可以理解*P(Text < 50 chars|Spam)*的作用；因为我们有实际数据，我们可以测量在查询条件下我们的假设有多可能。换句话说，我们定义了一个似然（与逻辑回归进行比较），它是先验概率和后验概率（分母中的项不太重要，因为它作为一个常规化因子）之间的权重：
- en: '![](img/84a1e8b9-ae4e-4091-8ba2-a8de4241f2f6.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/84a1e8b9-ae4e-4091-8ba2-a8de4241f2f6.png)'
- en: 'The normalization factor is often represented by the Greek letter alpha, so
    the formula becomes:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 常规化因子通常用希腊字母α表示，因此公式变为：
- en: '![](img/af320552-6e08-4cf4-b2ff-230fd4d455b5.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/af320552-6e08-4cf4-b2ff-230fd4d455b5.png)'
- en: 'The last step is considering the case when there are more concurrent conditions
    (this is more realistic in real-life problems):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是考虑存在更多并发条件的情况（这在现实生活中的问题中更为现实）：
- en: '![](img/1cd46b0a-20c6-4eb7-952c-de350e0b0d30.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1cd46b0a-20c6-4eb7-952c-de350e0b0d30.png)'
- en: 'A common assumption is called **conditional independence** (in other words,
    the effects produced by every cause are independent of each other) and this allows
    us to write a simplified expression:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的假设被称为**条件独立性**（换句话说，每个原因产生的影响是相互独立的）并且这允许我们写出简化的表达式：
- en: '![](img/422c5d68-3418-419b-a452-b67d76c1d3be.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/422c5d68-3418-419b-a452-b67d76c1d3be.png)'
- en: Naive Bayes classifiers
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单贝叶斯分类器
- en: 'A naive Bayes classifier is called so because it''s based on a naive condition,
    which implies the conditional independence of causes. This can seem very difficult
    to accept in many contexts where the probability of a particular feature is strictly
    correlated to another one. For example, in spam filtering, a text shorter than
    50 characters can increase the probability of the presence of an image, or if
    the domain has been already blacklisted for sending the same spam emails to million
    users, it''s likely to find particular keywords. In other words, the presence
    of a cause isn''t normally independent from the presence of other ones. However,
    in Zhang H., *The Optimality of Naive Bayes*, AAAI 1, no. 2 (2004): 3, the author
    showed that under particular conditions (not so rare to happen), different dependencies
    clears one another, and a naive Bayes classifier succeeds in achieving very high
    performances even if its naiveness is violated.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 简单贝叶斯分类器之所以被称为简单贝叶斯，是因为它基于一个简单的条件，这暗示了原因之间的条件独立性。这在许多情况下可能很难接受，在这些情况下，特定特征的概率严格地与另一个特征相关。例如，在垃圾邮件过滤中，文本长度小于50个字符可能会增加图像出现的概率，或者如果域名已经被列入黑名单，用于向数百万用户发送相同的垃圾邮件，那么很可能找到特定的关键词。换句话说，原因的存在通常不是独立于其他原因的存在。然而，在张H.的《简单贝叶斯的最优性》，AAAI
    1，第2期（2004年）：3中，作者表明在特定条件下（并不罕见），不同的依赖关系相互消除，即使其简单性被违反，简单贝叶斯分类器也能成功实现非常高的性能。
- en: 'Let''s consider a dataset:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个数据集：
- en: '![](img/6f771e2c-796e-46bc-8850-19489ca0cbfb.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6f771e2c-796e-46bc-8850-19489ca0cbfb.png)'
- en: 'Every feature vector, for simplicity, will be represented as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，每个特征向量将表示为：
- en: '![](img/e2d6f8c8-9514-4d42-9cab-37316828c236.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e2d6f8c8-9514-4d42-9cab-37316828c236.png)'
- en: 'We need also a target dataset:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个目标数据集：
- en: '![](img/0c5cb9f4-8da1-4b55-8e4a-a48ec24e38ee.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0c5cb9f4-8da1-4b55-8e4a-a48ec24e38ee.png)'
- en: 'Here, each *y* can belong to one of *P* different classes. Considering Bayes''
    theorem under conditional independence, we can write:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个*y*可以属于*P*个不同类别中的一个。在条件独立下考虑贝叶斯定理，我们可以写出：
- en: '![](img/4a9e8a86-54c2-4daa-a963-14fdbf7869cf.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4a9e8a86-54c2-4daa-a963-14fdbf7869cf.png)'
- en: The values of the marginal Apriori probability *P(y)* and of the conditional
    probabilities *P(x[i]|y)* is obtained through a frequency count; therefore, given
    an input vector *x*, the predicted class is the one for which the a posteriori
    probability is maximum.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘先验概率*P(y)*和条件概率*P(x[i]|y)*的值是通过频率计数获得的；因此，给定一个输入向量*x*，预测的类别是后验概率最大的那个类别。
- en: Naive Bayes in scikit-learn
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: scikit-learn中的朴素贝叶斯
- en: 'scikit-learn implements three naive Bayes variants based on the same number
    of different probabilistic distributions: Bernoulli, multinomial, and Gaussian.
    The first one is a binary distribution, useful when a feature can be present or
    absent. The second one is a discrete distribution and is used whenever a feature
    must be represented by a whole number (for example, in natural language processing,
    it can be the frequency of a term), while the third is a continuous distribution
    characterized by its mean and variance.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn实现了基于相同数量的不同概率分布的三个朴素贝叶斯变体：伯努利、多项式和高斯。第一个是一个二元分布，当特征可以存在或不存在时很有用。第二个是一个离散分布，当必须用整数表示特征时使用（例如，在自然语言处理中，它可以是一个术语的频率），而第三个是一个连续分布，其特征是均值和方差。
- en: Bernoulli naive Bayes
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伯努利朴素贝叶斯
- en: 'If `X` is random variable and is Bernoulli-distributed, it can assume only
    two values (for simplicity, let''s call them 0 and 1) and their probability is:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`X`是伯努利分布的随机变量，它只能假设两个值（为了简单起见，让我们称它们为0和1），它们的概率是：
- en: '![](img/846f4f5d-fcc5-4a35-9343-9f31856c4cfb.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/846f4f5d-fcc5-4a35-9343-9f31856c4cfb.png)'
- en: 'To try this algorithm with scikit-learn, we''re going to generate a dummy dataset.
    Bernoulli naive Bayes expects binary feature vectors; however, the class `BernoulliNB` has
    a `binarize` parameter, which allows us to specify a threshold that will be used
    internally to transform the features:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用scikit-learn尝试此算法，我们将生成一个虚拟数据集。伯努利朴素贝叶斯期望二元特征向量；然而，`BernoulliNB`类有一个`binarize`参数，允许我们指定一个阈值，该阈值将内部用于将特征转换：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We have generated the bidimensional dataset shown in the following figure:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经生成了以下图中所示的二维数据集：
- en: '![](img/16dea9d7-5994-4309-9a14-e8b63c13ea42.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/16dea9d7-5994-4309-9a14-e8b63c13ea42.png)'
- en: 'We have decided to use 0.0 as a binary threshold, so each point can be characterized
    by the quadrant where it''s located. Of course, this is a rational choice for
    our dataset, but Bernoulli naive Bayes is envisaged for binary feature vectors
    or continuous values, which can be precisely split with a predefined threshold:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定使用0.0作为二元阈值，因此每个点都可以用其所在象限来表征。当然，这对我们的数据集来说是一个合理的选择，但伯努利朴素贝叶斯是为二元特征向量或可以精确分割的连续值（例如，使用预定义的阈值）而设计的：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The score is rather good, but if we want to understand how the binary classifier
    worked, it''s useful to see how the data has been internally binarized:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 分数相当不错，但如果我们想了解二元分类器是如何工作的，看到数据是如何内部二值化的很有用：
- en: '![](img/012d8f2b-dcd6-47c6-ae6d-b28da747bb40.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/012d8f2b-dcd6-47c6-ae6d-b28da747bb40.png)'
- en: 'Now, checking the naive Bayes predictions, we obtain:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，检查朴素贝叶斯预测，我们得到：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is exactly what we expected.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们预期的。
- en: Multinomial naive Bayes
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯
- en: 'A multinomial distribution is useful to model feature vectors where each value
    represents, for example, the number of occurrences of a term or its relative frequency.
    If the feature vectors have *n* elements and each of them can assume *k* different
    values with probability *p[k]*, then:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式分布对于建模特征向量很有用，其中每个值代表，例如，一个术语出现的次数或其相对频率。如果特征向量有*n*个元素，并且每个元素可以以概率*p[k]*假设*k*个不同的值，那么：
- en: '![](img/a56c6bb0-c27d-48fb-877d-5c172b42624c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a56c6bb0-c27d-48fb-877d-5c172b42624c.png)'
- en: The conditional probabilities *P(x[i]|y)* are computed with a frequency count
    (which corresponds to applying a maximum likelihood approach), but in this case,
    it's important to consider the **alpha parameter** (called **Laplace smoothing
    factor**). Its default value is 1.0 and it prevents the model from setting null
    probabilities when the frequency is zero. It's possible to assign all non-negative
    values; however, larger values will assign higher probabilities to the missing
    features and this choice could alter the stability of the model. In our example,
    we're going to consider the default value of 1.0.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 条件概率*P(x[i]|y)*通过频率计数（这相当于应用最大似然方法）来计算，但在这个情况下，考虑**alpha参数**（称为**拉普拉斯平滑因子**）是很重要的。它的默认值是1.0，它防止模型在频率为零时设置零概率。可以分配所有非负值；然而，较大的值将赋予缺失特征更高的概率，这种选择可能会改变模型的稳定性。在我们的例子中，我们将考虑默认值1.0。
- en: 'For our purposes, we''re going to use `DictVectorizer`, already analyzed in
    [Chapter 2 - Important Elements in Machine Learning](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml).
    There are automatic instruments to compute the frequencies of terms, but we''re
    going to discuss them later. Let''s consider only two records: the first one representing
    a city, and the second one, the countryside. Our dictionary contains hypothetical
    frequencies, as if the terms were extracted from a text description:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们将使用`DictVectorizer`，这在[第2章 - 机器学习中的重要元素](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml)中已经分析过。有自动工具可以计算术语的频率，但我们将在稍后讨论。让我们只考虑两个记录：第一个代表一个城市，第二个代表乡村。我们的字典包含假设频率，就像术语是从文本描述中提取出来的一样：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that the term `''river''` is missing from the first set, so it''s useful
    to keep alpha equal to 1.0 to give it a small probability. The output classes
    are 1 for city and 0 for the countryside. Now we can train a `MultinomialNB` instance:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，术语`'river'`在第一个集合中缺失，因此保持alpha等于1.0以给它一个小的概率是有用的。输出类别是1代表城市，0代表乡村。现在我们可以训练一个`MultinomialNB`实例：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To test the model, we create a dummy city with a river and a dummy countryside
    place without any river:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试模型，我们创建了一个有河流的虚拟城市和一个没有河流的虚拟乡村：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As expected, the prediction is correct. Later on, when discussing some elements
    of natural language processing, we're going to use multinomial naive Bayes for
    text classification with larger corpora. Even if a multinomial distribution is
    based on the number of occurrences, it can be used successfully with frequencies
    or more complex functions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，预测是正确的。稍后，当讨论自然语言处理的一些元素时，我们将使用多项式朴素贝叶斯进行文本分类，并使用更大的语料库。即使多项式分布基于发生次数，它也可以成功地与频率或更复杂的函数一起使用。
- en: Gaussian naive Bayes
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯朴素贝叶斯
- en: 'Gaussian naive Bayes is useful when working with continuous values whose probabilities
    can be modeled using a Gaussian distribution:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯朴素贝叶斯在处理可以用高斯分布建模的连续值时很有用：
- en: '![](img/41e8549a-4c7b-4a3d-940b-db47c3a446a1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41e8549a-4c7b-4a3d-940b-db47c3a446a1.png)'
- en: 'The conditional probabilities *P(x[i]|y)* are also Gaussian distributed; therefore,
    it''s necessary to estimate the mean and variance of each of them using the maximum
    likelihood approach. This quite easy; in fact, considering the property of a Gaussian,
    we get:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 条件概率*P(x[i]|y)*也是高斯分布的；因此，有必要使用最大似然方法估计每个条件概率的均值和方差。这相当简单；事实上，考虑到高斯性质，我们得到：
- en: '![](img/b9a2a040-43aa-4710-98ee-3954fa808d69.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9a2a040-43aa-4710-98ee-3954fa808d69.png)'
- en: 'Here, the *k* index refers to the samples in our dataset and *P(x[i]|y)*  is
    a Gaussian itself. By minimizing the inverse of this expression (in Russel S.,
    Norvig P., *Artificial Intelligence: A Modern Approach*, Pearson, there''s a complete
    analytical explanation), we get the mean and variance for each Gaussian associated
    with *P(x[i]|y)*, and the model is hence trained.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*k*索引指的是我们数据集中的样本，*P(x[i]|y)*本身就是一个高斯分布。通过最小化这个表达式的倒数（在Russel S.，Norvig
    P.，《人工智能：现代方法》，Pearson中有一个完整的分析解释），我们得到与*P(x[i]|y)*相关的每个高斯分布的均值和方差，因此模型得到了训练。
- en: 'As an example, we compare Gaussian naive Bayes with logistic regression using
    the ROC curves. The dataset has 300 samples with two features. Each sample belongs
    to a single class:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，我们使用ROC曲线比较高斯朴素贝叶斯与逻辑回归。数据集有300个样本，两个特征。每个样本属于一个单一类别：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A plot of the dataset is shown in the following figure:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的图示如下所示：
- en: '![](img/71c833f1-52e5-4534-99b2-fd01f9f2fdbb.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71c833f1-52e5-4534-99b2-fd01f9f2fdbb.png)'
- en: 'Now we can train both models and generate the ROC curves (the `Y` scores for
    naive Bayes are obtained through the `predict_proba` method):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以训练这两个模型并生成 ROC 曲线（朴素贝叶斯的 `Y` 分数是通过 `predict_proba` 方法获得的）：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting ROC curves (generated in the same way shown in the previous chapter)
    are shown in the following figure:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的 ROC 曲线（与前一章中显示的相同方式生成）如下所示：
- en: '![](img/156d6f86-7911-4da7-8591-0909c279f5f1.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/156d6f86-7911-4da7-8591-0909c279f5f1.png)'
- en: 'Naive Bayes'' performance is slightly better than logistic regression; however,
    the two classifiers have similar accuracy and **Area Under the Curve** (**AUC**).
    It''s interesting to compare the performances of Gaussian and multinomial naive
    Bayes with the MNIST digit dataset. Each sample (belonging to 10 classes) is an
    8 x 8 image encoded as an unsigned integer (0-255); therefore, even if each feature
    doesn''t represent an actual count, it can be considered as a sort of magnitude
    or frequency:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Naive Bayes 的性能略优于逻辑回归；然而，这两个分类器的准确率和 **曲线下面积**（**AUC**）相似。比较高斯和多项式朴素贝叶斯在 MNIST
    数字数据集上的性能很有趣。每个样本（属于10个类别之一）是一个 8 x 8 的图像，编码为无符号整数（0-255）；因此，即使每个特征不表示实际的计数，也可以将其视为一种大小或频率：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Multinomial naive Bayes performs better than the Gaussian variant and the result
    is not really surprising. In fact, each sample can be thought of as a feature
    vector derived from a dictionary of 64 symbols. The value can be the count of
    each occurrence, so a multinomial distribution can better fit the data, while
    a Gaussian is slightly more limited by its mean and variance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯比高斯变体表现更好，这个结果并不令人惊讶。实际上，每个样本可以被视为从64个符号的字典中派生出的特征向量。值可以是每个发生的计数，因此多项式分布可以更好地拟合数据，而高斯则稍微受限于其均值和方差。
- en: References
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Russel S., Norvig P., *Artificial Intelligence: A Modern Approach*, Pearson'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russell S., Norvig P., *《人工智能：现代方法》*，Pearson
- en: 'Zhang H., *The Optimality of Naive Bayes, AAAI 1*, no. 2 (2004): 3'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang H., *《朴素贝叶斯的最优性，AAAI 1》*，第2期（2004年）：3
- en: Papoulis A., *Probability, Random Variables and Stochastic Processes*, McGraw-Hill
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papoulis A., *《概率论、随机变量与随机过程》*，McGraw-Hill
- en: Summary
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we exposed the generic naive Bayes approach, starting from
    the Bayes' theorem and its intrinsic philosophy. The naiveness of such algorithms
    is due to the choice to assume all the causes to be conditional independent. This
    means that each contribution is the same in every combination and the presence
    of a specific cause cannot alter the probability of the other ones. This is not
    so often realistic; however, under some assumptions, it's possible to show that
    internal dependencies clear each other so that the resulting probability appears
    unaffected by their relations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从贝叶斯定理及其内在哲学出发，介绍了朴素贝叶斯的一般方法。这些算法的朴素性是由于选择假设所有原因都是条件独立的。这意味着每个贡献在每种组合中都是相同的，特定原因的存在不能改变其他原因的概率。这并不总是现实的；然而，在某些假设下，可以证明内部依赖会相互清除，从而使结果概率不受其关系的影响。
- en: 'scikit-learn provides three naive Bayes implementations: Bernoulli, multinomial
    and Gaussian. The only difference between them is in the probability distribution
    adopted. The first one is a binary algorithm, particularly useful when a feature
    can be present or not. Multinomial assumes having feature vectors, where each
    element represents the number of times it appears (or, very often, its frequency).
    This technique is very efficient in natural language processing or whenever the
    samples are composed starting from a common dictionary. Gaussian, instead, is
    based on a continuous distribution and it''s suitable for more generic classification
    tasks.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了三种朴素贝叶斯实现：伯努利、多项式和高斯。它们之间的唯一区别在于采用的概率分布。第一个是一个二元算法，特别适用于一个特征可以存在或不存在的情况。多项式假设有特征向量，其中每个元素代表其出现的次数（或，非常常见的是，其频率）。这种技术在自然语言处理或样本由一个共同的字典组成时非常有效。相反，高斯基于连续分布，适用于更通用的分类任务。
- en: In the next chapter, we're going to introduce a new classification technique
    called **support vector machines**. These algorithms are very powerful for solving
    both linear and non-linear problems. They're often the first choice for more complex
    scenarios because, despite their efficiency, the internal dynamics are very simple
    and they can be trained in a very short time.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一种名为**支持向量机**的新分类技术。这些算法在解决线性和非线性问题方面非常强大。由于它们效率高，内部动态非常简单，并且可以在非常短的时间内进行训练，因此它们通常被选为更复杂场景的首选。
