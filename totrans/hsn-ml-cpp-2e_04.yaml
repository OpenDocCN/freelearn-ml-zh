- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Clustering
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: '**Clustering** is an unsupervised machine learning method that’s used for splitting
    the original dataset of objects into groups classified by properties. In **machine
    learning**, an object is typically represented as a point in a multidimensional
    metric space. Every space dimension corresponds to an object property (feature),
    and the metric is a function of the values of these properties. Depending on the
    types of dimensions in this space, which can be both numerical and categorical,
    we choose a type of clustering algorithm and specific metric function. This choice
    depends on the nature of different object properties’ types.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**是一种无监督的机器学习方法，用于将原始数据集的对象分割成按属性分类的组。在**机器学习**中，一个对象通常被表示为多维度量空间中的一个点。每个空间维度对应一个对象属性（特征），度量是一个属性值的函数。根据这个空间中维度的类型，这些维度可以是数值的也可以是分类的，我们选择一种聚类算法和特定的度量函数。这种选择取决于不同对象属性类型的本质。'
- en: At the present stage, clustering is often used as the first step in data analysis.
    The task of clustering was formulated in scientific areas such as statistics,
    pattern recognition, optimization, and machine learning. At the time of writing,
    the number of methods for partitioning groups of objects into clusters is quite
    large—several dozen algorithms, and even more when you take into account their
    various modifications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前阶段，聚类通常被用作数据分析的第一步。聚类任务在诸如统计学、模式识别、优化和机器学习等科学领域被提出。在撰写本文时，将对象分组划分成聚类的数量方法相当庞大——几十种算法，甚至更多，当你考虑到它们的各种修改时。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Measuring distance in clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类中的距离度量
- en: Types of clustering algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类算法的类型
- en: Examples of using the `mlpack` library for dealing with the clustering task
    samples
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `mlpack` 库处理聚类任务样本的示例
- en: Examples of using the `Dlib` library for dealing with the clustering task samples
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `Dlib` 库处理聚类任务样本的示例
- en: Plotting data with C++
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 C++ 绘制数据
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You’ll require the following technologies and installations to complete this
    chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章需要以下技术和安装：
- en: A modern C++ compiler with C++17 support
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 C++17 的现代 C++ 编译器
- en: CMake build system version >= 3.8
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMake 构建系统版本 >= 3.8
- en: The `Dlib` library
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dlib` 库'
- en: The `mlpack` library
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlpack` 库'
- en: The `plotcpp` library
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plotcpp` 库'
- en: 'The code files for this chapter can be found in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在本书的 GitHub 仓库中找到：[https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04)。
- en: Measuring distance in clustering
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类中的距离度量
- en: A metric or distance measure is essential in clustering because it determines
    the similarity between objects. However, before applying a distance measure to
    objects, we must make a vector of object characteristics; usually, this is a set
    of numerical values, such as human height or weight. Also, some algorithms can
    work with categorical object features (or characteristics). The standard practice
    is to normalize feature values. Normalization ensures that each feature has the
    same impact in a distance measure calculation. Many distance measure functions
    can be used in the scope of the clustering task. The most popular ones that are
    used for numerical properties are **Euclidean distance**, **squared Euclidean
    distance**, **Manhattan distance**, and **Chebyshev distance**. The following
    subsections describe them in detail.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，度量或距离度量是必不可少的，因为它决定了对象之间的相似性。然而，在将距离度量应用于对象之前，我们必须制作一个对象特征的向量；通常，这是一个数值集合，如人类身高或体重。此外，一些算法可以处理分类对象特征（或特性）。标准做法是对特征值进行归一化。归一化确保每个特征在距离度量计算中具有相同的影响。在聚类任务范围内可以使用许多距离度量函数。最常用的用于数值属性的函数是**欧几里得距离**、**平方欧几里得距离**、**曼哈顿距离**和**切比雪夫距离**。以下小节将详细描述它们。
- en: Euclidean distance
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'Euclidean distance is the most widely used distance measure. In general, this
    is a geometric distance in the multidimensional space. The formula for Euclidean
    distance is as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离是最常用的距离度量。一般来说，这是一个多维空间中的几何距离。欧几里得距离的公式如下：
- en: '![](img/B19849_Formula_012.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_012.jpg)'
- en: Squared Euclidean distance
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平方欧几里得距离
- en: 'Squared Euclidean distance has the same properties as Euclidean distance but
    assigns greater significance (weight) to the distant values than to closer ones.
    Here’s the formula for squared Euclidean distance:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 平方欧几里得距离具有与欧几里得距离相同的属性，但相对于较近的值，它赋予较远的值更大的重要性（权重）。以下是平方欧几里得距离的公式：
- en: '![](img/B19849_Formula_021.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_021.jpg)'
- en: Manhattan distance
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 曼哈顿距离
- en: 'Manhattan distance is an average difference by coordinates. In most cases,
    its value gives the same clustering results as Euclidean distance. However, it
    reduces the significance (weight) of the distant values (outliers). Here’s the
    formula for Manhattan distance:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离是坐标的平均差异。在大多数情况下，它的值给出的聚类结果与欧几里得距离相同。然而，它降低了远距离值（异常值）的重要性（权重）。以下是曼哈顿距离的公式：
- en: '![](img/B19849_Formula_031.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_031.jpg)'
- en: Chebyshev distance
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 切比雪夫距离
- en: 'Chebyshev distance can be useful when we need to classify two objects as different
    when they differ only by one of the coordinates. Here’s the formula for Chebyshev
    distance:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个对象仅在坐标中的一个坐标上不同时，切比雪夫距离可能很有用。以下是切比雪夫距离的公式：
- en: '![](img/B19849_Formula_041.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_041.jpg)'
- en: 'The following diagram shows the differences between the various distances:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了各种距离之间的差异：
- en: '![Figure 4.1 – The difference between different distance measures](img/B19849_04_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 不同距离度量之间的差异](img/B19849_04_01.jpg)'
- en: Figure 4.1 – The difference between different distance measures
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 不同距离度量之间的差异
- en: Here, we can see that *Manhattan* distance is the sum of the distances in both
    dimensions, like walking along city blocks. *Euclidean* distance is just the length
    of a straight line. *Chebyshev* distance is a more flexible alternative to *Manhattan*
    distance because diagonal moves are also taken into account.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到曼哈顿距离是两个维度距离的总和，就像在城市街区中行走一样。欧几里得距离只是直线长度。切比雪夫距离是曼哈顿距离的一个更灵活的替代方案，因为斜对角移动也被考虑在内。
- en: In this section, we became familiar with the main clustering concept, which
    is a distance measure. In the following section, we’ll discuss various types of
    clustering algorithms.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们熟悉了主要的聚类概念，即距离度量。在下一节中，我们将讨论各种类型的聚类算法。
- en: Types of clustering algorithms
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法的类型
- en: 'There are different types of clustering that we can classify into the following
    groups: **partition-based**, **spectral**, **hierarchical**, **density-based**,
    and **model-based**. The partition-based group of clustering algorithms can be
    logically divided into distance-based methods and ones based on graph theory.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将不同的聚类类型分类为以下几组：**基于划分的**、**基于谱的**、**层次化的**、**基于密度的**和**基于模型的**。基于划分的聚类算法可以逻辑上分为基于距离的方法和基于图论的方法。
- en: Before we cover different types of clustering algorithms, let’s understand the
    main difference between clustering and classification. The main difference between
    the two is an undefined set of target groups, which is determined by the clustering
    algorithm. The set of target groups (clusters) is the algorithm’s result.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍不同的聚类算法之前，让我们了解聚类和分类之间的主要区别。这两者之间的主要区别是未定义的目标组集合，该集合由聚类算法确定。目标组（簇）集合是算法的结果。
- en: 'We can split cluster analysis into the following phases:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将聚类分析分为以下阶段：
- en: Selecting objects for clustering
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择聚类对象
- en: Determining the set of object properties that we’ll use for the metric
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定我们将用于度量的对象属性集合
- en: Normalizing property values
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化属性值
- en: Calculating the metric
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算度量
- en: Identifying distinct groups of objects based on metric values
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据度量值识别不同的对象组
- en: After analyzing clustering results, some correction may be required for the
    selected metric of the chosen algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 分析聚类结果后，可能需要对所选算法的度量进行一些修正。
- en: 'We can use clustering for various real-world tasks, including the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用聚类来完成各种现实世界的任务，包括以下内容：
- en: Splitting news into several categories for advertisers
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新闻分割成几个类别供广告商使用
- en: Identifying customer groups by their preferences for market analysis
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过市场分析识别客户群体
- en: Identifying plant and animal groups for biological studies
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别用于生物研究的植物和动物群体
- en: Identifying and categorizing properties for city planning and management
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别和分类城市规划和管理中的属性
- en: Detecting earthquake epicenter clusters to identify danger zones
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测地震震中簇以确定危险区域
- en: Categorizing groups of insurance policyholders for risk management
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对保险政策持有人群体进行分类以进行风险管理
- en: Categorizing books in libraries
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图书馆中的书籍进行分类
- en: Searching for hidden structural similarities in the data
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据中寻找隐藏的结构相似性
- en: With that, let’s dive into the different types of clustering algorithms.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，让我们深入了解不同的聚类算法类型。
- en: Partition-based clustering algorithms
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于划分的聚类算法
- en: The partition-based methods use a similarity measure to combine objects into
    groups. A practitioner usually selects the similarity measure for such kinds of
    algorithms, using prior knowledge about a problem or heuristics to select the
    measure properly. Sometimes, several measures need to be tried with the same algorithm
    so that the best one can be chosen. Also, partition-based methods usually require
    either the number of desired clusters or a threshold that regulates the number
    of output clusters to be specified explicitly. The choice of a similarity measure
    can significantly affect the quality and accuracy of the clusters produced, potentially
    leading to misinterpretations of data patterns and insights.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于划分的方法使用相似度度量来将对象组合成组。从业者通常根据对问题的先验知识或启发式方法选择此类算法的相似度度量。有时，需要尝试几种度量与同一算法一起使用，以便选择最佳度量。此外，基于划分的方法通常需要显式指定所需簇的数量或调节输出簇数量的阈值。相似度度量的选择可以显著影响生成的簇的质量和准确性，可能导致对数据模式和洞察力的误解。
- en: Distance-based clustering algorithms
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于距离的聚类算法
- en: The most known representatives of this family of methods are the k-means and
    k-medoids algorithms. They take the *k* input parameter and divide the data space
    into *k* clusters so that the similarity between objects in one cluster is maximal.
    Also, they minimize the similarity between objects of different clusters. The
    similarity value is calculated as the distance from the object to the cluster
    center. The main difference between these methods lies in the way the cluster
    center is defined.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这类方法中最著名的代表是k-means和k-medoids算法。它们接受*k*个输入参数，并将数据空间划分为*k*个簇，使得一个簇中对象的相似度最大。同时，它们最小化不同簇中对象的相似度。相似度值是对象到簇中心的距离。这些方法之间的主要区别在于簇中心定义的方式。
- en: With the k-means algorithm, the similarity is proportional to the distance to
    the cluster center of mass. The cluster center of mass is the average value of
    cluster objects’ coordinates in the data space. The k-means algorithm can be briefly
    described with a few steps. First, we select *k* random objects and define each
    of them as a cluster prototype that represents the cluster’s center of mass. Then,
    the remaining objects are attached to the cluster with greater similarity. After
    that, the center of mass of each cluster is recalculated. For each obtained partition,
    a particular evaluation function is calculated, the values of which at each step
    form a converging series. This process continues until the specified series converges
    to its limit value.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k-means算法，相似度与对象到簇质心的距离成正比。簇质心是簇对象在数据空间中坐标的平均值。k-means算法可以简要描述为以下几步。首先，我们选择*k*个随机对象，并将每个对象定义为代表簇质心的簇原型。然后，将剩余的对象附加到具有更高相似度的簇上。之后，重新计算每个簇的质心。对于每个获得的划分，计算一个特定的评估函数，其值在每个步骤形成一个收敛序列。这个过程一直持续到指定的序列收敛到其极限值。
- en: In other words, moving objects from one cluster to another ends when the clusters
    remain unchanged. Minimizing the evaluation function allows the resulting clusters
    to be as compact and separate as possible. The k-means method works well when
    clusters are compact *clouds* that are significantly separated from each other.
    It’s useful for processing large amounts of data but isn’t applicable for detecting
    clusters of non-convex shapes or clusters with very different sizes. Moreover,
    the method is susceptible to noise and isolated points since even a small number
    of such points can significantly affect how the center mass of the cluster is
    calculated.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当簇保持不变时，将物体从一个簇移动到另一个簇的过程就会结束。最小化评估函数可以使生成的簇尽可能紧凑且分离。当簇是彼此显著分离的紧凑“云”时，k-means方法效果很好。它适用于处理大量数据，但不适用于检测非凸形状的簇或大小差异很大的簇。此外，该方法容易受到噪声和孤立点的影响，因为即使少量这样的点也会显著影响簇质心的计算。
- en: To reduce the influence of noise and isolated points on the clustering result,
    the k-medoids algorithm, in contrast to the k-means algorithm, uses one of the
    cluster objects (known as the representative object) as the center of the cluster.
    As in the k-means method, *k* representative objects are selected at random. Each
    of the remaining objects is combined into a cluster with the nearest representative
    object. Then, each representative object is replaced iteratively with an arbitrary
    unrepresentative object from the data space. The replacement process continues
    until the quality of the resulting clusters improves. The clustering quality is
    determined by the sum of deviations between objects and the representative object
    of the corresponding cluster, which the method tries to minimize. Thus, the iterations
    continue until the representative object in each of the clusters becomes the medoid.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少噪声和孤立点对聚类结果的影响，与k-means算法不同，k-medoids算法使用簇中的一个对象（称为代表对象）作为簇的中心。与k-means方法一样，随机选择*k*个代表对象。每个剩余的对象都与最近的代表对象组合成一个簇。然后，每个代表对象通过迭代地用数据空间中的一个任意非代表对象替换。替换过程继续进行，直到结果簇的质量提高。聚类质量由对象与对应簇的代表对象之间的偏差之和决定，该方法试图最小化这个偏差。因此，迭代继续进行，直到每个簇中的代表对象成为中位数。
- en: The **medoid** is the object closest to the center of the cluster. The algorithm
    is poorly scalable for processing large amounts of data, but this problem is solved
    by the **Clustering Large Applications based on RANdomized Search** (**CLARANS**)
    algorithm, which complements the k-medoids method. CLARANS attempts to address
    scalability issues by using a randomized search technique to find good solutions
    more efficiently. Such an approach makes it possible to quickly converge on a
    good solution without exhaustively searching all possible combinations of medoids.
    For multidimensional clustering, the **Projected Clustering** (**PROCLUS**) algorithm
    can be used.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**中位数**是距离簇中心最近的对象。该算法在处理大量数据时扩展性较差，但**基于随机搜索的聚类大应用**（**CLARANS**）算法解决了这个问题，它补充了k-medoids方法。CLARANS通过使用随机搜索技术来更有效地找到好的解决方案，试图解决可扩展性问题。这种方法使得能够快速收敛到一个好的解决方案，而无需搜索所有可能的medoids组合。对于多维聚类，可以使用**投影聚类**（**PROCLUS**）算法。'
- en: Graph theory-based clustering algorithms
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于图论的聚类算法
- en: The essence of algorithms based on graph theory is to represent target objects
    in graph form. Graph vertices correspond to objects, and the edge weights are
    equal to the distance between vertices. The advantages of graph clustering algorithms
    are their excellent visibility, relative ease of implementation, and their ability
    to make various improvements based on geometrical considerations. The main graph
    theory concepts used for clustering are selecting connected components, constructing
    a minimum spanning tree, and multilayer graph clustering.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图论的算法本质在于将目标对象以图的形式表示。图顶点对应于对象，边权重等于顶点间的距离。图聚类算法的优点在于其卓越的可视性、相对容易的实现以及基于几何考虑的各种改进能力。用于聚类的图论主要概念包括选择连通分量、构建最小生成树和多层图聚类。
- en: The algorithm for selecting connected components is based on the *R* input parameter,
    and the algorithm removes all edges in the graph with distances greater than *R*.
    Only the closest pairs of objects remain connected. The algorithm’s goal is to
    find the *R* value at which the graph collapses into several connected components.
    The resulting components are clusters. To select the *R* parameter, a histogram
    of the distribution of pairwise distances is usually constructed. For problems
    with a well-defined cluster data structure, there will be two peaks in the histogram—one
    corresponds to in-cluster distances and the second to inter-cluster distances.
    The *R* parameter is selected from the minimum zone between these peaks. Managing
    the number of clusters using the distance threshold can be difficult.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 选择连通分量的算法基于*R*输入参数，该算法移除图中距离大于*R*的所有边。只有最近的成对对象保持连接。算法的目标是找到使图塌缩为几个连通分量的*R*值。结果形成的分量即为聚类。为了选择*R*参数，通常构建成对距离分布的直方图。对于具有明确定义聚类数据结构的问题，直方图中将出现两个峰值——一个对应于簇内距离，另一个对应于簇间距离。*R*参数通常从这两个峰值之间的最小区域中选择。使用距离阈值管理簇的数量可能会很困难。
- en: 'The minimum spanning tree algorithm builds a minimal spanning tree on the graph,
    and then successively removes the edges with the highest weight. The following
    diagram shows the minimum spanning tree that’s been obtained for nine objects:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最小生成树算法在图上构建最小生成树，然后依次移除权重最高的边。以下图表显示了九个对象的最小生成树：
- en: '![Figure 4.2 – Spanning tree example](img/B19849_04_02.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 扩展树示例](img/B19849_04_02.jpg)'
- en: Figure 4.2 – Spanning tree example
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 扩展树示例
- en: 'By removing the link between *C* and *D*, with a length of 6 units (the edge
    with the maximum distance), we obtain two clusters: *{A, B, C}* and *{D, E, F,
    G, H, I}*. We can divide the second cluster into two more clusters by removing
    the *EF* edge, which has a length of 4 units.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过移除 *C* 和 *D* 之间的连接，长度为 6 个单位（最大距离的边），我们得到两个簇：*{A, B, C}* 和 *{D, E, F, G, H,
    I}*. 通过移除长度为 4 个单位的 *EF* 边，我们可以将第二个簇进一步划分为两个簇。
- en: The multilayer clustering algorithm is based on identifying connected components
    of a graph at some level of distance between objects (vertices). The threshold,
    *C,* defines the distance level—for example, if the distance between objects is
    ![](img/B19849_Formula_051.png), then ![](img/B19849_Formula_061.png).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 多层聚类算法基于在对象（顶点）之间某个距离级别上识别图的连通分量。阈值 *C* 定义了距离级别——例如，如果对象之间的距离是 ![](img/B19849_Formula_051.png)，则
    ![](img/B19849_Formula_061.png)。
- en: 'The layer clustering algorithm generates a sequence of sub-graphs of the graph,
    *G*, that reflect the hierarchical relationships between clusters, ![](img/B19849_Formula_071.png),
    where the following applies:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法生成图 *G* 的子图序列，这些子图反映了簇之间的层次关系，![](img/B19849_Formula_071.png)，其中以下适用：
- en: '![](img/B19849_Formula_081.png): A sub-graph on the ![](img/B19849_Formula_09.png)level'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B19849_Formula_081.png)：在 ![](img/B19849_Formula_09.png) 级别的子图'
- en: '![](img/B19849_Formula_101.png)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_101.png)'
- en: '![](img/B19849_Formula_112.png): The *t*th threshold of distance'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B19849_Formula_112.png)：距离的第 *t* 个阈值'
- en: '![](img/B19849_Formula_123.png): The number of hierarchy levels'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B19849_Formula_123.png)：层次级别的数量'
- en: '![](img/B19849_Formula_131.png), o: An empty set of graph edges, when ![](img/B19849_Formula_142.png)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_131.png)，o：当 ![](img/B19849_Formula_142.png) 时，一个空的图边集'
- en: '![](img/B19849_Formula_151.png): A graph of objects without thresholds on distance,
    when ![](img/B19849_Formula_162.png)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_151.png)：当 ![](img/B19849_Formula_162.png) 时，没有距离阈值的对象图'
- en: By changing the ![](img/B19849_Formula_171.png) distance thresholds, where ![](img/B19849_Formula_181.png),
    it’s possible to control the hierarchy depth of the resulting clusters. Thus,
    a multilayer clustering algorithm can create both flat and hierarchical data partitioning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变 ![](img/B19849_Formula_171.png) 距离阈值，其中 ![](img/B19849_Formula_181.png)，可以控制结果簇的层次深度。因此，多层聚类算法可以创建既平坦又分层的分区数据。
- en: Spectral clustering algorithms
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谱聚类算法
- en: Spectral clustering refers to all methods that divide a set of data into clusters
    using the eigenvectors of the adjacency matrix of a graph or other matrices derived
    from it. An adjacency matrix describes a complete graph with vertices in objects
    and edges between each pair of objects with a weight corresponding to the degree
    of similarity between these vertices. Spectral clustering involves transforming
    the initial set of objects into a set of points in space whose coordinates are
    elements of eigenvectors. The formal name for such a task is the **normalized**
    **cuts problem**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类指的是所有使用图邻接矩阵或由此派生的其他矩阵的特征向量将数据集划分为簇的方法。邻接矩阵描述了一个完全图，其中对象是顶点，每对对象之间的边具有与这些顶点之间相似度对应的权重。谱聚类涉及将初始对象集转换为空间中的一系列点，其坐标是特征向量的元素。此类任务的正式名称是
    **归一化** **切割问题**。
- en: The resulting set of points is then clustered using standard methods—for example,
    with the k-means algorithm. Changing the representation created by eigenvectors
    allows us to set the properties of the original set of clusters more clearly.
    Thus, spectral clustering can separate points that can’t be separated by applying
    k-means—for example, when the k-means method gets a convex set of points. The
    main disadvantage of spectral clustering is its cubic computational complexity
    and quadratic memory requirements.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用标准方法对得到的点集进行聚类——例如，使用k均值算法。改变由特征向量创建的表示，我们可以更清楚地设置原始簇集的性质。因此，谱聚类可以分离k均值方法无法分离的点——例如，当k均值方法得到一个凸点集时。谱聚类的主要缺点是其立方计算复杂度和二次内存需求。
- en: Hierarchical clustering algorithms
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次聚类算法
- en: 'Among the algorithms of hierarchical clustering, there are two main types:
    **bottom-up** and **top-down-based algorithms**. Top-down algorithms work on the
    principle that at the beginning, all objects are placed in one cluster, which
    is then divided into smaller and smaller clusters. Bottom-up algorithms are more
    common than top-down ones. They place each object in a separate cluster at the
    beginning of the work and then merge clusters into larger ones until all the objects
    in the dataset are contained in one cluster, building a system of nested partitions.
    The results of such algorithms are usually presented in tree form, called a **dendrogram**.
    A classic example of such a tree is the *Tree of Life*, which describes the classification
    of animals and plants.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在层次聚类算法中，有两种主要类型：**自底向上**和**基于自顶向下的算法**。自顶向下算法基于的原则是，最初，所有对象都放置在一个簇中，然后将其分割成越来越小的簇。自底向上算法比自顶向下算法更常见。它们在工作的开始时将每个对象放置在一个单独的簇中，然后合并簇以形成更大的簇，直到数据集中的所有对象都包含在一个簇中，构建一个嵌套分区系统。这些算法的结果通常以树形形式呈现，称为**树状图**。此类树的经典例子是*生命之树*，它描述了动物和植物的分类。
- en: The main problem with hierarchical methods is the difficulty of determining
    the stop condition in such a way as to isolate natural clusters and, at the same
    time, prevent their excessive splitting. Another problem with hierarchical clustering
    methods is choosing the point of separation or merging of clusters. This choice
    is critical because after splitting or merging clusters at each subsequent step,
    the method will operate only on newly formed clusters. Therefore, the wrong choice
    of a merge or split point at any step can lead to poor-quality clustering. Also,
    hierarchical methods can’t be applied to large datasets because deciding whether
    to divide or merge clusters requires a large number of objects and clusters to
    be analyzed, which leads to a significant computational complexity of the method.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类方法的主要问题是难以确定停止条件，以便隔离自然簇并防止其过度分裂。层次聚类方法的另一个问题是选择簇的分离或合并点。这个选择至关重要，因为在每个后续步骤中分裂或合并簇之后，该方法将仅对新形成的簇进行操作。因此，在任何步骤中错误地选择合并或分裂点可能导致聚类质量较差。此外，由于决定是否分割或合并簇需要分析大量对象和簇，因此层次聚类方法不能应用于大数据集，这导致该方法具有显著的计算复杂性。
- en: 'There are several metrics or linkage criteria for cluster union that are used
    in hierarchical clustering methods:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类方法中用于簇合并的几个度量或连接标准：
- en: '**Single linkage (nearest neighbor distance)**: In this method, the distance
    between the two clusters is determined by the distance between the two closest
    objects (nearest neighbors) in different clusters. The resulting clusters tend
    to chain together.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单连接（最近邻距离）**：在此方法中，两个簇之间的距离由不同簇中两个最近的对象（最近邻）之间的距离确定。得到的簇倾向于链状连接。'
- en: '**Complete linkage (distance between the most distant neighbors)**: In this
    method, the distances between clusters are determined by the largest distance
    between any two objects in different clusters (that is, the most distant neighbors).
    This method usually works very well when objects come from separate groups. If
    the clusters are elongated or their natural type is *chained*, then this method
    is unsuitable.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全连接（最远邻居之间的距离）**：在此方法中，簇之间的距离由不同簇中任意两个对象之间的最大距离（即最远邻居）确定。当对象来自不同的组时，此方法通常工作得非常好。如果簇是细长的或其自然类型是*链状*，则此方法不适用。'
- en: '**Unweighted pairwise mean linkage**: In this method, the distance between
    two different clusters is calculated as the average distance between all pairs
    of objects in them. This method is useful when objects form different groups,
    but it works equally well in the case of elongated (chained-type) clusters.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未加权成对平均链接法**：在此方法中，两个不同簇之间的距离被计算为它们所有对象对之间的平均距离。当对象形成不同的组时，此方法很有用，但在长形（链式）簇的情况下也工作得同样好。'
- en: '**Weighted pairwise mean linkage**: This method is identical to the unweighted
    pairwise mean method, except that the size of the corresponding clusters (the
    number of objects contained in them) is used as a weighting factor in the calculations.
    Therefore, this method should be used when we assume unequal cluster sizes.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权成对平均链接法**：这种方法与未加权的成对平均方法相同，只是在计算中使用了相应簇的大小（包含在其中的对象数量）作为权重因子。因此，当我们假设簇的大小不相等时，应使用此方法。'
- en: '**Weighted centroid linkage**: In this method, the distance between two clusters
    is defined as the distance between their centers of mass.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权质心链接法**：在此方法中，两个簇之间的距离定义为它们质心之间的距离。'
- en: '**Weighted centroid linkage** (**median**): This method is identical to the
    previous one, except that the calculations use weights for the distance measured
    between cluster sizes. Therefore, if there are significant differences in-cluster
    sizes, this method is preferable to the previous one.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权质心链接法（中位数）**：这种方法与前面的一种相同，只是在计算中使用了测量簇大小之间的距离的权重。因此，如果簇的大小存在显著差异，此方法比前一种方法更可取。'
- en: 'The following diagram displays a hierarchical clustering dendrogram:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个层次聚类的树状图：
- en: '![Figure 4.3 – Hierarchical clustering example](img/B19849_04_03.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – 层次聚类示例](img/B19849_04_03.jpg)'
- en: Figure 4.3 – Hierarchical clustering example
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 层次聚类示例
- en: The preceding diagram shows an example of a dendrogram for hierarchical clustering,
    where you can see how the number of clusters depends on the distance between objects.
    Larger distances lead to a smaller number of clusters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了层次聚类的树状图示例，你可以看到簇的数量如何取决于对象之间的距离。较大的距离会导致簇的数量减少。
- en: Density-based clustering algorithms
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于密度的聚类算法
- en: In density-based methods, clusters are considered as regions where the multiple
    objects’ density is high. This is separated by regions with a low density of objects.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于密度的方法中，簇被认为是多个对象密度高的区域。这是通过具有低密度对象区域来分隔的。
- en: 'The **Density-Based Spatial Clustering of Applications with Noise** (**DBSCAN**)
    algorithm is one of the first density clustering algorithms to be created. The
    basis of this algorithm is several statements, detailed as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于密度的空间聚类应用带噪声（DBSCAN**）算法是第一个创建的密度聚类算法之一。该算法的基础是几个陈述，详细说明如下：'
- en: The ![](img/B19849_Formula_191.png) property of an object is the ![](img/B19849_Formula_20.png)
    radius neighborhood area around the object.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象的 ![](img/B19849_Formula_191.png) 属性是围绕对象的 ![](img/B19849_Formula_20.png)
    半径邻域区域。
- en: The root object is an object whose ![](img/B19849_Formula_211.png) contains
    a minimum non-zero number of objects. Assume that this minimum number equals a
    predefined value named *MinPts*.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根对象是 ![](img/B19849_Formula_211.png) 包含最小非零对象数量的对象。假设这个最小数量等于一个预定义的值，称为 *MinPts*。
- en: The *p* object is directly densely accessible from the *q* object if *p* is
    in the ![](img/B19849_Formula_221.png) property of *q* and *q* is the root object.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *p* 在 *q* 的 ![](img/B19849_Formula_221.png) 属性中，并且 *q* 是根对象，则 *p* 对象可以直接从
    *q* 对象中密集访问。
- en: The *p* object is densely accessible from the *q* object for the given ![](img/B19849_Formula_23.png)
    and *MinPts* if there’s a sequence of ![](img/B19849_Formula_24.png) objects,
    where ![](img/B19849_Formula_25.png) and ![](img/B19849_Formula_261.png), such
    that ![](img/B19849_Formula_271.png) is directly densely accessible from ![](img/B19849_Formula_282.png),
    ![](img/B19849_Formula_29.png).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果存在一个 ![](img/B19849_Formula_24.png) 对象的序列，其中 ![](img/B19849_Formula_25.png)
    和 ![](img/B19849_Formula_261.png)，使得 ![](img/B19849_Formula_271.png) 可以直接从 ![](img/B19849_Formula_282.png)、![](img/B19849_Formula_29.png)
    中密集访问，则对于给定的 ![](img/B19849_Formula_23.png) 和 *MinPts*，*p* 对象可以从 *q* 对象中密集访问。
- en: The *p* object is densely connected to the *q* object for the given ![](img/B19849_Formula_301.png)
    and *MinPts* if there’s an *o* object such that *p* and *q* are densely accessible
    from *o*.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定的 ![](img/B19849_Formula_301.png) 和 *MinPts*，如果存在一个 *o* 对象，使得 *p* 和 *q*
    都可以从 *o* 中密集访问，则 *p* 对象与 *q* 对象是密集连接的。
- en: The DBSCAN algorithm checks the neighborhood of each object to search for clusters.
    If the ![](img/B19849_Formula_311.png) property of the *p* object contains more
    points than *MinPts*, then a new cluster is created with the *p* object as a root
    object. DBSCAN then iteratively collects objects directly densely accessible from
    root objects, which can lead to the union of several densely accessible clusters.
    The process ends when no new objects can be added to any cluster.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 算法检查每个对象的邻域以寻找聚类。如果 *p* 对象的 ![](img/B19849_Formula_311.png) 属性包含比 *MinPts*
    更多的点，则创建一个新的聚类，以 *p* 对象作为根对象。然后 DBSCAN 递归地收集直接从根对象可访问的对象，这可能导致几个密集可访问聚类的合并。当无法向任何聚类添加新对象时，过程结束。
- en: Unlike the partition-based methods, DBSCAN doesn’t require the number of clusters
    to be specified in advance; it only requires the ![](img/B19849_Formula_321.png)
    and *MinPts* values as these parameters directly affect the result of clustering.
    The optimal values of these parameters are difficult to determine, especially
    for multidimensional data spaces. Also, the distributed data in such spaces is
    often asymmetrical, which makes it impossible to use global density parameters
    for their clustering. For clustering multidimensional data spaces, there’s the
    **Subspace Clustering** (**SUBCLU**) algorithm, which is based on the DBSCAN algorithm.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于划分的方法不同，DBSCAN 不需要预先指定聚类数量；它只需要 ![](img/B19849_Formula_321.png) 和 *MinPts*
    值，因为这些参数直接影响聚类的结果。这些参数的最佳值很难确定，尤其是在多维数据空间中。此外，此类空间中的分布式数据通常是不对称的，这使得无法使用全局密度参数进行聚类。对于聚类多维数据空间，有基于
    DBSCAN 算法的 **子空间聚类**（**SUBCLU**）算法。
- en: The **MeanShift** approach also falls into the category of density-based clustering
    algorithms. It’s a non-parametric algorithm that shifts dataset points toward
    the center of the highest-density region within a certain radius. The algorithm
    makes such shifts iteratively until points converge to a local maximum of the
    density function. Such local maxima are also called the mode, so the algorithm
    is sometimes called mode-seeking. These local maximums represent the cluster centroids
    in the dataset.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**MeanShift** 方法也属于基于密度的聚类算法类别。它是一种非参数算法，将数据集点向一定半径内最高密度区域的中心移动。算法通过迭代进行这样的移动，直到点收敛到密度函数的局部最大值。这些局部最大值也被称为模式，因此该算法有时被称为模式寻找算法。这些局部最大值代表了数据集中的聚类中心。'
- en: Model-based clustering algorithms
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于模型的聚类算法
- en: Model-based algorithms assume that there’s a particular mathematical model of
    the cluster in the data space and try to maximize the likelihood of this model
    and the data available. Often, this uses the apparatus of mathematical statistics.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的算法假设数据空间中存在特定的聚类数学模型，并试图最大化该模型和可用数据的似然性。通常，这使用数学统计学的工具。
- en: The **Expectation-Maximization** (**EM**) algorithm assumes that the dataset
    can be modeled using a linear combination of multidimensional normal distributions.
    Its purpose is to estimate distribution parameters that maximize the likelihood
    function that’s used as a measure of model quality. In other words, it assumes
    that the data in each cluster obeys a particular distribution law—namely, the
    normal distribution. With this assumption, it’s possible to determine the optimal
    parameters of the distribution law—the mean and variance at which the likelihood
    function is maximal. Thus, we can assume that any object belongs to all clusters,
    but with a different probability. In this instance, the task will be to fit the
    set of distributions to the data and determine the probabilities of the object
    belonging to each cluster. The object should be assigned to the cluster for which
    this probability is higher than the others.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望最大化**（**EM**）算法假设数据集可以使用多维正态分布的线性组合进行建模。其目的是估计最大化似然函数的分布参数，该函数用作模型质量的度量。换句话说，它假设每个聚类中的数据遵循特定的分布定律——即正态分布。基于这个假设，可以确定分布定律的最佳参数——似然函数最大的均值和方差。因此，我们可以假设任何对象属于所有聚类，但概率不同。在这种情况下，任务将是将分布集拟合到数据中，并确定对象属于每个聚类的概率。对象应分配到概率高于其他聚类的聚类。'
- en: The EM algorithm is simple and easy to implement. It isn’t sensitive to isolated
    objects and quickly converges in the case of successful initialization. However,
    it requires us to specify *k* number of clusters, which implies a *priori* knowledge
    about the data. Also, if the initialization fails, the algorithm may be slow to
    converge, or we might obtain a poor-quality result. Such algorithms don’t apply
    to high-dimensionality spaces since, in this case, it’s complicated to assume
    a mathematical model for distributing data in this space.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: EM算法简单易实现。它对孤立对象不敏感，在成功初始化的情况下可以快速收敛。然而，它要求我们指定*簇数*，这暗示了对数据的先验知识。此外，如果初始化失败，算法可能收敛缓慢，或者我们可能会得到一个质量较差的结果。这类算法不适用于高维空间，因为在这种情况下，假设该空间中数据分布的数学模型是复杂的。
- en: 'Now that we understand the various types of clustering algorithms, let’s look
    at their uses in many industries to group similar data points into clusters. Here
    are some examples of how clustering algorithms can be applied:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了各种聚类算法的类型，让我们看看它们在许多行业中的应用，以将相似的数据点分组到簇中。以下是一些聚类算法如何应用的例子：
- en: '**Customer segmentation**: Clustering algorithms can be used to segment customers
    based on their purchase history, demographics, and other attributes. This information
    can then be used for targeted marketing campaigns, personalized product recommendations,
    and customer service.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户细分**：聚类算法可以根据客户的购买历史、人口统计信息和其他属性对客户进行细分。然后，这些信息可以用于定向营销活动、个性化产品推荐和客户服务。'
- en: '**Image recognition**: In the field of computer vision, clustering algorithms
    are used to group images based on visual features such as color, texture, and
    shape. This can be useful for image classification, object detection, and scene
    understanding.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像识别**：在计算机视觉领域，聚类算法根据视觉特征（如颜色、纹理和形状）对图像进行分组。这可以用于图像分类、目标检测和场景理解。'
- en: '**Fraud detection**: In finance, clustering algorithms can detect suspicious
    transactions by grouping them based on similarities in transaction patterns. This
    helps to identify potential fraud and prevent financial losses.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈检测**：在金融领域，聚类算法可以通过根据交易模式中的相似性对交易进行分组来检测可疑交易。这有助于识别潜在的欺诈行为并防止财务损失。'
- en: '**Recommender systems**: In e-commerce, clustering algorithms group products
    based on customer preferences. This allows recommender systems to suggest relevant
    products to customers, increasing sales and customer satisfaction.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐系统**：在电子商务中，聚类算法根据客户偏好对产品进行分组。这使得推荐系统可以向客户推荐相关产品，从而增加销售额和客户满意度。'
- en: '**Social network analysis**: In social media, clustering algorithms identify
    groups of users with similar interests or behaviors. This enables targeted advertising,
    content creation, and community building.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交网络分析**：在社交媒体中，聚类算法识别具有相似兴趣或行为的用户组。这使得定向广告、内容创作和社区建设成为可能。'
- en: '**Genomics**: In biology, clustering algorithms analyze gene expression data
    to identify groups of genes that are co-expressed under specific conditions. This
    aids in understanding gene function and disease mechanisms.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基因组学**：在生物学中，聚类算法分析基因表达数据以识别在特定条件下共同表达的基因组。这有助于理解基因功能和疾病机制。'
- en: '**Text mining**: In natural language processing, clustering algorithms categorize
    documents based on their content. This is useful for topic modeling, document
    classification, and information retrieval.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本挖掘**：在自然语言处理中，聚类算法根据文档的内容对文档进行分类。这对于主题建模、文档分类和信息检索很有用。'
- en: These are just a few examples of the wide range of applications of clustering
    algorithms. The specific use case will depend on the industry, dataset, and business
    objectives.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是聚类算法广泛应用的几个例子。具体的应用案例将取决于行业、数据集和商业目标。
- en: In this section, we discussed various clustering algorithms and their uses.
    In the following sections, we’ll learn how to use them in real-world examples
    with various C++ libraries.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了各种聚类算法及其用途。在接下来的章节中，我们将学习如何在各种真实世界示例中使用它们，并使用各种C++库。
- en: Examples of using the mlpack library for dealing with the clustering task samples
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用mlpack库处理聚类任务示例
- en: 'The `mlpack` library contains implementations of the model-based, density-based,
    and partition-based clustering approaches. The model-based algorithm is called
    **Gaussian Mixture Models** (**GMM**) and is based on EM, while the partition-based
    algorithm is the k-means algorithm. There are two density-based algorithms we
    can use: DBSCAN and MeanShift clustering.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack` 库包含基于模型、基于密度和基于划分的聚类方法的实现。基于模型的算法称为**高斯混合模型**（**GMM**），基于 EM，而基于划分的算法是
    k-means 算法。我们可以使用两种基于密度的算法：DBSCAN 和 MeanShift 聚类。'
- en: GMM and EM with mlpack
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 mlpack 的 GMM 和 EM
- en: 'The GMM algorithm assumes that clusters can be fit to some Gaussian (normal)
    distributions; it uses the EM approach for training. There are the `GMM` and `mlpack`
    library that implement this approach, as illustrated in the following code snippet:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 算法假设簇可以拟合到某些高斯（正态）分布；它使用 EM 方法进行训练。有 `GMM` 和 `mlpack` 库实现了这种方法，如下面的代码片段所示：
- en: '[PRE0]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Notice that the constructor of the `GMM` class takes the desired number of
    clusters and feature dimensionality as an argument. After `GMM` object initialization,
    the object of the `EMFit` class was initialized with maximum iterations, tolerance,
    and a clustering object. The tolerance parameter in `EMFit` controls how similar
    two points must be to be considered as part of the same cluster. A higher tolerance
    value means that the algorithm will group more points, resulting in fewer clusters.
    Conversely, a lower tolerance value leads to more clusters with fewer points in
    each one. The clustering object—in our case, `kmeans`—will be used by the algorithm
    to find initial centroids for Gaussian fitting. Then, we passed the training features
    and the `EM` object into the training method. Now, we have the trained `GMM` model.
    In the `mlpack` library, the trained `gmm` object should be used to classify new
    feature points, but we can use it to show cluster assignments for the original
    data that we used for training. The following piece of code shows these steps
    and also plots the results of clustering:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 `GMM` 类的构造函数接受所需的簇数和特征维度性作为参数。在 `GMM` 对象初始化后，`EMFit` 类的对象使用最大迭代次数、容差和聚类对象进行初始化。`EMFit`
    中的容差参数控制两个点必须多么相似才能被认为是同一簇的一部分。更高的容差值意味着算法将组合更多的点，导致簇数更少。相反，较低的容差值会导致簇数更多，每个簇中的点更少。聚类对象——在我们的案例中，`kmeans`——将被算法用于找到高斯拟合的初始质心。然后，我们将训练特征和
    `EM` 对象传递到训练方法中。现在，我们有了训练好的 `GMM` 模型。在 `mlpack` 库中，应该使用训练好的 `gmm` 对象来分类新的特征点，但我们可以用它来显示用于训练的原始数据的簇分配。以下代码片段展示了这些步骤，并绘制了聚类的结果：
- en: '[PRE1]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we used the `GMM::Classify()` method to identify which cluster our objects
    belong to. This method filled a row vector of cluster identifiers per element
    that corresponds to the input data. The resulting cluster indices were used for
    filling the `plot_clusters` container. This container maps cluster indices with
    input data coordinates for plotting. It was used as an argument for the `PlotClusters()`
    function, which visualized the clustering result, as illustrated in the following
    figure:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了 `GMM::Classify()` 方法来识别我们的对象属于哪个簇。此方法为每个元素填充了一个簇标识符的行向量，该向量对应于输入数据。得到的簇索引被用于填充
    `plot_clusters` 容器。此容器将簇索引与输入数据坐标映射，用于绘图。它被用作 `PlotClusters()` 函数的参数，该函数可视化了聚类结果，如下面的图所示：
- en: '![Figure 4.4 – MLPack GMM clustering visualization](img/B19849_04_04.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – MLPack GMM 聚类可视化](img/B19849_04_04.jpg)'
- en: Figure 4.4 – mlpack GMM clustering visualization
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – mlpack GMM 聚类可视化
- en: In the preceding picture, we can see how the GMM and EM algorithms work on different
    artificial datasets.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图片中，我们可以看到 GMM 和 EM 算法在不同的人工数据集上是如何工作的。
- en: K-means clustering with mlpack
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 mlpack 的 K-means 聚类
- en: 'The k-means algorithm in the `mlpack` library is implemented in the `KMeans`
    class. The constructor of this class takes several parameters, with the most important
    ones being the number of iterations and the object for distance metric calculation.
    In the following example, we’ll use the default values so that the constructor
    will be called without parameters. Once we’ve constructed an object of the `KMeans`
    type, we’ll use the `KMeans::Cluster()` method to run the algorithm and assign
    a cluster label to each of the input elements, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack` 库中的 k-means 算法是在 `KMeans` 类中实现的。这个类的构造函数接受多个参数，其中最重要的参数是迭代次数和距离度量计算的对象。在下面的示例中，我们将使用默认值，这样构造函数就可以不带参数被调用。一旦我们构建了一个
    `KMeans` 类型的对象，我们将使用 `KMeans::Cluster()` 方法来运行算法，并为每个输入元素分配一个簇标签，如下所示：'
- en: '[PRE2]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result of clusterization is the `assignments` container object with labels.
    Notice that the desired number of clusters was passed as an argument for the `Cluster`
    method. The following code sample shows how to plot the results of clustering:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的结果是带有标签的 `assignments` 容器对象。请注意，我们通过 `Cluster` 方法的参数传递了期望的簇数。以下代码示例展示了如何绘制聚类的结果：
- en: '[PRE3]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we can see, the code for visualization is the same as it was for the previous
    example—it’s the result of the uniform clustering API in the `mlpack` library.
    We received the same `assignments` container that we converted in the data structure,
    which is suitable for the visualization library we’re using, and called the `PlotClusters`
    function. The visualization result is illustrated in the following figure:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，可视化代码与上一个示例相同——它是 `mlpack` 库中统一聚类 API 的结果。我们收到了与之前转换的数据结构相同的 `assignments`
    容器，这对于我们使用的可视化库是合适的，并调用了 `PlotClusters` 函数。可视化结果如图所示：
- en: '![Figure 4.5 – MLPack K-means clustering visualization](img/B19849_04_05.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5 – MLPack K-means 聚类可视化](img/B19849_04_05.jpg)'
- en: Figure 4.5 – mlpack K-means clustering visualization
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – mlpack K-means 聚类可视化
- en: In the preceding figure, we can see how the k-means algorithm works on different
    artificial datasets.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到 k-means 算法在不同的人工数据集上的工作方式。
- en: DBSCAN with mlpack
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 mlpack 的 DBSCAN
- en: 'The `DBSCAN` class implements the corresponding algorithm in the `mlpack` library.
    The constructor of this class takes several parameters, but the two most important
    are the epsilon and the minimum points number. In the following code snippet,
    we’re creating the object of this class:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`DBSCAN` 类在 `mlpack` 库中实现了相应的算法。这个类的构造函数接受多个参数，但最重要的两个是 `epsilon` 和最小点数。在下面的代码片段中，我们创建了这个类的对象：'
- en: '[PRE4]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, `epsilon` is the radius of a range search, while `min_points` is the
    minimum number of points required to form a cluster. After constructing an object
    of the `BSSCAN` type, we can use the `Cluster()` method to run the algorithm and
    assign a cluster label to each of the input elements, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`epsilon` 是范围搜索的半径，而 `min_points` 是形成簇所需的最小点数。在构建了 `BSSCAN` 类型的对象之后，我们可以使用
    `Cluster()` 方法来运行算法，并为每个输入元素分配一个簇标签，如下所示：
- en: '[PRE5]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The result of clusterization is an `assignments` container object with labels.
    Notice that for this algorithm, we didn’t specify the desired number of clusters
    because the algorithm determined them by itself. The code for the visualization
    is the same as for the previous examples—we convert the `assignments` container
    into a data structure that’s suitable for the visualization library we’re using
    and call the `PlotClusters` function. The following figure shows the DBSCAN clustering
    visualization result:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的结果是带有标签的 `assignments` 容器对象。请注意，对于这个算法，我们没有指定期望的簇数，因为算法自己确定了它们。可视化代码与之前的示例相同——我们将
    `assignments` 容器转换为适合我们使用的可视化库的数据结构，并调用 `PlotClusters` 函数。以下图显示了 DBSCAN 聚类可视化结果：
- en: '![Figure 4.6 – MLPack DBSCAN clustering visualization](img/B19849_04_06.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – MLPack DBSCAN 聚类可视化](img/B19849_04_06.jpg)'
- en: Figure 4.6 – mlpack DBSCAN clustering visualization
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – mlpack DBSCAN 聚类可视化
- en: In the preceding figure, we can see how the DBSCAN algorithm works on different
    artificial datasets. The main difference from previous algorithms is the bigger
    number of clusters that the algorithm found. From this, we can see that their
    centroids are near some local density maximums.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到 DBSCAN 算法在不同的人工数据集上的工作方式。与之前算法的主要区别是算法找到了更多的簇。从这一点我们可以看出，它们的质心靠近某些局部密度最大值。
- en: MeanShift clustering with mlpack
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用mlpack进行MeanShift聚类
- en: 'The `MeanShift` class implements the corresponding algorithm in the `mlpack`
    library. The constructor of this class takes several parameters, with the most
    important one being the density region search radius. It’s quite tricky to manually
    find the appropriate value for this parameter. However, the library gives us a
    very useful method to determine it automatically. In the following code snippet,
    we’re creating an object of the `MeanShift` class without specifying the radius
    parameter explicitly:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`MeanShift`类实现了`mlpack`库中相应的算法。此类的构造函数接受几个参数，其中最重要的是密度区域搜索半径。手动找到此参数的适当值相当棘手。然而，库为我们提供了一个非常有用的方法来自动确定它。在以下代码片段中，我们创建了一个`MeanShift`类的对象，而没有明确指定半径参数：'
- en: '[PRE6]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we used the `EstimateRadius` method to get the automatic radius estimation,
    which is based on the `MeanShift` object with the appropriate search radius value,
    we can use the `Cluster()` method to run the algorithm and assign a cluster label
    to each of the input elements, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`EstimateRadius`方法来获取自动半径估计，该估计基于具有适当搜索半径值的`MeanShift`对象，我们可以使用`Cluster()`方法运行算法并为每个输入元素分配一个簇标签，如下所示：
- en: '[PRE7]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The result of clusterization is the `assignments` container object with labels
    and an additional matrix that contains the cluster centroids coordinates. For
    this algorithm, we also didn’t specify the number of clusters because the algorithm
    determined them by itself. The code for visualization is the same as for the previous
    examples—we convert the `assignments` container into a data structure suitable
    for the visualization library we’re using and call the `PlotClusters` function.
    The following figure shows the `MeanShift` clustering visualization result:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的结果是包含标签和包含簇中心坐标的附加矩阵的`assignments`容器对象。对于此算法，我们也没有指定簇的数量，因为算法自行确定了它们。可视化代码与前面的示例相同——我们将`assignments`容器转换为适合我们使用的可视化库的数据结构，并调用`PlotClusters`函数。以下图显示了`MeanShift`聚类可视化结果：
- en: '![Figure 4.7 – MLPack MeanShift clustering visualization](img/B19849_04_07.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7 – MLPack MeanShift聚类可视化](img/B19849_04_07.jpg)'
- en: Figure 4.7 – mlpack MeanShift clustering visualization
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – mlpack MeanShift聚类可视化
- en: The preceding figure shows how the MeanShift algorithm works on different artificial
    datasets. We can see that the results are somehow similar to those for K-means
    clustering but the number of clusters was determined automatically. We can also
    see that in the one of datasets, the algorithm failed to get the correct number
    of clusters, so we must perform experiments with search radius values to get more
    precise clustering results.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了MeanShift算法在不同的人工数据集上的工作方式。我们可以看到，结果与K-means聚类的结果在某些方面相似，但簇的数量是自动确定的。我们还可以看到，在其中一个数据集中，算法未能得到正确的簇数量，因此我们必须通过搜索半径值进行实验以获得更精确的聚类结果。
- en: Examples of using the Dlib library for dealing with the clustering task samples
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Dlib库处理聚类任务示例
- en: The `Dlib` library provides k-means, spectral, hierarchical, and two more graph
    clustering algorithms—**Newman** and **Chinese Whispers**—as clustering methods.
    Let’s take a look.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib`库提供了k-means、谱系、层次和另外两种图聚类算法——**Newman**和**Chinese Whispers**——作为聚类方法。让我们看看。'
- en: K-means clustering with Dlib
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Dlib进行K-means聚类
- en: 'The `Dlib` library uses kernel functions as the distance functions for the
    k-means algorithm. An example of such a function is the radial basis function.
    As an initial step, we define the required types, as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib`库使用核函数作为k-means算法的距离函数。此类函数的一个例子是径向基函数。作为第一步，我们定义所需的类型，如下所示：'
- en: '[PRE8]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we initialize an object of the `kkmeans` type. Its constructor takes
    an object that will define cluster centroids as input parameters. We can use an
    object of the `kcentroid` type for this purpose. Its constructor takes three parameters:
    the first one is the object that defines the kernel (distance function), the second
    is the numerical accuracy for the centroid estimation, and the third is the upper
    limit on the runtime complexity (actually, the maximum number of dictionary vectors
    the `kcentroid` object is allowed to use), as illustrated in the following code
    snippet:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化一个 `kkmeans` 类型的对象。其构造函数接受一个定义聚类质心的对象作为输入参数。我们可以使用 `kcentroid` 类型的对象来完成这个目的。其构造函数接受三个参数：第一个是定义核（距离函数）的对象，第二个是质心估计的数值精度，第三个是运行时间复杂度的上限（实际上，是
    `kcentroid` 对象允许使用的最大字典向量数），如下面的代码片段所示：
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As a next step, we initialize cluster centers with the `pick_initial_centers()`
    function. This function takes the number of clusters, the output container for
    center objects, the training data, and the distance function object as parameters,
    as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，我们使用 `pick_initial_centers()` 函数初始化聚类中心。此函数接受聚类数量、中心对象输出容器、训练数据和距离函数对象作为参数，如下所示：
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When initial centers are selected, we can use them for the `kkmeans::train()`
    method to determine exact clusters, as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当选择初始中心时，我们可以使用它们来调用 `kkmeans::train()` 方法以确定精确的聚类，如下所示：
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We used the `kmeans` object as a functor to perform clustering on a single
    data item. The clustering result will be the cluster’s index for the item. Then,
    we used cluster indices to visualize the final clustering result, as illustrated
    in the following figure:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `kmeans` 对象作为函数对象在单个数据项上执行聚类。聚类结果将是该项的聚类索引。然后，我们使用聚类索引来可视化最终的聚类结果，如下面的图所示：
- en: "![Figure 4.8 – D\uFEFFlib K-means clustering visualization](img/B19849_04_08.jpg)"
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – Dlib K-means 聚类可视化](img/B19849_04_08.jpg)'
- en: Figure 4.8 – Dlib K-means clustering visualization
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – Dlib K-means 聚类可视化
- en: In the preceding figure, we can see how the k-means clustering algorithm that’s
    implemented in the `Dlib` library works on different artificial datasets.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到 `Dlib` 库中实现的 k-means 聚类算法在不同的人工数据集上的工作方式。
- en: Spectral clustering with Dlib
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Dlib 进行谱聚类
- en: 'The spectral clustering algorithm in the `Dlib` library is implemented in the
    `spectral_cluster` function. It takes the distance function object, the training
    dataset, and the number of clusters as parameters. As a result, it returns a container
    with cluster indices, which have the same ordering as the input data. In the following
    sample, an object of the `knn_kernel` type is used as a distance function. You’ll
    find its implementation in the samples provided in this book. This `knn_kernel`
    distance function object estimates the first KNN objects to the given one. These
    objects are determined with the KNN algorithm, which uses the Euclidean distance
    for the distance measure, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib` 库中的谱聚类算法在 `spectral_cluster` 函数中实现。它接受距离函数对象、训练数据集和聚类数量作为参数。结果，它返回一个包含聚类索引的容器，其顺序与输入数据相同。在下面的示例中，使用
    `knn_kernel` 类型的对象作为距离函数。你可以在本书提供的示例中找到其实现。这个 `knn_kernel` 距离函数对象估计给定对象的第一个 KNN
    对象。这些对象通过使用 KNN 算法确定，该算法使用欧几里得距离作为距离度量，如下所示：'
- en: '[PRE12]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `spectral_cluster()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result, as illustrated
    in the following figure:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`spectral_cluster()` 函数调用将 `clusters` 对象填充了聚类索引值，我们可以使用这些值来可视化聚类结果，如下面的图所示：'
- en: "![Figure 4.9 – D\uFEFFlib spectral clustering visualization](img/B19849_04_09.jpg)"
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – Dlib 谱聚类可视化](img/B19849_04_09.jpg)'
- en: Figure 4.9 – Dlib spectral clustering visualization
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – Dlib 谱聚类可视化
- en: In the preceding figure, we can see how the spectral clustering algorithm that’s
    implemented in the `Dlib` library works on different artificial datasets.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到 `Dlib` 库中实现的谱聚类算法在不同的人工数据集上的工作方式。
- en: Hierarchical clustering with Dlib
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Dlib 进行层次聚类
- en: The `Dlib` library implements the agglomerative hierarchical (bottom-up) clustering
    algorithm. The `bottom_up_cluster()` function implements this algorithm. This
    function takes the matrix of distances between dataset objects, the cluster indices
    container (as the output parameter), and the number of clusters as input parameters.
    Note that it returns the container with cluster indices in the order of distances
    provided in the matrix.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib`库实现了聚合层次（自下而上）聚类算法。`bottom_up_cluster()`函数实现了此算法。此函数接受数据集对象之间的距离矩阵、聚类索引容器（作为输出参数）和聚类数量作为输入参数。请注意，它返回的容器中的聚类索引顺序与矩阵中提供的距离顺序相同。'
- en: 'In the following code sample, we’ve filled the distance matrix with pairwise
    Euclidean distances between each pair of elements in the input dataset:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，我们已将距离矩阵填充为输入数据集中每对元素之间的成对欧几里得距离：
- en: '[PRE13]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `bottom_up_cluster()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result, as illustrated
    in the following figure:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`bottom_up_cluster()`函数调用将`clusters`对象填充了聚类索引值，我们可以使用这些值来可视化聚类结果，如图所示：'
- en: "![Figure 4.10 – D\uFEFFlib hierarchical clustering visualization](img/B19849_04_10.jpg)"
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图4.10 – Dlib层次聚类可视化](img/B19849_04_10.jpg)'
- en: Figure 4.10 – Dlib hierarchical clustering visualization
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 – Dlib层次聚类可视化
- en: In the preceding figure, we can see how the hierarchical clustering algorithm
    that’s implemented in the `Dlib` library works on different artificial datasets.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到`Dlib`库中实现的层次聚类算法在不同的人工数据集上的工作方式。
- en: Newman modularity-based graph clustering algorithm with Dlib
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于 Dlib 的 Newman 模块性图聚类算法
- en: The implementation of this algorithm is based on the work *Modularity and community
    structure in networks*, by M. E. J. Newman. This algorithm is based on the modularity
    matrix for a network or a graph and it isn’t based on particular graph theory.
    However, it does have some similarities with spectral clustering because it also
    uses eigenvectors.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的实现基于M. E. J. Newman所著的*网络中的模块性和社区结构*一文。此算法基于网络或图的模块性矩阵，并且它不是基于特定的图论。然而，它确实与谱聚类有某些相似之处，因为它也使用了特征向量。
- en: The `Dlib` library implements this algorithm in the `newman_cluster()` function,
    which takes a vector of weighted graph edges and outputs the container with cluster
    indices for each vertex. The vector of weighted graph edges represents the connections
    between nodes in the network, with each edge having a weight that indicates its
    strength. The weights are used to determine the similarity between nodes and thus
    influence the clustering process. The initial step for using this algorithm is
    to define graph edges. In the following code sample, we’re making edges between
    almost every pair of dataset objects. Notice that we only use pairs with a distance
    greater than a threshold (this was done for performance considerations). The threshold
    distance can be adjusted to achieve different levels of granularity in the clustering
    results.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib`库通过`newman_cluster()`函数实现了此算法，该函数接受一个加权图边的向量，并输出每个顶点的聚类索引容器。加权图边的向量表示网络中节点之间的连接，每条边都有一个表示其强度的权重。这些权重用于确定节点之间的相似性，从而影响聚类过程。使用此算法的初始步骤是定义图边。在以下代码示例中，我们正在创建数据集对象几乎每对之间的边。请注意，我们只使用距离大于阈值的对（这是出于性能考虑）。阈值距离可以根据需要调整以达到不同的聚类结果粒度。'
- en: 'Also, this algorithm doesn’t require prior knowledge of the number of clusters
    as it can determine the number of clusters by itself. Here’s the code:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，此算法不需要预先知道聚类数量，因为它可以自行确定聚类数量。以下是代码：
- en: '[PRE14]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `newman_cluster()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result. Notice that
    another approach for edge weight calculation can lead to another clustering result.
    Also, edge weight values should be initialized according to a certain task. The
    edge length was chosen only for demonstration purposes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`newman_cluster()`函数调用将`clusters`对象填充了聚类索引值，我们可以使用这些值来可视化聚类结果。请注意，另一种用于计算边权重的方案可能会导致不同的聚类结果。此外，边权重值应根据特定任务进行初始化。边长仅用于演示目的。'
- en: 'The result can be seen in the following figure:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: "![Figure 4.11 – D\uFEFFlib Newman clustering visualization](img/B19849_04_11.jpg)"
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图4.11 – Dlib Newman聚类可视化](img/B19849_04_11.jpg)'
- en: Figure 4.11 – Dlib Newman clustering visualization
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 – Dlib Newman聚类可视化
- en: In the preceding figure, we can see how the Newman clustering algorithm that’s
    implemented in the `Dlib` library works on different artificial datasets.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到在`Dlib`库中实现的Newman聚类算法在不同的人工数据集上的工作方式。
- en: Chinese Whispers – graph clustering algorithm with Dlib
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Chinese Whispers – 基于Dlib的图聚类算法
- en: The Chinese Whispers algorithm is an algorithm that’s used to partition the
    nodes of weighted, undirected graphs. It was described in the paper *Chinese Whispers
    – an Efficient Graph Clustering Algorithm and its Application to Natural Language
    Processing Problems*, by Chris Biemann. This algorithm also doesn’t use any unique
    graph theory methods; instead, it uses the idea of using local contexts for clustering,
    so it can be classified as a density-based method.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Chinese Whispers算法是一种用于对加权无向图的节点进行划分的算法。它在Chris Biemann撰写的论文《Chinese Whispers
    – 一种高效的图聚类算法及其在自然语言处理问题中的应用》中进行了描述。此算法也不使用任何独特的图论方法；相反，它使用局部上下文进行聚类的想法，因此它可以被归类为基于密度的方法。
- en: 'In the `Dlib` library, this algorithm is implemented in the `chinese_whispers()`
    function, which takes the vector of weighted graph edges and outputs the container
    with cluster indices for each of the vertices. For performance considerations,
    we limit the number of edges between dataset objects with a threshold on distance.
    The meaning of weighted graph edges and threshold parameters are the same as for
    the Newman algorithm. Moreover, as with the Newman algorithm, this one also determines
    the number of resulting clusters by itself. The code can be seen in the following
    snippet:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Dlib`库中，此算法通过`chinese_whispers()`函数实现，该函数接收加权图边的向量，并为每个顶点输出包含聚类索引的容器。出于性能考虑，我们通过距离阈值限制数据集对象之间的边数。加权图边和阈值参数的含义与新曼算法相同。此外，与Newman算法一样，此算法也自行确定结果聚类的数量。代码可以在以下代码片段中查看：
- en: '[PRE15]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `chinese_whispers()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result. Notice that
    we used `1` as the threshold for edge weights; another threshold value can lead
    to another clustering result. Also, edge weight values should be initialized according
    to a certain task. The edge length was chosen only for demonstration purposes.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`chinese_whispers()`函数调用将聚类索引值填充到`clusters`对象中，我们可以使用这些值来可视化聚类结果。请注意，我们使用`1`作为边权重的阈值；另一个阈值值可能导致不同的聚类结果。此外，边权重值应根据特定任务进行初始化。边长仅用于演示目的。'
- en: 'The result can be seen in the following figure:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以在以下图中查看：
- en: "![Figure 4.12 – D\uFEFFlib Chinese Whispers clustering visualization](img/B19849_04_12.jpg)"
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图4.12 – Dlib Chinese Whispers聚类可视化](img/B19849_04_12.jpg)'
- en: Figure 4.12 – Dlib Chinese Whispers clustering visualization
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 – Dlib Chinese Whispers聚类可视化
- en: In the preceding figure, we can see how the Chinese Whispers clustering algorithm
    that’s implemented in the `Dlib` library works on different artificial datasets.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到在`Dlib`库中实现的Chinese Whispers聚类算法在不同的人工数据集上的工作方式。
- en: In this and previous sections, we saw a lot of examples of images that show
    clustering results. The following section will explain how to use the `plotcpp`
    library, which we used to plot these images.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节和前几节中，我们看到了许多显示聚类结果的图像示例。下一节将解释如何使用`plotcpp`库，这是我们用来绘制这些图像的库。
- en: Plotting data with C++
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用C++绘制数据
- en: After clustering, we plot the results with the `plotcpp` library, which is a
    thin wrapper around the `gnuplot` command-line utility. With this library, we
    can draw points on a scatter plot or draw lines. The initial step to start plotting
    with this library is creating an object of the `Plot` class. Then, we must specify
    the output destination of the drawing. We can set the destination with the `Plot::SetTerminal()`
    method, which takes a string with a destination point abbreviation. For example,
    we can use the `qt` string value to show the `Plot` class methods. However, it
    does not cover all possible configurations available for `gnuplot`. In cases where
    we need some unique options, we can use the `Plot::gnuplotCommand()` method to
    make a direct `gnuplot` configuration.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类后，我们使用`plotcpp`库绘制结果，这是一个围绕`gnuplot`命令行工具的轻量级包装器。使用这个库，我们可以在散点图上绘制点或绘制线条。使用这个库开始绘图的第一步是创建`Plot`类的对象。然后，我们必须指定绘图的输出目的地。我们可以使用`Plot::SetTerminal()`方法设置目的地，该方法接受一个包含目的地点缩写的字符串。例如，我们可以使用`qt`字符串值来显示`Plot`类方法。然而，它并不涵盖`gnuplot`所有可能的配置。在需要一些独特选项的情况下，我们可以使用`Plot::gnuplotCommand()`方法直接进行`gnuplot`配置。
- en: 'There are two drawing approaches we can follow to draw a set of different graphics
    on one plot:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以遵循两种绘图方法来在一个图表上绘制一组不同的图形：
- en: We can use the `Draw2D()` method with objects of the `Points` or `Lines` classes,
    but in this case, we should specify all graphics configurations before compilation.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`Draw2D()`方法与`Points`或`Lines`类的对象一起使用，但在这个情况下，我们应该在编译前指定所有图形配置。
- en: We can use the `Plot::StartDraw2D()` method to get an intermediate drawing state
    object. Then, we can use the `Plot::AddDrawing()` method to add different drawings
    to one plot. The `Plot::EndDraw2D()` method should be called after we’ve drawn
    the last graphics.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`Plot::StartDraw2D()`方法获取一个中间绘图状态对象。然后，我们可以使用`Plot::AddDrawing()`方法将不同的绘图添加到一个图表中。在绘制了最后一张图形后，应该调用`Plot::EndDraw2D()`方法。
- en: We can use the `Points` type to draw points. An object of this type should be
    initialized with start and end forward iterators for the integral numeric data
    types, which represent coordinates. We should specify three iterators as points
    coordinates, two iterators for the *x* coordinates, which is where they start
    and end, and one iterator for the *y* coordinates’ start. The number of coordinates
    in the containers should be the same. The last parameter is the `gnuplot` visual
    style configuration. Objects of the `Lines` class can be configured in the same
    way.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`Points`类型来绘制点。此类型对象应该使用起始和结束前向迭代器初始化，这些迭代器代表坐标。我们应该指定三个迭代器作为点坐标，两个迭代器用于*x*坐标，即它们的起始和结束位置，以及一个迭代器用于*y*坐标的起始位置。容器中的坐标数量应该相同。最后一个参数是`gnuplot`视觉风格配置。`Lines`类的对象可以以相同的方式进行配置。
- en: 'Once we’ve completed all drawing operations, we should call the `Plot::Flush()`
    method to render all commands to the window or the file, as shown in the following
    code block:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了所有的绘图操作，我们应该调用`Plot::Flush()`方法将所有命令渲染到窗口或文件中，如下面的代码块所示：
- en: '[PRE16]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this example, we learned how to plot our clustering results with the `plotcpp`
    library. We must be able to configure different visualization parameters, such
    as the type of the plot, point colors, and axes names as these parameters make
    our plot more informative. We also learned how to save this plot in a file so
    that we can use it later or insert it into another document. This library will
    be used throughout this book for visualizing results.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们学习了如何使用`plotcpp`库绘制我们的聚类结果。我们必须能够配置不同的可视化参数，例如图表类型、点颜色和轴名称，因为这些参数使我们的图表更具信息量。我们还学习了如何将此图表保存到文件中，以便我们以后可以使用它或将它插入到另一个文档中。这个库将在整本书中用于可视化结果。
- en: Summary
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we considered what clustering is and how it differs from classification.
    We looked at different types of clustering methods, such as partition-based, spectral,
    hierarchical, density-based, and model-based methods. We also observed that partition-based
    methods can be divided into more categories, such as distance-based methods and
    graph theory-based methods.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们考虑了聚类是什么以及它与分类有何不同。我们探讨了不同的聚类方法，例如基于划分、频谱、层次、基于密度和基于模型的方法。我们还观察到基于划分的方法可以分为更多类别，例如基于距离的方法和基于图论的方法。
- en: Then, we used implementations of these algorithms, including the k-means algorithm
    (the distance-based method), the GMM algorithm (the model-based method), the Newman
    modularity-based algorithm, and the Chinese Whispers algorithm, for graph clustering.
    We also learned how to use the hierarchical and spectral clustering algorithm
    implementations in programs. We saw that the crucial issues for successful clustering
    include the choice of the distance measure function, the initialization step,
    the splitting or merging strategy, and prior knowledge of the number of clusters.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用了这些算法的实现，包括k-means算法（基于距离的方法）、GMM算法（基于模型的方法）、基于Newman模块性的算法以及中国 whispers算法，用于图聚类。我们还学习了如何在程序中使用层次聚类和谱聚类算法的实现。我们看到了成功聚类的关键问题包括距离度量函数的选择、初始化步骤、分割或合并策略以及关于聚类数量的先验知识。
- en: A combination of these issues is unique for each specific algorithm. We also
    saw that a clustering algorithm’s results depend a lot on dataset characteristics
    and that we should choose the algorithm according to these.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特定算法的组合都是独特的。我们还看到，聚类算法的结果很大程度上取决于数据集的特征，因此我们应该根据这些特征来选择算法。
- en: At the end of this chapter, we studied how we can visualize clustering results
    with the `plotcpp` library.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，我们研究了如何使用`plotcpp`库可视化聚类结果。
- en: In the next chapter, we’ll learn what a data anomaly is and what machine learning
    algorithms exist for anomaly detection. We’ll also see how anomaly detection algorithms
    can be used to solve real-life problems, and which properties of such algorithms
    play a more significant role in different tasks.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习什么是数据异常以及存在哪些机器学习算法用于异常检测。我们还将看到异常检测算法如何用于解决现实生活中的问题，以及这些算法的哪些属性在不同任务中扮演着更重要的角色。
- en: Further reading
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于本章所涉及主题的信息，请查看以下资源：
- en: '*The 5 Clustering Algorithms Data Scientists Need to* *Know*: [https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据科学家需要了解的5种聚类算法*：[https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)'
- en: '*Clustering*: [https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类*：[https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html)'
- en: '*Different Types of Clustering* *Algorithm*: [https://www.geeksforgeeks.org/different-types-clustering-algorithm/](https://www.geeksforgeeks.org/different-types-clustering-algorithm/)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不同类型的聚类* *算法*：[https://www.geeksforgeeks.org/different-types-clustering-algorithm/](https://www.geeksforgeeks.org/different-types-clustering-algorithm/)'
- en: '*An introduction to clustering and different methods of* *clustering*: [https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/](https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类及其不同方法的介绍*：[https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/](https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/)'
- en: 'Graph theory introductory book: *Graph Theory* (*Graduate Texts in Mathematics*),
    by Adrian Bondy and U.S.R. Murty'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图论入门书籍：*图论* (*Graduate Texts in Mathematics*)，作者为Adrian Bondy和U.S.R. Murty
- en: '*The Elements of Statistical Learning: Data Mining, Inference, and Prediction*,
    by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, covers a lot of aspects
    of machine learning theory and algorithms'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《统计学习的要素：数据挖掘、推理与预测》*，作者为Trevor Hastie、Robert Tibshirani和Jerome Friedman，涵盖了机器学习理论和算法的许多方面。'
