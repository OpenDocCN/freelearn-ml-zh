- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Clustering** is an unsupervised machine learning method that’s used for splitting
    the original dataset of objects into groups classified by properties. In **machine
    learning**, an object is typically represented as a point in a multidimensional
    metric space. Every space dimension corresponds to an object property (feature),
    and the metric is a function of the values of these properties. Depending on the
    types of dimensions in this space, which can be both numerical and categorical,
    we choose a type of clustering algorithm and specific metric function. This choice
    depends on the nature of different object properties’ types.'
  prefs: []
  type: TYPE_NORMAL
- en: At the present stage, clustering is often used as the first step in data analysis.
    The task of clustering was formulated in scientific areas such as statistics,
    pattern recognition, optimization, and machine learning. At the time of writing,
    the number of methods for partitioning groups of objects into clusters is quite
    large—several dozen algorithms, and even more when you take into account their
    various modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring distance in clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of clustering algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of using the `mlpack` library for dealing with the clustering task
    samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of using the `Dlib` library for dealing with the clustering task samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting data with C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You’ll require the following technologies and installations to complete this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A modern C++ compiler with C++17 support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CMake build system version >= 3.8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Dlib` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mlpack` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `plotcpp` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be found in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04).'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring distance in clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A metric or distance measure is essential in clustering because it determines
    the similarity between objects. However, before applying a distance measure to
    objects, we must make a vector of object characteristics; usually, this is a set
    of numerical values, such as human height or weight. Also, some algorithms can
    work with categorical object features (or characteristics). The standard practice
    is to normalize feature values. Normalization ensures that each feature has the
    same impact in a distance measure calculation. Many distance measure functions
    can be used in the scope of the clustering task. The most popular ones that are
    used for numerical properties are **Euclidean distance**, **squared Euclidean
    distance**, **Manhattan distance**, and **Chebyshev distance**. The following
    subsections describe them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Euclidean distance is the most widely used distance measure. In general, this
    is a geometric distance in the multidimensional space. The formula for Euclidean
    distance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Squared Euclidean distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Squared Euclidean distance has the same properties as Euclidean distance but
    assigns greater significance (weight) to the distant values than to closer ones.
    Here’s the formula for squared Euclidean distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Manhattan distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Manhattan distance is an average difference by coordinates. In most cases,
    its value gives the same clustering results as Euclidean distance. However, it
    reduces the significance (weight) of the distant values (outliers). Here’s the
    formula for Manhattan distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Chebyshev distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chebyshev distance can be useful when we need to classify two objects as different
    when they differ only by one of the coordinates. Here’s the formula for Chebyshev
    distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_041.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram shows the differences between the various distances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The difference between different distance measures](img/B19849_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – The difference between different distance measures
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that *Manhattan* distance is the sum of the distances in both
    dimensions, like walking along city blocks. *Euclidean* distance is just the length
    of a straight line. *Chebyshev* distance is a more flexible alternative to *Manhattan*
    distance because diagonal moves are also taken into account.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we became familiar with the main clustering concept, which
    is a distance measure. In the following section, we’ll discuss various types of
    clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Types of clustering algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are different types of clustering that we can classify into the following
    groups: **partition-based**, **spectral**, **hierarchical**, **density-based**,
    and **model-based**. The partition-based group of clustering algorithms can be
    logically divided into distance-based methods and ones based on graph theory.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we cover different types of clustering algorithms, let’s understand the
    main difference between clustering and classification. The main difference between
    the two is an undefined set of target groups, which is determined by the clustering
    algorithm. The set of target groups (clusters) is the algorithm’s result.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can split cluster analysis into the following phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting objects for clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the set of object properties that we’ll use for the metric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing property values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating the metric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying distinct groups of objects based on metric values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After analyzing clustering results, some correction may be required for the
    selected metric of the chosen algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use clustering for various real-world tasks, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting news into several categories for advertisers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying customer groups by their preferences for market analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying plant and animal groups for biological studies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying and categorizing properties for city planning and management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting earthquake epicenter clusters to identify danger zones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorizing groups of insurance policyholders for risk management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorizing books in libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching for hidden structural similarities in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that, let’s dive into the different types of clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Partition-based clustering algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The partition-based methods use a similarity measure to combine objects into
    groups. A practitioner usually selects the similarity measure for such kinds of
    algorithms, using prior knowledge about a problem or heuristics to select the
    measure properly. Sometimes, several measures need to be tried with the same algorithm
    so that the best one can be chosen. Also, partition-based methods usually require
    either the number of desired clusters or a threshold that regulates the number
    of output clusters to be specified explicitly. The choice of a similarity measure
    can significantly affect the quality and accuracy of the clusters produced, potentially
    leading to misinterpretations of data patterns and insights.
  prefs: []
  type: TYPE_NORMAL
- en: Distance-based clustering algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most known representatives of this family of methods are the k-means and
    k-medoids algorithms. They take the *k* input parameter and divide the data space
    into *k* clusters so that the similarity between objects in one cluster is maximal.
    Also, they minimize the similarity between objects of different clusters. The
    similarity value is calculated as the distance from the object to the cluster
    center. The main difference between these methods lies in the way the cluster
    center is defined.
  prefs: []
  type: TYPE_NORMAL
- en: With the k-means algorithm, the similarity is proportional to the distance to
    the cluster center of mass. The cluster center of mass is the average value of
    cluster objects’ coordinates in the data space. The k-means algorithm can be briefly
    described with a few steps. First, we select *k* random objects and define each
    of them as a cluster prototype that represents the cluster’s center of mass. Then,
    the remaining objects are attached to the cluster with greater similarity. After
    that, the center of mass of each cluster is recalculated. For each obtained partition,
    a particular evaluation function is calculated, the values of which at each step
    form a converging series. This process continues until the specified series converges
    to its limit value.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, moving objects from one cluster to another ends when the clusters
    remain unchanged. Minimizing the evaluation function allows the resulting clusters
    to be as compact and separate as possible. The k-means method works well when
    clusters are compact *clouds* that are significantly separated from each other.
    It’s useful for processing large amounts of data but isn’t applicable for detecting
    clusters of non-convex shapes or clusters with very different sizes. Moreover,
    the method is susceptible to noise and isolated points since even a small number
    of such points can significantly affect how the center mass of the cluster is
    calculated.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the influence of noise and isolated points on the clustering result,
    the k-medoids algorithm, in contrast to the k-means algorithm, uses one of the
    cluster objects (known as the representative object) as the center of the cluster.
    As in the k-means method, *k* representative objects are selected at random. Each
    of the remaining objects is combined into a cluster with the nearest representative
    object. Then, each representative object is replaced iteratively with an arbitrary
    unrepresentative object from the data space. The replacement process continues
    until the quality of the resulting clusters improves. The clustering quality is
    determined by the sum of deviations between objects and the representative object
    of the corresponding cluster, which the method tries to minimize. Thus, the iterations
    continue until the representative object in each of the clusters becomes the medoid.
  prefs: []
  type: TYPE_NORMAL
- en: The **medoid** is the object closest to the center of the cluster. The algorithm
    is poorly scalable for processing large amounts of data, but this problem is solved
    by the **Clustering Large Applications based on RANdomized Search** (**CLARANS**)
    algorithm, which complements the k-medoids method. CLARANS attempts to address
    scalability issues by using a randomized search technique to find good solutions
    more efficiently. Such an approach makes it possible to quickly converge on a
    good solution without exhaustively searching all possible combinations of medoids.
    For multidimensional clustering, the **Projected Clustering** (**PROCLUS**) algorithm
    can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Graph theory-based clustering algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The essence of algorithms based on graph theory is to represent target objects
    in graph form. Graph vertices correspond to objects, and the edge weights are
    equal to the distance between vertices. The advantages of graph clustering algorithms
    are their excellent visibility, relative ease of implementation, and their ability
    to make various improvements based on geometrical considerations. The main graph
    theory concepts used for clustering are selecting connected components, constructing
    a minimum spanning tree, and multilayer graph clustering.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm for selecting connected components is based on the *R* input parameter,
    and the algorithm removes all edges in the graph with distances greater than *R*.
    Only the closest pairs of objects remain connected. The algorithm’s goal is to
    find the *R* value at which the graph collapses into several connected components.
    The resulting components are clusters. To select the *R* parameter, a histogram
    of the distribution of pairwise distances is usually constructed. For problems
    with a well-defined cluster data structure, there will be two peaks in the histogram—one
    corresponds to in-cluster distances and the second to inter-cluster distances.
    The *R* parameter is selected from the minimum zone between these peaks. Managing
    the number of clusters using the distance threshold can be difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'The minimum spanning tree algorithm builds a minimal spanning tree on the graph,
    and then successively removes the edges with the highest weight. The following
    diagram shows the minimum spanning tree that’s been obtained for nine objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Spanning tree example](img/B19849_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Spanning tree example
  prefs: []
  type: TYPE_NORMAL
- en: 'By removing the link between *C* and *D*, with a length of 6 units (the edge
    with the maximum distance), we obtain two clusters: *{A, B, C}* and *{D, E, F,
    G, H, I}*. We can divide the second cluster into two more clusters by removing
    the *EF* edge, which has a length of 4 units.'
  prefs: []
  type: TYPE_NORMAL
- en: The multilayer clustering algorithm is based on identifying connected components
    of a graph at some level of distance between objects (vertices). The threshold,
    *C,* defines the distance level—for example, if the distance between objects is
    ![](img/B19849_Formula_051.png), then ![](img/B19849_Formula_061.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The layer clustering algorithm generates a sequence of sub-graphs of the graph,
    *G*, that reflect the hierarchical relationships between clusters, ![](img/B19849_Formula_071.png),
    where the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_081.png): A sub-graph on the ![](img/B19849_Formula_09.png)level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_101.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B19849_Formula_112.png): The *t*th threshold of distance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_123.png): The number of hierarchy levels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_131.png), o: An empty set of graph edges, when ![](img/B19849_Formula_142.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B19849_Formula_151.png): A graph of objects without thresholds on distance,
    when ![](img/B19849_Formula_162.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: By changing the ![](img/B19849_Formula_171.png) distance thresholds, where ![](img/B19849_Formula_181.png),
    it’s possible to control the hierarchy depth of the resulting clusters. Thus,
    a multilayer clustering algorithm can create both flat and hierarchical data partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spectral clustering refers to all methods that divide a set of data into clusters
    using the eigenvectors of the adjacency matrix of a graph or other matrices derived
    from it. An adjacency matrix describes a complete graph with vertices in objects
    and edges between each pair of objects with a weight corresponding to the degree
    of similarity between these vertices. Spectral clustering involves transforming
    the initial set of objects into a set of points in space whose coordinates are
    elements of eigenvectors. The formal name for such a task is the **normalized**
    **cuts problem**.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting set of points is then clustered using standard methods—for example,
    with the k-means algorithm. Changing the representation created by eigenvectors
    allows us to set the properties of the original set of clusters more clearly.
    Thus, spectral clustering can separate points that can’t be separated by applying
    k-means—for example, when the k-means method gets a convex set of points. The
    main disadvantage of spectral clustering is its cubic computational complexity
    and quadratic memory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Among the algorithms of hierarchical clustering, there are two main types:
    **bottom-up** and **top-down-based algorithms**. Top-down algorithms work on the
    principle that at the beginning, all objects are placed in one cluster, which
    is then divided into smaller and smaller clusters. Bottom-up algorithms are more
    common than top-down ones. They place each object in a separate cluster at the
    beginning of the work and then merge clusters into larger ones until all the objects
    in the dataset are contained in one cluster, building a system of nested partitions.
    The results of such algorithms are usually presented in tree form, called a **dendrogram**.
    A classic example of such a tree is the *Tree of Life*, which describes the classification
    of animals and plants.'
  prefs: []
  type: TYPE_NORMAL
- en: The main problem with hierarchical methods is the difficulty of determining
    the stop condition in such a way as to isolate natural clusters and, at the same
    time, prevent their excessive splitting. Another problem with hierarchical clustering
    methods is choosing the point of separation or merging of clusters. This choice
    is critical because after splitting or merging clusters at each subsequent step,
    the method will operate only on newly formed clusters. Therefore, the wrong choice
    of a merge or split point at any step can lead to poor-quality clustering. Also,
    hierarchical methods can’t be applied to large datasets because deciding whether
    to divide or merge clusters requires a large number of objects and clusters to
    be analyzed, which leads to a significant computational complexity of the method.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several metrics or linkage criteria for cluster union that are used
    in hierarchical clustering methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single linkage (nearest neighbor distance)**: In this method, the distance
    between the two clusters is determined by the distance between the two closest
    objects (nearest neighbors) in different clusters. The resulting clusters tend
    to chain together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete linkage (distance between the most distant neighbors)**: In this
    method, the distances between clusters are determined by the largest distance
    between any two objects in different clusters (that is, the most distant neighbors).
    This method usually works very well when objects come from separate groups. If
    the clusters are elongated or their natural type is *chained*, then this method
    is unsuitable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unweighted pairwise mean linkage**: In this method, the distance between
    two different clusters is calculated as the average distance between all pairs
    of objects in them. This method is useful when objects form different groups,
    but it works equally well in the case of elongated (chained-type) clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted pairwise mean linkage**: This method is identical to the unweighted
    pairwise mean method, except that the size of the corresponding clusters (the
    number of objects contained in them) is used as a weighting factor in the calculations.
    Therefore, this method should be used when we assume unequal cluster sizes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted centroid linkage**: In this method, the distance between two clusters
    is defined as the distance between their centers of mass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted centroid linkage** (**median**): This method is identical to the
    previous one, except that the calculations use weights for the distance measured
    between cluster sizes. Therefore, if there are significant differences in-cluster
    sizes, this method is preferable to the previous one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram displays a hierarchical clustering dendrogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Hierarchical clustering example](img/B19849_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Hierarchical clustering example
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows an example of a dendrogram for hierarchical clustering,
    where you can see how the number of clusters depends on the distance between objects.
    Larger distances lead to a smaller number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Density-based clustering algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In density-based methods, clusters are considered as regions where the multiple
    objects’ density is high. This is separated by regions with a low density of objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Density-Based Spatial Clustering of Applications with Noise** (**DBSCAN**)
    algorithm is one of the first density clustering algorithms to be created. The
    basis of this algorithm is several statements, detailed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The ![](img/B19849_Formula_191.png) property of an object is the ![](img/B19849_Formula_20.png)
    radius neighborhood area around the object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The root object is an object whose ![](img/B19849_Formula_211.png) contains
    a minimum non-zero number of objects. Assume that this minimum number equals a
    predefined value named *MinPts*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *p* object is directly densely accessible from the *q* object if *p* is
    in the ![](img/B19849_Formula_221.png) property of *q* and *q* is the root object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *p* object is densely accessible from the *q* object for the given ![](img/B19849_Formula_23.png)
    and *MinPts* if there’s a sequence of ![](img/B19849_Formula_24.png) objects,
    where ![](img/B19849_Formula_25.png) and ![](img/B19849_Formula_261.png), such
    that ![](img/B19849_Formula_271.png) is directly densely accessible from ![](img/B19849_Formula_282.png),
    ![](img/B19849_Formula_29.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *p* object is densely connected to the *q* object for the given ![](img/B19849_Formula_301.png)
    and *MinPts* if there’s an *o* object such that *p* and *q* are densely accessible
    from *o*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DBSCAN algorithm checks the neighborhood of each object to search for clusters.
    If the ![](img/B19849_Formula_311.png) property of the *p* object contains more
    points than *MinPts*, then a new cluster is created with the *p* object as a root
    object. DBSCAN then iteratively collects objects directly densely accessible from
    root objects, which can lead to the union of several densely accessible clusters.
    The process ends when no new objects can be added to any cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the partition-based methods, DBSCAN doesn’t require the number of clusters
    to be specified in advance; it only requires the ![](img/B19849_Formula_321.png)
    and *MinPts* values as these parameters directly affect the result of clustering.
    The optimal values of these parameters are difficult to determine, especially
    for multidimensional data spaces. Also, the distributed data in such spaces is
    often asymmetrical, which makes it impossible to use global density parameters
    for their clustering. For clustering multidimensional data spaces, there’s the
    **Subspace Clustering** (**SUBCLU**) algorithm, which is based on the DBSCAN algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The **MeanShift** approach also falls into the category of density-based clustering
    algorithms. It’s a non-parametric algorithm that shifts dataset points toward
    the center of the highest-density region within a certain radius. The algorithm
    makes such shifts iteratively until points converge to a local maximum of the
    density function. Such local maxima are also called the mode, so the algorithm
    is sometimes called mode-seeking. These local maximums represent the cluster centroids
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Model-based clustering algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model-based algorithms assume that there’s a particular mathematical model of
    the cluster in the data space and try to maximize the likelihood of this model
    and the data available. Often, this uses the apparatus of mathematical statistics.
  prefs: []
  type: TYPE_NORMAL
- en: The **Expectation-Maximization** (**EM**) algorithm assumes that the dataset
    can be modeled using a linear combination of multidimensional normal distributions.
    Its purpose is to estimate distribution parameters that maximize the likelihood
    function that’s used as a measure of model quality. In other words, it assumes
    that the data in each cluster obeys a particular distribution law—namely, the
    normal distribution. With this assumption, it’s possible to determine the optimal
    parameters of the distribution law—the mean and variance at which the likelihood
    function is maximal. Thus, we can assume that any object belongs to all clusters,
    but with a different probability. In this instance, the task will be to fit the
    set of distributions to the data and determine the probabilities of the object
    belonging to each cluster. The object should be assigned to the cluster for which
    this probability is higher than the others.
  prefs: []
  type: TYPE_NORMAL
- en: The EM algorithm is simple and easy to implement. It isn’t sensitive to isolated
    objects and quickly converges in the case of successful initialization. However,
    it requires us to specify *k* number of clusters, which implies a *priori* knowledge
    about the data. Also, if the initialization fails, the algorithm may be slow to
    converge, or we might obtain a poor-quality result. Such algorithms don’t apply
    to high-dimensionality spaces since, in this case, it’s complicated to assume
    a mathematical model for distributing data in this space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the various types of clustering algorithms, let’s look
    at their uses in many industries to group similar data points into clusters. Here
    are some examples of how clustering algorithms can be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer segmentation**: Clustering algorithms can be used to segment customers
    based on their purchase history, demographics, and other attributes. This information
    can then be used for targeted marketing campaigns, personalized product recommendations,
    and customer service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image recognition**: In the field of computer vision, clustering algorithms
    are used to group images based on visual features such as color, texture, and
    shape. This can be useful for image classification, object detection, and scene
    understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fraud detection**: In finance, clustering algorithms can detect suspicious
    transactions by grouping them based on similarities in transaction patterns. This
    helps to identify potential fraud and prevent financial losses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommender systems**: In e-commerce, clustering algorithms group products
    based on customer preferences. This allows recommender systems to suggest relevant
    products to customers, increasing sales and customer satisfaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Social network analysis**: In social media, clustering algorithms identify
    groups of users with similar interests or behaviors. This enables targeted advertising,
    content creation, and community building.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Genomics**: In biology, clustering algorithms analyze gene expression data
    to identify groups of genes that are co-expressed under specific conditions. This
    aids in understanding gene function and disease mechanisms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text mining**: In natural language processing, clustering algorithms categorize
    documents based on their content. This is useful for topic modeling, document
    classification, and information retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of the wide range of applications of clustering
    algorithms. The specific use case will depend on the industry, dataset, and business
    objectives.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed various clustering algorithms and their uses.
    In the following sections, we’ll learn how to use them in real-world examples
    with various C++ libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of using the mlpack library for dealing with the clustering task samples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `mlpack` library contains implementations of the model-based, density-based,
    and partition-based clustering approaches. The model-based algorithm is called
    **Gaussian Mixture Models** (**GMM**) and is based on EM, while the partition-based
    algorithm is the k-means algorithm. There are two density-based algorithms we
    can use: DBSCAN and MeanShift clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: GMM and EM with mlpack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The GMM algorithm assumes that clusters can be fit to some Gaussian (normal)
    distributions; it uses the EM approach for training. There are the `GMM` and `mlpack`
    library that implement this approach, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the constructor of the `GMM` class takes the desired number of
    clusters and feature dimensionality as an argument. After `GMM` object initialization,
    the object of the `EMFit` class was initialized with maximum iterations, tolerance,
    and a clustering object. The tolerance parameter in `EMFit` controls how similar
    two points must be to be considered as part of the same cluster. A higher tolerance
    value means that the algorithm will group more points, resulting in fewer clusters.
    Conversely, a lower tolerance value leads to more clusters with fewer points in
    each one. The clustering object—in our case, `kmeans`—will be used by the algorithm
    to find initial centroids for Gaussian fitting. Then, we passed the training features
    and the `EM` object into the training method. Now, we have the trained `GMM` model.
    In the `mlpack` library, the trained `gmm` object should be used to classify new
    feature points, but we can use it to show cluster assignments for the original
    data that we used for training. The following piece of code shows these steps
    and also plots the results of clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we used the `GMM::Classify()` method to identify which cluster our objects
    belong to. This method filled a row vector of cluster identifiers per element
    that corresponds to the input data. The resulting cluster indices were used for
    filling the `plot_clusters` container. This container maps cluster indices with
    input data coordinates for plotting. It was used as an argument for the `PlotClusters()`
    function, which visualized the clustering result, as illustrated in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – MLPack GMM clustering visualization](img/B19849_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – mlpack GMM clustering visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding picture, we can see how the GMM and EM algorithms work on different
    artificial datasets.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering with mlpack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The k-means algorithm in the `mlpack` library is implemented in the `KMeans`
    class. The constructor of this class takes several parameters, with the most important
    ones being the number of iterations and the object for distance metric calculation.
    In the following example, we’ll use the default values so that the constructor
    will be called without parameters. Once we’ve constructed an object of the `KMeans`
    type, we’ll use the `KMeans::Cluster()` method to run the algorithm and assign
    a cluster label to each of the input elements, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of clusterization is the `assignments` container object with labels.
    Notice that the desired number of clusters was passed as an argument for the `Cluster`
    method. The following code sample shows how to plot the results of clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the code for visualization is the same as it was for the previous
    example—it’s the result of the uniform clustering API in the `mlpack` library.
    We received the same `assignments` container that we converted in the data structure,
    which is suitable for the visualization library we’re using, and called the `PlotClusters`
    function. The visualization result is illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – MLPack K-means clustering visualization](img/B19849_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – mlpack K-means clustering visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the k-means algorithm works on different
    artificial datasets.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN with mlpack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `DBSCAN` class implements the corresponding algorithm in the `mlpack` library.
    The constructor of this class takes several parameters, but the two most important
    are the epsilon and the minimum points number. In the following code snippet,
    we’re creating the object of this class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `epsilon` is the radius of a range search, while `min_points` is the
    minimum number of points required to form a cluster. After constructing an object
    of the `BSSCAN` type, we can use the `Cluster()` method to run the algorithm and
    assign a cluster label to each of the input elements, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of clusterization is an `assignments` container object with labels.
    Notice that for this algorithm, we didn’t specify the desired number of clusters
    because the algorithm determined them by itself. The code for the visualization
    is the same as for the previous examples—we convert the `assignments` container
    into a data structure that’s suitable for the visualization library we’re using
    and call the `PlotClusters` function. The following figure shows the DBSCAN clustering
    visualization result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – MLPack DBSCAN clustering visualization](img/B19849_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – mlpack DBSCAN clustering visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the DBSCAN algorithm works on different
    artificial datasets. The main difference from previous algorithms is the bigger
    number of clusters that the algorithm found. From this, we can see that their
    centroids are near some local density maximums.
  prefs: []
  type: TYPE_NORMAL
- en: MeanShift clustering with mlpack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `MeanShift` class implements the corresponding algorithm in the `mlpack`
    library. The constructor of this class takes several parameters, with the most
    important one being the density region search radius. It’s quite tricky to manually
    find the appropriate value for this parameter. However, the library gives us a
    very useful method to determine it automatically. In the following code snippet,
    we’re creating an object of the `MeanShift` class without specifying the radius
    parameter explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we used the `EstimateRadius` method to get the automatic radius estimation,
    which is based on the `MeanShift` object with the appropriate search radius value,
    we can use the `Cluster()` method to run the algorithm and assign a cluster label
    to each of the input elements, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of clusterization is the `assignments` container object with labels
    and an additional matrix that contains the cluster centroids coordinates. For
    this algorithm, we also didn’t specify the number of clusters because the algorithm
    determined them by itself. The code for visualization is the same as for the previous
    examples—we convert the `assignments` container into a data structure suitable
    for the visualization library we’re using and call the `PlotClusters` function.
    The following figure shows the `MeanShift` clustering visualization result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – MLPack MeanShift clustering visualization](img/B19849_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – mlpack MeanShift clustering visualization
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows how the MeanShift algorithm works on different artificial
    datasets. We can see that the results are somehow similar to those for K-means
    clustering but the number of clusters was determined automatically. We can also
    see that in the one of datasets, the algorithm failed to get the correct number
    of clusters, so we must perform experiments with search radius values to get more
    precise clustering results.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of using the Dlib library for dealing with the clustering task samples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Dlib` library provides k-means, spectral, hierarchical, and two more graph
    clustering algorithms—**Newman** and **Chinese Whispers**—as clustering methods.
    Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering with Dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Dlib` library uses kernel functions as the distance functions for the
    k-means algorithm. An example of such a function is the radial basis function.
    As an initial step, we define the required types, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize an object of the `kkmeans` type. Its constructor takes
    an object that will define cluster centroids as input parameters. We can use an
    object of the `kcentroid` type for this purpose. Its constructor takes three parameters:
    the first one is the object that defines the kernel (distance function), the second
    is the numerical accuracy for the centroid estimation, and the third is the upper
    limit on the runtime complexity (actually, the maximum number of dictionary vectors
    the `kcentroid` object is allowed to use), as illustrated in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As a next step, we initialize cluster centers with the `pick_initial_centers()`
    function. This function takes the number of clusters, the output container for
    center objects, the training data, and the distance function object as parameters,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When initial centers are selected, we can use them for the `kkmeans::train()`
    method to determine exact clusters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the `kmeans` object as a functor to perform clustering on a single
    data item. The clustering result will be the cluster’s index for the item. Then,
    we used cluster indices to visualize the final clustering result, as illustrated
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.8 – D\uFEFFlib K-means clustering visualization](img/B19849_04_08.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Dlib K-means clustering visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the k-means clustering algorithm that’s
    implemented in the `Dlib` library works on different artificial datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering with Dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The spectral clustering algorithm in the `Dlib` library is implemented in the
    `spectral_cluster` function. It takes the distance function object, the training
    dataset, and the number of clusters as parameters. As a result, it returns a container
    with cluster indices, which have the same ordering as the input data. In the following
    sample, an object of the `knn_kernel` type is used as a distance function. You’ll
    find its implementation in the samples provided in this book. This `knn_kernel`
    distance function object estimates the first KNN objects to the given one. These
    objects are determined with the KNN algorithm, which uses the Euclidean distance
    for the distance measure, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `spectral_cluster()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result, as illustrated
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.9 – D\uFEFFlib spectral clustering visualization](img/B19849_04_09.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Dlib spectral clustering visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the spectral clustering algorithm that’s
    implemented in the `Dlib` library works on different artificial datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering with Dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Dlib` library implements the agglomerative hierarchical (bottom-up) clustering
    algorithm. The `bottom_up_cluster()` function implements this algorithm. This
    function takes the matrix of distances between dataset objects, the cluster indices
    container (as the output parameter), and the number of clusters as input parameters.
    Note that it returns the container with cluster indices in the order of distances
    provided in the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code sample, we’ve filled the distance matrix with pairwise
    Euclidean distances between each pair of elements in the input dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `bottom_up_cluster()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result, as illustrated
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.10 – D\uFEFFlib hierarchical clustering visualization](img/B19849_04_10.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Dlib hierarchical clustering visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the hierarchical clustering algorithm
    that’s implemented in the `Dlib` library works on different artificial datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Newman modularity-based graph clustering algorithm with Dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The implementation of this algorithm is based on the work *Modularity and community
    structure in networks*, by M. E. J. Newman. This algorithm is based on the modularity
    matrix for a network or a graph and it isn’t based on particular graph theory.
    However, it does have some similarities with spectral clustering because it also
    uses eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: The `Dlib` library implements this algorithm in the `newman_cluster()` function,
    which takes a vector of weighted graph edges and outputs the container with cluster
    indices for each vertex. The vector of weighted graph edges represents the connections
    between nodes in the network, with each edge having a weight that indicates its
    strength. The weights are used to determine the similarity between nodes and thus
    influence the clustering process. The initial step for using this algorithm is
    to define graph edges. In the following code sample, we’re making edges between
    almost every pair of dataset objects. Notice that we only use pairs with a distance
    greater than a threshold (this was done for performance considerations). The threshold
    distance can be adjusted to achieve different levels of granularity in the clustering
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, this algorithm doesn’t require prior knowledge of the number of clusters
    as it can determine the number of clusters by itself. Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `newman_cluster()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result. Notice that
    another approach for edge weight calculation can lead to another clustering result.
    Also, edge weight values should be initialized according to a certain task. The
    edge length was chosen only for demonstration purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.11 – D\uFEFFlib Newman clustering visualization](img/B19849_04_11.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Dlib Newman clustering visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the Newman clustering algorithm that’s
    implemented in the `Dlib` library works on different artificial datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Chinese Whispers – graph clustering algorithm with Dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Chinese Whispers algorithm is an algorithm that’s used to partition the
    nodes of weighted, undirected graphs. It was described in the paper *Chinese Whispers
    – an Efficient Graph Clustering Algorithm and its Application to Natural Language
    Processing Problems*, by Chris Biemann. This algorithm also doesn’t use any unique
    graph theory methods; instead, it uses the idea of using local contexts for clustering,
    so it can be classified as a density-based method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Dlib` library, this algorithm is implemented in the `chinese_whispers()`
    function, which takes the vector of weighted graph edges and outputs the container
    with cluster indices for each of the vertices. For performance considerations,
    we limit the number of edges between dataset objects with a threshold on distance.
    The meaning of weighted graph edges and threshold parameters are the same as for
    the Newman algorithm. Moreover, as with the Newman algorithm, this one also determines
    the number of resulting clusters by itself. The code can be seen in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `chinese_whispers()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result. Notice that
    we used `1` as the threshold for edge weights; another threshold value can lead
    to another clustering result. Also, edge weight values should be initialized according
    to a certain task. The edge length was chosen only for demonstration purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.12 – D\uFEFFlib Chinese Whispers clustering visualization](img/B19849_04_12.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Dlib Chinese Whispers clustering visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the Chinese Whispers clustering algorithm
    that’s implemented in the `Dlib` library works on different artificial datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In this and previous sections, we saw a lot of examples of images that show
    clustering results. The following section will explain how to use the `plotcpp`
    library, which we used to plot these images.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting data with C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After clustering, we plot the results with the `plotcpp` library, which is a
    thin wrapper around the `gnuplot` command-line utility. With this library, we
    can draw points on a scatter plot or draw lines. The initial step to start plotting
    with this library is creating an object of the `Plot` class. Then, we must specify
    the output destination of the drawing. We can set the destination with the `Plot::SetTerminal()`
    method, which takes a string with a destination point abbreviation. For example,
    we can use the `qt` string value to show the `Plot` class methods. However, it
    does not cover all possible configurations available for `gnuplot`. In cases where
    we need some unique options, we can use the `Plot::gnuplotCommand()` method to
    make a direct `gnuplot` configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two drawing approaches we can follow to draw a set of different graphics
    on one plot:'
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `Draw2D()` method with objects of the `Points` or `Lines` classes,
    but in this case, we should specify all graphics configurations before compilation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can use the `Plot::StartDraw2D()` method to get an intermediate drawing state
    object. Then, we can use the `Plot::AddDrawing()` method to add different drawings
    to one plot. The `Plot::EndDraw2D()` method should be called after we’ve drawn
    the last graphics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can use the `Points` type to draw points. An object of this type should be
    initialized with start and end forward iterators for the integral numeric data
    types, which represent coordinates. We should specify three iterators as points
    coordinates, two iterators for the *x* coordinates, which is where they start
    and end, and one iterator for the *y* coordinates’ start. The number of coordinates
    in the containers should be the same. The last parameter is the `gnuplot` visual
    style configuration. Objects of the `Lines` class can be configured in the same
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we’ve completed all drawing operations, we should call the `Plot::Flush()`
    method to render all commands to the window or the file, as shown in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we learned how to plot our clustering results with the `plotcpp`
    library. We must be able to configure different visualization parameters, such
    as the type of the plot, point colors, and axes names as these parameters make
    our plot more informative. We also learned how to save this plot in a file so
    that we can use it later or insert it into another document. This library will
    be used throughout this book for visualizing results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we considered what clustering is and how it differs from classification.
    We looked at different types of clustering methods, such as partition-based, spectral,
    hierarchical, density-based, and model-based methods. We also observed that partition-based
    methods can be divided into more categories, such as distance-based methods and
    graph theory-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we used implementations of these algorithms, including the k-means algorithm
    (the distance-based method), the GMM algorithm (the model-based method), the Newman
    modularity-based algorithm, and the Chinese Whispers algorithm, for graph clustering.
    We also learned how to use the hierarchical and spectral clustering algorithm
    implementations in programs. We saw that the crucial issues for successful clustering
    include the choice of the distance measure function, the initialization step,
    the splitting or merging strategy, and prior knowledge of the number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: A combination of these issues is unique for each specific algorithm. We also
    saw that a clustering algorithm’s results depend a lot on dataset characteristics
    and that we should choose the algorithm according to these.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, we studied how we can visualize clustering results
    with the `plotcpp` library.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll learn what a data anomaly is and what machine learning
    algorithms exist for anomaly detection. We’ll also see how anomaly detection algorithms
    can be used to solve real-life problems, and which properties of such algorithms
    play a more significant role in different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The 5 Clustering Algorithms Data Scientists Need to* *Know*: [https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Clustering*: [https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Different Types of Clustering* *Algorithm*: [https://www.geeksforgeeks.org/different-types-clustering-algorithm/](https://www.geeksforgeeks.org/different-types-clustering-algorithm/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*An introduction to clustering and different methods of* *clustering*: [https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/](https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graph theory introductory book: *Graph Theory* (*Graduate Texts in Mathematics*),
    by Adrian Bondy and U.S.R. Murty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Elements of Statistical Learning: Data Mining, Inference, and Prediction*,
    by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, covers a lot of aspects
    of machine learning theory and algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
