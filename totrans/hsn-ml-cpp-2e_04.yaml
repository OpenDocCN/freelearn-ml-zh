- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Clustering
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: '**Clustering** is an unsupervised machine learning method that’s used for splitting
    the original dataset of objects into groups classified by properties. In **machine
    learning**, an object is typically represented as a point in a multidimensional
    metric space. Every space dimension corresponds to an object property (feature),
    and the metric is a function of the values of these properties. Depending on the
    types of dimensions in this space, which can be both numerical and categorical,
    we choose a type of clustering algorithm and specific metric function. This choice
    depends on the nature of different object properties’ types.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**是一种无监督的机器学习方法，用于将原始数据集的对象分割成按属性分类的组。在**机器学习**中，一个对象通常被表示为多维度量空间中的一个点。每个空间维度对应一个对象属性（特征），度量是一个属性值的函数。根据这个空间中维度的类型，这些维度可以是数值的也可以是分类的，我们选择一种聚类算法和特定的度量函数。这种选择取决于不同对象属性类型的本质。'
- en: At the present stage, clustering is often used as the first step in data analysis.
    The task of clustering was formulated in scientific areas such as statistics,
    pattern recognition, optimization, and machine learning. At the time of writing,
    the number of methods for partitioning groups of objects into clusters is quite
    large—several dozen algorithms, and even more when you take into account their
    various modifications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前阶段，聚类通常被用作数据分析的第一步。聚类任务在诸如统计学、模式识别、优化和机器学习等科学领域被提出。在撰写本文时，将对象分组划分成聚类的数量方法相当庞大——几十种算法，甚至更多，当你考虑到它们的各种修改时。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Measuring distance in clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类中的距离度量
- en: Types of clustering algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类算法的类型
- en: Examples of using the `mlpack` library for dealing with the clustering task
    samples
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `mlpack` 库处理聚类任务样本的示例
- en: Examples of using the `Dlib` library for dealing with the clustering task samples
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `Dlib` 库处理聚类任务样本的示例
- en: Plotting data with C++
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 C++ 绘制数据
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You’ll require the following technologies and installations to complete this
    chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章需要以下技术和安装：
- en: A modern C++ compiler with C++17 support
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 C++17 的现代 C++ 编译器
- en: CMake build system version >= 3.8
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMake 构建系统版本 >= 3.8
- en: The `Dlib` library
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dlib` 库'
- en: The `mlpack` library
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlpack` 库'
- en: The `plotcpp` library
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plotcpp` 库'
- en: 'The code files for this chapter can be found in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在本书的 GitHub 仓库中找到：[https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter04)。
- en: Measuring distance in clustering
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类中的距离度量
- en: A metric or distance measure is essential in clustering because it determines
    the similarity between objects. However, before applying a distance measure to
    objects, we must make a vector of object characteristics; usually, this is a set
    of numerical values, such as human height or weight. Also, some algorithms can
    work with categorical object features (or characteristics). The standard practice
    is to normalize feature values. Normalization ensures that each feature has the
    same impact in a distance measure calculation. Many distance measure functions
    can be used in the scope of the clustering task. The most popular ones that are
    used for numerical properties are **Euclidean distance**, **squared Euclidean
    distance**, **Manhattan distance**, and **Chebyshev distance**. The following
    subsections describe them in detail.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，度量或距离度量是必不可少的，因为它决定了对象之间的相似性。然而，在将距离度量应用于对象之前，我们必须制作一个对象特征的向量；通常，这是一个数值集合，如人类身高或体重。此外，一些算法可以处理分类对象特征（或特性）。标准做法是对特征值进行归一化。归一化确保每个特征在距离度量计算中具有相同的影响。在聚类任务范围内可以使用许多距离度量函数。最常用的用于数值属性的函数是**欧几里得距离**、**平方欧几里得距离**、**曼哈顿距离**和**切比雪夫距离**。以下小节将详细描述它们。
- en: Euclidean distance
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'Euclidean distance is the most widely used distance measure. In general, this
    is a geometric distance in the multidimensional space. The formula for Euclidean
    distance is as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离是最常用的距离度量。一般来说，这是一个多维空间中的几何距离。欧几里得距离的公式如下：
- en: '![](img/B19849_Formula_012.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_012.jpg)'
- en: Squared Euclidean distance
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平方欧几里得距离
- en: 'Squared Euclidean distance has the same properties as Euclidean distance but
    assigns greater significance (weight) to the distant values than to closer ones.
    Here’s the formula for squared Euclidean distance:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 平方欧几里得距离具有与欧几里得距离相同的属性，但相对于较近的值，它赋予较远的值更大的重要性（权重）。以下是平方欧几里得距离的公式：
- en: '![](img/B19849_Formula_021.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_021.jpg)'
- en: Manhattan distance
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 曼哈顿距离
- en: 'Manhattan distance is an average difference by coordinates. In most cases,
    its value gives the same clustering results as Euclidean distance. However, it
    reduces the significance (weight) of the distant values (outliers). Here’s the
    formula for Manhattan distance:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离是坐标的平均差异。在大多数情况下，它的值给出的聚类结果与欧几里得距离相同。然而，它降低了远距离值（异常值）的重要性（权重）。以下是曼哈顿距离的公式：
- en: '![](img/B19849_Formula_031.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_031.jpg)'
- en: Chebyshev distance
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 切比雪夫距离
- en: 'Chebyshev distance can be useful when we need to classify two objects as different
    when they differ only by one of the coordinates. Here’s the formula for Chebyshev
    distance:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个对象仅在坐标中的一个坐标上不同时，切比雪夫距离可能很有用。以下是切比雪夫距离的公式：
- en: '![](img/B19849_Formula_041.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_041.jpg)'
- en: 'The following diagram shows the differences between the various distances:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了各种距离之间的差异：
- en: '![Figure 4.1 – The difference between different distance measures](img/B19849_04_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 不同距离度量之间的差异](img/B19849_04_01.jpg)'
- en: Figure 4.1 – The difference between different distance measures
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 不同距离度量之间的差异
- en: Here, we can see that *Manhattan* distance is the sum of the distances in both
    dimensions, like walking along city blocks. *Euclidean* distance is just the length
    of a straight line. *Chebyshev* distance is a more flexible alternative to *Manhattan*
    distance because diagonal moves are also taken into account.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到曼哈顿距离是两个维度距离的总和，就像在城市街区中行走一样。欧几里得距离只是直线长度。切比雪夫距离是曼哈顿距离的一个更灵活的替代方案，因为斜对角移动也被考虑在内。
- en: In this section, we became familiar with the main clustering concept, which
    is a distance measure. In the following section, we’ll discuss various types of
    clustering algorithms.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们熟悉了主要的聚类概念，即距离度量。在下一节中，我们将讨论各种类型的聚类算法。
- en: Types of clustering algorithms
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法的类型
- en: 'There are different types of clustering that we can classify into the following
    groups: **partition-based**, **spectral**, **hierarchical**, **density-based**,
    and **model-based**. The partition-based group of clustering algorithms can be
    logically divided into distance-based methods and ones based on graph theory.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将不同的聚类类型分类为以下几组：**基于划分的**、**基于谱的**、**层次化的**、**基于密度的**和**基于模型的**。基于划分的聚类算法可以逻辑上分为基于距离的方法和基于图论的方法。
- en: Before we cover different types of clustering algorithms, let’s understand the
    main difference between clustering and classification. The main difference between
    the two is an undefined set of target groups, which is determined by the clustering
    algorithm. The set of target groups (clusters) is the algorithm’s result.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍不同的聚类算法之前，让我们了解聚类和分类之间的主要区别。这两者之间的主要区别是未定义的目标组集合，该集合由聚类算法确定。目标组（簇）集合是算法的结果。
- en: 'We can split cluster analysis into the following phases:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将聚类分析分为以下阶段：
- en: Selecting objects for clustering
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择聚类对象
- en: Determining the set of object properties that we’ll use for the metric
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定我们将用于度量的对象属性集合
- en: Normalizing property values
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化属性值
- en: Calculating the metric
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算度量
- en: Identifying distinct groups of objects based on metric values
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据度量值识别不同的对象组
- en: After analyzing clustering results, some correction may be required for the
    selected metric of the chosen algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 分析聚类结果后，可能需要对所选算法的度量进行一些修正。
- en: 'We can use clustering for various real-world tasks, including the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用聚类来完成各种现实世界的任务，包括以下内容：
- en: Splitting news into several categories for advertisers
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新闻分割成几个类别供广告商使用
- en: Identifying customer groups by their preferences for market analysis
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过市场分析识别客户群体
- en: Identifying plant and animal groups for biological studies
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别用于生物研究的植物和动物群体
- en: Identifying and categorizing properties for city planning and management
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别和分类城市规划和管理中的属性
- en: Detecting earthquake epicenter clusters to identify danger zones
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测地震震中簇以确定危险区域
- en: Categorizing groups of insurance policyholders for risk management
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对保险政策持有人群体进行分类以进行风险管理
- en: Categorizing books in libraries
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图书馆中的书籍进行分类
- en: Searching for hidden structural similarities in the data
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据中寻找隐藏的结构相似性
- en: With that, let’s dive into the different types of clustering algorithms.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，让我们深入了解不同的聚类算法类型。
- en: Partition-based clustering algorithms
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于划分的聚类算法
- en: The partition-based methods use a similarity measure to combine objects into
    groups. A practitioner usually selects the similarity measure for such kinds of
    algorithms, using prior knowledge about a problem or heuristics to select the
    measure properly. Sometimes, several measures need to be tried with the same algorithm
    so that the best one can be chosen. Also, partition-based methods usually require
    either the number of desired clusters or a threshold that regulates the number
    of output clusters to be specified explicitly. The choice of a similarity measure
    can significantly affect the quality and accuracy of the clusters produced, potentially
    leading to misinterpretations of data patterns and insights.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于划分的方法使用相似度度量来将对象组合成组。从业者通常根据对问题的先验知识或启发式方法选择此类算法的相似度度量。有时，需要尝试几种度量与同一算法一起使用，以便选择最佳度量。此外，基于划分的方法通常需要显式指定所需簇的数量或调节输出簇数量的阈值。相似度度量的选择可以显著影响生成的簇的质量和准确性，可能导致对数据模式和洞察力的误解。
- en: Distance-based clustering algorithms
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于距离的聚类算法
- en: The most known representatives of this family of methods are the k-means and
    k-medoids algorithms. They take the *k* input parameter and divide the data space
    into *k* clusters so that the similarity between objects in one cluster is maximal.
    Also, they minimize the similarity between objects of different clusters. The
    similarity value is calculated as the distance from the object to the cluster
    center. The main difference between these methods lies in the way the cluster
    center is defined.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这类方法中最著名的代表是k-means和k-medoids算法。它们接受*k*个输入参数，并将数据空间划分为*k*个簇，使得一个簇中对象的相似度最大。同时，它们最小化不同簇中对象的相似度。相似度值是对象到簇中心的距离。这些方法之间的主要区别在于簇中心定义的方式。
- en: With the k-means algorithm, the similarity is proportional to the distance to
    the cluster center of mass. The cluster center of mass is the average value of
    cluster objects’ coordinates in the data space. The k-means algorithm can be briefly
    described with a few steps. First, we select *k* random objects and define each
    of them as a cluster prototype that represents the cluster’s center of mass. Then,
    the remaining objects are attached to the cluster with greater similarity. After
    that, the center of mass of each cluster is recalculated. For each obtained partition,
    a particular evaluation function is calculated, the values of which at each step
    form a converging series. This process continues until the specified series converges
    to its limit value.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k-means算法，相似度与对象到簇质心的距离成正比。簇质心是簇对象在数据空间中坐标的平均值。k-means算法可以简要描述为以下几步。首先，我们选择*k*个随机对象，并将每个对象定义为代表簇质心的簇原型。然后，将剩余的对象附加到具有更高相似度的簇上。之后，重新计算每个簇的质心。对于每个获得的划分，计算一个特定的评估函数，其值在每个步骤形成一个收敛序列。这个过程一直持续到指定的序列收敛到其极限值。
- en: In other words, moving objects from one cluster to another ends when the clusters
    remain unchanged. Minimizing the evaluation function allows the resulting clusters
    to be as compact and separate as possible. The k-means method works well when
    clusters are compact *clouds* that are significantly separated from each other.
    It’s useful for processing large amounts of data but isn’t applicable for detecting
    clusters of non-convex shapes or clusters with very different sizes. Moreover,
    the method is susceptible to noise and isolated points since even a small number
    of such points can significantly affect how the center mass of the cluster is
    calculated.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当簇保持不变时，将物体从一个簇移动到另一个簇的过程就会结束。最小化评估函数可以使生成的簇尽可能紧凑且分离。当簇是彼此显著分离的紧凑“云”时，k-means方法效果很好。它适用于处理大量数据，但不适用于检测非凸形状的簇或大小差异很大的簇。此外，该方法容易受到噪声和孤立点的影响，因为即使少量这样的点也会显著影响簇质心的计算。
- en: To reduce the influence of noise and isolated points on the clustering result,
    the k-medoids algorithm, in contrast to the k-means algorithm, uses one of the
    cluster objects (known as the representative object) as the center of the cluster.
    As in the k-means method, *k* representative objects are selected at random. Each
    of the remaining objects is combined into a cluster with the nearest representative
    object. Then, each representative object is replaced iteratively with an arbitrary
    unrepresentative object from the data space. The replacement process continues
    until the quality of the resulting clusters improves. The clustering quality is
    determined by the sum of deviations between objects and the representative object
    of the corresponding cluster, which the method tries to minimize. Thus, the iterations
    continue until the representative object in each of the clusters becomes the medoid.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少噪声和孤立点对聚类结果的影响，与k-means算法不同，k-medoids算法使用簇中的一个对象（称为代表对象）作为簇的中心。与k-means方法一样，随机选择*k*个代表对象。每个剩余的对象都与最近的代表对象组合成一个簇。然后，每个代表对象通过迭代地用数据空间中的一个任意非代表对象替换。替换过程继续进行，直到结果簇的质量提高。聚类质量由对象与对应簇的代表对象之间的偏差之和决定，该方法试图最小化这个偏差。因此，迭代继续进行，直到每个簇中的代表对象成为中位数。
- en: The **medoid** is the object closest to the center of the cluster. The algorithm
    is poorly scalable for processing large amounts of data, but this problem is solved
    by the **Clustering Large Applications based on RANdomized Search** (**CLARANS**)
    algorithm, which complements the k-medoids method. CLARANS attempts to address
    scalability issues by using a randomized search technique to find good solutions
    more efficiently. Such an approach makes it possible to quickly converge on a
    good solution without exhaustively searching all possible combinations of medoids.
    For multidimensional clustering, the **Projected Clustering** (**PROCLUS**) algorithm
    can be used.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**中位数**是距离簇中心最近的对象。该算法在处理大量数据时扩展性较差，但**基于随机搜索的聚类大应用**（**CLARANS**）算法解决了这个问题，它补充了k-medoids方法。CLARANS通过使用随机搜索技术来更有效地找到好的解决方案，试图解决可扩展性问题。这种方法使得能够快速收敛到一个好的解决方案，而无需搜索所有可能的medoids组合。对于多维聚类，可以使用**投影聚类**（**PROCLUS**）算法。'
- en: Graph theory-based clustering algorithms
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于图论的聚类算法
- en: The essence of algorithms based on graph theory is to represent target objects
    in graph form. Graph vertices correspond to objects, and the edge weights are
    equal to the distance between vertices. The advantages of graph clustering algorithms
    are their excellent visibility, relative ease of implementation, and their ability
    to make various improvements based on geometrical considerations. The main graph
    theory concepts used for clustering are selecting connected components, constructing
    a minimum spanning tree, and multilayer graph clustering.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图论的算法本质在于将目标对象以图的形式表示。图顶点对应于对象，边权重等于顶点间的距离。图聚类算法的优点在于其卓越的可视性、相对容易的实现以及基于几何考虑的各种改进能力。用于聚类的图论主要概念包括选择连通分量、构建最小生成树和多层图聚类。
- en: The algorithm for selecting connected components is based on the *R* input parameter,
    and the algorithm removes all edges in the graph with distances greater than *R*.
    Only the closest pairs of objects remain connected. The algorithm’s goal is to
    find the *R* value at which the graph collapses into several connected components.
    The resulting components are clusters. To select the *R* parameter, a histogram
    of the distribution of pairwise distances is usually constructed. For problems
    with a well-defined cluster data structure, there will be two peaks in the histogram—one
    corresponds to in-cluster distances and the second to inter-cluster distances.
    The *R* parameter is selected from the minimum zone between these peaks. Managing
    the number of clusters using the distance threshold can be difficult.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 选择连通分量的算法基于*R*输入参数，该算法移除图中距离大于*R*的所有边。只有最近的成对对象保持连接。算法的目标是找到使图塌缩为几个连通分量的*R*值。结果形成的分量即为聚类。为了选择*R*参数，通常构建成对距离分布的直方图。对于具有明确定义聚类数据结构的问题，直方图中将出现两个峰值——一个对应于簇内距离，另一个对应于簇间距离。*R*参数通常从这两个峰值之间的最小区域中选择。使用距离阈值管理簇的数量可能会很困难。
- en: 'The minimum spanning tree algorithm builds a minimal spanning tree on the graph,
    and then successively removes the edges with the highest weight. The following
    diagram shows the minimum spanning tree that’s been obtained for nine objects:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最小生成树算法在图上构建最小生成树，然后依次移除权重最高的边。以下图表显示了九个对象的最小生成树：
- en: '![Figure 4.2 – Spanning tree example](img/B19849_04_02.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 扩展树示例](img/B19849_04_02.jpg)'
- en: Figure 4.2 – Spanning tree example
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 扩展树示例
- en: 'By removing the link between *C* and *D*, with a length of 6 units (the edge
    with the maximum distance), we obtain two clusters: *{A, B, C}* and *{D, E, F,
    G, H, I}*. We can divide the second cluster into two more clusters by removing
    the *EF* edge, which has a length of 4 units.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过移除 *C* 和 *D* 之间的连接，长度为 6 个单位（最大距离的边），我们得到两个簇：*{A, B, C}* 和 *{D, E, F, G, H,
    I}*. 通过移除长度为 4 个单位的 *EF* 边，我们可以将第二个簇进一步划分为两个簇。
- en: The multilayer clustering algorithm is based on identifying connected components
    of a graph at some level of distance between objects (vertices). The threshold,
    *C,* defines the distance level—for example, if the distance between objects is
    ![](img/B19849_Formula_051.png), then ![](img/B19849_Formula_061.png).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 多层聚类算法基于在对象（顶点）之间某个距离级别上识别图的连通分量。阈值 *C* 定义了距离级别——例如，如果对象之间的距离是 ![](img/B19849_Formula_051.png)，则
    ![](img/B19849_Formula_061.png)。
- en: 'The layer clustering algorithm generates a sequence of sub-graphs of the graph,
    *G*, that reflect the hierarchical relationships between clusters, ![](img/B19849_Formula_071.png),
    where the following applies:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法生成图 *G* 的子图序列，这些子图反映了簇之间的层次关系，![](img/B19849_Formula_071.png)，其中以下适用：
- en: '![](img/B19849_Formula_081.png): A sub-graph on the ![](img/B19849_Formula_09.png)level'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B19849_Formula_081.png)：在 ![](img/B19849_Formula_09.png) 级别的子图'
- en: '![](img/B19849_Formula_101.png)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_101.png)'
- en: '![](img/B19849_Formula_112.png): The *t*th threshold of distance'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B19849_Formula_112.png)：距离的第 *t* 个阈值'
- en: '![](img/B19849_Formula_123.png): The number of hierarchy levels'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B19849_Formula_123.png)：层次级别的数量'
- en: '![](img/B19849_Formula_131.png), o: An empty set of graph edges, when ![](img/B19849_Formula_142.png)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_131.png)，o：当 ![](img/B19849_Formula_142.png) 时，一个空的图边集'
- en: '![](img/B19849_Formula_151.png): A graph of objects without thresholds on distance,
    when ![](img/B19849_Formula_162.png)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_151.png)：当 ![](img/B19849_Formula_162.png) 时，没有距离阈值的对象图'
- en: By changing the ![](img/B19849_Formula_171.png) distance thresholds, where ![](img/B19849_Formula_181.png),
    it’s possible to control the hierarchy depth of the resulting clusters. Thus,
    a multilayer clustering algorithm can create both flat and hierarchical data partitioning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变 ![](img/B19849_Formula_171.png) 距离阈值，其中 ![](img/B19849_Formula_181.png)，可以控制结果簇的层次深度。因此，多层聚类算法可以创建既平坦又分层的分区数据。
- en: Spectral clustering algorithms
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谱聚类算法
- en: Spectral clustering refers to all methods that divide a set of data into clusters
    using the eigenvectors of the adjacency matrix of a graph or other matrices derived
    from it. An adjacency matrix describes a complete graph with vertices in objects
    and edges between each pair of objects with a weight corresponding to the degree
    of similarity between these vertices. Spectral clustering involves transforming
    the initial set of objects into a set of points in space whose coordinates are
    elements of eigenvectors. The formal name for such a task is the **normalized**
    **cuts problem**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类指的是所有使用图邻接矩阵或由此派生的其他矩阵的特征向量将数据集划分为簇的方法。邻接矩阵描述了一个完全图，其中对象是顶点，每对对象之间的边具有与这些顶点之间相似度对应的权重。谱聚类涉及将初始对象集转换为空间中的一系列点，其坐标是特征向量的元素。此类任务的正式名称是
    **归一化** **切割问题**。
- en: The resulting set of points is then clustered using standard methods—for example,
    with the k-means algorithm. Changing the representation created by eigenvectors
    allows us to set the properties of the original set of clusters more clearly.
    Thus, spectral clustering can separate points that can’t be separated by applying
    k-means—for example, when the k-means method gets a convex set of points. The
    main disadvantage of spectral clustering is its cubic computational complexity
    and quadratic memory requirements.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用标准方法对得到的点集进行聚类——例如，使用k均值算法。改变由特征向量创建的表示，我们可以更清楚地设置原始簇集的性质。因此，谱聚类可以分离k均值方法无法分离的点——例如，当k均值方法得到一个凸点集时。谱聚类的主要缺点是其立方计算复杂度和二次内存需求。
- en: Hierarchical clustering algorithms
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次聚类算法
- en: 'Among the algorithms of hierarchical clustering, there are two main types:
    **bottom-up** and **top-down-based algorithms**. Top-down algorithms work on the
    principle that at the beginning, all objects are placed in one cluster, which
    is then divided into smaller and smaller clusters. Bottom-up algorithms are more
    common than top-down ones. They place each object in a separate cluster at the
    beginning of the work and then merge clusters into larger ones until all the objects
    in the dataset are contained in one cluster, building a system of nested partitions.
    The results of such algorithms are usually presented in tree form, called a **dendrogram**.
    A classic example of such a tree is the *Tree of Life*, which describes the classification
    of animals and plants.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在层次聚类算法中，有两种主要类型：**自底向上**和**基于自顶向下的算法**。自顶向下算法基于的原则是，最初，所有对象都放置在一个簇中，然后将其分割成越来越小的簇。自底向上算法比自顶向下算法更常见。它们在工作的开始时将每个对象放置在一个单独的簇中，然后合并簇以形成更大的簇，直到数据集中的所有对象都包含在一个簇中，构建一个嵌套分区系统。这些算法的结果通常以树形形式呈现，称为**树状图**。此类树的经典例子是*生命之树*，它描述了动物和植物的分类。
- en: The main problem with hierarchical methods is the difficulty of determining
    the stop condition in such a way as to isolate natural clusters and, at the same
    time, prevent their excessive splitting. Another problem with hierarchical clustering
    methods is choosing the point of separation or merging of clusters. This choice
    is critical because after splitting or merging clusters at each subsequent step,
    the method will operate only on newly formed clusters. Therefore, the wrong choice
    of a merge or split point at any step can lead to poor-quality clustering. Also,
    hierarchical methods can’t be applied to large datasets because deciding whether
    to divide or merge clusters requires a large number of objects and clusters to
    be analyzed, which leads to a significant computational complexity of the method.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类方法的主要问题是难以确定停止条件，以便隔离自然簇并防止其过度分裂。层次聚类方法的另一个问题是选择簇的分离或合并点。这个选择至关重要，因为在每个后续步骤中分裂或合并簇之后，该方法将仅对新形成的簇进行操作。因此，在任何步骤中错误地选择合并或分裂点可能导致聚类质量较差。此外，由于决定是否分割或合并簇需要分析大量对象和簇，因此层次聚类方法不能应用于大数据集，这导致该方法具有显著的计算复杂性。
- en: 'There are several metrics or linkage criteria for cluster union that are used
    in hierarchical clustering methods:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类方法中用于簇合并的几个度量或连接标准：
- en: '**Single linkage (nearest neighbor distance)**: In this method, the distance
    between the two clusters is determined by the distance between the two closest
    objects (nearest neighbors) in different clusters. The resulting clusters tend
    to chain together.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单连接（最近邻距离）**：在此方法中，两个簇之间的距离由不同簇中两个最近的对象（最近邻）之间的距离确定。得到的簇倾向于链状连接。'
- en: '**Complete linkage (distance between the most distant neighbors)**: In this
    method, the distances between clusters are determined by the largest distance
    between any two objects in different clusters (that is, the most distant neighbors).
    This method usually works very well when objects come from separate groups. If
    the clusters are elongated or their natural type is *chained*, then this method
    is unsuitable.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全连接（最远邻居之间的距离）**：在此方法中，簇之间的距离由不同簇中任意两个对象之间的最大距离（即最远邻居）确定。当对象来自不同的组时，此方法通常工作得非常好。如果簇是细长的或其自然类型是*链状*，则此方法不适用。'
- en: '**Unweighted pairwise mean linkage**: In this method, the distance between
    two different clusters is calculated as the average distance between all pairs
    of objects in them. This method is useful when objects form different groups,
    but it works equally well in the case of elongated (chained-type) clusters.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted pairwise mean linkage**: This method is identical to the unweighted
    pairwise mean method, except that the size of the corresponding clusters (the
    number of objects contained in them) is used as a weighting factor in the calculations.
    Therefore, this method should be used when we assume unequal cluster sizes.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted centroid linkage**: In this method, the distance between two clusters
    is defined as the distance between their centers of mass.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted centroid linkage** (**median**): This method is identical to the
    previous one, except that the calculations use weights for the distance measured
    between cluster sizes. Therefore, if there are significant differences in-cluster
    sizes, this method is preferable to the previous one.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram displays a hierarchical clustering dendrogram:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Hierarchical clustering example](img/B19849_04_03.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Hierarchical clustering example
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows an example of a dendrogram for hierarchical clustering,
    where you can see how the number of clusters depends on the distance between objects.
    Larger distances lead to a smaller number of clusters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Density-based clustering algorithms
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In density-based methods, clusters are considered as regions where the multiple
    objects’ density is high. This is separated by regions with a low density of objects.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Density-Based Spatial Clustering of Applications with Noise** (**DBSCAN**)
    algorithm is one of the first density clustering algorithms to be created. The
    basis of this algorithm is several statements, detailed as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: The ![](img/B19849_Formula_191.png) property of an object is the ![](img/B19849_Formula_20.png)
    radius neighborhood area around the object.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The root object is an object whose ![](img/B19849_Formula_211.png) contains
    a minimum non-zero number of objects. Assume that this minimum number equals a
    predefined value named *MinPts*.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *p* object is directly densely accessible from the *q* object if *p* is
    in the ![](img/B19849_Formula_221.png) property of *q* and *q* is the root object.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *p* object is densely accessible from the *q* object for the given ![](img/B19849_Formula_23.png)
    and *MinPts* if there’s a sequence of ![](img/B19849_Formula_24.png) objects,
    where ![](img/B19849_Formula_25.png) and ![](img/B19849_Formula_261.png), such
    that ![](img/B19849_Formula_271.png) is directly densely accessible from ![](img/B19849_Formula_282.png),
    ![](img/B19849_Formula_29.png).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *p* object is densely connected to the *q* object for the given ![](img/B19849_Formula_301.png)
    and *MinPts* if there’s an *o* object such that *p* and *q* are densely accessible
    from *o*.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DBSCAN algorithm checks the neighborhood of each object to search for clusters.
    If the ![](img/B19849_Formula_311.png) property of the *p* object contains more
    points than *MinPts*, then a new cluster is created with the *p* object as a root
    object. DBSCAN then iteratively collects objects directly densely accessible from
    root objects, which can lead to the union of several densely accessible clusters.
    The process ends when no new objects can be added to any cluster.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 算法检查每个对象的邻域以寻找聚类。如果 *p* 对象的 ![](img/B19849_Formula_311.png) 属性包含比 *MinPts*
    更多的点，则创建一个新的聚类，以 *p* 对象作为根对象。然后 DBSCAN 递归地收集直接从根对象可访问的对象，这可能导致几个密集可访问聚类的合并。当无法向任何聚类添加新对象时，过程结束。
- en: Unlike the partition-based methods, DBSCAN doesn’t require the number of clusters
    to be specified in advance; it only requires the ![](img/B19849_Formula_321.png)
    and *MinPts* values as these parameters directly affect the result of clustering.
    The optimal values of these parameters are difficult to determine, especially
    for multidimensional data spaces. Also, the distributed data in such spaces is
    often asymmetrical, which makes it impossible to use global density parameters
    for their clustering. For clustering multidimensional data spaces, there’s the
    **Subspace Clustering** (**SUBCLU**) algorithm, which is based on the DBSCAN algorithm.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于划分的方法不同，DBSCAN 不需要预先指定聚类数量；它只需要 ![](img/B19849_Formula_321.png) 和 *MinPts*
    值，因为这些参数直接影响聚类的结果。这些参数的最佳值很难确定，尤其是在多维数据空间中。此外，此类空间中的分布式数据通常是不对称的，这使得无法使用全局密度参数进行聚类。对于聚类多维数据空间，有基于
    DBSCAN 算法的 **子空间聚类**（**SUBCLU**）算法。
- en: The **MeanShift** approach also falls into the category of density-based clustering
    algorithms. It’s a non-parametric algorithm that shifts dataset points toward
    the center of the highest-density region within a certain radius. The algorithm
    makes such shifts iteratively until points converge to a local maximum of the
    density function. Such local maxima are also called the mode, so the algorithm
    is sometimes called mode-seeking. These local maximums represent the cluster centroids
    in the dataset.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**MeanShift** 方法也属于基于密度的聚类算法类别。它是一种非参数算法，将数据集点向一定半径内最高密度区域的中心移动。算法通过迭代进行这样的移动，直到点收敛到密度函数的局部最大值。这些局部最大值也被称为模式，因此该算法有时被称为模式寻找算法。这些局部最大值代表了数据集中的聚类中心。'
- en: Model-based clustering algorithms
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于模型的聚类算法
- en: Model-based algorithms assume that there’s a particular mathematical model of
    the cluster in the data space and try to maximize the likelihood of this model
    and the data available. Often, this uses the apparatus of mathematical statistics.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的算法假设数据空间中存在特定的聚类数学模型，并试图最大化该模型和可用数据的似然性。通常，这使用数学统计学的工具。
- en: The **Expectation-Maximization** (**EM**) algorithm assumes that the dataset
    can be modeled using a linear combination of multidimensional normal distributions.
    Its purpose is to estimate distribution parameters that maximize the likelihood
    function that’s used as a measure of model quality. In other words, it assumes
    that the data in each cluster obeys a particular distribution law—namely, the
    normal distribution. With this assumption, it’s possible to determine the optimal
    parameters of the distribution law—the mean and variance at which the likelihood
    function is maximal. Thus, we can assume that any object belongs to all clusters,
    but with a different probability. In this instance, the task will be to fit the
    set of distributions to the data and determine the probabilities of the object
    belonging to each cluster. The object should be assigned to the cluster for which
    this probability is higher than the others.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望最大化**（**EM**）算法假设数据集可以使用多维正态分布的线性组合进行建模。其目的是估计最大化似然函数的分布参数，该函数用作模型质量的度量。换句话说，它假设每个聚类中的数据遵循特定的分布定律——即正态分布。基于这个假设，可以确定分布定律的最佳参数——似然函数最大的均值和方差。因此，我们可以假设任何对象属于所有聚类，但概率不同。在这种情况下，任务将是将分布集拟合到数据中，并确定对象属于每个聚类的概率。对象应分配到概率高于其他聚类的聚类。'
- en: The EM algorithm is simple and easy to implement. It isn’t sensitive to isolated
    objects and quickly converges in the case of successful initialization. However,
    it requires us to specify *k* number of clusters, which implies a *priori* knowledge
    about the data. Also, if the initialization fails, the algorithm may be slow to
    converge, or we might obtain a poor-quality result. Such algorithms don’t apply
    to high-dimensionality spaces since, in this case, it’s complicated to assume
    a mathematical model for distributing data in this space.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the various types of clustering algorithms, let’s look
    at their uses in many industries to group similar data points into clusters. Here
    are some examples of how clustering algorithms can be applied:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer segmentation**: Clustering algorithms can be used to segment customers
    based on their purchase history, demographics, and other attributes. This information
    can then be used for targeted marketing campaigns, personalized product recommendations,
    and customer service.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image recognition**: In the field of computer vision, clustering algorithms
    are used to group images based on visual features such as color, texture, and
    shape. This can be useful for image classification, object detection, and scene
    understanding.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fraud detection**: In finance, clustering algorithms can detect suspicious
    transactions by grouping them based on similarities in transaction patterns. This
    helps to identify potential fraud and prevent financial losses.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommender systems**: In e-commerce, clustering algorithms group products
    based on customer preferences. This allows recommender systems to suggest relevant
    products to customers, increasing sales and customer satisfaction.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Social network analysis**: In social media, clustering algorithms identify
    groups of users with similar interests or behaviors. This enables targeted advertising,
    content creation, and community building.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Genomics**: In biology, clustering algorithms analyze gene expression data
    to identify groups of genes that are co-expressed under specific conditions. This
    aids in understanding gene function and disease mechanisms.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text mining**: In natural language processing, clustering algorithms categorize
    documents based on their content. This is useful for topic modeling, document
    classification, and information retrieval.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of the wide range of applications of clustering
    algorithms. The specific use case will depend on the industry, dataset, and business
    objectives.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed various clustering algorithms and their uses.
    In the following sections, we’ll learn how to use them in real-world examples
    with various C++ libraries.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Examples of using the mlpack library for dealing with the clustering task samples
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `mlpack` library contains implementations of the model-based, density-based,
    and partition-based clustering approaches. The model-based algorithm is called
    **Gaussian Mixture Models** (**GMM**) and is based on EM, while the partition-based
    algorithm is the k-means algorithm. There are two density-based algorithms we
    can use: DBSCAN and MeanShift clustering.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: GMM and EM with mlpack
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The GMM algorithm assumes that clusters can be fit to some Gaussian (normal)
    distributions; it uses the EM approach for training. There are the `GMM` and `mlpack`
    library that implement this approach, as illustrated in the following code snippet:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Notice that the constructor of the `GMM` class takes the desired number of
    clusters and feature dimensionality as an argument. After `GMM` object initialization,
    the object of the `EMFit` class was initialized with maximum iterations, tolerance,
    and a clustering object. The tolerance parameter in `EMFit` controls how similar
    two points must be to be considered as part of the same cluster. A higher tolerance
    value means that the algorithm will group more points, resulting in fewer clusters.
    Conversely, a lower tolerance value leads to more clusters with fewer points in
    each one. The clustering object—in our case, `kmeans`—will be used by the algorithm
    to find initial centroids for Gaussian fitting. Then, we passed the training features
    and the `EM` object into the training method. Now, we have the trained `GMM` model.
    In the `mlpack` library, the trained `gmm` object should be used to classify new
    feature points, but we can use it to show cluster assignments for the original
    data that we used for training. The following piece of code shows these steps
    and also plots the results of clustering:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we used the `GMM::Classify()` method to identify which cluster our objects
    belong to. This method filled a row vector of cluster identifiers per element
    that corresponds to the input data. The resulting cluster indices were used for
    filling the `plot_clusters` container. This container maps cluster indices with
    input data coordinates for plotting. It was used as an argument for the `PlotClusters()`
    function, which visualized the clustering result, as illustrated in the following
    figure:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – MLPack GMM clustering visualization](img/B19849_04_04.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – mlpack GMM clustering visualization
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding picture, we can see how the GMM and EM algorithms work on different
    artificial datasets.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering with mlpack
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The k-means algorithm in the `mlpack` library is implemented in the `KMeans`
    class. The constructor of this class takes several parameters, with the most important
    ones being the number of iterations and the object for distance metric calculation.
    In the following example, we’ll use the default values so that the constructor
    will be called without parameters. Once we’ve constructed an object of the `KMeans`
    type, we’ll use the `KMeans::Cluster()` method to run the algorithm and assign
    a cluster label to each of the input elements, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result of clusterization is the `assignments` container object with labels.
    Notice that the desired number of clusters was passed as an argument for the `Cluster`
    method. The following code sample shows how to plot the results of clustering:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we can see, the code for visualization is the same as it was for the previous
    example—it’s the result of the uniform clustering API in the `mlpack` library.
    We received the same `assignments` container that we converted in the data structure,
    which is suitable for the visualization library we’re using, and called the `PlotClusters`
    function. The visualization result is illustrated in the following figure:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – MLPack K-means clustering visualization](img/B19849_04_05.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – mlpack K-means clustering visualization
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the k-means algorithm works on different
    artificial datasets.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN with mlpack
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `DBSCAN` class implements the corresponding algorithm in the `mlpack` library.
    The constructor of this class takes several parameters, but the two most important
    are the epsilon and the minimum points number. In the following code snippet,
    we’re creating the object of this class:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, `epsilon` is the radius of a range search, while `min_points` is the
    minimum number of points required to form a cluster. After constructing an object
    of the `BSSCAN` type, we can use the `Cluster()` method to run the algorithm and
    assign a cluster label to each of the input elements, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The result of clusterization is an `assignments` container object with labels.
    Notice that for this algorithm, we didn’t specify the desired number of clusters
    because the algorithm determined them by itself. The code for the visualization
    is the same as for the previous examples—we convert the `assignments` container
    into a data structure that’s suitable for the visualization library we’re using
    and call the `PlotClusters` function. The following figure shows the DBSCAN clustering
    visualization result:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – MLPack DBSCAN clustering visualization](img/B19849_04_06.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – mlpack DBSCAN clustering visualization
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the DBSCAN algorithm works on different
    artificial datasets. The main difference from previous algorithms is the bigger
    number of clusters that the algorithm found. From this, we can see that their
    centroids are near some local density maximums.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: MeanShift clustering with mlpack
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `MeanShift` class implements the corresponding algorithm in the `mlpack`
    library. The constructor of this class takes several parameters, with the most
    important one being the density region search radius. It’s quite tricky to manually
    find the appropriate value for this parameter. However, the library gives us a
    very useful method to determine it automatically. In the following code snippet,
    we’re creating an object of the `MeanShift` class without specifying the radius
    parameter explicitly:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we used the `EstimateRadius` method to get the automatic radius estimation,
    which is based on the `MeanShift` object with the appropriate search radius value,
    we can use the `Cluster()` method to run the algorithm and assign a cluster label
    to each of the input elements, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The result of clusterization is the `assignments` container object with labels
    and an additional matrix that contains the cluster centroids coordinates. For
    this algorithm, we also didn’t specify the number of clusters because the algorithm
    determined them by itself. The code for visualization is the same as for the previous
    examples—we convert the `assignments` container into a data structure suitable
    for the visualization library we’re using and call the `PlotClusters` function.
    The following figure shows the `MeanShift` clustering visualization result:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – MLPack MeanShift clustering visualization](img/B19849_04_07.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – mlpack MeanShift clustering visualization
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows how the MeanShift algorithm works on different artificial
    datasets. We can see that the results are somehow similar to those for K-means
    clustering but the number of clusters was determined automatically. We can also
    see that in the one of datasets, the algorithm failed to get the correct number
    of clusters, so we must perform experiments with search radius values to get more
    precise clustering results.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Examples of using the Dlib library for dealing with the clustering task samples
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Dlib` library provides k-means, spectral, hierarchical, and two more graph
    clustering algorithms—**Newman** and **Chinese Whispers**—as clustering methods.
    Let’s take a look.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering with Dlib
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Dlib` library uses kernel functions as the distance functions for the
    k-means algorithm. An example of such a function is the radial basis function.
    As an initial step, we define the required types, as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we initialize an object of the `kkmeans` type. Its constructor takes
    an object that will define cluster centroids as input parameters. We can use an
    object of the `kcentroid` type for this purpose. Its constructor takes three parameters:
    the first one is the object that defines the kernel (distance function), the second
    is the numerical accuracy for the centroid estimation, and the third is the upper
    limit on the runtime complexity (actually, the maximum number of dictionary vectors
    the `kcentroid` object is allowed to use), as illustrated in the following code
    snippet:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As a next step, we initialize cluster centers with the `pick_initial_centers()`
    function. This function takes the number of clusters, the output container for
    center objects, the training data, and the distance function object as parameters,
    as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When initial centers are selected, we can use them for the `kkmeans::train()`
    method to determine exact clusters, as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We used the `kmeans` object as a functor to perform clustering on a single
    data item. The clustering result will be the cluster’s index for the item. Then,
    we used cluster indices to visualize the final clustering result, as illustrated
    in the following figure:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.8 – D\uFEFFlib K-means clustering visualization](img/B19849_04_08.jpg)"
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Dlib K-means clustering visualization
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the k-means clustering algorithm that’s
    implemented in the `Dlib` library works on different artificial datasets.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering with Dlib
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The spectral clustering algorithm in the `Dlib` library is implemented in the
    `spectral_cluster` function. It takes the distance function object, the training
    dataset, and the number of clusters as parameters. As a result, it returns a container
    with cluster indices, which have the same ordering as the input data. In the following
    sample, an object of the `knn_kernel` type is used as a distance function. You’ll
    find its implementation in the samples provided in this book. This `knn_kernel`
    distance function object estimates the first KNN objects to the given one. These
    objects are determined with the KNN algorithm, which uses the Euclidean distance
    for the distance measure, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `spectral_cluster()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result, as illustrated
    in the following figure:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.9 – D\uFEFFlib spectral clustering visualization](img/B19849_04_09.jpg)"
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Dlib spectral clustering visualization
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the spectral clustering algorithm that’s
    implemented in the `Dlib` library works on different artificial datasets.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering with Dlib
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Dlib` library implements the agglomerative hierarchical (bottom-up) clustering
    algorithm. The `bottom_up_cluster()` function implements this algorithm. This
    function takes the matrix of distances between dataset objects, the cluster indices
    container (as the output parameter), and the number of clusters as input parameters.
    Note that it returns the container with cluster indices in the order of distances
    provided in the matrix.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code sample, we’ve filled the distance matrix with pairwise
    Euclidean distances between each pair of elements in the input dataset:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `bottom_up_cluster()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result, as illustrated
    in the following figure:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.10 – D\uFEFFlib hierarchical clustering visualization](img/B19849_04_10.jpg)"
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Dlib hierarchical clustering visualization
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the hierarchical clustering algorithm
    that’s implemented in the `Dlib` library works on different artificial datasets.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Newman modularity-based graph clustering algorithm with Dlib
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The implementation of this algorithm is based on the work *Modularity and community
    structure in networks*, by M. E. J. Newman. This algorithm is based on the modularity
    matrix for a network or a graph and it isn’t based on particular graph theory.
    However, it does have some similarities with spectral clustering because it also
    uses eigenvectors.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The `Dlib` library implements this algorithm in the `newman_cluster()` function,
    which takes a vector of weighted graph edges and outputs the container with cluster
    indices for each vertex. The vector of weighted graph edges represents the connections
    between nodes in the network, with each edge having a weight that indicates its
    strength. The weights are used to determine the similarity between nodes and thus
    influence the clustering process. The initial step for using this algorithm is
    to define graph edges. In the following code sample, we’re making edges between
    almost every pair of dataset objects. Notice that we only use pairs with a distance
    greater than a threshold (this was done for performance considerations). The threshold
    distance can be adjusted to achieve different levels of granularity in the clustering
    results.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, this algorithm doesn’t require prior knowledge of the number of clusters
    as it can determine the number of clusters by itself. Here’s the code:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `newman_cluster()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result. Notice that
    another approach for edge weight calculation can lead to another clustering result.
    Also, edge weight values should be initialized according to a certain task. The
    edge length was chosen only for demonstration purposes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'The result can be seen in the following figure:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.11 – D\uFEFFlib Newman clustering visualization](img/B19849_04_11.jpg)"
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Dlib Newman clustering visualization
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the Newman clustering algorithm that’s
    implemented in the `Dlib` library works on different artificial datasets.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Chinese Whispers – graph clustering algorithm with Dlib
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Chinese Whispers algorithm is an algorithm that’s used to partition the
    nodes of weighted, undirected graphs. It was described in the paper *Chinese Whispers
    – an Efficient Graph Clustering Algorithm and its Application to Natural Language
    Processing Problems*, by Chris Biemann. This algorithm also doesn’t use any unique
    graph theory methods; instead, it uses the idea of using local contexts for clustering,
    so it can be classified as a density-based method.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Dlib` library, this algorithm is implemented in the `chinese_whispers()`
    function, which takes the vector of weighted graph edges and outputs the container
    with cluster indices for each of the vertices. For performance considerations,
    we limit the number of edges between dataset objects with a threshold on distance.
    The meaning of weighted graph edges and threshold parameters are the same as for
    the Newman algorithm. Moreover, as with the Newman algorithm, this one also determines
    the number of resulting clusters by itself. The code can be seen in the following
    snippet:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `chinese_whispers()` function call filled the `clusters` object with cluster
    index values, which we can use to visualize the clustering result. Notice that
    we used `1` as the threshold for edge weights; another threshold value can lead
    to another clustering result. Also, edge weight values should be initialized according
    to a certain task. The edge length was chosen only for demonstration purposes.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'The result can be seen in the following figure:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.12 – D\uFEFFlib Chinese Whispers clustering visualization](img/B19849_04_12.jpg)"
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Dlib Chinese Whispers clustering visualization
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see how the Chinese Whispers clustering algorithm
    that’s implemented in the `Dlib` library works on different artificial datasets.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: In this and previous sections, we saw a lot of examples of images that show
    clustering results. The following section will explain how to use the `plotcpp`
    library, which we used to plot these images.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Plotting data with C++
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After clustering, we plot the results with the `plotcpp` library, which is a
    thin wrapper around the `gnuplot` command-line utility. With this library, we
    can draw points on a scatter plot or draw lines. The initial step to start plotting
    with this library is creating an object of the `Plot` class. Then, we must specify
    the output destination of the drawing. We can set the destination with the `Plot::SetTerminal()`
    method, which takes a string with a destination point abbreviation. For example,
    we can use the `qt` string value to show the `Plot` class methods. However, it
    does not cover all possible configurations available for `gnuplot`. In cases where
    we need some unique options, we can use the `Plot::gnuplotCommand()` method to
    make a direct `gnuplot` configuration.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two drawing approaches we can follow to draw a set of different graphics
    on one plot:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `Draw2D()` method with objects of the `Points` or `Lines` classes,
    but in this case, we should specify all graphics configurations before compilation.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can use the `Plot::StartDraw2D()` method to get an intermediate drawing state
    object. Then, we can use the `Plot::AddDrawing()` method to add different drawings
    to one plot. The `Plot::EndDraw2D()` method should be called after we’ve drawn
    the last graphics.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can use the `Points` type to draw points. An object of this type should be
    initialized with start and end forward iterators for the integral numeric data
    types, which represent coordinates. We should specify three iterators as points
    coordinates, two iterators for the *x* coordinates, which is where they start
    and end, and one iterator for the *y* coordinates’ start. The number of coordinates
    in the containers should be the same. The last parameter is the `gnuplot` visual
    style configuration. Objects of the `Lines` class can be configured in the same
    way.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we’ve completed all drawing operations, we should call the `Plot::Flush()`
    method to render all commands to the window or the file, as shown in the following
    code block:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this example, we learned how to plot our clustering results with the `plotcpp`
    library. We must be able to configure different visualization parameters, such
    as the type of the plot, point colors, and axes names as these parameters make
    our plot more informative. We also learned how to save this plot in a file so
    that we can use it later or insert it into another document. This library will
    be used throughout this book for visualizing results.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we considered what clustering is and how it differs from classification.
    We looked at different types of clustering methods, such as partition-based, spectral,
    hierarchical, density-based, and model-based methods. We also observed that partition-based
    methods can be divided into more categories, such as distance-based methods and
    graph theory-based methods.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Then, we used implementations of these algorithms, including the k-means algorithm
    (the distance-based method), the GMM algorithm (the model-based method), the Newman
    modularity-based algorithm, and the Chinese Whispers algorithm, for graph clustering.
    We also learned how to use the hierarchical and spectral clustering algorithm
    implementations in programs. We saw that the crucial issues for successful clustering
    include the choice of the distance measure function, the initialization step,
    the splitting or merging strategy, and prior knowledge of the number of clusters.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: A combination of these issues is unique for each specific algorithm. We also
    saw that a clustering algorithm’s results depend a lot on dataset characteristics
    and that we should choose the algorithm according to these.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, we studied how we can visualize clustering results
    with the `plotcpp` library.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll learn what a data anomaly is and what machine learning
    algorithms exist for anomaly detection. We’ll also see how anomaly detection algorithms
    can be used to solve real-life problems, and which properties of such algorithms
    play a more significant role in different tasks.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '*The 5 Clustering Algorithms Data Scientists Need to* *Know*: [https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Clustering*: [https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Different Types of Clustering* *Algorithm*: [https://www.geeksforgeeks.org/different-types-clustering-algorithm/](https://www.geeksforgeeks.org/different-types-clustering-algorithm/)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*An introduction to clustering and different methods of* *clustering*: [https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/](https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graph theory introductory book: *Graph Theory* (*Graduate Texts in Mathematics*),
    by Adrian Bondy and U.S.R. Murty'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Elements of Statistical Learning: Data Mining, Inference, and Prediction*,
    by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, covers a lot of aspects
    of machine learning theory and algorithms'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
