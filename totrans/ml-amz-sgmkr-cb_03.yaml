- en: '*Chapter 2*: Machine Learning Basics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*：机器学习基础'
- en: This chapter covers some basic concepts of machine learning that will be used
    and referenced in this book. This is the bare minimum you need to know in order
    to use DataRobot effectively. Experienced data scientists can safely skip this
    chapter. It is not the intention of this chapter to give you a comprehensive understanding
    of statistics or machine learning, but just a refresher of some key ideas and
    concepts. Also, the focus is on practical aspects of what you need to know in
    order to understand the core ideas without going into too much detail. It might
    be tempting to jump in and let DataRobot automatically build the models, but doing
    that without a basic understanding could backfire. If you are leading a data science
    team, please make sure that you have experienced data scientists in your teams
    who are mentoring others and that there are other governance processes in place.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了本书中将使用和参考的一些机器学习的基本概念。这是您为了有效地使用 DataRobot 所需了解的最基本知识。经验丰富的数据科学家可以安全地跳过本章。本章的目的不是让您对统计学或机器学习有一个全面的理解，而是对一些关键思想和概念进行复习。此外，重点在于了解您需要了解的实践方面，以便理解核心思想，而不必过多地深入细节。您可能会想直接跳入并让
    DataRobot 自动构建模型，但如果没有基本理解，这样做可能会适得其反。如果您是数据科学团队的领导者，请确保您的团队中有经验丰富的数据科学家，他们正在指导他人，并且已经建立了其他治理流程。
- en: Some of these concepts will come up again during the hands-on examples, but
    we are covering many concepts here that might not come up during a specific example,
    but might come up in relation to your project at some point. The topics listed
    here can be used as a guide to determine some of the basic knowledge that you
    require in order to start using powerful tools such as DataRobot.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作示例中，一些概念将会再次出现，但在这里我们涵盖了可能不会在特定示例中出现，但可能在某个时候与您的项目相关的许多概念。这里列出的主题可以用作指南，以确定您开始使用
    DataRobot 等强大工具所需的一些基本知识。
- en: 'By the end of this chapter, you will have learned some of the core concepts
    you need to know to use DataRobot effectively. In this chapter, we''re going to
    cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将学习到使用 DataRobot 有效地所需的一些核心概念。在本章中，我们将涵盖以下主要主题：
- en: Data preparation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Data visualization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Machine learning algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: Performance metrics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能指标
- en: Understanding the results
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解结果
- en: Data preparation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Before an algorithm can be applied to a dataset, the dataset needs to fit a
    certain pattern. The dataset also needs to be free of errors. Certain methods
    and techniques are used to ensure that the dataset is ready for the algorithms,
    and this will be the focus of this section.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法可以应用于数据集之前，数据集需要符合一定的模式。数据集还需要没有错误。某些方法和技术被用来确保数据集为算法做好准备，这将是本节的重点。
- en: Supervised learning dataset
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习数据集
- en: 'Since DataRobot mostly works with supervised learning problems, we will only
    focus on datasets for supervised machine learning (other types will be covered
    in a later section). In a supervised machine learning problem, we provide all
    the answers as part of the dataset. Imagine a table of data where each row represents
    a set of clues with their corresponding answers (*Figure 2.1*):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DataRobot 主要处理监督学习问题，因此我们只关注监督机器学习的数据集（其他类型将在后面的章节中介绍）。在监督机器学习问题中，我们将所有答案作为数据集的一部分提供。想象一下一张数据表，其中每一行代表一组线索及其相应的答案（*图
    2.1*）：
- en: '![Figure 2.1 – Supervised learning dataset'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1 – 监督学习数据集'
- en: '](img/Figure_2.1_B17159.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.1_B17159.jpg)'
- en: Figure 2.1 – Supervised learning dataset
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 监督学习数据集
- en: This dataset is made up of columns that contain clues (these are called **features**),
    and there is a column with the answers (this is called **target**). Given a dataset
    that looks like this, the algorithm learns how to produce the right answer given
    a set of clues. No matter what form your data is in, your task is to first transform
    it to make it look like the table in *Figure 2.1*. Note that the clues that you
    have might be spread across multiple databases or Excel files. You will have to
    compile all of that information into one table. If the datasets you have are complex,
    you will need to use languages such as SQL, tools such as **Python** **Pandas**,
    or **Excel**, or tools such as **Paxata**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集由包含线索（这些被称为**特征**）的列组成，还有一个包含答案的列（这被称为**目标**）。给定一个类似这样的数据集，算法学习如何根据一组线索产生正确的答案。无论你的数据以何种形式存在，你的任务首先是将其转换成类似于*图2.1*中的表格。请注意，你可能拥有的线索可能分布在多个数据库或Excel文件中。你必须将这些信息全部汇总到一个表格中。如果你拥有的数据集很复杂，你可能需要使用SQL等语言，**Python
    Pandas**或**Excel**等工具，或者**Paxata**等工具。
- en: Time series datasets
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列数据集
- en: 'Time series or forecasting problems have time as a key component of their datasets.
    They are similar to the supervised learning datasets, with slight differences,
    as shown in *Figure 2.2*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列或预测问题将时间作为数据集的关键组成部分。它们与监督学习数据集类似，略有不同，如*图2.2*所示：
- en: '![Figure 2.2 – Time series dataset'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2 – 时间序列数据集'
- en: '](img/Figure_2.2_B17159.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_2.2_B17159.jpg)'
- en: Figure 2.2 – Time series dataset
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 时间序列数据集
- en: You need to make sure that your time series datasets appear as shown in the
    preceding diagram. It should have a date or time-based column, and a column with
    the series values you are trying to forecast, and a set of clues as needed. You
    can also add columns that help to categorize different series, if there are multiple
    time series that you need to forecast. For example, you might be interested in
    forecasting units sold for dates 5 and 6\. If your data is in some other form,
    it needs to be transformed to look like the preceding diagram.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要确保你的时间序列数据集看起来与前面的图示一样。它应该有一个基于日期或时间的列，一个包含你试图预测的序列值的列，以及所需的线索列。如果你需要预测多个时间序列，你也可以添加帮助分类不同序列的列。例如，你可能对预测日期5和6的销量感兴趣。如果你的数据以其他形式存在，它需要被转换成类似于前面的图示。
- en: Data cleansing
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清洗
- en: 'The data that comes to you will typically have errors in it. For example, you
    might have text in a field that is supposed to contain numbers. You might see
    a price column where the values may contain a $ sign on occasion, but no sign
    at other times. DataRobot can catch some of these, but there are times when an
    automated tool will not catch these, so you need to look and analyze the dataset
    carefully. It is useful to sometimes upload your data to DataRobot to see what
    it finds, and then use its analysis to determine the next steps. Some of this
    cleansing will need to be performed outside DataRobot, so be prepared to iterate
    a few times to get the data set up correctly. Common issues to watch out for include
    the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你收到的数据通常会有错误。例如，你可能在应该包含数字的字段中看到文本。你可能会看到价格列中的值偶尔包含美元符号，但其他时候没有。DataRobot可以捕捉到其中的一些，但有时自动化工具不会捕捉到这些错误，所以你需要仔细查看和分析数据集。有时将你的数据上传到DataRobot以查看它发现了什么，然后使用其分析来确定下一步行动是有用的。一些清洗工作需要在DataRobot之外完成，所以请准备好迭代几次以正确设置数据集。需要注意的常见问题包括以下内容：
- en: Wrong data type in a column
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列中的数据类型错误
- en: Mixed data types in a column
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列中的混合数据类型
- en: Spaces or other characters in numeric columns that make them look like text
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字列中的空格或其他字符使它们看起来像文本
- en: Synonyms or misspelled words
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同义词或拼写错误
- en: Dates encoded as strings
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为字符串编码的日期
- en: Dates with differing formats
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期格式不同
- en: Data normalization and standardization
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据归一化和标准化
- en: When different data features have varying scales and ranges, it becomes harder
    to compare their impacts on the target values. Also, many algorithms have difficulty
    in dealing with different scales of values, sometimes leading to stability issues.
    One method for avoiding these problems is to normalize (not to be confused with
    database normal forms) or standardize the values.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当不同的数据特征具有不同的规模和范围时，比较它们对目标值的影响变得更加困难。此外，许多算法在处理不同规模的数据值时存在困难，有时会导致稳定性问题。避免这些问题的方法之一是对值进行归一化（不要与数据库范式混淆）或标准化。
- en: 'In normalization (also known as scaling), you scale the values such that they
    range from 0 to 1:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在归一化（也称为缩放）中，您将值缩放，使它们在 0 到 1 之间：
- en: Xnormalized = (X – Xmin) / (Xmax – Xmin)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Xnormalized = (X – Xmin) / (Xmax – Xmin)
- en: 'Standardization, on the other hand, centers the data such that the mean becomes
    zero and scales it such that the standard deviation becomes 1\. This is also known
    as **z-scoring** the data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，标准化会将数据中心化，使得平均值变为零，并按比例缩放，使得标准差变为 1。这也被称为数据的 **z 分数**：
- en: Xstandardized = (X – Xmean) / XSD
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Xstandardized = (X – Xmean) / XSD
- en: Here, Xmean is the mean of all X values, and XSD is the standard deviation of
    X values.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Xmean 是所有 X 值的平均值，XSD 是 X 值的标准差。
- en: In general, you will not need to worry about this because DataRobot automatically
    does this for the datasets as required.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您不必担心这个问题，因为 DataRobot 会根据需要自动对数据集进行此操作。
- en: Outliers
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常值
- en: Outliers are values that seem to be out of place compared to the rest of the
    dataset. These values can be very large or very small. In general, values that
    are more than three standard deviations from the mean are considered outliers,
    but this only applies to features where values are expected to be normally distributed.
    Outliers typically come from data quality issues or some unusual situations that
    are not considered relevant enough to be trained on. The data points deemed to
    be outliers are typically removed from the dataset to prevent them from overpowering
    your models. The rules of thumb are only for highlighting the candidates. You
    will have to use your judgment to determine whether any values are outliers and
    whether they need to be removed. Once again, DataRobot will highlight potential
    outliers, but you will have to review those data points and determine whether
    to remove them or leave them in.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是相对于其他数据集值似乎不合适的数据。这些值可能非常大或非常小。一般来说，与平均值相差超过三个标准差的数据被视为异常值，但这仅适用于预期值呈正态分布的特征。异常值通常来自数据质量问题或一些不寻常的情况，这些情况被认为不足以作为训练数据。通常，被认为异常的数据点会被从数据集中移除，以防止它们对模型产生过度影响。这些经验法则仅用于突出候选值。您将必须运用自己的判断来确定是否有任何值是异常值，以及是否需要将其移除。再次强调，DataRobot
    将突出潜在的异常值，但您必须审查这些数据点并确定是否将其移除或保留。
- en: Missing values
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失值
- en: This is a very common problem in datasets. Your dataset may contain many missing
    values, marked as **NULL** or **NaN**. In some cases, you will see a **?**, or
    you might see an unusual value, such as **-999**, that an organization might be
    using to represent a missing or unknown value. How you choose to handle such values
    depends a lot on the problem you are trying to solve and what the dataset represents.
    Many times, you might choose to remove the row of data that contains a missing
    value. Sometimes, that is not possible because you might not have enough data,
    and removing such rows might lead to the removal of a significant portion of your
    dataset. Sometimes, you will see a large number of values in a feature (or column)
    that might be missing. In those situations, you might want to remove that feature
    from the dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在数据集中非常常见的问题。您的数据集可能包含许多标记为 **NULL** 或 **NaN** 的缺失值。在某些情况下，您可能会看到一个 **?**，或者您可能会看到一个组织可能用来表示缺失或未知值的异常值，例如
    **-999**。您如何处理这些值在很大程度上取决于您试图解决的问题以及数据集代表的内容。很多时候，您可能会选择删除包含缺失值的行数据。有时，这可能不可行，因为您可能没有足够的数据，删除这些行可能会导致您数据集的重要部分被移除。有时，您可能会看到一个特征（或列）中有大量可能缺失的值。在这些情况下，您可能希望从数据集中删除该特征。
- en: Another possible way of dealing with this situation is to fill the missing values
    with a reasonable guess. This could take the form of a zero value, or the mean
    value for that feature, or a median value of that feature. For categorical data,
    missing values are typically treated as their own separate category.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这种情况的另一种可能方法是使用合理的猜测来填充缺失值。这可能采取零值、该特征的平均值或该特征的中间值的形式。对于分类数据，缺失值通常被视为一个单独的类别。
- en: More sophisticated methods use the k-nearest neighbor algorithm to compute missing
    values based on other similar data points. No one answer will be appropriate every
    time, so you will need to use your judgment and understanding of the problem to
    make a decision. One final option is to leave it as it is and let DataRobot figure
    out how to deal with the situation. DataRobot has many imputation strategies as
    well as algorithms to handle missing values. But you have to be careful, as that
    might not always lead to the best solution. Talk to an experienced data scientist
    and use your understanding of the business problem to plot a course of action.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的方法使用k最近邻算法根据其他相似数据点计算缺失值。没有一个答案在每次都适用，所以你需要使用你的判断和对问题的理解来做出决定。最后一个选项是让它保持原样，让DataRobot找出如何处理这种情况。DataRobot有许多插补策略以及处理缺失值的算法。但你必须小心，因为这可能并不总是导致最佳解决方案。与经验丰富的数据科学家交谈，并使用你对业务问题的理解来规划行动方案。
- en: Category encoding
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类别编码
- en: In many problems, you have to transform your features into numeric values. This
    is because many algorithms cannot handle categorical data. There are many ways
    to encode categorical values and DataRobot has many of these methods built in.
    Some of these techniques are one-hot encoding, leave one out encoding, and target
    encoding. We will not get into the details, as normally you would let DataRobot
    handle this for you, but there might be cases where you will want to encode it
    yourself in a specific way due to your understanding of the business problem.
    This feature of DataRobot is a great time saver and typically works very well
    for most problems.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多问题中，你必须将你的特征转换为数值。这是因为许多算法无法处理类别数据。有许多方法可以编码类别值，DataRobot内置了许多这些方法。其中一些技术包括单热编码、留一编码和目标编码。我们不会深入细节，因为通常你会让DataRobot为你处理这些，但可能会有一些情况，由于你对业务问题的理解，你可能需要以特定方式自行编码。DataRobot的这个特性可以节省大量时间，并且通常对大多数问题都工作得很好。
- en: Consolidate categories
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并类别
- en: Sometimes, you have categorical data that contains a large number of categories.
    Although there are methods for dealing with large category counts (as discussed
    in the preceding section), many times, it is advisable to consolidate the categories.
    For example, you might have many categories that contain very few data points,
    but are very similar to one another. In this case, you can combine them into a
    single category. In other cases, it might just be that someone used a different
    spelling, a synonym, or an abbreviation. In such cases, it is better to combine
    them into a single category as well. Sometimes, you might want to split up a numerical
    feature into bins that have a business meaning for your users or stakeholders.
    This is an example of data preparation that you will need to do on your own based
    on your understanding of the problem. You should do this prior to uploading the
    data into DataRobot.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你会有包含大量类别的类别数据。尽管有处理大量类别计数的方法（如前文所述），但很多时候，合并类别是明智的。例如，你可能有很多包含非常少数据点但彼此非常相似的类别。在这种情况下，你可以将它们合并为一个类别。在其他情况下，可能只是有人使用了不同的拼写、同义词或缩写。在这种情况下，最好也将它们合并为一个类别。有时，你可能想将数值特征分割成对用户或利益相关者有业务意义的区间。这是你需要根据对问题的理解自行进行的数据准备的一个例子。你应该在将数据上传到DataRobot之前完成这项工作。
- en: Target leakage
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标泄漏
- en: Sometimes, the dataset contains features that are derived from the target itself.
    These are not known in advance or are not known at the time of prediction. Inadvertently
    using these features to build a model causes problems downstream. This issue is
    called target leakage. The dataset should be inspected carefully and such features
    should be removed from the training features. DataRobot will also analyze the
    features automatically and try to flag any features that might lead to target
    leakage.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，数据集包含来自目标本身的特征。这些特征在事先未知或在预测时未知。无意中使用这些特征来构建模型会导致下游问题。这个问题被称为目标泄漏。应仔细检查数据集，并从训练特征中删除这些特征。DataRobot还会自动分析特征，并尝试标记可能导致目标泄漏的任何特征。
- en: Term-document matrix
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项-文档矩阵
- en: Your dataset may contain features that contain text or notes. These notes frequently
    contain important information that is useful for making decisions. Many of the
    algorithms, however, cannot make use of this text directly. This text has to be
    parsed into numeric values for it to become useful to modeling algorithms. There
    are several methods for doing that, with the most common one being the term-document
    matrices. Document here refers to a single text or notes entry. Each of these
    documents can be parsed to split it up into terms. Now you can count how many
    times a term showed up in a document. This result can be stored in a matrix called
    a **Term Frequency** (**TF**) matrix. Some of this information can also be visualized
    in word clouds. DataRobot will automatically build these word clouds for you.
    While TF is useful, it can be limiting because some terms might be very common
    in all the documents, hence they are not very useful in distinguishing between
    them. This leads to another idea, whereby perhaps we should look for terms that
    are somewhat unique to a document. This concept of giving more weight to a term
    that is present in some documents only is called **Inverse Document Frequency**
    (**IDF**). The combination of a term showing up multiple times in a document (TF)
    and it being somewhat rare (IDF) is called **TFIDF**. TFIDF is something that
    DataRobot will compute automatically for you and gets applied to features that
    contain text.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据集中可能包含包含文本或注释的特征。这些注释通常包含对做出决策有用的重要信息。然而，许多算法却无法直接利用这些文本。这些文本必须被解析成数值，以便对建模算法变得有用。为此有几种方法，其中最常见的是术语-文档矩阵。这里的文档指的是单个文本或注释条目。这些文档可以被解析以分割成术语。现在您可以计算一个术语在文档中出现的次数。这个结果可以存储在一个称为**词频（TF**）矩阵中。其中一些信息也可以在词云中进行可视化。DataRobot会自动为您构建这些词云。虽然TF很有用，但它可能存在局限性，因为某些术语可能在所有文档中都非常常见，因此它们在区分文档方面并不很有用。这导致另一个想法，即我们可能应该寻找仅对某些文档具有独特性的术语。这种给仅在某些文档中出现的术语赋予更多权重的概念被称为**逆文档频率（IDF**）。一个术语在文档中多次出现（TF）并且相对罕见（IDF）的组合被称为**TFIDF**。TFIDF是DataRobot会自动为您计算并应用于包含文本的特征。
- en: Data transformations
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换
- en: 'While DataRobot will do many data transformations for you (and it keeps adding
    more all the time), there are many transformations that will impact your model
    but that DataRobot will not be able to catch. You will have to do these on your
    own. Examples of these are mathematical transformations such as log, square, square
    root, absolute values, and differences. Some of the simple ones can be set up
    inside DataRobot, but for more complex ones, you will have to perform the operations
    outside of DataRobot or in tools such as Paxata. Sometimes, you will do a transformation
    to linearize your problem or to deal with features that have long-tailed data.
    Some of the transformations that DataRobot does automatically are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然DataRobot会为您执行许多数据转换（并且它一直在添加更多），但有许多转换会影响您的模型，但DataRobot却无法捕捉到。您将不得不自己执行这些操作。这些操作的例子包括数学转换，如对数、平方、平方根、绝对值和差值。其中一些简单的可以在DataRobot内部设置，但对于更复杂的转换，您必须在DataRobot外部或使用Paxata等工具执行操作。有时，您会进行转换以线性化问题或处理具有长尾数据的特征。DataRobot自动执行的一些转换如下：
- en: Computing aggregates such as counts, min, max, average, median, most frequent,
    and entropy
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算聚合，如计数、最小值、最大值、平均值、中位数、最频繁值和熵
- en: An extensive list of time-based features, such as change over time, max over
    time, and averages over time
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个广泛的时间相关特征列表，例如随时间的变化、随时间的最大值和随时间的平均值
- en: Some text extraction features, such as word counts, extracted tokens, and term-document
    matrices
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些文本提取特征，例如词数、提取的标记和术语-文档矩阵
- en: Geospatial features from geospatial data
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自地理空间数据的地理空间特征
- en: We will discuss this topic again in more detail in [*Chapter 4*](B17159_04_Final_NM_ePub.xhtml#_idTextAnchor087),
    *Preparing Data for DataRobot*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第4章*](B17159_04_Final_NM_ePub.xhtml#_idTextAnchor087)中更详细地讨论这个主题，*为DataRobot准备数据*。
- en: Collinearity checks
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性相关性检查
- en: In any given dataset, there will be features that are highly correlated to other
    features. In essence, they carry the same information as some other features.
    It is generally desirable to remove such features that are highly duplicative
    of some other features in the dataset. DataRobot performs these checks automatically
    for you and will flag these collinear features. This is especially critical for
    linear models, but some of the newer methods can deal with this issue better.
    What thresholds to use varies based on the modeling algorithms and your business
    problem. It is fairly easy in DataRobot to remove these features from your feature
    sets to be used for modeling.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何给定的数据集中，都存在一些与其他特征高度相关的特征。本质上，它们携带与某些其他特征相同的信息。通常希望删除这些在数据集中高度重复其他特征的特性。DataRobot会自动为您执行这些检查，并将这些共线性特征标记出来。这对于线性模型尤为重要，但一些新的方法可以更好地处理这个问题。使用的阈值取决于建模算法和您的业务问题。在DataRobot中删除这些特征以用于建模是非常容易的。
- en: DataRobot also produces a correlation matrix that shows how the different features
    are correlated to one another. This helps identify collinear features as well
    as key candidate features to be used in the model. You can gain a lot of insight
    into your data and the problem by analyzing the correlation matrix. In [*Chapter
    5*](B17159_05_Final_NM_ePub.xhtml#_idTextAnchor097), *Exploratory Data Analysis
    with DataRobot*, we will discuss examples of how this is done.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot还生成一个相关矩阵，显示不同特征之间的相关性。这有助于识别共线性特征以及模型中要使用的候选关键特征。通过分析相关矩阵，您可以深入了解您的数据和问题。在[*第5章*](B17159_05_Final_NM_ePub.xhtml#_idTextAnchor097)，“使用DataRobot进行探索性数据分析”，我们将讨论如何进行这种分析的示例。
- en: Data partitioning
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据划分
- en: Before you start building the models, you need to partition your dataset into
    three parts. These parts are called training, validation, and holdout. These three
    parts are used for different purposes during the model building process. It is
    common to split 10-20% of the dataset into the holdout set. The remaining portion
    is split up further, with 70-80% going to training and 20-30% going to the validation
    set. This splitting is done to make sure that the models are not overfitted and
    that the expected results in deployment are in line with results seen during model
    building.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始构建模型之前，您需要将数据集划分为三个部分。这些部分被称为训练集、验证集和保留集。在模型构建过程中，这三个部分用于不同的目的。通常将数据集的10-20%划分为保留集。剩余的部分进一步划分，其中70-80%用于训练集，20-30%用于验证集。这种划分是为了确保模型不会过拟合，并且部署时预期的结果与模型构建期间看到的结果一致。
- en: Only the training dataset is used to train the model. The validation set is
    designed to tune the algorithms in order to optimize the results by performing
    multiple cross-validation tests. Finally, the holdout set is used after the models
    are built to test the model on data that it has never seen before. If the results
    on the holdout set are acceptable, then the model can be considered for deployment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用训练集来训练模型。验证集的设计是为了通过执行多次交叉验证测试来调整算法，以优化结果。最后，在构建模型后，使用保留集来测试模型在之前从未见过的数据上的表现。如果保留集上的结果可以接受，那么模型可以考虑部署。
- en: DataRobot automates most of this process, but it does allow the user to customize
    the split percentages, as well as how the partitioning should be done. It also
    performs a similar function for time series or forecasting problems by automatically
    splitting the data for time-based backtests.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot自动化了这一过程的大部分，但它允许用户自定义分割百分比，以及如何进行分割。它还通过自动分割数据以进行基于时间的回溯测试，为时间序列或预测问题执行类似的功能。
- en: Data visualization
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化
- en: One of the most important tasks a data analyst or data scientist needs to do
    is to understand the dataset. Data visualization is key to this understanding.
    DataRobot provides various ways to visualize the datasets to help you understand
    the dataset. These visualizations are built automatically for you so that you
    can spend your time analyzing them instead of preparing them. Let's look at what
    these are and how to use them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析师或数据科学家需要完成的最重要的任务之一是理解数据集。数据可视化是实现这一理解的关键。DataRobot提供了各种可视化数据集的方法，以帮助您理解数据集。这些可视化是自动为您构建的，这样您就可以花时间分析它们，而不是准备它们。让我们看看这些是什么，以及如何使用它们。
- en: 'When you go to the data page (*Figure 1.20*) for your project, you will see
    high-level profile information for your dataset. Inspect this information carefully
    to understand your dataset in totality. If you click on the **Feature Association**
    menu (top left), you will see how the features are related to one another (*Figure
    2.3*):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Feature associations using mutual information'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.3_B17159.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 – Feature associations using mutual information
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: This diagram shows the interrelationships using the mutual information metric.
    **Mutual Information** (**MI**) uses information theory to determine the amount
    of information you obtain about one feature from the other feature. The benefit
    of using MI compared to the Pearson correlation coefficient is that it can be
    used for any type of feature. The value goes from 0 (the two features are independent)
    to 1 (they carry the same information). This is useful in determining which features
    will be good candidates for the model and which features will not provide any
    useful information or are redundant. This view is extremely important to understand
    and use before model building starts, even though DataRobot automatically uses
    this information to make modeling decisions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another metric that is also used in a similar capacity. If you click
    on the metric dropdown at the bottom of the preceding screenshot, you can select
    the other metric called **Cramer''s V**. Once you select Cramer''s V, you will
    see a similar graphical view (*Figure 2.4*):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Feature associations using Cramer''s V'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.4_B17159.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – Feature associations using Cramer's V
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Cramer's V is an alternative metric to MI, and it is used similarly. Its value
    also ranges from 0 (no relationship) to 1 (the features are highly correlated).
    Cramer's V is often used with categorical variables as an alternative to the Pearson
    correlation coefficient.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Notice that DataRobot automatically found clusters of interrelated features.
    Each cluster is color-coded in a different color, and the features are sorted
    by clusters in *Figure 2.4*. You can zoom into specific clusters to inspect them
    further. This is an important feature of the DataRobot environment as very few
    data scientists know about this idea or make use of it. The clusters are important
    because they highlight groups of interrelated features. These complex interdependencies
    are typically very important for understanding the business problem. Normally,
    the only people who know about these complex interdependencies are people with
    a lot of domain experience. Most others will not even be aware of these complexities.
    If you are new to a domain, then understanding these will give you an equivalent
    of multiple years of experience. Study these carefully, discuss them with your
    business experts to fully understand what they are trying to highlight, and then
    use these insights to improve your models as well as your business processes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, note that DataRobot provides a list of the top 10 strongest associations.
    It is important to note these associations and spend some time thinking about
    what they mean for your problem. Are these consistent with what you know about
    your domain, or are there some surprises? It is the surprises that often result
    in key insights that could prove to be valuable insights for your business. In
    the following list, you see a **View Feature Association Pairs** button. If you
    click on that button, you will see *Figure 2.5*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意DataRobot提供了一份前10个最强关联的列表。注意这些关联，并花些时间思考它们对您的问题意味着什么。这些关联与您对领域的了解一致吗，或者有一些惊喜？往往是惊喜导致了关键的见解，这些见解可能对您的业务具有价值。在以下列表中，您可以看到一个**查看特征关联对**按钮。如果您点击该按钮，您将看到*图2.5*：
- en: '![Figure 2.5 – Feature association details'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.5 – 特征关联细节'
- en: '](img/Figure_2.5_B17159.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_2.5_B17159.jpg]'
- en: Figure 2.5 – Feature association details
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – 特征关联细节
- en: This graphic shows the relationship between two selected features in detail.
    In this example, one feature is categorical while the other is numeric. The diagram
    shows how the two are related and could provide additional insights into the problem.
    Be sure to investigate the relationships, especially the ones that might be counterintuitive.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此图形详细展示了两个选定特征之间的关系。在这个例子中，一个特征是分类的，而另一个是数值的。该图显示了这两个特征之间的关系，并可能为问题提供额外的见解。务必调查这些关系，特别是那些可能不符合直觉的关系。
- en: 'Now you can click on the specific features to see how they are distributed
    (*Figure 2.6*):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以点击特定的特征来查看它们的分布情况(*图2.6*)：
- en: '![Figure 2.6 – Feature details'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.6 – 特征细节'
- en: '](img/Figure_2.6_B17159.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_2.6_B17159.jpg]'
- en: Figure 2.6 – Feature details
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – 特征细节
- en: This view shows a histogram of how the values are distributed and how they are
    related to the target values. Key things to focus on are ranges where you do not
    have enough data and where you have non-linearities. These could give you ideas
    about feature engineering. These are also areas where you ask the question why
    does the system exhibit this behavior?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此视图显示了值的分布直方图以及它们与目标值的关系。需要关注的关键点是数据不足的范围以及非线性区域。这些可能给您提供关于特征工程的想法。这些也是您询问系统为何表现出这种行为的区域？
- en: With this background work done, you are now ready to dive into modeling algorithms.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成这项背景工作后，您现在可以开始深入研究建模算法。
- en: Machine learning algorithms
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: 'There are now hundreds of machine learning algorithms available to be used
    for a machine learning project, and more are being invented every day. DataRobot
    supports a wide array of open source machine learning algorithms, including several
    deep learning algorithms – Prophet, SparkML-based algorithms, and H2O algorithms.
    Let''s now take a look at what types of algorithms exist and what they are used
    for (*Figure 2.7*):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有成百上千种机器学习算法可用于机器学习项目，而且每天都有新的算法被发明。DataRobot支持广泛的开源机器学习算法，包括几个深度学习算法——Prophet、基于SparkML的算法和H2O算法。现在让我们看看存在哪些类型的算法以及它们的应用(*图2.7*)：
- en: '![Figure 2.7 – Machine learning algorithms'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.7 – 机器学习算法'
- en: '](img/Figure_2.7_B17159-DESKTOP-C2VUV36.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_2.7_B17159-DESKTOP-C2VUV36.jpg]'
- en: Figure 2.7 – Machine learning algorithms
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 – 机器学习算法
- en: Our focus will mostly be on the algorithm types that DataRobot supports. These
    algorithm types are described in the following sub-sections.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的重点将主要放在DataRobot支持的算法类型上。这些算法类型将在以下子节中描述。
- en: Supervised learning
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'Supervised learning algorithms are used when you can provide an answer (also
    called a label) as part of the training dataset. For supervised learning, you
    have to assign a feature of your dataset to be the answer, and the algorithm tries
    to learn to predict the answer by seeing multiple examples and learning from these
    examples. See *Figure 2.8* for the different types of answers:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当您可以在训练数据集中提供答案（也称为标签）时，会使用监督学习算法。对于监督学习，您必须将数据集的一个特征指定为答案，算法通过观察多个示例并从这些示例中学习来尝试预测答案。参见*图2.8*了解不同类型的答案：
- en: '![Figure 2.8 – Targets for supervised learning algorithms'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.8 – 监督学习算法的目标'
- en: '](img/Figure_2.8_B17159.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_2.8_B17159.jpg]'
- en: Figure 2.8 – Targets for supervised learning algorithms
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – 监督学习算法的目标
- en: 'DataRobot functionality is primarily focused on supervised learning algorithms.
    Included in the set are deep learning algorithms as well as big data algorithms
    from SparkML and H2O. DataRobot has built-in best practices to select the best-suited
    algorithms for your problem and dataset. There are four major types of supervised
    learning problems:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot的功能主要集中于监督学习算法。其中包括深度学习算法以及来自SparkML和H2O的大数据算法。DataRobot内置了最佳实践来选择最适合你问题和数据集的算法。有四种主要的监督学习问题类型：
- en: Regression
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归
- en: Regression problems are the ones where the answer (target) takes a numeric form
    (see *Figure 2.8*). Regression models try to fit a curve such that the error between
    the prediction and the actual value is minimized for the entire training dataset.
    Sometimes, even a classification problem can be set up as a numeric regression
    problem. In such cases, the answer is a number that can then be turned into a
    bin by using thresholds. Logistic regression is one such method that produces
    a value between zero and one. You can mark all answers below a certain threshold
    to be zero, and all above as ones. There are linear as well as non-linear regression
    algorithms that are used based on the problem. The models are assessed based on
    how well the regression line matches the data. Typical metrics used are **RMSE**,
    **MAPE**, **LogLoss**, and **Rsquared**. Typical algorithms used are **XGBoost**,
    **Elastic Net**, **Random Forest**, and **GA2M**.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题是指答案（目标）以数值形式出现的问题（参见*图2.8*）。回归模型试图拟合一条曲线，使得预测值与实际值之间的误差在整个训练数据集上最小化。有时，甚至可以将分类问题设置为一个数值回归问题。在这种情况下，答案是数字，然后可以通过阈值将其转换为二进制。逻辑回归就是这样一种方法，它产生一个介于零和一之间的值。你可以将低于某个阈值的所有答案标记为零，而高于阈值的答案标记为一。根据问题，可以使用线性回归算法以及非线性回归算法。模型评估基于回归线与数据匹配的程度。常用的指标包括**RMSE**、**MAPE**、**LogLoss**和**R-squared**。常用的算法包括**XGBoost**、**Elastic
    Net**、**随机森林**和**GA2M**。
- en: Binary classification
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二元分类
- en: Binary classification problems have answers that can only take two distinct
    values (called classes). These could be in the form of 0 or 1, Yes or No, and
    so on. Please refer to *Figure 2.8* for an example of the target feature for binary
    classification. A typical issue that you commonly face is the problem of class
    imbalance. This happens when most of the dataset is biased toward one class. These
    are typically addressed by downsampling the overrepresented class when sufficient
    training data is present. When this is not possible, you can try oversampling
    the underrepresented class or use other methods. None of these methods is perfect,
    and sometimes you have to try different approaches to see what works best. DataRobot
    provides mechanisms to specify downsampling if needed. Some of the algorithms
    that are commonly used for binary classification are **logistic regression**,
    **k-nearest neighbors**, **tree-based algorithms**, **SVM**, and **Naïve Bayes**.
    In the case of classification problems, it is best to avoid using accuracy as
    a metric to assess results. The results are often shown in the form of a confusion
    matrix (described later in this chapter). DataRobot will automatically select
    an appropriate metric to use in such cases.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类问题具有只能取两个不同值的答案（称为类别）。这些可以是0或1、是或否等形式。请参阅*图2.8*以了解二元分类的目标特征的示例。你通常会遇到的一个典型问题是类别不平衡问题。当大多数数据集偏向于一个类别时，这种情况就会发生。这些通常通过在存在足够训练数据的情况下对过度代表的类别进行下采样来解决。当这不可能时，你可以尝试对代表性不足的类别进行过采样或使用其他方法。这些方法都不是完美的，有时你必须尝试不同的方法来查看哪种方法最有效。DataRobot提供了在需要时指定下采样的机制。常用的二元分类算法包括**逻辑回归**、**k最近邻**、**基于树的算法**、**SVM**和**朴素贝叶斯**。在分类问题的情况下，最好避免使用准确率作为评估结果的指标。结果通常以混淆矩阵的形式展示（本章后面将描述）。DataRobot将自动选择在这种情况下使用适当的指标。
- en: Multiclass classification
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多类分类
- en: Multiclass classification problems are the ones where you are trying to predict
    more than two classes or categories. For a simple example of what the target might
    look like, see *Figure 2.8*. Multiclass capability was added recently and many
    of the DataRobot features might not work with such problems. Since downsampling
    is not available, you might want to adjust your sampling prior to uploading your
    dataset into DataRobot. Also, note that you can frequently collapse your problem
    into a binary classification problem by collapsing the classes into two classes.
    That may or may not work for your use case, but it is an option if required. Also,
    not all algorithms are appropriate for multiclass problems. DataRobot will automatically
    select the appropriate algorithms to build the models for multiclass problems.
    Typical metrics to use are AUC, LogLoss, or Balanced Accuracy. The results are
    often shown in the form of a confusion matrix (described later in this chapter).
    Typical algorithms used are XGBoost, Random Forest, and TensorFlow.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类问题是指您试图预测两个以上类别或类别的那些问题。为了简单说明目标可能的样子，请参阅*图2.8*。多类分类能力是最近添加的，DataRobot的许多功能可能无法与这类问题一起使用。由于没有下采样可用，您可能需要在将数据集上传到DataRobot之前调整您的采样。此外，请注意，您可以通过将类别合并为两个类别来经常将问题简化为二元分类问题。这可能或可能不会适用于您的用例，但如果需要，这是一个选项。此外，并非所有算法都适用于多类问题。DataRobot将自动选择适当的算法来构建多类问题的模型。典型的指标包括AUC、LogLoss或平衡准确率。结果通常以混淆矩阵的形式显示（本章后面将描述）。典型的算法包括XGBoost、随机森林和TensorFlow。
- en: Time series/forecasting
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间序列/预测
- en: 'Time series or forecasting models are also referred to as time-aware models
    in DataRobot. In these problems, you have data that is changing over time and
    you are interested in predicting/forecasting a target value in the future (*Figure
    2.2*). DataRobot not only supports the usual algorithms for time series such as
    ARIMA, but can also adapt these problems to machine learning regression problems
    and then apply algorithms such as XGBoost to solve them. These problems require
    that the series should be transformed into stationary series and require extensive
    feature engineering to create time-based features. The problems also require that
    you take into account important events in the past that may repeat (such as holidays
    or major shopping days). Time series models also require special ways of handling
    validation and testing via a method called backtests:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列或预测模型在DataRobot中也被称为时间感知模型。在这些问题中，您有随时间变化的数据，并且您对预测/预测未来的目标值感兴趣（*图2.2*）。DataRobot不仅支持时间序列的常用算法，如ARIMA，还可以将这些问题调整为机器学习回归问题，然后应用XGBoost等算法来解决它们。这些问题要求将序列转换为平稳序列，并需要大量特征工程来创建基于时间的特征。这些问题还要求您考虑过去可能重复发生的重要事件（如假日或大型购物日）。时间序列模型还需要通过称为回测的特殊方法来处理验证和测试：
- en: '![Figure 2.9 – Backtesting for time series problems'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.9 – 时间序列问题的回测'
- en: '](img/Figure_2.9_B17159.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.9_B17159.jpg)'
- en: Figure 2.9 – Backtesting for time series problems
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 – 时间序列问题的回测
- en: In backtesting, models are built using past data, and then tested using holdout
    data that is newer and has never been seen by the model. This time-based slicing
    of holdout data is also referred to as out-of-time validation. DataRobot automates
    many of these tasks for you, as we will see in more detail later.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在回测中，使用过去数据构建模型，然后使用较新的、模型从未见过的保留数据来测试。这种基于时间的保留数据切片也被称为时间外验证。DataRobot为您自动化了许多这些任务，我们将在后面更详细地看到。
- en: Algorithms
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法
- en: 'Let''s review some of the main algorithms used in DataRobot. Here, we only
    provide a high-level overview of these algorithms These algorithms can be tuned
    for a given problem by changing their hyperparameters. For a more detailed understanding
    of any specific algorithm, you can refer to a machine learning book or the DataRobot
    documentation. Some of the important algorithms are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下DataRobot中使用的几种主要算法。在这里，我们只提供这些算法的高级概述。这些算法可以通过改变它们的超参数来针对特定问题进行调整。要更详细地了解任何特定算法，您可以参考机器学习书籍或DataRobot文档。以下是一些重要的算法：
- en: '**Random Forest**. A random forest model is built by creating multiple decision
    tree models and then uses the mean of the output. This is done by creating bootstrap
    samples of the training data and building decision trees (*Figure 2.10*) on these
    samples:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林**。随机森林模型是通过创建多个决策树模型，然后使用输出的平均值来构建的。这是通过创建训练数据的自助样本并在这些建立决策树（*Figure
    2.10*）来实现的：'
- en: '![Figure 2.10 – Random forest'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.10 – Random forest]'
- en: '](img/Figure_2.10_B17159.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.10_B17159.jpg]'
- en: Figure 2.10 – Random forest
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 2.10 – Random forest
- en: 'Random forest models handle missing data and non-linearities and have proven
    to work great in many situations. A random forest model can be used for regression
    as well as classification problems:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林模型可以处理缺失数据和非线性，并在许多情况下被证明非常有效。随机森林模型可以用于回归问题以及分类问题：
- en: '**XGBoost**: Also known as **eXtreme** gradient boosted trees, are decision
    tree-based algorithms that have become very popular because they tend to produce
    very effective predictions and can handle missing values. They can handle non-linear
    problems and interactions between features. XGBoost builds upon random forest
    models by creating a random forest and then creating trees on the residuals of
    the previous trees. This way, every new set of trees is able to produce a better
    result. XGBoost can be used for regression as well as classification problems.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**XGBoost**：也称为**eXtreme**梯度提升树，是基于决策树的算法，因其通常能产生非常有效的预测并能够处理缺失值而变得非常流行。它们可以处理非线性问题和特征之间的交互。XGBoost通过创建一个随机森林并在先前树的残差上构建树来建立在随机森林模型之上。这样，每一组新的树都能够产生更好的结果。XGBoost既可以用于回归问题，也可以用于分类问题。'
- en: '**Rulefit**: Rulefit models are ensembles of simple rules. You can think of
    these rules as being chained together like a decision tree. Rulefit models are
    much easier to understand as most people can relate to a combination of rules
    being applied to solve a problem. DataRobot typically builds this model to help
    you understand a problem and provide insights. You can go to the insights section
    of your **Models** tab and see the insights generated from a Rulefit model and
    how effective a given rule is for the problem. They can be used for classification
    as well as regression problems.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rulefit**：Rulefit模型是简单规则的集成。你可以将这些规则想象成像决策树一样链接在一起。Rulefit模型更容易理解，因为大多数人可以理解将规则组合起来解决问题。DataRobot通常构建此模型以帮助您理解问题并提供见解。您可以去“模型”标签页的见解部分查看从Rulefit模型生成的见解以及给定规则对问题的有效性。它们可以用于分类问题以及回归问题。'
- en: '**ElasticNet**, **Ridge regressor**, **Lasso regressor**: These models use
    regularization to make sure that the models are not overfitting and are not unnecessarily
    complex. Regularization is done by adding a penalty for adding more features,
    which in turn forces the models to either drop some features or reduce their relative
    impact. Lasso regressor (also known as **L1 regressor**) uses penalty weights
    that are the absolute values of the coefficients. The effect of using Lasso is
    that it tries to reduce the coefficients to zero, thereby selecting important
    features and removing the ones that do not contribute much. Ridge regressor (also
    known as **L2 regressor**) uses penalty weights that are squared coefficients.
    The impact of this is to reduce the magnitude of coefficients. **ElasticNet**
    is used to refer to linear models that use both Lasso and Ridge regularization
    to produce models that are simpler as well as regularized. This comes in handy
    when you have a lot of features that are correlated with each other.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ElasticNet**、**Ridge回归器**、**Lasso回归器**：这些模型使用正则化来确保模型不会过拟合且不会过于复杂。正则化是通过添加更多特征的惩罚来实现的，这反过来迫使模型要么丢弃一些特征，要么减少它们的相对影响。Lasso回归器（也称为**L1回归器**）使用的是系数的绝对值作为惩罚权重。使用Lasso的效果是尝试将系数减少到零，从而选择重要的特征并移除那些贡献不大的特征。Ridge回归器（也称为**L2回归器**）使用的是平方系数作为惩罚权重。这种影响是减少系数的幅度。**ElasticNet**用于指代同时使用Lasso和Ridge正则化的线性模型，以产生既简单又正则化的模型。当你有很多相互关联的特征时，这非常有用。'
- en: '**Logistic Regression**: Logistic regression is a non-linear regression model
    that is used for binary classification. The output is in the form of a probability
    with a value ranging from 0 to 1\. This is then typically used with a threshold
    to assign the value to be a 0 or a 1.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归**：逻辑回归是一种用于二元分类的非线性回归模型。输出形式为概率值，范围从0到1。这通常与一个阈值一起使用，以将值分配为0或1。'
- en: '**SVM** (**Support Vector Machine**): This is a classification algorithm that
    tries to find a vector that best separates classes. It is easy to see what this
    looks like in a two-dimensional space (*Figure 2.11*), but the algorithm is known
    to work well in high dimension spaces. Another benefit of SVM is its ability to
    handle non-linearity by using non-linear kernel functions, which can be used to
    linearize the problem:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Targets for supervised learning algorithms'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.11_B17159.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 – Targets for supervised learning algorithms
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '**GA2M** (**Generalized Additive Model**): This is one of those rare algorithms
    that offers understandability, while also offering high accuracy even in a non-linear
    problem. The number "2" in the name represents its ability to model interactions
    between features. GAM model output is a summation of outputs of the effects of
    individual features that have been binned. Since GAM allows these effects to be
    non-linear, it can capture the non-linear nature of the problem. The results of
    the model can be represented as a simple table that shows you the contribution
    of each feature to the overall answer. This type of table representation is easily
    understandable by most people. For industries or use cases where understandability
    and explainability are very important, this is perhaps one of the best options
    you can choose.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-Nearest Neighbors**: This is a very straightforward algorithm that finds
    the k closest data points (based on a specific way of computing distance). Now
    it finds the classification answers for these k points. It then determines the
    answer with the most votes and then assigns that as the answer. The default distance
    metric used is **Euclidian** distance, but DataRobot chooses the appropriate metric
    based on the dataset. A user can also specify a specific distance metric to be
    used.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow**. TensorFlow is a deep learning model that is based on deep neural
    networks. A deep neural network is one that has hidden deep layers made up of
    ensembles of artificial neurons. The neurons carry highly non-linear activation
    functions that allow them to fit highly non-linear problems. These models are
    very good at producing high accuracy without the need for feature engineering,
    but they do require a lot more training data as compared to other algorithms.
    These models are generally considered very opaque and are prone to overfitting
    and are therefore not suitable for some applications. They are especially successful
    for applications where the features and feature engineering are hard to extract,
    for example, image processing. These models can be used for regression as well
    as classification problems.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keras Neural Network**: Keras is a high-level deep learning library built
    on top of TensorFlow that allows many types of deep learning models to be incorporated
    into DataRobot. Being a higher-level library, it makes building a TensorFlow model
    a lot easier. Everything described in the preceding section applies to Keras.
    The particular implementation in DataRobot is well suited for sparse datasets
    and is particularly useful for text processing and classification problems.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keras神经网络**：Keras是一个基于TensorFlow构建的高级深度学习库，它允许将多种类型的深度学习模型集成到DataRobot中。作为一个高级库，它使得构建TensorFlow模型变得容易得多。前述章节中描述的一切都适用于Keras。在DataRobot中的特定实现非常适合稀疏数据集，并且特别适用于文本处理和分类问题。'
- en: Unsupervised learning
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Unsupervised learning problems are those where you are not provided with an
    answer or a label. Examples of such problems are clustering or anomaly detection.
    DataRobot does not offer much for these problems, but it does have some capability
    for anomaly or outlier detection. These are problems where you have data points
    that are unusual in a way that happens very rarely. Examples include fraud detection,
    cybersecurity breach detection, failure detection, and data outlier detection.
    DataRobot allows you to set up a project without a target and it will then attempt
    to identify anomalous data points. For any clustering problems, you should try
    to use Python or R to create clustering models.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习问题是指你未提供答案或标签的问题。这类问题的例子包括聚类或异常检测。DataRobot对于这些问题没有太多提供，但它确实有一些异常或离群值检测的能力。这些问题是指数据点以一种非常罕见的方式不寻常。例子包括欺诈检测、网络安全漏洞检测、故障检测和数据离群值检测。DataRobot允许你设置一个没有目标的项目，然后它会尝试识别异常数据点。对于任何聚类问题，你应该尝试使用Python或R来创建聚类模型。
- en: Reinforcement learning
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning problems are where you want to learn a series of decisions
    to be taken by an agent such that you achieve a certain goal. This goal is associated
    with a reward that is given to the agent for achieving the goal either completely
    or partially. There is no dataset available for this training, so the agent must
    try multiple times (with different strategies) and learn something on each attempt.
    Over many attempts, the agent will learn the strategy or rules that produce the
    best reward. As you can now guess, these algorithms work best when you do not
    have data, but you can experiment repeatedly in the real world (or a synthetic
    world). As we discussed before, DataRobot is not a suitable tool for such problems.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题是你希望学习一系列由代理做出的决策，以便实现某个特定目标的情况。这个目标与一个奖励相关联，该奖励是给予代理的，以奖励其完全或部分实现目标。对于这种训练没有可用的数据集，因此代理必须尝试多次（使用不同的策略）并在每次尝试中学习一些东西。经过多次尝试，代理将学会产生最佳奖励的策略或规则。正如你现在可以猜到的，这些算法在没有数据但可以在现实世界（或合成世界）中反复实验的情况下工作得最好。正如我们之前讨论的，DataRobot不是这类问题的合适工具。
- en: Ensemble/blended models
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成/混合模型
- en: Ensembling is a technique for creating a model that aggregates or blends predictions
    of other models. Different algorithms are sometimes able to exploit different
    aspects of the problem or dataset better. This means that many times, you can
    increase prediction accuracy by combining several good models. This, of course,
    comes with increasing complexity and cost. DataRobot offers many blending approaches
    and, in most circumstances, builds the blended model automatically for your project.
    You can then evaluate whether the increase in accuracy is enough to justify the
    additional complexity.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 集成是一种创建聚合或混合其他模型预测的模型的技术。不同的算法有时能更好地利用问题的不同方面或数据集。这意味着很多时候，通过结合几个好的模型，你可以提高预测的准确性。当然，这也伴随着复杂性和成本的提高。DataRobot提供了许多混合方法，并且在大多数情况下，它会自动为你的项目构建混合模型。然后你可以评估提高的准确性是否足以证明额外的复杂性是合理的。
- en: Blueprints
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图
- en: 'In DataRobot, every model is associated with a blueprint. A blueprint is a
    step-by-step recipe used by DataRobot to train a specific model. See *Figure 2.12*
    for an example:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在DataRobot中，每个模型都与一个蓝图相关联。蓝图是DataRobot用来训练特定模型的一步一步的食谱。参见*Figure 2.12*以获取示例：
- en: '![Figure 2.12 – Model blueprint'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.12 – 模型蓝图'
- en: '](img/Figure_2.12_B17159.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.12_B17159.jpg]'
- en: Figure 2.12 – Model blueprint
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 2.12 – 模型蓝图
- en: The blueprint shows all the steps taken by DataRobot to build that specific
    model, including any data preparation and feature engineering done by DataRobot.
    Clicking on any specific box will show more details on the actions taken, parameters
    used, and documentation of the particular algorithm used. This also serves as
    great documentation for your modeling project that is automatically created for
    you.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝图显示了DataRobot构建特定模型所采取的所有步骤，包括DataRobot所做的任何数据准备和特征工程。点击任何特定的框将显示采取的行动、使用的参数以及特定算法的文档。这也为您的建模项目提供了出色的文档，这些文档是自动为您创建的。
- en: Now, let's look at how to determine how well an algorithm did. For this, we
    will require some performance metrics.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何确定算法的表现如何。为此，我们需要一些性能指标。
- en: Performance metrics
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能指标
- en: 'DataRobot offers a wide range of performance metrics for the models. You have
    to specify the metric you want to use to optimize the models for your project.
    Typically, the best metric to use is the one recommended by DataRobot. DataRobot
    does compute the other metrics as well once the model is built, so you can review
    the results of your model across multiple metrics. Please keep in mind that no
    metric is perfect for every situation, and you should be careful in selecting
    the metric for evaluating your results. Listed here are some details regarding
    commonly used metrics:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot为模型提供了一系列的性能指标。您必须指定您想要用于优化项目模型的指标。通常，最佳指标是DataRobot推荐的指标。一旦模型构建完成，DataRobot也会计算其他指标，因此您可以跨多个指标查看您模型的成果。请记住，没有哪个指标适合所有情况，您在选择评估结果的指标时应该谨慎。以下是关于常用指标的一些详细信息：
- en: '**RMSE** (**Root Mean Squared Error**): RMSE is a metric that first computes
    the square of errors (the difference between actual and predicted). These are
    then averaged over the entire dataset and then we compute a square root of that
    average. Given that this metric is dependent on the scale of the values, its interpretation
    is dependent on the problem. You cannot compare RMSE for two different datasets.
    This metric is frequently used for regression problems when the data is not highly
    skewed.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSE**（**均方根误差**）：RMSE是一个首先计算误差平方（实际值与预测值之间的差异）的指标。然后，在整个数据集上平均这些值，然后计算这个平均值的平方根。鉴于这个指标依赖于值的规模，其解释依赖于问题。您不能比较两个不同数据集的RMSE。这个指标在数据不是高度偏斜的回归问题中经常使用。'
- en: '**MAPE** (**Mean Absolute Percentage Error**): MAPE is somewhat similar to
    RMSE in the sense that it first computes the absolute value of the percentage
    error. Then, these values are averaged over the dataset. Given that this metric
    is scaled in terms of percentage, it is easier to compare MAPE for different datasets.
    However, you have to be mindful of the fact that the percentage error for very
    small values (or zero values) tends to look very big.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MAPE**（**平均绝对百分比误差**）：MAPE在某种意义上与RMSE相似，因为它首先计算百分比误差的绝对值。然后，这些值在数据集上平均。鉴于这个指标按百分比缩放，比较不同数据集的MAPE更容易。然而，您必须注意，非常小的值（或零值）的百分比误差往往看起来很大。'
- en: '**SMAPE** (**Symmetric MAPE**): SMAPE is similar to MAPE, but addresses some
    of the shortcomings discussed above. SMAPE bounds the upper percentage value so
    that errors from small values do not overpower the metric. This makes SMAPE a
    good metric that you can easily compare across different problems.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SMAPE**（**对称MAPE**）：SMAPE与MAPE相似，但解决了上述讨论的一些缺点。SMAPE限制了上限百分比值，这样小的值的误差就不会压倒指标。这使得SMAPE成为一个可以轻松跨不同问题比较的好指标。'
- en: '**Accuracy**: Accuracy is one of the metrics used for classification problems.
    It can be represented as follows:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：准确率是用于分类问题的指标之一。它可以表示如下：'
- en: '*Accuracy = number of correct predictions/number of total predictions*'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*准确率 = 正确预测数/总预测数*'
- en: It is essentially the ratio of the number of correct predictions and all predictions.
    For unbalanced problems, this metric can be misleading, hence it is never used
    by itself to determine how well a model did. It is typically used in combination
    with other metrics.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它实际上是正确预测数与所有预测数的比率。对于不平衡问题，这个指标可能会误导，因此它从不单独用来确定模型的表现。它通常与其他指标结合使用。
- en: '**Balanced Accuracy**: Balanced accuracy overcomes the issues with accuracy
    by normalizing the accuracy across the two classes being predicted. Let''s say
    that the two classes are A and B:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平衡准确率**：平衡准确率通过在预测的两个类别上归一化准确率来克服准确率的问题。假设两个类别是A和B：'
- en: (a) *Accuracy rate for A = number of correct A predictions/total number of As*
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) *A的准确率 = 正确预测A的数量/总数A*
- en: (b) *Accuracy rate for B = number of correct B predictions/total number of Bs*
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (b) *B的准确率 = 正确预测B的数量/总数B*
- en: (c) *Balanced accuracy = accuracy rate for A + accuracy rate for B/2*
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (c) *平衡准确率 = A的准确率 + B的准确率/2*
- en: Balanced accuracy is essentially the average of the accuracy rate for A and
    the accuracy rate for B.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平衡准确率实际上是A的准确率和B的准确率的平均值。
- en: '**AUC** (**Area Under the ROC Curve**): AUC is the area under the **ROC** (**Received
    Operator Characteristic**) curve. This metric is frequently used for classification
    problems as this also overcomes the deficiencies associated with the accuracy
    metric. The ROC curve represents the relationship between the true positive rate
    and the false positive rate. The AUC goes from 0 to 1 and it shows how well the
    model discriminates between the two classes. A value of 0.5 represents a random
    model, so you would want the AUC for your model to be greater than 0.5.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AUC**（**ROC曲线下的面积**）：AUC是ROC曲线下的面积。这个指标经常用于分类问题，因为它也克服了与准确率指标相关的缺陷。ROC曲线表示真正例率和假正例率之间的关系。AUC从0到1，它显示了模型区分两个类别的效果。0.5表示随机模型，因此你希望你的模型的AUC大于0.5。'
- en: '**Gamma Deviance**: Gamma deviance is used for regression problems when the
    target values are gamma-distributed. For such targets, gamma deviance measures
    twice the average deviance (using the log-likelihood function) of the predictions
    from the actuals. A model that fits perfectly will have a deviance of zero.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伽马偏差**：伽马偏差用于目标值呈伽马分布的回归问题。对于此类目标，伽马偏差测量预测值与实际值之间的平均偏差的两倍（使用对数似然函数）。拟合完美的模型将具有零偏差。'
- en: '**Poisson Deviance**: Poisson deviance is used for regression problems when
    the aim is to count data that is skewed. It works in a way that is very similar
    to gamma deviance.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泊松偏差**：泊松偏差用于回归问题，当目标是计数偏斜数据时。它的工作方式与伽马偏差非常相似。'
- en: '**LogLoss**: LogLoss (also known as cross-entropy loss) is a measure of the
    inaccuracy of predicted probabilities for a classification problem. A value of
    0 indicates a perfect model, and as the model becomes worse, the logloss value
    increases.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LogLoss**: LogLoss（也称为交叉熵损失）是衡量分类问题预测概率不准确性的指标。值为0表示模型完美，随着模型变差，logloss值增加。'
- en: '**Rsquared**: Rsquared is a metric used for regression problems that tells
    how well the fitted line represents the dataset. Its value ranges between 0 and
    1\. 0 indicates a poor model that explains none of the variation, while a value
    of 1 indicates a perfect model that explains 100% of the variation. It is one
    of the most commonly used metrics, but it can suffer from the problem that you
    can increase it by adding more variables without necessarily improving the model.
    It is also not suitable for non-linear problems.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R平方**：R平方是用于回归问题的指标，它说明了拟合线如何代表数据集。其值介于0和1之间。0表示模型较差，无法解释任何变化，而1表示模型完美，解释了100%的变化。它是使用最广泛的指标之一，但可能会因为增加更多变量而提高，而不一定改善模型。它也不适用于非线性问题。'
- en: Now that we have discussed some of the commonly used metrics, let's look at
    how to look at other results to assess the quality of your model, and the effects
    of different features on your model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了一些常用的指标，让我们看看如何查看其他结果来评估模型的质量，以及不同特征对模型的影响。
- en: Understanding the results
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解结果
- en: In this section, we will discuss various visualizations of metrics and other
    information to understand the results of the modeling exercise. These are important
    visualizations that need to be inspected carefully in addition to looking at the
    model metrics discussed in the previous section. These visualizations are generated
    automatically by DataRobot for any model that it trains.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论各种指标和其他信息的可视化，以了解建模练习的结果。这些是除了上一节讨论的模型指标外，需要仔细检查的重要可视化。这些可视化是由DataRobot为它训练的任何模型自动生成的。
- en: Lift chart
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升值图
- en: 'The lift chart shows how effective the model is at predicting the target values.
    As the number of data points is typically very large to show in one graphic, the
    lift chart sorts the output and aggregates the data into multiple bins. It then
    compares the averages of predictions and actuals in each bin (*Figure 2.13*):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Lift chart'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.13_B17159.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 – Lift chart
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: The preceding lift chart shows how the predictions have been sorted from low
    to high and then binned (60 bins in this case). You can now see the average prediction
    and average actual value in each bin. This gives you a sense of how well the model
    is doing across the entire spectrum. You can see whether there are ranges where
    the model is doing worse. If the model is not doing well in a range that is important
    to your business, you can then investigate further to see how you can improve
    the model in that range. You can also inspect different models to see whether
    there is a model that does better in the region that is more important. Lift charts
    are more meaningful for regression problems.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix (binary and multiclass)
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For classification problems, one of the best ways to assess model results is
    by looking at the confusion matrix and its associated metrics (*Figure 2.14*).
    This tab is available for multiclass problems:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Confusion matrix'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.14_B17159.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.14 – Confusion matrix
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix maps predicted versus actual counts (frequency) for each
    class. Let's look at the sedan column. The big green circle indicates how many
    times we correctly classified a sedan as a sedan. In that column, you will also
    see red dots where the model predicted it to be a sedan, but it is a different
    type. You can see these for all classes. The relative scales should give you an
    idea of how well your model did and where it is having difficulty.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'If you select a specific class, you can look at the class-specific confusion
    matrix on the right. You can see two columns (+ for predicting a sedan, - for
    predicting something that isn''t a sedan). Similarly, you see two rows (+ where
    it is a sedan, and - for when it is not a sedan). You also see some critical definitions
    and metrics:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positives** (**TP**) = Where it is a sedan and is predicted as a sedan'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positives** (**FP**) = Where it is not a sedan but is predicted as
    a sedan'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negatives** (**TN**) = Where it is not a sedan and is predicted as not
    being a sedan'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negatives** (**FN**) = Where it is a sedan but is predicted as not
    being a sedan'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these, we can now compute some specific metrics for this class:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision = correct fraction of predictions = TP/All Positive Predictions
    = TP/(TP+FP)*'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recall = correct fraction of actuals = TP/All Positive Actuals = TP/(TP+FN)*'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*F1 Score = harmonic mean of precision and recall. So, 1/F1 = 1/Precision +
    1/Recall*'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROC
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This tab is available for binary classification problems. The **ROC** (**Receiver
    Operator Characteristic**) curve is the relationship between the true positive
    rate and the false positive rate. The area under this curve is known as AUC. It
    goes from 0 to 1 and it shows how well the model discriminates between the two
    classes (*Figure 2.15*):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – ROC curve and confusion matrix'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.15_B17159.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.15 – ROC curve and confusion matrix
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: You can also see the confusion matrix (described earlier) and the associated
    metrics for the two classes. You can move the thresholds and assess the resulting
    trade-offs and cumulative gains. Since most problems are not symmetric in the
    sense that true positives have different business values compared to true negatives,
    you should select the threshold that makes sense for your business problem.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy over time
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This tab is available for time series problems (*Figure 2.16*) and compares
    the actual versus predicted values over time for a series:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Model accuracy over time'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.16_B17159.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.16 – Model accuracy over time
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: You can view these values for the backtests or the holdout datasets. The diagram
    will clearly show where the model is not performing well and what you might want
    to focus on to improve your model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Feature impacts
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides model performance, one of the first things you want to understand is
    how impactful the features are in terms of your model''s performance. The **Feature
    Impacts** tab (*Figure 2.17*) is perhaps the most critical for understanding your
    model:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Feature impacts'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.17_B17159.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.17 – Feature impacts
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The graphic shows a sorted list of the most important features. For each feature,
    you can see the relative impact that a feature has on this model. You can see
    which features contribute very little; this can be used to create new feature
    lists by removing some of the features that have very little impact.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Feature Fit
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Feature Fit** tab (*Figure 2.18*) shows an alternative view of the contribution
    of a feature. The graphic shows the features ranked by their importance:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18 – Feature Fit'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.18_B17159.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.18 – Feature Fit
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: For the selected feature, it shows how the predictions compare to actuals for
    the range of values of a feature. Reviewing these graphs for the key features
    can provide a lot of insight about how a feature impacts the results and range
    of values that perform better and ranges where it performs the worst. This could
    sometimes highlight the regions where you might need to collect more data to improve
    your model.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Feature Effects
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Feature Effects** show information that is very similar to **Feature Fit**
    (*Figure 2.19*). In this graphic, the features are sorted by **Feature Impacts**.
    Also, **Feature Effects** are focused on partial dependence:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Feature Effects and Partial Dependence'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.19_B17159.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.19 – Feature Effects and Partial Dependence
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Partial dependence plots are one of the most important plots that you want to
    study carefully. These plots tell you how a change in the value of a feature impacts
    the change in the average value of the target over a range of values for the other
    features. This insight is critical to understanding the business problem, understanding
    what the model is doing, and, more importantly, what aspects of the model are
    actionable and what range of values will produce the maximum impact.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Prediction Explanations
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Prediction Explanations** describe the reasons for a specific prediction
    in terms of feature values for the specific instance or row that is being scored
    (*Figure 2.20*). Note that this is different from **Feature Impacts**, which tell
    you the importance of a feature at a global level:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20 – Prediction Explanations'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.20_B17159.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.20 – Prediction Explanations
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction Explanations** can be generated for an entire dataset or a subset
    of data, as shown in the preceding screenshot. For example, it will provide the
    top three reasons why the model predicted a specific value. These explanations
    are sometimes required for regulatory reasons in certain use cases, but it is
    a good idea to produce these explanations as they do help in understanding why
    a model predicts a certain way and can be very useful in validating or catching
    errors in a model. DataRobot uses two algorithms for computing the explanations:
    **XEMP** (**exemplar-based explanations**) or **Shapley values**. XEMP is supported
    for a broader range of models and is selected by default. Shapley values are described
    in the next section.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Shapley values
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Shapley** **values** (**SHAP**) are an alternative mechanism for producing
    prediction explanations (*Figure 2.21*). If you want to use SHAP for explanations,
    you have to specify this in the advanced options during the project setup before
    you press the **Start** button. Once DataRobot starts building the models, you
    cannot switch to SHAP. SHAP values are only available for linear or tree-based
    models and are not available for ensemble models:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21 – SHAP-based explanations'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.21_B17159.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.21 – SHAP-based explanations
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: SHAP values are based on cooperative game theory, which tries to assign values
    to contributions of a team member in a collaborative project. In the context of
    machine learning, it tries to assign the value contribution of a specific feature
    when there is a team of features collaborating to make a prediction. SHAP values
    are additive and you can easily see how much of the final answer is due to a specific
    feature value.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered some of the basic machine learning concepts that
    will come in handy as we go through the remaining chapters, and they will also
    be useful in your data science journey. Please note that we have only covered
    concepts at a high level, and depending on your job role, you might want to explore
    some areas in more detail. We have also related this material to how DataRobot
    performs certain functions and where you need to pay closer attention.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopefully, this has given you some insights into what DataRobot will be displaying
    and where to focus your attention in different stages of your project. Since DataRobot
    automates a good chunk of model building and prediction tasks, it might be tempting
    to ignore many of the outputs that DataRobot is automatically producing for you.
    Please resist that temptation. DataRobot software is taking considerable pains
    and resources to produce those outputs for a very good reason. It is also doing
    much of the grunt work for you, so please take advantage of those capabilities.
    Specifically, we have covered the following: What are the things to watch out
    for during data preparation? What data visualizations are important for gaining
    an understanding of your dataset? What are the key machine learning algorithms,
    and when do you use them? How do you measure the goodness of your model results?
    How do you assess model performance and understand what the model is telling you
    about your problem?'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the basics, we will start our data science journey in the next
    chapter by learning how to understand the business problem and how to turn it
    into a specification that can be solved by using machine learning.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
