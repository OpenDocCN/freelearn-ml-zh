<html><head></head><body>
		<div id="_idContainer075">
			<h1 id="_idParaDest-60"><em class="italic"><a id="_idTextAnchor059"/>Chapter 3</em>: Data Preparation and Transformation </h1>
			<p>You have probably heard that data scientists spend most of their time working on data preparation-related activities. It is now time to explain why that happens and which types of activities we are talking about. </p>
			<p>In this chapter, you will learn how to deal with categorical and numerical features, as well as applying different techniques to transform your data, such as one-hot encoding, binary encoders, ordinal encoding, binning, and text transformations. You will also learn how to handle missing values and outliers in your data, which are two important tasks you can implement to build good machine learning models.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Identifying types of features</li>
				<li>Dealing with categorical features</li>
				<li>Dealing with numerical features</li>
				<li>Understanding data distributions</li>
				<li>Handling missing values</li>
				<li>Dealing with outliers</li>
				<li>Dealing with unbalanced datasets</li>
				<li>Dealing with text data</li>
			</ul>
			<p>This is a lengthy chapter, so bear with us! Knowing about these topics in detail will definitely put you in a good position for the AWS Machine Learning Specialty exam.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor060"/>Identifying types of features</h1>
			<p>We <em class="italic">cannot</em> start <a id="_idIndexMarker184"/>modeling without<a id="_idIndexMarker185"/> knowing what a <strong class="bold">feature</strong> is and which type of information it might store. You have already read about different processes that deal with features. For example, you know that feature engineering is related to the task of building and preparing features to your models; you also know that feature selection is related to the task of choosing the best set of features to feed a particular algorithm. These two tasks have one behavior in common: they may vary according to the types of features they are processing.</p>
			<p>It is very<a id="_idIndexMarker186"/> important to understand this behavior (feature type versus applicable transformations) because it will help you eliminate invalid answers during <a id="_idIndexMarker187"/>your exam (and, most importantly, you will become a better data scientist).</p>
			<p>When we refer to types of features, we are talking about the data type that a particular feature is supposed to store. The following diagram shows how we could potentially describe the different types of features of a model:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B16735_03_001.jpg" alt="Figure 3.1 – Feature types&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Feature types</p>
			<p>In <a href="B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Machine Learning Fundamentals</em>, you were introduced to the feature classification shown in the preceding diagram. Now, let's look at some real examples in order to eliminate any remaining questions you may have:</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B16735_03_002.jpg" alt="Figure 3.2 – Real examples of feature values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Real examples of feature values</p>
			<p>Although <a id="_idIndexMarker188"/>looking at the values of the variable may help you find its type, you <a id="_idIndexMarker189"/>should never rely only on this aspect. The nature of the<a id="_idIndexMarker190"/> variable is also very important for making such decisions. For example, someone could encode the cloud provider variable (shown in the preceding table) as follows: 1 (AWS), 2 (MS), 3 (Google). In that case, the variable is still a nominal feature, even if it is now represented by discrete numbers. </p>
			<p>If you are building a ML model and you don't tell your algorithm that this variable is not a discrete number, but instead a nominal variable, the algorithm will treat it as a number and the model won't be interpretable anymore. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Before feeding any ML algorithm with data, make sure your feature types have been properly identified. </p>
			<p>In theory, if you are happy with your features and have properly classified each of them, you should be ready to go into the modeling phase of the CRISP-DM methodology, shouldn't you? Well, maybe not. There are many reasons you may want to spend a little more time on data preparation, even after you have correctly classified your features:</p>
			<ul>
				<li>Some algorithm implementations, such as <strong class="source-inline">scikit-learn</strong>, may not accept string values on your categorical features. </li>
				<li>The data<a id="_idIndexMarker191"/> distribution of your variable may not be the most <a id="_idIndexMarker192"/>optimal distribution for your algorithm. </li>
				<li>Your ML algorithm may be impacted by the scale of your data.</li>
				<li>Some observations (rows) of your variable may be missing information and you will have to fix it. These are also known as missing values.</li>
				<li>You may find outlier values of your variable that can potentially add bias to your model.</li>
				<li>Your variable may be storing different types of information and you may only be interested in a few of them (for example, a date variable can store the day of the week or the week of the month). </li>
				<li>You might want to find a mathematical representation for a text variable. </li>
				<li>And believe me, this list will never end.</li>
			</ul>
			<p>In the following sections, we will understand how to address all these points, starting with categorical features.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor061"/>Dealing with categorical features</h1>
			<p>Data transformation <a id="_idIndexMarker193"/>methods for categorical features will vary according to the sub-type of your variable. In the upcoming sections, we will understand how to transform nominal and ordinal features. </p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor062"/>Transforming nominal features</h2>
			<p>You may have to create<a id="_idIndexMarker194"/> numerical representations of your <a id="_idIndexMarker195"/>categorical features before applying ML algorithms to them. Some libraries may have embedded logic to handle that transformation for you, but most of them do not.</p>
			<p>The first transformation we will cover is<a id="_idIndexMarker196"/> known as <strong class="bold">label encoding</strong>. A label encoder <a id="_idIndexMarker197"/>is suitable for categorical/nominal variables and it will just associate a number with each distinct label of your variable. The following table shows how a label encoder works:</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B16735_03_003.jpg" alt="Figure 3.3 – Label encoder in action&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Label encoder in action</p>
			<p>A label encoder will always ensure that a unique number is associated with each distinct label. In the preceding table, although "India" appears twice, the same number was assigned to it. </p>
			<p>You now have a numerical representation of each country, but this does not mean you can use that numerical representation in your models! In this particular case, we are transforming a nominal feature, <em class="italic">which does not have an order</em>. </p>
			<p>According to the preceding table, if we pass the encoded version of the <em class="italic">country</em> variable to a model, it will make assumptions such as "Brazil (3) is greater than Canada (2)", which does not make any sense.</p>
			<p>A possible solution for that scenario is applying another type of transformation on top of "<em class="italic">country"</em>: <strong class="bold">one-hot encoding</strong>. This <a id="_idIndexMarker198"/>transformation will represent all the categories from the original feature as individual features (also known as <strong class="bold">dummy variables</strong>), which <a id="_idIndexMarker199"/>will store the "presence or absence" of each category. The following table is transforming the same information we looked at in the preceding table, but this time it's applying one-hot encoding:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B16735_03_004.jpg" alt="Figure 3.4 – One-hot encoding in action&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – One-hot encoding in action</p>
			<p>We can now use the <a id="_idIndexMarker200"/>one-hot encoded version of the <em class="italic">country</em> variable as a feature of a ML model. However, your work as a skeptical data scientist is <a id="_idIndexMarker201"/>never done, and your critical thinking ability will be tested in the AWS Machine Learning Specialty exam.</p>
			<p>Let's suppose you have 150 distinct countries in your dataset. How many dummy variables would you come up with? 150, right? Here, we just found a potential issue: apart from adding complexity to your model (which is not a desired characteristic of any model at all), dummy variables also add <strong class="bold">sparsity</strong> to your data.</p>
			<p>A sparse dataset has a lot of variables filled with zeros. Often, it is hard to fit this type of data structure into memory (you can easily run out of memory) and it is very time-consuming for ML algorithms to process sparse structures.</p>
			<p>You can work around the sparsity problem by grouping your original data and reducing the number of categories, and you can even use custom libraries that compress your sparse data and make it easier for manipulation (such as <strong class="source-inline">scipy.sparse.csr_matrix</strong>, from Python).</p>
			<p>Therefore, during the exam, remember that one-hot encoding is definitely the right way to go when you need to transform categorical/nominal data to feed ML models; however, take the number of unique categories of your original feature into account and think about if it makes sense to create dummy variables for all of them (maybe it does not make sense, if you have a very large number of unique categories).</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor063"/>Applying binary encoding</h2>
			<p>For those types of<a id="_idIndexMarker202"/> variables with a higher number of unique <a id="_idIndexMarker203"/>categories, a potential approach to creating a numerical representation for them is applying <strong class="bold">binary encoding</strong>. In this approach, the goal is transforming a categorical column into multiple binary columns, but minimizing the number of new columns.</p>
			<p>This process consists of three basic steps:</p>
			<ol>
				<li>The categorical data is converted into numerical data after being passed through an ordinal encoder.</li>
				<li>The resulting number is then converted into a binary value.</li>
				<li>The binary value is split into different columns.</li>
			</ol>
			<p>Let's reuse our data from <em class="italic">Figure 3.3</em> to see how we could use binary encoding in this particular case:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B16735_03_005.jpg" alt="Figure 3.5 – Binary encoding in action&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Binary encoding in action</p>
			<p>As we can see, we now have three columns (Col1, Col2, and Col3) instead of four.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/>Transforming ordinal features</h2>
			<p>Ordinal features <a id="_idIndexMarker204"/>have a very specific characteristic: <em class="italic">they have an order</em>. Because they have this quality, it does <em class="italic">not</em> make sense to apply one-hot encoding to them; if you do so, you will lose the magnitude of order of your feature.</p>
			<p>The most common <a id="_idIndexMarker205"/>transformation for this type of variable is <a id="_idIndexMarker206"/>known as <strong class="bold">ordinal encoding</strong>. An <a id="_idIndexMarker207"/>ordinal encoder will associate a number with each distinct label of your variable, just like a label encoder does, but this time, it will respect the order of each category. The following table shows how an ordinal encoder works:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B16735_03_006.jpg" alt="Figure 3.6 – Ordinal encoding in action&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – Ordinal encoding in action</p>
			<p>We can now pass the encoded variable to ML models and they will be able to handle this variable properly, with no need to apply one-hot encoding transformations. This time, comparisons such as "Sr Data Analyst is greater than Jr. Data Analyst" make total sense. </p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor065"/>Avoiding confusion in our train and test datasets</h2>
			<p>Do not forget the <a id="_idIndexMarker208"/>following statement: encoders are <strong class="bold">fitted</strong> on training data and <strong class="bold">transformed</strong> on test and production data. This is how your ML pipeline should work.</p>
			<p>Let's suppose you have created a one-hot encoder that fits the data from <em class="italic">Figure 3.3</em> and returns data according to <em class="italic">Figure 3.4</em>. In this example, we will assume this is our training data. Once you have completed your training process, you may want to apply the same one-hot encoding transformation to your testing data to check the model's results. </p>
			<p>In the scenario that we just described (which is a very common situation in modeling pipelines), you <em class="italic">cannot</em> retrain your encoder on top of the testing data! You should just reuse the previous encoder object that you have created on top of the training data. Technically, we say that you shouldn't use the <strong class="source-inline">fit</strong> method again, and use the <strong class="source-inline">transform</strong> method instead.</p>
			<p>You may already <a id="_idIndexMarker209"/>know the reasons why you should follow this rule, but let's recap: the testing data was created to extract the performance metrics of your model, so you should not use it to extract any other knowledge. If you do so, your performance metrics will be biased by the testing data and you cannot infer that the same performance (shown in the test data) is likely to happen in production (when new data will come in).</p>
			<p>Alright, all good so far. However, what if our testing set has a new category that was not present in the train set? How are we supposed to transform this data? Let's walk through this particular scenario.</p>
			<p>Going back to our one-hot encoding example we looked at in <em class="italic">Figures 3.3</em> (input data) and <em class="italic">Figure</em> <em class="italic">3.4</em> (output data), our encoder knows how to transform the following countries: Australia, Brazil, Canada, and India. If we had a different country in the testing set, the encoder would not know how to transform it, and that's why we need to define how it will behave in scenarios where there are exceptions.</p>
			<p>Most of the ML libraries provide specific parameters for these situations. In our example, we could program the encoder to either raise an error or set all zeros on our dummy variables, as shown in the following table:    </p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B16735_03_007.jpg" alt="Figure 3.7 – Handling unknown values on one-hot encoding transformations &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – Handling unknown values on one-hot encoding transformations </p>
			<p>As we can see, Portugal was not present in the training set (<em class="italic">Figure 3.3</em>), so during the transformation, we are keeping the same list of known countries and saying that Portugal <em class="italic">IS NOT</em> any of them (all zeros).</p>
			<p>As the very<a id="_idIndexMarker210"/> good, skeptical data scientist we know you are becoming, should you be concerned about the fact that you have a particular category that has not been used during training? Well, maybe. This type of analysis really depends on your problem domain. </p>
			<p>Handling unknown values is very common and something that you should expect to do in your ML pipeline. However, you should also ask yourself, due to the fact that you did not use that particular category during your training process, if your model can be extrapolated and generalized.</p>
			<p>Remember, your testing data must follow the same data distribution as your training data, and you are very likely to find all (or at least most) of the categories (of a categorical feature) either in the training or test sets. Furthermore, if you are facing overfitting issues (doing well in the training, but poorly in the test set) and, at the same time, you realize that your categorical encoders are transforming a lot of unknown values in the test set, guess what? It's likely that your training and testing samples are not following the same distribution, invalidating your model entirely.</p>
			<p>As you can see, slowly, we are getting there. We are talking about bias and investigation strategies in fine-grained detail! Now, let's move on and look at performing transformations on numerical features. Yes, each type of data matters and drives your decisions!</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor066"/>Dealing with numerical features</h1>
			<p>In terms of <a id="_idIndexMarker211"/>numerical features (discrete and continuous), we can think of transformations that rely on the training data and others that rely purely on the observation being transformed.</p>
			<p>Those that rely on the training data will use the train set to learn the necessary parameters during <strong class="source-inline">fit</strong>, and then use them to transform any test or new data. The logic is pretty much the same as what we just reviewed for categorical features; however, this time, the encoder will learn different parameters.</p>
			<p>On the other hand, those that rely purely on observations do not care about train or test sets. They will simply perform a mathematical computation on top of an individual value. For example, we could apply an exponential transformation to a particular variable by squaring its value. There is no dependency on learned parameters from anywhere – just get the value and square it.</p>
			<p>At this point, you<a id="_idIndexMarker212"/> might be thinking about dozens of available transformations for numerical features! Indeed, there are so many options that we can't describe all of them here, and you are not supposed to know all of them for the AWS Machine Learning Specialty exam anyway. We will cover the most important ones (for the exam) here, but I don't want to limit your modeling skills: take a moment to think about the unlimited options you have by creating custom transformations according to your use case. </p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor067"/>Data normalization </h2>
			<p>Applying data <strong class="bold">normalization</strong> means <a id="_idIndexMarker213"/>changing the scale of the data. For example, your feature may store employee salaries that range between 20,000 and 200,000 dollars/year and you want to put this data in the range of 0 and 1, where 20,000 (the minimum observed value) will be transformed into 0 and 200,000 (the maximum observed value) will be transformed into 1.</p>
			<p>This type of technique is specifically important when you want to fit your training data on top of certain types of algorithms that are impacted by the scale/magnitude of the underlying data. For instance, we can think about those algorithms that use the dot product of the<a id="_idIndexMarker214"/> input variables (such as neural networks or linear regression) and those algorithms that rely on<a id="_idIndexMarker215"/> distance measures (such as <strong class="bold">k-nearest neighbor</strong> (<strong class="bold">KNN</strong>) or <strong class="bold">k-means</strong>). </p>
			<p>On the other hand, applying data normalization will not result in performance improvements for rule-based algorithms, such as decision trees, since they will be able to check the predictive <a id="_idIndexMarker216"/>power of the<a id="_idIndexMarker217"/> features (either via <strong class="bold">entropy</strong> or <strong class="bold">information gain</strong> analysis), regardless of the scale of the data. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">We will learn about these algorithms, along with the appropriate details, in the later chapters of this book. For instance, you can look at entropy and information gain as two types of metrics used by decision trees to check feature importance. Knowing the predictive power of each feature helps the algorithm define the optimal root, intermediaries, and leaf nodes of the tree. </p>
			<p>Let's take a moment to <a id="_idIndexMarker218"/>understand why data normalization will help those types of algorithms. We already know that the goal of a clustering algorithm is to find groups or clusters in your data, and one of the most used clustering algorithms is known as k-means. We will use k-means to see this problem in action, since it is impacted by data scaling.</p>
			<p>The following image shows how different scales of the variable could change the hyper plan's projection:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B16735_03_008.jpg" alt="Figure 3.8 – Plotting data of different scales in a hyper plan    &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – Plotting data of different scales in a hyper plan    </p>
			<p>On the left-hand side of the preceding image,  we can see a single data point plotted in a hyper plan that has three dimensions (x, y, and z). All three dimensions (also known as features) were normalized to the scale of 0 and 1. On the right-hand side, we can see the same data point, but this time, the "x" dimension was <em class="italic">not</em> normalized. We can clearly see that the hyper plan has changed.</p>
			<p>In a real scenario, we would have far more dimensions and data points. The difference in the scale of the data would change the centroids of each clusters and could potentially change the assigned clusters of some points. This same problem will happen on other algorithms that rely on distances calculation, such as KNN.</p>
			<p>Other algorithms, such as neural networks and linear regression, will compute weighted sums using your input data. Usually, these types of algorithms will perform operations such as <em class="italic">W1*X1 + W2*X2 + Wi*Xi</em>, where <em class="italic">Xi</em> and <em class="italic">Wi</em> refer to a particular feature value and its weight, respectively. Again, we will cover details of neural networks and linear models later, but can you see the data scaling problem by just looking at the calculations that we just described? We can easily come up with very large values if X (feature) and W (weight) are large numbers. That will make the algorithm's optimizations much more complex.</p>
			<p>I hope you now have a <a id="_idIndexMarker219"/>very good understanding about the reasons you should apply data normalization (and when you should not). Data normalization is often implemented in ML<a id="_idIndexMarker220"/> libraries as <strong class="bold">Min Max Scaler</strong>. If you find this term in the exam, then remember it is the same as data normalization.</p>
			<p>Additionally, data normalization does not necessarily need to transform your feature into a range between 0 and 1. In reality, we can transform the feature into any range we want. The following is how a normalization is formally defined:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B16735_03_009.jpg" alt="Figure 3.9 – Normalization formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – Normalization formula</p>
			<p>Here, <em class="italic">X</em><span class="subscript">min</span> and <em class="italic">X</em><span class="subscript">max</span> are the lower and upper values of the range; <em class="italic">X</em> is the value of the feature. Apart from data normalization, there is another very important technique regarding numerical transformations that you <em class="italic">must</em> be aware of, not only for the exam, but also for your data science career. We'll look at this in the next section.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor068"/>Data standardization</h2>
			<p>Data <strong class="bold">standardization</strong> is <a id="_idIndexMarker221"/>another scaling method that transforms the distribution of the data, so that the mean will become zero and the standard deviation will become one. The following image formally describes this scaling technique, where <em class="italic">X</em> represents the value to be transformed, <em class="italic">µ</em> refers to the mean of <em class="italic">X</em>, and <em class="italic">σ</em> is the standard deviation of <em class="italic">X</em>:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B16735_03_010.jpg" alt="Figure 3.10 – Standardization formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10 – Standardization formula</p>
			<p>Unlike normalization, data standardization will <em class="italic">not</em> result in a predefined range of values. Instead, it will transform your data into a <a id="_idIndexMarker222"/>standard <strong class="bold">Gaussian distribution</strong>, where your transformed values will represent the number of standard deviations of each value to the mean of the distribution. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Gaussian distribution, also known as normal distribution, is one of the most used distribution on statistical models. This is a continuous distribution with two main controlled parameters: µ (mean) and σ (standard deviation). Normal distributions are symmetric around the mean. In other words, most of the values will be close to the mean of the distribution. </p>
			<p>Data standardization if often referred to as <a id="_idIndexMarker223"/>the <strong class="bold">zscore</strong> and is widely used to identify outliers on your variable, which we will see later in this chapter. For the sake of demonstration, the following table simulates the data standardization of a small dataset. The input value is present in the "Age" column, while the scaled value is present in the "Zscore" column:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B16735_03_011.jpg" alt="Figure 3.11 – Data standardization in action&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11 – Data standardization in action</p>
			<p>Make sure you are confident when applying normalization and standardization by hand in the AWS Machine Learning Specialty exam. They might provide a list of values, as well as mean and standard deviation, and ask you the scaled value of each element of the list.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor069"/>Applying binning and discretization</h2>
			<p><strong class="bold">Binning</strong> is a<a id="_idIndexMarker224"/> technique where you can group a set of values into a bucket or bin; for example, grouping people between 0 and 14 years old into a bucket named "children," another group of people between 15 and 18 years old into a bucket named "teenager," and so on.</p>
			<p><strong class="bold">Discretization</strong> is the<a id="_idIndexMarker225"/> process of transforming a continuous variable into discrete or nominal attributes. These continuous values can be discretized by multiple<a id="_idIndexMarker226"/> strategies, such <a id="_idIndexMarker227"/>as <strong class="bold">equal-width</strong> and <strong class="bold">equal-frequency</strong>. </p>
			<p>An equal-width strategy will split your data across multiple bins of the same width. Equal-frequency will split your data across multiple bins with the same number of frequencies.</p>
			<p>Let's look at an example. Suppose we have the following list containing 16 numbers: 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 90. As we can see, this list ranges between 10 and 90. Assuming we want to create four bins using an equal-width strategy, we would come up with the following bins:</p>
			<ul>
				<li>Bin &gt;= 10 &lt;= 30 &gt; 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24</li>
				<li>Bin &gt; 30 &lt;= 50 &gt;</li>
				<li>Bin &gt; 50 &lt;= 70 &gt;</li>
				<li>Bin &gt; 71 &lt;= 90 &gt; 90</li>
			</ul>
			<p>In this case, the <a id="_idIndexMarker228"/>width of each bin is the same (20 units), but the observations are not equally distributed. Now, let's simulate an equal-frequency strategy:</p>
			<ul>
				<li>Bin &gt;= 10 &lt;= 13 &gt; 10, 11, 12, 13</li>
				<li>Bin &gt; 13 &lt;= 17 &gt; 14, 15, 16, 17</li>
				<li>Bin &gt; 17 &lt;= 21 &gt;  18, 19, 20, 21</li>
				<li>Bin &gt; 21 &lt;= 90 &gt; 22, 23, 24, 90</li>
			</ul>
			<p>In this case, all the bins have the same frequency of observations, although they have been built with different bin widths to make that possible. </p>
			<p>Once you have computed your bins, you should be wondering what's next, right? Here, you have some options:</p>
			<ul>
				<li>You can name your bins and use them as a nominal feature on your model! Of course, as a nominal variable, you should think about applying one-hot encoding before feeding a ML model with this data.</li>
				<li>You might want to order your bins and use them as an ordinal feature.</li>
				<li>Maybe you want to remove some noise from your feature by averaging the minimum and maximum values of each bin and using that value as your transformed feature.</li>
			</ul>
			<p>Take a look at the following table to understand these approaches using our equal-frequency example:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B16735_03_012.jpg" alt="Figure 3.12 – Different approaches to working with bins and discretization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – Different approaches to working with bins and discretization</p>
			<p>Again, playing <a id="_idIndexMarker229"/>with different binning strategies will give you different results and you should analyze/test the best approach for your dataset. There is no standard answer here – it is all about data exploration! </p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor070"/>Applying other types of numerical transformations</h2>
			<p>Normalization and <a id="_idIndexMarker230"/>standardization rely on your training data to fit their parameters: minimum and maximum values in the case of normalization, and mean and standard deviation in the case of standard scaling. This also means you must fit those parameters using <em class="italic">only</em> your training data and never the testing data.</p>
			<p>However, there are other types of numerical transformations that do not require parameters from training data to be applied. These types of transformations rely purely on mathematical <a id="_idIndexMarker231"/>computations. For example, one of these transformations is known as <strong class="bold">logarithmic transformation</strong>. This is a very common type of transformation<a id="_idIndexMarker232"/> in machine learning models and is especially beneficial for <strong class="bold">skewed</strong> features. In case you don't know what a skewed distribution is, take a look at the following diagram:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B16735_03_013.jpg" alt="Figure 3.13 – Skewed distributions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13 – Skewed distributions</p>
			<p>In the middle, we <a id="_idIndexMarker233"/>have a normal distribution (or Gaussian distribution). On the left- and right-hand sides, we have skewed distributions. In terms of skewed features, there will be some values far away from the mean in one single direction (either left or right). Such behavior will push both the median and mean values of this distribution in the same direction of the long tail we can see in the preceding diagram.</p>
			<p>One very clear example of data that used to be skewed is the annual salaries of a particular group of professionals in a given region, such as senior data scientists working in Florida, US. This type of variable usually has most of its values close to the others (because people used to earn an average salary) and just has a few very high values (because a small group of people makes much more money than others). </p>
			<p>Hopefully, you can now easily understand why the mean and median values will move to the tail direction, right? The big salaries will push them in that direction. </p>
			<p>Alright, but why will a logarithmic transformation be beneficial for this type of feature? The answer to this question can be explained by the math behind it:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B16735_03_014.jpg" alt="Figure 3.14 – Logarithmic properties&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14 – Logarithmic properties</p>
			<p>Computing the log of a<a id="_idIndexMarker234"/> number is the inverse of the exponential function. Log transformation will then reduce the scale of your number according to a given base (such as base 2, base 10, or base e, in the case of a natural logarithm). Looking at our salary's distribution from the previous example, we would bring all those numbers down so that the higher the number, the higher the reduction; however, we would do this in a log scale and not in a linear fashion. Such behavior will remove the outliers of this distribution (making it closer to a normal distribution), which is beneficial for many ML algorithms, such as linear regression. The following table shows you some of the differences when transforming a number in a linear scale versus a log scale:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B16735_03_015.jpg" alt="Figure 3.15 – Differences between linear transformation and log transformation &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15 – Differences between linear transformation and log transformation </p>
			<p>I hope you can see that the linear transformation kept the original magnitude of the data (we can still see outliers, but in another scale), while the log transformation removed those differences of magnitude and still kept the order of the values.</p>
			<p>Would you be able to think about another type of mathematical transformation that follows the same behavior of <em class="italic">log</em> (making the distribution closer to Gaussian)? OK, I can give you another: square root. Take the square root of those numbers shown in the preceding table and see yourself!</p>
			<p>Now, pay attention to this: both log and square root belong to a set of transformations known as <strong class="bold">power transformations</strong>, and there<a id="_idIndexMarker235"/> is very popular method, which is likely to be mentioned on your AWS exam, that can perform a range of power transformations like those we have seen. This method was proposed by George Box and David Cox and its <a id="_idIndexMarker236"/>name is <strong class="bold">Box-Cox</strong>.  </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">During your exam, if you see questions around the Box-Cox transformation, remember that it is a method that can perform many power transformations (according to a lambda parameter), and its end goal is making the original distribution closer to a normal distribution (do not forget that). </p>
			<p>Just to conclude our discussion regarding why mathematical transformations can really make a difference to ML models, I will give you<a id="_idIndexMarker237"/> an intuitive example about <strong class="bold">exponential transformations</strong>. </p>
			<p>Suppose you have a set of data points, such as those on the left-hand side of <em class="italic">Figure 3.16</em>. Your goal is to draw a line that will perfectly split blue and red points. Just by looking at the original data (again, on the left-hand side), we know that our best guess for performing this linear task would be the one you can see in the same image. However, the science (not magic) happens on the right-hand side of the image! By squaring those numbers and plotting them in another hyper plan, we can perfectly separate each group of points:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B16735_03_016.jpg" alt="Figure 3.16 – Exponential transformation in action&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.16 – Exponential transformation in action</p>
			<p>I know you might be <a id="_idIndexMarker238"/>thinking about the infinite ways you can deal with your data. Although this is true, you should always take the business scenario you are working on into account and plan your work accordingly. Remember that model improvements or exploration is always possible, but you have to define your goals (remember the CRISP-DM methodology) and move on. </p>
			<p>By the way, data transformation is important, but just one piece of your work as a data scientist. Your modeling journey still has to move to other important topics, such as missing values and outliers handling, which we will look at next. However, before that, you may have noticed that you were introduced to Gaussian distributions during this section, so let's take a moment to discuss them in a bit more detail. </p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Understanding data distributions</h1>
			<p>Although the Gaussian distribution<a id="_idIndexMarker239"/> is probably the most common distribution for statistical and machine learning models, you should be aware that it is not the only one. There are other types of data distributions, such as the <strong class="bold">Bernoulli</strong>, <strong class="bold">Binomial</strong>, and <strong class="bold">Poisson</strong> distributions.</p>
			<p>The Bernoulli distribution<a id="_idIndexMarker240"/> is a very simple one, as there are only two types of possible events: success or failure. The success event has a probability "p" of happening, while the failure one has a probability of "1-p". </p>
			<p>Some examples that follow a Bernoulli distribution are rolling a six-sided die or flipping a coin. In both cases, you must define the event of success and the event of failure. For example, suppose our events for success and failure in the die example are as follows:</p>
			<ul>
				<li>Success: Getting a number 6</li>
				<li>Failure: Getting any other number</li>
			</ul>
			<p>We can then say that we have a p probability of success (1/6 = 0.16 = 16%) and a 1-p probability of failure (1 - 0.16 = 0.84 = 84%).</p>
			<p>The Binomial distribution<a id="_idIndexMarker241"/> generalizes the Bernoulli distribution. The Bernoulli distribution has only one repetition of an event, while the Binomial distribution allows the event to be repeated many times, and we must count the number of successes. Let's continue with our prior example; that is, counting the number of times we got a 6 out of our 10 dice rolls. Due to the nature of this example, Binomial distribution has two parameters, n and p, where n is the number of repetitions and p is the probability of success in every repetition.</p>
			<p>Finally, a Poisson <a id="_idIndexMarker242"/>distribution<a id="_idIndexMarker243"/> allows us to find a number of events in a time period, given the number of times an event occurs in an interval. It has three parameters: lambda, e, and k, where lambda is the average number of events per interval, e is the Euler number, and k is the number of times an event occurs in an interval.</p>
			<p>With all those distributions, including the Gaussian one, it is possible to compute the expected mean value and variance based on their parameters. This information is usually used in hypothesis tests to check whether some sample data follows a given distribution, by comparing the mean and variance <strong class="bold">of the sample</strong> against the <strong class="bold">expected</strong> mean and variance of the distribution. </p>
			<p>I hope you are now more familiar with data distributions generally, not only Gaussian distributions. We will keep talking about data distributions throughout this book. For now, let's move on to missing values and outlier detection.</p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Handling missing values</h1>
			<p>As the name suggests, missing values <a id="_idIndexMarker244"/>refer to the absence of data. Such absences are usually represented by tokens, which may or may not be implemented in a standard way.</p>
			<p>Although using tokens is standard, the way those tokens are displayed may vary across different platforms. For example, relational databases represent missing data with <em class="italic">NULL</em>, core Python<a id="_idIndexMarker245"/> code will use <em class="italic">None</em>, and some Python libraries will represent missing numbers as (<strong class="bold">Not a Number</strong> (<strong class="bold">NaN</strong>).</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">For numerical fields, don't replace those standard missing tokens with <em class="italic">zeros</em>. By default, zero is not a missing value, but another number. I said "by default" because, in data science, we may face some data quality issues, which we will cover next.</p>
			<p>However, in real business scenarios, you may or may not find those standard tokens. For example, a software engineering team might have designed the system to automatically fill missing data with specific tokens, such as "unknown" for strings or "-1" for numbers. In that case, you would have to search by those two tokens to find missing data. People can set anything.</p>
			<p>In the previous example, the software engineering team was still kind enough to give us standard tokens. However, there are many cases where legacy systems do not add any data quality layer in front of the user, and you may find an address field filled with "I don't want to share" or a phone number field filled with "Don't call me". This is clearly missing data, but not as standard as the previous example.</p>
			<p>There are many more nuances that you will learn regarding missing data, all of which we will cover in this section, but be advised: before you start making decisions about missing values, you should prepare a good data exploration and make sure you find those values. You can either compute data frequencies or use missing plots, but please do something. Never assume that your missing data is represented only by those handy standard tokens.</p>
			<p>Why should we care about this type of data? Well, first, because most algorithms (apart from decision trees implemented on very specific ML libraries) will raise errors when they find a missing value. Second (and maybe most importantly), by grouping all the missing data in the same bucket, you are assuming that they are all the same, but in reality, you don't know that. </p>
			<p>Such a decision will<a id="_idIndexMarker246"/> not only add bias to your model – it will reduce its interpretability, as you will be unable to explain the missing data. Once we know why we want to treat the missing values, we can take a look at our options.</p>
			<p>Theoretically, we<a id="_idIndexMarker247"/> can classify missing values into two main groups: <strong class="bold">MCAR</strong> or <strong class="bold">MNAR</strong>. MCAR stands for <strong class="bold">Missing Completely at Random</strong> and states that there is no pattern associated with the missing data. On the other hand, MNAR<a id="_idIndexMarker248"/> stands for <strong class="bold">Missing Not at Random</strong> and means that the underlying process used to generate the data is strictly connected to the missing values.   </p>
			<p>Let me give you an example of MNAR missing values. Suppose you are collecting user feedback about a particular product in an online survey. Your process of asking questions is dynamic and depends on user answers. When a user specifies an age lower than 18 years old, you never ask his/her marital status. In this case, missing values of marital status are connected to the age of the user (MNAR).</p>
			<p>Knowing the class of missing values that you are dealing with will help you understand if you have any control over the underlying process that generates the data. Sometimes, you can come back to the source process and, somehow, complete your missing data.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Although, in real scenarios, we usually have to treat missing data via exclusion or imputation, never forget that you can always try to look at the source process and check if you can retrieve (or, at least, better understand) the missing data. You may face this option in the exam.</p>
			<p>If you don't have an opportunity to<a id="_idIndexMarker249"/> recover your missing data from somewhere, then you should move<a id="_idIndexMarker250"/> on to other approaches, such as <strong class="bold">listwise deletion</strong> and <strong class="bold">imputation</strong>.</p>
			<p>Listwise deletion <a id="_idIndexMarker251"/>refers to the process of discarding some data, which is the downside of this choice. This may happen at the row level or at the column level. For example, suppose you have a DataFrame containing four columns and one of them has 90% of its data missing. In such cases, what usually makes more sense is dropping the entire feature (column), since you don't have that information for the majority of your observations (rows).</p>
			<p>From a row <a id="_idIndexMarker252"/>perspective, you may have a DataFrame with a small number of observations (rows) containing missing data in one of its features (columns). In such scenarios, instead of removing the entire feature, what makes more sense is removing only those few observations.</p>
			<p>The benefit of using this method is the simplicity of dropping a row or a column. Again, the downside is losing information. If you don't want to lose information while handling your missing data, then you should go for an imputation strategy.</p>
			<p>Imputation <a id="_idIndexMarker253"/>is also known as replacement, where you will replace missing values by substituting a value. The most common approach of imputation is replacing the missing value with the mean of the feature. Please take a note of this approach because it is likely to appear in your exam:</p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B16735_03_017.jpg" alt="Figure 3.17 – Replacing missing values with the mean or median&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.17 – Replacing missing values with the mean or median</p>
			<p>The preceding table shows a very simple dataset with one single feature and five observations, where the third observation has a missing value. If we decide to replace that missing data with the mean value of the feature, we will come up with 49. Sometimes, when we have outliers in the data, the median might be more appropriate (in this case, the median would be 35):</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B16735_03_018.jpg" alt="Figure 3.18 – Replacing missing values with the mean or median of the group&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.18 – Replacing missing values with the mean or median of the group</p>
			<p>If you want to go deeper, you could find the mean or median value according to a given group of features. For example, in the preceding table, we expanded our previous dataset by adding the Job status column. Now, we have clues that help us suspect that our initial approach of changing the missing value by using the overall median (35 years old) is likely to be wrong (since that person is retired).</p>
			<p>What you can do now is replace the missing value with the mean or median of the other observations that belong to the same job status. Using this new approach, we can change the missing information to 77.5. Taking into account that the person is retired, 77.5 makes more sense than 35 years old. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In the case of categorical variables, you can replace the missing data with the value that has the highest occurrence in your dataset. The same logic of grouping the dataset according to specific features is still applicable.</p>
			<p>You can also use more sophisticated <a id="_idIndexMarker254"/>methods of imputation, including constructing a ML model to predict the value of your missing data. The downside of these imputation approaches (either by averaging or predicting the value) is that you are making inferences on the data, which are not necessarily right and will add bias to the dataset. </p>
			<p>To sum this up, the trade-off while dealing with missing data is having a balance between losing data or adding bias to the dataset. Unfortunately, there is no scientific manual that you can follow, whatever your problem. To decide on what you are going to do, you must look to your success criteria, explore your data, run experiments, and then make your decisions. </p>
			<p>We will now move to another headache for many ML algorithms, also known as outliers.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor073"/>Dealing with outliers </h1>
			<p>We are not on this studying journey just to pass the AWS Machine Learning Specialty exam, but also to become better data scientists. There are many different ways to look at the outlier problem purely from a mathematical perspective; however, the datasets we use are derived from the underlying business process, so we must include a business perspective during an outlier analysis.</p>
			<p>An <strong class="bold">outlier</strong> is an <a id="_idIndexMarker255"/>atypical data point in a set of data. For example, the following chart shows some data points that have been plotted in a two-dimension plan; that is, x and y. The red point is an outlier, since it is an atypical value on this series of data:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B16735_03_019.jpg" alt="Figure 3.19 – Identifying an outlier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.19 – Identifying an outlier</p>
			<p>We want to treat outlier values because some statistical methods are impacted by them. Still, in the preceding chart, we can see this behavior in action. On the left-hand side, we drew a line that best fits those data points, ignoring the red point. On the right-hand side, we also drew the best line to fit the data but included the red point. </p>
			<p>We can visually conclude that, by ignoring the outlier point, we will come up with a better solution on the plan of the left-hand side of the preceding chart since it was able to pass closer to most of the values. We can also prove this by computing an associated error for each line (which we will discuss later in this book).</p>
			<p>It is worth reminding you that you have also seen the outlier issue in action in another situation in this book: specifically, in <em class="italic">Figure 3.17</em>, when we had to deal with missing values. In that example, we used the median (instead of the mean) to work around the problem. Feel free to go back and read it again, but what should be very clear at this point is that median values are less impacted by outliers than average values.</p>
			<p>You now know <a id="_idIndexMarker256"/>what outliers are and why you should treat them. You should always consider your business perspective while dealing with outliers, but there are mathematical methods to find them. Now, let's look at these methods of outlier detection. </p>
			<p>You have already learned about the most common method: zscore. In <em class="italic">Figure 3.11</em>, we saw a table containing a set of ages. Refer to it again to refresh your memory. In the last column of that table, we are computing the <a id="_idIndexMarker257"/>zscore of each age, according to the equation shown in <em class="italic">Figure 3.10</em>.</p>
			<p>There is no well-defined range for those zscore values; however, in a normal distribution <em class="italic">without</em> outliers, they will range between -3 and 3. Remember: zscore will give you the number of standard deviations from the mean of the distribution. The following diagram shows some of the properties of a normal distribution:</p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B16735_03_020.jpg" alt="Figure 3.20 – Normal distribution properties. Image adapted from https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.20 – Normal distribution properties. Image adapted from https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg</p>
			<p>According to the normal distribution properties, 95% of values will belong to the range of -2 and 2 standard deviations from the mean, while 99% of the values will belong to the range of -3 and 3. Coming back to the outlier detection context, we can set thresholds on top of those<a id="_idIndexMarker258"/> zscore values to specify whether a data point is an outlier or not!</p>
			<p>There is no <a id="_idIndexMarker259"/>standard threshold that you can use to classify outliers. Ideally, you should look at your data and see what makes more sense for you… usually (this is not a rule), you will use some number between 2 and 3 standard deviations from the mean to set outliers, since more than 95% of your data will be out of that range. You may remember that there are outliers <em class="italic">below</em> and <em class="italic">above</em> the mean value of the distribution, as shown in the following table, where we have flagged outliers with an <strong class="bold">absolute</strong> zscore greater than 3 (the value column is hidden for the sake of this demonstration):</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B16735_03_021.jpg" alt="Figure 3.21 – Flagging outliers according to the zscore value&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.21 – Flagging outliers according to the zscore value</p>
			<p>We found two <a id="_idIndexMarker260"/>outliers in the preceding table: row number three and row number five. Another way to find outliers in the data is by applying the <strong class="bold">box plot</strong> logic. You will learn about box plots in more detail in the next chapter of this book. For now, let's focus on using this method to find outliers.</p>
			<p>When we look at a numerical variable, it is possible to extract many descriptive statistics from it, not only the mean, median, minimum, and maximum values, as we have seen previously. Another property that's present in data distributions is known as <strong class="bold">quantiles</strong>.</p>
			<p>Quantiles are <a id="_idIndexMarker261"/>cut-off points that are established at regular intervals from the cumulative distribution function of a random variable. Those regular intervals, also known as <em class="italic">q-quantiles</em>, will be nearly the same size and will receive special names in some situations; for example:</p>
			<ul>
				<li>The 4-quantiles are called quartiles.</li>
				<li>The 10-quantiles are called deciles.</li>
				<li>The 100-quantiles are called percentiles.</li>
			</ul>
			<p>For example, the 20th percentile (of a 100-quantile regular interval) specifies that 20% of the data is below that point. In a box plot, we use regular intervals of 4-quantiles (also known as <em class="italic">quartiles</em>) to expose the distribution of the data (Q1 and Q3), as shown in the following diagram:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B16735_03_022.jpg" alt="Figure 3.22 – Box plot definition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.22 – Box plot definition</p>
			<p>Q1 is also known as the lower quartile or 25th quartile, and this means that 25% of the data is below that point in the distribution. Q3 is also known as the upper quartile or 75<span class="superscript">th</span> quartile, and this means that 75% of the data is below that point in the distribution.</p>
			<p>Computing the <a id="_idIndexMarker262"/>difference between Q1 and Q3 will give you the <strong class="bold">interquartile range (IQR)</strong> value, which you can then use to compute the limits of the box plot, shown by the "minimum" and "maximum" labels in the preceding diagram.</p>
			<p>After all, we can finally infer that anything below the "minimum" value or above the "maximum" value of the box plot will be flagged as an outlier.</p>
			<p>You have now learned about two different ways you can flag outliers on your data: zscore and box plot. You can decide whether you are going to remove these points from your dataset or create another variable to specify that they are, indeed, outliers (as we did in <em class="italic">Figure 3.21</em>).</p>
			<p>Let's continue our journey of data preparation and look at other types of problems we will find in real life. Next, you will learn that several use cases have something known as <strong class="bold">rare events</strong>, which makes<a id="_idIndexMarker263"/> ML algorithms focus on the wrong side of the problem and propose bad solutions. Luckily, we will learn how to either tune them or prepare the data to make them smarter.    </p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor074"/>Dealing with unbalanced datasets</h1>
			<p>At this point, I hope you<a id="_idIndexMarker264"/> have realized why data preparation is probably the longest part of our work. We have learned about data transformation, missing data values, and outliers, but the list of problems goes on. Don't worry – bear with me and let's master this topic together!</p>
			<p>Another well-known problem with ML models, specifically with binary classification problems, is unbalanced classes. In a binary classification model, we say that a dataset is unbalanced when most of its observations belong to the same class (target variable).</p>
			<p>This is very common in fraud identification systems, for example, where most of the events belong to a regular operation, while a very small number of events belong to a fraudulent operation. In this case, we can also say that fraud is a rare event.</p>
			<p>There is no strong rule for defining whether a dataset is unbalanced or not, in the sense of it being necessary to worry about it. Most challenge problems will present more than 99% of the observations in the majority class.</p>
			<p>The problem with unbalanced datasets is very simple: ML algorithms will try to find the best fit in the training data to maximize their accuracy. In a dataset where 99% of the cases belong to one single class, without any tuning, the algorithm is likely to prioritize the assertiveness of the majority class. In the worst-case scenario, it will classify all the observations as the majority class and ignore the minority one, which is usually our interest when modeling.</p>
			<p>To deal with unbalanced datasets, we have two major directions we can follow: tuning the algorithm to handle the issue or resampling the data to make it more balanced.</p>
			<p>By tuning the algorithm, you have to specify the weight of each class under classification. This class weight configuration belongs to the algorithm, not to the training data, so it is a hyperparameter setting. It is important to keep in mind that not all algorithms will have that type of configuration, and that not all ML frameworks will expose it, either. As a quick reference, we can mention the <strong class="source-inline">DecisionTreeClassifier</strong> class, from the scikit-learn ML library, as a good example that does implement the class weight hyperparameter.</p>
			<p>Another way to work <a id="_idIndexMarker265"/>around unbalanced problems is changing the training dataset by applying <strong class="bold">undersampling</strong> or <strong class="bold">oversampling</strong>. If you decide to apply undersampling, all you<a id="_idIndexMarker266"/> have to do is remove <a id="_idIndexMarker267"/>some observations from the majority class until you get a more balanced dataset. Of course, the downside of this approach is that you may lose important information about the majority class that you are removing observations from. </p>
			<p>The most common approach for undersampling is known as random undersampling, which is a naïve resampling approach where we randomly remove some observations from the training set. </p>
			<p>On the other hand, you can decide to go for oversampling, where you will create new observations/samples of the minority class. The simplest approach is the naïve one, where you randomly select observations from the training set (with replacement) for duplication. The downside of this method is the potential issue of overfitting, since you will be duplicating/highlighting the observed pattern of the minority class. </p>
			<p>To either underfit or overfit of your model, you should always test the fitted model on your testing set.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The testing set cannot be under/over sampled: only the training set should pass through these resampling techniques.</p>
			<p>You can also oversample the training set by applying synthetic sampling techniques. Random oversample does not add any new information to the training set: it just duplicates the existing ones. By creating synthetic samples, you are deriving those new observations from the existing ones (instead of simply copying them). This is a type of data augmentation technique<a id="_idIndexMarker268"/> known as the <strong class="bold">Synthetic Minority Oversampling Technique</strong> (<strong class="bold">SMOTE</strong>).   </p>
			<p>Technically, what SMOTE does is plot a line in the feature space of the minority class and extract points that are close to that line.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You may find questions in your exam where the term SMOTE has been used. If that happens, keep the context where this term is applied in mind: oversampling.</p>
			<p>Now, let's move on to the next topic regarding data preparation, where we will learn how to prepare text data for machine learning models.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor075"/>Dealing with text data</h1>
			<p>We have already learned<a id="_idIndexMarker269"/> how to transform categorical features into numerical representations, either using label encoders, ordinal encoders, or one-hot encoding. However, what if we have fields containing long piece of text in our dataset? How are we <a id="_idIndexMarker270"/>supposed to provide a mathematical representation for them in order to properly feed ML algorithms? This is a common issue in <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), a subfield of AI.</p>
			<p>NLP models aim to extract knowledge from texts; for example, translating text between languages, identifying entities in a corpus of text (also known as <strong class="bold">Name Entity Recognition</strong> (<strong class="bold">NER</strong>)), classifying <a id="_idIndexMarker271"/>sentiments from a user review, and many other applications.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In <a href="B16735_02_Final_VK_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a><em class="italic">, AWS Application Services for AI/ML</em>, you learned about some AWS application services that apply NLP to their solutions, such as Amazon Translate and Amazon Comprehend. During the exam, you might be asked to think about the fastest or easiest way (with the least development effort) to build certain types of NLP applications. The fastest or easiest way is usually to use those out of the box AWS services, since they offer pre-trained models for some use cases (especially machine translation, sentiment analysis, topic modeling, document classification, and entity recognition). </p>
			<p class="callout">In a few chapters' time, you will also <a id="_idIndexMarker272"/>learn about some built-in AWS algorithms for NLP applications, such as BlazingText, <strong class="bold">Latent Dirichlet Allocation</strong> (<strong class="bold">LDA</strong>), <strong class="bold">Neural Topic Modeling</strong>, and the Sequence-to-Sequence algorithm. Those algorithms <a id="_idIndexMarker273"/>also let you create the same NLP solutions that are created by those out of box services; however, you have to use them on SageMaker and write your own solution. In other words, they offer more flexibility, but demand more development effort. </p>
			<p class="callout">Keep that in mind for your exam!</p>
			<p>Although AWS <a id="_idIndexMarker274"/>offers many out of the box services and built-in algorithms that allow us to create NLP applications, we will not look at those AWS product features now (as we did in <a href="B16735_02_Final_VK_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a><em class="italic">, AWS Application Services for AI/ML</em>, and will do so again in <a href="B16735_07_Final_VK_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 7</em></a><em class="italic">, Applying Machine Learning Algorithms</em>). We will finish this chapter by looking at some data preparation techniques that are extremely important for preparing your data for NLP.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor076"/>Bag of words</h2>
			<p>The first one we will cover is <a id="_idIndexMarker275"/>known as <strong class="bold">bag of words</strong> (<strong class="bold">BoW</strong>). This is a very common and simple technique, applied to text data, that creates matrix representations to describe the number of words within the text. BoW consists of two main steps: creating a vocabulary and creating a representation of the presence of those known words from the vocabulary in the text. These steps can be seen in the following diagram:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B16735_03_023.jpg" alt="Figure 3.23 – BoW in action&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.23 – BoW in action</p>
			<p>First things first, we<a id="_idIndexMarker276"/> usually can't use raw texts to prepare a bag of words representation. There is a data cleansing step where we will lowercase the text; split each work into tokens; remove punctuation, non-alphabetical, and stop words; and, whenever necessary, apply any other custom cleansing technique you may want.</p>
			<p>Once you have cleansed your raw text, you can add each word to a global vocabulary. Technically, this is usually a dictionary of tuples, in the form of (word, number of occurrences); for example, {(apple, 10), (watermelon, 20)}. As I mentioned previously, this is a global dictionary, and you should consider all the texts you are analyzing.</p>
			<p>Now, with the cleansed text and updated vocabulary, we can build our text representation in the form of a matrix, where each column represents one word from the global vocabulary and each row represents a text you have analyzed. The way you represent those texts on each row may vary according to different strategies, such as binary, frequency, and count. Let's dive into these strategies a little more.</p>
			<p>In the preceding diagram, we are processing a single text but trying the three different strategies for bag of words. That's why you can see three rows on that table, instead of just one (in a real scenario, you have to choose one of them for implementation). </p>
			<p>In the first row, we have used a binary strategy, which will assign 1 if the word exists in the global vocabulary and 0 if not. Because our vocabulary was built on a single text, all the words from that text belong to the vocabulary (the reason you can only see 1s in the binary strategy).</p>
			<p>In the second row, we have used a<a id="_idIndexMarker277"/> frequency strategy, which will check the number of occurrences of each word within the text and divide it by the total number of words within the text. For example, the word "this" appears just once (1) and there are seven other words in the text (7), so 1/7 is equal to 0.14.</p>
			<p>Finally, in the third row, we have used a count strategy, which is a simple count of occurrences of each word within the text.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">This note is really important – you are likely to find it in your exam. You may have noticed that our BoW matrix <a id="_idIndexMarker278"/>contains <strong class="bold">unique words</strong> in the <em class="italic">columns</em> and <strong class="bold">each text</strong> representation is in the <em class="italic">rows</em>. If you have 100 long texts with only 50 unique words across them, your BoW matrix will have 50 columns and 100 rows. During your exam, you are likely to receive a list of texts and be asked to prepare the BoW matrix.</p>
			<p>There is one more extremely important <a id="_idIndexMarker279"/>concept you should know about BoW, which is the <strong class="bold">n-gram</strong> configuration. The term n-gram is used to describe the way you would like to look at your vocabulary, either via single words (uni-gram), groups of two words (bi-gram), groups of three words (tri-gram), or even groups of n words (n-gram). So far, we have seen BoW representations using a uni-gram approach, but more sophisticated representations of BoW may use bi-grams, tri-grams, or n-grams.</p>
			<p>The main logic itself does not change, but you need to know how to represent n-grams in BoW. Still using our example from the preceding diagram, a bi-gram approach would combine those words in the following way: [this movie, movie really, really good, good although, although old, old production]. Make sure you understand this before taking the exam.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The power and simplicity of BoW comes from the fact that you can easily come up with a training set to test your algorithms, or even create a baseline model. If you look at <em class="italic">Figure 3.23</em>, can you see that having more data and just adding a classification column to that table, such as good or bad review, would allow us to train a binary classification model to predict sentiments?</p>
			<p>Alright – you might have noticed that many of the awesome techniques that I have introduced for you come with some downsides. The problem with BoW is the challenge of maintaining its vocabulary. We can easily see that, in a huge corpus of texts, the vocabulary size tends to become bigger and bigger and the matrices' representations tend to be sparse (I know – the sparsity issue again). </p>
			<p>One possible way to solve the vocabulary size<a id="_idIndexMarker280"/> issue is by using word hashing (also known in ML as the <strong class="bold">hashing trick</strong>). Hash functions map data of arbitrary sizes to data of a fixed size. This means you can use the hash trick to represent each text with a fixed number of features (regardless of the vocabulary's size). Technically, this hashing space allows collisions (different texts represented by the same features), so this is something to take into account when you're implementing feature hashing.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>TF-IDF</h2>
			<p>Another problem that comes with BoW, especially when we use the frequency strategy to build the feature space, is that more frequent words will strongly boost their scores due to the high number of occurrences within the document. It turns out that, often, those words with high occurrences are not the key words of the document, but just other words that <em class="italic">also</em> appear many times in several other documents. </p>
			<p><strong class="bold">Term Frequency – Inverse Term Frequency</strong> (<strong class="bold">TF-IDF</strong>) helps <a id="_idIndexMarker281"/>penalize these types of words, by checking how frequent they are in other documents and using that information to rescale the frequency of the words within the document.</p>
			<p>At the end of the process, TF-IDF<a id="_idIndexMarker282"/> tends to give more importance to words that are unique to the document (document-specific words). Let's look at a concrete example so that you can understand it in depth.</p>
			<p>Consider that we have a text corpus containing 100 words and the word "Amazon" appears three times. The <strong class="bold">Term Frequency</strong> (<strong class="bold">TF</strong>) of <a id="_idIndexMarker283"/>this word would be 3/100, which is equal to 0.03. Now, suppose we have other 1,000 documents and that the word "Amazon" appears in 50 of these. In this case, the <strong class="bold">Inverse Term Frequency</strong> (<strong class="bold">IDF</strong>) would <a id="_idIndexMarker284"/>be given by the log as 1,000/50, which is equal to 1.30. The TF-IDF score of the word "Amazon", in that specific document under analysis, will be the product of TF * IDF, which is 0.03 * 1.30 (<em class="italic">0.039</em>).</p>
			<p>Let's suppose that instead of 50 documents, the word "Amazon" had also appeared on another 750 documents – in other words, much more frequently than in the prior scenario. In this case, we will not change the TF part of this equation – it is still 0.03. However, the IDF piece will change a little since this time, it will be log 1,000/750, which is equal to <em class="italic">0.0036</em>. As you can see, this time, the word "Amazon" has much less importance than in the previous example.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor078"/>Word embedding</h2>
			<p>Unlike traditional approaches, such as BoW and TD-IDF, modern methods of text representation will take care of the context of the information, as well as the presence or the frequency of words. One very popular and powerful approach that follows this concept is known as <strong class="bold">word embedding</strong>. Word embeddings<a id="_idIndexMarker285"/> create a dense vector of a fixed length that can store information about the context and meaning of the document.</p>
			<p>Each word is represented by a data point in a multidimensional hyper plan, which we call an embedding space. This embedding space will have "n" dimensions, where each of these dimensions refers to a particular position of this dense vector.</p>
			<p>Although it may sound confusing, the concept it is actually pretty simple. Let's suppose we have a list of four words, and we want to plot them in an embedding space of five dimensions. The words are king, queen, live, and castle. The following table shows how to do this:</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B16735_03_024.jpg" alt="Figure 3.24 – An embedding space representation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.24 – An embedding space representation</p>
			<p>Forget the hypothetical <a id="_idIndexMarker286"/>numbers in the preceding table and focus on the data structure; you will see that each word is now represented by "n" dimensions in the embedding space. This process of transforming words into vectors can be performed by many different methods, but the<a id="_idIndexMarker287"/> most popular<a id="_idIndexMarker288"/> ones are <strong class="bold">word2vec</strong> and <strong class="bold">GloVe</strong>.</p>
			<p>Once you have each word represented as a vector of a fixed length, you can apply many other techniques to do whatever you need. One very common task is plotting those "words" (actually, their dimensions) in a hyper plan and, visually, checking how close they are to each other! </p>
			<p>Technically, we don't use this to plot them as-is, since human brains cannot interpret more than three dimensions. Then, we usually apply a dimensionality reduction technique (such as principal component analysis, which you will learn about later) to reduce the number of dimensions to two, and finally plot the words in a cartesian plan. That's why you might have seen pictures like the one at the bottom of the following diagram. Have you ever asked yourself how is it possible to plot words on a graph?</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B16735_03_025.jpg" alt="Figure 3.25 – Plotting words&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.25 – Plotting words</p>
			<p>In case you are wondering how we came up with the dimensions shown in <em class="italic">Figure 3.24</em>, let's dive into it. Again, there are different methods to do this, but let's look at one of the most popular, which uses a co-occurrence matrix with a fixed context window.</p>
			<p>First, we have to come up with<a id="_idIndexMarker289"/> some logic to represent each word, keeping in mind that we also have to take their context into consideration. To <a id="_idIndexMarker290"/>solve the context requirement, we will define a <strong class="bold">fixed context window</strong>, which is going to be responsible for specifying how many words will be grouped together for context learning. For instance, let's set this fixed context window to 2.</p>
			<p>Next, we will create a <strong class="bold">co-occurrence matrix</strong>, which will count the number of occurrences of each pair of words, according to <a id="_idIndexMarker291"/>the pre-defined context window. Let's see this in action. Consider the following text: "I will pass this exam, you will see. I will pass it".</p>
			<p>The context window of the first word "pass" would be the ones in <em class="italic">bold</em>: "<em class="italic">I will</em> pass <em class="italic">this exam</em>, you will see. I will pass it". Considering this logic, let's see how many times each pair of words appears in the context window:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B16735_03_026.jpg" alt="Figure 3.26 – Co-occurrence matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.26 – Co-occurrence matrix</p>
			<p>As you can see, the pair of words "I will" appears <a id="_idIndexMarker292"/>three times when we use a context window of size 2:</p>
			<ol>
				<li value="1"><em class="italic">I will</em> pass this exam, you will see. I will pass it.</li>
				<li>I will pass this exam, you <em class="italic">will</em> see. <em class="italic">I</em> will pass it.</li>
				<li>I will pass this exam, you will see. <em class="italic">I will</em> pass it.</li>
			</ol>
			<p>Looking at the preceding table,  the same logic should be applied to all other pairs of words, replacing "…" with the associated number of occurrences. You now have a numerical representation for each word!</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You should be aware that there many alternatives to co-occurrence matrices with a fixed context window, such as using TD-IDF vectorization or even simpler counters of words per document. The most important message here is that, somehow, you must come up with a numerical representation for each word.</p>
			<p>The last step is finally finding those dimensions shown in <em class="italic">Figure 3.24</em>. You can do this by creating a multilayer model, usually based on neural networks, where the hidden layer will represent your embedding space. The following diagram shows a simplified example where we could potentially compress our words shown in the preceding table into an embedding space of five dimensions:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B16735_03_027.jpg" alt="Figure 3.27 – Building embedding spaces with neural networks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.27 – Building embedding spaces with neural networks</p>
			<p>We will talk about neural networks in more detail later in this book. For now, understanding where the embedding vector comes from is already an awesome achievement!</p>
			<p>Another important thing you should keep in mind while modeling natural language problems is that you can reuse a pre-trained embedding space in your models. Some companies have created modern neural network architectures, trained on billions of documents, which has become the state of the<a id="_idIndexMarker293"/> art in this field. For your reference, take a look at <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>), which was proposed by Google and has been widely used by the data science community and industry.   </p>
			<p>We have now reached the end of this long – but very important – chapter about data preparation and transformation. Let's take this opportunity to do a quick recap of the awesome things we have learned. </p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Summary</h1>
			<p>First, you were introduced to the different types of features that you might have to work with. Identifying the type of variable you'll be working with is very important for defining the types of transformations and techniques that can be applied to each case.</p>
			<p>Then, we learned how to deal with categorical features. We saw that, sometimes, categorical variables do have an order (such as the ordinal ones), while other times, they don't (such as the nominal ones). You learned that one-hot encoding (or dummy variables) is probably the most common type of transformation for nominal features; however, depending on the number of unique categories, after applying one-hot encoding, your data might suffer from sparsity issues. Regarding ordinal features, you shouldn't create dummy variables on top of them, since you would be losing the information of order that's been incorporated into the variable. In those cases, ordinal encoding is the most appropriate transformation.</p>
			<p>We continued our journey by looking at numerical features, where we learned how to deal with continuous and discrete data. We walked through the most important types of transformations, such as normalization, standardization, binning, and discretization. You saw that some types of transformation rely on the underlying data to find their parameters, so it is very important to avoid using the testing set to learn anything from the data (it must be used strictly for testing). </p>
			<p>You have also seen that we can even apply pure math to transform our data; for example, you learned that power transformations can be used to reduce the skewness of your feature and make it more normal.</p>
			<p>Next, we looked at missing data and got a sense for how critical this task is. When you are modeling, you <em class="italic">can't</em> look at the missing values as a simple computational problem, where you just have to replace x with y. This is a much bigger problem where you need to start solving it by exploring your data, and then checking if your missing data was generated at random or not.</p>
			<p>When you are making the decision to remove or replace missing data, you must be aware that you are either losing information or adding bias to the data, respectively. Remember to review all the important notes we gave you, since they are likely to be, somehow, present in your exam.</p>
			<p>Next, you learned about outlier detection. You looked at different ways to find outliers, such as the zscore and box plot approaches. Most importantly, you learned that you can either flag or smooth them.</p>
			<p>At the start, I told you that this chapter would be a long but worthwhile journey about data preparation, which is why I needed to give you a good sense of how to deal with rare events, since this is one the most challenging problems on ML. You learned that, sometimes, your data might be unbalanced, and you must either trick your algorithm (by changing the class weight) or resample your data (applying undersampling and oversampling).</p>
			<p>Finally, you learned how to deal with text data for NLP. You should now be able to manually compute bag of words and TF-IDF matrices! We went even deeper and learned how word embedding works. During this subsection, we learned that we can either create our own embedding space (using many different methods) or reuse a pre-trained one, such as BERT.</p>
			<p>We are done! I am so glad you made it here and I am sure this chapter is crucial to your success in the exam. Finally, we have prepared some practice questions for you; I hope you enjoy them.</p>
			<p>In the next chapter, we will dive into data visualization techniques.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Questions</h1>
			<ol>
				<li value="1">You are working as a data scientist for a healthcare company and are creating a machine learning model to predict fraud, waste, and abuse across the company's claims. One of the features of this model is the number of times a particular drug has been prescribed, to the same patient of the claim, in a period of 2 years. Which type of feature is this?<p>a) Discrete</p><p>b) Continuous</p><p>c) Nominal</p><p>d) Ordinal </p><p class="callout-heading">Answer</p><p class="callout">a, The feature is counting the number of times that a particular drug has been prescribed. Individual and countable items are classified as discrete data.</p></li>
				<li>You are building a ML model for an educational group that owns schools and universities across the globe. Your model aims to predict how likely a particular student is to leave his/her studies. Many factors may contribute to school dropout, but one of your features is the current academic stage of each student: preschool, elementary school, middle school, or high school. Which type of feature is this?<p>a) Discrete</p><p>b) Continuous</p><p>c) Nominal</p><p>d) Ordinal </p><p class="callout-heading">Answer</p><p class="callout">d, The feature has an implicit order and should be considered a categorical/ordinal variable.</p></li>
				<li>You are building a machine learning model for a car insurance company. The company wants to create a binary classification model that aims to predict how likely their insured vehicles are to be stolen. You have considered many features to this model, including the type of vehicle (economy, compact, premium, luxury, van, sport, and convertible). How would you transform the type of vehicle in order to use it in your model?<p>a) Applying ordinal encoding  </p><p>b) Applying one-hot encoding</p><p>c) No transformation is needed</p><p>d) Options A and B are valid transformations for this problem</p><p class="callout-heading">Answer</p><p class="callout">b, In this case, we have a categorical/nominal variable (there is no order among each category). Additionally, the number of unique categories looks pretty manageable; furthermore, one-hot encoding would fit very well for this type of data.</p></li>
				<li>You are working as a data scientist for a financial company. The company wants to create a model that aims to classify improper payments. You have decided to use "type of transaction" as one of your features (local, international, pre-approved, and so on). After applying one-hot encoding to this variable, you realize that your dataset has many more variables, and your model is taking a lot of time to train. How could you potentially solve this problem?<p>a) By applying ordinal encoding instead of one-hot encoding. In this case, we would be creating just one feature.</p><p>b) By applying label encoding.</p><p>c) By analyzing which types of transactions have the most impact on improper/proper payments. Only apply one-hot encoding to the reduced types of transactions.</p><p>d) By porting your model to another programming language that can handle sparse data in a better way.</p><p class="callout-heading">Answer</p><p class="callout">c, Your transformation resulted in more features due to the excessive number of categories in the original variable. Although the one-hot encoding approach looks right, since the variable is a nominal feature, the number of levels (unique values) for that feature is probably too high. </p><p class="callout">In this case, you could do exploratory data analysis to understand the most important types of transactions for your problem. Once you know that information, you can then restrict the transformation to just those specific types (reducing the sparsity of your data). It is worth adding that you would be missing some information during this process because now, your dummy variables would only be focusing only on a subset of categories, but it is a valid approach. </p></li>
				<li>You are working as a data scientist for a marketing company. Your company is building a clustering model that will be used to segment customers. You decided to normalize the variable "annual income", which ranges from between 50,000 and 300,000.<p>After applying normalization, what would be the normalized values of a group of customers that earn 50,000, 125,000 and 300,000? </p><p>a) 1, 2, and 3</p><p>b) 0, 0.5, and 1</p><p>c) 0, 0.25, and 1</p><p>d) 5, 12, and 30</p><p class="callout-heading">Answer</p><p class="callout">b, Applying the normalization formula and assuming the expected range would be 0 and 1, the correct answer is b.</p></li>
				<li>Consider a dataset that stores the salaries of employees in a particular column. The mean value of salaries on this column is $2,000, while the standard deviation is equal to $300. What is the standard scaled value of someone that earns $3,000?<p>a) 3.33</p><p>b) 6.66</p><p>c) 10</p><p>d) 1</p><p class="callout-heading">Answer</p><p class="callout">a, Remember the standard scale formula: (X - µ) / σ, which is (3,000 – 2,000) / 300.</p></li>
				<li>Which type of data transformations can we apply to convert a continuous variable into a binary variable?<p>a) Binning and one-hot encoding</p><p>b) Standardization and binning</p><p>c) Normalization and one-hot encoding</p><p>d) Standardization and one-hot encoding</p><p class="callout-heading">Answer</p><p class="callout">a, In this case, we could discretize a continuous variable by applying binning and then getting the dummy variables.</p></li>
				<li>You are a data scientist for a financial company and you have been assigned the task of creating a binary classification model to predict whether a customer will leave the company or not (also known as churn). During your exploratory work, you realize that there is a particular feature (credit utilization amount) with some missing values. This variable is expressed in real numbers; for example, $1,000. What would be the fastest approach to dealing with those missing values, assuming you don't want to lose information?<p>a) Dropping any missing values.</p><p>b) Creating a classification model to predict the credit utilization amount and use it to predict the missing data.</p><p>c) Creating a regression model to predict the credit utilization amount and use it to predict the missing data.</p><p>d) Replacing the missing data with the mean or median value of the variable.</p><p class="callout-heading">Answer</p><p class="callout">d, In this case, almost all the options are valid approaches to deal with missing data for this problem, except option b because predicting the missing data would require you to create a regression model, not a classification model. Option a is a valid approach to deal with missing data, but not to this problem where we don't want to lose information. Option c is also a valid approach, but not the fastest one. Option d is the most appropriate answer for this question.</p></li>
				<li>You have to create a machine learning model for a particular client, but you realize that most of the features have more than 50% of data missing. What's our best option on this critical case?<p>a) Dropping entire columns with more than 50% of missing data</p><p>b) Removing all the rows that contains at least one missing information</p><p>c) Check with the dataset owner if you can retrieve the missing data from somewhere else</p><p>d) Replace the missing information by the mean or median of the feature</p><p class="callout-heading">Answer</p><p class="callout">c, This is a very critical case where most of your information is actually missing. You should work with the dataset owner to understand why that problem is occurring and check the process that generates this data. If you decide to drop missing values, you would be losing a lot of information. On the other hand, if you decide to replace missing values, you would be adding a lot of bias to your data.</p></li>
				<li>You are working as a senior data scientist from a human resource company. You are creating a particular machine learning model that uses an algorithm that does not perform well on skewed features, such as the one in the following image:<div id="_idContainer072" class="IMG---Figure"><img src="image/B16735_03_028.jpg" alt=""/></div><p class="figure-caption">Figure 3.28 – Skewed feature</p><p>Which transformations could you apply to this feature to reduce its skewness (choose all the correct answers)?</p><p>a) Normalization</p><p>b) Log transformation</p><p>c) Exponential transformation</p><p>d) Box-Cox transformation</p><p class="callout-heading">Answer</p><p class="callout">b, d, To reduce skewness, power transformations are the most appropriate. Particularly, you could apply the log transformation or Box-Cox transformation to make this distribution more similar to a Gaussian one. </p></li>
				<li>You are working on a fraud identification issue where most of your labeled data belongs to one single class (not fraud). Only 0.1% of the data refers to fraudulent cases. Which modeling techniques would you propose to use on this use case (choose all the correct answers)?<p>a) Applying random oversampling to create copies of the fraudulent cases.</p><p>b) Applying random undersampling to remove observations from the not fraudulent cases.</p><p>c) We can't create a classification model on such an unbalanced dataset. The best thing to do would be to ask for more fraudulent cases from the dataset owner.</p><p>d) Applying synthetic oversampling to create copies of the not fraudulent cases.</p><p class="callout-heading">Answer</p><p class="callout">a, b, Unbalanced datasets are very common with ML, and there are many different ways to deal with this problem. Option c is definitely not the right one. Pay attention to option d; you should be able to apply synthetic oversampling to your data, but to create more observations of the minority class, not from the majority class. Options a and b are correct.</p></li>
				<li>You are preparing text data for machine learning. This time, you want to create a bi-gram BoW matrix on top of the following texts:<p>"I will master this certification exam"</p><p>"I will pass this certification exam"</p><p>How many rows and columns would you have on your BoW matrix representation?</p><p>a) 7 rows and 2 columns</p><p>b) 2 rows and 7 columns</p><p>c) 14 rows and 4 columns</p><p>d) 4 columns and 14 rows</p><p class="callout-heading">Answer</p><p class="callout">b, Let's compute the matrix in the following table  together.</p><p class="callout">As you can see, the trick is knowing which tokens are common to the two texts. We only have two texts; the number of rows will be also two – one for each of them:</p><div id="_idContainer073" class="IMG---Figure"><img src="image/B16735_03_029.jpg" alt=""/></div><p class="figure-caption">Figure 3.29 – Resulting bag of words </p></li>
				<li>You are running a survey to check how many years of experience a group of people has on a particular development tool. You came up with the following distribution:<div id="_idContainer074" class="IMG---Figure"><img src="image/B16735_03_030.jpg" alt=""/></div><p class="figure-caption">Figure 3.30 – Skewed data distribution</p><p>What can you say about the mode, mean, and median of this distribution (choose all the correct answers)?</p><p>a<a id="_idTextAnchor081"/>) The mean is greater than the median</p><p>b) The mean is smaller than the median</p><p>c) The median is greater than the mode</p><p>d) The median is smaller than the mode</p><p class="callout-heading">Answer</p><p class="callout">a, c, This is a skewed distribution (to the right-hand side). This means your mean value will be pushed to the same side of the tail, followed by the median. As result, we will have mode &lt; median &lt; mean.</p></li>
				<li>You are working as a data scientist for a retail company. You are building a decision tree-based model for classification purposes. During your evaluation process, you realize that the model's accuracy is not acceptable. Which of the following tasks is <em class="italic">not</em> a valid approach for increasing the accuracy of your decision tree model?<p>a) Tuning the hyperparameters of the model.</p><p>b) Scaling numerical features.</p><p>c) Trying different approaches for transforming categorical features, such as binary encoding, one-hot encoding, and ordinal encoding (whenever applicable).</p><p>d) If the dataset is not balanced, it is worth trying different types of resampling techniques (undersampling and oversampling).</p><p class="callout-heading">Answer</p><p class="callout">b, Scaling numerical features is an important task with machine learning models but is not always needed. Particularly on decision tree-based models, changing the scale of the data will not result in better model performance, since this type of model is not impacted by the scale of the data.</p></li>
				<li>You are working on a data science project where you must create an NLP model. You have decided to test Word2vec and GloVe during your model development, as an attempt to improve model accuracy. Word2vec and GloVe are two types of what?<p>a) Pre-trained word embeddings</p><p>b) Pre-trained TF-IDF vectors</p><p>c) One-hot encoding techniques</p><p>d) None of above</p><p class="callout-heading">Answer</p><p class="callout">a, Some NPL architectures may include embedding layers. If you are facing this type of project, you can either create your own embedding space by training a specific model for that purpose (using your own corpus of data) or you can use any pre-trained word embedding model, such as the Word2vec model, which is pre-trained by Google. </p></li>
			</ol>
		</div>
	</body></html>