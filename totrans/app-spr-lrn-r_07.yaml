- en: '*Chapter 7:*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第七章：*'
- en: Model Improvements
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型改进
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Explain and implement the concept of bias and variance trade-off in machine
    learning models.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释并实现机器学习模型中偏差和方差权衡的概念。
- en: Perform model assessment with cross-validation.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证进行模型评估。
- en: Implement hyperparameter tuning for machine learning models.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习模型实现超参数调整。
- en: Improve a model's performance with various hyperparameter tuning techniques.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种超参数调整技术提高模型性能。
- en: In this chapter, we will focus on improving a model's performance using cross-validation
    techniques and hyperparameter tuning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注使用交叉验证技术和超参数调整来提高模型性能。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: In the previous chapter, we explored a few strategies that helped us build improved
    models using **feature selection** and **dimensionality reduction**. These strategies
    primarily focus on improving the model's computational performance and interpretability;
    however, to improve the model's performance with respect to performance metrics,
    such as overall accuracy or error estimates to build robust and more generalized
    models, we will need to focus on cross-validation and hyperparameter tuning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了几个策略，这些策略帮助我们通过**特征选择**和**降维**构建改进的模型。这些策略主要关注提高模型的计算性能和可解释性；然而，为了根据性能指标（如整体准确率或误差估计）提高模型性能，构建稳健且更具普遍性的模型，我们需要关注交叉验证和超参数调整。
- en: In this chapter, we will walk you through the fundamental topics in machine
    learning to build generalized and robust models using cross-validation and hyperparameter
    tuning and implement them in R.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您介绍机器学习的核心主题，使用交叉验证和超参数调整构建通用的稳健模型，并在R中实现它们。
- en: We will first study the topics in this chapter in detail with layman examples
    and leverage simple use cases to see the implementation in action.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用通俗易懂的例子详细研究本章的主题，并利用简单的用例来观察实现过程。
- en: Bias-Variance Trade-off
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: An interesting, arduous, and repetitive part of machine learning is the **model
    evaluation journey**. There is again, art and a different mindset required to
    build models that are robust. Throughout this book, we have simplified the model
    evaluation process with training and testing datasets that were derived by splitting
    the available data into a **70:30** or **80:20** ratio. Although this approach
    was effective in helping us understand how the model performs on unseen data,
    it still leaves several loopholes that might render the model futile for most
    other cases. We will need a more formal, thorough, and exhaustive method of validation
    for a machine learning model to be robust for future prediction events. In this
    chapter, we will study **cross-validation** and its various approaches to assess
    the performance of a machine learning model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的一个有趣、艰难且重复的部分是**模型评估之旅**。再次强调，构建稳健模型需要艺术和不同的思维方式。在整个书中，我们通过将可用数据分成**70:30**或**80:20**的比例来简化模型评估过程，从而得到训练集和测试集。尽管这种方法有助于我们理解模型在未见数据上的表现，但它仍然留下了一些可能导致模型在其他大多数情况下无效的漏洞。为了使机器学习模型在未来的预测事件中稳健，我们需要一个更正式、更彻底和更全面的验证方法。在本章中，我们将研究**交叉验证**及其评估机器学习模型性能的各种方法。
- en: Before we delve into the specifics of the topic, we need to explore a crucial
    topic in machine learning called **bias-variance trade-off**. This topic has been
    much in discussion in most machine learning forums and academia. A crucial topic
    to the machine learning fraternity, it forms the foundation before studying model
    validation and improvements in depth. From the title of the topic, it may be easy
    to infer that in machine learning models, the bias-variance trade-off is a behavior
    exhibited by models, where models that showcase low bias in estimating model parameters,
    unfortunately, demonstrate higher variance in estimating model parameters, and
    vice versa. To understand the topic from a layman's perspective, let's first break
    down the topic into individual components, understand each component, and then
    reconstruct the larger picture with all components together.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨这个话题的具体内容之前，我们需要探索机器学习中的一个关键话题，称为**偏差-方差权衡**。这个话题在大多数机器学习论坛和学术界都备受讨论。对于机器学习社区来说，这是一个关键话题，它构成了在深入研究模型验证和改进之前的基础。从话题的标题来看，我们可以很容易地推断出，在机器学习模型中，偏差-方差权衡是模型展示的行为，其中在估计模型参数时表现出低偏差的模型，不幸地表现出更高的方差，反之亦然。为了从普通人的角度理解这个话题，让我们首先将这个话题分解成单个组件，理解每个组件，然后使用所有组件一起重建更大的图景。
- en: What is Bias and Variance in Machine Learning Models?
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习模型中的偏差和方差是什么？
- en: In general, when a machine learning model fails to learn important (or sometimes
    complex) patterns exhibited in data, we say the model is **biased**. Such a model
    oversimplifies itself or only learns extremely simple rules that may not be helpful
    in making accurate predictions. The net outcome from such models is that the predictions
    tend to remain mostly the same (and incorrect), irrespective of the differences
    in input data. The patterns learned by the model are so simple or biased that
    the variations in the input data, unfortunately, don't yield the expected predictions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当一个机器学习模型未能学习到数据中展示的重要（或有时是复杂）模式时，我们说该模型是**有偏差的**。这样的模型过于简化自己，或者只学习极端简单的规则，这些规则可能对做出准确预测没有帮助。这样的模型的结果是，预测结果往往保持大部分相同（且错误），无论输入数据有何差异。模型学习到的模式过于简单或有偏差，不幸的是，输入数据的这些变化并没有产生预期的预测结果。
- en: On the other hand, if we reverse the rationale, we can easily define variance
    in machine learning models. Think about models that learn unnecessary patterns,
    such as noise from the data, such that even small variations in the input data
    lead to significantly large undesirable changes in the prediction. In such cases,
    we say that the model has a high variance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们反转这个逻辑，我们就可以很容易地定义机器学习模型中的方差。想想那些学习不必要模式的模型，比如数据中的噪声，以至于输入数据的微小变化会导致预测结果产生显著的不希望的变化。在这种情况下，我们说该模型具有高方差。
- en: The ideal scenario would be a model with low bias and low variance; that is,
    a model that has learned the necessary patterns from data. It successfully ignored
    the noise and delivers reasonable and desirable changes in predictive behavior
    with reasonable changes in the input data. Unfortunately, the ideal scenario is
    difficult to achieve and thus we arrive at the topic of **the bias-variance trade-off**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的情况是一个具有低偏差和低方差的模型；也就是说，一个从数据中学习到必要模式的模型。它成功地忽略了噪声，并在输入数据合理变化的情况下，合理地改变了预测行为。不幸的是，理想的情况很难实现，因此我们来到了**偏差-方差权衡**这个话题。
- en: 'Putting together all the individual components we studied, we can say every
    attempt to reduce bias or variance will lead to an increase in the other dimension,
    resulting in a situation where we would need to strike a balance between the desired
    bias and variance in model performance. The necessary changes that can be incorporated
    in machine learning models to strike the balance between bias and variance are
    achieved using a combination of hyperparameter tuning methods. We will study the
    concept of hyperparameter tuning in the upcoming sections. The following is a
    famous example used to demonstrate the bias-variance concept with a visual bullseye
    diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们研究过的所有单个组件放在一起，我们可以这样说，每个试图减少偏差或方差的努力都会导致另一个维度的增加，从而产生我们需要在模型性能的期望偏差和方差之间取得平衡的情况。为了在机器学习模型中实现偏差和方差的平衡，我们可以使用超参数调整方法的组合。我们将在接下来的章节中研究超参数调整的概念。以下是一个著名的例子，用视觉靶心图来展示偏差-方差概念：
- en: '![Figure 7.1: The bias-variance concept with a visual bullseye diagram'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.1：使用视觉靶心图表示偏差-方差概念'
- en: '](img/C12624_07_01.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 C12624_07_01.jpg](img/C12624_07_01.jpg)'
- en: 'Figure 7.1: The bias-variance concept with a visual bullseye diagram'
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.1：使用视觉靶心图表示偏差-方差概念
- en: In the previous diagram, we can see four quadrants to specifically distinguish
    the bias-variance trade-off. The diagram is used to interpret the model bias and
    variance for a regression use case. Inferring a similar idea visually for a classification
    use case might be challenging; however, we get the bigger picture with the illustrated
    example.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到四个象限，用于具体区分偏差-方差权衡。该图用于解释回归用例中的模型偏差和方差。对于分类用例，以类似的方式直观地推断可能具有挑战性；然而，通过图例示例，我们获得了更大的图景。
- en: Our ideal goal is to train a machine learning model with low bias and low variance.
    However, when we have low bias and high variance (the top-right quadrant in the
    preceding visualization), we see significantly large changes in the end outcome
    for a small variation in the input data. On the other hand, when we have high
    bias and low variance (the bottom-left quadrant in the visualization), we can
    see the end outcome getting concentrated in a region away from the target, demonstrating
    barely any variations for changes in the input. Lastly, we have high bias and
    high variance, that is, we hit far away from the target, as well as have large
    variations for small changes in the input. This would be the most undesirable
    state for a model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理想的目标是训练一个具有低偏差和低方差的机器学习模型。然而，当我们有低偏差和高方差（前一个可视化中的右上象限）时，我们会看到对于输入数据的小幅变化，最终结果有显著的变化。另一方面，当我们有高偏差和低方差（可视化中的左下象限）时，我们可以看到最终结果集中在远离目标区域，对于输入的变化几乎没有变化。最后，我们具有高偏差和高方差，即我们远离目标，以及对于输入的小幅变化有大的变化。这将是模型最不理想的状态。
- en: Underfitting and Overfitting
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欠拟合和过拟合
- en: In the previous scenario, where we have a high bias, we denote a phenomenon
    called **underfitting** in machine learning models. Similarly, when we have high
    variance, we denote a phenomenon called **overfitting** in machine learning models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的场景中，当我们有高偏差时，我们在机器学习模型中定义了一个称为**欠拟合**的现象。同样，当我们有高方差时，我们在机器学习模型中定义了一个称为**过拟合**的现象。
- en: 'The following visual demonstrates the idea of **overfitting**, **underfitting**,
    and **ideal balance** for a regression model. We can see high bias resulting in
    an oversimplified model (that is, underfitting); high variance resulting in overcomplicated
    models (that is, overfitting); and lastly, striking the right balance between
    bias and variance:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的视觉演示了回归模型中**过拟合**、**欠拟合**和**理想平衡**的概念。我们可以看到高偏差导致过度简化的模型（即欠拟合）；高方差导致过度复杂的模型（即过拟合）；最后，我们在偏差和方差之间找到了正确的平衡：
- en: '![Figure 7.2: Visual demonstration of overfitting, underfitting, and ideal
    balance'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2：过拟合、欠拟合和理想平衡的视觉演示'
- en: '](img/C12624_07_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 C12624_07_02.jpg](img/C12624_07_02.jpg)'
- en: 'Figure 7.2: Visual demonstration of overfitting, underfitting, and ideal balance'
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.2：过拟合、欠拟合和理想平衡的视觉演示
- en: To study bias and variance in machine learning models more effectively, we have
    cross-validation techniques. These techniques help us understand the model performance
    more intuitively.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更有效地研究机器学习模型中的偏差和方差，我们使用了交叉验证技术。这些技术帮助我们更直观地理解模型性能。
- en: Defining a Sample Use Case
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个示例用例
- en: For the purpose of exploring topics in this chapter with a practical dataset,
    we use a small dataset already available in the `mlbench` package, called `PimaIndiansDiabetes`,
    which is a handy dataset for classification use cases.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索本章中的实际数据集，我们使用了`mlbench`包中已可用的小数据集，称为`PimaIndiansDiabetes`，这是一个适用于分类用例的便捷数据集。
- en: The dataset is originally from the National Institute of Diabetes and Digestive
    and Kidney Diseases. The use case that can be tailored from the dataset is when
    predicting if a patient has diabetes as a function of few medical diagnostic measurements.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集最初来自美国糖尿病、消化和肾脏疾病国家研究所。可以从数据集中定制出以下用例：当预测患者是否患有糖尿病作为少量医学诊断测量函数时。
- en: Note
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Additional information can be found at http://math.furman.edu/~dcs/courses/math47/R/library/mlbench/html/PimaIndiansDiabetes.html.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息可以在http://math.furman.edu/~dcs/courses/math47/R/library/mlbench/html/PimaIndiansDiabetes.html找到。
- en: The selection of the use case with a dataset size of less than 1000 rows is
    intentional. The topics explored in this chapter require high computation time
    on commodity hardware for regular use cases with large datasets. The selection
    of small datasets for the purpose of demonstration helps in achieving the outcome
    with fairly normal computational time for most readers using mainstream hardware.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 选择小于 1000 行的数据集作为用例是有意为之的。本章探讨的主题在常规使用案例中需要在大数据集上执行高计算时间。选择小数据集用于演示目的有助于在大多数使用主流硬件的读者中实现相对正常的计算时间。
- en: 'Exercise 88: Loading and Exploring Data'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 88：加载数据和探索数据
- en: 'To quickly study the overall characteristics of `PimaIndiansDiabetes` and explore
    the nature of the contents in each column, perform the following steps:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速研究 `PimaIndiansDiabetes` 的整体特征并探索每列内容的特点，请执行以下步骤：
- en: 'Use the following commands to load the `mlbench`, `randomForest`, and `dplyr`
    libraries:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令加载 `mlbench`、`randomForest` 和 `dplyr` 库：
- en: '[PRE0]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Use the following command to load data from the `PimaIndiansDiabetes` dataset:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从 `PimaIndiansDiabetes` 数据集中加载数据：
- en: '[PRE1]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Explore the dimensions of the dataset and study the content within each column
    using the `str` command:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `str` 命令探索数据集的维度并研究每列的内容：
- en: '[PRE2]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE3]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can see, the dataset has `768` observations and `9` variables, that is,
    *8* independent variables and *1* dependent categorical variable `diabetes` with
    values as `pos` for positive and `neg` for negative.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，数据集有 `768` 个观测值和 `9` 个变量，即 *8* 个独立变量和 *1* 个依赖的类别变量 `diabetes`，其值为 `pos`
    表示阳性，`neg` 表示阴性。
- en: We will use this dataset and develop several classification models for the further
    topics in this chapter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个数据集，并在这个章节的后续主题中开发几个分类模型。
- en: Cross-Validation
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Cross-validation is a model validation technique that aids in assessing the
    performance and ability of a machine learning model to generalize on an independent
    dataset. It is also called **rotation validation**, as it approaches the validation
    of a model with several repetitions by drawing the training and validation data
    from the same distribution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一种模型验证技术，有助于评估机器学习模型在独立数据集上的性能和泛化能力。它也被称为**旋转验证**，因为它通过从同一分布中抽取训练和验证数据，通过多次重复来接近模型的验证。
- en: 'The cross-validation helps us:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证帮助我们：
- en: Evaluate the robustness of the model on unseen data.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型在未见数据上的鲁棒性。
- en: Estimate a realistic range for desired performance metrics.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计期望性能指标的合理范围。
- en: Mitigate overfitting and underfitting of models.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少模型的过拟合和欠拟合。
- en: The general principle of cross-validation is to test the model on the entire
    dataset in several iterations by partitioning data into groups and using majority
    to train and minority to test. The repetitive rotations ensure the model has been
    tested on all available observations. The final performance metrics of the model
    are aggregated and summarized from the results of all rotations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证的一般原则是通过将数据分组并使用多数数据训练和少数数据测试，在多个迭代中在整个数据集上测试模型。重复的旋转确保模型已经在所有可用观测值上进行了测试。模型的最终性能指标是从所有旋转的结果中汇总和总结的。
- en: To study if the model has high bias, we can check the mean (average) performance
    of the model across all rotations. If the mean performance metrics say overall
    accuracy (for classification) or **mean absolute percentage error** (for regression)
    is low, then there is a high bias and the model is underfitting. To study if the
    model has a high variance, we can study the standard deviation of the desired
    performance metrics across rotations. A high standard deviation would indicate
    the model will have high variance; that is, the model will be overfitting.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究模型是否存在高偏差，我们可以检查模型在所有旋转中的平均性能。如果平均性能指标表明整体准确率（对于分类）或**平均绝对百分比误差**（对于回归）较低，那么存在高偏差，模型欠拟合。为了研究模型是否存在高方差，我们可以研究期望性能指标在旋转中的标准差。高标准差将表明模型将具有高方差；也就是说，模型将过拟合。
- en: 'There are several popular approaches in cross-validation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证中有几种流行的方法：
- en: Holdout validation
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留样本验证
- en: K-fold cross-validation
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K 折交叉验证
- en: Hold-one-out validation (LOOCV)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留一个样本验证（LOOCV）
- en: Let's explore each of these approaches in details.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨这些方法中的每一个。
- en: Holdout Approach/Validation
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保留样本方法/验证
- en: This is the easiest approach (though not the most recommended) used in validating
    model performance. We have used this approach throughout the book to test our
    model performance in the previous chapters. Here, we randomly divide the available
    dataset into training and testing datasets. Most common split ratios used between
    the train and test datasets are **70:30** or **80:20**.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在验证模型性能中使用的最简单的方法（尽管不是最推荐的方法）。我们一直在本书中使用这种方法来测试前几章中的模型性能。在这里，我们随机将可用数据集分为训练集和测试集。训练集和测试集之间最常用的分割比例是**70:30**或**80:20**。
- en: The major drawbacks of this approach are that the model performance is purely
    evaluated from a fractional test dataset, and it might not be the best representation
    for the model performance. The evaluation of the model will completely depend
    on the type of split, and therefore, the nature of the data points that end up
    in the training and testing datasets, which might then lead to significantly different
    results and thus high variance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点主要是模型性能完全从分数测试数据集中评估，可能不是模型性能的最佳表示。模型的评估将完全取决于分割的类型，因此，最终进入训练集和测试集的数据点的性质，这可能会导致显著不同的结果和因此高方差。
- en: '![Figure 7.3: Holdout validation'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.3：留出法验证'
- en: '](img/C12624_07_03.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/C12624_07_03.jpg)'
- en: 'Figure 7.3: Holdout validation'
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.3：留出法验证
- en: The following exercise divides the dataset into 70% training and 30% testing,
    and builds a random forest model on the training dataset and then evaluates the
    performance using the testing dataset. This method was widely used in *Chapter
    5*, *Classification*, so you shouldn't be surprised by the process.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下练习将数据集分为70%的训练集和30%的测试集，并在训练集上构建随机森林模型，然后使用测试集评估性能。这种方法在*第五章*，*分类*中广泛使用，所以你对此过程不应该感到惊讶。
- en: 'Exercise 89: Performing Model Assessment Using Holdout Validation'
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习89：使用留出法进行模型评估
- en: 'In this exercise, we will leverage the data we loaded into memory in *Exercise
    1*: *Loading and Exploring the Data*, to create a simple random forest classification
    model and perform model assessment using the holdout validation technique.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将利用在*练习1*：*加载数据和探索数据*中加载到内存中的数据，创建一个简单的随机森林分类模型，并使用留出验证技术进行模型评估。
- en: 'Perform the following steps:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'First, import the `caret` package into the system using the following command.
    The `caret` package provides us with ready-to-use functions for model assessment,
    namely, `ConfusionMatrix`:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令将`caret`包导入系统。`caret`包为我们提供了用于模型评估的现成函数，即`ConfusionMatrix`：
- en: '[PRE4]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, set up the seed for reproducibility as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，按照以下方式设置种子以实现可重复性：
- en: '[PRE5]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create 70% `train` and a 30% `test` dataset using the following command:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建70%的`train`和30%的`test`数据集：
- en: '[PRE6]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Use the `print` function to display the output required:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`print`函数显示所需的输出：
- en: '[PRE7]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create a random forest model by fitting on the `train` dataset:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对`train`数据集进行拟合来创建随机森林模型：
- en: '[PRE8]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Print the model using the following command:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令打印模型：
- en: '[PRE9]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Use the `predict` method on `test` dataset as illustrated here:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下图所示，在`test`数据集上使用`predict`方法：
- en: '[PRE10]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Create and print the `Confusion-Matrix` using the following command:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建并打印`Confusion-Matrix`：
- en: '[PRE11]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Print the overall accuracy using the following command:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令打印总体准确率：
- en: '[PRE12]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.4: Model assessment using holdout validation'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.4：使用留出法进行模型评估'
- en: '](img/C12624_07_04.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/C12624_07_04.jpg)'
- en: 'Figure 7.4: Model assessment using holdout validation'
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.4：使用留出法进行模型评估
- en: We can see the overall accuracy is 77%. This might not be the best representation
    of the model performance as we have only evaluated it on a random test sample.
    The results might be different if we use a different test sample. Let's now explore
    additional cross-validation approaches that overcome this trade-off.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到总体准确率为77%。这可能不是模型性能的最佳表示，因为我们只在一个随机测试样本上进行了评估。如果使用不同的测试样本，结果可能会有所不同。现在，让我们探索其他可以克服这种权衡的交叉验证方法。
- en: K-Fold Cross-Validation
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: This technique is the most recommended approach for model evaluation. In this
    technique, we partition the data into *k* groups and use *k-1* groups for training
    and the remainder (1 group) for validation. The process is repeated *k* times,
    where a new group is used for validation in each successive iteration, and therefore,
    each group is used for testing at one point of time. The overall results are the
    average error estimates across *k* iterations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术是模型评估最推荐的方法。在这个技术中，我们将数据分为*k*组，使用*k-1*组进行训练，剩余的（1组）用于验证。这个过程重复*k*次，在每次迭代中，使用一个新的组进行验证，因此，每个组在某个时刻都会被用于测试。整体结果是*k*次迭代的平均误差估计。
- en: '*k*-fold cross-validations, therefore, overcomes the drawbacks of the holdout
    technique by mitigating the perils associated with the nature of split as each
    data point is tested once over the book of *k* iterations. The variance of the
    model is reduced as the value of *k* increases. The most common values used for
    *k* are 5 or 10\. The major drawback of this technique is that it trains the model
    *k* times (for *k* iterations). Therefore, the total compute time required for
    the model to train and validate is approximately *k* times the holdout method.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*折交叉验证因此克服了保留法技术的缺点，通过减轻与分割性质相关的风险，因为每个数据点在*k*次迭代中都会被测试一次。随着*k*值的增加，模型的方差降低。最常用的*k*值是5或10。这种技术的最大缺点是它需要训练模型*k*次（对于*k*次迭代）。因此，模型训练和验证所需的总计算时间大约是保留法方法的*k*倍。'
- en: 'The following visual demonstrates a five-fold cross-validation and the aggregate
    results (hypothetical) from all rotations:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下可视化演示了五折交叉验证和所有旋转的汇总结果（假设）：
- en: '![Figure 7.5: K-fold validation'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.5：K折验证'
- en: '](img/C12624_07_05.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_07_05.jpg)'
- en: 'Figure 7.5: K-fold validation'
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.5：K折验证
- en: The following code snippet implements the 5-fold cross-validation on the same
    dataset used in the previous example and prints the average accuracy across all
    folds.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段在之前示例中使用相同的数据集执行5折交叉验证，并打印所有折的平均准确率。
- en: 'Exercise 90: Performing Model Assessment Using K-Fold Cross-Validation'
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习90：使用K折交叉验证进行模型评估
- en: We will leverage the same dataset for the use case as from the previous two
    exercises and build a sample random forest classification model and evaluate the
    performance using k-fold cross-validation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用之前两个练习中使用的相同数据集，构建一个随机森林分类模型样本，并使用k折交叉验证评估性能。
- en: 'To perform model assessment using the k-fold cross validation approach, perform
    the following steps:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用k折交叉验证方法进行模型评估，请执行以下步骤：
- en: 'First, import the `caret` package into the system using the following command:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令将`caret`包导入系统：
- en: '[PRE13]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, set the `seed` as `2019` using the following command:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令将`seed`设置为`2019`：
- en: '[PRE14]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, define a function for five-fold cross validation using the following command:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令定义一个用于五折交叉验证的函数：
- en: '[PRE15]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define the value of `mtry` as `3` (to match our previous example):'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`mtry`的值定义为`3`（以匹配我们之前的示例）：
- en: '[PRE16]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Fit the model using the following command:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令拟合模型：
- en: '[PRE17]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, print overall accuracy (averaged across all folds):'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，打印整体准确率（平均所有折）：
- en: '[PRE18]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, print the detailed prediction dataset using the following command:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令打印详细的预测数据集：
- en: '[PRE19]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE20]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As we can see, the overall accuracy has dropped a bit to `76% (rounded off from
    75.9)`. This is the average accuracy from each fold. We have also manually calculated
    the mean and standard deviation of accuracy from each fold toward the end. The
    standard deviation for accuracy across each fold is `2%`, which is considerably
    low and therefore, we can conclude there is low variance. The overall accuracy
    is not low, so the model has a moderately low bias. There is scope for improvement
    in the overall performance, but our model is neither overfitting nor underfitting
    at the moment.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，整体准确率略有下降至`76%（四舍五入自75.9）`。这是每个折的平均准确率。我们还在最后手动计算了每个折的准确率的平均值和标准差。每个折的准确率标准差为`2%`，这相当低，因此，我们可以得出结论，方差较低。整体准确率并不低，所以模型具有适度的低偏差。整体性能有改进的空间，但我们的模型目前既不过拟合也没有欠拟合。
- en: If you observe the code, we used the `trainControl` function that provides us
    with the necessary constructs to define the type of cross-validation with the
    `cv` method, and the number of folds as equal to `5`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您观察代码，我们使用了`trainControl`函数，它为我们提供了必要的结构来定义使用`cv`方法的交叉验证类型，以及折叠数等于`5`。
- en: We use an additional construct to indicate the need to save the prediction,
    which we can later analyze in detail. The `trainControl` object is then passed
    to the `train` function in the `caret` package, where we also define the type
    of algorithm to be used as random forest with `rf` method, and the metric as `tuneGrid`
    construct was ideally not necessary at this point; it is used for hyperparameter
    tuning that we will cover later. However, the `train` function in the `caret`
    package, by default, simplifies the function by using hyperparameter tuning. It
    tries different values of `mtry` in various iterations and returns the final prediction
    with the best value. In order to make apples to apple comparison with the previous
    example, we had to restrict the value of `mtry` to `3`. We, therefore, used the
    `expand.grid` object to define the value of `mtry` to be used in the cross-validation
    process.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个额外的结构来指示需要保存预测，我们可以在以后详细分析。然后，将`trainControl`对象传递给`caret`包中的`train`函数，在那里我们定义要使用的算法类型为随机森林，使用`rf`方法，并将度量作为`tuneGrid`结构在此点理想上不是必要的；它用于超参数调整，我们将在后面介绍。然而，`caret`包中的`train`函数默认通过使用超参数调整来简化函数。它尝试在多次迭代中不同的`mtry`值，并返回具有最佳值的最终预测。为了与前面的例子进行苹果对苹果的比较，我们必须将`mtry`的值限制为`3`。因此，我们使用了`expand.grid`对象来定义在交叉验证过程中使用的`mtry`值。
- en: The train function, when supplied with the `trainControl` object defined for
    cross-validation, divides the data into five partitions and leverages four partitions
    for training and one for testing. The process is repeated five times (*k* is set
    to `5`) and the model is tested on each partition in the dataset iteratively.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当`train`函数提供用于交叉验证的`trainControl`对象时，会将数据分为五个部分，并利用四个部分进行训练，一个部分进行测试。这个过程重复五次（`k`设置为`5`），模型在数据集的每个部分上迭代测试。
- en: We can see the detailed results in the `pred` object (data frame) in the model
    results. Here, we can see the observed (actual) and predicted value of the data
    on each row. It additionally also annotates the value of the hyperparameter used
    in the fold, and the fold number it was a part of for testing.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在模型结果中的`pred`对象（数据表）中看到详细的结果。在这里，我们可以看到每行数据的观测值（实际值）和预测值。此外，它还标注了在折叠中使用的超参数值以及它所属的折叠编号，用于测试。
- en: The `resample` DataFrame in the `model` object records the accuracy and additional
    metrics across each fold in cross-validation. We can explore the average and standard
    deviation of the metrics of our interest to study bias and variance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`model`对象中的`resample` DataFrame 记录了交叉验证中每个折叠的准确性和其他指标。我们可以探索我们感兴趣的指标的均值和标准差，以研究偏差和方差。'
- en: The final take-away from the *k*-fold cross validation is that the accuracy
    of the random forest model for the use case is 76% (that is, the average accuracy
    across all partitions).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从`k`折叠交叉验证中得到的最终结论是，对于该用例，随机森林模型的准确率为76%（即所有部分的平均准确率）。
- en: Hold-One-Out Validation
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单样本留出法验证
- en: In this technique, we take the *k*-fold validation to the logical extreme. Instead
    of creating *k*-partitions where, *k* would be 5 or 10, we choose the number of
    partitions as the number of available data points. Therefore, we would have only
    one sample in a partition. We use all the samples except one for training, and
    test the model on the sample which was held out and repeat this `n` number of
    times, where `n` is the number of training samples. Finally, the average error
    akin to *k*-fold validation is computed. The major drawback of this technique
    is that the model is trained *n* number of times, making it computationally expensive.
    If we are dealing with a fairly large data sample, this validation method is best
    avoided.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种技术中，我们将`k`折叠验证推向了逻辑上的极限。我们不是创建`k`个部分，其中`k`可以是5或10，而是选择部分的数量与可用数据点的数量相同。因此，每个部分只有一个样本。我们使用所有样本除了一个用于训练，并在留出的样本上测试模型，重复`n`次，其中`n`是训练样本的数量。最后，计算类似于`k`折叠验证的平均误差。这种技术的缺点是模型被训练了`n`次，使其计算成本高昂。如果我们处理的是相当大的数据样本，最好避免这种验证方法。
- en: 'Hold-one-out validation is also called **L**eave-**O**ne-**O**ut **C**ross-**V**alidation
    (LOOCV). The following visual demonstrates hold-one-out validation for *n* samples:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 留一法验证也称为**L**eave-**O**ne-**O**ut **C**ross-**V**alidation (LOOCV)。以下视觉演示了对于*n*个样本的留一法验证：
- en: '![Figure 7.6: Hold-one-out validation'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6：留一法验证'
- en: '](img/C12624_07_06.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_07_06.jpg)'
- en: 'Figure 7.6: Hold-one-out validation'
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.6：留一法验证
- en: The following exercise performs **hold-one-out** or leave-one-out cross-validation
    on the same dataset using random forest with the same experimental setup.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下练习使用随机森林和相同的实验设置在相同的dataset上执行留一法或留一法交叉验证。
- en: 'Exercise 91: Performing Model Assessment Using Hold-One-Out Validation'
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 91：使用留一法验证进行模型评估
- en: 'Similar to *Exercise 2*: *Performing Model Assessment using Holdout Validation*
    and *Exercise 3*: *Performing Model Assessment using K-Fold Cross Validation*,
    we will continue to leverage the same dataset and perform hold-one-out validation
    to assess model performance.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 与*练习 2*：*使用留出法进行模型评估*和*练习 3*：*使用K折交叉验证进行模型评估*类似，我们将继续使用相同的dataset，并执行留一法验证来评估模型性能。
- en: 'To perform the model assessment using the hold-one-out validation approach,
    perform the following steps:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用留一法验证方法进行模型评估，执行以下步骤：
- en: 'First, define function for hold-one-out validation using the following command:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令定义留一法验证的函数：
- en: '[PRE21]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, define the value of `mtry` equals `3` (to match our previous example):'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义`mtry`的值为`3`（以匹配我们之前的示例）：
- en: '[PRE22]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Fit the model:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE23]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, print the overall accuracy (averaged across all folds):'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，打印整体准确率（平均所有折）：
- en: '[PRE24]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Print the detailed prediction dataset using the following commands:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令打印详细的预测dataset：
- en: '[PRE25]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE26]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As we can see, the overall accuracy at `77%` is almost the same as *K* fold
    cross-validation (a marginal increase of `1%`). The **LOOCV** construct here stands
    for **Leave-One-Out Cross-Validation**. The process is computationally expensive
    as it iterates the training process for as many times as there are data points
    (in this case, 768).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，整体准确率在`77%`几乎与*K*折交叉验证相同（增加了`1%`）。这里的**LOOCV**结构代表**留一法交叉验证**。这个过程计算成本很高，因为它需要迭代与数据点数量一样多的训练过程（在本例中为768）。
- en: Hyperparameter Optimization
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数优化
- en: '**Hyperparameter optimization** is the process of optimizing or finding the
    most optimal set of hyperparameters for a machine learning model. A hyperparameter
    is a parameter that defines the macro characteristics for a machine learning model.
    It is basically a metaparameter for the model. Hyperparameters are different from
    model parameters; model parameters are learned by the model during the learning
    process, however, hyperparameters are set by the data scientist designing the
    model and cannot be learned by the model.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数优化**是寻找或优化机器学习模型最佳超参数集的过程。超参数是定义机器学习模型宏观特性的参数。它基本上是模型的元参数。超参数与模型参数不同；模型参数是在学习过程中由模型学习的，然而，超参数是由设计模型的科学家设置的，并且不能由模型学习。'
- en: To understand the concept more intuitively, let's explore the topic in layman
    terms. Consider the example of a decision tree model. The tree structure with
    the root node, decision nodes, and leaf nodes are (akin to the beta coefficients
    in logistic regression) are learned through training (fitting) of data. When the
    model finally converges (finds the optimal set of values for model parameters),
    we have the final tree structure that defines the traversal path for the end prediction.
    The macro characteristic for the model is however something different; in the
    case of decision tree, it would be the complexity parameter, denoted by `cp`.
    The complexity parameter, `cp`, restricts the growth of the tree with respect
    to depth; that is, doesn't allow branching of a node if the information gain or
    any other associated metric doesn't yield above a threshold. Applying this new
    rule restricts the depth of the tree beyond a point and helps in generalizing
    the tree better. The complexity parameter is therefore a parameter that defines
    a macro characteristic for the model that then tailors the book of the training
    process, which we call a hyperparameter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地理解这个概念，让我们用通俗易懂的语言来探讨这个话题。以决策树模型为例。树结构，包括根节点、决策节点和叶节点（类似于逻辑回归中的 beta 系数）是通过数据的训练（拟合）来学习的。当模型最终收敛（找到模型参数的最优值集）时，我们就得到了最终的树结构，它定义了最终预测的遍历路径。然而，对于模型来说，宏观特征是不同的；在决策树的情况下，它将是复杂度参数，表示为
    `cp`。复杂度参数 `cp` 限制了树相对于深度的增长；也就是说，如果信息增益或任何其他相关指标没有超过阈值，则不允许节点分支。应用这个新规则限制了树的深度，并有助于更好地泛化树。因此，复杂度参数是一个定义模型宏观特征的参数，然后调整训练过程，我们称之为超参数。
- en: 'Every machine learning algorithm will have a different set of hyperparameters
    associated that will help the model ignore errors (noise), and therefore improve
    generalizing capabilities. A few examples of hyperparameters in machine learning
    algorithms are illustrated in the following table:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习算法都将有一组不同的超参数与之关联，这将帮助模型忽略错误（噪声），从而提高泛化能力。以下表格中展示了机器学习算法中的一些超参数示例：
- en: '![Figure 7.7: Hyperparameters in machine learning algorithms'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.7：机器学习算法中的超参数](img/C12624_07_07.jpg)'
- en: '](img/C12624_07_07.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/C12624_07_07.jpg](img/C12624_07_07.jpg)'
- en: 'Figure 7.7: Hyperparameters in machine learning algorithms'
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.7：机器学习算法中的超参数
- en: Note
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The number of hyperparameters is sometimes different in the implementations
    offered in R and Python. For example, the logistic regression implementation in
    R with the `caret` package doesn't tune the `c` parameter unlike the Python implementation
    in `sklearn`. Similarly, random forest implementation in `sklearn` of Python allows
    the use-of-depth of a tree as a hyperparameter.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 和 Python 提供的实现中，超参数的数量有时是不同的。例如，R 中的 `caret` 包实现的逻辑回归不调整 `c` 参数，而与 `sklearn`
    中的 Python 实现不同。同样，Python 的 `sklearn` 中的随机森林实现允许将树的深度作为超参数使用。
- en: 'Information on gradient-based hyperparameter optimization can be found at the
    following links:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于梯度的超参数优化的信息可以在以下链接中找到：
- en: https://arxiv.org/abs/1502.03492
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1502.03492](https://arxiv.org/abs/1502.03492)'
- en: http://proceedings.mlr.press/v37/maclaurin15.pdf
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://proceedings.mlr.press/v37/maclaurin15.pdf](http://proceedings.mlr.press/v37/maclaurin15.pdf)'
- en: The process of hyperparameter tuning can be summarized as the iterative process
    of finding the optimal set of values for hyperparameters that results in the best
    machine learning model for our task of prediction. There are several approaches
    that can be taken to achieve this. With the fact that this process is iterative,
    we can affirm that there would be several approaches to optimize the path used
    to find the optimal set of values. Let's discuss in depth the broad strategies
    that can adopted for hyperparameter tuning.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整的过程可以概括为寻找最优超参数值集的迭代过程，以实现最佳的机器学习模型。有几种方法可以实现这一点。鉴于这个过程是迭代的，我们可以断定会有几种方法来优化寻找最优值集的路径。让我们深入讨论可以采用的广泛策略，用于超参数调整。
- en: Grid Search Optimization
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索优化
- en: The most naïve approach to find the optimal set of hyperparameters for a model
    would be to use **brute-force** methods and iterate with every combination of
    values for the hyperparameters and then find the most optimal combination. This
    will deliver the desired results, but not in the desired time. In most cases,
    the models we train will be significantly large and require heavy compute time
    for training. Iterating through each combination wouldn't be an ideal option.
    To improve upon the brute-force method, we have grid search optimization; as the
    name has already indicated, here, we define a grid of values that will be used
    for an exhaustive combination of values of hyperparameters to iterate.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找模型最优超参数集的最简单方法就是使用**暴力搜索**方法，迭代每个超参数值的组合，然后找到最优化组合。这将产生期望的结果，但不是在期望的时间内。在大多数情况下，我们训练的模型将非常大，需要大量的计算时间进行训练。迭代每个组合不是理想的选择。为了改进暴力搜索方法，我们有了网格搜索优化；正如其名所示，在这里，我们定义了一个值网格，将用于迭代超参数值的全面组合。
- en: In layman's terms, for grid search optimization, we define a finite set of values
    for each hyperparameter that we would be interested in optimizing for the model.
    The model is then trained for exhaustive combinations of all possible hyperparameter
    values and the combination with the best performance is selected as the optimal
    set.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 用通俗易懂的话来说，对于网格搜索优化，我们为每个我们感兴趣要优化的超参数定义一个有限值的集合。然后，模型将针对所有可能的超参数值的组合进行训练，并选择表现最佳的组合作为最优集。
- en: 'The following diagram demonstrates the idea of grid search optimization for
    a hypothetical set of parameters. Using the hyperparameter grid, the combinations
    are defined, and the model is trained for each combination:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了针对假设参数集的网格搜索优化理念。使用超参数网格定义组合，并对每个组合进行模型训练：
- en: '![Figure 7.8: The hyperparameter grid and combinations'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.8：超参数网格和组合'
- en: '](img/C12624_07_08.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/C12624_07_08.jpg)'
- en: 'Figure 7.8: The hyperparameter grid and combinations'
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.8：超参数网格和组合
- en: The advantage of grid search optimization is that it heavily reduces the time
    required to find the optimal set of hyperparameters given the limited set of candidate
    values to iterate upon (when compared to brute force). However, this comes with
    a trade-off. The grid search optimization model assumes that the optimal value
    of hyperparameter resides within the provided list of candidate values for each
    hyperparameter. If we don't provide the best value as a candidate value in the
    list (grid), we will never have the optimal set of values for the algorithm. Therefore,
    we would need to explore some suggestions for most recommended list of values
    for each hyperparameter before finalizing the list of candidate values. Hyperparameter
    optimization works best for experienced data science professionals, who have strong
    judgement for a variety of different machine learning problems.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索优化的优势在于，它大大减少了在有限的候选值集合上迭代寻找最优超参数集所需的时间（与暴力搜索相比）。然而，这也带来了一定的权衡。网格搜索优化模型假设每个超参数的最优值位于为每个超参数提供的候选值列表中。如果我们不将最佳值作为候选值列入列表（网格），我们将永远不会得到算法的最优值集。因此，在最终确定候选值列表之前，我们需要探索每个超参数最推荐值列表的一些建议。超参数优化最适合经验丰富的数据科学专业人士，他们对于各种不同的机器学习问题都有很强的判断力。
- en: We define hyperparameters for machine learning models that tailor the book of
    learning (fitting) for the models.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为机器学习模型定义超参数，以定制模型的学习（拟合）过程。
- en: 'Exercise 92: Performing Grid Search Optimization – Random Forest'
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习92：执行网格搜索优化 – 随机森林
- en: In this exercise, we will perform grid search optimization for the model using
    the `caret` package, where we define a grid of the values that we want to test
    and evaluate for the best model. We will use the random forest algorithm on the
    same dataset as was used in the previous topic.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用`caret`包对模型执行网格搜索优化，其中我们定义了一个要测试和评估的最佳模型的值网格。我们将使用与之前主题中相同的随机森林算法和数据集。
- en: 'Perform the following steps:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'First, set the `seed` as `2019` using the following command:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令将`seed`设置为`2019`：
- en: '[PRE27]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, define the cross-validation method using the following command:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令定义交叉验证方法：
- en: '[PRE28]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, define `parameter_grid` as illustrated here:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义`parameter_grid`如下所示：
- en: '[PRE29]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Fit the model with cross-validation and grid search optimization:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用交叉验证和网格搜索优化拟合模型：
- en: '[PRE30]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Print the overall accuracy (averaged across all folds for each hyperparameter
    combination):'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印整体准确率（平均每个超参数组合的所有折数）：
- en: '[PRE31]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE32]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Print the detailed prediction dataset:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印详细的预测数据集：
- en: '[PRE33]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Plot the grid metrics:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制网格度量图：
- en: '[PRE34]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output is as follows:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.9: The accuracy of the random forest model across various values
    of hyperparameter'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.9：随机森林模型在不同超参数值下的准确率'
- en: '](img/C12624_07_09.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_07_09.jpg)'
- en: 'Figure 7.9: The accuracy of the random forest model across various values of
    hyperparameter'
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.9：随机森林模型在不同超参数值下的准确率
- en: As we can see, the best results in terms of accuracy were delivered using the
    `mtry` hyperparameter with the value of 4\. The highlighted portion of the output
    will help you in understanding the overall takeaway process. We used a 5-fold
    cross-validation along with grid search optimization, where we defined a grid
    for the `mtry` hyperparameter with the values of (1,2,3,4,5, and 6). The accuracy
    from each value is also shown, and we can see that the results from `mtry` equal
    to `4` are a notch higher than the others. Lastly, we also printed the final model
    that was returned by the grid search optimization process.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在准确率方面，使用`mtry`超参数的值为4时取得了最佳结果。输出中高亮的部分将帮助您理解整体总结过程。我们使用了5折交叉验证以及网格搜索优化，其中我们为`mtry`超参数定义了值为（1，2，3，4，5和6）的网格。每个值的准确率也显示出来，我们可以看到`mtry`等于`4`的结果比其他结果略高。最后，我们还打印了网格搜索优化过程返回的最终模型。
- en: So far, we have only looked at random forest as a model to implement cross-validation
    and hyperparameter tuning. We can extend this to any other algorithm. Algorithms
    such as **XGBoost** have many more hyperparameters than random forest (with the
    R implementation), and therefore make the overall process a little more computationally
    expensive, as well as complicated. In the following exercise, we perform 5-fold
    cross validation, as well as grid search optimization for XGBoost, on the same
    dataset. The highlighted parts of the code are the changes for XGBoost.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看了随机森林作为实现交叉验证和超参数调整的模型。我们可以将此扩展到任何其他算法。例如**XGBoost**算法比随机森林（使用R实现）有更多的超参数，因此使整个过程在计算上稍微昂贵一些，也更为复杂。在下面的练习中，我们在同一数据集上对XGBoost执行5折交叉验证，以及网格搜索优化。代码中高亮的部分是针对XGBoost的更改。
- en: 'Automated hyperparameter tuning:'
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自动超参数调整：
- en: https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a
- en: 'Exercise 93: Grid Search Optimization – XGBoost'
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习93：网格搜索优化 – XGBoost
- en: Similarly to the previous exercise, we will perform grid search optimization
    on the XGBoost model, instead of random forest, and on a larger set of hyperparameters
    to find the best model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一个练习类似，我们将对XGBoost模型执行网格搜索优化，而不是随机森林，并在一组更大的超参数上找到最佳模型。
- en: 'To perform grid search optimization on the XGBoost model, perform the following
    steps:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 要对XGBoost模型执行网格搜索优化，请执行以下步骤：
- en: 'First, set the `seed` as `2019` using the following command:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令将`seed`设置为`2019`：
- en: '[PRE35]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, import the `dplyr` library using the following command:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令导入`dplyr`库：
- en: '[PRE36]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Define the cross-validation method using the following command:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令定义交叉验证方法：
- en: '[PRE37]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, define the parameter grid as illustrated here:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义参数网格，如图所示：
- en: '[PRE38]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Fit the model with cross-validation and grid search optimization:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用交叉验证和网格搜索优化拟合模型：
- en: '[PRE39]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Print the detailed prediction dataset:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印详细的预测数据集：
- en: '[PRE40]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Print the overall accuracy (averaged across all folds for each hyperparameter
    combination):'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印整体准确率（平均每个超参数组合的所有折数）：
- en: '[PRE41]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE42]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Plot the graph:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制图表：
- en: '[PRE43]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.10: Visualizing the accuracy across various hyperparameter values
    for the XGBoost model'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.10：可视化XGBoost模型在不同超参数值下的准确率'
- en: '](img/C12624_07_10.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_07_10.jpg)'
- en: 'Figure 7.10: Visualizing the accuracy across various hyperparameter values
    for the XGBoost model'
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.10：可视化XGBoost模型在不同超参数值下的准确率
- en: The output from the exercise might seem quite lengthy, but let's quickly summarize
    the results. Since we are performing cross-validation and hyperparameter tuning
    on XGBoost, we would need to provide a grid for a larger number of hyperparameters.
    The first line in the output indicates that the size of the prediction dataset
    is 49152 x 11\. This indicates the exhaustive predictions from each combination
    of hyperparameters across each fold in cross-validation. We have printed the head
    and tail of the prediction dataset (the first and last six rows of data), and
    we can see the predicted outcome for each instance of the model with the associated
    hyperparameters, as well as the corresponding fold.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 练习的输出可能看起来相当长，但让我们快速总结一下结果。由于我们在XGBoost上执行交叉验证和超参数调整，我们需要为更多的超参数提供一个网格。输出中的第一行表明预测数据集的大小为49152
    x 11。这表明了在交叉验证的每个折叠中，每个超参数组合的全面预测。我们已经打印了预测数据集的头部和尾部（数据的前六行和后六行），我们可以看到每个模型实例的预测结果以及相关的超参数，以及相应的折叠。
- en: The next table shows us the best set of values for hyperparameters based on
    the accuracy of the model. We can see that the values of `nrounds=60`, `max_depth=3`,
    `eta=0.01`, `gamma=1`, `colsample_bytree=0.7`, `min_child_weight=1`, and `subsample=0.6`
    returned the best performance for the model.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个表格显示了基于模型准确度的最佳超参数值集。我们可以看到，`nrounds=60`、`max_depth=3`、`eta=0.01`、`gamma=1`、`colsample_bytree=0.7`、`min_child_weight=1`和`subsample=0.6`的值返回了模型的最佳性能。
- en: The next table displays the corresponding accuracy for each combination of hyperparameters
    in the descending order of performance. The best accuracy is in the first line
    of the table that was achieved using the best set of hyperparameters. We achieved
    an accuracy of `76.8%`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个表格显示了按性能降序排列的每个超参数组合的对应准确度。最佳准确度位于表格的第一行，这是使用最佳超参数集实现的。我们实现了`76.8%`的准确度。
- en: Lastly, we plotted the results across hyperparameters. Given the larger number
    of hyperparameters, we have a denser plot showcasing the results. However, we
    can directly check the results for the quadrant with `eta=0.01`, and study the
    variation for max depth and `nrounds` and conclude the best performance is from
    the same combination of hyperparameters.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在超参数上绘制了结果。鉴于超参数的数量较多，我们有一个更密集的图表来展示结果。然而，我们可以直接检查`eta=0.01`象限的结果，并研究最大深度和`nrounds`的变化，从而得出最佳性能来自相同超参数组合的结论。
- en: Random Search Optimization
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机搜索优化
- en: In random search optimization, we overcome one of the disadvantages of grid
    search optimization, which is choosing the best set of optimal values within the
    candidate values for each hyperparameter in the grid. Here, we opt for random
    choices from a distribution (in case of a continuous value for hyperparameters),
    instead of a static list that we would define. In random search optimization,
    we have a wider gamut of options to search from, as the continuous values for
    a hyperparameter are chosen randomly from a distribution. This increases the chances
    of finding the best value for a hyperparameter to a great extent.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机搜索优化中，我们克服了网格搜索优化的一个缺点，即在每个超参数的网格中从候选值中选择最佳的一组最优值。在这里，我们选择从分布中随机选择（对于超参数的连续值），而不是我们定义的静态列表。在随机搜索优化中，我们有更广泛的选项可供搜索，因为超参数的连续值是从分布中随机选择的。这在很大程度上增加了找到超参数最佳值的机会。
- en: Some of us might have already started understanding how random choices can always
    have the possibility of incorporating the best values for a hyperparameter. The
    true answer is that it doesn't always have an absolute advantage over grid search,
    but with a fairly large number of iterations, the chances of finding a more optimal
    set of hyperparameter increases with random search over grid search. There might
    be instances where random search would return less optimal values for hyperparameter
    tuning over grid search, given the random selection of values, however, most data
    science professionals have empirical validations of the fact that with a fairly
    decent number of iterations, random search trumps over grid search for most cases.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们中的一些人可能已经开始了对随机选择如何始终有可能包含超参数的最佳值的理解。真正的答案是，它并不总是比网格搜索有绝对优势，但通过相当大量的迭代，随机搜索相对于网格搜索找到更优超参数集的机会会增加。可能会有随机搜索在给定随机选择值的情况下，对于超参数调整返回比网格搜索更差值的情况，然而，大多数数据科学专业人士都有实证验证，即相当数量的迭代后，随机搜索在大多数情况下优于网格搜索。
- en: Implementation of random search optimization is simplified in the `caret` package.
    We have to define a parameter called `tuneLength`, which will set a maximum cap
    on the number of iterations for random search. The number of iterations would
    be equivalent to the number of times the model will be trained, and therefore
    the higher the number is, the higher the chances of getting the best set of hyperparameters
    and the associated performance boost. However, the higher the number of iterations,
    the higher the compute time required to execute.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索优化的实现已在`caret`包中简化。我们必须定义一个名为`tuneLength`的参数，它将为随机搜索设置一个最大迭代次数的上限。迭代次数相当于模型将被训练的次数，因此数值越高，获得最佳超参数集和关联性能提升的机会就越高。然而，迭代次数越高，执行所需的计算时间也就越长。
- en: In the following exercise, let's perform random search optimization on the random
    forest algorithm for the same dataset.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下练习中，让我们对相同的数据集在随机森林算法上执行随机搜索优化。
- en: 'Exercise 94: Using Random Search Optimization on a Random Forest Model'
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习94：在随机森林模型上使用随机搜索优化
- en: We will extend the optimization process for machine learning models with random
    search optimization. Here, we only define the number of iterations that we would
    like to perform with random combinations of hyperparameter values for the model.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将扩展随机搜索优化在机器学习模型中的优化过程。在这里，我们只定义我们希望执行的迭代次数，这些迭代次数将使用随机组合的超参数值对模型进行。
- en: The aim of this exercise is to perform random search optimization on a random
    forest model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的目的是在随机森林模型上执行随机搜索优化。
- en: 'Perform the following steps:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'First, set the `seed` as `2019` using the following command:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令将`seed`设置为`2019`：
- en: '[PRE44]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Define the cross-validation method as illustrated here:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义交叉验证方法，如图所示：
- en: '[PRE45]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Fit the model with cross-validation and random search optimization:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用交叉验证和随机搜索优化拟合模型：
- en: '[PRE46]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Print the detailed prediction dataset:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印详细的预测数据集：
- en: '[PRE47]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Print the overall accuracy (averaged across all folds for each hyperparameter
    combination):'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印总体准确率（平均每个超参数组合的所有折的准确率）：
- en: '[PRE48]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE49]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Plot the data for the random search optimization of the random forest model:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制随机森林模型随机搜索优化的数据：
- en: '[PRE50]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.11: Visualizing accuracy across values of hyperparameters'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.11：可视化超参数值下的准确率'
- en: '](img/C12624_07_11.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_07_11.jpg)'
- en: 'Figure 7.11: Visualizing accuracy across values of hyperparameters'
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.11：可视化超参数值下的准确率
- en: We set the `tuneLength` parameter to `15`; however, since random forest in R
    only focuses on hyperparameter tuning for 1 parameter, that is, `mtry`, the number
    of iterations is exhausted at `7`. This is because we have only eight independent
    variables in the dataset. In most general cases, it would be advisable to set
    a higher number based on the number of features in the data. We can see the best
    value for `mtry` was found at 7\. The plot showcases the differences between various
    values of `mtry`. The best accuracy we achieved with this model was 76%.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`tuneLength`参数设置为`15`；然而，由于R中的随机森林只关注1个超参数的调优，即`mtry`（特征选择数量），迭代次数在`7`次时耗尽。这是因为数据集中只有八个独立变量。在大多数一般情况下，建议根据数据中的特征数量设置一个更高的数值。我们可以看到`mtry`的最佳值是在7时找到的。该图展示了不同`mtry`值之间的差异。使用此模型我们达到的最佳准确率是76%。
- en: Let's now try the same experiment with XGBoost. Here, we will set `tuneLength`
    to `35`, which will be computationally expensive, that is, *15 x 5 (folds) = 75*
    model iterations. This would take significantly longer to execute than any of
    the previous iterations. If you want to see the results faster, you might have
    to reduce the number of iterations with `tuneLength`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们尝试使用XGBoost进行相同的实验。在这里，我们将`tuneLength`设置为`35`，这将非常耗时，即*15 x 5（折）= 75*模型迭代。这将比之前的任何迭代都要长得多。如果您想更快地看到结果，您可能需要通过减少`tuneLength`的迭代次数。
- en: 'Exercise 95: Random Search Optimization – XGBoost'
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习95：随机搜索优化 – XGBoost
- en: As with random forest, we will perform random search optimization on the XGBoost
    model. The XGBoost model has a larger number of hyperparameters to tune, and therefore
    is more suitable for random search optimization. We will leverage the same dataset
    as in previous exercises to build the XGBoost model and then perform optimization.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林类似，我们将对XGBoost模型进行随机搜索优化。XGBoost模型有更多的超参数需要调优，因此更适合随机搜索优化。我们将利用与之前练习相同的相同数据集来构建XGBoost模型，然后进行优化。
- en: The aim of this exercise is to perform random search optimization on the XGBoost
    model.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的目的是对XGBoost模型进行随机搜索优化。
- en: 'Perform the following steps:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Set the `seed` as `2019` using the following command:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将`seed`设置为`2019`：
- en: '[PRE51]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Define the cross-validation method using the following command:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令定义交叉验证方法：
- en: '[PRE52]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Fit the model with cross-validation and random search optimization:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用交叉验证和随机搜索优化来拟合模型：
- en: '[PRE53]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Print the detailed prediction dataset:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印详细的预测数据集：
- en: '[PRE54]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Print the overall accuracy (averaged across all folds for each hyperparameter
    combination):'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印总体准确率（平均每个超参数组合的所有折的准确率）：
- en: '[PRE55]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output is as follows:'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE56]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '![Figure 7.12: Detailed prediction results'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.12：详细的预测结果'
- en: '](img/C12624_07_12.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 C12624_07_12.jpg](img/C12624_07_12.jpg)'
- en: 'Figure 7.12: Detailed prediction results'
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.12：详细的预测结果
- en: Using *random search optimization*, we can see a different set of parameters
    selected as the optimal combination for the XGBoost model. Here, notice that the
    accuracy of grid search and random search are the same (the differences are marginal),
    however, the parameter values are completely different. The learning rate (`eta`)
    is 0.4, `max_depth` is 1 instead of 3, `colsample_byTree` is 0.8 instead of 0.7,
    and `nrounds` is 50 instead of 60\. We have not passed any of these values as
    candidate values for grid search. In random search, given the wider gamut of options
    to select from, we may have promising results when compared to grid search. This,
    however, comes at a cost of *higher computation time*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*随机搜索优化*，我们可以看到为XGBoost模型选定的不同参数集作为最佳组合。在这里，请注意，网格搜索和随机搜索的准确率相同（差异很小），然而参数值却完全不同。学习率（`eta`）为0.4，`max_depth`为1而不是3，`colsample_byTree`为0.8而不是0.7，`nrounds`为50而不是60。我们没有将这些值作为网格搜索的候选值。在随机搜索中，由于有更广泛的选项可供选择，与网格搜索相比，我们可能会得到更有希望的结果。然而，这却是以*更高的计算时间*为代价的。
- en: Also, the current dataset used is a small one (~800 samples). As the use case
    becomes a more compelling one (with more features and more data), the performance
    difference between random search and grid search might be wider.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当前使用的当前数据集是一个较小的数据集（约800个样本）。随着用例变得更加有说服力（具有更多特征和更多数据），随机搜索和网格搜索之间的性能差异可能会更大。
- en: As a rule of thumb, it is highly recommended to opt for random search over grid
    search, especially in cases where our judgement about the problem space is insignificant.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项经验法则，强烈建议在问题空间判断不显著的情况下，选择随机搜索而不是网格搜索。
- en: Bayesian Optimization
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: One of the major trade-offs within grid search and random search is that both
    techniques do not keep track of the past evaluations of hyperparameter combinations
    used for the model training. Ideally, if there was some artificial intelligence
    were induced in this path that could indicate the process with the historic performance
    on the selected list of hyperparameters and a mechanism to improve performance
    by advancing iterations in the right direction, it would drastically reduce the
    number of iterations required to find the optimal set of values for the hyperparameters.
    Grid search and random search, however, miss on this front and iterate through
    all provided combinations without considering any cues from previous iterations.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索和随机搜索之间的一项主要权衡是，这两种技术都不跟踪用于模型训练的超参数组合的过去评估。理想情况下，如果在这个路径中引入一些人工智能，能够指示所选超参数列表的历史性能以及通过在正确方向上推进迭代来提高性能的机制，这将大大减少找到超参数最优值所需迭代的数量。然而，网格搜索和随机搜索在这方面有所欠缺，它们会迭代所有提供的组合，而不考虑任何来自先前迭代的线索。
- en: With **Bayesian optimization**, we overcome this trade-off by enabling the tuning
    process to keep track of previous iterations and their evaluation by developing
    a probabilistic model that would map the hyperparameters to a probability score
    of the selected loss function (an objective function) for the machine learning
    model. This probabilistic model is also called a **surrogate model** to the primary
    loss function in the machine learning model and in contrast is far easier to optimize
    than the loss function.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**贝叶斯优化**，我们通过开发一个将超参数映射到所选损失函数（目标函数）的概率得分的概率模型，从而克服了这个权衡，该模型能够跟踪先前迭代及其评估。这个概率模型也被称为机器学习模型中主要损失函数的**代理模型**，与损失函数相比，它更容易优化。
- en: 'Discussing the mathematical context and derivations for the process will be
    beyond the scope of the chapter. The overall process in Bayesian optimization
    for hyperparameter tuning can be simplified as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论该过程的数学背景和推导将超出本章的范围。贝叶斯优化中用于超参数调整的整体过程可以简化如下：
- en: Define a surrogate model (a probabilistic mapping of hyperparameters to the
    loss function).
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个代理模型（超参数到损失函数的概率映射）。
- en: Find the optimal parameters for the surrogate model.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到代理模型的最佳参数。
- en: Apply the parameters on primary loss function and update the model in the right
    direction based on results.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将参数应用于主要损失函数，并根据结果在正确的方向上更新模型。
- en: Repeat until defined performance of iterations has been reached.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复执行，直到达到定义的迭代性能。
- en: Using this simplistic framework, Bayesian optimization has in most cases delivered
    the ideal set of hyperparameters with the least number of iterations. As this
    approach keeps track of the past iterations and associated performance, its practices
    being more accurate with increasing amount of data. Bayesian methods are efficient
    and more practical to use as they operate in many ways like the human brain; for
    any task to be performed, we try understanding the initial view of the world,
    and then we improve our understanding based on new experiences. Bayesian hyperparameter
    optimization leverages the same rationale and enables an optimized path in tuning
    the hyperparameters for a model using informed decisions. A paper by *Bergstra
    et al*. (http://proceedings.mlr.press/v28/bergstra13.pdf) neatly explains the
    advantages of Bayesian optimization over random search optimization.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个简单的框架，贝叶斯优化在大多数情况下都通过最少的迭代次数提供了理想的超参数集。由于这种方法跟踪了过去的迭代及其关联的性能，因此随着数据量的增加，其实践变得更加准确。贝叶斯方法效率高且更实用，因为它们在许多方面类似于人脑；对于任何要执行的任务，我们试图理解世界的初始观点，然后根据新的经验改进我们的理解。贝叶斯超参数优化利用了相同的原理，并允许通过明智的决策在调整模型超参数时找到优化的路径。*Bergstra等人*（http://proceedings.mlr.press/v28/bergstra13.pdf）的一篇论文清楚地解释了贝叶斯优化相对于随机搜索优化的优势。
- en: There are several techniques within Bayesian optimization that can be applied
    to hyperparameter tuning in the machine learning domain. A popular formalization
    of the Bayesian approach is **Sequential Model-Based Optimization** (**SMBO**),
    which again has several variations within. Each approach within SMBO varies based
    on the way the surrogate model is defined and the criteria used to evaluate and
    update parameters. A few popular choices for the surrogate model are **Gaussian
    Processes**, **Random Forest Regressions**, and **Tree Parzen Estimators** (**TPE**).
    Similarly, the criteria based on each successive iteration is evaluated in the
    optimization process and leveraged using **UCB**, that is, GP **Upper Confidence
    Bound**, for example, **Expected Improvement** (EI) or **Probability of Improvement**
    (**POI**).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化中有几种技术可以应用于机器学习领域的超参数调整。贝叶斯方法的流行形式是**序列模型优化**（SMBO），它本身又有几种变体。SMBO中的每种方法都根据代理模型的定义和用于评估和更新参数的准则而有所不同。代理模型的一些流行选择包括**高斯过程**、**随机森林回归**和**树帕累托估计器**（TPE）。同样，基于每次迭代后的标准在优化过程中被评估并利用，例如使用**UCB**（即高斯过程**上置信界**），例如**期望改进**（EI）或**改进概率**（POI）。
- en: To implement Bayesian optimization in R, we already have a handful of well-written
    libraries that abstract the entire process for us. `MlBayesOpt` is a popular package
    implemented by Yuya Matsumura in R. It is based on the **Gaussian Process** in
    SMBO for Bayesian optimization and allows the use of several functions to update
    the surrogate model.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中实现贝叶斯优化，我们已经有了一些编写良好的库，可以为我们抽象整个流程。`MlBayesOpt`是由Yuya Matsumura在R中实现的流行包。它基于SMBO（序列模型优化）中的高斯过程进行贝叶斯优化，并允许使用多个函数来更新代理模型。
- en: The following exercise performs Bayesian optimization on the random forest model
    for the `mtry` and minimum node size hyperparameters. The output of the optimization
    process returns the best combination evaluated for each hyperparameter. We would
    then need to implement a regular model using the same combination for hyperparameter
    values.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下练习对随机森林模型中的`mtry`和最小节点大小超参数进行贝叶斯优化。优化过程的输出返回每个超参数评估的最佳组合。然后我们需要实现一个使用相同组合的超参数值的常规模型。
- en: 'Bayesian optimization:'
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贝叶斯优化：
- en: https://towardsdatascience.com/the-intuitions-behind-bayesian-optimization-with-gaussian-processes-7e00fcc898a0
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/the-intuitions-behind-bayesian-optimization-with-gaussian-processes-7e00fcc898a0](https://towardsdatascience.com/the-intuitions-behind-bayesian-optimization-with-gaussian-processes-7e00fcc898a0)'
- en: https://papers.nips.cc/paper/7838-automating-bayesian-optimization-with-bayesian-optimization.pdf
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://papers.nips.cc/paper/7838-automating-bayesian-optimization-with-bayesian-optimization.pdf](https://papers.nips.cc/paper/7838-automating-bayesian-optimization-with-bayesian-optimization.pdf)'
- en: https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)'
- en: 'Exercise 96: Performing Bayesian Optimization on the Random Forest Model'
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习96：在随机森林模型上执行贝叶斯优化
- en: Perform Bayesian optimization for the same dataset and study the output. In
    this optimization technique, we will leverage Bayesian optimization to intuitively
    select the best value for the `mtry` hyperparameter by iterating with the knowledge/context
    of previous iterations.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 对相同的数据集进行贝叶斯优化并研究输出。在这种优化技术中，我们将利用贝叶斯优化通过迭代前一次迭代的已知/上下文知识直观地选择`mtry`超参数的最佳值。
- en: The aim of this exercise is to perform Bayesian optimization for the random
    forest model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的目的是对随机森林模型进行贝叶斯优化。
- en: 'Perform the following steps:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'First, set the `seed` as `2019` using the following command:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令将`seed`设置为`2019`：
- en: '[PRE57]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Import the `MlBayesOpt` library using the following command:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令导入`MlBayesOpt`库：
- en: '[PRE58]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Perform Bayesian optimization for random forest model using the `rf_opt` function
    from the `MlBayesOpt` package:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`MlBayesOpt`包中的`rf_opt`函数对随机森林模型进行贝叶斯优化：
- en: '[PRE59]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output is as follows:'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.13: Output for Bayesian optimization for random forest'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.13：贝叶斯优化随机森林的输出'
- en: '](img/C12624_07_13.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/C12624_07_13.jpg)'
- en: 'Figure 7.13: Output for Bayesian optimization for random forest'
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.13：贝叶斯优化随机森林的输出
- en: The output displays the iterations computed for model evaluation and returns
    the result at the end of each iteration. You can increase the number of iterations
    by increasing the value of `n_iter`. The `init_points` variable defines the number
    of randomly chosen points to sample the target function before Bayesian optimization
    fits the Gaussian process. Additionally, we define the criteria function as `eps`
    parameter is an additional parameter that can be used to tune **EI** and **POI**,
    to balance exploitation against exploration, increasing epsilon will make the
    optimized hyperparameters more spread out across the whole range.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了用于模型评估的迭代次数，并在每次迭代的末尾返回结果。可以通过增加`n_iter`的值来增加迭代次数。`init_points`变量定义了在贝叶斯优化拟合高斯过程之前随机选择的点的数量以采样目标函数。此外，我们定义了标准函数作为`eps`参数，这是一个额外的参数，可以用来调整**EI**和**POI**，以平衡利用和探索，增加epsilon将使优化的超参数在整个范围内分布得更广。
- en: Lastly, we also define the kernel, that is, the correlation function for the
    underlying Gaussian process. This parameter should be a list that specifies the
    type of correlation function along with the smoothness parameter. Popular choices
    are square exponential (the default) or `Matern 5/2`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还定义了核，即底层高斯过程的关联函数。此参数应是一个列表，指定了关联函数的类型以及平滑度参数。流行的选择是平方指数（默认）或`Matern 5/2`。
- en: The result of the Bayesian optimization process returned the best value for
    `mtry` as 6.12 (this needs to be truncated to 6, as `mtry` in **Random Forest**
    doesn't accept decimal values) and the best minimum node size as 17\. We can use
    these parameter settings in the regular implementation of random forest and evaluate
    the model results.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化过程的结果返回了`mtry`的最佳值为6.12（需要截断为6，因为**随机森林**中的`mtry`不接受小数值）和最佳最小节点大小为17。我们可以使用这些参数设置在随机森林的常规实现中，并评估模型结果。
- en: Let's now use the same approach for the XGBoost model on the same use case.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用相同的方法对同一用例的XGBoost模型进行操作。
- en: 'Exercise 97: Performing Bayesian Optimization using XGBoost'
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习97：使用XGBoost进行贝叶斯优化
- en: Similar to the previous exercise, we will perform Bayesian optimization for
    the same dataset and study the output, albeit this time for the XGBoost model.
    Given that XGBoost has a larger set of hyperparameters to optimize, we would need
    to provide a range of values for each hyperparameter of interest to the optimization
    process.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的练习类似，我们将对同一数据集进行贝叶斯优化，并研究输出，尽管这次是针对XGBoost模型。鉴于XGBoost有更多的超参数需要优化，我们需要为优化过程中感兴趣的每个超参数提供一个值范围。
- en: 'To perform Bayesian optimization on the XGBoost model, carry out the following
    steps:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 要在XGBoost模型上执行贝叶斯优化，请执行以下步骤：
- en: 'First, set the `seed` as `2019` using the following command:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令将`seed`设置为`2019`：
- en: '[PRE60]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Perform Bayesian optimization for the XGBoost model using the `xgb_opt` function
    from the `MlBayesOpt` package:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`MlBayesOpt`包中的`xgb_opt`函数对XGBoost模型进行贝叶斯优化：
- en: '[PRE61]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output is as follows:'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.14: Output for Bayesian Optimization using XGBoost'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.14：使用XGBoost进行贝叶斯优化的输出'
- en: '](img/C12624_07_14.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_07_14.jpg)'
- en: 'Figure 7.14: Output for Bayesian Optimization using XGBoost'
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.14：使用XGBoost进行贝叶斯优化的输出
- en: Like random forest, the `xgb_opt` function returns the optimal list of hyperparameters
    with the candidate values. Unlike **Random Forest**, we have a larger list of
    hyperparameters for XGBoost. We need to define the range for which we want the
    Bayesian optimization process to operate for each hyperparameter.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林类似，`xgb_opt`函数返回了包含候选值的最佳超参数列表。与**随机森林**不同，XGBoost有更多的超参数。我们需要定义贝叶斯优化过程为每个超参数操作的值范围。
- en: 'We can see that the best combination of hyperparameters from Bayesian optimization
    is different from what we found for `nrounds` would need to be truncated (as a
    decimal value won''t make sense) when we implement a standard XGBoost Tree model
    the parameters evaluate above namely: `nrounds`, `max_depth`, `eta`, and `subsample`.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，贝叶斯优化得到的最佳超参数组合与我们为`nrounds`找到的不同，当我们实现标准的XGBoost树模型时，上述参数（`nrounds`、`max_depth`、`eta`和`subsample`）需要截断（因为小数值没有意义）。
- en: In the chapter, we studied various cross-validation techniques to perform model
    assessment. A small amendment to the k-fold cross-validation helps us perform
    an improved validation of model performance. Before moving from k-fold to LOOCV,
    we can instead perform repeated k-fold to get a more robust evaluation of the
    model. The process repeats cross-validation multiple times where the folds are
    split in a different way in each repetition. Performing repeated k-fold is a better
    approach than LOOCV.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了各种交叉验证技术以进行模型评估。对k折交叉验证的小幅修改有助于我们进行改进的模型性能验证。在从k折到LOOCV之前，我们可以进行重复的k折，以获得对模型更稳健的评估。这个过程重复进行交叉验证多次，其中每次重复中折的划分方式都不同。进行重复的k折比LOOCV更好。
- en: 'Activity 12: Performing Repeated K-Fold Cross Validation and Grid Search Optimization'
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动十二：执行重复的K折交叉验证和网格搜索优化
- en: In this activity, we will leverage the same dataset (as used in previous exercises),
    train a random forest model, perform repeated k-fold validation 10 times, and
    study the model performance. Within each fold iteration, we can try the different
    grid values of the hyperparameter and have a robust validation for the best model.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将利用相同的数据库（如前述练习中使用），训练一个随机森林模型，进行10次重复的k折验证，并研究模型性能。在每个折迭代中，我们可以尝试不同的超参数网格值，并对最佳模型进行稳健的验证。
- en: The aim of the activity is to perform repeated k-fold cross-validation and grid
    search optimization on the same model.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 活动的目的是对同一模型进行重复的k折交叉验证和网格搜索优化。
- en: 'Perform the following steps:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: Load the required packages (`mlbench`, `caret`, and `dplyr`).
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的包（`mlbench`，`caret`和`dplyr`）。
- en: Load the `PimaIndianDiabetes` dataset into memory from `mlbench` package.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`mlbench`包中将`PimaIndianDiabetes`数据集加载到内存中。
- en: Set a seed value for reproducibility.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个种子值以实现可重复性。
- en: Define the k-fold validation object using the `trainControl` function from `caret`
    package and define `method` as `repeatedcv` instead of `cv`.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`caret`包中的`trainControl`函数定义k折验证对象，并将`method`定义为`repeatedcv`而不是`cv`。
- en: Define an additional construct in the `trainControl` function for the number
    of repeats in the validation of `repeats = 10`.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`trainControl`函数中定义一个额外的结构，用于验证重复次数，`repeats = 10`。
- en: Define the grid for the `mtry` hyperparameter of a random forest model as `(3,4,5)`.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将随机森林模型的`mtry`超参数的网格定义为`(3,4,5)`。
- en: Fit the model with the grid values, cross-validation objects, and random forest
    classifiers.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用网格值、交叉验证对象和随机森林分类器来拟合模型。
- en: Study the model performance by plotting the accuracy across different values
    of the hyperparameter.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过绘制不同超参数值下的准确率来研究模型性能。
- en: 'The final output should be as follows:'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终输出应如下所示：
- en: '![Figure 7.15: Model performance accuracy across different values of the hyperparameter'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.15：不同超参数值下的模型性能准确率'
- en: '](img/C12624_07_15.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12624_07_15.jpg)'
- en: 'Figure 7.15: Model performance accuracy across different values of the hyperparameter'
  id: totrans-346
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.15：不同超参数值下的模型性能准确率
- en: Note
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 461.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第461页找到。
- en: Summary
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned a few important aspects of model performance improvement
    techniques. We started with **Bias-Variance Trade-off** and understood it impacts
    a model's performance. We now know that high bias will result in underfitting,
    whereas high variance will result in overfitting of models, and that achieving
    one comes at the expense of the other. Therefore, in order to build the best models,
    we need to strike the ideal balance between bias and variance in machine learning
    models.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了模型性能改进技术的一些重要方面。我们从**偏差-方差权衡**开始，并了解了它对模型性能的影响。我们现在知道，高偏差会导致欠拟合，而高方差会导致模型过拟合，并且实现一个是以牺牲另一个为代价的。因此，为了构建最佳模型，我们需要在机器学习模型中找到偏差和方差之间的理想平衡。
- en: Next, we explored various types of cross-validation techniques in R that provide
    ready-to-use functions to implement the same. We studied holdout, k-fold, and
    hold-one-out validation approaches to cross-validation and understood how we can
    perform robust assessment of performance of machine learning models. We then studied
    hyperparameter tuning and explored grid search optimization, random search optimization,
    and Bayesian optimization techniques in detail. Hyperparameter tuning of machine
    learning models helped us to develop more generalized models with better performance.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们探讨了R中各种交叉验证技术，这些技术提供了现成的函数来实现相同的功能。我们研究了保留法、k折法和留一法验证方法，了解了如何对机器学习模型的性能进行稳健评估。然后，我们研究了超参数调整，并详细探讨了网格搜索优化、随机搜索优化和贝叶斯优化技术。机器学习模型的超参数调整帮助我们开发出性能更优的更通用模型。
- en: In the next chapter, we will explore the process of deploying a machine learning
    model in the cloud.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨在云中部署机器学习模型的过程。
