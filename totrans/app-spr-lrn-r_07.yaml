- en: '*Chapter 7:*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Improvements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain and implement the concept of bias and variance trade-off in machine
    learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform model assessment with cross-validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement hyperparameter tuning for machine learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve a model's performance with various hyperparameter tuning techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will focus on improving a model's performance using cross-validation
    techniques and hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we explored a few strategies that helped us build improved
    models using **feature selection** and **dimensionality reduction**. These strategies
    primarily focus on improving the model's computational performance and interpretability;
    however, to improve the model's performance with respect to performance metrics,
    such as overall accuracy or error estimates to build robust and more generalized
    models, we will need to focus on cross-validation and hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will walk you through the fundamental topics in machine
    learning to build generalized and robust models using cross-validation and hyperparameter
    tuning and implement them in R.
  prefs: []
  type: TYPE_NORMAL
- en: We will first study the topics in this chapter in detail with layman examples
    and leverage simple use cases to see the implementation in action.
  prefs: []
  type: TYPE_NORMAL
- en: Bias-Variance Trade-off
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An interesting, arduous, and repetitive part of machine learning is the **model
    evaluation journey**. There is again, art and a different mindset required to
    build models that are robust. Throughout this book, we have simplified the model
    evaluation process with training and testing datasets that were derived by splitting
    the available data into a **70:30** or **80:20** ratio. Although this approach
    was effective in helping us understand how the model performs on unseen data,
    it still leaves several loopholes that might render the model futile for most
    other cases. We will need a more formal, thorough, and exhaustive method of validation
    for a machine learning model to be robust for future prediction events. In this
    chapter, we will study **cross-validation** and its various approaches to assess
    the performance of a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Before we delve into the specifics of the topic, we need to explore a crucial
    topic in machine learning called **bias-variance trade-off**. This topic has been
    much in discussion in most machine learning forums and academia. A crucial topic
    to the machine learning fraternity, it forms the foundation before studying model
    validation and improvements in depth. From the title of the topic, it may be easy
    to infer that in machine learning models, the bias-variance trade-off is a behavior
    exhibited by models, where models that showcase low bias in estimating model parameters,
    unfortunately, demonstrate higher variance in estimating model parameters, and
    vice versa. To understand the topic from a layman's perspective, let's first break
    down the topic into individual components, understand each component, and then
    reconstruct the larger picture with all components together.
  prefs: []
  type: TYPE_NORMAL
- en: What is Bias and Variance in Machine Learning Models?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, when a machine learning model fails to learn important (or sometimes
    complex) patterns exhibited in data, we say the model is **biased**. Such a model
    oversimplifies itself or only learns extremely simple rules that may not be helpful
    in making accurate predictions. The net outcome from such models is that the predictions
    tend to remain mostly the same (and incorrect), irrespective of the differences
    in input data. The patterns learned by the model are so simple or biased that
    the variations in the input data, unfortunately, don't yield the expected predictions.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if we reverse the rationale, we can easily define variance
    in machine learning models. Think about models that learn unnecessary patterns,
    such as noise from the data, such that even small variations in the input data
    lead to significantly large undesirable changes in the prediction. In such cases,
    we say that the model has a high variance.
  prefs: []
  type: TYPE_NORMAL
- en: The ideal scenario would be a model with low bias and low variance; that is,
    a model that has learned the necessary patterns from data. It successfully ignored
    the noise and delivers reasonable and desirable changes in predictive behavior
    with reasonable changes in the input data. Unfortunately, the ideal scenario is
    difficult to achieve and thus we arrive at the topic of **the bias-variance trade-off**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting together all the individual components we studied, we can say every
    attempt to reduce bias or variance will lead to an increase in the other dimension,
    resulting in a situation where we would need to strike a balance between the desired
    bias and variance in model performance. The necessary changes that can be incorporated
    in machine learning models to strike the balance between bias and variance are
    achieved using a combination of hyperparameter tuning methods. We will study the
    concept of hyperparameter tuning in the upcoming sections. The following is a
    famous example used to demonstrate the bias-variance concept with a visual bullseye
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: The bias-variance concept with a visual bullseye diagram'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.1: The bias-variance concept with a visual bullseye diagram'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the previous diagram, we can see four quadrants to specifically distinguish
    the bias-variance trade-off. The diagram is used to interpret the model bias and
    variance for a regression use case. Inferring a similar idea visually for a classification
    use case might be challenging; however, we get the bigger picture with the illustrated
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Our ideal goal is to train a machine learning model with low bias and low variance.
    However, when we have low bias and high variance (the top-right quadrant in the
    preceding visualization), we see significantly large changes in the end outcome
    for a small variation in the input data. On the other hand, when we have high
    bias and low variance (the bottom-left quadrant in the visualization), we can
    see the end outcome getting concentrated in a region away from the target, demonstrating
    barely any variations for changes in the input. Lastly, we have high bias and
    high variance, that is, we hit far away from the target, as well as have large
    variations for small changes in the input. This would be the most undesirable
    state for a model.
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting and Overfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous scenario, where we have a high bias, we denote a phenomenon
    called **underfitting** in machine learning models. Similarly, when we have high
    variance, we denote a phenomenon called **overfitting** in machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following visual demonstrates the idea of **overfitting**, **underfitting**,
    and **ideal balance** for a regression model. We can see high bias resulting in
    an oversimplified model (that is, underfitting); high variance resulting in overcomplicated
    models (that is, overfitting); and lastly, striking the right balance between
    bias and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2: Visual demonstration of overfitting, underfitting, and ideal
    balance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.2: Visual demonstration of overfitting, underfitting, and ideal balance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To study bias and variance in machine learning models more effectively, we have
    cross-validation techniques. These techniques help us understand the model performance
    more intuitively.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a Sample Use Case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the purpose of exploring topics in this chapter with a practical dataset,
    we use a small dataset already available in the `mlbench` package, called `PimaIndiansDiabetes`,
    which is a handy dataset for classification use cases.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is originally from the National Institute of Diabetes and Digestive
    and Kidney Diseases. The use case that can be tailored from the dataset is when
    predicting if a patient has diabetes as a function of few medical diagnostic measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Additional information can be found at http://math.furman.edu/~dcs/courses/math47/R/library/mlbench/html/PimaIndiansDiabetes.html.
  prefs: []
  type: TYPE_NORMAL
- en: The selection of the use case with a dataset size of less than 1000 rows is
    intentional. The topics explored in this chapter require high computation time
    on commodity hardware for regular use cases with large datasets. The selection
    of small datasets for the purpose of demonstration helps in achieving the outcome
    with fairly normal computational time for most readers using mainstream hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 88: Loading and Exploring Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To quickly study the overall characteristics of `PimaIndiansDiabetes` and explore
    the nature of the contents in each column, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following commands to load the `mlbench`, `randomForest`, and `dplyr`
    libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following command to load data from the `PimaIndiansDiabetes` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Explore the dimensions of the dataset and study the content within each column
    using the `str` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, the dataset has `768` observations and `9` variables, that is,
    *8* independent variables and *1* dependent categorical variable `diabetes` with
    values as `pos` for positive and `neg` for negative.
  prefs: []
  type: TYPE_NORMAL
- en: We will use this dataset and develop several classification models for the further
    topics in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cross-validation is a model validation technique that aids in assessing the
    performance and ability of a machine learning model to generalize on an independent
    dataset. It is also called **rotation validation**, as it approaches the validation
    of a model with several repetitions by drawing the training and validation data
    from the same distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cross-validation helps us:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the robustness of the model on unseen data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimate a realistic range for desired performance metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigate overfitting and underfitting of models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The general principle of cross-validation is to test the model on the entire
    dataset in several iterations by partitioning data into groups and using majority
    to train and minority to test. The repetitive rotations ensure the model has been
    tested on all available observations. The final performance metrics of the model
    are aggregated and summarized from the results of all rotations.
  prefs: []
  type: TYPE_NORMAL
- en: To study if the model has high bias, we can check the mean (average) performance
    of the model across all rotations. If the mean performance metrics say overall
    accuracy (for classification) or **mean absolute percentage error** (for regression)
    is low, then there is a high bias and the model is underfitting. To study if the
    model has a high variance, we can study the standard deviation of the desired
    performance metrics across rotations. A high standard deviation would indicate
    the model will have high variance; that is, the model will be overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several popular approaches in cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: Holdout validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-fold cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hold-one-out validation (LOOCV)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's explore each of these approaches in details.
  prefs: []
  type: TYPE_NORMAL
- en: Holdout Approach/Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the easiest approach (though not the most recommended) used in validating
    model performance. We have used this approach throughout the book to test our
    model performance in the previous chapters. Here, we randomly divide the available
    dataset into training and testing datasets. Most common split ratios used between
    the train and test datasets are **70:30** or **80:20**.
  prefs: []
  type: TYPE_NORMAL
- en: The major drawbacks of this approach are that the model performance is purely
    evaluated from a fractional test dataset, and it might not be the best representation
    for the model performance. The evaluation of the model will completely depend
    on the type of split, and therefore, the nature of the data points that end up
    in the training and testing datasets, which might then lead to significantly different
    results and thus high variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3: Holdout validation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.3: Holdout validation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The following exercise divides the dataset into 70% training and 30% testing,
    and builds a random forest model on the training dataset and then evaluates the
    performance using the testing dataset. This method was widely used in *Chapter
    5*, *Classification*, so you shouldn't be surprised by the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 89: Performing Model Assessment Using Holdout Validation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will leverage the data we loaded into memory in *Exercise
    1*: *Loading and Exploring the Data*, to create a simple random forest classification
    model and perform model assessment using the holdout validation technique.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the `caret` package into the system using the following command.
    The `caret` package provides us with ready-to-use functions for model assessment,
    namely, `ConfusionMatrix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, set up the seed for reproducibility as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create 70% `train` and a 30% `test` dataset using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `print` function to display the output required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a random forest model by fitting on the `train` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `predict` method on `test` dataset as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create and print the `Confusion-Matrix` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the overall accuracy using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.4: Model assessment using holdout validation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.4: Model assessment using holdout validation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see the overall accuracy is 77%. This might not be the best representation
    of the model performance as we have only evaluated it on a random test sample.
    The results might be different if we use a different test sample. Let's now explore
    additional cross-validation approaches that overcome this trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: K-Fold Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This technique is the most recommended approach for model evaluation. In this
    technique, we partition the data into *k* groups and use *k-1* groups for training
    and the remainder (1 group) for validation. The process is repeated *k* times,
    where a new group is used for validation in each successive iteration, and therefore,
    each group is used for testing at one point of time. The overall results are the
    average error estimates across *k* iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-fold cross-validations, therefore, overcomes the drawbacks of the holdout
    technique by mitigating the perils associated with the nature of split as each
    data point is tested once over the book of *k* iterations. The variance of the
    model is reduced as the value of *k* increases. The most common values used for
    *k* are 5 or 10\. The major drawback of this technique is that it trains the model
    *k* times (for *k* iterations). Therefore, the total compute time required for
    the model to train and validate is approximately *k* times the holdout method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following visual demonstrates a five-fold cross-validation and the aggregate
    results (hypothetical) from all rotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5: K-fold validation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.5: K-fold validation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The following code snippet implements the 5-fold cross-validation on the same
    dataset used in the previous example and prints the average accuracy across all
    folds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 90: Performing Model Assessment Using K-Fold Cross-Validation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will leverage the same dataset for the use case as from the previous two
    exercises and build a sample random forest classification model and evaluate the
    performance using k-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform model assessment using the k-fold cross validation approach, perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the `caret` package into the system using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, set the `seed` as `2019` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, define a function for five-fold cross validation using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the value of `mtry` as `3` (to match our previous example):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, print overall accuracy (averaged across all folds):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, print the detailed prediction dataset using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, the overall accuracy has dropped a bit to `76% (rounded off from
    75.9)`. This is the average accuracy from each fold. We have also manually calculated
    the mean and standard deviation of accuracy from each fold toward the end. The
    standard deviation for accuracy across each fold is `2%`, which is considerably
    low and therefore, we can conclude there is low variance. The overall accuracy
    is not low, so the model has a moderately low bias. There is scope for improvement
    in the overall performance, but our model is neither overfitting nor underfitting
    at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: If you observe the code, we used the `trainControl` function that provides us
    with the necessary constructs to define the type of cross-validation with the
    `cv` method, and the number of folds as equal to `5`.
  prefs: []
  type: TYPE_NORMAL
- en: We use an additional construct to indicate the need to save the prediction,
    which we can later analyze in detail. The `trainControl` object is then passed
    to the `train` function in the `caret` package, where we also define the type
    of algorithm to be used as random forest with `rf` method, and the metric as `tuneGrid`
    construct was ideally not necessary at this point; it is used for hyperparameter
    tuning that we will cover later. However, the `train` function in the `caret`
    package, by default, simplifies the function by using hyperparameter tuning. It
    tries different values of `mtry` in various iterations and returns the final prediction
    with the best value. In order to make apples to apple comparison with the previous
    example, we had to restrict the value of `mtry` to `3`. We, therefore, used the
    `expand.grid` object to define the value of `mtry` to be used in the cross-validation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The train function, when supplied with the `trainControl` object defined for
    cross-validation, divides the data into five partitions and leverages four partitions
    for training and one for testing. The process is repeated five times (*k* is set
    to `5`) and the model is tested on each partition in the dataset iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: We can see the detailed results in the `pred` object (data frame) in the model
    results. Here, we can see the observed (actual) and predicted value of the data
    on each row. It additionally also annotates the value of the hyperparameter used
    in the fold, and the fold number it was a part of for testing.
  prefs: []
  type: TYPE_NORMAL
- en: The `resample` DataFrame in the `model` object records the accuracy and additional
    metrics across each fold in cross-validation. We can explore the average and standard
    deviation of the metrics of our interest to study bias and variance.
  prefs: []
  type: TYPE_NORMAL
- en: The final take-away from the *k*-fold cross validation is that the accuracy
    of the random forest model for the use case is 76% (that is, the average accuracy
    across all partitions).
  prefs: []
  type: TYPE_NORMAL
- en: Hold-One-Out Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this technique, we take the *k*-fold validation to the logical extreme. Instead
    of creating *k*-partitions where, *k* would be 5 or 10, we choose the number of
    partitions as the number of available data points. Therefore, we would have only
    one sample in a partition. We use all the samples except one for training, and
    test the model on the sample which was held out and repeat this `n` number of
    times, where `n` is the number of training samples. Finally, the average error
    akin to *k*-fold validation is computed. The major drawback of this technique
    is that the model is trained *n* number of times, making it computationally expensive.
    If we are dealing with a fairly large data sample, this validation method is best
    avoided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hold-one-out validation is also called **L**eave-**O**ne-**O**ut **C**ross-**V**alidation
    (LOOCV). The following visual demonstrates hold-one-out validation for *n* samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6: Hold-one-out validation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.6: Hold-one-out validation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The following exercise performs **hold-one-out** or leave-one-out cross-validation
    on the same dataset using random forest with the same experimental setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 91: Performing Model Assessment Using Hold-One-Out Validation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to *Exercise 2*: *Performing Model Assessment using Holdout Validation*
    and *Exercise 3*: *Performing Model Assessment using K-Fold Cross Validation*,
    we will continue to leverage the same dataset and perform hold-one-out validation
    to assess model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform the model assessment using the hold-one-out validation approach,
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define function for hold-one-out validation using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define the value of `mtry` equals `3` (to match our previous example):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, print the overall accuracy (averaged across all folds):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the detailed prediction dataset using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, the overall accuracy at `77%` is almost the same as *K* fold
    cross-validation (a marginal increase of `1%`). The **LOOCV** construct here stands
    for **Leave-One-Out Cross-Validation**. The process is computationally expensive
    as it iterates the training process for as many times as there are data points
    (in this case, 768).
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Hyperparameter optimization** is the process of optimizing or finding the
    most optimal set of hyperparameters for a machine learning model. A hyperparameter
    is a parameter that defines the macro characteristics for a machine learning model.
    It is basically a metaparameter for the model. Hyperparameters are different from
    model parameters; model parameters are learned by the model during the learning
    process, however, hyperparameters are set by the data scientist designing the
    model and cannot be learned by the model.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand the concept more intuitively, let's explore the topic in layman
    terms. Consider the example of a decision tree model. The tree structure with
    the root node, decision nodes, and leaf nodes are (akin to the beta coefficients
    in logistic regression) are learned through training (fitting) of data. When the
    model finally converges (finds the optimal set of values for model parameters),
    we have the final tree structure that defines the traversal path for the end prediction.
    The macro characteristic for the model is however something different; in the
    case of decision tree, it would be the complexity parameter, denoted by `cp`.
    The complexity parameter, `cp`, restricts the growth of the tree with respect
    to depth; that is, doesn't allow branching of a node if the information gain or
    any other associated metric doesn't yield above a threshold. Applying this new
    rule restricts the depth of the tree beyond a point and helps in generalizing
    the tree better. The complexity parameter is therefore a parameter that defines
    a macro characteristic for the model that then tailors the book of the training
    process, which we call a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every machine learning algorithm will have a different set of hyperparameters
    associated that will help the model ignore errors (noise), and therefore improve
    generalizing capabilities. A few examples of hyperparameters in machine learning
    algorithms are illustrated in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7: Hyperparameters in machine learning algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.7: Hyperparameters in machine learning algorithms'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The number of hyperparameters is sometimes different in the implementations
    offered in R and Python. For example, the logistic regression implementation in
    R with the `caret` package doesn't tune the `c` parameter unlike the Python implementation
    in `sklearn`. Similarly, random forest implementation in `sklearn` of Python allows
    the use-of-depth of a tree as a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Information on gradient-based hyperparameter optimization can be found at the
    following links:'
  prefs: []
  type: TYPE_NORMAL
- en: https://arxiv.org/abs/1502.03492
  prefs: []
  type: TYPE_NORMAL
- en: http://proceedings.mlr.press/v37/maclaurin15.pdf
  prefs: []
  type: TYPE_NORMAL
- en: The process of hyperparameter tuning can be summarized as the iterative process
    of finding the optimal set of values for hyperparameters that results in the best
    machine learning model for our task of prediction. There are several approaches
    that can be taken to achieve this. With the fact that this process is iterative,
    we can affirm that there would be several approaches to optimize the path used
    to find the optimal set of values. Let's discuss in depth the broad strategies
    that can adopted for hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Grid Search Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most naïve approach to find the optimal set of hyperparameters for a model
    would be to use **brute-force** methods and iterate with every combination of
    values for the hyperparameters and then find the most optimal combination. This
    will deliver the desired results, but not in the desired time. In most cases,
    the models we train will be significantly large and require heavy compute time
    for training. Iterating through each combination wouldn't be an ideal option.
    To improve upon the brute-force method, we have grid search optimization; as the
    name has already indicated, here, we define a grid of values that will be used
    for an exhaustive combination of values of hyperparameters to iterate.
  prefs: []
  type: TYPE_NORMAL
- en: In layman's terms, for grid search optimization, we define a finite set of values
    for each hyperparameter that we would be interested in optimizing for the model.
    The model is then trained for exhaustive combinations of all possible hyperparameter
    values and the combination with the best performance is selected as the optimal
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates the idea of grid search optimization for
    a hypothetical set of parameters. Using the hyperparameter grid, the combinations
    are defined, and the model is trained for each combination:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8: The hyperparameter grid and combinations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.8: The hyperparameter grid and combinations'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The advantage of grid search optimization is that it heavily reduces the time
    required to find the optimal set of hyperparameters given the limited set of candidate
    values to iterate upon (when compared to brute force). However, this comes with
    a trade-off. The grid search optimization model assumes that the optimal value
    of hyperparameter resides within the provided list of candidate values for each
    hyperparameter. If we don't provide the best value as a candidate value in the
    list (grid), we will never have the optimal set of values for the algorithm. Therefore,
    we would need to explore some suggestions for most recommended list of values
    for each hyperparameter before finalizing the list of candidate values. Hyperparameter
    optimization works best for experienced data science professionals, who have strong
    judgement for a variety of different machine learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: We define hyperparameters for machine learning models that tailor the book of
    learning (fitting) for the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 92: Performing Grid Search Optimization – Random Forest'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will perform grid search optimization for the model using
    the `caret` package, where we define a grid of the values that we want to test
    and evaluate for the best model. We will use the random forest algorithm on the
    same dataset as was used in the previous topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, set the `seed` as `2019` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define the cross-validation method using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, define `parameter_grid` as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model with cross-validation and grid search optimization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the overall accuracy (averaged across all folds for each hyperparameter
    combination):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the detailed prediction dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the grid metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.9: The accuracy of the random forest model across various values
    of hyperparameter'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.9: The accuracy of the random forest model across various values of
    hyperparameter'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see, the best results in terms of accuracy were delivered using the
    `mtry` hyperparameter with the value of 4\. The highlighted portion of the output
    will help you in understanding the overall takeaway process. We used a 5-fold
    cross-validation along with grid search optimization, where we defined a grid
    for the `mtry` hyperparameter with the values of (1,2,3,4,5, and 6). The accuracy
    from each value is also shown, and we can see that the results from `mtry` equal
    to `4` are a notch higher than the others. Lastly, we also printed the final model
    that was returned by the grid search optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have only looked at random forest as a model to implement cross-validation
    and hyperparameter tuning. We can extend this to any other algorithm. Algorithms
    such as **XGBoost** have many more hyperparameters than random forest (with the
    R implementation), and therefore make the overall process a little more computationally
    expensive, as well as complicated. In the following exercise, we perform 5-fold
    cross validation, as well as grid search optimization for XGBoost, on the same
    dataset. The highlighted parts of the code are the changes for XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Automated hyperparameter tuning:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 93: Grid Search Optimization – XGBoost'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similarly to the previous exercise, we will perform grid search optimization
    on the XGBoost model, instead of random forest, and on a larger set of hyperparameters
    to find the best model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform grid search optimization on the XGBoost model, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, set the `seed` as `2019` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, import the `dplyr` library using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the cross-validation method using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define the parameter grid as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model with cross-validation and grid search optimization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the detailed prediction dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the overall accuracy (averaged across all folds for each hyperparameter
    combination):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.10: Visualizing the accuracy across various hyperparameter values
    for the XGBoost model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.10: Visualizing the accuracy across various hyperparameter values
    for the XGBoost model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The output from the exercise might seem quite lengthy, but let's quickly summarize
    the results. Since we are performing cross-validation and hyperparameter tuning
    on XGBoost, we would need to provide a grid for a larger number of hyperparameters.
    The first line in the output indicates that the size of the prediction dataset
    is 49152 x 11\. This indicates the exhaustive predictions from each combination
    of hyperparameters across each fold in cross-validation. We have printed the head
    and tail of the prediction dataset (the first and last six rows of data), and
    we can see the predicted outcome for each instance of the model with the associated
    hyperparameters, as well as the corresponding fold.
  prefs: []
  type: TYPE_NORMAL
- en: The next table shows us the best set of values for hyperparameters based on
    the accuracy of the model. We can see that the values of `nrounds=60`, `max_depth=3`,
    `eta=0.01`, `gamma=1`, `colsample_bytree=0.7`, `min_child_weight=1`, and `subsample=0.6`
    returned the best performance for the model.
  prefs: []
  type: TYPE_NORMAL
- en: The next table displays the corresponding accuracy for each combination of hyperparameters
    in the descending order of performance. The best accuracy is in the first line
    of the table that was achieved using the best set of hyperparameters. We achieved
    an accuracy of `76.8%`.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we plotted the results across hyperparameters. Given the larger number
    of hyperparameters, we have a denser plot showcasing the results. However, we
    can directly check the results for the quadrant with `eta=0.01`, and study the
    variation for max depth and `nrounds` and conclude the best performance is from
    the same combination of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Random Search Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In random search optimization, we overcome one of the disadvantages of grid
    search optimization, which is choosing the best set of optimal values within the
    candidate values for each hyperparameter in the grid. Here, we opt for random
    choices from a distribution (in case of a continuous value for hyperparameters),
    instead of a static list that we would define. In random search optimization,
    we have a wider gamut of options to search from, as the continuous values for
    a hyperparameter are chosen randomly from a distribution. This increases the chances
    of finding the best value for a hyperparameter to a great extent.
  prefs: []
  type: TYPE_NORMAL
- en: Some of us might have already started understanding how random choices can always
    have the possibility of incorporating the best values for a hyperparameter. The
    true answer is that it doesn't always have an absolute advantage over grid search,
    but with a fairly large number of iterations, the chances of finding a more optimal
    set of hyperparameter increases with random search over grid search. There might
    be instances where random search would return less optimal values for hyperparameter
    tuning over grid search, given the random selection of values, however, most data
    science professionals have empirical validations of the fact that with a fairly
    decent number of iterations, random search trumps over grid search for most cases.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of random search optimization is simplified in the `caret` package.
    We have to define a parameter called `tuneLength`, which will set a maximum cap
    on the number of iterations for random search. The number of iterations would
    be equivalent to the number of times the model will be trained, and therefore
    the higher the number is, the higher the chances of getting the best set of hyperparameters
    and the associated performance boost. However, the higher the number of iterations,
    the higher the compute time required to execute.
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, let's perform random search optimization on the random
    forest algorithm for the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 94: Using Random Search Optimization on a Random Forest Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will extend the optimization process for machine learning models with random
    search optimization. Here, we only define the number of iterations that we would
    like to perform with random combinations of hyperparameter values for the model.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this exercise is to perform random search optimization on a random
    forest model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, set the `seed` as `2019` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the cross-validation method as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model with cross-validation and random search optimization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the detailed prediction dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the overall accuracy (averaged across all folds for each hyperparameter
    combination):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the data for the random search optimization of the random forest model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.11: Visualizing accuracy across values of hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.11: Visualizing accuracy across values of hyperparameters'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We set the `tuneLength` parameter to `15`; however, since random forest in R
    only focuses on hyperparameter tuning for 1 parameter, that is, `mtry`, the number
    of iterations is exhausted at `7`. This is because we have only eight independent
    variables in the dataset. In most general cases, it would be advisable to set
    a higher number based on the number of features in the data. We can see the best
    value for `mtry` was found at 7\. The plot showcases the differences between various
    values of `mtry`. The best accuracy we achieved with this model was 76%.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now try the same experiment with XGBoost. Here, we will set `tuneLength`
    to `35`, which will be computationally expensive, that is, *15 x 5 (folds) = 75*
    model iterations. This would take significantly longer to execute than any of
    the previous iterations. If you want to see the results faster, you might have
    to reduce the number of iterations with `tuneLength`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 95: Random Search Optimization – XGBoost'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with random forest, we will perform random search optimization on the XGBoost
    model. The XGBoost model has a larger number of hyperparameters to tune, and therefore
    is more suitable for random search optimization. We will leverage the same dataset
    as in previous exercises to build the XGBoost model and then perform optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this exercise is to perform random search optimization on the XGBoost
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the `seed` as `2019` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the cross-validation method using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model with cross-validation and random search optimization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the detailed prediction dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the overall accuracy (averaged across all folds for each hyperparameter
    combination):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 7.12: Detailed prediction results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.12: Detailed prediction results'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using *random search optimization*, we can see a different set of parameters
    selected as the optimal combination for the XGBoost model. Here, notice that the
    accuracy of grid search and random search are the same (the differences are marginal),
    however, the parameter values are completely different. The learning rate (`eta`)
    is 0.4, `max_depth` is 1 instead of 3, `colsample_byTree` is 0.8 instead of 0.7,
    and `nrounds` is 50 instead of 60\. We have not passed any of these values as
    candidate values for grid search. In random search, given the wider gamut of options
    to select from, we may have promising results when compared to grid search. This,
    however, comes at a cost of *higher computation time*.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the current dataset used is a small one (~800 samples). As the use case
    becomes a more compelling one (with more features and more data), the performance
    difference between random search and grid search might be wider.
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, it is highly recommended to opt for random search over grid
    search, especially in cases where our judgement about the problem space is insignificant.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the major trade-offs within grid search and random search is that both
    techniques do not keep track of the past evaluations of hyperparameter combinations
    used for the model training. Ideally, if there was some artificial intelligence
    were induced in this path that could indicate the process with the historic performance
    on the selected list of hyperparameters and a mechanism to improve performance
    by advancing iterations in the right direction, it would drastically reduce the
    number of iterations required to find the optimal set of values for the hyperparameters.
    Grid search and random search, however, miss on this front and iterate through
    all provided combinations without considering any cues from previous iterations.
  prefs: []
  type: TYPE_NORMAL
- en: With **Bayesian optimization**, we overcome this trade-off by enabling the tuning
    process to keep track of previous iterations and their evaluation by developing
    a probabilistic model that would map the hyperparameters to a probability score
    of the selected loss function (an objective function) for the machine learning
    model. This probabilistic model is also called a **surrogate model** to the primary
    loss function in the machine learning model and in contrast is far easier to optimize
    than the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussing the mathematical context and derivations for the process will be
    beyond the scope of the chapter. The overall process in Bayesian optimization
    for hyperparameter tuning can be simplified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a surrogate model (a probabilistic mapping of hyperparameters to the
    loss function).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the optimal parameters for the surrogate model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the parameters on primary loss function and update the model in the right
    direction based on results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat until defined performance of iterations has been reached.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this simplistic framework, Bayesian optimization has in most cases delivered
    the ideal set of hyperparameters with the least number of iterations. As this
    approach keeps track of the past iterations and associated performance, its practices
    being more accurate with increasing amount of data. Bayesian methods are efficient
    and more practical to use as they operate in many ways like the human brain; for
    any task to be performed, we try understanding the initial view of the world,
    and then we improve our understanding based on new experiences. Bayesian hyperparameter
    optimization leverages the same rationale and enables an optimized path in tuning
    the hyperparameters for a model using informed decisions. A paper by *Bergstra
    et al*. (http://proceedings.mlr.press/v28/bergstra13.pdf) neatly explains the
    advantages of Bayesian optimization over random search optimization.
  prefs: []
  type: TYPE_NORMAL
- en: There are several techniques within Bayesian optimization that can be applied
    to hyperparameter tuning in the machine learning domain. A popular formalization
    of the Bayesian approach is **Sequential Model-Based Optimization** (**SMBO**),
    which again has several variations within. Each approach within SMBO varies based
    on the way the surrogate model is defined and the criteria used to evaluate and
    update parameters. A few popular choices for the surrogate model are **Gaussian
    Processes**, **Random Forest Regressions**, and **Tree Parzen Estimators** (**TPE**).
    Similarly, the criteria based on each successive iteration is evaluated in the
    optimization process and leveraged using **UCB**, that is, GP **Upper Confidence
    Bound**, for example, **Expected Improvement** (EI) or **Probability of Improvement**
    (**POI**).
  prefs: []
  type: TYPE_NORMAL
- en: To implement Bayesian optimization in R, we already have a handful of well-written
    libraries that abstract the entire process for us. `MlBayesOpt` is a popular package
    implemented by Yuya Matsumura in R. It is based on the **Gaussian Process** in
    SMBO for Bayesian optimization and allows the use of several functions to update
    the surrogate model.
  prefs: []
  type: TYPE_NORMAL
- en: The following exercise performs Bayesian optimization on the random forest model
    for the `mtry` and minimum node size hyperparameters. The output of the optimization
    process returns the best combination evaluated for each hyperparameter. We would
    then need to implement a regular model using the same combination for hyperparameter
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian optimization:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: https://towardsdatascience.com/the-intuitions-behind-bayesian-optimization-with-gaussian-processes-7e00fcc898a0
  prefs: []
  type: TYPE_NORMAL
- en: https://papers.nips.cc/paper/7838-automating-bayesian-optimization-with-bayesian-optimization.pdf
  prefs: []
  type: TYPE_NORMAL
- en: https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 96: Performing Bayesian Optimization on the Random Forest Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Perform Bayesian optimization for the same dataset and study the output. In
    this optimization technique, we will leverage Bayesian optimization to intuitively
    select the best value for the `mtry` hyperparameter by iterating with the knowledge/context
    of previous iterations.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this exercise is to perform Bayesian optimization for the random
    forest model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, set the `seed` as `2019` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `MlBayesOpt` library using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform Bayesian optimization for random forest model using the `rf_opt` function
    from the `MlBayesOpt` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.13: Output for Bayesian optimization for random forest'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.13: Output for Bayesian optimization for random forest'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The output displays the iterations computed for model evaluation and returns
    the result at the end of each iteration. You can increase the number of iterations
    by increasing the value of `n_iter`. The `init_points` variable defines the number
    of randomly chosen points to sample the target function before Bayesian optimization
    fits the Gaussian process. Additionally, we define the criteria function as `eps`
    parameter is an additional parameter that can be used to tune **EI** and **POI**,
    to balance exploitation against exploration, increasing epsilon will make the
    optimized hyperparameters more spread out across the whole range.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we also define the kernel, that is, the correlation function for the
    underlying Gaussian process. This parameter should be a list that specifies the
    type of correlation function along with the smoothness parameter. Popular choices
    are square exponential (the default) or `Matern 5/2`.
  prefs: []
  type: TYPE_NORMAL
- en: The result of the Bayesian optimization process returned the best value for
    `mtry` as 6.12 (this needs to be truncated to 6, as `mtry` in **Random Forest**
    doesn't accept decimal values) and the best minimum node size as 17\. We can use
    these parameter settings in the regular implementation of random forest and evaluate
    the model results.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now use the same approach for the XGBoost model on the same use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 97: Performing Bayesian Optimization using XGBoost'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to the previous exercise, we will perform Bayesian optimization for
    the same dataset and study the output, albeit this time for the XGBoost model.
    Given that XGBoost has a larger set of hyperparameters to optimize, we would need
    to provide a range of values for each hyperparameter of interest to the optimization
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform Bayesian optimization on the XGBoost model, carry out the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, set the `seed` as `2019` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform Bayesian optimization for the XGBoost model using the `xgb_opt` function
    from the `MlBayesOpt` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.14: Output for Bayesian Optimization using XGBoost'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.14: Output for Bayesian Optimization using XGBoost'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Like random forest, the `xgb_opt` function returns the optimal list of hyperparameters
    with the candidate values. Unlike **Random Forest**, we have a larger list of
    hyperparameters for XGBoost. We need to define the range for which we want the
    Bayesian optimization process to operate for each hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the best combination of hyperparameters from Bayesian optimization
    is different from what we found for `nrounds` would need to be truncated (as a
    decimal value won''t make sense) when we implement a standard XGBoost Tree model
    the parameters evaluate above namely: `nrounds`, `max_depth`, `eta`, and `subsample`.'
  prefs: []
  type: TYPE_NORMAL
- en: In the chapter, we studied various cross-validation techniques to perform model
    assessment. A small amendment to the k-fold cross-validation helps us perform
    an improved validation of model performance. Before moving from k-fold to LOOCV,
    we can instead perform repeated k-fold to get a more robust evaluation of the
    model. The process repeats cross-validation multiple times where the folds are
    split in a different way in each repetition. Performing repeated k-fold is a better
    approach than LOOCV.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 12: Performing Repeated K-Fold Cross Validation and Grid Search Optimization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will leverage the same dataset (as used in previous exercises),
    train a random forest model, perform repeated k-fold validation 10 times, and
    study the model performance. Within each fold iteration, we can try the different
    grid values of the hyperparameter and have a robust validation for the best model.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of the activity is to perform repeated k-fold cross-validation and grid
    search optimization on the same model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the required packages (`mlbench`, `caret`, and `dplyr`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `PimaIndianDiabetes` dataset into memory from `mlbench` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a seed value for reproducibility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the k-fold validation object using the `trainControl` function from `caret`
    package and define `method` as `repeatedcv` instead of `cv`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define an additional construct in the `trainControl` function for the number
    of repeats in the validation of `repeats = 10`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the grid for the `mtry` hyperparameter of a random forest model as `(3,4,5)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model with the grid values, cross-validation objects, and random forest
    classifiers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study the model performance by plotting the accuracy across different values
    of the hyperparameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final output should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.15: Model performance accuracy across different values of the hyperparameter'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_07_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.15: Model performance accuracy across different values of the hyperparameter'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 461.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you learned a few important aspects of model performance improvement
    techniques. We started with **Bias-Variance Trade-off** and understood it impacts
    a model's performance. We now know that high bias will result in underfitting,
    whereas high variance will result in overfitting of models, and that achieving
    one comes at the expense of the other. Therefore, in order to build the best models,
    we need to strike the ideal balance between bias and variance in machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we explored various types of cross-validation techniques in R that provide
    ready-to-use functions to implement the same. We studied holdout, k-fold, and
    hold-one-out validation approaches to cross-validation and understood how we can
    perform robust assessment of performance of machine learning models. We then studied
    hyperparameter tuning and explored grid search optimization, random search optimization,
    and Bayesian optimization techniques in detail. Hyperparameter tuning of machine
    learning models helped us to develop more generalized models with better performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the process of deploying a machine learning
    model in the cloud.
  prefs: []
  type: TYPE_NORMAL
