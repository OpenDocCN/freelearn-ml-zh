<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 5. Regression and Classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Regression and Classification</h1></div></div></div><p>In the previous chapter, we got familiar with supervised and unsupervised learning. Another standard taxonomy of the machine learning methods is based on the label is from continuous or discrete space. Even if the discrete labels are ordered, there is a significant difference, particularly how the goodness of fit metrics is evaluated.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Learning about the origin of the word regression</li><li class="listitem" style="list-style-type: disc">Learning metrics for evaluating the goodness of fit in continuous and discrete space</li><li class="listitem" style="list-style-type: disc">Discussing how to write simple code in Scala for linear and logistic regression</li><li class="listitem" style="list-style-type: disc">Learning about advanced concepts such as regularization, multiclass predictions, and heteroscedasticity</li><li class="listitem" style="list-style-type: disc">Discussing an example of MLlib application for regression tree analysis</li><li class="listitem" style="list-style-type: disc">Learning about the different ways of evaluating classification models</li></ul></div><div class="section" title="What regression stands for?"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/>What regression stands for?</h1></div></div></div><p>While the word<a id="id354" class="indexterm"/> classification is intuitively clear, the word regression does not seem to imply a predictor of a continuous label. According to the Webster dictionary, regression is:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"a return to a former or less developed state."</em></span></p></blockquote></div><p>It does also mention a special definition for statistics as <span class="emphasis"><em>a measure of the relation between the mean value of one variable (for example, output) and corresponding values of other variables (for example, time and cost)</em></span>, which is actually correct these days. However, historically, the regression coefficient was meant to signify the hereditability of certain characteristics, such as weight and size, from one generation to another, with the hint of planned gene selection, including humans (<a class="ulink" href="http://www.amstat.org/publications/jse/v9n3/stanton.html">http://www.amstat.org/publications/jse/v9n3/stanton.html</a>). More specifically, in 1875, Galton, a cousin of Charles Darwin and an accomplished 19th-century scientist in his own right, which was also widely criticized for the promotion of eugenics, had distributed packets of sweet pea seeds to seven friends. Each friend received seeds of uniform weight, but with substantial variation across the seven packets. Galton's friends were supposed to harvest the next generation seeds and ship them back to him. Galton then proceeded to analyze the statistical properties of the seeds within each group, and one of the analysis was to plot the regression line, which always appeared to <a id="id355" class="indexterm"/>have the slope less than one—the specific number cited was 0.33 (Galton, F. (1894), Natural Inheritance (5th ed.), New York: Macmillan and Company), as opposed to either <span class="emphasis"><em>0</em></span>, in the case of no correlation and no inheritance; or <span class="emphasis"><em>1</em></span>, in the case the total replication of the parent's characteristics in the descendants. We will discuss why the coefficient of the regression line should always be less than <span class="emphasis"><em>1</em></span> in the presence of noise in the data, even if the correlation is perfect. However, beyond the discussion and details, the origin of the term regression is partly due to planned breeding of plants and humans. Of course, Galton did not have access to PCA, Scala, or any other computing machinery at the time, which might shed more light on the differences between correlation and the slope of the regression line.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Continuous space and metrics"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec34"/>Continuous space and metrics</h1></div></div></div><p>As most of this chapter's content will be dealing with trying to predict or optimize continuous variables, let's first understand how to measure the difference in a continuous space. Unless <a id="id356" class="indexterm"/>a drastically new discovery is made pretty soon, the space we <a id="id357" class="indexterm"/>live in is a three-dimensional Euclidian space. Whether we like it or not, this is the world we are mostly comfortable with today. We can completely specify our location with three continuous numbers. The difference in locations is usually measured by distance, or a metric, which is a function of a two arguments that returns a single positive real number. Naturally, the distance, <span class="inlinemediaobject"><img src="Images/B04935_05_01F.jpg" alt="Continuous space and metrics" width="80" height="33"/></span>, between <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> should always be equal or smaller than the sum of distances between <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Z</em></span> and <span class="emphasis"><em>Y</em></span> and <span class="emphasis"><em>Z</em></span>:</p><div class="mediaobject"><img src="Images/B04935_05_02F.jpg" alt="Continuous space and metrics" width="265" height="33"/></div><p>For any <span class="emphasis"><em>X</em></span>, <span class="emphasis"><em>Y</em></span>, and <span class="emphasis"><em>Z</em></span>, which is also <a id="id358" class="indexterm"/>called triangle inequality. The two other properties of a metric is symmetry:</p><div class="mediaobject"><img src="Images/B04935_05_03F.jpg" alt="Continuous space and metrics" width="173" height="33"/></div><p>Non-negativity of distance:</p><div class="mediaobject"><img src="Images/B04935_05_04F.jpg" alt="Continuous space and metrics" width="183" height="33"/></div><div class="mediaobject"><img src="Images/B04935_05_05F.jpg" alt="Continuous space and metrics" width="182" height="33"/></div><p>Here, the <a id="id359" class="indexterm"/>metric is <code class="literal">0</code> if, and only if, <span class="emphasis"><em>X=Y</em></span>. The <span class="inlinemediaobject"><img src="Images/B04935_05_06F.jpg" alt="Continuous space and metrics" width="23" height="30"/></span> distance is the<a id="id360" class="indexterm"/> distance as we understand it in everyday life, the square root of the sum of the squared differences along each of the dimensions. A generalization of our physical distance is p-norm (<span class="emphasis"><em>p = 2</em></span> for the <span class="inlinemediaobject"><img src="Images/B04935_05_06F.jpg" alt="Continuous space and metrics" width="23" height="30"/></span> distance):</p><div class="mediaobject"><img src="Images/B04935_05_07F.jpg" alt="Continuous space and metrics" width="235" height="65"/></div><p>Here, the sum is the overall components of the <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> vectors. If <span class="emphasis"><em>p=1</em></span>, the 1-norm is the sum of absolute differences, or Manhattan distance, as if the only path from point <span class="emphasis"><em>X</em></span> to point <span class="emphasis"><em>Y</em></span> would be to move only along one of the components. This distance is also often referred to as <span class="inlinemediaobject"><img src="Images/B04935_05_08F.jpg" alt="Continuous space and metrics" width="22" height="30"/></span> distance:</p><div class="mediaobject"><img src="Images/B04935_05_01.jpg" alt="Continuous space and metrics" width="800" height="881"/><div class="caption"><p>Figure 05-1. The <span class="inlinemediaobject"><img src="Images/B04935_05_08F.jpg" alt="Continuous space and metrics" width="22" height="30"/></span> circle in two-dimensional space (the set of points exactly one unit from the origin (0, 0))</p></div></div><p>Here is a<a id="id361" class="indexterm"/> representation <a id="id362" class="indexterm"/>of a circle in a two-dimensional space:</p><div class="mediaobject"><img src="Images/B04935_05_02.jpg" alt="Continuous space and metrics" width="800" height="859"/><div class="caption"><p>Figure 05-2. <span class="inlinemediaobject"><img src="Images/B04935_05_06F.jpg" alt="Continuous space and metrics" width="23" height="30"/></span> circle in two-dimensional space (the set of points equidistant from the origin (0, 0)), which actually looks like a circle in our everyday understanding of distance.</p></div></div><p>Another <a id="id363" class="indexterm"/>frequently <a id="id364" class="indexterm"/>used special case is <span class="inlinemediaobject"><img src="Images/B04935_05_09F.jpg" alt="Continuous space and metrics" width="27" height="30"/></span>, the limit when <span class="inlinemediaobject"><img src="Images/B04935_05_10F.jpg" alt="Continuous space and metrics" width="62" height="22"/></span>, which is the maximum deviation along any of the components, as follows:</p><div class="mediaobject"><img src="Images/B04935_05_11F.jpg" alt="Continuous space and metrics" width="203" height="38"/></div><p>The equidistant circle for the <span class="inlinemediaobject"><img src="Images/B04935_05_09F.jpg" alt="Continuous space and metrics" width="27" height="30"/></span> distance is shown in <span class="emphasis"><em>Figure 05-3</em></span>:</p><div class="mediaobject"><img src="Images/B04935_05_03.jpg" alt="Continuous space and metrics" width="800" height="866"/><div class="caption"><p>Figure 05-3. <span class="inlinemediaobject"><img src="Images/B04935_05_09F.jpg" alt="Continuous space and metrics" width="27" height="30"/></span> circle in two-dimensional space (the set of points equidistant from the origin (0, 0)). This is a square as the <span class="inlinemediaobject"><img src="Images/B04935_05_09F.jpg" alt="Continuous space and metrics" width="27" height="30"/></span> metric is the maximum distance along any of the components.</p></div></div><p>I'll consider the <span class="strong"><strong>Kullback-Leibler</strong></span> (<span class="strong"><strong>KL</strong></span>) distance later when I talk about classification, which measures<a id="id365" class="indexterm"/> the difference between two probability distributions, but it is an example of distance that is not symmetric and thus it is not a metric.</p><p>The metric <a id="id366" class="indexterm"/>properties make it easier to decompose the problem. Due to<a id="id367" class="indexterm"/> the triangle inequality, one can potentially reduce a difficult problem of optimizing a goal by substituting it by a set of problems by optimizing along a number of dimensional components of the problem separately.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Linear regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec35"/>Linear regression</h1></div></div></div><p>As explained in <a class="link" href="ch02.xhtml" title="Chapter 2. Data Pipelines and Modeling">Chapter 2</a>, <span class="emphasis"><em>Data Pipelines and Modeling</em></span>, most complex machine learning problems <a id="id368" class="indexterm"/>can be reduced to optimization as our final goal is to optimize the whole process where the machine is involved as an intermediary or the complete solution. The metric can be explicit, such as error rate, or more indirect, such as <span class="strong"><strong>Monthly Active Users</strong></span> (<span class="strong"><strong>MAU</strong></span>), but the effectiveness of an algorithm is<a id="id369" class="indexterm"/> finally judged by how it improves some metrics and processes in our lives. Sometimes, the goals may consist of multiple subgoals, or other metrics such as maintainability and stability might eventually be considered, but essentially, we need to either maximize or minimize a continuous metric in one or other way.</p><p>For the rigor of the flow, let's show how the linear regression can be formulated as an optimization problem. The classical linear regression needs to optimize the cumulative <span class="inlinemediaobject"><img src="Images/B04935_05_06F.jpg" alt="Linear regression" width="23" height="30"/></span> error rate:</p><div class="mediaobject"><img src="Images/B04935_05_16F.jpg" alt="Linear regression" width="240" height="57"/></div><p>Here, <span class="inlinemediaobject"><img src="Images/B04935_05_17F.jpg" alt="Linear regression" width="18" height="27"/></span> is the estimate given by a model, which, in the case of linear regression, is as follows:</p><div class="mediaobject"><img src="Images/B04935_05_18F.jpg" alt="Linear regression" width="93" height="30"/></div><p>(Other potential <span class="strong"><strong>loss functions</strong></span><a id="id370" class="indexterm"/> have been enumerated in <a class="link" href="ch03.xhtml" title="Chapter 3. Working with Spark and MLlib">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span>). As the <span class="inlinemediaobject"><img src="Images/B04935_05_06F.jpg" alt="Linear regression" width="23" height="30"/></span> metric is a differentiable convex function of <span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>b</em></span>, the extreme value can be found by equating the derivative of the cumulative error rate to <code class="literal">0</code>:</p><div class="mediaobject"><img src="Images/B04935_05_19F.jpg" alt="Linear regression" width="122" height="55"/></div><p>Computing the <a id="id371" class="indexterm"/>derivatives is straightforward in this case and leads to the following equation:</p><div class="mediaobject"><img src="Images/B04935_05_21F.jpg" alt="Linear regression" width="397" height="65"/></div><div class="mediaobject"><img src="Images/B04935_05_22F.jpg" alt="Linear regression" width="353" height="65"/></div><p>This can be solved to give:</p><div class="mediaobject"><img src="Images/B04935_05_23F.jpg" alt="Linear regression" width="263" height="67"/></div><div class="mediaobject"><img src="Images/B04935_05_24F.jpg" alt="Linear regression" width="193" height="33"/></div><p>Here, <span class="emphasis"><em>avg()</em></span> denotes the average overall input records. Note that if <span class="emphasis"><em>avg(x)=0</em></span> the preceding equation is reduced to the following:</p><div class="mediaobject"><img src="Images/B04935_05_25F.jpg" alt="Linear regression" width="105" height="65"/></div><div class="mediaobject"><img src="Images/B04935_05_26F.jpg" alt="Linear regression" width="95" height="33"/></div><p>So, we can quickly compute the linear regression coefficients using basic Scala operators (we can always make <span class="emphasis"><em>avg(x)</em></span> to be zero by performing a <span class="inlinemediaobject"><img src="Images/B04935_05_27F.jpg" alt="Linear regression" width="137" height="33"/></span>):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ scala</strong></span>

<span class="strong"><strong>Welcome to Scala version 2.11.6 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>

<span class="strong"><strong>scala&gt; import scala.util.Random</strong></span>
<span class="strong"><strong>import scala.util.Random</strong></span>

<span class="strong"><strong>scala&gt; val x = -5 to 5</strong></span>
<span class="strong"><strong>x: scala.collection.immutable.Range.Inclusive = Range(-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)</strong></span>

<span class="strong"><strong>scala&gt; val y = x.map(_ * 2 + 4 + Random.nextGaussian)</strong></span>
<span class="strong"><strong>y: scala.collection.immutable.IndexedSeq[Double] = Vector(-4.317116812989753, -4.4056031270948015, -2.0376543660274713, 0.0184679796245639, 1.8356532746253016, 3.2322795591658644, 6.821999810895798, 7.7977904139852035, 10.288549406814154, 12.424126535332453, 13.611442206874917)</strong></span>

<span class="strong"><strong>scala&gt; val a = (x, y).zipped.map(_ * _).sum / x.map(x =&gt; x * x).sum</strong></span>
<span class="strong"><strong>a: Double = 1.9498665133868092</strong></span>

<span class="strong"><strong>scala&gt; val b = y.sum / y.size</strong></span>
<span class="strong"><strong>b: Double = 4.115448625564203</strong></span>
</pre></div><p>Didn't I inform <a id="id372" class="indexterm"/>you previously that Scala is a very concise language? We just did linear regression with five lines of code, three of which were just data-generation statements.</p><p>Although there are libraries written in Scala for performing (multivariate) linear regression, such <a id="id373" class="indexterm"/>as Breeze (<a class="ulink" href="https://github.com/scalanlp/breeze">https://github.com/scalanlp/breeze</a>), which provides a more extensive functionality, it is nice to be able to use pure Scala functionality to get some simple statistical results.</p><p>Let's look at the problem of Mr. Galton, where he found that the regression line always has the slope of less than one, which implies that we should always regress to some predefined mean. I will generate the same points as earlier, but they will be distributed along the horizontal line with some predefined noise. Then, I will rotate the line by <span class="emphasis"><em>45</em></span> degrees by doing a linear rotation transformation in the <span class="emphasis"><em>xy</em></span>-space. Intuitively, it should be clear that if <a id="id374" class="indexterm"/>anything, <span class="emphasis"><em>y</em></span> is strongly correlated with x and absent, the <span class="emphasis"><em>y</em></span> noise should be nothing else but <span class="emphasis"><em>x</em></span>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[akozlov@Alexanders-MacBook-Pro]$ scala</strong></span>
<span class="strong"><strong>Welcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>

<span class="strong"><strong>scala&gt; import scala.util.Random.nextGaussian</strong></span>
<span class="strong"><strong>import scala.util.Random.nextGaussian</strong></span>

<span class="strong"><strong>scala&gt; val x0 = Vector.fill(201)(100 * nextGaussian)</strong></span>
<span class="strong"><strong>x0: scala.collection.immutable.IndexedSeq[Double] = Vector(168.28831870102465, -40.56031270948016, -3.7654366027471324, 1.84679796245639, -16.43467253746984, -76.77204408341358, 82.19998108957988, -20.22095860147962, 28.854940681415442, 42.41265353324536, -38.85577931250823, -17.320873680820082, 64.19368427702135, -8.173507833084892, -198.6064655461397, 40.73700995880357, 32.36849515282444, 0.07758364225363915, -101.74032407199553, 34.789280276495646, 46.29624756866302, 35.54024768650289, 24.7867839701828, -11.931948933554782, 72.12437623460166, 30.51440227306552, -80.20756177356768, 134.2380548346385, 96.14401034937691, -205.48142161773896, -73.48186022765427, 2.7861465340245215, 39.49041527572774, 12.262899592863906, -118.30408039749234, -62.727048950163855, -40.58557796128219, -23.42...</strong></span>
<span class="strong"><strong>scala&gt; val y0 = Vector.fill(201)(30 * nextGaussian)</strong></span>
<span class="strong"><strong>y0: scala.collection.immutable.IndexedSeq[Double] = Vector(-51.675658534203876, 20.230770706186128, 32.47396891906855, -29.35028743620815, 26.7392929946199, 49.85681312583139, 24.226102932450917, 31.19021547086266, 26.169544117916704, -4.51435617676279, 5.6334117227063985, -59.641661744341775, -48.83082934374863, 29.655750956280304, 26.000847703123497, -17.43319605936741, 0.8354318740518344, 11.44787080976254, -26.26312164695179, 88.63863939038357, 45.795968719043785, 88.12442528090506, -29.829048945601635, -1.0417034396751037, -27.119245702417494, -14.055969115249258, 6.120344305721601, 6.102779172838027, -6.342516875566529, 0.06774080659895702, 46.364626315486014, -38.473161588561, -43.25262339890197, 19.77322736359687, -33.78364440355726, -29.085765762613683, 22.87698648100551, 30.53...</strong></span>
<span class="strong"><strong>scala&gt; val x1 = (x0, y0).zipped.map((a,b) =&gt; 0.5 * (a + b) )</strong></span>
<span class="strong"><strong>x1: scala.collection.immutable.IndexedSeq[Double] = Vector(58.30633008341039, -10.164771001647015, 14.354266158160707, -13.75174473687588, 5.152310228575029, -13.457615478791094, 53.213042011015396, 5.484628434691521, 27.51224239966607, 18.949148678241286, -16.611183794900917, -38.48126771258093, 7.681427466636357, 10.741121561597705, -86.3028089215081, 11.651906949718079, 16.601963513438136, 5.7627272260080895, -64.00172285947366, 61.71395983343961, 46.0461081438534, 61.83233648370397, -2.5211324877094174, -6.486826186614943, 22.50256526609208, 8.229216578908131, -37.04360873392304, 70.17041700373827, 44.90074673690519, -102.70684040557, -13.558616956084126, -17.843507527268237, -1.8811040615871129, 16.01806347823039, -76.0438624005248, -45.90640735638877, -8.85429574013834, 3.55536787...</strong></span>
<span class="strong"><strong>scala&gt; val y1 = (x0, y0).zipped.map((a,b) =&gt; 0.5 * (a - b) )</strong></span>
<span class="strong"><strong>y1: scala.collection.immutable.IndexedSeq[Double] = Vector(109.98198861761426, -30.395541707833143, -18.11970276090784, 15.598542699332269, -21.58698276604487, -63.31442860462248, 28.986939078564482, -25.70558703617114, 1.3426982817493691, 23.463504855004075, -22.244595517607316, 21.160394031760845, 56.51225681038499, -18.9146293946826, -112.3036566246316, 29.08510300908549, 15.7665316393863, -5.68514358375445, -37.73860121252187, -26.924679556943964, 0.2501394248096176, -26.292088797201085, 27.30791645789222, -5.445122746939839, 49.62181096850958, 22.28518569415739, -43.16395303964464, 64.06763783090022, 51.24326361247172, -102.77458121216895, -59.92324327157014, 20.62965406129276, 41.37151933731485, -3.755163885366482, -42.26021799696754, -16.820641593775086, -31.73128222114385, -26.9...</strong></span>
<span class="strong"><strong>scala&gt; val a = (x1, y1).zipped.map(_ * _).sum / x1.map(x =&gt; x * x).sum</strong></span>
<span class="strong"><strong>a: Double = 0.8119662470457414</strong></span>
</pre></div><p>The slope is<a id="id375" class="indexterm"/> only <code class="literal">0.81</code>! Note that if one runs PCA on the <code class="literal">x1</code> and <code class="literal">y1</code> data, the first principal component is correctly along the diagonal.</p><p>For completeness, I am giving a plot of (<span class="emphasis"><em>x1, y1</em></span>) zipped here:</p><div class="mediaobject"><img src="Images/B04935_05_04.jpg" alt="Linear regression" width="800" height="942"/><div class="caption"><p>Figure 05-4. The regression curve slope of a seemingly perfectly correlated dataset is less than one. This has to do with the metric the regression problem optimizes (y-distance).</p></div></div><p>I will leave<a id="id376" class="indexterm"/> it to the reader to find the reason why the slope is less than one, but it has to do with the specific question the regression problem is supposed to answer and the metric it optimizes.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Logistic regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec36"/>Logistic regression</h1></div></div></div><p>Logistic regression optimizes<a id="id377" class="indexterm"/> the logit loss function with respect to <span class="emphasis"><em>w</em></span>:</p><div class="mediaobject"><img src="Images/B04935_05_28F.jpg" alt="Logistic regression" width="165" height="40"/></div><p>Here, <span class="emphasis"><em>y</em></span> is binary (in this case plus or minus one). While there is no closed-form solution for the error minimization problem like there was in the previous case of linear regression, logistic function is differentiable and allows iterative algorithms that converge very fast.</p><p>The gradient is as follows:</p><div class="mediaobject"><img src="Images/B04935_05_29F.jpg" alt="Logistic regression" width="330" height="75"/></div><p>Again, we can quickly concoct a Scala program that uses the gradient to converge to the value, where <span class="inlinemediaobject"><img src="Images/B04935_05_30F.jpg" alt="Logistic regression" width="233" height="40"/></span> (we use the MLlib <code class="literal">LabeledPoint</code> data structure only for convenience of reading the data):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1-SNAPSHOT</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vector</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vector</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.util._</strong></span>
<span class="strong"><strong>import org.apache.spark.util._</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util._</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util._</strong></span>

<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "data/iris/iris-libsvm.txt")</strong></span>
<span class="strong"><strong>data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[291] at map at MLUtils.scala:112</strong></span>

<span class="strong"><strong>scala&gt; var w = Vector.random(4)</strong></span>
<span class="strong"><strong>w: org.apache.spark.util.Vector = (0.9515155226069267, 0.4901713461728122, 0.4308861351586426, 0.8030814804136821)</strong></span>

<span class="strong"><strong>scala&gt; for (i &lt;- 1.to(10)) println { val gradient = data.map(p =&gt; ( - p.label / (1+scala.math.exp(p.label*(Vector(p.features.toDense.values) dot w))) * Vector(p.features.toDense.values) )).reduce(_+_); w -= 0.1 * gradient; w }</strong></span>
<span class="strong"><strong>(-24.056553839570114, -16.585585503253142, -6.881629923278653, -0.4154730884796032)</strong></span>
<span class="strong"><strong>(38.56344616042987, 12.134414496746864, 42.178370076721365, 16.344526911520397)</strong></span>
<span class="strong"><strong>(13.533446160429868, -4.95558550325314, 34.858370076721364, 15.124526911520398)</strong></span>
<span class="strong"><strong>(-11.496553839570133, -22.045585503253143, 27.538370076721364, 13.9045269115204)</strong></span>
<span class="strong"><strong>(-4.002010810020908, -18.501520148476196, 32.506256310962314, 15.455945245916512)</strong></span>
<span class="strong"><strong>(-4.002011353029471, -18.501520429824225, 32.50625615219947, 15.455945209971787)</strong></span>
<span class="strong"><strong>(-4.002011896036225, -18.501520711171313, 32.50625599343715, 15.455945174027184)</strong></span>
<span class="strong"><strong>(-4.002012439041171, -18.501520992517463, 32.506255834675365, 15.455945138082699)</strong></span>
<span class="strong"><strong>(-4.002012982044308, -18.50152127386267, 32.50625567591411, 15.455945102138333)</strong></span>
<span class="strong"><strong>(-4.002013525045636, -18.501521555206942, 32.506255517153384, 15.455945066194088)</strong></span>

<span class="strong"><strong>scala&gt; w *= 0.24 / 4</strong></span>
<span class="strong"><strong>w: org.apache.spark.util.Vector = (-0.24012081150273815, -1.1100912933124165, 1.950375331029203, 0.9273567039716453)</strong></span>
</pre></div><p>The logistic regression was reduced to only one line of Scala code! The last line was to normalize the weights—only the relative values are important to define the separating plane—to <a id="id378" class="indexterm"/>compare them to the one obtained with the MLlib in previous chapter.</p><p>The <span class="strong"><strong>Stochastic Gradient Descent</strong></span> (<span class="strong"><strong>SGD</strong></span>) algorithm used in the actual implementation is essentially<a id="id379" class="indexterm"/> the same gradient descent, but optimized in the following ways:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The actual gradient is computed on a subsample of records, which may lead to faster conversion due to less rounding noise and avoid local minima.</li><li class="listitem" style="list-style-type: disc">
The step—a fixed <span class="emphasis"><em>0.1</em></span> in our case—is a monotonically decreasing function of the iteration as <span class="inlinemediaobject"><img src="Images/B04935_05_31F.jpg" alt="Logistic regression" width="45" height="63"/></span>, which might also lead to better conversion.
</li><li class="listitem" style="list-style-type: disc">It incorporates regularization; instead of minimizing just the loss function, you minimize the sum of the loss function, plus some penalty metric, which is a function of model complexity. I will discuss this in the following section.</li></ul></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Regularization"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec37"/>Regularization</h1></div></div></div><p>The regularization<a id="id380" class="indexterm"/> was originally developed to cope with ill-poised problems, where the problem was underconstrained—allowed multiple solutions given the data—or the data and the solution that contained too much noise (<span class="emphasis"><em>A.N. Tikhonov</em></span>, <span class="emphasis"><em>A.S. Leonov</em></span>, <span class="emphasis"><em>A.G. Yagola. Nonlinear Ill-Posed Problems</em></span>, <span class="emphasis"><em>Chapman and Hall</em></span>, <span class="emphasis"><em>London</em></span>, <span class="emphasis"><em>Weinhe</em></span>). Adding additional penalty function that skews a solution if it does not have a desired property, such as the smoothness in curve fitting or spectral analysis, usually solves the problem.</p><p>The choice of the penalty function is somewhat arbitrary, but it should reflect a desired skew in the solution. If the penalty function is differentiable, it can be incorporated into the gradient descent process; ridge regression is an example where the penalty is the <span class="inlinemediaobject"><img src="Images/B04935_05_06F.jpg" alt="Regularization" width="23" height="30"/></span>metric for the weights or the sum of squares of the coefficients.</p><p>MLlib currently implements <span class="inlinemediaobject"><img src="Images/B04935_05_06F.jpg" alt="Regularization" width="23" height="30"/></span>, <span class="inlinemediaobject"><img src="Images/B04935_05_08F.jpg" alt="Regularization" width="22" height="30"/></span>, and a <a id="id381" class="indexterm"/>mixture thereof called <span class="strong"><strong>Elastic Net</strong></span>, as was shown in <a class="link" href="ch03.xhtml" title="Chapter 3. Working with Spark and MLlib">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span>. The <span class="inlinemediaobject"><img src="Images/B04935_05_08F.jpg" alt="Regularization" width="22" height="30"/></span> regularization effectively penalizes for the number of non-zero entries in the regression weights, but <a id="id382" class="indexterm"/>has been known to have slower convergence. <span class="strong"><strong>Least Absolute Shrinkage and Selection Operator</strong></span> (<span class="strong"><strong>LASSO</strong></span>) uses the <span class="inlinemediaobject"><img src="Images/B04935_05_08F.jpg" alt="Regularization" width="22" height="30"/></span> regularization.</p><p>Another way<a id="id383" class="indexterm"/> to reduce the uncertainty in underconstrained problems is to take the prior information that may be coming from domain experts into account. This can be done using Bayesian analysis and introducing additional factors into the posterior probability—the probabilistic rules are generally expressed as multiplication rather than sum. However, since the goal is often minimizing the log likelihood, the Bayesian correction can often be expressed as standard regularizer as well.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Multivariate regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Multivariate regression</h1></div></div></div><p>It is possible to <a id="id384" class="indexterm"/>minimize multiple metrics at the same time. While Spark only has a few multivariate analysis tools, other more traditional well-established<a id="id385" class="indexterm"/> packages come <a id="id386" class="indexterm"/>with <span class="strong"><strong>Multivariate Analysis of Variance</strong></span> (<span class="strong"><strong>MANOVA</strong></span>), a generalization of <span class="strong"><strong>Analysis of Variance</strong></span> (<span class="strong"><strong>ANOVA</strong></span>) method. I will cover ANOVA and MANOVA in <a class="link" href="ch07.xhtml" title="Chapter 7. Working with Graph Algorithms">Chapter 7</a>, <span class="emphasis"><em>Working with Graph Algorithms</em></span>.</p><p>For a practical analysis, we first need to understand if the target variables are correlated, for which we can use the PCA Spark implementation covered in <a class="link" href="ch03.xhtml" title="Chapter 3. Working with Spark and MLlib">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span>. If the dependent variables are strongly correlated, maximizing one leads to maximizing the other, and we can just maximize the first principal component (and potentially build a regression model on the second component to understand what drives the difference).</p><p>If the targets are uncorrelated, building a separate model for each of them can pinpoint the important variables that drive either and whether these two sets are disjoint. In the latter case, we could build two separate models to predict each of the targets independently.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Heteroscedasticity"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Heteroscedasticity</h1></div></div></div><p>One of the<a id="id387" class="indexterm"/> fundamental assumptions in regression approach is that the target variance is not correlated with either independent (attributes) or dependent (target) variables. An example where this assumption might break is counting data, which is generally described by <span class="strong"><strong>Poisson distribution</strong></span>. For Poisson distribution, the <a id="id388" class="indexterm"/>variance is proportional to the expected value, and the higher values can contribute more to the final variance of the weights.</p><p>While heteroscedasticity may or may not significantly skew the resulting weights, one practical way to compensate for heteroscedasticity is to perform a log transformation, which will <a id="id389" class="indexterm"/>compensate for it in the case of Poisson distribution:</p><div class="mediaobject"><img src="Images/B04935_05_32F.jpg" alt="Heteroscedasticity" width="97" height="33"/></div><div class="mediaobject"><img src="Images/B04935_05_33F.jpg" alt="Heteroscedasticity" width="152" height="58"/></div><p>Some <a id="id390" class="indexterm"/>other (parametrized) transformations are the <span class="strong"><strong>Box-Cox transformation</strong></span>:</p><div class="mediaobject"><img src="Images/B04935_05_34F.jpg" alt="Heteroscedasticity" width="92" height="55"/></div><p>Here, <span class="inlinemediaobject"><img src="Images/B04935_05_35F.jpg" alt="Heteroscedasticity" width="18" height="23"/></span> is a parameter (the log transformation is a partial case, where <span class="inlinemediaobject"><img src="Images/B04935_05_36F.jpg" alt="Heteroscedasticity" width="48" height="23"/></span>) and Tuckey's lambda transformation (for attributes between <span class="emphasis"><em>0</em></span> and <span class="emphasis"><em>1</em></span>):</p><div class="mediaobject"><img src="Images/B04935_05_38F.jpg" alt="Heteroscedasticity" width="223" height="43"/></div><p>These compensate for Poisson binomial distributed attributes or the estimates of the probability of success in a sequence of trails with potentially a mix of <span class="emphasis"><em>n</em></span> Bernoulli distributions.</p><p>Heteroscedasticity is one of the main reasons that logistic function minimization works better than linear regression with <span class="inlinemediaobject"><img src="Images/B04935_05_06F.jpg" alt="Heteroscedasticity" width="23" height="30"/></span> minimization in a binary prediction problem. Let's consider discrete labels in more details.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Regression trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Regression trees</h1></div></div></div><p>We have seen<a id="id391" class="indexterm"/> classification trees in the previous chapter. One can build a recursive split-and-concur structure for a regression problem, where a split is chosen to minimize the remaining variance. Regression trees are less popular than decision trees or classical ANOVA analysis; however, let's provide an example of a regression tree here as a part of MLlib:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1-SNAPSHOT</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.DecisionTree</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.DecisionTree</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.model.DecisionTreeModel</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.tree.model.DecisionTreeModel</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>

<span class="strong"><strong>scala&gt; // Load and parse the data file.</strong></span>

<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")</strong></span>
<span class="strong"><strong>data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112</strong></span>

<span class="strong"><strong>scala&gt; // Split the data into training and test sets (30% held out for testing)</strong></span>

<span class="strong"><strong>scala&gt; val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))</strong></span>
<span class="strong"><strong>trainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:26</strong></span>
<span class="strong"><strong>testData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:26</strong></span>

<span class="strong"><strong>scala&gt; val categoricalFeaturesInfo = Map[Int, Int]()</strong></span>
<span class="strong"><strong>categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()</strong></span>

<span class="strong"><strong>scala&gt; val impurity = "variance"</strong></span>
<span class="strong"><strong>impurity: String = variance</strong></span>

<span class="strong"><strong>scala&gt; val maxDepth = 5</strong></span>
<span class="strong"><strong>maxDepth: Int = 5</strong></span>

<span class="strong"><strong>scala&gt; val maxBins = 32</strong></span>
<span class="strong"><strong>maxBins: Int = 32</strong></span>

<span class="strong"><strong>scala&gt; val model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo, impurity, maxDepth, maxBins)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel regressor of depth 2 with 5 nodes</strong></span>

<span class="strong"><strong>scala&gt; val labelsAndPredictions = testData.map { point =&gt;</strong></span>
<span class="strong"><strong>     |   val prediction = model.predict(point.features)</strong></span>
<span class="strong"><strong>     |   (point.label, prediction)</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>labelsAndPredictions: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[20] at map at &lt;console&gt;:36</strong></span>

<span class="strong"><strong>scala&gt; val testMSE = labelsAndPredictions.map{ case(v, p) =&gt; math.pow((v - p), 2)}.mean()</strong></span>
<span class="strong"><strong>testMSE: Double = 0.07407407407407407</strong></span>

<span class="strong"><strong>scala&gt; println(s"Test Mean Squared Error = $testMSE")</strong></span>
<span class="strong"><strong>Test Mean Squared Error = 0.07407407407407407</strong></span>

<span class="strong"><strong>scala&gt; println("Learned regression tree model:\n" + model.toDebugString)</strong></span>
<span class="strong"><strong>Learned regression tree model:</strong></span>
<span class="strong"><strong>DecisionTreeModel regressor of depth 2 with 5 nodes</strong></span>
<span class="strong"><strong>  If (feature 378 &lt;= 71.0)</strong></span>
<span class="strong"><strong>   If (feature 100 &lt;= 165.0)</strong></span>
<span class="strong"><strong>    Predict: 0.0</strong></span>
<span class="strong"><strong>   Else (feature 100 &gt; 165.0)</strong></span>
<span class="strong"><strong>    Predict: 1.0</strong></span>
<span class="strong"><strong>  Else (feature 378 &gt; 71.0)</strong></span>
<span class="strong"><strong>   Predict: 1.0</strong></span>
</pre></div><p>The splits at<a id="id392" class="indexterm"/> each level are made to minimize the variance, as follows:</p><div class="mediaobject"><img src="Images/B04935_05_39F.jpg" alt="Regression trees" width="333" height="67"/></div><p>which is equivalent to minimizing the <span class="inlinemediaobject"><img src="Images/B04935_05_06F.jpg" alt="Regression trees" width="23" height="30"/></span> distances between the label values and their mean within each leaf summed over all the leaves of the node.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Classification metrics"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec41"/>Classification metrics</h1></div></div></div><p>If the label is<a id="id393" class="indexterm"/> discrete, the prediction problem is called classification. In general, the target can take only one of the values for each record (even though multivalued targets are possible, particularly for text classification problems to be considered in <a class="link" href="ch06.xhtml" title="Chapter 6. Working with Unstructured Data">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span>).</p><p>If the discrete values are ordered and the ordering makes sense, such as <span class="emphasis"><em>Bad</em></span>, <span class="emphasis"><em>Worse</em></span>, <span class="emphasis"><em>Good</em></span>, the discrete labels can be cast into integer or double, and the problem is reduced to regression (we believe if you are between <span class="emphasis"><em>Bad</em></span> and <span class="emphasis"><em>Worse</em></span>, you are definitely farther away from being <span class="emphasis"><em>Good</em></span> than <span class="emphasis"><em>Worse</em></span>).</p><p>A generic metric to optimize is the misclassification rate is as follows:</p><div class="mediaobject"><img src="Images/B04935_05_40F.jpg" alt="Classification metrics" width="272" height="57"/></div><p>However, if the algorithm can predict the distribution of possible values for the target, a more general metric such as the KL divergence or Manhattan can be used.</p><p>KL divergence is a measure of information loss when probability distribution <span class="inlinemediaobject"><img src="Images/B04935_05_44F.jpg" alt="Classification metrics" width="20" height="30"/></span> is used to approximate probability distribution <span class="inlinemediaobject"><img src="Images/B04935_05_41F.jpg" alt="Classification metrics" width="22" height="30"/></span>:</p><div class="mediaobject"><img src="Images/B04935_05_42F.jpg" alt="Classification metrics" width="308" height="60"/></div><p>It is closely <a id="id394" class="indexterm"/>related to entropy gain split criteria used in the decision tree induction, as the latter is the sum of KL divergences of the node probability distribution to the leaf probability distribution over all leaf nodes.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Multiclass problems"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec42"/>Multiclass problems</h1></div></div></div><p>If the number<a id="id395" class="indexterm"/> of possible outcomes for target is larger than two, in general, we have to predict either the expected probability distribution of the target values or at least the list of ordered values—hopefully augmented by a rank variable, which can be used for additional analysis.</p><p>While some algorithms, such as decision trees, can natively predict multivalued attributes. A common technique is to reduce the prediction of one of the <span class="emphasis"><em>K</em></span> target values to <span class="emphasis"><em>(K-1)</em></span> binary classification problems by choosing one of the values as the base and building <span class="emphasis"><em>(K-1)</em></span> binary classifiers. It is usually a good idea to select the most populated level as the base.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Perceptron"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec43"/>Perceptron</h1></div></div></div><p>In the early days <a id="id396" class="indexterm"/>of machine learning, researchers were trying to imitate the functionality of the human brain. At the beginning of the 20th century, people thought that the human brain consisted entirely of cells that are called neurons—cells with long appendages called axons that were able to transmit signals by means of electric impulses. The AI researchers were trying to replicate the functionality of neurons by a perceptron, which is a function that is firing, based on a linearly-weighted sum of its input values:</p><div class="mediaobject"><img src="Images/B04935_05_43F.jpg" alt="Perceptron" width="138" height="63"/></div><p>This is a very simplistic representation of the processes in the human brain—biologists have since then discovered other ways in which information is transferred besides electric impulses such as chemical ones. Moreover, they have found over 300 different types of cells that may be classified as neurons (<a class="ulink" href="http://neurolex.org/wiki/Category:Neuron">http://neurolex.org/wiki/Category:Neuron</a>). Also, the process of neuron firing is more complex than just linear transmission of voltages as it involves complex time patterns as well. Nevertheless, the concept turned out to be very productive, and multiple algorithms and techniques were developed for neural nets, or the sets of perceptions connected to each other in layers. Specifically, it can be shown that the neural network, with certain <a id="id397" class="indexterm"/>modification, where the step function is replaced by a logistic function in the firing equation, can approximate an arbitrary differentiable function with any desired precision.</p><p>MLlib <a id="id398" class="indexterm"/>implements <span class="strong"><strong>Multilayer Perceptron Classifier</strong></span> (<span class="strong"><strong>MLCP</strong></span>) as an <code class="literal">org.apache.spark.ml.classification.MultilayerPerceptronClassifier </code>class:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1-SNAPSHOT</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.ml.classification.MultilayerPerceptronClassifier</strong></span>
<span class="strong"><strong>import org.apache.spark.ml.classification.MultilayerPerceptronClassifier</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator</strong></span>
<span class="strong"><strong>import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.util.MLUtils</strong></span>

<span class="strong"><strong>scala&gt; </strong></span>

<span class="strong"><strong>scala&gt; val data = MLUtils.loadLibSVMFile(sc, "iris-libsvm-3.txt").toDF()</strong></span>
<span class="strong"><strong>data: org.apache.spark.sql.DataFrame = [label: double, features: vector]        </strong></span>

<span class="strong"><strong>scala&gt; </strong></span>

<span class="strong"><strong>scala&gt; val Array(train, test) = data.randomSplit(Array(0.6, 0.4), seed = 13L)</strong></span>
<span class="strong"><strong>train: org.apache.spark.sql.DataFrame = [label: double, features: vector]</strong></span>
<span class="strong"><strong>test: org.apache.spark.sql.DataFrame = [label: double, features: vector]</strong></span>

<span class="strong"><strong>scala&gt; // specify layers for the neural network: </strong></span>

<span class="strong"><strong>scala&gt; // input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)</strong></span>

<span class="strong"><strong>scala&gt; val layers = Array(4, 5, 4, 3)</strong></span>
<span class="strong"><strong>layers: Array[Int] = Array(4, 5, 4, 3)</strong></span>

<span class="strong"><strong>scala&gt; // create the trainer and set its parameters</strong></span>

<span class="strong"><strong>scala&gt; val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(13L).setMaxIter(100)</strong></span>
<span class="strong"><strong>trainer: org.apache.spark.ml.classification.MultilayerPerceptronClassifier = mlpc_b5f2c25196f9</strong></span>

<span class="strong"><strong>scala&gt; // train the model</strong></span>

<span class="strong"><strong>scala&gt; val model = trainer.fit(train)</strong></span>
<span class="strong"><strong>model: org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel = mlpc_b5f2c25196f9</strong></span>

<span class="strong"><strong>scala&gt; // compute precision on the test set</strong></span>

<span class="strong"><strong>scala&gt; val result = model.transform(test)</strong></span>
<span class="strong"><strong>result: org.apache.spark.sql.DataFrame = [label: double, features: vector, prediction: double]</strong></span>

<span class="strong"><strong>scala&gt; val predictionAndLabels = result.select("prediction", "label")</strong></span>
<span class="strong"><strong>predictionAndLabels: org.apache.spark.sql.DataFrame = [prediction: double, label: double]</strong></span>

<span class="strong"><strong>scala&gt; val evaluator = new MulticlassClassificationEvaluator().setMetricName("precision")</strong></span>
<span class="strong"><strong>evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_55757d35e3b0</strong></span>

<span class="strong"><strong>scala&gt; println("Precision = " + evaluator.evaluate(predictionAndLabels))</strong></span>
<span class="strong"><strong>Precision = 0.9375</strong></span>
</pre></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Generalization error and overfitting"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec44"/>Generalization error and overfitting</h1></div></div></div><p>So, how do <a id="id399" class="indexterm"/>we know that the model we have discussed is <a id="id400" class="indexterm"/>good? One obvious and ultimate criterion is its performance in practice.</p><p>One common problem that plagues the more complex models, such as decision trees and neural nets, is overfitting. The model can minimize the desired metric on the provided data, but does a very poor job on a slightly different dataset in practical deployments, Even a standard technique, when we split the dataset into training and test, the training for deriving the model and test for validating that the model works well on a hold-out data, may not capture all the changes that are in the deployments. For example, linear models such as ANOVA, logistic, and linear regression are usually relatively stable and less of a subject to overfitting. However, you might find that any particular technique either works or doesn't work for your specific domain.</p><p>Another case when generalization may fail is time-drift. The data may change over time significantly so that the model trained on the old data no longer generalizes on the new data in a deployment. In practice, it is always a good idea to have several models in production and constantly monitor their relative performance.</p><p>I will consider standard ways to avoid overfitting such as hold out datasets and cross-validation in <a class="link" href="ch07.xhtml" title="Chapter 7. Working with Graph Algorithms">Chapter 7</a>, <span class="emphasis"><em>Working with Graph Algorithms</em></span> and model monitoring in <a class="link" href="ch09.xhtml" title="Chapter 9. NLP in Scala">Chapter 9</a>, <span class="emphasis"><em>NLP in Scala</em></span>.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec45"/>Summary</h1></div></div></div><p>We now have all the necessary tools to look at more complex problems that are more commonly called the big data problems. Armed with standard statistical algorithms—I understand that I have not covered many details and I am completely ready to accept the criticism—there is an entirely new ground to explore where we do not have clearly defined records, the variables in the datasets may be sparse and nested, and we have to cover a lot of ground and do a lot of preparatory work just to get to the stage where we can apply the standard statistical models. This is where Scala shines best.</p><p>In the next chapter, we will look more at working with unstructured data.</p></div></div>



  </body></html>