- en: 'Chapter 7: Feast Alternatives and ML Best Practices'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章：Feast 替代方案和机器学习最佳实践
- en: In the last chapter, we discussed how to use Amazon Managed Workflows with Apache
    Airflow for orchestration and productionizing online and batch models with **Feast**.
    So far in this book, we have been discussing one feature store – Feast. However,
    there are a bunch of feature stores available on the market today. In this chapter,
    we will look at a few of them and discuss how they are different from Feast and
    the advantages or disadvantages of using them over Feast.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了如何使用 Amazon Managed Workflows 和 Apache Airflow 进行编排，以及如何使用 **Feast**
    将在线和批量模型投入生产。到目前为止，在这本书中，我们一直在讨论一个特征存储库——Feast。然而，目前市场上有很多特征存储库。在本章中，我们将查看其中的一些，并讨论它们与
    Feast 的不同之处，以及使用它们相对于 Feast 的优缺点。
- en: In this chapter, we will try out one other feature store, specifically Amazon
    SageMaker. We will take the same feature set that we generated while building
    the customer **lifetime value (LTV)** model and ingest it into SageMaker Feature
    Store and also run a couple of queries. The reason for choosing AWS over other
    feature stores such as Tecton, Hopworks, and H2O.ai is the easy access to the
    trial version. However, choosing the right feature store for you depends on the
    tools and infrastructure that you already have and more, which we will discuss
    in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将尝试使用另一个特征存储库，具体是 Amazon SageMaker。我们将使用在构建客户**终身价值（LTV）**模型时生成的相同特征集，并将其导入
    SageMaker 特征存储库，并运行几个查询。选择 AWS 而不是 Tecton、Hopworks 和 H2O.ai 等其他特征存储库的原因是易于访问试用版。然而，选择适合您的特征存储库取决于您已经拥有的工具和基础设施，以及更多内容，我们将在本章中讨论。
- en: The aim of this chapter is to give you the gist of what is available on the
    market and how it differs from self-managed feature stores such as Feast. We will
    also discuss the similarities and differences between these feature stores. The
    other aspect that I want to discuss in this chapter is the best practices in ML
    development. Irrespective of the tools/software we use for ML development, there
    are a few things that can be universally adopted by all of us to improve ML engineering.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是向您展示市场上可用的内容以及它与自行管理的特征存储库（如 Feast）的不同之处。我们还将讨论这些特征存储库之间的相似性和差异性。本章还想讨论的另一个方面是机器学习开发中的最佳实践。无论我们使用什么工具/软件进行机器学习开发，都有一些事情我们可以普遍采用来提高机器学习工程。
- en: 'In this chapter, we will discuss the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: The available feature stores on the market
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场上可用的特征存储库
- en: Feature management with SageMaker Feature Store
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker 特征存储库进行特征管理
- en: ML best practices
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习最佳实践
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To run through the examples and get a better understanding of this chapter,
    the topics covered in previous chapters will be useful but not required. To follow
    the code examples in the chapter, you need familiarity with a notebook environment,
    which could be a local setup such as Jupyter or an online notebook environment
    such as Google Colab, Kaggle, or SageMaker. You will also need an AWS account
    with full access to SageMaker and the AWS Glue console. You can create a new account
    and use all the services for free during the trial period. You can find the code
    examples of the book using the following GitHub link:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行示例并更好地理解本章内容，前几章中涵盖的主题将很有用，但不是必需的。要跟随本章中的代码示例，您需要熟悉笔记本环境，这可以是本地设置，如 Jupyter，或在线笔记本环境，如
    Google Colab、Kaggle 或 SageMaker。您还需要一个具有对 SageMaker 和 AWS Glue 控制台完全访问权限的 AWS
    账户。您可以在试用期间创建新账户并免费使用所有服务。您可以使用以下 GitHub 链接找到本书的代码示例：
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter07](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter07)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter07](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter07)'
- en: The available feature stores on the market
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 市场上可用的特征存储库
- en: In this section, we will briefly discuss some of the available feature stores
    on the market and how they compare with Feast, as well as some commonalities and
    differences between these feature stores.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要讨论市场上可用的特征存储库，以及它们与 Feast 的比较，以及这些特征存储库之间的相似之处和不同之处。
- en: The Tecton Feature Store
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tecton 特征存储库
- en: '*Tecton is an enterprise feature store, built by the creators of Uber''s machine
    learning platform Michelangelo* (https://eng.uber.com/michelangelo-machine-learning-platform/).
    Tecton is also one of the major contributors to Feast. Hence, when you look at
    Tecton''s documentation ([https://docs.tecton.ai/index.html](https://docs.tecton.ai/index.html)),
    you will see a lot of similarities in the APIs and terminology. However, there
    are a lot of functionalities in Tecton that don''t exist in Feast. Also, Tecton
    is a managed feature store, which means that you don''t need to build and manage
    the infrastructure; it will be managed for you.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*Tecton是由Uber机器学习平台Michelangelo的创建者构建的企业级特征存储*（https://eng.uber.com/michelangelo-machine-learning-platform/）。Tecton也是Feast的主要贡献者之一。因此，当你查看Tecton的文档([https://docs.tecton.ai/index.html](https://docs.tecton.ai/index.html))时，你会在API和术语中看到很多相似之处。然而，Tecton中有许多功能在Feast中不存在。此外，Tecton是一个托管特征存储，这意味着你不需要构建和管理基础设施；它将为你管理。'
- en: 'As with most feature stores, Tecton uses online and offline stores for low
    latency and historical storage respectively. However, there are fewer options
    for online and offline stores compared to Feast, and it is currently supported
    only on AWS. If you prefer Azure or GCP, you don''t have any other option but
    to wait for now. I believe multiple cloud providers and data stores will be eventually
    supported. Tecton uses a **Software as a Service** (**SaaS**) deployment model
    and separates deployment into data and control planes. You can find their deployment
    model at the following link: [https://docs.tecton.ai/setting-up-tecton/07a-deployment_saas.html](https://docs.tecton.ai/setting-up-tecton/07a-deployment_saas.html).
    The best part is that data never leaves the customer''s AWS account, and only
    the metadata required for the control panel to work is accessed by the Tecton-owned
    AWS account; also, the UI will be hosted in their account. However, if you want
    to expose online data through a REST/gRPC API endpoint, the service will be hosted
    in Tecton''s AWS account. The online feature request and response will be routed
    through their account.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数特征存储一样，Tecton使用在线和离线存储分别用于低延迟和历史存储。然而，与Feast相比，在线和离线存储的选项较少，并且目前仅支持AWS。如果你更喜欢Azure或GCP，你现在没有其他选择，只能等待。我相信最终将支持多个云提供商和数据存储。Tecton使用**软件即服务**（**SaaS**）部署模型，并将部署分为数据平面和控制平面。你可以在以下链接中找到他们的部署模型：[https://docs.tecton.ai/setting-up-tecton/07a-deployment_saas.html](https://docs.tecton.ai/setting-up-tecton/07a-deployment_saas.html)。最好的部分是数据永远不会离开客户的AWS账户，只有控制面板运行所需的元数据由Tecton拥有的AWS账户访问；此外，UI将托管在他们账户中。然而，如果你想通过REST/gRPC
    API端点公开在线数据，该服务将托管在Tecton的AWS账户中。在线特征请求和响应将通过他们的账户路由。
- en: 'Once Tecton is deployed into your AWS account, you can interact with it using
    the Python SDK. The CLI commands are similar to Feast commands; however, there
    are options such as being able to manage versions of your feature definitions
    and downgrading to a previous version of the definitions. As well as the common
    workflows that you can do with a feature store such as ingesting, querying at
    low latency, and performing point-in-time joins, with Tecton, you can define transformation
    as part of the feature store. This is one of my favorite features of Tecton. Here
    is the link to the feature views and transformation page in the feature store:
    [https://docs.tecton.ai/overviews/framework/feature_views/feature_views.html](https://docs.tecton.ai/overviews/framework/feature_views/feature_views.html).
    What this means is that you can define a raw data source configuration for a data
    warehouse (Snowflake), database, Kinesis, or Kafka, and define a PySpark, Spark
    SQL, or pandas transformation to generate features. Tecton orchestrates these
    jobs on a defined schedule and generates features, and ingests them into online
    and offline stores. This can help in tracking data lineage.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Tecton部署到你的AWS账户，你可以使用Python SDK与之交互。CLI命令与Feast命令类似；然而，有一些选项，例如可以管理特征定义的版本，以及降级到定义的先前版本。以及你可以使用特征存储执行的常见工作流程，如摄取、低延迟查询和执行点时间连接，使用Tecton，你可以将转换定义为特征存储的一部分。这是Tecton我最喜欢的功能之一。以下是特征存储中特征视图和转换页面的链接：[https://docs.tecton.ai/overviews/framework/feature_views/feature_views.html](https://docs.tecton.ai/overviews/framework/feature_views/feature_views.html)。这意味着你可以为数据仓库（Snowflake）、数据库、Kinesis或Kafka定义原始数据源配置，并定义PySpark、Spark
    SQL或pandas转换以生成特征。Tecton在定义的日程上编排这些作业，生成特征，并将它们摄取到在线和离线存储中。这有助于跟踪数据血缘。
- en: 'The following is an example code snippet on how to define feature views and
    transformation:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例代码片段，说明如何定义特征视图和转换：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You may recognize some of the parameters that you see in the preceding code
    block. Here, the annotation says it's a batch transformation on which you can
    define parameters such as which entities to use, what the schedule is, and whether
    it should ingest data into online and offline stores. In the method definition,
    input data will be injected based on whatever is assigned to the `input` parameter
    in the annotation definition (you can assume it to be a DataFrame from a raw data
    source). On the DataFrame, you add your transformation and return the output DataFrame,
    which will be features. These features will be ingested into the online and offline
    stores on the defined schedule. Once you define the preceding transformation,
    you will have to run `tecton apply`, which is similar to the `feast apply` command,
    to register this transformation. The other functionalities are similar to what
    other feature stores offer; hence, I will skip over them and let you explore their
    documentation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认出在前一个代码块中看到的一些参数。在这里，注释说明这是一个批量转换，你可以定义诸如使用哪些实体、什么时间表以及是否应将数据摄入在线和离线存储等参数。在方法定义中，输入数据将根据注释定义中分配给`input`参数的内容注入（你可以假设它来自原始数据源的DataFrame）。在DataFrame上，你添加你的转换并返回输出DataFrame，这将作为特征。这些特征将按照定义的时间表摄入在线和离线存储。一旦定义了前面的转换，你必须运行`tecton
    apply`，这与`feast apply`命令类似，以注册此转换。其他功能与其他特征存储提供的功能类似；因此，我将跳过它们，并让你探索它们的文档。
- en: What is worth keeping in mind though is that the Tecton deployments are single-tenant
    at the time of writing, which means that if there are teams that cannot share
    data, you might need multiple deployments. There is a set of roles that needs
    to be created that will allow Tecton to install and create required resources
    using cross-account roles, which involves a one-time initial setup from you.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在撰写本文时，Tecton的部署是单租户的，这意味着如果存在无法共享数据的团队，你可能需要多个部署。需要创建一组角色，以便Tecton可以使用跨账户角色安装和创建所需资源，这涉及到您的一次性初始设置。
- en: Databricks Feature Store
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Databricks特征存储
- en: Databricks Feature Store is another option that is available out there for users.
    It makes sense if you are already using Databricks as your notebook environment
    and for data processing jobs. It comes with the Databricks workspaces, so you
    can't have just the feature store. However, you can get a workspace and not use
    anything else except the feature store. It can be hosted on AWS, GCP, or Azure.
    So, if you are on any of the major cloud providers, this could be an option.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks特征存储是用户可用的另一个选项。如果你已经将Databricks用作你的笔记本环境以及数据处理作业，这很有意义。它包含Databricks工作区，所以你不能只有特征存储。然而，你可以获得一个工作区，除了特征存储之外不使用任何其他东西。它可以托管在AWS、GCP或Azure上。因此，如果你在任何一个主要云服务提供商上，这可以是一个选择。
- en: 'The concepts are similar to other feature stores, such as feature tables, timestamp
    versioning on the rows, the ability to do point-in-time joins, and online and
    offline stores. It uses a delta lake for its offline store and uses one of the
    key-value stores, available on a cloud based on which cloud provider you are on.
    The best part about Databricks Feature Store is that it integrates well with all
    the other aspects and components of Databricks, such as Spark DataFrame ingestion,
    retrieval, out-of-the-box integration with the MLflow model repository, access
    control, and tracking the lineage of notebooks that are used to generate a particular
    feature table. It also has a nice UI where you can browse and search for features.
    The next best part is there is no setup required if you already have the Databricks
    workspace. Here is a link to the notebook, which features examples of feature
    creation, ingestion, retrieval, training, and model scoring: [https://docs.databricks.com/_static/notebooks/machine-learning/feature-store-taxi-example.html](https://docs.databricks.com/_static/notebooks/machine-learning/feature-store-taxi-example.html).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念与其他特征存储类似，例如特征表、行的时间戳版本、进行点时间连接的能力以及在线和离线存储。它使用delta lake作为其离线存储，并使用基于您所在云提供商的关键值存储之一。Databricks特征存储的最好之处在于它与Databricks的所有其他方面和组件集成良好，例如Spark
    DataFrame的摄取、检索、与MLflow模型存储库的即插即用集成、访问控制和跟踪用于生成特定特征表的笔记本的谱系。它还有一个友好的用户界面，您可以浏览和搜索特征。最好的下一部分是，如果您已经拥有Databricks工作区，则无需设置。以下是笔记本的链接，其中包含特征创建、摄取、检索、训练和模型评分的示例：[https://docs.databricks.com/_static/notebooks/machine-learning/feature-store-taxi-example.html](https://docs.databricks.com/_static/notebooks/machine-learning/feature-store-taxi-example.html)。
- en: 'However, there are a few things to keep in mind. Databricks Feature Store doesn''t
    have a concept of projects; hence, feature tables are the highest level of abstraction,
    and access control is at the feature table level. Additionally, Databricks'' online
    model hosting is still in public preview (although no doubt it will eventually
    become a standard offering). This means that if you use Databricks Feature Store
    for an online model that is hosted outside of Databricks, it might have to connect
    to the online store using the direct client. For example, if you use DynamoDB
    as an online store (Databricks offers multiple choices, depending on the cloud
    provider) and host the model in Amazon `boto3` client for features during prediction.
    Also, sharing features across the workspace might need additional configuration
    for either access tokens or using a central workspace for the feature store. Here
    is the link to the Databricks Feature Store documentation for more details: [https://docs.databricks.com/applications/machine-learning/feature-store/index.html](https://docs.databricks.com/applications/machine-learning/feature-store/index.html).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一些事情需要记住。Databricks特征存储没有项目概念；因此，特征表是最高级别的抽象，访问控制是在特征表级别。此外，Databricks的在线模型托管仍在公共预览中（尽管毫无疑问它最终将成为一项标准服务）。这意味着如果您使用Databricks特征存储来托管在Databricks之外的在线模型，它可能需要通过直接客户端连接到在线商店。例如，如果您使用DynamoDB作为在线商店（Databricks根据云提供商提供多种选择）并在Amazon
    `boto3`客户端中托管预测期间的特征模型。此外，跨工作区共享特征可能需要额外的配置，无论是访问令牌还是使用中央工作区作为特征存储。以下是Databricks特征存储文档的链接，其中包含更多详细信息：[https://docs.databricks.com/applications/machine-learning/feature-store/index.html](https://docs.databricks.com/applications/machine-learning/feature-store/index.html)。
- en: Google's Vertex AI Feature Store
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google的Vertex AI特征存储
- en: Google's Vertex AI is a **Platform as a Service** (**PaaS**) offering from Google
    for ML and AI. Vertex AI aims at offering an end-to-end ML platform that provides
    a set of tools for ML development, training, orchestration, model deployment,
    monitoring, and more. The tool that we are most interested in is Vertex AI Feature
    Store. If you are already using GCP for your services, it should be an automatic
    pick.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Google的Vertex AI是Google为机器学习和人工智能提供的**平台即服务**（**PaaS**）产品。Vertex AI旨在提供一个端到端的机器学习平台，提供一系列用于机器学习开发、训练、编排、模型部署、监控等工具。我们最感兴趣的工具是Vertex
    AI特征存储。如果您已经使用GCP来提供服务，它应该是一个自动的选择。
- en: 'The concepts and terminology are very similar to that of Feast. The highest
    level of abstraction in Vertex AI is called a *featurestore*, similar to a *project*
    in Feast, and a *featurestore* can have *entities*, and *features* should belong
    to *entities*. It supports online and batch serving, just like all the other feature
    stores. However, unlike Feast and Tecton, there are no options available for online
    and historical stores. Since it is a managed infrastructure, users don''t need
    to worry about installation and choosing online and offline stores – probably
    just the pricing. Here is a link to its prices: [https://cloud.google.com/vertex-ai/pricing#featurestore](https://cloud.google.com/vertex-ai/pricing#featurestore).
    It uses **IAM** (short for **Identity and Access Management**) for authentication
    and authorization, and you also get a UI to search and browse features.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 概念和术语与 Feast 非常相似。Vertex AI 中的最高抽象级别称为 *特征存储*，类似于 Feast 中的 *项目*，一个 *特征存储* 可以有
    *实体*，而 *特征* 应属于 *实体*。它支持在线和批量服务，就像所有其他特征存储一样。然而，与 Feast 和 Tecton 不同，没有可用的在线和历史存储选项。由于它是一个托管基础设施，用户无需担心安装和选择在线和离线存储——可能只是价格问题。以下是其价格的链接：[https://cloud.google.com/vertex-ai/pricing#featurestore](https://cloud.google.com/vertex-ai/pricing#featurestore)。它使用
    **IAM**（代表 **身份和访问管理**）进行身份验证和授权，并且您还可以获得一个用于搜索和浏览特征的 UI。
- en: The best part of Vertex AI is its integration with other components of GCP and
    the Vertex AI service itself for feature generation, pipeline management, and
    data lineage tracking. One of my favorite features is drift monitoring. You can
    set up a feature monitoring configuration on the feature tables, which can generate
    data distribution reports for you without requiring any additional work.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 的最佳部分是其与其他 GCP 组件以及 Vertex AI 服务本身的集成，用于特征生成、管道管理和数据血缘跟踪。我最喜欢的功能之一是漂移监控。您可以在特征表上设置特征监控配置，这样它就可以为您生成数据分布报告，而无需进行任何额外的工作。
- en: 'Again, there are a few things to keep in mind. For online serving, you need
    to do capacity sizing and set up the number of nodes required to handle your traffic.
    The autoscaling option for online serving is still in public preview (although
    it''s just a matter of time before it becomes a standard offering), but capacity
    planning should be a major problem to solve. A few load test simulations should
    help you figure that out easily. Also, there are quotas and limits on the number
    of online serving nodes you can have for a feature store, the length of data retention,
    and the number of features per entity. Some of these can be increased on request
    whereas others can''t. Here is a link to the list of quotas and limits on a feature
    store: https://cloud.google.com/vertex-ai/docs/quotas#featurestore.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，有几件事情需要记住。对于在线服务，你需要进行容量规划并设置处理您流量的所需节点数量。在线服务的自动扩展选项目前仍处于公开预览阶段（尽管它很快就会成为标准服务），但容量规划应该是一个需要解决的主要问题。一些负载测试模拟可以帮助你轻松解决这个问题。此外，对于特征存储，您拥有的在线服务节点数量、数据保留长度以及每个实体的特征数量都有配额和限制。其中一些可以在请求后增加，而其他则不行。以下是特征存储配额和限制的链接：[https://cloud.google.com/vertex-ai/docs/quotas#featurestore](https://cloud.google.com/vertex-ai/docs/quotas#featurestore)。
- en: The Hopsworks Feature Store
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hopsworks 特征存储
- en: Hopsworks is another open source feature store under the AGPL-V3 license that
    can be run on-premises, on AWS or Azure. It also has an enterprise version of
    the feature store that supports GCP as well as any Kubernetes environment. Similar
    to other ML platform services, it also offers multiple components, such as model
    management and compute environment management.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Hopsworks 是一个在 AGPL-V3 许可下运行的另一个开源特征存储，可以在本地、AWS 或 Azure 上运行。它还提供了一个支持 GCP 以及任何
    Kubernetes 环境的企业版特征存储。与其他机器学习平台服务类似，它也提供多个组件，例如模型管理和计算环境管理。
- en: The concepts are similar to that of other feature stores; however, the terminology
    is different. It doesn't have a concept of entities, and *featuregroups* in *Hopsworks*
    are analogous to *featureviews* in *Feast*. Just like other feature stores, Hopsworks
    supports online and offline serving. It uses Apache Hive with Apache Hudi as an
    offline store and MySQL Cluster as an online store. Again, there are no options
    for online or offline stores. However, there are different storage connectors
    developed by Hopsworks that can be used to create on-demand external feature groups,
    such as *RedShiftSource*, which we defined in *Feast* in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*. But there is a limitation on external feature
    groups, meaning there is no time travel, online serving, and so on.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念与其他特征存储类似；然而，术语不同。它没有实体的概念，*Hopsworks*中的*featuregroups*与*Feast*中的*featureviews*类似。就像其他特征存储一样，Hopsworks支持在线和离线服务。它使用Apache
    Hive与Apache Hudi作为离线存储，MySQL Cluster作为在线存储。再次强调，没有在线或离线存储的选项。然而，Hopsworks开发了不同的存储连接器，可以用来创建按需的外部特征组，例如我们在*Feast*中定义的*RedShiftSource*，见[*第4章*](B18024_04_ePub.xhtml#_idTextAnchor065)，*将特征存储添加到机器学习模型中*。但是，外部特征组有一些限制，意味着没有时间旅行、在线服务等。
- en: 'There are a lot of features in the Hopsworks Feature Store that are fancy and
    very interesting. Some of the best ones are as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Hopsworks特征存储中有许多有趣且功能强大的特性。以下是一些最好的特性：
- en: '**Project-level multi-tenancy**: Each project has an owner and can share resources
    with other members in the team and across teams.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项目级多租户**：每个项目都有一个所有者，并且可以与其他团队成员以及跨团队共享资源。'
- en: '**Feature group versioning**: Hopsworks supports feature group versioning,
    which is not currently supported by any other feature stores on the market.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征组版本控制**：Hopsworks支持特征组版本控制，这是市场上任何其他特征存储所不支持的功能。'
- en: '**Statistics on feature groups**: It provides a few out-of-the-box statistics
    on feature groups, such as feature co-relation computation, a frequency histogram
    on features, and uniqueness. The following is an example feature group:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征组的统计信息**：它为特征组提供了一些开箱即用的统计信息，例如特征相关性计算、特征的频率直方图和唯一性。以下是一个示例特征组：'
- en: '[PRE24]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Feature validation**: This is another fancy feature that is available out
    of the box. This is a set of predefined validation rules that exist on feature
    groups such as the minimum and maximum values of a feature, a uniqueness count
    of features, the entropy of a feature, and the maximum length of features. It
    has enough rule types that you won''t have a use case where you need to customize
    a validation rule. The following are a couple of example rules:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征验证**：这是另一个开箱即用的有趣特性。这是一组预定义的验证规则，存在于特征组中，例如特征的最低和最高值、特征的唯一性计数、特征的熵以及特征的长度最大值。它有足够的规则类型，您不会遇到需要自定义验证规则的场景。以下是一些示例规则：'
- en: '[PRE25]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Transformation functions**: Similar to Tecton transformations for feature
    views, in Hopsworks, you can define or use built-in transformation on a training
    dataset (Hopsworks has a concept of training data where you can pick features
    from different feature groups and create a training dataset definition on top
    of them– a concept similar to database views).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换函数**：与Tecton的转换函数类似，在Hopsworks中，您可以在训练数据集上定义或使用内置的转换（Hopsworks有一个训练数据的概念，您可以从不同的特征组中选择特征，并在其上创建训练数据集定义——一个类似于数据库视图的概念）。'
- en: 'There are some things to keep in mind though. If you choose the open source
    version, you may not have several features, and infrastructure will have to be
    self-managed. Conversely, for the enterprise version, you will have to collaborate
    with a Hopsworks engineer and create a few resources and roles required for the
    installation of Hopsworks on the cloud provider. Here is a link to all the documentation:
    https://docs.hopsworks.ai/feature-store-api/2.5.8/. I recommend having a look
    at the features even if you don''t use them; this might give an idea of some of
    the features you might want to build or have in your feature store.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一些事情需要记住。如果您选择开源版本，您可能不会拥有所有这些特性，并且基础设施将需要自行管理。相反，对于企业版本，您将需要与Hopsworks工程师合作，创建在云服务提供商上安装Hopsworks所需的一些资源和角色。以下是所有文档的链接：https://docs.hopsworks.ai/feature-store-api/2.5.8/。即使您不使用这些特性，我也建议您查看一下；这可能会给您一些关于您可能想要构建或拥有的特征存储中的特性的想法。
- en: SageMaker Feature Store
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker特征存储
- en: SageMaker is an end-to-end ML platform offered by AWS. Just like Vertex AI,
    it has a notebook environment, AutoML, processing jobs and model management, a
    feature store, and so on. If you are an AWS-focused company, this must be a natural
    pick over the others.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker是AWS提供的一个端到端机器学习平台。就像Vertex AI一样，它有一个笔记本环境，AutoML，处理作业和模型管理，特征存储等。如果你是一个专注于AWS的公司，这必须是自然的选择，而不是其他选择。
- en: The concepts are close to that of other feature stores, although some of the
    terms are different. For example, SageMaker Feature Store also doesn't have the
    concept of entities, and *featureviews* in Feast are analogous to *featuregroups*
    in SageMaker. It has all the basic features, such as online and offline stores
    and serving. However, you don't have options to pick from. It uses S3 as an offline
    store and one of the key-value stores as an online store (AWS doesn't say what
    is used for an online store in its documentation). AWS uses IAM for authentication
    and authorization. To access feature store currently, you need full access to
    SageMaker and the AWS Glue console. If you compare SageMaker to Feast, both use/support
    S3 as an offline store, a key-value store as an online store, and a Glue catalog
    for managing the schema. Apart from SageMaker being a managed feature store, another
    difference is that Feast uses Redshift for querying offline data, whereas SageMaker
    uses Amazon Athena (serverless) for querying. You can add this functionality to
    Feast if you are a fan of serverless technologies.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念与其他特征存储的概念相近，尽管一些术语不同。例如，SageMaker Feature Store也没有实体的概念，Feast中的*featureviews*与SageMaker中的*featuregroups*类似。它具有所有基本功能，如在线和离线存储以及服务。然而，你没有选择。它使用S3作为离线存储，其中一个键值存储作为在线存储（AWS在其文档中没有说明在线存储用于什么）。AWS使用IAM进行身份验证和授权。要访问特征存储库，目前需要完全访问SageMaker和AWS
    Glue控制台。如果你将SageMaker与Feast进行比较，两者都使用/支持S3作为离线存储，键值存储作为在线存储，以及Glue目录来管理模式。除了SageMaker是一个托管特征存储之外，另一个区别是Feast使用Redshift查询离线数据，而SageMaker使用Amazon
    Athena（无服务器）查询。如果你是服务器无服务技术的粉丝，你可以将此功能添加到Feast中。
- en: One of my favorite things about SageMaker Feature Store is that there is no
    infrastructure management. Apart from creating an IAM role to access the feature
    store, you don't need to manage anything. All the resources for any given load
    are managed by AWS. All you need to worry about is just developing and ingesting
    features. SageMaker Feature Store also supports ingestion using Spark on EMR or
    Glue jobs (serverless). Along with the features, it also adds metadata, such as
    `write_time` and `api_invocation_time`, that can be used in queries. The best
    part is that you can query offline data using Amazon Athena SQL queries.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢的SageMaker Feature Store的特点之一是无需管理基础设施。除了创建一个IAM角色以访问特征存储库之外，你不需要管理任何东西。任何给定负载的所有资源都由AWS管理。你只需要关注开发和摄取特征。SageMaker
    Feature Store还支持在EMR或Glue作业（无服务器）上使用Spark进行摄取。除了特征外，它还添加了元数据，如`write_time`和`api_invocation_time`，这些可以在查询中使用。最好的部分是你可以使用Amazon
    Athena SQL查询查询离线数据。
- en: There are a few things to keep in mind though. The current implementation doesn't
    yet have granular access management. Right now, you need full access to SageMaker
    to use Feature Store, although I believe that it's only a matter of time before
    AWS starts offering granular access. Point-in-time joins are not available out
    of the box; however, these can be achieved using SQL queries or Spark.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有一些需要注意的事项。当前的实现还没有细粒度的访问管理。目前，你需要完全访问SageMaker才能使用特征存储，尽管我相信这只是AWS开始提供细粒度访问的时间问题。点时间连接不是现成的；然而，这些可以通过SQL查询或Spark实现。
- en: 'So far, we have looked at a few of the available options on the market; you
    can find other feature stores that are available at this link: [https://www.featurestore.org/](https://www.featurestore.org/).
    However, picking the right feature store for your project or team can be tricky.
    The following are a few things to keep in mind while picking a feature store:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经查看了一些市场上的可用选项；你可以通过此链接找到其他可用的特征存储：[https://www.featurestore.org/](https://www.featurestore.org/)。然而，为你的项目或团队选择正确的特征存储可能很棘手。在挑选特征存储时，以下是一些需要注意的事项：
- en: Your primary cloud provider makes a huge difference. If you are GCP-focused,
    it doesn't make sense to use SageMaker Feature Store and vice versa. If you are
    multi-cloud, then you will have more options.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的主要云提供商有很大影响。如果你专注于GCP，使用SageMaker Feature Store就没有意义，反之亦然。如果你是多云，那么你将有更多选择。
- en: The data processing framework is also another key factor that decides what feature
    store to use. For example, if you use SageMaker as your ML platform, trying out
    SageMaker Feature Store before others makes more sense.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理框架也是决定使用哪个特征存储的另一个关键因素。例如，如果你使用SageMaker作为你的机器学习平台，在其他人之前尝试SageMaker Feature
    Store更有意义。
- en: Integration with other components in your ecosystem is also key – for instance,
    answering questions such as how well it integrates with your processing platform,
    the orchestration framework, the model management service, data validation frameworks,
    and your ML development process can really help in picking the right feature store.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与你生态系统中的其他组件的集成也很关键——例如，回答诸如它与你的处理平台、编排框架、模型管理服务、数据验证框架以及你的机器学习开发过程如何良好集成等问题，真的有助于选择正确的特征存储。
- en: The required functionalities and your team structure make a big difference.
    If you are a small team who wants to just concentrate on ML, then a managed offering
    of a feature store makes sense, whereas if you have a platform team to manage
    the infrastructure, you may look into open source offerings and also evaluate
    the build versus buy options. If you have a platform team, they might look for
    additional features such as multi-tenancy, granular access control, and SaaS/PaaS.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需的功能和你的团队结构有很大影响。如果你是一个只想专注于机器学习的中小团队，那么托管特征存储的提供方案是有意义的，而如果你有一个平台团队来管理基础设施，你可能需要考虑开源提供方案，并评估构建与购买选项。如果你有平台团队，他们可能会寻找额外的功能，如多租户、细粒度访问控制和SaaS/PaaS。
- en: In conclusion, a lot of factors influence the choice of a feature store other
    than the functionalities it offers, as it must integrate well with a broader ecosystem.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，除了它提供的功能外，许多因素都会影响特征存储的选择，因为它必须与更广泛的生态系统良好集成。
- en: Next, let's look at how a managed feature store works.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看一个托管特征存储是如何工作的。
- en: Feature management with SageMaker Feature Store
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker Feature Store进行特征管理
- en: In this section, we will look into what action we might have to take if we were
    to use a managed feature store instead of Feast in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如果我们使用托管特征存储而不是Feast在[*第4章*](B18024_04_ePub.xhtml#_idTextAnchor065)，“将特征存储添加到机器学习模型”中，我们可能需要采取哪些行动。
- en: Important Note
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: All managed feature stores have a similar workflow; some may be API-based and
    some work through a CLI. But irrespective of this, the amount of work involved
    in using the feature store would be similar to what we will discuss in this section.
    The only reason I am going through SageMaker is familiarity and ease of access
    to it, using the free trial as a featured product in AWS.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所有托管特征存储都有一个类似的流程；有些可能是基于API的，有些则通过CLI工作。但无论哪种方式，使用特征存储所需的工作量将与我们在本节中讨论的内容相似。我之所以选择SageMaker，是因为熟悉它并且易于访问，利用AWS中的免费试用作为特色产品。
- en: Resources to use SageMaker
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker的资源
- en: In [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065), *Adding Feature Store
    to ML Models*, before we started using the feature store, we created a bunch of
    resources on AWS, such as an S3 bucket, a Redshift cluster, an IAM role, and a
    Glue catalog table. Conversely, for a managed feature store such as SageMaker,
    all you need to have is an IAM role that has full access to SageMaker and you
    are all set. Let's try that out now.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B18024_04_ePub.xhtml#_idTextAnchor065)“将特征存储添加到机器学习模型”中，在我们开始使用特征存储之前，我们在AWS上创建了一系列资源，例如一个S3存储桶、一个Redshift集群、一个IAM角色和一个Glue目录表。相反，对于像SageMaker这样的托管特征存储，你所需要的只是一个具有完全访问SageMaker的IAM角色，你就准备好了。现在让我们试试看。
- en: We need some IAM user credentials and an IAM role that SageMaker Feature Store
    can assume. Creating an IAM user is similar to what we have done before. Follow
    the same steps and create an IAM user, and assign **AmazonS3FullAccess** and **AmazonSageMakerFullAccess**
    permissions. IAM role creation is the same as we have done before; however, we
    need to allow the SageMaker service to assume the role.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些IAM用户凭证和一个SageMaker Feature Store可以承担的IAM角色。创建IAM用户与之前我们所做的是类似的。遵循相同的步骤创建一个IAM用户，并分配**AmazonS3FullAccess**和**AmazonSageMakerFullAccess**权限。IAM角色的创建与之前我们所做的是一样的；然而，我们需要允许SageMaker服务承担该角色。
- en: Important Note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: As mentioned many times before, it is never a good idea to assign full access;
    permissions should always be restrictive based on resources.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述多次，赋予完全访问权限从来不是一个好主意；权限应该始终基于资源进行限制。
- en: 'Let''s create an IAM role:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个IAM角色：
- en: 'Log in to your AWS account and navigate to the IAM role page, using the search
    bar; alternatively, visit the following URL: https://us-east-1.console.aws.amazon.com/iamv2/home#/roles.
    The following page will be displayed:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录您的AWS账户，并使用搜索栏导航到IAM角色页面；或者，访问以下URL：https://us-east-1.console.aws.amazon.com/iamv2/home#/roles。将显示以下页面：
- en: '![Figure 7.1 – The IAM role home page'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.1 – IAM角色主页'
- en: '](img/B18024_07_001.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_07_001.jpg)'
- en: Figure 7.1 – The IAM role home page
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – IAM角色主页
- en: 'On the displayed web page, click on **Create role** to navigate to the following
    screen:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在显示的网页上，点击**创建角色**以导航到以下屏幕：
- en: '![Figure 7.2 – The IAM role creation page'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.2 – IAM角色创建页面'
- en: '](img/B18024_07_002.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_07_002.jpg)'
- en: Figure 7.2 – The IAM role creation page
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – IAM角色创建页面
- en: 'On the screen displayed in *Figure 7.2*, in the **Use cases for other AWS services**
    dropdown, select **SageMaker** and then click the **SageMaker - Execution** radio
    button. Scroll down and click on **Next**, leaving everything as default on the
    **Add Permissions** page, and then click on **Next**. The following page will
    be displayed:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*图7.2*显示的屏幕上，在**其他AWS服务的用例**下拉菜单中，选择**SageMaker**，然后点击**SageMaker - 执行**单选按钮。向下滚动并点击**下一步**，在**添加权限**页面保持默认设置，然后点击**下一步**。接下来将显示以下页面：
- en: '![Figure 7.3 – The Name, review and create page'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.3 – 名称、审查和创建页面'
- en: '](img/B18024_07_003.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_07_003.jpg)'
- en: Figure 7.3 – The Name, review and create page
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 名称、审查和创建页面
- en: On the displayed web page, fill in the `sagemaker-iam-role`. Scroll all the
    way down and click on `arn:aws:iam::<account_number>:role/sagemaker-iam-role`.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在显示的网页上，填写`sagemaker-iam-role`。滚动到页面底部并点击`arn:aws:iam::<account_number>:role/sagemaker-iam-role`。
- en: That's all we need to access SageMaker Feature Store. Let's create the feature
    definitions next.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们访问SageMaker Feature Store所需的所有内容。接下来，让我们创建特征定义。
- en: Generating features
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成特征
- en: 'To define the feature group, since we are trying to compare how it differs
    from Feast, we will take the same feature set. You can download the previously
    ingested features from an S3 bucket or download it from the GitHub link: [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter07/rfm_features.parquet](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter07/rfm_features.parquet).
    After downloading the Parquet file, copy it to a location that can be accessed
    from the notebook. The next step is to create a new notebook, which I am calling
    `ch7-sagemaker-feature-store.ipynb`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义特征组，因为我们正在尝试比较它与Feast的不同之处，我们将使用相同的特征集。您可以从S3存储桶下载之前导入的特征，或者从GitHub链接下载：[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter07/rfm_features.parquet](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter07/rfm_features.parquet)。下载Parquet文件后，将其复制到可以从笔记本访问的位置。下一步是创建一个新的笔记本，我将其命名为`ch7-sagemaker-feature-store.ipynb`：
- en: 'Let''s install the required libraries first:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先安装所需的库：
- en: '[PRE26]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After installing the libraries, let''s generate the features. Here, we will
    be just reading the copied file from the location and making minor modifications
    to the dataset:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装库后，让我们生成特征。在这里，我们将从位置读取复制的文件并对数据集进行一些小的修改：
- en: '[PRE27]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The preceding code block reads the file and drops the `created_timestamp` column,
    as it is not required by SageMaker. We are also updating the `event_timestamp`
    column to the latest time and changing the type to `float` instead of `datetime`.
    The reason for this is that SageMaker only supports the `int`, `float`, and `string`
    features at the time of writing, and `datetime` files can either be a `float`
    or `string` object in the `datetime` ISO format.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块读取文件并删除了`created_timestamp`列，因为它对于SageMaker不是必需的。我们还将`event_timestamp`列更新为最新时间，并将其类型更改为`float`而不是`datetime`。这样做的原因是，SageMaker在编写时仅支持`int`、`float`和`string`特征，而`datetime`文件可以是`float`或`string`对象，格式为`datetime`
    ISO格式。
- en: 'The code block produces the following output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块生成了以下输出：
- en: '![Figure 7.4 – Recency, Frequency, and Monetary value (RFM) features'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.4 – 近期、频率和货币价值（RFM）特征'
- en: '](img/B18024_07_004.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_07_004.jpg)'
- en: Figure 7.4 – Recency, Frequency, and Monetary value (RFM) features
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 近期、频率和货币价值（RFM）特征
- en: Now that we have RFM features, the next step is to define the feature group.
    If you recall correctly from [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*, after generating the features, we created
    the feature definitions and applied them to the feature store.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 RFM 特征，下一步是定义特征组。如果你正确地回忆起 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065)，*将特征存储添加到机器学习模型中*，在生成特征后，我们创建了特征定义并将它们应用到特征存储中。
- en: Defining the feature group
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义特征组
- en: 'To define the feature group, since it''s a one-time activity, it should be
    done in a separate notebook rather than by feature engineering. For this exercise,
    let''s continue in the same notebook and define the feature group:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义特征组，因为它是一项一次性活动，应该在一个单独的笔记本中完成，而不是通过特征工程。对于这个练习，让我们继续在同一个笔记本中定义特征组：
- en: 'The following code block defines a few imports and creates the SageMaker session:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码块定义了一些导入并创建 SageMaker 会话：
- en: '[PRE28]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the code block, replace `<aws_key_id>` and `<aws_secret_id>` with the key
    and secret of the IAM user created earlier. Also, assign `role` with your IAM
    role ARN.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码块中，将 `<aws_key_id>` 和 `<aws_secret_id>` 替换为之前创建的 IAM 用户的密钥和密钥。同时，使用你的 IAM
    角色ARN分配 `role`。
- en: 'The following code block creates the feature group object and loads the feature
    definitions from the input DataFrame:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码块创建特征组对象并从输入 DataFrame 中加载特征定义：
- en: '[PRE29]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding code block produces the following output:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块产生以下输出：
- en: '![Figure 7.5 – The load feature definitions call'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5 – 加载特征定义的调用]'
- en: '](img/B18024_07_005.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5 – 加载特征定义的调用](img/B18024_07_005.jpg)'
- en: Figure 7.5 – The load feature definitions call
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 加载特征定义的调用
- en: As you can see in *Figure 7.5*, the `load_feature_definitions` call reads the
    input DataFrame and loads the feature definition automatically.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在 *图 7.5* 中看到的那样，`load_feature_definitions` 调用读取输入 DataFrame 并自动加载特征定义。
- en: 'The next step is to create the feature group. The following code block creates
    the feature group in SageMaker:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是创建特征组。下面的代码块在 SageMaker 中创建特征组：
- en: '[PRE30]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The preceding code block invokes the create API by passing the following parameters:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块通过传递以下参数调用 create API：
- en: '`s3_uri`: The location where the feature data will be stored'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s3_uri`：特征数据将被存储的位置'
- en: '`record_identifier_name`: The name of the `id` column (the same as the entity
    column in Feast)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`record_identifier_name`：`id` 列的名称（与 Feast 中的实体列相同）'
- en: '`event_time_feature_name`: The timestamp column that will be used for time
    travel'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`event_time_feature_name`：将用于时间旅行的时戳列'
- en: '`role_arn`: The role that SageMaker Feature Store can assume'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`role_arn`：SageMaker Feature Store 可以承担的角色'
- en: '`enable_online_store`: Whether to enable online serving or not for this feature
    group'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enable_online_store`：是否为此特征组启用在线服务'
- en: 'The code block produces the following output on the successful creation of
    the feature group:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块在成功创建特征组时产生以下输出：
- en: '![Figure 7.6 – Feature group creation'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – 特征组创建](img/B18024_07_006.jpg)'
- en: '](img/B18024_07_006.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – 特征组创建](img/B18024_07_006.jpg)'
- en: Figure 7.6 – Feature group creation
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 特征组创建
- en: That's all – our feature group is ready to use. Let's ingest the features next.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 那就结束了 – 我们的特征组已经准备好使用。接下来让我们导入特征。
- en: Feature ingestion
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征导入
- en: 'Feature ingestion in SageMaker Feature Store is simple. It is a simple API
    call, as shown in the following code block:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SageMaker Feature Store 中进行特征导入很简单。它是一个简单的 API 调用，如下面的代码块所示：
- en: '[PRE31]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The preceding code block will ingest the features and print the failed row numbers
    if there are any.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块将导入特征，如果有任何失败的行数，将打印出来。
- en: One thing to keep in mind here is that, like Feast, you don't need to do anything
    extra to materialize the latest features from an offline to an online store. If
    the online store is enabled, the data will be ingested to both online and offline
    stores, and the latest data will be available in the online store for querying
    right away.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要记住的一件事是，就像 Feast 一样，你不需要做任何额外的事情来将最新功能从离线商店转换为在线商店。如果启用了在线商店，数据将被导入到在线和离线商店，最新数据将立即可在在线商店中查询。
- en: Let's query the online store next.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们查询在线商店。
- en: Getting records from an online store
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从在线商店获取记录
- en: 'Like Feast, querying from an online store is simple. All you need is the record
    ID and the feature group name. The following code block gets the record from the
    online store:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Feast 一样，从在线商店查询很简单。你所需要的是记录 ID 和特征组名称。下面的代码块从在线商店获取记录：
- en: '[PRE34]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The preceding code block gets all the features for the customer with the `12747.0`
    ID from the online store. The query should return the results within milliseconds.
    The output will be similar to the following code block:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码块从在线商店获取了具有`12747.0` ID的客户的全部特征。查询应在毫秒内返回结果。输出将类似于以下代码块：
- en: '[PRE42]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: As you can see, the output contains all the features and corresponding values.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，输出包含所有特征及其对应值。
- en: Now that we have looked at querying an online store, let's check out how to
    generate the training dataset and query historical data next.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何查询在线商店，接下来让我们看看如何生成训练数据集和查询历史数据。
- en: Querying historical data with Amazon Athena
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Amazon Athena查询历史数据
- en: As mentioned previously, SageMaker Feature Store offers the ability to run SQL
    queries on a historical store using Amazon Athena.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，SageMaker Feature Store提供了使用Amazon Athena在历史存储上运行SQL查询的能力。
- en: 'The following code block generates the latest snapshot of all customers and
    their features:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块生成了所有客户及其特征的最新快照：
- en: '[PRE44]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The code block uses a nested SQL query, where the inner query gets all customers
    and their features, in descending order, from the `event_time`, `Api_Invocation_Time`,
    and `write_time` columns. The outer query selects the first occurrence of every
    customer from the results of the inner query. On successful execution of the query,
    the code block outputs the location of the query results along with additional
    details.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码块使用嵌套SQL查询，其中内部查询从`event_time`、`Api_Invocation_Time`和`write_time`列按降序获取所有客户及其特征。外部查询从内部查询的结果中选择每个客户的第一个出现。查询成功执行后，代码块输出查询结果的存储位置以及附加详情。
- en: 'The results can be loaded as a DataFrame, as shown in the following code block:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以像以下代码块所示那样加载为DataFrame：
- en: '[PRE56]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The preceding code block output the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码块输出了以下内容：
- en: '![Figure 7.7 – Athena query results'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.7 – Athena查询结果'
- en: '](img/B18024_07_007.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_07_007.jpg]'
- en: Figure 7.7 – Athena query results
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – Athena查询结果
- en: 'Feel free to try out other Athena queries on Feature Store. Here is the documentation
    of the Amazon Athena query: [https://docs.aws.amazon.com/athena/latest/ug/what-is.html](https://docs.aws.amazon.com/athena/latest/ug/what-is.html).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 随意尝试在特征存储上运行其他Athena查询。以下是Amazon Athena查询的文档：[https://docs.aws.amazon.com/athena/latest/ug/what-is.html](https://docs.aws.amazon.com/athena/latest/ug/what-is.html)。
- en: Cleaning up a SageMaker feature group
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理SageMaker特征组
- en: 'Let''s clean up the SageMaker resources to save costs before we move forward.
    The cleanup is pretty easy; it is just another API call to delete the feature
    group. The following code block performs that:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续前进之前，让我们清理SageMaker资源以节省成本。清理很简单；它只是另一个API调用以删除特征组。以下代码块执行此操作：
- en: '[PRE58]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'That''s all. After successful execution, it deletes the feature group but leaves
    behind the data in S3 and the Glue catalog, which can still be queried with Amazon
    Athena (using the `boto3` client) if required. Just to make sure everything is
    cleaned up, run the following code block in the same notebook. It should return
    an empty list of feature groups:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了。在成功执行后，它将删除特征组，但会留下S3和Glue目录中的数据，如果需要，仍然可以使用Amazon Athena（使用`boto3`客户端）进行查询。为了确保一切都被清理干净，请在同一笔记本中运行以下代码块。它应该返回一个空的特征组列表：
- en: '[PRE59]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now that we have looked at the SageMaker feature group, let's look into ML best
    practices next.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了SageMaker特征组，接下来让我们看看机器学习的最佳实践。
- en: ML best practices
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习最佳实践
- en: So far in the book, we have discussed feature stores, how to use them for ML
    development and production, and what the available options are when choosing a
    feature store. Though a feature store is one of the major components/aspects of
    ML, there are other aspects of ML that we haven't concentrated on much in this
    book. In this section, let's briefly talk through some of the other aspects and
    best practices in ML.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书中我们讨论了特征存储，如何使用它们进行机器学习开发和生产，以及选择特征存储时可供选择的各种选项。尽管特征存储是机器学习的主要组件/方面之一，但本书中我们并未过多关注机器学习的其他方面。在本节中，让我们简要地讨论一下机器学习的其他方面和最佳实践。
- en: Data validation at source
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 源数据验证
- en: Irrespective of the technologies, algorithms, and infrastructure we use for
    building an ML model, if there are errors and anomalies in data, model performance
    will be severely impacted. Data should be treated as a first-class citizen of
    any ML system. Hence, it is very important to detect errors and anomalies in the
    data before it enters the ML pipeline.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 不论我们用于构建机器学习模型的科技、算法和基础设施如何，如果数据中存在错误和异常，模型的表现将受到严重影响。数据应该被视为任何机器学习系统中的第一公民。因此，在数据进入机器学习流程之前检测错误和异常非常重要。
- en: To run validation on raw data sources, we need a component to create and orchestrate
    the validation rules against the data. Users of the data should be able to write
    any custom rules in SQL queries, Python scripts, or Spark SQL. Any failures in
    the rule should be notified to the data consumers who, in turn, should be able
    to make a decision on whether to stop the pipeline execution, retrain the model,
    or take no action.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要对原始数据源进行验证，我们需要一个组件来创建和编排针对数据的验证规则。数据的使用者应该能够在SQL查询、Python脚本或Spark SQL中编写任何自定义规则。任何规则失败都应通知数据消费者，他们反过来应该能够决定是否停止管道执行、重新训练模型或采取不采取行动。
- en: Some of the common rules include descriptive analytics of the dataset on schedule,
    which can provide insights into data drift. More advanced statistics such as **Kullback–Leibler**
    (**KL**) divergence and the **Population Stability Index** (**PSI**) are good
    to have. Having simple data validation rules such as data freshness, unique values,
    string field length, patterns, and value range thresholds can be very beneficial.
    Schema validation is another important aspect of data validation. Any changes
    in the validation can affect all the consumers and pipelines. The better data
    validation we have at source, the healthier and more performant our models and
    pipeline will be.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的规则包括按计划对数据集进行描述性分析，这可以提供数据漂移的见解。更高级的统计，如**库尔巴克-莱布勒**（**KL**）散度和**人口稳定性指数**（**PSI**），也是很好的。拥有简单的数据验证规则，如数据新鲜度、唯一值、字符串字段长度、模式和值范围阈值，可以非常有帮助。模式验证是数据验证的另一个重要方面。任何验证的变化都可能影响所有消费者和流程。我们在源头拥有的数据验证越好，我们的模型和流程就越健康、性能越好。
- en: Breaking down ML pipeline and orchestration
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分解机器学习流程和编排
- en: One bad practice is to develop everything in a single notebook, from data validation
    and feature engineering to model prediction. This is not a scalable or reusable
    approach. Most of the time is spent cleaning up unwanted code and productionizing
    the model. Hence, it is always a good idea to break down the ML pipeline into
    multiple smaller steps, such as data validation, cleaning, transformation, feature
    engineering, model training, and model prediction. The smaller the transformation
    steps, the more readable, reusable, and easy it will be to debug code for errors.
    This is one of the reasons that Feature Views and transformation in Tecton, and
    storage connectors and transformation functions in Hopsworks are great features.
    Similar features are also offered by many **Extract, Transform and Load** (**ETL**)
    frameworks.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一种不良的做法是将所有内容都在单个笔记本中开发，从数据验证和特征工程到模型预测。这不是一个可扩展或可重用的方法。大部分时间都花在清理不必要的代码和将模型投入生产上。因此，将机器学习流程分解成多个更小的步骤是一个好主意，例如数据验证、清理、转换、特征工程、模型训练和模型预测。转换步骤越小，代码的可读性、可重用性和调试错误就越容易。这也是为什么Tecton中的特征视图和转换，以及Hopsworks中的存储连接器和转换函数是优秀功能的原因之一。许多**提取、转换和加载**（**ETL**）框架也提供了类似的功能。
- en: Apart from breaking down the ML pipeline, orchestration is another important
    part of ML platforms. Every cloud provider has one, and there are many open source
    offerings as well. Developing pipeline steps that can be orchestrated without
    much work is key. Nowadays, there are a lot of tools for orchestration, and as
    long as the steps are small and meaningful, it should be easy to orchestrate with
    any of the existing frameworks.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分解机器学习流程外，编排也是机器学习平台的重要部分。每个云提供商都有自己的编排工具，同时也有很多开源的提供。开发无需太多工作即可编排的流程步骤是关键。如今，有很多编排工具，只要步骤小且有意义，就应能够与任何现有框架轻松编排。
- en: Tracking data lineage and versioning
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪数据血缘和版本控制
- en: If you recall [*Chapter 6*](B18024_06_ePub.xhtml#_idTextAnchor096), *Model to
    Production and Beyond*, we discussed debugging prediction issues. In that example,
    we discussed generating the same feature set that produced the anomaly in prediction;
    however, many times it won't be enough to figure out what went wrong in the system
    and whether it was caused by code or the dataset. Hence, along with that, being
    able to track the data lineage of that feature set all the way to the data source
    can be very helpful in debugging the issue.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得[*第6章*](B18024_06_ePub.xhtml#_idTextAnchor096)，*模型到生产及之后*，我们讨论了预测问题的调试。在那个例子中，我们讨论了生成导致预测异常的相同特征集；然而，很多时候仅仅找出系统出了什么问题以及是否由代码或数据集引起是不够的。因此，能够追踪该特征集的数据来源直到数据源，对于调试问题非常有帮助。
- en: For every run of the pipeline, saving the input and output of every step with
    the timestamp version is the key here. With this, we can track the anomaly in
    prediction all the way back to its source, which is data. For example, instead
    of just having the features that generated a bad recommendation for a customer
    on a website, it would be better to also have the ability to trace the features
    all the way back to their interactions at the time of the event and the different
    transformation that was generated in the ML pipeline when this event occurred.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于管道的每一次运行，保存每个步骤的输入和输出以及时间戳版本是关键。有了这个，我们可以追踪预测中的异常直到其源头，即数据。例如，除了拥有生成网站客户不良推荐的特性外，更好的是能够追踪这些特性直到它们在事件发生时的交互以及在此事件发生时在机器学习管道中生成的不同转换。
- en: 'The following is some of the pipeline information that can help in better lineage
    tracking:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些有助于更好地跟踪血缘信息的管道信息：
- en: Versions of all the libraries used in the steps
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在步骤中使用到的所有库的版本
- en: Versions of code that was run in the pipeline, including the pipeline version
    itself
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在管道中运行的代码版本，包括管道本身的版本
- en: Input arguments and artifacts produced by every step of the pipeline, such as
    the raw data, the dataset, and models
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道每个步骤产生的输入参数和工件，例如原始数据、数据集和模型
- en: Next, let's look at the feature repository.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看特征库。
- en: The feature repository
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征库
- en: Having a feature repository can be very beneficial for ML development. Though
    there are a few gray areas in respect of updates to feature table schema, the
    benefits of a feature store, such as reusability, browsable features, the readiness
    of online serving, time travel, and point-in-time joins, are very useful in model
    development. As we observed in the previous chapter, the features developed during
    the development of the customer lifetime value model were useful in the Next Purchase
    Day model. Similarly, as the feature repository grows in size, more and more features
    become available for use, and there is less duplication of work for data scientists
    and engineers to do, thereby accelerating the development of the model.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个特征库对于机器学习开发非常有好处。尽管在更新特征表模式方面存在一些灰色区域，但特征存储的好处，如可重用性、可浏览的特征、在线服务的准备就绪、时间旅行和点时间连接，在模型开发中非常有用。正如我们在上一章中观察到的，在客户终身价值模型开发期间开发的特征在下一购买日模型中很有用。同样，随着特征库规模的扩大，越来越多的特征可供使用，数据科学家和工程师的工作重复性减少，从而加速了模型的开发。
- en: 'The following screenshot depicts the cost of developing an ML model versus
    the number of curated features in the feature store:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了开发机器学习模型的成本与特征存储中精心策划的特征数量之间的关系：
- en: '![Figure 7.8 – The average cost of the model versus the number of curated features
    in the feature store'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.8 – 模型的平均成本与特征存储中精心策划的特征数量之间的关系'
- en: '](img/B18024_07_008.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_07_008.jpg]'
- en: Figure 7.8 – The average cost of the model versus the number of curated features
    in the feature store
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 模型的平均成本与特征存储中精心策划的特征数量之间的关系
- en: As shown in *Figure 7.8*, the cost of developing and productionizing the model
    goes down as the feature repository grows. Going by the reuse and add new if not
    available, all the features available in the feature repository are either production-ready
    or serving production models. We will be just adding delta features for each new
    model. This means that the only additional cost on the infrastructure would be
    to run these additional feature engineering transformations and new feature tables,
    and the rest is assumed to auto-scale for the production load if we are using
    a managed feature store. Hence, the cost involved in the development and production
    of the new model should decrease over time and flatten once the feature repository
    is saturated.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图7.8*所示，随着特征仓库的增长，开发和生产模型的成本会降低。按照重用和添加新特征（如果不可用）的原则，特征仓库中所有可用的特征要么是生产就绪的，要么正在为生产模型提供服务。我们只需为每个新模型添加增量特征。这意味着在基础设施上增加的唯一成本就是运行这些额外的特征工程转换和新特征表，其余的假设在如果我们使用托管特征存储的情况下会自动扩展以适应生产负载。因此，新模型开发和生产的成本应该随着时间的推移而降低，并在特征仓库饱和后趋于平稳。
- en: Experiment tracking, model versioning, and the model repository
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验跟踪、模型版本控制和模型仓库。
- en: Experiment tracking and the model repository are other important aspects of
    ML development. When developing a model, we run different experiments – it could
    be different algorithms, different implementations such as TensorFlow versus PyTorch,
    hyperparameter tuning, a different set of features for the model, a different
    training dataset, and also different transformations on the training dataset.
    Keeping track of these experiments is not easy, as some of these experiments can
    go on for days or weeks. Hence, using experiment-tracking software that comes
    out of the box with many of the notebook environments is very important.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 实验跟踪和模型仓库是机器学习开发的其他重要方面。在开发模型时，我们会运行不同的实验——可能是不同的算法，不同的实现，如TensorFlow与PyTorch，超参数调整，为模型选择不同的特征集，不同的训练数据集，以及训练数据集上的不同转换。跟踪这些实验并不容易，因为其中一些实验可能持续数天或数周。因此，使用与许多笔记本环境一起提供的实验跟踪软件非常重要。
- en: 'Every run should log the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 每次运行都应该记录以下内容：
- en: The version of model training notebooks or scripts.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练笔记本或脚本的版本。
- en: Some parameters about the dataset that can be used to reproduce the same training
    and evaluation dataset. If you are using a feature store, then it could be the
    timestamps and entities used; if not, you can also save the training dataset to
    a file and log the location of the dataset.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些关于数据集的参数，可用于重现相同的训练和评估数据集。如果你使用特征存储，那么可能是时间戳和实体；如果没有，你也可以将训练数据集保存到文件中并记录数据集的位置。
- en: All the parameters that are used in the training algorithm.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练算法中使用的所有参数。
- en: The performance metrics of each run.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次运行的性能指标。
- en: Any visualization of the results can also be very useful.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对结果的任何可视化也可能非常有用。
- en: The logged metrics for each run can be used for comparing the performance metrics
    of the models for different runs. These metrics will be critical in making a decision
    on which run of the model is better performing and should be moved to new stages,
    such as stage deployment, and AB testing. In addition, each run also helps you
    browse through the history of the experiments if you or anybody else on the team
    ever need to look back and reproduce some specific run.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 每次运行的记录指标可用于比较不同运行中模型的性能指标。这些指标在决定哪个模型的运行性能更好并应转移到新阶段，如部署阶段和A/B测试阶段时至关重要。此外，每次运行也有助于你浏览实验的历史记录，如果你或团队中的其他人需要回顾并重现某些特定的运行。
- en: Similarly, a model repository can help in keeping track of all the different
    versions of the model. The model registry/repository stores the information required
    to load and run the model – for instance, an MLflow model repository stores information
    such as the conda environment, the model's `pickle` file, and any other additional
    *dependencies* of the model. If you have a central repository of the model, it
    can be useful for consumers to browse and search, and also for the life cycle
    management of models, such as moving models to different stages – development,
    staging, production, and archived. Model repositories can also be used for scanning
    any vulnerabilities in code and any packages used in the model. Hence, the model
    repository plays a key role in ML development.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，模型仓库可以帮助跟踪所有不同版本的模型。模型注册/仓库存储了加载和运行模型所需的信息——例如，MLflow模型仓库存储有关conda环境、模型的`pickle`文件以及模型的其他任何附加*依赖项*的信息。如果您有一个模型的中央仓库，这对于消费者浏览和搜索以及模型的生命周期管理（如将模型移动到不同的阶段——开发、预发布、生产以及存档）非常有用。模型仓库还可以用于扫描代码中的任何漏洞以及模型中使用的任何包。因此，模型仓库在机器学习开发中发挥着关键作用。
- en: Feature and model monitoring
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征和模型监控
- en: 'As we discussed in the previous chapter, feature monitoring is another important
    aspect. An important counterpart of the feature repository is monitoring for changes
    and anomalies. The feature monitoring rules will be similar to that of data monitoring.
    Some of the useful rules of features are feature freshness, minimum and maximum
    rules, monitoring for outliers, descriptive statistics of the latest features,
    and metrics such as KL divergence and PSI. The Hopsworks monitoring rules should
    be a good starting point for the list of rules that you may have on features.
    Here is a link to the documentation: [https://docs.hopsworks.ai/feature-store-api/2.5.8/generated/feature_validation/](https://docs.hopsworks.ai/feature-store-api/2.5.8/generated/feature_validation/).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中讨论的，特征监控是另一个重要的方面。特征仓库的一个重要对应物是对变化和异常的监控。特征监控规则将与数据监控的规则相似。一些有用的特征规则包括特征新鲜度、最小和最大规则、监控异常值、最新特征的描述性统计以及如KL散度和PSI等指标。Hopsworks的监控规则应该是您可能拥有的特征规则列表的一个良好起点。以下是文档链接：[https://docs.hopsworks.ai/feature-store-api/2.5.8/generated/feature_validation/](https://docs.hopsworks.ai/feature-store-api/2.5.8/generated/feature_validation/)。
- en: Model monitoring is another important aspect. After moving a model to production,
    it tends to decay in performance over time. This happens as user behaviors change;
    hence the data profiles. It is important to keep track of how the model is performing
    in production. These performance reports should be generated on schedule, if not
    in real time, and appropriate actions must be taken, such as model retraining
    with the new data or starting a new iteration altogether.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控是另一个重要的方面。在将模型部署到生产环境中后，其性能往往会随着时间的推移而下降。这是因为用户行为发生变化；因此，数据特征也会随之变化。跟踪模型在生产中的表现非常重要。这些性能报告应该按计划生成，如果不是实时生成，还必须采取适当的行动，例如使用新数据重新训练模型或启动全新的迭代。
- en: Miscellaneous
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他事项
- en: A few other things to keep in mind during ML development include keeping track
    of runtime environments, library upgrades, and depreciations. It is better to
    proactively act on these. For instance, if you use tools that are strictly tied
    to a specific environment, such as a Python or Spark version, once a specific
    runtime is deprecated and removed from production support, the jobs might start
    failing and the production system may be hampered. Another example could be that
    Databricks has runtimes that are tied to specific Python and Spark versions. If
    you are running jobs on a deprecated version, once it goes out of support, the
    jobs might start failing if there are breaking changes in the new version. Hence,
    it is better to upgrade proactively.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习开发过程中需要注意的其他事项包括跟踪运行时环境、库升级和降级。最好是主动采取行动。例如，如果您使用与特定环境严格绑定的工具，如Python或Spark版本，一旦特定的运行时被弃用并从生产支持中移除，作业可能会开始失败，生产系统可能会受到影响。另一个例子是Databricks的运行时与特定的Python和Spark版本绑定。如果您在已弃用的版本上运行作业，一旦它停止支持，如果新版本中有破坏性更改，作业可能会开始失败。因此，最好是主动升级。
- en: With that, let's summarize what we have learned in this chapter before looking
    at an end-to-end use case in the next chapter.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，让我们总结一下本章所学的内容，然后再查看下一章中的端到端用例。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we took a look at some of the available feature stores on the
    market. We discussed five of them, namely Tecton, Databricks, Vertex AI, Hopsworks,
    and SageMaker Feature Store. We also did a deep dive into SageMaker Feature Store
    to get a feel of using a managed feature store instead of Feast and how it differs
    when it comes to resource creation, feature ingestion, and querying. In the last
    section, we briefly discussed a set of best practices for ML development.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们审视了市场上一些可用的特征存储。我们讨论了其中的五个，分别是Tecton、Databricks、Vertex AI、Hopsworks和SageMaker
    Feature Store。我们还深入探讨了SageMaker Feature Store，以了解使用托管特征存储而非Feast的感觉，以及它在资源创建、特征摄取和查询方面的差异。在最后一节，我们简要讨论了机器学习开发的一些最佳实践。
- en: In the next chapter, we'll go through an end-to-end use case on a managed ML
    platform.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过一个端到端的用例来介绍一个托管机器学习平台。
