- en: 'Chapter 7: Feast Alternatives and ML Best Practices'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we discussed how to use Amazon Managed Workflows with Apache
    Airflow for orchestration and productionizing online and batch models with **Feast**.
    So far in this book, we have been discussing one feature store – Feast. However,
    there are a bunch of feature stores available on the market today. In this chapter,
    we will look at a few of them and discuss how they are different from Feast and
    the advantages or disadvantages of using them over Feast.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will try out one other feature store, specifically Amazon
    SageMaker. We will take the same feature set that we generated while building
    the customer **lifetime value (LTV)** model and ingest it into SageMaker Feature
    Store and also run a couple of queries. The reason for choosing AWS over other
    feature stores such as Tecton, Hopworks, and H2O.ai is the easy access to the
    trial version. However, choosing the right feature store for you depends on the
    tools and infrastructure that you already have and more, which we will discuss
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this chapter is to give you the gist of what is available on the
    market and how it differs from self-managed feature stores such as Feast. We will
    also discuss the similarities and differences between these feature stores. The
    other aspect that I want to discuss in this chapter is the best practices in ML
    development. Irrespective of the tools/software we use for ML development, there
    are a few things that can be universally adopted by all of us to improve ML engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The available feature stores on the market
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature management with SageMaker Feature Store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run through the examples and get a better understanding of this chapter,
    the topics covered in previous chapters will be useful but not required. To follow
    the code examples in the chapter, you need familiarity with a notebook environment,
    which could be a local setup such as Jupyter or an online notebook environment
    such as Google Colab, Kaggle, or SageMaker. You will also need an AWS account
    with full access to SageMaker and the AWS Glue console. You can create a new account
    and use all the services for free during the trial period. You can find the code
    examples of the book using the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter07](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter07)'
  prefs: []
  type: TYPE_NORMAL
- en: The available feature stores on the market
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will briefly discuss some of the available feature stores
    on the market and how they compare with Feast, as well as some commonalities and
    differences between these feature stores.
  prefs: []
  type: TYPE_NORMAL
- en: The Tecton Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Tecton is an enterprise feature store, built by the creators of Uber''s machine
    learning platform Michelangelo* (https://eng.uber.com/michelangelo-machine-learning-platform/).
    Tecton is also one of the major contributors to Feast. Hence, when you look at
    Tecton''s documentation ([https://docs.tecton.ai/index.html](https://docs.tecton.ai/index.html)),
    you will see a lot of similarities in the APIs and terminology. However, there
    are a lot of functionalities in Tecton that don''t exist in Feast. Also, Tecton
    is a managed feature store, which means that you don''t need to build and manage
    the infrastructure; it will be managed for you.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with most feature stores, Tecton uses online and offline stores for low
    latency and historical storage respectively. However, there are fewer options
    for online and offline stores compared to Feast, and it is currently supported
    only on AWS. If you prefer Azure or GCP, you don''t have any other option but
    to wait for now. I believe multiple cloud providers and data stores will be eventually
    supported. Tecton uses a **Software as a Service** (**SaaS**) deployment model
    and separates deployment into data and control planes. You can find their deployment
    model at the following link: [https://docs.tecton.ai/setting-up-tecton/07a-deployment_saas.html](https://docs.tecton.ai/setting-up-tecton/07a-deployment_saas.html).
    The best part is that data never leaves the customer''s AWS account, and only
    the metadata required for the control panel to work is accessed by the Tecton-owned
    AWS account; also, the UI will be hosted in their account. However, if you want
    to expose online data through a REST/gRPC API endpoint, the service will be hosted
    in Tecton''s AWS account. The online feature request and response will be routed
    through their account.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once Tecton is deployed into your AWS account, you can interact with it using
    the Python SDK. The CLI commands are similar to Feast commands; however, there
    are options such as being able to manage versions of your feature definitions
    and downgrading to a previous version of the definitions. As well as the common
    workflows that you can do with a feature store such as ingesting, querying at
    low latency, and performing point-in-time joins, with Tecton, you can define transformation
    as part of the feature store. This is one of my favorite features of Tecton. Here
    is the link to the feature views and transformation page in the feature store:
    [https://docs.tecton.ai/overviews/framework/feature_views/feature_views.html](https://docs.tecton.ai/overviews/framework/feature_views/feature_views.html).
    What this means is that you can define a raw data source configuration for a data
    warehouse (Snowflake), database, Kinesis, or Kafka, and define a PySpark, Spark
    SQL, or pandas transformation to generate features. Tecton orchestrates these
    jobs on a defined schedule and generates features, and ingests them into online
    and offline stores. This can help in tracking data lineage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example code snippet on how to define feature views and
    transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You may recognize some of the parameters that you see in the preceding code
    block. Here, the annotation says it's a batch transformation on which you can
    define parameters such as which entities to use, what the schedule is, and whether
    it should ingest data into online and offline stores. In the method definition,
    input data will be injected based on whatever is assigned to the `input` parameter
    in the annotation definition (you can assume it to be a DataFrame from a raw data
    source). On the DataFrame, you add your transformation and return the output DataFrame,
    which will be features. These features will be ingested into the online and offline
    stores on the defined schedule. Once you define the preceding transformation,
    you will have to run `tecton apply`, which is similar to the `feast apply` command,
    to register this transformation. The other functionalities are similar to what
    other feature stores offer; hence, I will skip over them and let you explore their
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: What is worth keeping in mind though is that the Tecton deployments are single-tenant
    at the time of writing, which means that if there are teams that cannot share
    data, you might need multiple deployments. There is a set of roles that needs
    to be created that will allow Tecton to install and create required resources
    using cross-account roles, which involves a one-time initial setup from you.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Databricks Feature Store is another option that is available out there for users.
    It makes sense if you are already using Databricks as your notebook environment
    and for data processing jobs. It comes with the Databricks workspaces, so you
    can't have just the feature store. However, you can get a workspace and not use
    anything else except the feature store. It can be hosted on AWS, GCP, or Azure.
    So, if you are on any of the major cloud providers, this could be an option.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concepts are similar to other feature stores, such as feature tables, timestamp
    versioning on the rows, the ability to do point-in-time joins, and online and
    offline stores. It uses a delta lake for its offline store and uses one of the
    key-value stores, available on a cloud based on which cloud provider you are on.
    The best part about Databricks Feature Store is that it integrates well with all
    the other aspects and components of Databricks, such as Spark DataFrame ingestion,
    retrieval, out-of-the-box integration with the MLflow model repository, access
    control, and tracking the lineage of notebooks that are used to generate a particular
    feature table. It also has a nice UI where you can browse and search for features.
    The next best part is there is no setup required if you already have the Databricks
    workspace. Here is a link to the notebook, which features examples of feature
    creation, ingestion, retrieval, training, and model scoring: [https://docs.databricks.com/_static/notebooks/machine-learning/feature-store-taxi-example.html](https://docs.databricks.com/_static/notebooks/machine-learning/feature-store-taxi-example.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are a few things to keep in mind. Databricks Feature Store doesn''t
    have a concept of projects; hence, feature tables are the highest level of abstraction,
    and access control is at the feature table level. Additionally, Databricks'' online
    model hosting is still in public preview (although no doubt it will eventually
    become a standard offering). This means that if you use Databricks Feature Store
    for an online model that is hosted outside of Databricks, it might have to connect
    to the online store using the direct client. For example, if you use DynamoDB
    as an online store (Databricks offers multiple choices, depending on the cloud
    provider) and host the model in Amazon `boto3` client for features during prediction.
    Also, sharing features across the workspace might need additional configuration
    for either access tokens or using a central workspace for the feature store. Here
    is the link to the Databricks Feature Store documentation for more details: [https://docs.databricks.com/applications/machine-learning/feature-store/index.html](https://docs.databricks.com/applications/machine-learning/feature-store/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Google's Vertex AI Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google's Vertex AI is a **Platform as a Service** (**PaaS**) offering from Google
    for ML and AI. Vertex AI aims at offering an end-to-end ML platform that provides
    a set of tools for ML development, training, orchestration, model deployment,
    monitoring, and more. The tool that we are most interested in is Vertex AI Feature
    Store. If you are already using GCP for your services, it should be an automatic
    pick.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concepts and terminology are very similar to that of Feast. The highest
    level of abstraction in Vertex AI is called a *featurestore*, similar to a *project*
    in Feast, and a *featurestore* can have *entities*, and *features* should belong
    to *entities*. It supports online and batch serving, just like all the other feature
    stores. However, unlike Feast and Tecton, there are no options available for online
    and historical stores. Since it is a managed infrastructure, users don''t need
    to worry about installation and choosing online and offline stores – probably
    just the pricing. Here is a link to its prices: [https://cloud.google.com/vertex-ai/pricing#featurestore](https://cloud.google.com/vertex-ai/pricing#featurestore).
    It uses **IAM** (short for **Identity and Access Management**) for authentication
    and authorization, and you also get a UI to search and browse features.'
  prefs: []
  type: TYPE_NORMAL
- en: The best part of Vertex AI is its integration with other components of GCP and
    the Vertex AI service itself for feature generation, pipeline management, and
    data lineage tracking. One of my favorite features is drift monitoring. You can
    set up a feature monitoring configuration on the feature tables, which can generate
    data distribution reports for you without requiring any additional work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, there are a few things to keep in mind. For online serving, you need
    to do capacity sizing and set up the number of nodes required to handle your traffic.
    The autoscaling option for online serving is still in public preview (although
    it''s just a matter of time before it becomes a standard offering), but capacity
    planning should be a major problem to solve. A few load test simulations should
    help you figure that out easily. Also, there are quotas and limits on the number
    of online serving nodes you can have for a feature store, the length of data retention,
    and the number of features per entity. Some of these can be increased on request
    whereas others can''t. Here is a link to the list of quotas and limits on a feature
    store: https://cloud.google.com/vertex-ai/docs/quotas#featurestore.'
  prefs: []
  type: TYPE_NORMAL
- en: The Hopsworks Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hopsworks is another open source feature store under the AGPL-V3 license that
    can be run on-premises, on AWS or Azure. It also has an enterprise version of
    the feature store that supports GCP as well as any Kubernetes environment. Similar
    to other ML platform services, it also offers multiple components, such as model
    management and compute environment management.
  prefs: []
  type: TYPE_NORMAL
- en: The concepts are similar to that of other feature stores; however, the terminology
    is different. It doesn't have a concept of entities, and *featuregroups* in *Hopsworks*
    are analogous to *featureviews* in *Feast*. Just like other feature stores, Hopsworks
    supports online and offline serving. It uses Apache Hive with Apache Hudi as an
    offline store and MySQL Cluster as an online store. Again, there are no options
    for online or offline stores. However, there are different storage connectors
    developed by Hopsworks that can be used to create on-demand external feature groups,
    such as *RedShiftSource*, which we defined in *Feast* in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*. But there is a limitation on external feature
    groups, meaning there is no time travel, online serving, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of features in the Hopsworks Feature Store that are fancy and
    very interesting. Some of the best ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Project-level multi-tenancy**: Each project has an owner and can share resources
    with other members in the team and across teams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature group versioning**: Hopsworks supports feature group versioning,
    which is not currently supported by any other feature stores on the market.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistics on feature groups**: It provides a few out-of-the-box statistics
    on feature groups, such as feature co-relation computation, a frequency histogram
    on features, and uniqueness. The following is an example feature group:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Feature validation**: This is another fancy feature that is available out
    of the box. This is a set of predefined validation rules that exist on feature
    groups such as the minimum and maximum values of a feature, a uniqueness count
    of features, the entropy of a feature, and the maximum length of features. It
    has enough rule types that you won''t have a use case where you need to customize
    a validation rule. The following are a couple of example rules:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Transformation functions**: Similar to Tecton transformations for feature
    views, in Hopsworks, you can define or use built-in transformation on a training
    dataset (Hopsworks has a concept of training data where you can pick features
    from different feature groups and create a training dataset definition on top
    of them– a concept similar to database views).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are some things to keep in mind though. If you choose the open source
    version, you may not have several features, and infrastructure will have to be
    self-managed. Conversely, for the enterprise version, you will have to collaborate
    with a Hopsworks engineer and create a few resources and roles required for the
    installation of Hopsworks on the cloud provider. Here is a link to all the documentation:
    https://docs.hopsworks.ai/feature-store-api/2.5.8/. I recommend having a look
    at the features even if you don''t use them; this might give an idea of some of
    the features you might want to build or have in your feature store.'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker is an end-to-end ML platform offered by AWS. Just like Vertex AI,
    it has a notebook environment, AutoML, processing jobs and model management, a
    feature store, and so on. If you are an AWS-focused company, this must be a natural
    pick over the others.
  prefs: []
  type: TYPE_NORMAL
- en: The concepts are close to that of other feature stores, although some of the
    terms are different. For example, SageMaker Feature Store also doesn't have the
    concept of entities, and *featureviews* in Feast are analogous to *featuregroups*
    in SageMaker. It has all the basic features, such as online and offline stores
    and serving. However, you don't have options to pick from. It uses S3 as an offline
    store and one of the key-value stores as an online store (AWS doesn't say what
    is used for an online store in its documentation). AWS uses IAM for authentication
    and authorization. To access feature store currently, you need full access to
    SageMaker and the AWS Glue console. If you compare SageMaker to Feast, both use/support
    S3 as an offline store, a key-value store as an online store, and a Glue catalog
    for managing the schema. Apart from SageMaker being a managed feature store, another
    difference is that Feast uses Redshift for querying offline data, whereas SageMaker
    uses Amazon Athena (serverless) for querying. You can add this functionality to
    Feast if you are a fan of serverless technologies.
  prefs: []
  type: TYPE_NORMAL
- en: One of my favorite things about SageMaker Feature Store is that there is no
    infrastructure management. Apart from creating an IAM role to access the feature
    store, you don't need to manage anything. All the resources for any given load
    are managed by AWS. All you need to worry about is just developing and ingesting
    features. SageMaker Feature Store also supports ingestion using Spark on EMR or
    Glue jobs (serverless). Along with the features, it also adds metadata, such as
    `write_time` and `api_invocation_time`, that can be used in queries. The best
    part is that you can query offline data using Amazon Athena SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few things to keep in mind though. The current implementation doesn't
    yet have granular access management. Right now, you need full access to SageMaker
    to use Feature Store, although I believe that it's only a matter of time before
    AWS starts offering granular access. Point-in-time joins are not available out
    of the box; however, these can be achieved using SQL queries or Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have looked at a few of the available options on the market; you
    can find other feature stores that are available at this link: [https://www.featurestore.org/](https://www.featurestore.org/).
    However, picking the right feature store for your project or team can be tricky.
    The following are a few things to keep in mind while picking a feature store:'
  prefs: []
  type: TYPE_NORMAL
- en: Your primary cloud provider makes a huge difference. If you are GCP-focused,
    it doesn't make sense to use SageMaker Feature Store and vice versa. If you are
    multi-cloud, then you will have more options.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data processing framework is also another key factor that decides what feature
    store to use. For example, if you use SageMaker as your ML platform, trying out
    SageMaker Feature Store before others makes more sense.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with other components in your ecosystem is also key – for instance,
    answering questions such as how well it integrates with your processing platform,
    the orchestration framework, the model management service, data validation frameworks,
    and your ML development process can really help in picking the right feature store.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The required functionalities and your team structure make a big difference.
    If you are a small team who wants to just concentrate on ML, then a managed offering
    of a feature store makes sense, whereas if you have a platform team to manage
    the infrastructure, you may look into open source offerings and also evaluate
    the build versus buy options. If you have a platform team, they might look for
    additional features such as multi-tenancy, granular access control, and SaaS/PaaS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, a lot of factors influence the choice of a feature store other
    than the functionalities it offers, as it must integrate well with a broader ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's look at how a managed feature store works.
  prefs: []
  type: TYPE_NORMAL
- en: Feature management with SageMaker Feature Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look into what action we might have to take if we were
    to use a managed feature store instead of Feast in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: All managed feature stores have a similar workflow; some may be API-based and
    some work through a CLI. But irrespective of this, the amount of work involved
    in using the feature store would be similar to what we will discuss in this section.
    The only reason I am going through SageMaker is familiarity and ease of access
    to it, using the free trial as a featured product in AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Resources to use SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065), *Adding Feature Store
    to ML Models*, before we started using the feature store, we created a bunch of
    resources on AWS, such as an S3 bucket, a Redshift cluster, an IAM role, and a
    Glue catalog table. Conversely, for a managed feature store such as SageMaker,
    all you need to have is an IAM role that has full access to SageMaker and you
    are all set. Let's try that out now.
  prefs: []
  type: TYPE_NORMAL
- en: We need some IAM user credentials and an IAM role that SageMaker Feature Store
    can assume. Creating an IAM user is similar to what we have done before. Follow
    the same steps and create an IAM user, and assign **AmazonS3FullAccess** and **AmazonSageMakerFullAccess**
    permissions. IAM role creation is the same as we have done before; however, we
    need to allow the SageMaker service to assume the role.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned many times before, it is never a good idea to assign full access;
    permissions should always be restrictive based on resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create an IAM role:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to your AWS account and navigate to the IAM role page, using the search
    bar; alternatively, visit the following URL: https://us-east-1.console.aws.amazon.com/iamv2/home#/roles.
    The following page will be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.1 – The IAM role home page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_07_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – The IAM role home page
  prefs: []
  type: TYPE_NORMAL
- en: 'On the displayed web page, click on **Create role** to navigate to the following
    screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.2 – The IAM role creation page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_07_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – The IAM role creation page
  prefs: []
  type: TYPE_NORMAL
- en: 'On the screen displayed in *Figure 7.2*, in the **Use cases for other AWS services**
    dropdown, select **SageMaker** and then click the **SageMaker - Execution** radio
    button. Scroll down and click on **Next**, leaving everything as default on the
    **Add Permissions** page, and then click on **Next**. The following page will
    be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.3 – The Name, review and create page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_07_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – The Name, review and create page
  prefs: []
  type: TYPE_NORMAL
- en: On the displayed web page, fill in the `sagemaker-iam-role`. Scroll all the
    way down and click on `arn:aws:iam::<account_number>:role/sagemaker-iam-role`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's all we need to access SageMaker Feature Store. Let's create the feature
    definitions next.
  prefs: []
  type: TYPE_NORMAL
- en: Generating features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To define the feature group, since we are trying to compare how it differs
    from Feast, we will take the same feature set. You can download the previously
    ingested features from an S3 bucket or download it from the GitHub link: [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter07/rfm_features.parquet](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter07/rfm_features.parquet).
    After downloading the Parquet file, copy it to a location that can be accessed
    from the notebook. The next step is to create a new notebook, which I am calling
    `ch7-sagemaker-feature-store.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s install the required libraries first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After installing the libraries, let''s generate the features. Here, we will
    be just reading the copied file from the location and making minor modifications
    to the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code block reads the file and drops the `created_timestamp` column,
    as it is not required by SageMaker. We are also updating the `event_timestamp`
    column to the latest time and changing the type to `float` instead of `datetime`.
    The reason for this is that SageMaker only supports the `int`, `float`, and `string`
    features at the time of writing, and `datetime` files can either be a `float`
    or `string` object in the `datetime` ISO format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code block produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Recency, Frequency, and Monetary value (RFM) features'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_07_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – Recency, Frequency, and Monetary value (RFM) features
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have RFM features, the next step is to define the feature group.
    If you recall correctly from [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*, after generating the features, we created
    the feature definitions and applied them to the feature store.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the feature group
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To define the feature group, since it''s a one-time activity, it should be
    done in a separate notebook rather than by feature engineering. For this exercise,
    let''s continue in the same notebook and define the feature group:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block defines a few imports and creates the SageMaker session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the code block, replace `<aws_key_id>` and `<aws_secret_id>` with the key
    and secret of the IAM user created earlier. Also, assign `role` with your IAM
    role ARN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block creates the feature group object and loads the feature
    definitions from the input DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code block produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – The load feature definitions call'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_07_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – The load feature definitions call
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 7.5*, the `load_feature_definitions` call reads the
    input DataFrame and loads the feature definition automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create the feature group. The following code block creates
    the feature group in SageMaker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code block invokes the create API by passing the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`s3_uri`: The location where the feature data will be stored'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`record_identifier_name`: The name of the `id` column (the same as the entity
    column in Feast)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`event_time_feature_name`: The timestamp column that will be used for time
    travel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`role_arn`: The role that SageMaker Feature Store can assume'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_online_store`: Whether to enable online serving or not for this feature
    group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code block produces the following output on the successful creation of
    the feature group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Feature group creation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_07_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – Feature group creation
  prefs: []
  type: TYPE_NORMAL
- en: That's all – our feature group is ready to use. Let's ingest the features next.
  prefs: []
  type: TYPE_NORMAL
- en: Feature ingestion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature ingestion in SageMaker Feature Store is simple. It is a simple API
    call, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block will ingest the features and print the failed row numbers
    if there are any.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind here is that, like Feast, you don't need to do anything
    extra to materialize the latest features from an offline to an online store. If
    the online store is enabled, the data will be ingested to both online and offline
    stores, and the latest data will be available in the online store for querying
    right away.
  prefs: []
  type: TYPE_NORMAL
- en: Let's query the online store next.
  prefs: []
  type: TYPE_NORMAL
- en: Getting records from an online store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like Feast, querying from an online store is simple. All you need is the record
    ID and the feature group name. The following code block gets the record from the
    online store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block gets all the features for the customer with the `12747.0`
    ID from the online store. The query should return the results within milliseconds.
    The output will be similar to the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the output contains all the features and corresponding values.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at querying an online store, let's check out how to
    generate the training dataset and query historical data next.
  prefs: []
  type: TYPE_NORMAL
- en: Querying historical data with Amazon Athena
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned previously, SageMaker Feature Store offers the ability to run SQL
    queries on a historical store using Amazon Athena.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block generates the latest snapshot of all customers and
    their features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The code block uses a nested SQL query, where the inner query gets all customers
    and their features, in descending order, from the `event_time`, `Api_Invocation_Time`,
    and `write_time` columns. The outer query selects the first occurrence of every
    customer from the results of the inner query. On successful execution of the query,
    the code block outputs the location of the query results along with additional
    details.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results can be loaded as a DataFrame, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Athena query results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_07_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – Athena query results
  prefs: []
  type: TYPE_NORMAL
- en: 'Feel free to try out other Athena queries on Feature Store. Here is the documentation
    of the Amazon Athena query: [https://docs.aws.amazon.com/athena/latest/ug/what-is.html](https://docs.aws.amazon.com/athena/latest/ug/what-is.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up a SageMaker feature group
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s clean up the SageMaker resources to save costs before we move forward.
    The cleanup is pretty easy; it is just another API call to delete the feature
    group. The following code block performs that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s all. After successful execution, it deletes the feature group but leaves
    behind the data in S3 and the Glue catalog, which can still be queried with Amazon
    Athena (using the `boto3` client) if required. Just to make sure everything is
    cleaned up, run the following code block in the same notebook. It should return
    an empty list of feature groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have looked at the SageMaker feature group, let's look into ML best
    practices next.
  prefs: []
  type: TYPE_NORMAL
- en: ML best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in the book, we have discussed feature stores, how to use them for ML
    development and production, and what the available options are when choosing a
    feature store. Though a feature store is one of the major components/aspects of
    ML, there are other aspects of ML that we haven't concentrated on much in this
    book. In this section, let's briefly talk through some of the other aspects and
    best practices in ML.
  prefs: []
  type: TYPE_NORMAL
- en: Data validation at source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Irrespective of the technologies, algorithms, and infrastructure we use for
    building an ML model, if there are errors and anomalies in data, model performance
    will be severely impacted. Data should be treated as a first-class citizen of
    any ML system. Hence, it is very important to detect errors and anomalies in the
    data before it enters the ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: To run validation on raw data sources, we need a component to create and orchestrate
    the validation rules against the data. Users of the data should be able to write
    any custom rules in SQL queries, Python scripts, or Spark SQL. Any failures in
    the rule should be notified to the data consumers who, in turn, should be able
    to make a decision on whether to stop the pipeline execution, retrain the model,
    or take no action.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the common rules include descriptive analytics of the dataset on schedule,
    which can provide insights into data drift. More advanced statistics such as **Kullback–Leibler**
    (**KL**) divergence and the **Population Stability Index** (**PSI**) are good
    to have. Having simple data validation rules such as data freshness, unique values,
    string field length, patterns, and value range thresholds can be very beneficial.
    Schema validation is another important aspect of data validation. Any changes
    in the validation can affect all the consumers and pipelines. The better data
    validation we have at source, the healthier and more performant our models and
    pipeline will be.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down ML pipeline and orchestration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One bad practice is to develop everything in a single notebook, from data validation
    and feature engineering to model prediction. This is not a scalable or reusable
    approach. Most of the time is spent cleaning up unwanted code and productionizing
    the model. Hence, it is always a good idea to break down the ML pipeline into
    multiple smaller steps, such as data validation, cleaning, transformation, feature
    engineering, model training, and model prediction. The smaller the transformation
    steps, the more readable, reusable, and easy it will be to debug code for errors.
    This is one of the reasons that Feature Views and transformation in Tecton, and
    storage connectors and transformation functions in Hopsworks are great features.
    Similar features are also offered by many **Extract, Transform and Load** (**ETL**)
    frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from breaking down the ML pipeline, orchestration is another important
    part of ML platforms. Every cloud provider has one, and there are many open source
    offerings as well. Developing pipeline steps that can be orchestrated without
    much work is key. Nowadays, there are a lot of tools for orchestration, and as
    long as the steps are small and meaningful, it should be easy to orchestrate with
    any of the existing frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking data lineage and versioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you recall [*Chapter 6*](B18024_06_ePub.xhtml#_idTextAnchor096), *Model to
    Production and Beyond*, we discussed debugging prediction issues. In that example,
    we discussed generating the same feature set that produced the anomaly in prediction;
    however, many times it won't be enough to figure out what went wrong in the system
    and whether it was caused by code or the dataset. Hence, along with that, being
    able to track the data lineage of that feature set all the way to the data source
    can be very helpful in debugging the issue.
  prefs: []
  type: TYPE_NORMAL
- en: For every run of the pipeline, saving the input and output of every step with
    the timestamp version is the key here. With this, we can track the anomaly in
    prediction all the way back to its source, which is data. For example, instead
    of just having the features that generated a bad recommendation for a customer
    on a website, it would be better to also have the ability to trace the features
    all the way back to their interactions at the time of the event and the different
    transformation that was generated in the ML pipeline when this event occurred.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is some of the pipeline information that can help in better lineage
    tracking:'
  prefs: []
  type: TYPE_NORMAL
- en: Versions of all the libraries used in the steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Versions of code that was run in the pipeline, including the pipeline version
    itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input arguments and artifacts produced by every step of the pipeline, such as
    the raw data, the dataset, and models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let's look at the feature repository.
  prefs: []
  type: TYPE_NORMAL
- en: The feature repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a feature repository can be very beneficial for ML development. Though
    there are a few gray areas in respect of updates to feature table schema, the
    benefits of a feature store, such as reusability, browsable features, the readiness
    of online serving, time travel, and point-in-time joins, are very useful in model
    development. As we observed in the previous chapter, the features developed during
    the development of the customer lifetime value model were useful in the Next Purchase
    Day model. Similarly, as the feature repository grows in size, more and more features
    become available for use, and there is less duplication of work for data scientists
    and engineers to do, thereby accelerating the development of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot depicts the cost of developing an ML model versus
    the number of curated features in the feature store:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – The average cost of the model versus the number of curated features
    in the feature store'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_07_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 – The average cost of the model versus the number of curated features
    in the feature store
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 7.8*, the cost of developing and productionizing the model
    goes down as the feature repository grows. Going by the reuse and add new if not
    available, all the features available in the feature repository are either production-ready
    or serving production models. We will be just adding delta features for each new
    model. This means that the only additional cost on the infrastructure would be
    to run these additional feature engineering transformations and new feature tables,
    and the rest is assumed to auto-scale for the production load if we are using
    a managed feature store. Hence, the cost involved in the development and production
    of the new model should decrease over time and flatten once the feature repository
    is saturated.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment tracking, model versioning, and the model repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experiment tracking and the model repository are other important aspects of
    ML development. When developing a model, we run different experiments – it could
    be different algorithms, different implementations such as TensorFlow versus PyTorch,
    hyperparameter tuning, a different set of features for the model, a different
    training dataset, and also different transformations on the training dataset.
    Keeping track of these experiments is not easy, as some of these experiments can
    go on for days or weeks. Hence, using experiment-tracking software that comes
    out of the box with many of the notebook environments is very important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every run should log the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The version of model training notebooks or scripts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some parameters about the dataset that can be used to reproduce the same training
    and evaluation dataset. If you are using a feature store, then it could be the
    timestamps and entities used; if not, you can also save the training dataset to
    a file and log the location of the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the parameters that are used in the training algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance metrics of each run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any visualization of the results can also be very useful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The logged metrics for each run can be used for comparing the performance metrics
    of the models for different runs. These metrics will be critical in making a decision
    on which run of the model is better performing and should be moved to new stages,
    such as stage deployment, and AB testing. In addition, each run also helps you
    browse through the history of the experiments if you or anybody else on the team
    ever need to look back and reproduce some specific run.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a model repository can help in keeping track of all the different
    versions of the model. The model registry/repository stores the information required
    to load and run the model – for instance, an MLflow model repository stores information
    such as the conda environment, the model's `pickle` file, and any other additional
    *dependencies* of the model. If you have a central repository of the model, it
    can be useful for consumers to browse and search, and also for the life cycle
    management of models, such as moving models to different stages – development,
    staging, production, and archived. Model repositories can also be used for scanning
    any vulnerabilities in code and any packages used in the model. Hence, the model
    repository plays a key role in ML development.
  prefs: []
  type: TYPE_NORMAL
- en: Feature and model monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in the previous chapter, feature monitoring is another important
    aspect. An important counterpart of the feature repository is monitoring for changes
    and anomalies. The feature monitoring rules will be similar to that of data monitoring.
    Some of the useful rules of features are feature freshness, minimum and maximum
    rules, monitoring for outliers, descriptive statistics of the latest features,
    and metrics such as KL divergence and PSI. The Hopsworks monitoring rules should
    be a good starting point for the list of rules that you may have on features.
    Here is a link to the documentation: [https://docs.hopsworks.ai/feature-store-api/2.5.8/generated/feature_validation/](https://docs.hopsworks.ai/feature-store-api/2.5.8/generated/feature_validation/).'
  prefs: []
  type: TYPE_NORMAL
- en: Model monitoring is another important aspect. After moving a model to production,
    it tends to decay in performance over time. This happens as user behaviors change;
    hence the data profiles. It is important to keep track of how the model is performing
    in production. These performance reports should be generated on schedule, if not
    in real time, and appropriate actions must be taken, such as model retraining
    with the new data or starting a new iteration altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Miscellaneous
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A few other things to keep in mind during ML development include keeping track
    of runtime environments, library upgrades, and depreciations. It is better to
    proactively act on these. For instance, if you use tools that are strictly tied
    to a specific environment, such as a Python or Spark version, once a specific
    runtime is deprecated and removed from production support, the jobs might start
    failing and the production system may be hampered. Another example could be that
    Databricks has runtimes that are tied to specific Python and Spark versions. If
    you are running jobs on a deprecated version, once it goes out of support, the
    jobs might start failing if there are breaking changes in the new version. Hence,
    it is better to upgrade proactively.
  prefs: []
  type: TYPE_NORMAL
- en: With that, let's summarize what we have learned in this chapter before looking
    at an end-to-end use case in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a look at some of the available feature stores on the
    market. We discussed five of them, namely Tecton, Databricks, Vertex AI, Hopsworks,
    and SageMaker Feature Store. We also did a deep dive into SageMaker Feature Store
    to get a feel of using a managed feature store instead of Feast and how it differs
    when it comes to resource creation, feature ingestion, and querying. In the last
    section, we briefly discussed a set of best practices for ML development.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll go through an end-to-end use case on a managed ML
    platform.
  prefs: []
  type: TYPE_NORMAL
