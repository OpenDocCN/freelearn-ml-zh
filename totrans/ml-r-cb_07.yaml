- en: Chapter 7. Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Estimating model performance with k-fold cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing cross-validation with the e1071 package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing cross-validation with the caret package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranking the variable importance with the caret package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranking the variable importance with the rminer package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding highly correlated features with the caret package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting features using the caret package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the performance of a regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the prediction performance with the confusion matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the prediction performance using ROCR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing an ROC curve using the caret package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring performance differences between models with the caret package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model evaluation is performed to ensure that a fitted model can accurately predict
    responses for future or unknown subjects. Without model evaluation, we might train
    models that over-fit in the training data. To prevent overfitting, we can employ
    packages, such as `caret`, `rminer`, and `rocr` to evaluate the performance of
    the fitted model. Furthermore, model evaluation can help select the optimum model,
    which is more robust and can accurately predict responses for future subjects.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will discuss how one can implement a simple R script
    or use one of the packages (for example, `caret` or `rminer`) to evaluate the
    performance of a fitted model.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating model performance with k-fold cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-fold cross-validation technique is a common technique used to estimate
    the performance of a classifier as it overcomes the problem of over-fitting. For
    k-fold cross-validation, the method does not use the entire dataset to build the
    model, instead it splits the data into a training dataset and a testing dataset.
    Therefore, the model built with a training dataset can then be used to assess
    the performance of the model on the testing dataset. By performing n repeats of
    the k-fold validation, we can then use the average of *n* accuracies to truly
    assess the performance of the built model. In this recipe, we will illustrate
    how to perform a k-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the telecom `churn` dataset as the input
    data source to train the support vector machine. For those who have not prepared
    the dataset, please refer to [Chapter 5](part0060_split_000.html#page "Chapter 5. Classification
    (I) – Tree, Lazy, and Probabilistic"), *Classification (I) – Tree, Lazy, and Probabilistic*,
    for detailed information.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to cross-validate the telecom `churn` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the index into `10` fold using the cut function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, use `for` loop to perform a 10 fold cross-validation, repeated `10` times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then print the accuracies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can generate average accuracies with the `mean` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implement a simple script performing 10-fold cross-validations.
    We first generate an index with 10 fold with the `cut` function. Then, we implement
    a `for` loop to perform a 10-fold cross-validation 10 times. Within the loop,
    we first apply `svm` on `9` folds of data as the training set. We then use the
    fitted model to predict the label of the rest of the data (the testing dataset).
    Next, we use the sum of the correctly predicted labels to generate the accuracy.
    As a result of this, the loop stores 10 generated accuracies. Finally, we use
    the `mean` function to retrieve the average of the accuracies.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you wish to perform the k-fold validation with the use of other models,
    simply replace the line to generate the variable fit to whatever classifier you
    prefer. For example, if you would like to assess the Naïve Bayes model with a
    10-fold cross-validation, you just need to replace the calling function from `svm`
    to `naiveBayes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Performing cross-validation with the e1071 package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides implementing a `loop` function to perform the k-fold cross-validation,
    you can use the `tuning` function (for example, `tune.nnet`, `tune.randomForest`,
    `tune.rpart`, `tune.svm`, and `tune.knn`.) within the `e1071` package to obtain
    the minimum error value. In this recipe, we will illustrate how to use `tune.svm`
    to perform the 10-fold cross-validation and obtain the optimum classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we continue to use the telecom `churn` dataset as the input
    data source to perform 10-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to retrieve the minimum estimation error using
    cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply `tune.svm` on the training dataset, `trainset`, with the 10-fold cross-validation
    as the tuning control. (If you find an error message, such as `could not find
    function predict.func`, please clear the workspace, restart the R session and
    reload the `e1071` library again):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can obtain the summary information of the model, tuned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can access the performance details of the tuned model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can use the optimum model to generate a classification table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `e1071` package provides miscellaneous functions to build and assess models,
    therefore, you do not need to reinvent the wheel to evaluate a fitted model. In
    this recipe, we use the `tune.svm` function to tune the svm model with the given
    formula, dataset, gamma, cost, and control functions. Within the `tune.control`
    options, we configure the option as `cross=10`, which performs a 10-fold cross
    validation during the tuning process. The tuning process will eventually return
    the minimum estimation error, performance detail, and the best model during the
    tuning process. Therefore, we can obtain the performance measures of the tuning
    and further use the optimum model to generate a classification table.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the `e1071` package, the `tune` function uses a grid search to tune parameters.
    For those interested in other tuning functions, use the help function to view
    the `tune` document:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Performing cross-validation with the caret package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Caret` (classification and regression training) package contains many functions
    in regard to the training process for regression and classification problems.
    Similar to the `e1071` package, it also contains a function to perform the k-fold
    cross validation. In this recipe, we will demonstrate how to the perform k-fold
    cross validation using the `caret` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the telecom `churn` dataset as the input
    data source to perform the k-fold cross validation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to perform the k-fold cross-validation with the
    `caret` package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, set up the control parameter to train with the 10-fold cross validation
    in `3` repetitions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can train the classification model on telecom churn data with `rpart`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can examine the output of the generated model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how convenient it is to conduct the k-fold cross-validation
    using the `caret` package. In the first step, we set up the training control and
    select the option to perform the 10-fold cross-validation in three repetitions.
    The process of repeating the k-fold validation is called repeated k-fold validation,
    which is used to test the stability of the model. If the model is stable, one
    should get a similar test result. Then, we apply `rpart` on the training dataset
    with the option to scale the data and to train the model with the options configured
    in the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: After the training process is complete, the model outputs three resampling results.
    Of these results, the model with `cp=0.05555556` has the largest accuracy value
    (`0.904`), and is therefore selected as the optimal model for classification.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can configure the `resampling` function in `trainControl`, in which you
    can specify `boot`, `boot632`, `cv`, `repeatedcv`, `LOOCV`, `LGOCV`, `none`, `oob`,
    `adaptive`_`cv`, `adaptive_boot`, or `adaptive_LGOCV`. To view more detailed information
    of how to choose the resampling method, view the `trainControl` document:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Ranking the variable importance with the caret package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After building a supervised learning model, we can estimate the importance of
    features. This estimation employs a sensitivity analysis to measure the effect
    on the output of a given model when the inputs are varied. In this recipe, we
    will show you how to rank the variable importance with the `caret` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have completed the previous recipe by storing the fitted `rpart`
    object in the `model` variable.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to rank the variable importance with the `caret`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can estimate the variable importance with the `varImp` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can generate the variable importance plot with the `plot` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00123.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1: The visualization of variable importance using the caret package'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we first use the `varImp` function to retrieve the variable
    importance and obtain the summary. The overall results show the sensitivity measure
    of each attribute. Next, we plot the variable importance in terms of rank, which
    shows that the `number_customer_service_calls` attribute is the most important
    variable in the sensitivity measure.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In some classification packages, such as `rpart`, the object generated from
    the training model contains the variable importance. We can examine the variable
    importance by accessing the output object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Ranking the variable importance with the rminer package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides using the `caret` package to generate variable importance, you can use
    the `rminer` package to generate the variable importance of a classification model.
    In the following recipe, we will illustrate how to use `rminer` to obtain the
    variable importance of a fitted model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the telecom `churn` dataset as the input
    data source to rank the variable importance.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to rank the variable importance with `rminer`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and load the package, `rminer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the svm model with the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `Importance` function to obtain the variable importance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the variable importance ranked by the variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00124.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2: The visualization of variable importance using the `rminer` package'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the `caret` package, the `rminer` package can also generate the variable
    importance of a classification model. In this recipe, we first train the svm model
    on the training dataset, `trainset`, with the `fit` function. Then, we use the
    `Importance` function to rank the variable importance with a sensitivity measure.
    Finally, we use `mgraph` to plot the rank of the variable importance. Similar
    to the result obtained from using the `caret` package, `number_customer_service_calls`
    is the most important variable in the measure of sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `rminer` package provides many classification models for one to choose
    from. If you are interested in using models other than svm, you can view these
    options with the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finding highly correlated features with the caret package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When performing regression or classification, some models perform better if
    highly correlated attributes are removed. The `caret` package provides the `findCorrelation`
    function, which can be used to find attributes that are highly correlated to each
    other. In this recipe, we will demonstrate how to find highly correlated features
    using the `caret` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the telecom `churn` dataset as the input
    data source to find highly correlated features.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to find highly correlated attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remove the features that are not coded in numeric characters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can obtain the correlation of each attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we use `findCorrelation` to search for highly correlated attributes with
    a cut off equal to 0.75:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then obtain the name of highly correlated attributes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we search for highly correlated attributes using the `caret`
    package. In order to retrieve the correlation of each attribute, one should first
    remove nonnumeric attributes. Then, we perform correlation to obtain a correlation
    matrix. Next, we use `findCorrelation` to find highly correlated attributes with
    the cut off set to 0.75\. We finally obtain the names of highly correlated (with
    a correlation coefficient over 0.75) attributes, which are `total_intl_minutes`,
    `total_day_charge`, `total_eve_minutes`, and `total_night_minutes`. You can consider
    removing some highly correlated attributes and keep one or two attributes for
    better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the `caret` package, you can use the `leaps`, `genetic`, and
    `anneal` functions in the `subselect` package to achieve the same goal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting features using the caret package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The feature selection method searches the subset of features with minimized
    predictive errors. We can apply feature selection to identify which attributes
    are required to build an accurate model. The `caret` package provides a recursive
    feature elimination function, `rfe`, which can help automatically select the required
    features. In the following recipe, we will demonstrate how to use the `caret`
    package to perform feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the telecom `churn` dataset as the input
    data source for feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to select features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform the feature named as `international_plan` of the training dataset,
    `trainset`, to `intl_yes` and `intl_no`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform the feature named as `voice_mail_plan` of the training dataset, `trainset`,
    to `voice_yes` and `voice_no`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove the `international_plan` and `voice_mail_plan` attributes and combine
    the training dataset, `trainset` with the data frames, `intl_plan` and `voice_plan`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform the feature named as `international_plan` of the testing dataset,
    `testset`, to `intl_yes` and `intl_no`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform the feature named as `voice_mail_plan` of the training dataset, `trainset`,
    to `voice_yes` and `voice_no`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove the `international_plan` and `voice_mail_plan` attributes and combine
    the testing dataset, `testset` with the data frames, `intl_plan` and `voice_plan`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then create a feature selection algorithm using linear discriminant analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we perform a backward feature selection on the training dataset, `trainset`
    using subsets from 1 to 18:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we can plot the selection result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00125.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3: The feature selection result'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can then examine the best subset of the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can examine the fitted model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can calculate the performance across resamples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we perform feature selection using the `caret` package. As there
    are factor-coded attributes within the dataset, we first use a function called
    `model.matrix` to transform the factor-coded attributes into multiple binary attributes.
    Therefore, we transform the `international_plan` attribute to `intl_yes` and `intl_no`.
    Additionally, we transform the `voice_mail_plan` attribute to `voice_yes` and
    `voice_no`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we set up control parameters for training using the cross-validation method,
    `cv`, with the linear discriminant function, `ldaFuncs`. Then, we use the recursive
    feature elimination, `rfe`, to perform feature selection with the use of the `control`
    function, `ldaFuncs`. The `rfe` function generates the summary of feature selection,
    which contains resampling a performance over the subset size and top variables.
  prefs: []
  type: TYPE_NORMAL
- en: We can then use the obtained model information to plot the number of variables
    against accuracy. From Figure 3, it is obvious that using 12 features can obtain
    the best accuracy. In addition to this, we can retrieve the best subset of the
    variables in (12 variables in total) the fitted model. Lastly, we can calculate
    the performance across resamples, which yields an accuracy of 0.86 and a kappa
    of 0.27.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to specify the algorithm used to control feature selection, one can
    change the control function specified in `rfeControl`. Here are some of the options
    you can use:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Measuring the performance of the regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To measure the performance of a regression model, we can calculate the distance
    from predicted output and the actual output as a quantifier of the performance
    of the model. Here, we often use the **root mean square error** (**RMSE**), **relative
    square error** (**RSE**) and R-Square as common measurements. In the following
    recipe, we will illustrate how to compute these measurements from a built regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use the `Quartet` dataset, which contains four regression
    datasets, as our input data source.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to measure the performance of the regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `Quartet` dataset from the `car` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the attribute, `y3`, against x using the `lm` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00126.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4: The linear regression plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can retrieve predicted values by using the `predict` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can calculate the root mean square error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can calculate the relative square error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, you can use R-Square as a measurement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can plot attribute, y3, against x using the `rlm` function from the
    MASS package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00127.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5: The robust linear regression plot on the Quartet dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can then retrieve the predicted value using the `predict` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can calculate the root mean square error using the distance of the
    predicted and actual value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the relative square error between the predicted and actual labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can calculate the R-Square value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The measurement of the performance of the regression model employs the distance
    between the predicted value and the actual value. We often use these three measurements,
    root mean square error, relative square error, and R-Square, as the quantifier
    of the performance of regression models. In this recipe, we first load the `Quartet`
    data from the `car` package. We then use the `lm` function to fit the linear model,
    and add the regression line on a scatter plot of the x variable against the `y3`
    variable. Next, we compute the predicted value using the predict function, and
    begin to compute the **root mean square error** (**RMSE**), **relative square
    error** (**RSE**), and R-Square for the built model.
  prefs: []
  type: TYPE_NORMAL
- en: As this dataset has an outlier at `x=13`, we would like to quantify how the
    outlier affects the performance measurement. To achieve this, we first train a
    regression model using the `rlm` function from the `MASS` package. Similar to
    the previous step, we then generate a performance measurement of the root square
    mean error, relative error and R-Square. From the output measurement, it is obvious
    that the mean square error and the relative square errors of the `lm` model are
    smaller than the model built by `rlm`, and the score of R-Square shows that the
    model built with `lm` has a greater prediction power. However, for the actual
    scenario, we should remove the outlier at `x=13`. This comparison shows that the
    outlier may be biased toward the performance measure and may lead us to choose
    the wrong model.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you would like to perform cross-validation on a linear regression model,
    you can use the `tune` function within the `e1071` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Other than the `e1071` package, you can use the `train` function from the `caret`
    package to perform cross-validation. In addition to this, you can also use `cv.lm`
    from the `DAAG` package to achieve the same goal.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring prediction performance with a confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To measure the performance of a classification model, we can first generate
    a classification table based on our predicted label and actual label. Then, we
    can use a confusion matrix to obtain performance measures such as precision, recall,
    specificity, and accuracy. In this recipe, we will demonstrate how to retrieve
    a confusion matrix using the `caret` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the telecom `churn` dataset as our example
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to generate a classification measurement:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train an svm model using the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then predict labels using the fitted model, `svm.model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can generate a classification table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can generate a confusion matrix using the prediction results and
    the actual labels from the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to obtain a confusion matrix to measure the
    performance of a classification model. First, we use the `train` function from
    the `caret` package to train an svm model. Next, we use the `predict` function
    to extract the predicted labels of the svm model using the testing dataset. Then,
    we perform the `table` function to obtain the classification table based on the
    predicted and actual labels. Finally, we use the `confusionMatrix` function from
    the `caret` package to a generate a confusion matrix to measure the performance
    of the classification model.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are interested in the available methods that can be used in the `train`
    function, you can refer to this website: [http://topepo.github.io/caret/modelList.html](http://topepo.github.io/caret/modelList.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring prediction performance using ROCR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **receiver operating characteristic** (**ROC**) curve is a plot that illustrates
    the performance of a binary classifier system, and plots the true positive rate
    against the false positive rate for different cut points. We most commonly use
    this plot to calculate the **area under curve** (**AUC**) to measure the performance
    of a classification model. In this recipe, we will demonstrate how to illustrate
    an ROC curve and calculate the AUC to measure the performance of a classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue using the telecom `churn` dataset as our example
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to generate two different classification examples
    with different costs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you should install and load the `ROCR` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the svm model using the training dataset with a probability equal to
    `TRUE`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make predictions based on the trained model on the testing dataset with the
    probability set as `TRUE`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain the probability of labels with `yes`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `prediction` function to generate a prediction result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `performance` function to obtain the performance measurement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the ROC curve using the `plot` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00128.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6: The ROC curve for the svm classifier performance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrated how to generate an ROC curve to illustrate the
    performance of a binary classifier. First, we should install and load the library,
    `ROCR`. Then, we use svm, from the `e1071` package, to train a classification
    model, and then use the model to predict labels for the testing dataset. Next,
    we use the prediction function (from the package, `ROCR`) to generate prediction
    results. We then adapt the performance function to obtain the performance measurement
    of the true positive rate against the false positive rate. Finally, we use the
    `plot` function to visualize the ROC plot, and add the value of AUC on the title.
    In this example, the AUC value is 0.92, which indicates that the svm classifier
    performs well in classifying telecom user churn datasets.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For those interested in the concept and terminology of ROC, you can refer to
    [http://en.wikipedia.org/wiki/Receiver_operating_characteristic](http://en.wikipedia.org/wiki/Receiver_operating_characteristic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing an ROC curve using the caret package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we introduced many classification methods; each method
    has its own advantages and disadvantages. However, when it comes to the problem
    of how to choose the best fitted model, you need to compare all the performance
    measures generated from different prediction models. To make the comparison easy,
    the caret package allows us to generate and compare the performance of models.
    In this recipe, we will use the function provided by the `caret` package to compare
    different algorithm trained models on the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we will continue to use telecom dataset as our input data source.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to generate an ROC curve of each fitted model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and load the library, `pROC`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up the training control with a 10-fold cross-validation in 3 repetitions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can train a classifier on the training dataset using `glm`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, you can train a classifier on the training dataset using `svm`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To see how `rpart` performs on the training data, we use the `rpart` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can make predictions separately based on different trained models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can generate the ROC curve of each model, and plot the curve on the same
    figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00129.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7: The ROC curve for the performance of three classifiers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we demonstrate how we can compare fitted models by illustrating their
    ROC curve in one figure. First, we set up the control of the training process
    with a 10-fold cross validation in 3 repetitions with the performance evaluation
    in `twoClassSummary`. After setting up control of the training process, we then
    apply `glm`, `svm`, and `rpart` algorithms on the training dataset to fit the
    classification models. Next, we can make a prediction based on each generated
    model and plot the ROC curve, respectively. Within the generated figure, we find
    that the model trained by svm has the largest area under curve, which is 0.9233
    (plotted in green), the AUC of the `glm` model (red) is 0.82, and the AUC of the
    `rpart` model (blue) is 0.7581\. From *Figure 7*, it is obvious that `svm` performs
    the best among all the fitted models on this training dataset (without requiring
    tuning).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use another ROC visualization package, `pROC`, which can be employed to
    display and analyze ROC curves. If you would like to know more about the package,
    please use the `help` function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Measuring performance differences between models with the caret package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we introduced how to generate ROC curves for each generated
    model, and have the curve plotted on the same figure. Apart from using an ROC
    curve, one can use the resampling method to generate statistics of each fitted
    model in ROC, sensitivity and specificity metrics. Therefore, we can use these
    statistics to compare the performance differences between each model. In the following
    recipe, we will introduce how to measure performance differences between fitted
    models with the `caret` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One needs to have completed the previous recipe by storing the `glm` fitted
    model, `svm` fitted model, and the `rpart` fitted model into `glm.model`, `svm.model`,
    and `rpart.model`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to measure performance differences between each
    fitted model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Resample the three generated models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can obtain a summary of the resampling result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `dotplot` to plot the resampling result in the ROC metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00130.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8: The dotplot of resampling result in ROC metric'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Also, you can use a box-whisker plot to plot the resampling result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00131.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 9: The box-whisker plot of resampling result'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to measure the performance differences among
    three fitted models using the resampling method. First, we use the `resample`
    function to generate the statistics of each fitted model (`svm.model`, `glm.model`,
    and `rpart.model`). Then, we can use the `summary` function to obtain the statistics
    of these three models in the ROC, sensitivity and specificity metrics. Next, we
    can apply a `dotplot` on the resampling result to see how ROC varied between each
    model. Last, we use a box-whisker plot on the resampling results to show the box-whisker
    plot of different models in the ROC, sensitivity and specificity metrics on a
    single plot.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides using `dotplot` and `bwplot` to measure performance differences, one
    can use `densityplot`, `splom`, and `xyplot` to visualize the performance differences
    of each fitted model in the ROC, sensitivity, and specificity metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
