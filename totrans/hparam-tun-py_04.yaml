- en: '*Chapter 3*: Exploring Exhaustive Search'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hyperparameter tuning doesn't always correspond to fancy and complex search
    algorithms. In fact, a simple `for` loop or manual search based on the developer's
    instinct can also be utilized to achieve the goal of hyperparameter tuning, which
    is to get the maximum evaluation score on the validation score without causing
    an overfitting issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll discuss the first out of four groups of hyperparameter
    tuning, called an **exhaustive search**. This is the *most widely used* and *most
    straightforward* hyperparameter-tuning group in practice. As explained by its
    name, hyperparameter-tuning methods that belong to this group work by *exhaustively
    searching* through the hyperparameter space. Except for one method, all of the
    methods in this group are categorized as **uninformed search** algorithms, meaning
    they are not learning from previous iterations to have a better search space in
    the future. Three methods will be discussed in this chapter: manual search, grid
    search, and random search.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the concepts of each of the
    hyperparameter-tuning methods that belong to the exhaustive search group. You
    will be able to explain these methods with confidence when someone asks you about
    them, in both a high-level and detailed fashion, along with the pros and cons.
    More importantly, you will be able to apply all of the methods with high confidence
    in practice. You will also be able to understand what's happening if there are
    errors or unexpected results and understand how to set up the method configuration
    to match your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following main topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding manual search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding grid search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding random search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding manual search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Manual search** is the most straightforward hyperparameter-tuning method
    that belongs to the exhaustive search group. In fact, it''s not even an algorithm!
    There''s no clear rule on how to perform this method. As its name would suggest,
    a manual search is performed based on your instinct. You simply have to tweak
    the hyperparameters until you are satisfied enough with the result.'
  prefs: []
  type: TYPE_NORMAL
- en: This method is *the one exception* mentioned before in the introduction of this
    chapter. Except for this method, other methods in the exhaustive search group
    are categorized as **uninformed search methods**. You may already know the reason
    why this method is the exception. It's because the developer themselves learn
    what is the impact of changing a particular or a set of hyperparameters in each
    iteration. In other words, they learn from previous iterations to have a (hopefully)
    better "hyperparameter space" in the next iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform a manual search, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the original full data into train and test sets (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,
    Evaluating Machine Learning Models*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify initial hyperparameter values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform cross-validation on the train set (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,
    Evaluating Machine Learning Models*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the cross-evaluation score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify new hyperparameter values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 3-5* until you are satisfied enough.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on the full training set using the final hyperparameter values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the final trained model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Although this method seems very straightforward and easy to do, it is actually
    *the other way around for a beginner*. This is because you need to really understand
    how the model works and the usage of each hyperparameter. It is also worth noting
    that, when it comes to manual search, there is *no clear definition of the hyperparameter
    space*. The hyperparameter space can be surprisingly narrow or vast, based on
    the developer's willingness and initiative to experiment with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of pros and cons of the manual search hyperparameter-tuning
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Manual search: pros and cons'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_03_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.1 – Manual search: pros and cons'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are aware of how manual search works, along with the pros and cons,
    we will learn the simplest automated hyperparameter-tuning strategy, which will
    be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`for` loop that tests all possible hyperparameter values in the search space.
    Although many packages have grid search as one of their hyperparameter-tuning
    method implementations, it is super easy to write your own code from scratch to
    implement this method. The name *grid* comes from the fact that we have to test
    the whole hyperparameter space just like creating a grid, as illustrated in the
    following diagram.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Grid search illustration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_03_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Grid search illustration
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say we want to perform hyperparameter tuning using the
    grid search method on a random forest. We decide to focus only on the number of
    estimators, splitting criterion, and maximum tree-depth hyperparameters. Then,
    we can specify a list of possible values for each of the hyperparameters. Let''s
    say we define the hyperparameter space as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of estimators: `n_estimators = [25, 50, 100, 150, 200]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Splitting criterion: `criterion = ["gini", "entropy"]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maximum depth: `max_depth = [3, 5, 10, 15, 20, None]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Notice that for the grid search method, we do not have to specify the underlying
    distribution of the hyperparameters. We simply create a list of all values that
    we want to test on for each hyperparameter. Then, we can call the grid search
    implementation from our favorite package or write the code for grid search by
    ourselves, as illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we create a nested `for` loop consisting of three layers,
    each for the hyperparameter in our search space. To perform a grid search in general,
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the original full data into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a nested for loop of H layers, where H is the number of hyperparameters
    in the space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Within each loop, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform cross-validation on the train set
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Store the cross-validation score along with the hyperparameter combination in
    a data structure—for example, a dictionary
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Train on the full training set using the best hyperparameter combination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the final trained model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see from the detailed steps on how to perform a grid search, this
    method is actually a *brute-force* method since we have to test all possible combinations
    of the predefined hyperparameter space. That's why it is very important to have
    a proper or *well-defined hyperparameter space*. If not, then we will waste a
    lot of time testing all of the combinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of pros and cons of the grid search hyperparameter-tuning method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Grid search: pros and cons'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_03_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.3 – Grid search: pros and cons'
  prefs: []
  type: TYPE_NORMAL
- en: The *COD* in *Table 3.2* means that adding another value to the hyperparameter
    space will *exponentially increase* the experiment time. Let's use the preceding
    example where we performed hyperparameter tuning on a random forest. In our initial
    hyperparameter space, there are ![](img/Formula_B18753_03_001.png) combinations
    we have to test. If we add just another value to our space—let's say we add `30`
    to the `max_depth` list—there will be ![](img/Formula_B18753_03_002.png) combinations
    or an additional `10` combinations that we have to test. This exponential behavior
    will even become more apparent when we have a bigger hyperparameter space! Sadly,
    it is also possible that after defining a big hyperparameter space and spending
    a long time performing hyperparameter tuning, we can still *miss better hyperparameter
    values* since they are located outside of the predefined space!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we have learned what grid search is, how it works, and what
    the pros and cons are. In the next section, we will discuss the last hyperparameter-tuning
    method that is categorized in the exhaustive search group: the random search method.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding random search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Random search** is the third and the last hyperparameter-tuning method that
    belongs in the exhaustive search group. It is a *simple method but works surprisingly
    well* in practice. As implied by its name, random search works by *randomly selecting
    hyperparameter values* in each iteration. There''s nothing more to it. The selected
    set of hyperparameters in the previous iteration will not impact how the method
    selects another set of hyperparameters in the following iterations. That''s why
    random search is also categorized as an *uninformed search* method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see an illustration of the random search method in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Random search illustration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_03_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Random search illustration
  prefs: []
  type: TYPE_NORMAL
- en: Random search usually works better than grid search when we have little or no
    idea of the proper hyperparameter space for our case, and this applies most of
    the time. Compared to grid search, random search is also more efficient in terms
    of computing cost and in finding the optimal set of hyperparameters. This is because
    we do not have to test each of the hyperparameter combinations; we can just let
    it run stochastically—or, in layman's terms, we can just *let luck play its part*.
  prefs: []
  type: TYPE_NORMAL
- en: You may wonder how picking a random set of hyperparameters can lead to a better
    tuning result compared to grid search most of the time. It is actually not the
    case if the predefined hyperparameter space is exactly the same as the one we
    provide to the grid search method. We have to provide a *bigger hyperparameter
    space* in order to support random search to play its role. A bigger search space
    doesn't always mean we have to *increase the dimensionality*, either by widening
    existing hyperparameters' range or adding new hyperparameters. We can also create
    a bigger hyperparameter space by *adding granularity* to it.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth noting that, unlike grid search, which doesn't require defining
    the hyperparameter's distribution when defining a search space, in random search,
    it is suggested to *define the distribution of each hyperparameter*. In some package
    implementations, if you do not specify the distribution, it will default to the
    uniform distribution. We will discuss more on the implementation part from [*Chapter
    7*](B18753_07_ePub.xhtml#_idTextAnchor062)*, Hyperparameter Tuning via Scikit*
    to [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092)*, Advanced Hyperparameter
    Tuning with DEAP and Microsoft NNI*
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use a similar example to what we saw in the *Understanding grid search*
    section to get a better understanding of how random search works. Apart from focusing
    on the number of estimators, splitting criterion, and maximum tree depth, we will
    also add a minimum samples split hyperparameter to our space. Unlike grid search,
    we have to also provide a distribution of each of the hyperparameters when defining
    a search space. Let''s say we define the hyperparameter space as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of estimators: `n_estimators = randint(25,200)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Splitting criterion: `criterion = ["gini", "entropy"]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maximum depth: `max_depth = [3, 5, 10, 15, 20, None]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minimum samples split: `min_samples_split = truncnorm(a=1, b=5, loc=2, scale=0.5)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, compared to the search space in the *Understanding grid search*
    section, we are *increasing the size of the space* by adding granularity and adding
    a new hyperparameter. We add granularity for the `n_estimators` hyperparameter
    by utilizing the `randint` uniform random integer distribution, ranging from `25`
    to `200`. This means we can test any value between `25` and `200`, where all of
    them will have the same probability of being tested.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from increasing the size of the search space by adding granularity, we
    also add a new hyperparameter called `min_samples_split`. This hyperparameter
    has the `truncnorm` distribution or `a=1` and `b=5`, with a mean of `loc=2` and
    standard deviation of `scale=0.5`.
  prefs: []
  type: TYPE_NORMAL
- en: As for `criterion` and `max_depth`, we are still using the same configuration
    as the previous search space. Note that *not specifying any distribution* means
    we are applying uniform distribution to the hyperparameter, where all values will
    have the same probability of being tested. For now, you don't have to worry about
    what are the available distributions and how to implement them, since we will
    also discuss them from [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)*,
    Hyperparameter Tuning via Scikit* to [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092)*,
    Advanced Hyperparameter Tuning with DEAP and Microsoft NNI*.
  prefs: []
  type: TYPE_NORMAL
- en: In random search, apart from the need to define a hyperparameter space, we also
    need to define a hyperparameter for this method itself, which is called the **number
    of trials**. This hyperparameter will *control how many trials or iterations*
    we want to perform on the predefined search space. This hyperparameter is needed
    since we *are not aiming to test all possible combinations* in the space; if we
    were, then it would be the same grid search method. It is also worth noting that
    since this method has a stochastic nature, we also need to specify a *random seed*
    to get the exact same result every time we run the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike grid search, it is quite cumbersome to implement this method from scratch,
    although it is possible to do so. Therefore, many packages support the implementation
    of the random search method. Regardless of the implementation variations, in general,
    random search works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the original full data into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the number of trials and a random seed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a hyperparameter space with the accompanied distributions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an iterator consisting of random hyperparameter combinations with the
    number of elements equal to the defined number of trials in Step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Loop through the iterator, where the following actions will be performed within
    each loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Getting the hyperparameter combination for this trial from the iterator
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing cross-validation on the train set
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing the cross-validation score along with the hyperparameter combination
    in a data structure—for example, a dictionary
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Train on the full training set using the best hyperparameter combination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the final trained model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please note that it is guaranteed there is *no duplicate* in the generated hyperparameter
    combinations in *Step 4*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of pros and cons of the random search hyperparameter-tuning
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Random search: pros and cons'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_03_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.5 – Random search: pros and cons'
  prefs: []
  type: TYPE_NORMAL
- en: The random search produces high variance during the process due to the property
    of *uninformed search* methods. There is no way for the random search to learn
    from past experiences so that it can learn better and be more effective in the
    next iterations. In [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)*, Exploring
    Multi-Fidelity Optimization*, we will learn other *variations of grid search and
    random search* that are categorized as informed search methods.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned all you need to know about random search, starting
    from what it is, how it works, what makes it different from grid search, and the
    pros and cons of this method.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed the first out of four groups of hyperparameter-tuning
    methods, called the exhaustive search group. We have discussed manual search,
    grid search, and random search. We not only discussed the definition of those
    methods, but also how those methods work at both a high level and a technical
    level, and what are the pros and cons for each of them. From now on, you should
    be able to explain these exhaustive search methods with confidence when someone
    asks you about them and apply all of the exhaustive search methods with high confidence
    in practice.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start discussing Bayesian optimization, the second
    group of hyperparameter-tuning methods. The goal of the next chapter is similar
    to this chapter, which is to give a better understanding of methods belonging
    to the Bayesian optimization group so that you can utilize those methods with
    high confidence in practice.
  prefs: []
  type: TYPE_NORMAL
