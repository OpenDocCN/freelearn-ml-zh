- en: '*Chapter 3*: Exploring Exhaustive Search'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第三章*：探索穷举搜索'
- en: Hyperparameter tuning doesn't always correspond to fancy and complex search
    algorithms. In fact, a simple `for` loop or manual search based on the developer's
    instinct can also be utilized to achieve the goal of hyperparameter tuning, which
    is to get the maximum evaluation score on the validation score without causing
    an overfitting issue.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整并不总是对应于复杂和花哨的搜索算法。实际上，一个简单的`for`循环或基于开发者直觉的手动搜索也可以用来实现超参数调整的目标，即在验证分数上获得最大评估分数，同时不引起过拟合问题。
- en: 'In this chapter, we''ll discuss the first out of four groups of hyperparameter
    tuning, called an **exhaustive search**. This is the *most widely used* and *most
    straightforward* hyperparameter-tuning group in practice. As explained by its
    name, hyperparameter-tuning methods that belong to this group work by *exhaustively
    searching* through the hyperparameter space. Except for one method, all of the
    methods in this group are categorized as **uninformed search** algorithms, meaning
    they are not learning from previous iterations to have a better search space in
    the future. Three methods will be discussed in this chapter: manual search, grid
    search, and random search.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论四个超参数调整组中的第一个，称为**穷举搜索**。这是实践中*最广泛使用*和*最直接*的超参数调整组。正如其名称所解释的，属于这一组超参数调整方法通过*穷举搜索*超参数空间来工作。除了一个方法外，这一组中的所有方法都被归类为**无信息搜索**算法，这意味着它们不会从之前的迭代中学习以在未来获得更好的搜索空间。本章将讨论三种方法：手动搜索、网格搜索和随机搜索。
- en: By the end of this chapter, you will understand the concepts of each of the
    hyperparameter-tuning methods that belong to the exhaustive search group. You
    will be able to explain these methods with confidence when someone asks you about
    them, in both a high-level and detailed fashion, along with the pros and cons.
    More importantly, you will be able to apply all of the methods with high confidence
    in practice. You will also be able to understand what's happening if there are
    errors or unexpected results and understand how to set up the method configuration
    to match your specific problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将理解属于穷举搜索组的每种超参数调整方法的概念。当有人问你关于这些方法时，你将能够自信地解释这些方法，无论是从高层次还是详细的角度，包括它们的优缺点。更重要的是，你将能够以高信心将所有这些方法应用于实践中。你还将能够理解如果出现错误或意外结果时会发生什么，并理解如何设置方法配置以匹配你的具体问题。
- en: 'The following main topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: Understanding manual search
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解手动搜索
- en: Understanding grid search
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解网格搜索
- en: Understanding random search
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解随机搜索
- en: Understanding manual search
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解手动搜索
- en: '**Manual search** is the most straightforward hyperparameter-tuning method
    that belongs to the exhaustive search group. In fact, it''s not even an algorithm!
    There''s no clear rule on how to perform this method. As its name would suggest,
    a manual search is performed based on your instinct. You simply have to tweak
    the hyperparameters until you are satisfied enough with the result.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**手动搜索**是穷举搜索组中最直接的超参数调整方法。实际上，它甚至不是一个算法！没有明确的规则来执行这种方法。正如其名称所暗示的，手动搜索是基于你的直觉进行的。你只需调整超参数，直到你对结果满意为止。'
- en: This method is *the one exception* mentioned before in the introduction of this
    chapter. Except for this method, other methods in the exhaustive search group
    are categorized as **uninformed search methods**. You may already know the reason
    why this method is the exception. It's because the developer themselves learn
    what is the impact of changing a particular or a set of hyperparameters in each
    iteration. In other words, they learn from previous iterations to have a (hopefully)
    better "hyperparameter space" in the next iterations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是本章介绍中提到的*一个例外*。除了这种方法外，穷举搜索组中的其他方法都被归类为**无信息搜索方法**。你可能已经知道为什么这个方法是例外的原因。这是因为开发者自己学习到在每次迭代中改变特定或一组超参数的影响。换句话说，他们从之前的迭代中学习，以便在下一个迭代中获得（希望是）更好的“超参数空间”。
- en: 'To perform a manual search, do the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行手动搜索，请执行以下操作：
- en: Split the original full data into train and test sets (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,
    Evaluating Machine Learning Models*).
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始完整数据分割成训练集和测试集（参见[*第一章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，评估机器学习模型*)。
- en: Specify initial hyperparameter values.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定初始超参数值。
- en: Perform cross-validation on the train set (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,
    Evaluating Machine Learning Models*).
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上执行交叉验证（见 [*第 1 章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，评估机器学习模型*）。
- en: Get the cross-evaluation score.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取交叉验证分数。
- en: Specify new hyperparameter values.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定新的超参数值。
- en: Repeat *steps 3-5* until you are satisfied enough.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 3-5* 直到您满意为止。
- en: Train on the full training set using the final hyperparameter values.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最终超参数值在完整训练集上训练。
- en: Evaluate the final trained model on the test set.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终训练好的模型。
- en: Although this method seems very straightforward and easy to do, it is actually
    *the other way around for a beginner*. This is because you need to really understand
    how the model works and the usage of each hyperparameter. It is also worth noting
    that, when it comes to manual search, there is *no clear definition of the hyperparameter
    space*. The hyperparameter space can be surprisingly narrow or vast, based on
    the developer's willingness and initiative to experiment with it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法看起来非常直接且容易操作，但实际上对于初学者来说却是 *相反的*。这是因为你需要真正理解模型的工作原理以及每个超参数的用法。还值得注意的是，当涉及到手动搜索时，*没有超参数空间的明确定义*。超参数空间可能非常狭窄或非常广泛，这取决于开发者愿意和主动进行实验的意愿。
- en: 'Here is a list of pros and cons of the manual search hyperparameter-tuning
    method:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是手动搜索超参数调整方法的优缺点列表：
- en: '![Figure 3.1 – Manual search: pros and cons'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1 – 手动搜索：优缺点'
- en: '](img/B18753_03_001.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_03_001.jpg)'
- en: 'Figure 3.1 – Manual search: pros and cons'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 手动搜索：优缺点
- en: Now that you are aware of how manual search works, along with the pros and cons,
    we will learn the simplest automated hyperparameter-tuning strategy, which will
    be discussed in the next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了手动搜索的工作原理，以及其优缺点，我们将学习最简单的自动超参数调整策略，这将在下一节中讨论。
- en: Understanding grid search
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解网格搜索
- en: '`for` loop that tests all possible hyperparameter values in the search space.
    Although many packages have grid search as one of their hyperparameter-tuning
    method implementations, it is super easy to write your own code from scratch to
    implement this method. The name *grid* comes from the fact that we have to test
    the whole hyperparameter space just like creating a grid, as illustrated in the
    following diagram.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`for` 循环测试搜索空间中所有可能超参数值。尽管许多包将网格搜索作为它们超参数调整方法实现之一，但从头开始编写自己的代码来实现这种方法非常简单。名称
    *grid* 来自于我们必须像创建网格一样测试整个超参数空间，如下面的图示所示。'
- en: '![Figure 3.2 – Grid search illustration'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.2 – 网格搜索示意图'
- en: '](img/B18753_03_002.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_03_002.jpg)'
- en: Figure 3.2 – Grid search illustration
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 网格搜索示意图
- en: 'For example, let''s say we want to perform hyperparameter tuning using the
    grid search method on a random forest. We decide to focus only on the number of
    estimators, splitting criterion, and maximum tree-depth hyperparameters. Then,
    we can specify a list of possible values for each of the hyperparameters. Let''s
    say we define the hyperparameter space as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想使用网格搜索方法对随机森林进行超参数调整。我们决定只关注估计器的数量、分割标准和最大树深度超参数。然后，我们可以为每个超参数指定可能值的列表。假设我们定义超参数空间如下：
- en: 'Number of estimators: `n_estimators = [25, 50, 100, 150, 200]`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计器数量：`n_estimators = [25, 50, 100, 150, 200]`
- en: 'Splitting criterion: `criterion = ["gini", "entropy"]`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割标准：`criterion = ["gini", "entropy"]`
- en: 'Maximum depth: `max_depth = [3, 5, 10, 15, 20, None]`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大深度：`max_depth = [3, 5, 10, 15, 20, None]`
- en: 'Notice that for the grid search method, we do not have to specify the underlying
    distribution of the hyperparameters. We simply create a list of all values that
    we want to test on for each hyperparameter. Then, we can call the grid search
    implementation from our favorite package or write the code for grid search by
    ourselves, as illustrated in the following snippet:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到对于网格搜索方法，我们不需要指定超参数的潜在分布。我们只需为每个超参数创建一个包含我们想要测试的所有值的列表。然后，我们可以从我们喜欢的包中调用网格搜索实现，或者像下面的代码片段所示，自己编写网格搜索的代码：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this example, we create a nested `for` loop consisting of three layers,
    each for the hyperparameter in our search space. To perform a grid search in general,
    do the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们创建了一个包含三个层次的嵌套 `for` 循环，每个层次对应于搜索空间中的一个超参数。要进行一般的网格搜索，请执行以下操作：
- en: Split the original full data into train and test sets.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始完整数据分割成训练集和测试集。
- en: Define the hyperparameter space.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义超参数空间。
- en: Construct a nested for loop of H layers, where H is the number of hyperparameters
    in the space.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个H层的嵌套循环，其中H是空间中超参数的数量。
- en: 'Within each loop, do the following:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个循环中，执行以下操作：
- en: Perform cross-validation on the train set
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练集上执行交叉验证
- en: Store the cross-validation score along with the hyperparameter combination in
    a data structure—for example, a dictionary
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将交叉验证分数与超参数组合一起存储在数据结构中——例如，一个字典
- en: Train on the full training set using the best hyperparameter combination.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳超参数组合在完整训练集上进行训练。
- en: Evaluate the final trained model on the test set.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终训练好的模型。
- en: As you can see from the detailed steps on how to perform a grid search, this
    method is actually a *brute-force* method since we have to test all possible combinations
    of the predefined hyperparameter space. That's why it is very important to have
    a proper or *well-defined hyperparameter space*. If not, then we will waste a
    lot of time testing all of the combinations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从如何执行网格搜索的详细步骤中可以看到，这种方法实际上是一种*暴力搜索*方法，因为我们必须测试所有预定义的超参数空间的所有可能组合。这就是为什么拥有一个适当或*明确定义的超参数空间*非常重要。如果没有，那么我们将浪费大量时间测试所有组合。
- en: 'Here is a list of pros and cons of the grid search hyperparameter-tuning method:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是网格搜索超参数调整方法的优缺点列表：
- en: '![Figure 3.3 – Grid search: pros and cons'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.3 – 网格搜索：优点和缺点'
- en: '](img/B18753_03_003.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_03_003.jpg)'
- en: 'Figure 3.3 – Grid search: pros and cons'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 网格搜索：优点和缺点
- en: The *COD* in *Table 3.2* means that adding another value to the hyperparameter
    space will *exponentially increase* the experiment time. Let's use the preceding
    example where we performed hyperparameter tuning on a random forest. In our initial
    hyperparameter space, there are ![](img/Formula_B18753_03_001.png) combinations
    we have to test. If we add just another value to our space—let's say we add `30`
    to the `max_depth` list—there will be ![](img/Formula_B18753_03_002.png) combinations
    or an additional `10` combinations that we have to test. This exponential behavior
    will even become more apparent when we have a bigger hyperparameter space! Sadly,
    it is also possible that after defining a big hyperparameter space and spending
    a long time performing hyperparameter tuning, we can still *miss better hyperparameter
    values* since they are located outside of the predefined space!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*表3.2*中的*COD*表示向超参数空间添加另一个值将*指数级增加*实验时间。让我们用前面的例子来说明，我们在随机森林上执行了超参数调整。在我们的初始超参数空间中，有![](img/Formula_B18753_03_001.png)种组合我们必须测试。如果我们只是向我们的空间添加另一个值——比如说我们向`max_depth`列表添加`30`——那么将有![](img/Formula_B18753_03_002.png)种组合，或者额外的`10`种组合我们必须测试。这种指数行为在我们拥有更大的超参数空间时将变得更加明显！遗憾的是，也有可能在我们定义了一个大的超参数空间并花费了大量时间进行超参数调整之后，我们仍然可能*错过更好的超参数值*，因为它们位于预定义空间之外！'
- en: 'In this section, we have learned what grid search is, how it works, and what
    the pros and cons are. In the next section, we will discuss the last hyperparameter-tuning
    method that is categorized in the exhaustive search group: the random search method.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了什么是网格搜索，它是如何工作的，以及它的优缺点。在下一节中，我们将讨论最后一种被归类为穷举搜索组的超参数调整方法：随机搜索方法。
- en: Understanding random search
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解随机搜索
- en: '**Random search** is the third and the last hyperparameter-tuning method that
    belongs in the exhaustive search group. It is a *simple method but works surprisingly
    well* in practice. As implied by its name, random search works by *randomly selecting
    hyperparameter values* in each iteration. There''s nothing more to it. The selected
    set of hyperparameters in the previous iteration will not impact how the method
    selects another set of hyperparameters in the following iterations. That''s why
    random search is also categorized as an *uninformed search* method.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机搜索**是穷举搜索组中的第三种也是最后一种超参数调整方法。它是一种*简单但实际效果出奇的好*的方法。正如其名称所暗示的，随机搜索通过*在每次迭代中随机选择超参数值*来工作。除此之外没有其他内容。前一次迭代中选择的超参数集不会影响该方法在下一次迭代中选择另一组超参数的方式。这就是为什么随机搜索也被归类为*无信息搜索*方法。'
- en: 'You can see an illustration of the random search method in the following diagram:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下图表中看到随机搜索方法的示意图：
- en: '![Figure 3.4 – Random search illustration'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.4 – 随机搜索示意图'
- en: '](img/B18753_03_004.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_03_004.jpg)'
- en: Figure 3.4 – Random search illustration
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 随机搜索示意图
- en: Random search usually works better than grid search when we have little or no
    idea of the proper hyperparameter space for our case, and this applies most of
    the time. Compared to grid search, random search is also more efficient in terms
    of computing cost and in finding the optimal set of hyperparameters. This is because
    we do not have to test each of the hyperparameter combinations; we can just let
    it run stochastically—or, in layman's terms, we can just *let luck play its part*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对我们的案例合适的超参数空间知之甚少或一无所知时，随机搜索通常比网格搜索更有效，这在大多数情况下都适用。与网格搜索相比，随机搜索在计算成本和寻找最佳超参数集方面也更有效率。这是因为我们不必测试每个超参数组合；我们只需让它随机运行——或者用通俗的话说，我们只需*让运气发挥作用*。
- en: You may wonder how picking a random set of hyperparameters can lead to a better
    tuning result compared to grid search most of the time. It is actually not the
    case if the predefined hyperparameter space is exactly the same as the one we
    provide to the grid search method. We have to provide a *bigger hyperparameter
    space* in order to support random search to play its role. A bigger search space
    doesn't always mean we have to *increase the dimensionality*, either by widening
    existing hyperparameters' range or adding new hyperparameters. We can also create
    a bigger hyperparameter space by *adding granularity* to it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，随机选择一组超参数如何比网格搜索在大多数情况下产生更好的调整结果。如果预定义的超参数空间与我们提供给网格搜索方法的完全相同，实际上并不是这样。我们必须提供一个*更大的超参数空间*，以便支持随机搜索发挥作用。更大的搜索空间并不意味着我们必须*增加维度性*，无论是通过扩大现有超参数的范围还是添加新的超参数。我们也可以通过*增加粒度*来创建更大的超参数空间。
- en: It is also worth noting that, unlike grid search, which doesn't require defining
    the hyperparameter's distribution when defining a search space, in random search,
    it is suggested to *define the distribution of each hyperparameter*. In some package
    implementations, if you do not specify the distribution, it will default to the
    uniform distribution. We will discuss more on the implementation part from [*Chapter
    7*](B18753_07_ePub.xhtml#_idTextAnchor062)*, Hyperparameter Tuning via Scikit*
    to [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092)*, Advanced Hyperparameter
    Tuning with DEAP and Microsoft NNI*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，与网格搜索不同，在定义搜索空间时不需要定义超参数的分布，在随机搜索中，建议*定义每个超参数的分布*。在一些包的实现中，如果您没有指定分布，它将默认为均匀分布。我们将在[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)*，通过Scikit进行超参数调整*到[*第10章*](B18753_10_ePub.xhtml#_idTextAnchor092)*，使用DEAP和Microsoft
    NNI进行高级超参数调整*中进一步讨论实现部分。
- en: 'Let''s use a similar example to what we saw in the *Understanding grid search*
    section to get a better understanding of how random search works. Apart from focusing
    on the number of estimators, splitting criterion, and maximum tree depth, we will
    also add a minimum samples split hyperparameter to our space. Unlike grid search,
    we have to also provide a distribution of each of the hyperparameters when defining
    a search space. Let''s say we define the hyperparameter space as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个与我们在*理解网格搜索*部分看到的类似例子来更好地理解随机搜索的工作原理。除了关注估计器的数量、分割准则和最大树深度外，我们还将添加一个最小样本分割超参数到我们的空间中。与网格搜索不同，在定义搜索空间时，我们必须提供每个超参数的分布。让我们假设我们定义的超参数空间如下：
- en: 'Number of estimators: `n_estimators = randint(25,200)`'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计器数量：`n_estimators = randint(25,200)`
- en: 'Splitting criterion: `criterion = ["gini", "entropy"]`'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割准则：`criterion = ["gini", "entropy"]`
- en: 'Maximum depth: `max_depth = [3, 5, 10, 15, 20, None]`'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大深度：`max_depth = [3, 5, 10, 15, 20, None]`
- en: 'Minimum samples split: `min_samples_split = truncnorm(a=1, b=5, loc=2, scale=0.5)`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小样本分割：`min_samples_split = truncnorm(a=1, b=5, loc=2, scale=0.5)`
- en: As you can see, compared to the search space in the *Understanding grid search*
    section, we are *increasing the size of the space* by adding granularity and adding
    a new hyperparameter. We add granularity for the `n_estimators` hyperparameter
    by utilizing the `randint` uniform random integer distribution, ranging from `25`
    to `200`. This means we can test any value between `25` and `200`, where all of
    them will have the same probability of being tested.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，与*理解网格搜索*部分中的搜索空间相比，我们通过添加粒度和添加新的超参数来*增加空间的大小*。我们通过利用`randint`均匀随机整数分布，将`n_estimators`超参数的粒度添加到其中，范围从`25`到`200`。这意味着我们可以测试`25`到`200`之间的任何值，其中所有这些值都有相同的被测试概率。
- en: Apart from increasing the size of the search space by adding granularity, we
    also add a new hyperparameter called `min_samples_split`. This hyperparameter
    has the `truncnorm` distribution or `a=1` and `b=5`, with a mean of `loc=2` and
    standard deviation of `scale=0.5`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过增加粒度来增加搜索空间的大小外，我们还添加了一个新的超参数，称为`min_samples_split`。这个超参数具有`truncnorm`分布或`a=1`和`b=5`，均值为`loc=2`，标准差为`scale=0.5`。
- en: As for `criterion` and `max_depth`, we are still using the same configuration
    as the previous search space. Note that *not specifying any distribution* means
    we are applying uniform distribution to the hyperparameter, where all values will
    have the same probability of being tested. For now, you don't have to worry about
    what are the available distributions and how to implement them, since we will
    also discuss them from [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)*,
    Hyperparameter Tuning via Scikit* to [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092)*,
    Advanced Hyperparameter Tuning with DEAP and Microsoft NNI*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`criterion`和`max_depth`，我们仍然使用与之前搜索空间相同的配置。请注意，**未指定任何分布**意味着我们对超参数应用均匀分布，其中所有值都有相同的概率被测试。目前，你不必担心有哪些可用的分布以及如何实现它们，因为我们将从[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)“通过Scikit进行超参数调整”到[*第10章*](B18753_10_ePub.xhtml#_idTextAnchor092)“使用DEAP和Microsoft
    NNI进行高级超参数调整”进行讨论。
- en: In random search, apart from the need to define a hyperparameter space, we also
    need to define a hyperparameter for this method itself, which is called the **number
    of trials**. This hyperparameter will *control how many trials or iterations*
    we want to perform on the predefined search space. This hyperparameter is needed
    since we *are not aiming to test all possible combinations* in the space; if we
    were, then it would be the same grid search method. It is also worth noting that
    since this method has a stochastic nature, we also need to specify a *random seed*
    to get the exact same result every time we run the code.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机搜索中，除了需要定义超参数空间外，我们还需要为该方法本身定义一个超参数，称为**试验次数**。这个超参数将**控制我们希望在预定义的搜索空间上执行多少次试验或迭代**。由于我们**不是旨在测试空间中所有可能的组合**，因此需要这个超参数；如果我们这样做，那么它将与网格搜索方法相同。还值得注意的是，由于这种方法具有随机性，我们还需要指定一个**随机种子**，以确保每次运行代码时都能得到完全相同的结果。
- en: 'Unlike grid search, it is quite cumbersome to implement this method from scratch,
    although it is possible to do so. Therefore, many packages support the implementation
    of the random search method. Regardless of the implementation variations, in general,
    random search works like this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与网格搜索不同，从头实现这种方法相当繁琐，尽管这是可能的。因此，许多包支持随机搜索方法的实现。无论实现方式如何，一般来说，随机搜索的工作方式如下：
- en: Split the original full data into train and test sets.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始完整数据分割成训练集和测试集。
- en: Define the number of trials and a random seed.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义试验次数和随机种子。
- en: Define a hyperparameter space with the accompanied distributions.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个带有伴随分布的超参数空间。
- en: Generate an iterator consisting of random hyperparameter combinations with the
    number of elements equal to the defined number of trials in Step 2.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个迭代器，包含随机超参数组合，元素数量等于步骤2中定义的试验次数。
- en: 'Loop through the iterator, where the following actions will be performed within
    each loop:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历迭代器，在每次循环中执行以下操作：
- en: Getting the hyperparameter combination for this trial from the iterator
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从迭代器中获取本次试验的超参数组合
- en: Performing cross-validation on the train set
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练集上执行交叉验证
- en: Storing the cross-validation score along with the hyperparameter combination
    in a data structure—for example, a dictionary
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将交叉验证分数与超参数组合一起存储在数据结构中——例如，字典
- en: Train on the full training set using the best hyperparameter combination.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳超参数组合在完整训练集上进行训练。
- en: Evaluate the final trained model on the test set.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终训练好的模型。
- en: Please note that it is guaranteed there is *no duplicate* in the generated hyperparameter
    combinations in *Step 4*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在步骤4中生成的超参数组合中**保证没有重复**。
- en: 'Here is a list of pros and cons of the random search hyperparameter-tuning
    method:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是随机搜索超参数调整方法的优缺点列表：
- en: '![Figure 3.5 – Random search: pros and cons'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 3.5 – Random search: pros and cons'
- en: '](img/B18753_03_005.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_03_005.jpg]'
- en: 'Figure 3.5 – Random search: pros and cons'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – 随机搜索：优点和缺点
- en: The random search produces high variance during the process due to the property
    of *uninformed search* methods. There is no way for the random search to learn
    from past experiences so that it can learn better and be more effective in the
    next iterations. In [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)*, Exploring
    Multi-Fidelity Optimization*, we will learn other *variations of grid search and
    random search* that are categorized as informed search methods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*无信息搜索*方法的特性，随机搜索在过程中会产生高方差。随机搜索无法从过去的经验中学习，以便在下一轮迭代中更好地学习和更有效地工作。在[*第6章*](B18753_06_ePub.xhtml#_idTextAnchor054)*，探索多保真优化*中，我们将学习其他被归类为信息搜索方法的*网格搜索和随机搜索的变体*。
- en: In this section, we have learned all you need to know about random search, starting
    from what it is, how it works, what makes it different from grid search, and the
    pros and cons of this method.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了关于随机搜索所需了解的所有内容，从它是什么，如何工作，它与网格搜索的不同之处，以及这种方法的优势和劣势。
- en: Summary
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have discussed the first out of four groups of hyperparameter-tuning
    methods, called the exhaustive search group. We have discussed manual search,
    grid search, and random search. We not only discussed the definition of those
    methods, but also how those methods work at both a high level and a technical
    level, and what are the pros and cons for each of them. From now on, you should
    be able to explain these exhaustive search methods with confidence when someone
    asks you about them and apply all of the exhaustive search methods with high confidence
    in practice.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了四组超参数调整方法中的第一组，称为穷举搜索组。我们讨论了手动搜索、网格搜索和随机搜索。我们不仅讨论了这些方法的定义，还讨论了这些方法在高级和专业技术层面的工作原理，以及它们各自的优缺点。从现在起，当有人向你询问这些穷举搜索方法时，你应该能够自信地解释它们，并在实践中充满信心地应用所有穷举搜索方法。
- en: In the next chapter, we will start discussing Bayesian optimization, the second
    group of hyperparameter-tuning methods. The goal of the next chapter is similar
    to this chapter, which is to give a better understanding of methods belonging
    to the Bayesian optimization group so that you can utilize those methods with
    high confidence in practice.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始讨论贝叶斯优化，这是超参数调整方法的第二组。下一章的目标与本章类似，即更好地理解属于贝叶斯优化组的方法，以便你在实践中能够充满信心地利用这些方法。
