<html><head></head><body>
<div id="_idContainer032">
<h1 class="chapter-number" id="_idParaDest-43"><a id="_idTextAnchor042"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.2.1">Identify Common Machine Learning Techniques</span></h1>
<p><span class="koboSpan" id="kobo.3.1">So far, we’ve introduced you to AI technologies (such as computer vision or generative AI) as well as Microsoft’s principles for responsible AI. </span><span class="koboSpan" id="kobo.3.2">Now, it’s time to start talking about the substance </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">of AI.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">One of the most important questions you might have about AI is how AI manages to know what it does. </span><span class="koboSpan" id="kobo.5.2">Just as humans learn, AI systems have been designed to be capable of learning. </span><span class="koboSpan" id="kobo.5.3">And, just like humans learn through a variety of mechanisms (such as memorization and practice or repetition), AI systems also learn through different techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">and scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">To be honest, though, the term </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">machine learning</span></strong><span class="koboSpan" id="kobo.9.1"> is a bit of a misnomer. </span><span class="koboSpan" id="kobo.9.2">Since computers aren’t exactly sentient at this point, one might argue that they’re not capable of really learning. </span><span class="koboSpan" id="kobo.9.3">What they are capable of, however, is something quite useful: examining vast data sets to establish patterns and predict outcomes. </span><span class="koboSpan" id="kobo.9.4">Humans can sometimes be pretty good at recognizing patterns for small data sets. </span><span class="koboSpan" id="kobo.9.5">Once the data set includes tens of thousands or millions of data points, it becomes much more difficult for a human to keep up—and this is where machine </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">learning excels.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">The core idea of machine learning is looking at these vast data sets and predicting outcomes or values of similar actions or scenarios. </span><span class="koboSpan" id="kobo.11.2">Examples of machine learning might include </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">A power company combining historical weather patterns and historical energy usage to estimate the load on an </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">electric grid</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">An insurance company using miles driven, whether driven hours are daylight or nighttime, and driver age to predict the likelihood of </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">an accident</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">A biologist researcher using visual data of animals to automate the identification of known species observed on cameras and highlight potentially </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">unknown species</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.19.1">In each of these cases, an AI system is trained on known data sets and then either asked questions or exposed to new data and is instructed to apply its past observations on the new data or queries to come up with new outputs </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">or results.</span></span></p>
<p><span class="koboSpan" id="kobo.21.1">In this chapter, we’ll cover some of the high-level concepts related to machine learning. </span><span class="koboSpan" id="kobo.21.2">The objectives and skills we’ll cover in this chapter include </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.23.1">Identify regression machine </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">learning scenarios</span></span></li>
<li><span class="koboSpan" id="kobo.25.1">Identify classification machine </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">learning scenarios</span></span></li>
<li><span class="koboSpan" id="kobo.27.1">Identify clustering machine </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">learning scenarios</span></span></li>
<li><span class="koboSpan" id="kobo.29.1">Identify features of deep </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">learning techniques</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.31.1">By the end of this chapter, you should be able to identify and describe some of the common machine </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">learning scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.33.1">First, let’s establish a little background information on </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">machine learning.</span></span></p>
<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.35.1">Understanding machine learning terminology</span></h1>
<p><span class="koboSpan" id="kobo.36.1">As you’ve already learned, machine </span><a id="_idIndexMarker079"/><span class="koboSpan" id="kobo.37.1">learning is another way to think about predicting outcomes based on observed </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">data sets.</span></span></p>
<p><span class="koboSpan" id="kobo.39.1">Machine learning models </span><a id="_idIndexMarker080"/><span class="koboSpan" id="kobo.40.1">are essentially software applications that use mathematical functions to calculate output values based on input values. </span><span class="koboSpan" id="kobo.40.2">This process involves two main phases: </span><strong class="bold"><span class="koboSpan" id="kobo.41.1">training</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.42.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.43.1">inferencing</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">.</span></span></p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.45.1">Training</span></h2>
<p><span class="koboSpan" id="kobo.46.1">During </span><a id="_idIndexMarker081"/><span class="koboSpan" id="kobo.47.1">training, the model learns to predict output values based on input values by analyzing past observations. </span><span class="koboSpan" id="kobo.47.2">These past observations include both</span><a id="_idIndexMarker082"/><span class="koboSpan" id="kobo.48.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.49.1">features</span></strong><span class="koboSpan" id="kobo.50.1"> (input values) and </span><strong class="bold"><span class="koboSpan" id="kobo.51.1">labels</span></strong><span class="koboSpan" id="kobo.52.1"> (</span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">output </span></span><span class="No-Break"><a id="_idIndexMarker083"/></span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">values).</span></span></p>
<p><span class="koboSpan" id="kobo.55.1">In a typical scenario, features </span><a id="_idIndexMarker084"/><span class="koboSpan" id="kobo.56.1">are represented as variables denoted by </span><em class="italic"><span class="koboSpan" id="kobo.57.1">x</span></em><span class="koboSpan" id="kobo.58.1">, while labels are denoted by </span><em class="italic"><span class="koboSpan" id="kobo.59.1">y</span></em><span class="koboSpan" id="kobo.60.1">. </span><span class="koboSpan" id="kobo.60.2">Features can consist of multiple values, forming a </span><strong class="bold"><span class="koboSpan" id="kobo.61.1">vector</span></strong><span class="koboSpan" id="kobo.62.1"> represented by </span><em class="italic"><span class="koboSpan" id="kobo.63.1">[x1, x2, x3, ...],y</span></em><span class="koboSpan" id="kobo.64.1">. </span><span class="koboSpan" id="kobo.64.2">For example, in predicting</span><a id="_idIndexMarker085"/><span class="koboSpan" id="kobo.65.1"> bottled water sales based on weather, weather measurements are features (</span><em class="italic"><span class="koboSpan" id="kobo.66.1">x</span></em><span class="koboSpan" id="kobo.67.1">) and the number of bottles sold is the </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">label (</span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.69.1">y</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.71.1">An </span><strong class="bold"><span class="koboSpan" id="kobo.72.1">algorithm</span></strong><span class="koboSpan" id="kobo.73.1"> is then</span><a id="_idIndexMarker086"/><span class="koboSpan" id="kobo.74.1"> applied to the data to establish a relationship between the features and labels, creating a calculation to predict the label based on the features. </span><span class="koboSpan" id="kobo.74.2">The choice of algorithm depends on the type of predictive problem being addressed. </span><span class="koboSpan" id="kobo.74.3">The outcome</span><a id="_idIndexMarker087"/><span class="koboSpan" id="kobo.75.1"> is a </span><strong class="bold"><span class="koboSpan" id="kobo.76.1">model</span></strong><span class="koboSpan" id="kobo.77.1"> represented</span><a id="_idIndexMarker088"/><span class="koboSpan" id="kobo.78.1"> by a function </span><em class="italic"><span class="koboSpan" id="kobo.79.1">f</span></em><span class="koboSpan" id="kobo.80.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.81.1">y = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.82.1">f(x)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.84.1">What are vectors and algorithms, anyway?</span></p>
<p class="callout"><span class="koboSpan" id="kobo.85.1">Vectors, foundational units of algebra, are</span><a id="_idIndexMarker089"/><span class="koboSpan" id="kobo.86.1"> essentially </span><strong class="bold"><span class="koboSpan" id="kobo.87.1">tuples</span></strong><span class="koboSpan" id="kobo.88.1"> (or collections) of values. </span><span class="koboSpan" id="kobo.88.2">These groups of values can be thought of as being similar to an array, though tuples can contain mixed data types such as strings, floating point numbers, </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">and integers.</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.90.1">At a high level, an algorithm is a set of functions, rules, formulas, or processes that an AI system uses to analyze data, discover insights, and predict outcomes. </span><span class="koboSpan" id="kobo.90.2">Algorithms represent the math that enables </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">machine learning.</span></span></p>
<p><span class="koboSpan" id="kobo.92.1">The learning part of machine learning can be divided into two types: </span><strong class="bold"><span class="koboSpan" id="kobo.93.1">supervised</span></strong><span class="koboSpan" id="kobo.94.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.95.1">unsupervised</span></strong><span class="koboSpan" id="kobo.96.1">. </span><span class="koboSpan" id="kobo.96.2">Each of these types has a variety of algorithms and processes associated </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">with them.</span></span></p>
<h3><span class="koboSpan" id="kobo.98.1">Supervised machine learning techniques</span></h3>
<p><span class="koboSpan" id="kobo.99.1">Supervised </span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.100.1">machine learning encompasses algorithms that learn from data containing both input features and corresponding target labels. </span><span class="koboSpan" id="kobo.100.2">The</span><a id="_idIndexMarker091"/><span class="koboSpan" id="kobo.101.1"> goal is to uncover patterns that link the input features to their outcomes, enabling the model to forecast outcomes for new, unseen data. </span><span class="koboSpan" id="kobo.101.2">Let’s look at types of supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">learning techniques:</span></span></p>
<h4><span class="koboSpan" id="kobo.103.1">Regression</span></h4>
<p><strong class="bold"><span class="koboSpan" id="kobo.104.1">Regression</span></strong><span class="koboSpan" id="kobo.105.1">-based</span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.106.1"> learning is designed to identify and understand the relationship between independent and dependent variables and is frequently used to make business projections. </span><span class="koboSpan" id="kobo.106.2">Regression is a type of</span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.107.1"> supervised learning where the output is a continuous number. </span><span class="koboSpan" id="kobo.107.2">Examples include </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.109.1">Predicting bottled water sales based on weather conditions such </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">as temperature</span></span></li>
<li><span class="koboSpan" id="kobo.111.1">Estimating a dealership’s vehicle sales prices from amount of available inventory and previous </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">sales prices</span></span></li>
</ul>
<h4><span class="koboSpan" id="kobo.113.1">Classification</span></h4>
<p><strong class="bold"><span class="koboSpan" id="kobo.114.1">Classification</span></strong><span class="koboSpan" id="kobo.115.1"> uses an </span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.116.1">algorithm to assign the training data to categories. </span><span class="koboSpan" id="kobo.116.2">Classification identifies or recognizes </span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.117.1">entities in the training data and attempts to draw conclusions about how the entities are defined or labeled. </span><span class="koboSpan" id="kobo.117.2">Classification involves categorizing data points into </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">distinct classes:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.119.1">Binary classification</span></strong><span class="koboSpan" id="kobo.120.1">: This predicts one of</span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.121.1"> two possible outcomes. </span><span class="koboSpan" id="kobo.121.2">Examples </span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.122.1">include diagnosing diabetes from health metrics, assessing loan default risk from financial history, and predicting marketing response from </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">consumer profiles.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.124.1">Multiclass classification</span></strong><span class="koboSpan" id="kobo.125.1">: This predicts which one</span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.126.1"> of several classes an </span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.127.1">observation belongs to. </span><span class="koboSpan" id="kobo.127.2">For instance, classifying an animal species based on physical features or a movie’s genre from its production details. </span><span class="koboSpan" id="kobo.127.3">Unlike binary classification, a single observation in multiclass classification is assigned to one </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">exclusive category.</span></span></li>
</ul>
<h4><span class="koboSpan" id="kobo.129.1">Algorithms</span></h4>
<p><span class="koboSpan" id="kobo.130.1">As you’ve already learned, algorithms are</span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.131.1"> the mathematical formulas used to process data. </span><span class="koboSpan" id="kobo.131.2">From a supervised learning perspective, these algorithms are </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">commonly used:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.133.1">AdaBoost</span></strong><span class="koboSpan" id="kobo.134.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.135.1">gradient boosting</span></strong><span class="koboSpan" id="kobo.136.1">: These </span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.137.1">methods enhance the accuracy of simple models by aggregating them into a more robust model. </span><span class="koboSpan" id="kobo.137.2">By sequentially correcting errors </span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.138.1">of a basic model using additional weak models, they collectively improve prediction accuracy. </span><span class="koboSpan" id="kobo.138.2">Boosting models can be applied to both classification and </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">regression problems.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.140.1">Artificial Neural Networks</span></strong><span class="koboSpan" id="kobo.141.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.142.1">ANNs</span></strong><span class="koboSpan" id="kobo.143.1">): ANNs are inspired by the human brain’s neural networks and are foundational to</span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.144.1"> deep learning. </span><span class="koboSpan" id="kobo.144.2">They process data through interconnected units called neurons, learning to </span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.145.1">recognize patterns and make decisions over time. </span><span class="koboSpan" id="kobo.145.2">ANNs are used in a variety of contexts, such as natural language processing, speech and image recognition, and game-playing (such as chess </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">and Go).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.147.1">Decision trees</span></strong><span class="koboSpan" id="kobo.148.1">: These algorithms</span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.149.1"> predict outcomes or </span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.150.1">classify data by breaking down decisions into a tree-like structure of choices. </span><span class="koboSpan" id="kobo.150.2">Decision trees are transparent, making them easier to understand and validate compared to more opaque models such as neural networks. </span><span class="koboSpan" id="kobo.150.3">You might picture a decision tree as a type of flow chart, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.151.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.152.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer020">
<span class="koboSpan" id="kobo.154.1"><img alt="Figure 3.1 – Example of a simple decision tree" src="image/B22207_03_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.155.1">Figure 3.1 – Example of a simple decision tree</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.156.1">Dimensionality reduction</span></strong><span class="koboSpan" id="kobo.157.1">: This technique reduces the complexity of data by decreasing the number of input features, focusing on retaining only the most relevant information. </span><span class="koboSpan" id="kobo.157.2">Principal </span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.158.1">component analysis is</span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.159.1"> a common method used for </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">this purpose.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.161.1">K-Nearest Neighbor</span></strong><span class="koboSpan" id="kobo.162.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.163.1">KNN</span></strong><span class="koboSpan" id="kobo.164.1">): KNN classifies data points based on the closest neighboring points in the data set. </span><span class="koboSpan" id="kobo.164.2">It</span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.165.1"> calculates the distance (often Euclidean) between points and assigns a category based on the most common category among its nearest neighbors. </span><span class="koboSpan" id="kobo.165.2">KNN makes predictions based on the majority or average value of the nearest </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">data points.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.167.1">Linear regression</span></strong><span class="koboSpan" id="kobo.168.1">: This approach models the relationship between a dependent variable and one or more</span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.169.1"> independent variables to predict continuous outcomes. </span><span class="koboSpan" id="kobo.169.2">Simple linear regression involves just one independent and one dependent variable. </span><span class="koboSpan" id="kobo.169.3">An example of a linear regression might be predicting house prices based on its </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">square footage.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.171.1">Logistic regression</span></strong><span class="koboSpan" id="kobo.172.1">: Used for binary</span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.173.1"> outcomes (e.g., yes/no, true/false), logistic regression models the probability of a categorical dependent variable based on one or more independent variables, ideal for binary classification tasks. </span><span class="koboSpan" id="kobo.173.2">Logistic regression can be used in spam email detection by building a model based on the features of messages such as keywords that indicate spam content, source IP address, length of the email, volume of misspelled words, or </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">other characteristics.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.175.1">Naïve Bayes</span></strong><span class="koboSpan" id="kobo.176.1">: Based on Bayes’ theorem, this</span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.177.1"> technique assumes independence among predictors and is effective for text classification, spam detection, and recommendation systems. </span><span class="koboSpan" id="kobo.177.2">Variants include multinomial, Bernoulli, and Gaussian Naïve Bayes. </span><span class="koboSpan" id="kobo.177.3">Bayes algorithms are frequently used in text classification tasks such as spam detection and </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">sentiment analysis.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.179.1">Random forests</span></strong><span class="koboSpan" id="kobo.180.1">: This ensemble method </span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.181.1">uses multiple decision trees to make more reliable and accurate predictions by averaging their results, effectively reducing overfitting and variance in predictions. </span><span class="koboSpan" id="kobo.181.2">A common use case of a random forest algorithm might be detecting if a customer is likely to leave a subscription service (or churn) based on a number of features (number of calls made to support, length of calls, length of subscription service, telemetry data of how often the service used). </span><span class="koboSpan" id="kobo.181.3">Each of those features can be evaluated in a decision tree and then</span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.182.1"> used together to predict a customer’s </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">churn potential.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.184.1">Support Vector Machines</span></strong><span class="koboSpan" id="kobo.185.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.186.1">SVM</span></strong><span class="koboSpan" id="kobo.187.1">): SVMs are </span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.188.1">used for classification and regression by finding the optimal boundary (hyperplane) that maximizes the margin or distance between </span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.189.1">different classes of data points, enhancing the model’s discriminative power. </span><span class="koboSpan" id="kobo.189.2">Imagine you have a collection of points on a graph that you need to divide into two groups. </span><span class="koboSpan" id="kobo.189.3">An SVM would determine how to draw a line that best separates them. </span><span class="koboSpan" id="kobo.189.4">The optimal line path would be one that divides the points on the plane, maximizing the gaps between the line and points. </span><span class="koboSpan" id="kobo.189.5">See </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.190.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.191.1">.2</span></em><span class="koboSpan" id="kobo.192.1"> for a very </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">simple example:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer021">
<span class="koboSpan" id="kobo.194.1"><img alt="Figure 3.2 – Simple representation of a support vector machine model" src="image/B22207_03_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.195.1">Figure 3.2 – Simple representation of a support vector machine model</span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.196.1">Further reading</span></p>
<p class="callout"><span class="koboSpan" id="kobo.197.1">While you won’t see all of these individual algorithms on the AI-900 exam, they’re neat to learn about. </span><span class="koboSpan" id="kobo.197.2">You can explore the basic mathematical concepts behind many of these algorithms at sites such as </span><a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/"><span class="koboSpan" id="kobo.198.1">https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/</span></a> <span class="No-Break"><span class="koboSpan" id="kobo.199.1">and </span></span><a href="https://www.kdnuggets.com/a-beginner-guide-to-the-top-10-machine-learning-algorithms"><span class="No-Break"><span class="koboSpan" id="kobo.200.1">https://www.kdnuggets.com/a-beginner-guide-to-the-top-10-machine-learning-algorithms</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.201.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.202.1">Unsupervised machine learning techniques</span></h3>
<p><span class="koboSpan" id="kobo.203.1">Unsupervised learning models are trained </span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.204.1">on data without labels, aiming to find underlying patterns or groupings in the data based </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">on </span></span><span class="No-Break"><a id="_idIndexMarker118"/></span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">similarities.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.207.1">Clustering</span></strong><span class="koboSpan" id="kobo.208.1"> is a primary technique in </span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.209.1">unsupervised learning that groups data points based on feature similarities. </span><span class="koboSpan" id="kobo.209.2">Examples include categorizing different types of flowers or segmenting customers by purchasing habits. </span><span class="koboSpan" id="kobo.209.3">Unlike classification, clustering does not require pre-defined categories; the algorithm identifies these groups autonomously. </span><span class="koboSpan" id="kobo.209.4">Clustering can also be a preliminary step to define classes for a subsequent classification model, such as segmenting customers into categories for targeted </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">marketing strategies.</span></span></p>
<h3><span class="koboSpan" id="kobo.211.1">Semi-supervised machine learning techniques</span></h3>
<p><span class="koboSpan" id="kobo.212.1">Semi-supervised learning occupies a </span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.213.1">space between </span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.214.1">supervised and unsupervised learning. </span><span class="koboSpan" id="kobo.214.2">This technique combines both aspects of supervised learning (providing labeled input data) as well as unsupervised learning (training with </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">unlabeled data).</span></span></p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.216.1">Inferencing</span></h2>
<p><span class="koboSpan" id="kobo.217.1">Once the training phase is complete, the model</span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.218.1"> can be used for </span><strong class="bold"><span class="koboSpan" id="kobo.219.1">inferencing</span></strong><span class="koboSpan" id="kobo.220.1"> or making predictions. </span><span class="koboSpan" id="kobo.220.2">The model acts as a software program encapsulating the learned function, allowing users to input feature values and </span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.221.1">receive predictions of corresponding labels. </span><span class="koboSpan" id="kobo.221.2">The predicted label is represented by </span><em class="italic"><span class="koboSpan" id="kobo.222.1">ŷ</span></em><span class="koboSpan" id="kobo.223.1"> (pronounced “y-hat”) to distinguish it from </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">observed values.</span></span></p>
<p><span class="koboSpan" id="kobo.225.1">Understanding machine learning involves grasping these fundamental concepts of training and inferencing, as well as recognizing the role of algorithms in establishing predictive relationships between features and labels. </span><span class="koboSpan" id="kobo.225.2">By applying mathematical functions to data, machine learning models can make predictions and facilitate decision-making in various domains, from weather forecasting to </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">medical diagnosis.</span></span></p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.227.1">Identify regression machine learning scenarios</span></h1>
<p><span class="koboSpan" id="kobo.228.1">Regression models aim to forecast numerical outcomes using training data that encompasses input features along with their corresponding target values. </span><span class="koboSpan" id="kobo.228.2">The development of a regression</span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.229.1"> model, as with any supervised learning approach, unfolds through several cycles. </span><span class="koboSpan" id="kobo.229.2">In each cycle, you select a suitable algorithm—often configurable with various parameters—to build the model. </span><span class="koboSpan" id="kobo.229.3">You then assess how well the model predicts outcomes and adjust it by experimenting with alternative algorithms and tuning the parameters. </span><span class="koboSpan" id="kobo.229.4">This iterative process continues until the model reaches a satisfactory level of </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">prediction accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.231.1">The overall process for regression training is </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.233.1">Divide the training data randomly to form a training set for model development, reserving a portion for model validation. </span><span class="koboSpan" id="kobo.233.2">For example, consider setting aside 30-50% of the training data to test </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">against later.</span></span></li>
<li><span class="koboSpan" id="kobo.235.1">Employ a fitting algorithm, such as linear regression for regression models, to construct the model based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">training set.</span></span></li>
<li><span class="koboSpan" id="kobo.237.1">Then, use the set aside validation data from </span><em class="italic"><span class="koboSpan" id="kobo.238.1">step 1</span></em><span class="koboSpan" id="kobo.239.1"> to evaluate the model’s effectiveness by making predictions and comparing these predicted values against the actual labels in the </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">validation set.</span></span></li>
<li><span class="koboSpan" id="kobo.241.1">Summarize the discrepancies between predicted and actual values to derive a performance metric reflecting the model’s </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">prediction accuracy.</span></span></li>
<li><span class="koboSpan" id="kobo.243.1">Iterate this train–validate–evaluate cycle, experimenting with various algorithms and settings, until the model’s performance matches </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">your expectations.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.245.1">Through these steps, you can build regression models to predict a number of </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">real-world scenarios.</span></span></p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.247.1">Example</span></h2>
<p><span class="koboSpan" id="kobo.248.1">Earlier, we discussed an example</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.249.1"> of predicting bottled water sales based on how warm it is. </span><span class="koboSpan" id="kobo.249.2">To see how regression training works, let’s dive into the bottled water </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">sales example.</span></span></p>
<p><span class="koboSpan" id="kobo.251.1">Let’s say you want to predict, based on the outside temperature, how many bottles of water you anticipate selling. </span><span class="koboSpan" id="kobo.251.2">This would be important to you as a vendor since it helps you understand how much you need to stock to meet </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">the demand.</span></span></p>
<p><span class="koboSpan" id="kobo.253.1">First, you need to gather historical </span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.254.1">data that will be used to train the model (as well as validate the model later). </span><span class="koboSpan" id="kobo.254.2">Take the following sample data set in </span><em class="italic"><span class="koboSpan" id="kobo.255.1">Table 3.1</span></em><span class="koboSpan" id="kobo.256.1">—it captures two critical pieces of data: how many bottles of water sold (</span><em class="italic"><span class="koboSpan" id="kobo.257.1">y</span></em><span class="koboSpan" id="kobo.258.1">) at a given </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">temperature (</span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.260.1">x</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">):</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.262.1">Sample</span></strong></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.263.1">Temperature (x)</span></strong></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.264.1">Bottled water </span></strong><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.265.1">sales (y)</span></strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.266.1">1</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.267.1">50</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.268.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.269.1">2</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.270.1">53</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.271.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.272.1">3</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.273.1">62</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.274.1">5</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.275.1">4</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.276.1">63</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.277.1">7</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.278.1">5</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.279.1">65</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.280.1">9</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.281.1">6</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.282.1">68</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.283.1">12</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.284.1">7</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.285.1">70</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.286.1">18</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.287.1">8</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.288.1">74</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.289.1">22</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.290.1">9</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.291.1">77</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.292.1">28</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.293.1">10</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.294.1">84</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.295.1">36</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.296.1">11</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.297.1">64</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.298.1">7</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.299.1">12</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.300.1">78</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.301.1">33</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.302.1">13</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.303.1">81</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.304.1">34</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.305.1">14</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.306.1">79</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.307.1">31</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.308.1">15</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.309.1">54</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.310.1">2</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.311.1">Table 3.1 – Bottled water sales</span></p>
<p><span class="koboSpan" id="kobo.312.1">The next step is to select the </span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.313.1">amount of data that we’ll use for training and the amount we’ll set aside for validation and testing. </span><span class="koboSpan" id="kobo.313.2">Let’s go ahead and take the first 10 rows for training our fictional model, leaving the last 5 rows </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">for validation.</span></span></p>
<p><span class="koboSpan" id="kobo.315.1">In this case, an easy way to understand the relationship between temperature and bottles of water sold is to plot them on a simple graph, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.316.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.317.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<span class="koboSpan" id="kobo.319.1"><img alt="Figure 3.3 – Temperature and bottles of water sold plotted on a graph" src="image/B22207_03_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.320.1">Figure 3.3 – Temperature and bottles of water sold plotted on a graph</span></p>
<p><span class="koboSpan" id="kobo.321.1">Through the training process, an </span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.322.1">algorithm applies a formula or function to calculate the value of </span><em class="italic"><span class="koboSpan" id="kobo.323.1">y</span></em><span class="koboSpan" id="kobo.324.1"> from the value of </span><em class="italic"><span class="koboSpan" id="kobo.325.1">x</span></em><span class="koboSpan" id="kobo.326.1">. </span><span class="koboSpan" id="kobo.326.2">In this case, the algorithm used would be one of linear regression—one that calculates a straight line through the points. </span><span class="koboSpan" id="kobo.326.3">See </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.327.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.328.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<span class="koboSpan" id="kobo.330.1"><img alt="Figure 3.4 – Linear regression" src="image/B22207_03_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.331.1">Figure 3.4 – Linear regression</span></p>
<p><span class="koboSpan" id="kobo.332.1">As you can see from the graph, for every degree of change, the historical data trends upward. </span><span class="koboSpan" id="kobo.332.2">The slope can be expressed using the equation </span><em class="italic"><span class="koboSpan" id="kobo.333.1">y</span></em><span class="koboSpan" id="kobo.334.1"> = 1.11</span><em class="italic"><span class="koboSpan" id="kobo.335.1">x</span></em><span class="koboSpan" id="kobo.336.1"> – 60.02, where </span><em class="italic"><span class="koboSpan" id="kobo.337.1">x</span></em><span class="koboSpan" id="kobo.338.1"> is the temperature and </span><em class="italic"><span class="koboSpan" id="kobo.339.1">y</span></em><span class="koboSpan" id="kobo.340.1"> is the</span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.341.1"> number of water bottles sold. </span><span class="koboSpan" id="kobo.341.2">Put another way, starting at 60 degrees, for every 1 degree increase in temperature, the number of water bottles sold increases </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">by 1.11.</span></span></p>
<p><span class="koboSpan" id="kobo.343.1">Given this formula that’s been developed, the next step is to test the formula based on the remaining data in the training set. </span><span class="koboSpan" id="kobo.343.2">In this example, the additional data left in the training set has been plotted on the same graph and a linear regression run against them </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">as well:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer024">
<span class="koboSpan" id="kobo.345.1"><img alt="Figure 3.5 – Validation data set composited on to original graph" src="image/B22207_03_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.346.1">Figure 3.5 – Validation data set composited on to original graph</span></p>
<p><span class="koboSpan" id="kobo.347.1">To test the formula, you could take a value from the held back training data (such as 81 degrees) and project the number of water bottles sold by calculating </span><em class="italic"><span class="koboSpan" id="kobo.348.1">y</span></em><span class="koboSpan" id="kobo.349.1"> = 1.11(81) – 60.02, which results in 30 water</span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.350.1"> bottles (or 29.91 rounded up to the next </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">whole number).</span></span></p>
<p><span class="koboSpan" id="kobo.352.1">You can repeat that process for every temperature value held back in the data set, using the function to calculate a prediction for the number of water bottles sold. </span><span class="koboSpan" id="kobo.352.2">See </span><em class="italic"><span class="koboSpan" id="kobo.353.1">Table 3.2</span></em><span class="koboSpan" id="kobo.354.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">an example:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.356.1">Sample</span></strong></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.357.1">Temperature</span></strong></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.358.1">Bottles of </span></strong><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.359.1">water sold</span></strong></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.360.1">Prediction</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.361.1">11</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.362.1">64</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.363.1">7</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.364.1">12</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.365.1">12</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.366.1">78</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.367.1">33</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.368.1">27</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.369.1">13</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.370.1">81</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.371.1">34</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.372.1">29</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.373.1">14</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.374.1">79</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.375.1">31</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.376.1">28</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.377.1">15</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.378.1">54</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.379.1">2</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.380.1">0</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.381.1">Table 3.2 – Predicting the water bottles sold with the training data</span></p>
<p><span class="koboSpan" id="kobo.382.1">How do you express the accuracy of the model? </span><span class="koboSpan" id="kobo.382.2">If you’re not sure how accurate your model is, read on! </span><span class="koboSpan" id="kobo.382.3">There are a few </span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.383.1">metrics that can be used to help understand how accurate (or inaccurate) the model and its </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">predictions are.</span></span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.385.1">Evaluation metrics</span></h2>
<p><span class="koboSpan" id="kobo.386.1">Evaluation metrics are statistical formulas </span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.387.1">used to evaluate the validity of predictions against a </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">data set.</span></span></p>
<h3><span class="koboSpan" id="kobo.389.1">Mean Absolute Error (MAE)</span></h3>
<p><span class="koboSpan" id="kobo.390.1">This </span><strong class="bold"><span class="koboSpan" id="kobo.391.1">mean absolute error</span></strong><span class="koboSpan" id="kobo.392.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.393.1">MAE</span></strong><span class="koboSpan" id="kobo.394.1">) represents</span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.395.1"> how many units of variance there are (either positive or negative) on average. </span><span class="koboSpan" id="kobo.395.2">For example, in sample </span><em class="italic"><span class="koboSpan" id="kobo.396.1">11</span></em><span class="koboSpan" id="kobo.397.1"> from </span><em class="italic"><span class="koboSpan" id="kobo.398.1">Table 3.2</span></em><span class="koboSpan" id="kobo.399.1">, the prediction was to sell 12 bottles of water. </span><span class="koboSpan" id="kobo.399.2">The </span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.400.1">actual value sold (based on the training data) was seven, meaning that the prediction was five units higher than actual. </span><span class="koboSpan" id="kobo.400.2">This variance is known as the </span><strong class="bold"><span class="koboSpan" id="kobo.401.1">absolute error</span></strong><span class="koboSpan" id="kobo.402.1">. </span><span class="koboSpan" id="kobo.402.2">In </span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.403.1">sample 12, the prediction was for 27 bottles to be sold, but the actual number sold was 33 (6 units higher, or an absolute error </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">of 6).</span></span></p>
<p><span class="koboSpan" id="kobo.405.1">To calculate the MAE, add up all the absolute error values and divide by the number of samples in the validation set. </span><span class="koboSpan" id="kobo.405.2">In </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">this case:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table003">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.407.1">Sample</span></strong></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.408.1">Bottles of </span></strong><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.409.1">water sold</span></strong></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.410.1">Prediction</span></strong></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.411.1"> (ŷ)</span></strong></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.412.1">Absolute error</span></strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.413.1">11</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.414.1">7</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.415.1">12</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.416.1">5</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.417.1">12</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.418.1">33</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.419.1">27</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.420.1">6</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.421.1">13</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.422.1">34</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.423.1">29</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.424.1">5</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.425.1">14</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.426.1">31</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.427.1">28</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.428.1">4</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.429.1">15</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.430.1">2</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.431.1">0</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.432.1">2</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.433.1">Table 3.3 – Absolute error table</span></p>
<p><span class="koboSpan" id="kobo.434.1">The total of all of the absolute errors is 22, and the number of items in the validation set was 5, resulting in an</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.435.1"> MAE for</span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.436.1"> the validation set of 4.4 (22 divided </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">by 5).</span></span></p>
<h3><span class="koboSpan" id="kobo.438.1">Mean squared error</span></h3>
<p><span class="koboSpan" id="kobo.439.1">One of the drawbacks of the mean absolute error is that it treats all discrepancies equally. </span><span class="koboSpan" id="kobo.439.2">While the overall average error rate</span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.440.1"> may be acceptable, there are industries or scenarios where it’s more desirable to have more (but smaller) errors as opposed to fewer (but larger) errors. </span><span class="koboSpan" id="kobo.440.2">For example, with fresh produce, it’s very undesirable to overstock because you have a higher likelihood of having to throw away larger quantities of </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">expired food.</span></span></p>
<p><span class="koboSpan" id="kobo.442.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.443.1">mean squared error</span></strong><span class="koboSpan" id="kobo.444.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.445.1">MSE</span></strong><span class="koboSpan" id="kobo.446.1">) functions as a</span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.447.1"> measure of the quality of the model itself. </span><span class="koboSpan" id="kobo.447.2">This metric gives individual errors more weight—and larger error predictions in the training data result in a much </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">higher MSE.</span></span></p>
<p><span class="koboSpan" id="kobo.449.1">To calculate this, each absolute error value is squared, and then the sum of those values is averaged. </span><span class="koboSpan" id="kobo.449.2">Using the training data output in </span><em class="italic"><span class="koboSpan" id="kobo.450.1">Table 3.3</span></em><span class="koboSpan" id="kobo.451.1">, the mean squared error value is 21.2, highlighting the fact that the model may need tweaking or more </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">training data.</span></span></p>
<h3><span class="koboSpan" id="kobo.453.1">Root Mean Squared Error (RMSE)</span></h3>
<p><span class="koboSpan" id="kobo.454.1">The next metric is </span><strong class="bold"><span class="koboSpan" id="kobo.455.1">root mean squared error</span></strong><span class="koboSpan" id="kobo.456.1">, which is the square root of the MSE. </span><span class="koboSpan" id="kobo.456.2">While the MSE provides a measure of the quality </span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.457.1">of the predictor function, the RMSE is converted back to the </span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.458.1">original unit, making it a little easier to interpret and communicate the model’s performance. </span><span class="koboSpan" id="kobo.458.2">RMSE is sensitive to outliers and gives relatively high weight to large errors. </span><span class="koboSpan" id="kobo.458.3">Like MSE, a smaller RMSE indicates a </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">better fit.</span></span></p>
<p><span class="koboSpan" id="kobo.460.1">With our validation data set, the RMSE </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">is 4.6.</span></span></p>
<h3><span class="koboSpan" id="kobo.462.1">Coefficient of determination</span></h3>
<p><span class="koboSpan" id="kobo.463.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.464.1">coefficient of determination</span></strong><span class="koboSpan" id="kobo.465.1">, known as </span><em class="italic"><span class="koboSpan" id="kobo.466.1">R</span></em><span class="superscript"><span class="koboSpan" id="kobo.467.1">2</span></span><span class="koboSpan" id="kobo.468.1">, measures how well a statistical model predicts the actual outcome. </span><span class="koboSpan" id="kobo.468.2">It’s a score between 0 </span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.469.1">and 1 that tells us the percentage of the variation in our target variable (what we’re trying to predict) that can be explained by the model. </span><span class="koboSpan" id="kobo.469.2">A score </span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.470.1">of 1 means the model predicts perfectly, with no difference between predicted and actual values, while a score of 0 means the model doesn’t explain any of </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">the variation.</span></span></p>
<p><span class="koboSpan" id="kobo.472.1">An </span><em class="italic"><span class="koboSpan" id="kobo.473.1">R</span></em><span class="superscript"><span class="koboSpan" id="kobo.474.1">2</span></span><span class="koboSpan" id="kobo.475.1"> closer to 1 indicates a model that fits our data well, suggesting a strong relationship between our input variables and the outcome. </span><span class="koboSpan" id="kobo.475.2">However, a high </span><em class="italic"><span class="koboSpan" id="kobo.476.1">R</span></em><span class="superscript"><span class="koboSpan" id="kobo.477.1">2</span></span><span class="koboSpan" id="kobo.478.1"> alone doesn’t guarantee that the model is accurate or useful for making predictions, especially in complex models with many variables. </span><span class="koboSpan" id="kobo.478.2">It’s essential to look beyond </span><em class="italic"><span class="koboSpan" id="kobo.479.1">R</span></em><span class="superscript"><span class="koboSpan" id="kobo.480.1">2</span></span><span class="koboSpan" id="kobo.481.1"> to ensure the model isn’t just fitting the noise in our data (overfitting), which could lead to misleading results. </span><span class="koboSpan" id="kobo.481.2">So, while </span><em class="italic"><span class="koboSpan" id="kobo.482.1">R</span></em><span class="superscript"><span class="koboSpan" id="kobo.483.1">2</span></span><span class="koboSpan" id="kobo.484.1"> is a helpful indicator of model fit, it’s just one piece of the puzzle in evaluating a </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">model’s performance.</span></span></p>
<p><span class="koboSpan" id="kobo.486.1">The coefficient of determination for our sample validation data set is 0.898—essentially </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">89.8% accurate.</span></span></p>
<p><span class="koboSpan" id="kobo.488.1">Based on that, it appears that our model may be pretty good (for bottled water). </span><span class="koboSpan" id="kobo.488.2">Since bottled water isn’t as perishable of a product as say, strawberries, stocking a little extra water likely won’t result in a </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">product loss.</span></span></p>
<p><span class="koboSpan" id="kobo.490.1">If, however, we were talking about more volatile products, you would probably perform some </span><strong class="bold"><span class="koboSpan" id="kobo.491.1">iterative training</span></strong><span class="koboSpan" id="kobo.492.1"> (that is</span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.493.1"> repeated training sessions) by varying the input data sets, regression algorithms, and </span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.494.1">algorithm </span><strong class="bold"><span class="koboSpan" id="kobo.495.1">hyperparameters</span></strong><span class="koboSpan" id="kobo.496.1"> (parameters that control how the algorithm works, as opposed to the data supplied to the algorithm) to come up with a function that more accurately predicts the </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">sales outcomes.</span></span></p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.498.1">Applications</span></h2>
<p><span class="koboSpan" id="kobo.499.1">So where is regression</span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.500.1"> machine learning useful? </span><span class="koboSpan" id="kobo.500.2">As you can see from the bottled water example, it’s useful for making a number of predictions, especially when projecting sales, quotas, housing prices, stock prices, and other forecasting activities. </span><span class="koboSpan" id="kobo.500.3">It’s also useful in analyzing user trends for advertising or </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">media services.</span></span></p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.502.1">Identify classification machine learning scenarios</span></h1>
<p><span class="koboSpan" id="kobo.503.1">Classification is a supervised machine learning</span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.504.1"> technique that essentially puts values into groups (classes) based on a criteria. </span><span class="koboSpan" id="kobo.504.2">There are two main types of classification techniques: binary and multiclass. </span><span class="koboSpan" id="kobo.504.3">Let’s look at both </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">of them.</span></span></p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor052"/><span class="koboSpan" id="kobo.506.1">Binary classification</span></h2>
<p><span class="koboSpan" id="kobo.507.1">You may have heard the terminology </span><strong class="bold"><span class="koboSpan" id="kobo.508.1">binary</span></strong><span class="koboSpan" id="kobo.509.1"> before and know that it’s the language of ones and zeroes that computers use to</span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.510.1"> process information. </span><span class="koboSpan" id="kobo.510.2">Binary simply means that a data item can be set to one of two values. </span><span class="koboSpan" id="kobo.510.3">For example, 0 or 1 and true or</span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.511.1"> false are </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">binary selections.</span></span></p>
<p><span class="koboSpan" id="kobo.513.1">In the machine learning context, binary classification works similarly—using the value of a feature (</span><em class="italic"><span class="koboSpan" id="kobo.514.1">x</span></em><span class="koboSpan" id="kobo.515.1">, just like in regression machine learning), the model predicts whether a label (</span><em class="italic"><span class="koboSpan" id="kobo.516.1">y</span></em><span class="koboSpan" id="kobo.517.1">) is 0 or 1. </span><span class="koboSpan" id="kobo.517.2">Binary classification categories data into mutually exclusive groups based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">feature data.</span></span></p>
<h3><span class="koboSpan" id="kobo.519.1">Example</span></h3>
<p><span class="koboSpan" id="kobo.520.1">Let’s look at a sample data set that </span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.521.1">might be used for training a binary classification model on whether a patient might be at risk of heart disease based on their LDL cholesterol level. </span><span class="koboSpan" id="kobo.521.2">Just like the regressive data set we looked at earlier, we’ll have a feature (</span><em class="italic"><span class="koboSpan" id="kobo.522.1">x</span></em><span class="koboSpan" id="kobo.523.1">) column containing data measurements and a corresponding </span><a id="_idIndexMarker151"/><span class="No-Break"><span class="koboSpan" id="kobo.524.1">label (</span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.525.1">y</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">):</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table004">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.527.1">Patient ID</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.528.1">LDL cholesterol level (x) (measured </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.529.1">in mg/DL)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.530.1">Heart </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.531.1">disease (y)</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.532.1">0 = no, 1 = </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.533.1">yes</span></strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.534.1">1</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.535.1">100</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.536.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.537.1">2</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.538.1">87</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.539.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.540.1">3</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.541.1">132</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.542.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.543.1">4</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.544.1">159</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.545.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.546.1">5</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.547.1">152</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.548.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.549.1">6</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.550.1">171</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.551.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.552.1">7</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.553.1">188</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.554.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.555.1">8</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.556.1">161</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.557.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.558.1">9</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.559.1">118</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.560.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.561.1">10</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.562.1">141</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.563.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.564.1">11</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.565.1">102</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.566.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.567.1">12</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.568.1">144</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.569.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.570.1">13</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.571.1">155</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.572.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.573.1">14</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.574.1">167</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.575.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.576.1">15</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.577.1">142</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.578.1">1</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.579.1">Table 3.4 – Binary classification data for heart disease patients</span></p>
<p><span class="koboSpan" id="kobo.580.1">Just as with the regression model techniques, there are many algorithms that can be used with binary classification. </span><span class="koboSpan" id="kobo.580.2">One </span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.581.1">popular algorithm is </span><em class="italic"><span class="koboSpan" id="kobo.582.1">logistic regression</span></em><span class="koboSpan" id="kobo.583.1"> (which, despite its name, is not an algorithm for regression-based models), which is typically identified by its sigmoid (S-shaped) function graph depicting values between 0 and 1. </span><span class="koboSpan" id="kobo.583.2">See </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.584.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.585.1">.6</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer025">
<span class="koboSpan" id="kobo.587.1"><img alt="Figure 3.6 – An example of a Sigmoid function graph" src="image/B22207_03_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.588.1">Figure 3.6 – An example of a Sigmoid function graph</span></p>
<p><span class="koboSpan" id="kobo.589.1">Like a regression algorithm, the resultant function can be expressed mathematically. </span><span class="koboSpan" id="kobo.589.2">In the graph, the </span><em class="italic"><span class="koboSpan" id="kobo.590.1">y</span></em><span class="koboSpan" id="kobo.591.1"> axis represents the</span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.592.1"> probability of a label being true (ranked 0 to 1), using the </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">following expression:</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.594.1">f(x) = P(y=1 | </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.595.1">x)</span></em></span></p>
<p><span class="koboSpan" id="kobo.596.1">The training data set in </span><em class="italic"><span class="koboSpan" id="kobo.597.1">Table 3.4</span></em><span class="koboSpan" id="kobo.598.1"> shows seven patients that definitely do not have heart disease and eight patients that do. </span><span class="koboSpan" id="kobo.598.2">The graph depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.599.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.600.1">.6</span></em><span class="koboSpan" id="kobo.601.1"> also shows an optional horizontal line that</span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.602.1"> can indicate the </span><strong class="bold"><span class="koboSpan" id="kobo.603.1">threshold</span></strong><span class="koboSpan" id="kobo.604.1"> at which a prediction switches from false </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">to true.</span></span></p>
<p><span class="koboSpan" id="kobo.606.1">As with the regression model training, you should divide the dataset into two selections: one piece to be used for training and the other </span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">for validation.</span></span></p>
<p><span class="koboSpan" id="kobo.608.1">Plugging the data into a simple binary classification function, you may plot a graph similar to the one in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.609.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.610.1">.7</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">.</span></span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<span class="koboSpan" id="kobo.612.1"><img alt="Figure 3.7 – A sample binary classification graph" src="image/B22207_03_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.613.1">Figure 3.7 – A sample binary classification graph</span></p>
<p><span class="koboSpan" id="kobo.614.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.615.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.616.1">.7</span></em><span class="koboSpan" id="kobo.617.1">, the plotted points indicate the patient’s LDL cholesterol reading (plotted along the </span><em class="italic"><span class="koboSpan" id="kobo.618.1">x</span></em><span class="koboSpan" id="kobo.619.1"> axis)., and their location</span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.620.1"> on the </span><em class="italic"><span class="koboSpan" id="kobo.621.1">y</span></em><span class="koboSpan" id="kobo.622.1"> axis indicates if the patient had heart disease or not, where 0 represents “no” and 1 </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">represents “yes.”</span></span></p>
<p><span class="koboSpan" id="kobo.624.1">Using the same method for validating the algorithm with the reserved training data as we did with regression training, you can do the same thing with binary classification. </span><span class="koboSpan" id="kobo.624.2">With the model created, predicting heart disease based on the features (LDL cholesterol levels) should </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">be easy:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table005">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.626.1">Patient ID</span></strong></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><strong class="bold"><span class="koboSpan" id="kobo.627.1">LDL cholesterol level (x) (measured </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.628.1">in mg/DL)</span></strong></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.629.1">Prediction (ŷ)</span></strong></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><strong class="bold"><span class="koboSpan" id="kobo.630.1">Heart </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.631.1">disease (y)</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.632.1">0 = no, 1 = </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.633.1">yes</span></strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.634.1">11</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.635.1">102</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.636.1">0</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.637.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.638.1">12</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.639.1">144</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.640.1">0</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.641.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.642.1">13</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.643.1">155</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.644.1">1</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.645.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.646.1">14</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.647.1">167</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.648.1">1</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.649.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.650.1">15</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.651.1">142</span></span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.652.1">0</span></p>
</td>
<td class="No-Table-Style T---Body">
<p><span class="koboSpan" id="kobo.653.1">1</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.654.1">Table 3.5 – Predictions for heart disease based on cholesterol</span></p>
<p><span class="koboSpan" id="kobo.655.1">Next, let’s look at how to evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">the data.</span></span></p>
<h3><span class="koboSpan" id="kobo.657.1">Evaluation metrics</span></h3>
<p><span class="koboSpan" id="kobo.658.1">When evaluating a model, it’s important to be </span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.659.1">able to determine not only </span><em class="italic"><span class="koboSpan" id="kobo.660.1">where</span></em><span class="koboSpan" id="kobo.661.1"> the predictions were right and wrong but to understand </span><em class="italic"><span class="koboSpan" id="kobo.662.1">how</span></em><span class="koboSpan" id="kobo.663.1"> they were right or wrong. </span><span class="koboSpan" id="kobo.663.2">These</span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.664.1"> results can </span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.665.1">be expressed in a graph called a </span><strong class="bold"><span class="koboSpan" id="kobo.666.1">confusion matrix</span></strong><span class="koboSpan" id="kobo.667.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.668.1">confusion diagram</span></strong><span class="koboSpan" id="kobo.669.1">, as</span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.670.1"> shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.671.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.672.1">.8</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<span class="koboSpan" id="kobo.674.1"><img alt="Figure 3.8 – Confusion matrix" src="image/B22207_03_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.675.1">Figure 3.8 – Confusion matrix</span></p>
<p><span class="koboSpan" id="kobo.676.1">The data is broken into </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">four categories:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.678.1">Actual Yes</span></strong><span class="koboSpan" id="kobo.679.1">: The training data said the patient had </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">heart disease</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.681.1">Actual No</span></strong><span class="koboSpan" id="kobo.682.1">: The training data said the patient did not have </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">heart disease</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.684.1">Predicted Yes</span></strong><span class="koboSpan" id="kobo.685.1">: The model predicted based on the cholesterol level that the patient would have </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">heart disease</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.687.1">Predicted No</span></strong><span class="koboSpan" id="kobo.688.1">: The model predicted based on the cholesterol level that the patient would not have </span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">heart disease</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.690.1">Those are then laid out on a quadrant, and</span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.691.1"> the </span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.692.1">intersections are </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">labelled accordingly:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table006">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.694.1">Predicted No</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.695.1">Predicted Yes</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.696.1">Actual No</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.697.1">True negative</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.698.1">False positive</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.699.1">Actual Yes</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.700.1">False negative</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.701.1">True positive</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.702.1">Table 3.6 – Confusion matrix table</span></p>
<p><span class="koboSpan" id="kobo.703.1">Here’s how to interpret </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">the values:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.705.1">True Negatives</span></strong><span class="koboSpan" id="kobo.706.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.707.1">TN</span></strong><span class="koboSpan" id="kobo.708.1">): This is the number </span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.709.1">of instances where the model correctly predicted the absence of the condition (class 0). </span><span class="koboSpan" id="kobo.709.2">In our case, this would represent the number of patients correctly identified as not being at risk for </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">heart disease.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.711.1">False Positives</span></strong><span class="koboSpan" id="kobo.712.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.713.1">FP</span></strong><span class="koboSpan" id="kobo.714.1">): This is the </span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.715.1">number of instances where the model incorrectly predicted the presence of the condition (class 1). </span><span class="koboSpan" id="kobo.715.2">In our case, this would represent the number of patients incorrectly identified as being at risk for heart disease when they </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">are not.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.717.1">False Negatives</span></strong><span class="koboSpan" id="kobo.718.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.719.1">FN</span></strong><span class="koboSpan" id="kobo.720.1">): This is the</span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.721.1"> number of instances where the model incorrectly predicted the absence of the condition (class 0). </span><span class="koboSpan" id="kobo.721.2">In our case, this would represent the number of patients who are actually at risk for heart disease but were incorrectly identified as not being </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">at risk.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.723.1">True Positives</span></strong><span class="koboSpan" id="kobo.724.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.725.1">TP</span></strong><span class="koboSpan" id="kobo.726.1">): This is the number of instances where the model correctly predicted the presence of the</span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.727.1"> condition (class 1). </span><span class="koboSpan" id="kobo.727.2">In our case, this would represent the number of patients correctly identified as being at risk for </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">heart disease.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.729.1">These data points (the true </span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.730.1">negatives, true positives, false </span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.731.1">positives, and false negatives) can then be used to further evaluate how accurate the model is using </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">several metrics.</span></span></p>
<h4><span class="koboSpan" id="kobo.733.1">Accuracy</span></h4>
<p><strong class="bold"><span class="koboSpan" id="kobo.734.1">Accuracy</span></strong><span class="koboSpan" id="kobo.735.1"> is used to describe how often the model is </span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.736.1">correct</span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.737.1"> in general. </span><span class="koboSpan" id="kobo.737.2">The formula or computing accuracy is </span><em class="italic"><span class="koboSpan" id="kobo.738.1">(TP + TN) / (total)</span></em><span class="koboSpan" id="kobo.739.1">. </span><span class="koboSpan" id="kobo.739.2">In this case, the accuracy of the model </span><span class="No-Break"><span class="koboSpan" id="kobo.740.1">is 60%.</span></span></p>
<h4><span class="koboSpan" id="kobo.741.1">Recall</span></h4>
<p><strong class="bold"><span class="koboSpan" id="kobo.742.1">Recall</span></strong><span class="koboSpan" id="kobo.743.1"> (sometimes called </span><strong class="bold"><span class="koboSpan" id="kobo.744.1">sensitivity</span></strong><span class="koboSpan" id="kobo.745.1">) indicates </span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.746.1">how well the model identifies the positive class (1, or in this case, patients at risk</span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.747.1"> of heart disease). </span><span class="koboSpan" id="kobo.747.2">The formula is </span><em class="italic"><span class="koboSpan" id="kobo.748.1">TP / (TP + FN)</span></em><span class="koboSpan" id="kobo.749.1">. </span><span class="koboSpan" id="kobo.749.2">The recall for class 1 (with heart disease) </span><span class="No-Break"><span class="koboSpan" id="kobo.750.1">is 50%.</span></span></p>
<h4><span class="koboSpan" id="kobo.751.1">Precision</span></h4>
<p><strong class="bold"><span class="koboSpan" id="kobo.752.1">Precision</span></strong><span class="koboSpan" id="kobo.753.1"> is an</span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.754.1"> indicator of </span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.755.1">how often the model is right for each class prediction. </span><span class="koboSpan" id="kobo.755.2">For the positive class, this formula is TP / (TP + FP). </span><span class="koboSpan" id="kobo.755.3">For class 1 (heart disease), the model correctly predicts 100% of </span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">the time.</span></span></p>
<h4><span class="koboSpan" id="kobo.757.1">Specificity</span></h4>
<p><strong class="bold"><span class="koboSpan" id="kobo.758.1">Specificity</span></strong><span class="koboSpan" id="kobo.759.1"> measures how often a </span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.760.1">model is right for the negative class (0, or in this case, patients without heart disease). </span><span class="koboSpan" id="kobo.760.2">The </span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.761.1">formula is </span><em class="italic"><span class="koboSpan" id="kobo.762.1">TN / (TN + FP)</span></em><span class="koboSpan" id="kobo.763.1">. </span><span class="koboSpan" id="kobo.763.2">This model’s specificity for the negative class is 33%, indicating that it is only correct 33% of the time when predicting that a patient will not have </span><span class="No-Break"><span class="koboSpan" id="kobo.764.1">heart disease.</span></span></p>
<p><span class="koboSpan" id="kobo.765.1">If there were discrepancies in prediction versus actual data, it could suggest areas where the model might need improvement, such as collecting more diverse data, using a different model, or tuning the existing model. </span><span class="koboSpan" id="kobo.765.2">Understanding where the model fails can help in reducing false positives (incorrectly predicting</span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.766.1"> risk where there is none) and false negatives (missing out on identifying actual risk), which are particularly critical in </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">medical diagnostics.</span></span></p>
<h3><span class="koboSpan" id="kobo.768.1">Applications</span></h3>
<p><span class="koboSpan" id="kobo.769.1">Binary classifications are most useful when there are a limited number of factors influencing a true or false outcome. </span><span class="koboSpan" id="kobo.769.2">It is common in fields such as medicine, biology, technology, and chemistry when you’re trying to determine a yes/no or true/false result based on a small set </span><span class="No-Break"><span class="koboSpan" id="kobo.770.1">of variables.</span></span></p>
<p><span class="koboSpan" id="kobo.771.1">Common real-world examples of binary classification</span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.772.1"> include </span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.774.1">Email spam detection</span></strong><span class="koboSpan" id="kobo.775.1">: Classifying emails as either spam or not spam. </span><span class="koboSpan" id="kobo.775.2">This is one of the most common applications of binary classification, used by email services to filter out </span><span class="No-Break"><span class="koboSpan" id="kobo.776.1">unwanted messages.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.777.1">Medical diagnosis</span></strong><span class="koboSpan" id="kobo.778.1">: Diagnosing patients with a disease or condition as either positive (having the disease) or negative (not having the disease). </span><span class="koboSpan" id="kobo.778.2">For example, binary classification models can be used to detect the presence of a tumor as malignant or benign based on </span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">medical imaging.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.780.1">Credit approval</span></strong><span class="koboSpan" id="kobo.781.1">: Deciding whether to approve or decline a credit application. </span><span class="koboSpan" id="kobo.781.2">Financial institutions use binary classification algorithms to predict whether an applicant is likely to default on </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">a loan.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.783.1">Churn prediction</span></strong><span class="koboSpan" id="kobo.784.1">: Predicting whether a customer will churn (leave) or stay with a company or service. </span><span class="koboSpan" id="kobo.784.2">Companies use binary classification to identify at-risk customers and develop strategies to </span><span class="No-Break"><span class="koboSpan" id="kobo.785.1">retain them.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.786.1">Fraud detection</span></strong><span class="koboSpan" id="kobo.787.1">: Identifying transactions as fraudulent or legitimate. </span><span class="koboSpan" id="kobo.787.2">Banks and financial institutions use binary classification models to detect suspicious activities and </span><span class="No-Break"><span class="koboSpan" id="kobo.788.1">prevent fraud.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.789.1">Sentiment analysis</span></strong><span class="koboSpan" id="kobo.790.1">: Determining whether a piece of text (such as a product review or social media post) expresses a positive or negative sentiment. </span><span class="koboSpan" id="kobo.790.2">This is widely used in marketing and customer service to gauge public opinion and </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">customer satisfaction.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.792.1">Malware detection</span></strong><span class="koboSpan" id="kobo.793.1">: Classifying files or programs as malicious or safe, used by cybersecurity systems to</span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.794.1"> protect computers and networks from viruses and other </span><span class="No-Break"><span class="koboSpan" id="kobo.795.1">malicious software.</span></span></li>
</ul>
<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/><span class="koboSpan" id="kobo.796.1">Multiclass classification</span></h2>
<p><span class="koboSpan" id="kobo.797.1">Shifting gears a little bit, let’s look at </span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.798.1">multiclass classification. </span><span class="koboSpan" id="kobo.798.2">Like binary classification, it’s a probability method that assigns a </span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.799.1">classification based on a feature. </span><span class="koboSpan" id="kobo.799.2">However, instead of using a single feature or a binary label (true/false, 0/1), it can use multiple features and multiple classifications. </span><span class="koboSpan" id="kobo.799.3">The underlying ideas are the same, but let’s just look at a quick example of how </span><span class="No-Break"><span class="koboSpan" id="kobo.800.1">it works.</span></span></p>
<h3><span class="koboSpan" id="kobo.801.1">Example</span></h3>
<p><span class="koboSpan" id="kobo.802.1">In this example, we’ll be evaluating</span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.803.1"> peppers and their heat (or Scoville heat unit rating). </span><span class="koboSpan" id="kobo.803.2">Every pepper has a heat rating, ranked in Scoville units, that describes how spicy </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">it is.</span></span></p>
<p><span class="koboSpan" id="kobo.805.1">The data in </span><em class="italic"><span class="koboSpan" id="kobo.806.1">Table 3.7</span></em><span class="koboSpan" id="kobo.807.1"> depicts a variety of peppers and their average Scoville </span><span class="No-Break"><span class="koboSpan" id="kobo.808.1">heat units:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table007">
<colgroup>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.809.1">Scoville </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.810.1">heat units</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.811.1">Pepper</span></strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.812.1">1,500,000 – </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">2,200,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.814.1">Carolina Reaper</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.815.1">1,000,000 – </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">1,500,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.817.1">Trinidad Scorpion</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.818.1">855,000 – </span><span class="No-Break"><span class="koboSpan" id="kobo.819.1">1,000,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.820.1">Ghost pepper</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.821.1">350,000 – </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">577,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.823.1">Red </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">Savina Habanero</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.825.1">100,000 – </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">350,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.827.1">Habanero</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.828.1">70,000 – </span><span class="No-Break"><span class="koboSpan" id="kobo.829.1">100,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.830.1">Charleston hot</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.831.1">30,000 – </span><span class="No-Break"><span class="koboSpan" id="kobo.832.1">50,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.833.1">Cayenne pepper</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.834.1">10,000 – </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">23,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.836.1">Serrano pepper</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.837.1">8,000 – </span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">10,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.839.1">Hungarian pepper</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.840.1">2,000 – </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">7,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.842.1">Jalapeno pepper</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.843.1">0-100</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.844.1">Bell pepper</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.845.1">Table 3.7 – Selected peppers and their Scoville heat unit ratings</span></p>
<p><span class="koboSpan" id="kobo.846.1">Now, just like the binary</span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.847.1"> classification training, let’s generate a sample table of features (Scoville heat unit ratings) and </span><span class="No-Break"><span class="koboSpan" id="kobo.848.1">labels (peppers):</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table008">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.849.1">ID</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.850.1">Scoville heat </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.851.1">unit (x)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.852.1">Pepper (y)</span></strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.853.1">1</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.854.1">1,900,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.855.1">Carolina reaper</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.856.1">2</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.857.1">10</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.858.1">Bell</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.859.1">3</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.860.1">2,850</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.861.1">Jalapeno</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.862.1">4</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.863.1">447,700</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.864.1">Red Savina</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.865.1">5</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.866.1">8,700</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.867.1">Hungarian</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.868.1">6</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.869.1">127,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.870.1">Habanero</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.871.1">7</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.872.1">88,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.873.1">Charleston hot</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.874.1">8</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.875.1">289,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.876.1">Habanero</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.877.1">9</span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.878.1">0</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.879.1">Bell</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.880.1">10</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.881.1">900,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.882.1">Ghost</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.883.1">11</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.884.1">1,250,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.885.1">Scorpion</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.886.1">12</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.887.1">11,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.888.1">Serrano</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.889.1">13</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.890.1">42,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.891.1">Cayenne</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.892.1">14</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.893.1">7,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.894.1">Jalapeno</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.895.1">15</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.896.1">2,200,000</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.897.1">Carolina Reaper</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.898.1">Table 3.8 – Scoville rating sample data</span></p>
<p><span class="koboSpan" id="kobo.899.1">Once the training data is assembled, it’s time to use an algorithm to fit the training data to a function that will calculate</span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.900.1"> the probability for our classes. </span><span class="koboSpan" id="kobo.900.2">There are 11 possible answers (classes) for our data set based on our training data. </span><span class="koboSpan" id="kobo.900.3">They’re zero-indexed, meaning they’re numbered from 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">to 10.</span></span></p>
<p><span class="koboSpan" id="kobo.902.1">The most common algorithms in multiclass classification models are </span><strong class="bold"><span class="koboSpan" id="kobo.903.1">one-vs-rest</span></strong><span class="koboSpan" id="kobo.904.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.905.1">OvR</span></strong><span class="koboSpan" id="kobo.906.1">) and </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.907.1">multinominal</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.908.1"> algorithms.</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.909.1">One-vs-Rest</span></strong><span class="koboSpan" id="kobo.910.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.911.1">OvR</span></strong><span class="koboSpan" id="kobo.912.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.913.1">One-vs-All</span></strong><span class="koboSpan" id="kobo.914.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.915.1">OvA</span></strong><span class="koboSpan" id="kobo.916.1">): With this algorithm, you train a binary classification function for each class individually. </span><span class="koboSpan" id="kobo.916.2">Each function targets a specific class compared to any of the </span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.917.1">other classes in the set. </span><span class="koboSpan" id="kobo.917.2">In this case, since there are 11 pepper types represented, the algorithm would create 11 binary classification functions. </span><span class="koboSpan" id="kobo.917.3">Like the binary classifications previously, the algorithm produces a sigmoid function. </span><span class="koboSpan" id="kobo.917.4">The outcome of this model predicts the class for the function that results in the highest probability output. </span><span class="koboSpan" id="kobo.917.5">OvR or OvA strategies could be</span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.918.1"> applied, for example, in a Carolina Reaper vs. </span><span class="koboSpan" id="kobo.918.2">all classifier—essentially determining if an element is a Carolina Reaper </span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">or not.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.920.1">Multinomial</span></strong><span class="koboSpan" id="kobo.921.1">: This algorithm approaches the problem differently, creating a single function with a multivalued (or vector) output. </span><span class="koboSpan" id="kobo.921.2">This vector can contain a probability distribution across all the potential </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.922.1">classes, though it’s really designed for mutually exclusive data sets (for example, if a pepper’s Scoville rating can’t expand into another pepper’s range). </span><span class="koboSpan" id="kobo.922.2">With a vector containing multiple elements, each class element is scored individually between 0 and 1, with the total adding up </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">to 1.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.924.1">These types of models may need more information to reliably predict. </span><span class="koboSpan" id="kobo.924.2">For example, review the sample data in </span><em class="italic"><span class="koboSpan" id="kobo.925.1">Table 3.7</span></em><span class="koboSpan" id="kobo.926.1"> for the Scoville pepper ratings. </span><span class="koboSpan" id="kobo.926.2">As you can see, there are a few instances</span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.927.1"> where a pepper might have a similar heat rating to another pepper (either milder or spicier). </span><span class="koboSpan" id="kobo.927.2">A larger training data set would help the model understand concepts of frequency to help more accurately predict and select an appropriate classification strategy (depending on the factors you want to identify) would help you be most successful in this </span><span class="No-Break"><span class="koboSpan" id="kobo.928.1">classification methodology.</span></span></p>
<h3><span class="koboSpan" id="kobo.929.1">Evaluation metrics</span></h3>
<p><span class="koboSpan" id="kobo.930.1">Since multiclass classification can really be </span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.931.1">looked at as an extension of binary classification (in many cases), the same techniques and </span><span class="No-Break"><span class="koboSpan" id="kobo.932.1">terminology apply.</span></span></p>
<p><span class="koboSpan" id="kobo.933.1">For example, with the training data, you could develop a confusion matrix. </span><span class="koboSpan" id="kobo.933.2">The layout is very similar to the confusion matrix for binary classification—it’s just got more labels to deal with, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.934.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.935.1">.9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<span class="koboSpan" id="kobo.937.1"><img alt="Figure 3.9 – Multiclass confusion matrix based on Scoville pepper data" src="image/B22207_03_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.938.1">Figure 3.9 – Multiclass confusion matrix based on Scoville pepper data</span></p>
<h3><span class="koboSpan" id="kobo.939.1">Applications</span></h3>
<p><span class="koboSpan" id="kobo.940.1">Multiclass classification refers to the </span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.941.1">scenarios where you need to classify items into more than two categories. </span><span class="koboSpan" id="kobo.941.2">Unlike binary classification, which differentiates between two classes (such as yes/no or true/false), multiclass classification deals with situations where there are three or </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">more classes.</span></span></p>
<p><span class="koboSpan" id="kobo.943.1">Potential use cases include </span><span class="No-Break"><span class="koboSpan" id="kobo.944.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.945.1">Predicting animal types</span></strong><span class="koboSpan" id="kobo.946.1">: Suppose you have a dataset containing images of animals and you want to classify each image as a dog, cat, bird, or fish. </span><span class="koboSpan" id="kobo.946.2">This is a multiclass classification problem with </span><span class="No-Break"><span class="koboSpan" id="kobo.947.1">four classes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.948.1">Weather forecasting</span></strong><span class="koboSpan" id="kobo.949.1">: If you’re predicting whether the weather will be sunny, cloudy, rainy, or snowy, you’re dealing with </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">multiclass classification.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.951.1">Handwritten numeral recognition</span></strong><span class="koboSpan" id="kobo.952.1">: A classic example is the MNIST dataset, where the task is to classify images of handwritten digits into 10 classes (0 </span><span class="No-Break"><span class="koboSpan" id="kobo.953.1">through 9).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.954.1">Medical diagnosis</span></strong><span class="koboSpan" id="kobo.955.1">: Suppose a particular diagnostic test can indicate one of several different diseases. </span><span class="koboSpan" id="kobo.955.2">If you’re</span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.956.1"> developing a model to predict which specific disease (out of a possible set) a patient might have based on their symptoms and test results, you’re engaging in </span><span class="No-Break"><span class="koboSpan" id="kobo.957.1">multiclass classification.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.958.1">In multiclass classification, techniques and metrics are slightly different from binary classification in that you have to consider how the model performs across all the different classes (not just two), but overall the process is very similar. </span><span class="koboSpan" id="kobo.958.2">Metrics such as precision and recall are calculated for each class and then averaged in some way (such as micro, macro, or weighted average) to understand the </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">overall performance.</span></span></p>
<p><span class="koboSpan" id="kobo.960.1">A classification’s F1 score is a measure used to summarize the overall accuracy of a class. </span><span class="koboSpan" id="kobo.960.2">Previously, you learned</span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.961.1"> about </span><strong class="bold"><span class="koboSpan" id="kobo.962.1">precision</span></strong><span class="koboSpan" id="kobo.963.1"> (ratio of correctly predicted positive observations to the total positives using the formula </span><em class="italic"><span class="koboSpan" id="kobo.964.1">P = TP / (TP+FP)</span></em><span class="koboSpan" id="kobo.965.1">) and recall (ratio of correctly predicted positive observations to all the observations in the class, calculated using the formula </span><em class="italic"><span class="koboSpan" id="kobo.966.1">R = TP / (</span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.967.1">TP +FN)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.969.1">An F1 score is calculated using the formula </span><em class="italic"><span class="koboSpan" id="kobo.970.1">F1 = 2 * (P * R) / (P + </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.971.1">R)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.972.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.973.1">Up to this point, you’ve been learning about </span><strong class="bold"><span class="koboSpan" id="kobo.974.1">supervised learning </span></strong><span class="koboSpan" id="kobo.975.1">scenarios such as binary and multiclass classification. </span><span class="koboSpan" id="kobo.975.2">Next, we’ll be shifting to </span><span class="No-Break"><span class="koboSpan" id="kobo.976.1">unsupervised scenarios.</span></span></p>
<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/><span class="koboSpan" id="kobo.977.1">Identify clustering machine learning scenarios</span></h1>
<p><span class="koboSpan" id="kobo.978.1">Clustering is an unsupervised</span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.979.1"> machine learning scenario where algorithms are employed to try to identify patterns in data. </span><span class="koboSpan" id="kobo.979.2">Unlike supervised learning, where training data has labels and features, unsupervised learning does not. </span><span class="koboSpan" id="kobo.979.3">The main goal of clustering is to be able to let the machine learning algorithms discover natural groupings within the data based on the similarities in the data </span><span class="No-Break"><span class="koboSpan" id="kobo.980.1">points themselves.</span></span></p>
<p><span class="koboSpan" id="kobo.981.1">Just as supervised learning had its </span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.982.1">algorithms, there are several popular algorithms available to use with </span><span class="No-Break"><span class="koboSpan" id="kobo.983.1">clustering scenarios:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.984.1">K-means clustering</span></strong><span class="koboSpan" id="kobo.985.1">: This algorithm partitions the data into </span><em class="italic"><span class="koboSpan" id="kobo.986.1">K</span></em><span class="koboSpan" id="kobo.987.1"> distinct, non-overlapping subsets (or clusters) based on the </span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.988.1">mean distance from the centroid of each cluster. </span><span class="koboSpan" id="kobo.988.2">The value of </span><em class="italic"><span class="koboSpan" id="kobo.989.1">K</span></em><span class="koboSpan" id="kobo.990.1"> needs to be </span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">specified beforehand.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.992.1">Hierarchical clustering</span></strong><span class="koboSpan" id="kobo.993.1">: Builds a hierarchy</span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.994.1"> of clusters either with a bottom-up approach (agglomerative) or a top-down approach (divisive). </span><span class="koboSpan" id="kobo.994.2">It does not require pre-specification of the number </span><span class="No-Break"><span class="koboSpan" id="kobo.995.1">of clusters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.996.1">Density-Based Spatial Clustering of Applications with Noise </span></strong><span class="koboSpan" id="kobo.997.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.998.1">DBSCAN</span></strong><span class="koboSpan" id="kobo.999.1">): Forms clusters based on the</span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.1000.1"> density of data points, capable of discovering clusters of arbitrary shape and handling noise and </span><span class="No-Break"><span class="koboSpan" id="kobo.1001.1">outliers effectively.</span></span></li>
</ul>
<h2 id="_idParaDest-56"><strong class="bold"><a id="_idTextAnchor055"/><span class="koboSpan" id="kobo.1002.1">Example</span></strong></h2>
<p><span class="koboSpan" id="kobo.1003.1">Let’s say we have some</span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.1004.1"> sample data from grocery shoppers; 15 shoppers have put the items (or features, in this case) depicted in </span><em class="italic"><span class="koboSpan" id="kobo.1005.1">Table 3.9</span></em><span class="koboSpan" id="kobo.1006.1"> in </span><span class="No-Break"><span class="koboSpan" id="kobo.1007.1">their baskets:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table009">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1008.1">Basket Id</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1009.1">Bread (x1)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1010.1">Milk (x2)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1011.1">Eggs (x3)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1012.1">Bananas (x4)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1013.1">Chicken (x5)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1014.1">Apples (x6)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1015.1">Cheese (x7)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1016.1">Tomatoes (x8)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1017.1">Potatoes (x9)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1018.1">Onions (x10)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1019.1">Coffee (x11)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1020.1">Lettuce (x12)</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1021.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1022.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1023.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1024.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1025.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1026.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1027.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1028.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1029.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1030.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1031.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1032.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1033.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1034.1">2</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1035.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1036.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1037.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1038.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1039.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1040.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1041.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1042.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1043.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1044.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1045.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1046.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1047.1">3</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1048.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1049.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1050.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1051.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1052.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1053.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1054.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1055.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1056.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1057.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1058.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1059.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1060.1">4</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1061.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1062.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1063.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1064.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1065.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1066.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1067.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1068.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1069.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1070.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1071.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1072.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1073.1">5</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1074.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1075.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1076.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1077.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1078.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1079.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1080.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1081.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1082.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1083.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1084.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1085.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1086.1">6</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1087.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1088.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1089.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1090.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1091.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1092.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1093.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1094.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1095.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1096.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1097.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1098.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1099.1">7</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1100.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1101.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1102.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1103.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1104.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1105.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1106.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1107.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1108.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1109.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1110.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1111.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1112.1">8</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1113.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1114.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1115.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1116.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1117.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1118.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1119.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1120.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1121.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1122.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1123.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1124.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1125.1">9</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1126.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1127.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1128.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1129.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1130.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1131.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1132.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1133.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1134.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1135.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1136.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1137.1">0</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1138.1">10</span></span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1139.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1140.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1141.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1142.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1143.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1144.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1145.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1146.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1147.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1148.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1149.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1150.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">11</span></span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1152.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1153.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1154.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1155.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1156.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1157.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1158.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1159.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1160.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1161.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1162.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1163.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1164.1">12</span></span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1165.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1166.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1167.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1168.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1169.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1170.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1171.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1172.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1173.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1174.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1175.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1176.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1177.1">13</span></span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1178.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1179.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1180.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1181.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1182.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1183.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1184.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1185.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1186.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1187.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1188.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1189.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1190.1">14</span></span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1191.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1192.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1193.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1194.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1195.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1196.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1197.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1198.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1199.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1200.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1201.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1202.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1203.1">15</span></span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1204.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1205.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1206.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1207.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1208.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1209.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1210.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1211.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1212.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1213.1">0</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1214.1">1</span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="koboSpan" id="kobo.1215.1">0</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1216.1">Table 3.9 – Example shopping baskets of food</span></p>
<p><span class="koboSpan" id="kobo.1217.1">In this example, we can</span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.1218.1"> instruct a K-means algorithm to partition the data into three groups of shopping baskets. </span><span class="koboSpan" id="kobo.1218.2">Based on the output, we end up with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1219.1">three clusters:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1220.1">Cluster 0: Baskets 2, 5, </span><span class="No-Break"><span class="koboSpan" id="kobo.1221.1">12, 15</span></span></li>
<li><span class="koboSpan" id="kobo.1222.1">Cluster 1: Baskets 5, 9, </span><span class="No-Break"><span class="koboSpan" id="kobo.1223.1">11, 14</span></span></li>
<li><span class="koboSpan" id="kobo.1224.1">Cluster 2: Baskets 1, 3, 4, 6, 8, </span><span class="No-Break"><span class="koboSpan" id="kobo.1225.1">10, 13</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1226.1">Here’s how the K-means algorithm was applied to the dataset of grocery items purchased </span><span class="No-Break"><span class="koboSpan" id="kobo.1227.1">by shoppers:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1228.1">First, the grocery items (such as bread, milk, eggs, etc.) were represented in a format suitable for machine learning algorithms. </span><span class="koboSpan" id="kobo.1228.2">This was done using a binary representation where each item was encoded as 0 (not purchased) or 1 (purchased), as shown in </span><em class="italic"><span class="koboSpan" id="kobo.1229.1">Table 3.9</span></em><span class="koboSpan" id="kobo.1230.1">. </span><span class="koboSpan" id="kobo.1230.2">This binary matrix forms the dataset where each row represents a shopping basket and each column represents a different </span><span class="No-Break"><span class="koboSpan" id="kobo.1231.1">grocery item.</span></span></li>
<li><span class="koboSpan" id="kobo.1232.1">Next, we chose a number of clusters (or groups) that would be used for grouping the items. </span><span class="koboSpan" id="kobo.1232.2">In this case, we set </span><em class="italic"><span class="koboSpan" id="kobo.1233.1">K</span></em><span class="koboSpan" id="kobo.1234.1"> to three, meaning we decided to group the shopping baskets into three distinct clusters based on the items they contained. </span><span class="koboSpan" id="kobo.1234.2">The choice of </span><em class="italic"><span class="koboSpan" id="kobo.1235.1">K</span></em><span class="koboSpan" id="kobo.1236.1"> can be influenced by domain knowledge, experimentation, or other techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.1237.1">and algorithms.</span></span></li>
<li><span class="koboSpan" id="kobo.1238.1">Once the prerequisites for the process are set, we then choose some random points on a graph. </span><span class="koboSpan" id="kobo.1238.2">These </span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.1239.1">points, called </span><strong class="bold"><span class="koboSpan" id="kobo.1240.1">centroids</span></strong><span class="koboSpan" id="kobo.1241.1">, are the center points</span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.1242.1"> of the clusters being formed. </span><span class="koboSpan" id="kobo.1242.2">In our case, three shopping baskets were randomly chosen as the </span><span class="No-Break"><span class="koboSpan" id="kobo.1243.1">initial centroids.</span></span></li>
<li><span class="koboSpan" id="kobo.1244.1">Each shopping basket (or row of our dataset) was then assigned to the nearest centroid. </span><span class="koboSpan" id="kobo.1244.2">The “nearest” is typically determined by calculating the distance between the basket and each centroid. </span><span class="koboSpan" id="kobo.1244.3">Each basket is assigned to the cluster whose centroid is closest to it, forming three initial clusters based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1245.1">current centroids.</span></span></li>
</ol>
<p class="callout-heading"><span class="koboSpan" id="kobo.1246.1">Go the distance</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1247.1">There are many different methods for </span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.1248.1">determining distance in clustering. </span><span class="koboSpan" id="kobo.1248.2">The three most common types of distance </span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.1249.1">calculations are </span><strong class="bold"><span class="koboSpan" id="kobo.1250.1">Euclidean</span></strong><span class="koboSpan" id="kobo.1251.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1252.1">Hamming</span></strong><span class="koboSpan" id="kobo.1253.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1254.1">Manhattan</span></strong><span class="koboSpan" id="kobo.1255.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1256.1">Minkowski</span></strong><span class="koboSpan" id="kobo.1257.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.1258.1">Jaccard</span></strong><span class="koboSpan" id="kobo.1259.1">. </span><span class="koboSpan" id="kobo.1259.2">Each type</span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.1260.1"> of</span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.1261.1"> distance is used for </span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.1262.1">different types of data (for example, binary data versus linear or continuous numerical data). </span><span class="koboSpan" id="kobo.1262.2">The good news is that none of these things appear on the exam, so you don’t need to learn them. </span><span class="koboSpan" id="kobo.1262.3">However, if you want to explore different mathematical foundations for determining clustering distance, </span><span class="No-Break"><span class="koboSpan" id="kobo.1263.1">see </span></span><a href="https://www.displayr.com/understanding-cluster-analysis-a-comprehensive-guide/"><span class="No-Break"><span class="koboSpan" id="kobo.1264.1">https://www.displayr.com/understanding-cluster-analysis-a-comprehensive-guide/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1265.1">.</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.1266.1">Once all baskets have been assigned to clusters, the centroids of these clusters are recalculated. </span><span class="koboSpan" id="kobo.1266.2">This is done by taking the mean (or average) of all baskets in each cluster. </span><span class="koboSpan" id="kobo.1266.3">Since our data is binary, this average may not be exactly 0 or 1; it represents the proportion of baskets in the cluster that contain </span><span class="No-Break"><span class="koboSpan" id="kobo.1267.1">each item.</span></span></li>
<li><span class="koboSpan" id="kobo.1268.1">The steps of assigning baskets to the nearest centroid and then updating the centroids based on the current cluster memberships were repeated. </span><span class="koboSpan" id="kobo.1268.2">With each iteration, baskets might </span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.1269.1">shift from one cluster to another as the </span><span class="No-Break"><span class="koboSpan" id="kobo.1270.1">centroids change.</span></span></li>
<li><span class="koboSpan" id="kobo.1271.1">This process was repeated until the centroids no longer changed significantly between iterations, meaning that the clusters had stabilized and the algorithm had converged. </span><span class="koboSpan" id="kobo.1271.2">This is the stopping criterion </span><span class="No-Break"><span class="koboSpan" id="kobo.1272.1">for K-means.</span></span></li>
<li><span class="koboSpan" id="kobo.1273.1">Once the algorithm converged, each shopping basket was assigned to one of the three clusters based on the items it contained. </span><span class="koboSpan" id="kobo.1273.2">The final clusters represent groups of shopping baskets that are similar to each other based on the presence or absence of </span><span class="No-Break"><span class="koboSpan" id="kobo.1274.1">certain items.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1275.1">In the context of our grocery item dataset, applying K-means allowed us to group shopping baskets into clusters that could potentially reflect different types of shopping patterns or preferences among customers. </span><span class="koboSpan" id="kobo.1275.2">For example, one cluster might represent weekly staple shopping (including items such as bread, milk, and eggs), another might represent fresh produce shopping (including items such as fruits and vegetables), and another might represent specialty item shopping (such as coffee </span><span class="No-Break"><span class="koboSpan" id="kobo.1276.1">or cheese).</span></span></p>
<p><span class="koboSpan" id="kobo.1277.1">This type of information is frequently used by merchants to help understand buying habits and suggestive or upselling opportunities. </span><span class="koboSpan" id="kobo.1277.2">If you’ve ever wondered how Amazon determines how items show up as suggested for you to buy or why you get certain coupons in the mail, clustering machine learning models were very likely involved at </span><span class="No-Break"><span class="koboSpan" id="kobo.1278.1">some point.</span></span></p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.1279.1">Evaluation metrics</span></h2>
<p><span class="koboSpan" id="kobo.1280.1">Since there are no labels to compare against, evaluating the performance of clustering is less straightforward than </span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.1281.1">supervised learning. </span><span class="koboSpan" id="kobo.1281.2">However, metrics</span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.1282.1"> such as </span><strong class="bold"><span class="koboSpan" id="kobo.1283.1">silhouette score</span></strong><span class="koboSpan" id="kobo.1284.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.1285.1">silhouette coefficient</span></strong><span class="koboSpan" id="kobo.1286.1">), the </span><strong class="bold"><span class="koboSpan" id="kobo.1287.1">Davies-Bouldin index</span></strong><span class="koboSpan" id="kobo.1288.1">, and the </span><strong class="bold"><span class="koboSpan" id="kobo.1289.1">Calinski-Harabasz index</span></strong><span class="koboSpan" id="kobo.1290.1"> can be </span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.1291.1">used to assess the </span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.1292.1">quality of clustering by measuring the distance between clusters and the density of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1293.1">clusters</span></span><span class="No-Break"><a id="_idIndexMarker211"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1294.1"> themselves.</span></span></p>
<h3><span class="koboSpan" id="kobo.1295.1">Silhouette score</span></h3>
<p><span class="koboSpan" id="kobo.1296.1">The silhouette score or silhouette </span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.1297.1">coefficient </span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.1298.1">measures how well each data point fits within its final assigned cluster. </span><span class="koboSpan" id="kobo.1298.2">The score ranges from -1 to 1, where values closer to 1 indicate better-defined, </span><span class="No-Break"><span class="koboSpan" id="kobo.1299.1">less-overlapping clusters.</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1300.1">A score close to 1 indicates that the clusters are well apart from each other and </span><span class="No-Break"><span class="koboSpan" id="kobo.1301.1">clearly defined</span></span></li>
<li><span class="koboSpan" id="kobo.1302.1">A score of 0 indicates that the clusters </span><span class="No-Break"><span class="koboSpan" id="kobo.1303.1">are overlapping</span></span></li>
<li><span class="koboSpan" id="kobo.1304.1">A score close to -1 indicates that the clusters are </span><span class="No-Break"><span class="koboSpan" id="kobo.1305.1">assigned inappropriately</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1306.1">In this case, this model’s silhouette score is 0.128, which suggests that the clusters are overlapping or might not be distinctly separated. </span><span class="koboSpan" id="kobo.1306.2">That can be expected with this type of data (a grocery shopping list), since shoppers use items at different rates and have different needs than each other, leading them to complex purchase decisions that might result in them buying the same things but for </span><span class="No-Break"><span class="koboSpan" id="kobo.1307.1">different reasons.</span></span></p>
<h3><span class="koboSpan" id="kobo.1308.1">Davies-Bouldin index</span></h3>
<p><span class="koboSpan" id="kobo.1309.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.1310.1">Davies-Bouldin index</span></strong><span class="koboSpan" id="kobo.1311.1"> is </span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.1312.1">used to measure the</span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.1313.1"> quality of clustering, where lower scores indicate </span><span class="No-Break"><span class="koboSpan" id="kobo.1314.1">better clustering.</span></span></p>
<p><span class="koboSpan" id="kobo.1315.1">The Davies-Bouldin index score for our </span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.1316.1">grocery basket K-means clustering model is approximately 1.65. </span><span class="koboSpan" id="kobo.1316.2">The score essentially evaluates the average similarity between each cluster and its most similar cluster, where similarity is a measure that combines the compactness (how close the points in a cluster are to each other) and the separation (how far apart different clusters are from each other) of </span><span class="No-Break"><span class="koboSpan" id="kobo.1317.1">the clusters.</span></span></p>
<p><span class="koboSpan" id="kobo.1318.1">A lower Davies-Bouldin index indicates that the clusters are compact (i.e., the points within each cluster are close to each other) and well-separated (i.e., the clusters are far apart from each other). </span><span class="koboSpan" id="kobo.1318.2">Conversely, a</span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.1319.1"> higher score</span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.1320.1"> suggests less </span><span class="No-Break"><span class="koboSpan" id="kobo.1321.1">distinct clusters.</span></span></p>
<p><span class="koboSpan" id="kobo.1322.1">Generally, scores closer to 0 are </span><span class="No-Break"><span class="koboSpan" id="kobo.1323.1">more desirable.</span></span></p>
<h3><span class="koboSpan" id="kobo.1324.1">Calinski-Barabasz index</span></h3>
<p><span class="koboSpan" id="kobo.1325.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.1326.1">Calinski-Harabasz index</span></strong><span class="koboSpan" id="kobo.1327.1">, also known as the</span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.1328.1"> Variance Ratio Criterion, does not have a fixed range such as some other metrics (such as the silhouette score, which goes from -1 to 1). </span><span class="koboSpan" id="kobo.1328.2">Instead, its value depends on the dataset’s characteristics, including the number of samples, dimensions, and the inherent cluster structure. </span><span class="koboSpan" id="kobo.1328.3">The Calinski-Harabasz index score for this model </span><span class="No-Break"><span class="koboSpan" id="kobo.1329.1">is 3.10.</span></span></p>
<p><span class="koboSpan" id="kobo.1330.1">A higher Calinski-Harabasz index score</span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.1331.1"> indicates that the clusters are dense (meaning points within a cluster are close to each other) and well-separated (meaning clusters are far apart from each other). </span><span class="koboSpan" id="kobo.1331.2">This is interpreted as a model with a better-defined cluster structure. </span><span class="koboSpan" id="kobo.1331.3">Unlike metrics such as accuracy, which ranges from 0 to 1, or the silhouette score, which has a theoretical range from -1 to 1, the Calinski-Harabasz index does not have a maximum value and is not bounded in a fixed range. </span><span class="koboSpan" id="kobo.1331.4">Its absolute value is less informative </span><span class="No-Break"><span class="koboSpan" id="kobo.1332.1">without context.</span></span></p>
<p><span class="koboSpan" id="kobo.1333.1">The index is most useful when it’s being used to compare different clustering models or configurations on the same dataset. </span><span class="koboSpan" id="kobo.1333.2">For example, comparing the scores obtained from clustering the same data with different numbers of clusters (</span><em class="italic"><span class="koboSpan" id="kobo.1334.1">K</span></em><span class="koboSpan" id="kobo.1335.1">) can help in choosing the best </span><em class="italic"><span class="koboSpan" id="kobo.1336.1">K</span></em><span class="koboSpan" id="kobo.1337.1"> by selecting the one with the highest Calinski-Harabasz score. </span><span class="koboSpan" id="kobo.1337.2">In this example, we told the model to group the grocery baskets into three clusters. </span><span class="koboSpan" id="kobo.1337.3">To effectively use the Calinski-Harabasz score, it would be useful to re-run the model breaking the baskets into two, four, or </span><span class="No-Break"><span class="koboSpan" id="kobo.1338.1">five clusters.</span></span></p>
<p><span class="koboSpan" id="kobo.1339.1">For example, after rerunning the K-means algorithm with parameters for two, four, and five clusters, the following scores </span><span class="No-Break"><span class="koboSpan" id="kobo.1340.1">were achieved:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1341.1">Two </span><span class="No-Break"><span class="koboSpan" id="kobo.1342.1">clusters: 2.997</span></span></li>
<li><span class="koboSpan" id="kobo.1343.1">Four </span><span class="No-Break"><span class="koboSpan" id="kobo.1344.1">clusters: 2.73</span></span></li>
<li><span class="koboSpan" id="kobo.1345.1">Five </span><span class="No-Break"><span class="koboSpan" id="kobo.1346.1">clusters: 2.504</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1347.1">In practice, because there is no fixed “good” threshold, the best approach is to use the Calinski-Harabasz index to compare </span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.1348.1">the effectiveness of different clustering solutions on the same data and choose the one with the highest score. </span><span class="koboSpan" id="kobo.1348.2">However, this score should be used alongside other metrics such as the</span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.1349.1"> silhouette score and the Davies-Bouldin index for a more </span><span class="No-Break"><span class="koboSpan" id="kobo.1350.1">comprehensive evaluation.</span></span></p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.1351.1">Applications</span></h2>
<p><span class="koboSpan" id="kobo.1352.1">Clustering has a lot of real-world </span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.1353.1">applications, from social sciences to marketing and threat modeling. </span><span class="koboSpan" id="kobo.1353.2">Example applications include </span><span class="No-Break"><span class="koboSpan" id="kobo.1354.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1355.1">Market segmentation</span></strong><span class="koboSpan" id="kobo.1356.1">: Grouping customers based on purchasing behavior, interests, or </span><span class="No-Break"><span class="koboSpan" id="kobo.1357.1">demographic profiles.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1358.1">Anomaly detection</span></strong><span class="koboSpan" id="kobo.1359.1">: Identifying unusual data points by finding which ones do not fit into </span><span class="No-Break"><span class="koboSpan" id="kobo.1360.1">any cluster.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1361.1">Data organization</span></strong><span class="koboSpan" id="kobo.1362.1">: Organizing large volumes of data into manageable and </span><span class="No-Break"><span class="koboSpan" id="kobo.1363.1">meaningful groups.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1364.1">Pattern discovery</span></strong><span class="koboSpan" id="kobo.1365.1">: Finding hidden patterns or intrinsic structures in data, such as grouping genes with similar expression patterns </span><span class="No-Break"><span class="koboSpan" id="kobo.1366.1">in bioinformatics.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1367.1">Customer behavior analysis</span></strong><span class="koboSpan" id="kobo.1368.1">: Understanding different customer groups and their purchasing patterns can help in optimizing product placements, store layouts, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1369.1">inventory management.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1370.1">Recommendation systems</span></strong><span class="koboSpan" id="kobo.1371.1">: Clustering can help identify groups of similar items or users, which can then be used to recommend items to users based on the preferences of others in </span><span class="No-Break"><span class="koboSpan" id="kobo.1372.1">their group.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1373.1">Social network analysis</span></strong><span class="koboSpan" id="kobo.1374.1">: Clustering can help identify communities or groups within large networks of individuals based on their interactions or </span><span class="No-Break"><span class="koboSpan" id="kobo.1375.1">shared interests.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1376.1">These applications show how versatile and powerful clustering can be in extracting meaningful patterns from vast </span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.1377.1">amounts of data across </span><span class="No-Break"><span class="koboSpan" id="kobo.1378.1">different fields.</span></span></p>
<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/><span class="koboSpan" id="kobo.1379.1">Identify features of deep learning techniques</span></h1>
<p><span class="koboSpan" id="kobo.1380.1">Deep learning is an advanced subset of machine learning that mimics the human brain’s way of learning through an artificial </span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.1381.1">neural network structure. </span><span class="koboSpan" id="kobo.1381.2">These networks consist of multiple layers of neurons that process data in a hierarchical manner, which is why the models are called </span><strong class="bold"><span class="koboSpan" id="kobo.1382.1">deep neural networks</span></strong><span class="koboSpan" id="kobo.1383.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1384.1">DNNs</span></strong><span class="koboSpan" id="kobo.1385.1">). </span><span class="koboSpan" id="kobo.1385.2">Deep learning </span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.1386.1">automates feature extraction from large volumes of unstructured data, such as images and text, significantly enhancing machine learning tasks’ accuracy </span><span class="No-Break"><span class="koboSpan" id="kobo.1387.1">and efficiency.</span></span></p>
<p><span class="koboSpan" id="kobo.1388.1">Unlike traditional machine learning, which relies on manual feature extraction, deep learning models learn to identify and differentiate data features automatically. </span><span class="koboSpan" id="kobo.1388.2">This learning process requires significant computational power and data, utilizing backpropagation and optimization algorithms such as stochastic gradient descent to adjust neuron connections and minimize </span><span class="No-Break"><span class="koboSpan" id="kobo.1389.1">prediction errors.</span></span></p>
<p><span class="koboSpan" id="kobo.1390.1">Deep learning applications include regression, classification, natural language processing, and computer vision, revolutionizing fields such as autonomous driving, virtual assistants, facial recognition, and recommendation systems. </span><span class="koboSpan" id="kobo.1390.2">The training process involves fitting data to predict outcomes based on features, iteratively adjusting the model to improve accuracy. </span><span class="koboSpan" id="kobo.1390.3">This technology represents a significant advancement in machines’ abilities to perform complex tasks by learning </span><span class="No-Break"><span class="koboSpan" id="kobo.1391.1">from data.</span></span></p>
<p><span class="koboSpan" id="kobo.1392.1">Just like other machine learning techniques discussed in this chapter, deep learning involves fitting training data to a function that can predict a label (</span><em class="italic"><span class="koboSpan" id="kobo.1393.1">y</span></em><span class="koboSpan" id="kobo.1394.1">) based on the value of one or more features (</span><em class="italic"><span class="koboSpan" id="kobo.1395.1">x</span></em><span class="koboSpan" id="kobo.1396.1">). </span><span class="koboSpan" id="kobo.1396.2">The function (</span><em class="italic"><span class="koboSpan" id="kobo.1397.1">f</span></em><span class="koboSpan" id="kobo.1398.1">(</span><em class="italic"><span class="koboSpan" id="kobo.1399.1">x</span></em><span class="koboSpan" id="kobo.1400.1">)) is the outer layer of a nested function in which each layer of the neural network encapsulates functions that operate on </span><em class="italic"><span class="koboSpan" id="kobo.1401.1">x</span></em><span class="koboSpan" id="kobo.1402.1"> and the weight (</span><em class="italic"><span class="koboSpan" id="kobo.1403.1">w</span></em><span class="koboSpan" id="kobo.1404.1">) values associated with them. </span><span class="koboSpan" id="kobo.1404.2">The algorithm used to train the model involves iteratively feeding the feature values (</span><em class="italic"><span class="koboSpan" id="kobo.1405.1">x</span></em><span class="koboSpan" id="kobo.1406.1">) in the training data forward through the layers to calculate output values for </span><em class="italic"><span class="koboSpan" id="kobo.1407.1">ŷ</span></em><span class="koboSpan" id="kobo.1408.1">, validating the model to evaluate how far off the calculated </span><em class="italic"><span class="koboSpan" id="kobo.1409.1">ŷ</span></em><span class="koboSpan" id="kobo.1410.1"> values are from the known </span><em class="italic"><span class="koboSpan" id="kobo.1411.1">y</span></em><span class="koboSpan" id="kobo.1412.1"> values (which quantifies the level of error, or loss, in the model), and then modifying the weights (</span><em class="italic"><span class="koboSpan" id="kobo.1413.1">w</span></em><span class="koboSpan" id="kobo.1414.1">) to reduce the loss. </span><span class="koboSpan" id="kobo.1414.2">The trained model includes the final weight values that result in the most </span><span class="No-Break"><span class="koboSpan" id="kobo.1415.1">accurate predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.1416.1">So how does deep learning work? </span><span class="koboSpan" id="kobo.1416.2">The foundation </span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.1417.1">of deep learning is a structure called the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1418.1">neural network</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1419.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1420.1">Neural networks refer to computational systems designed to mimic the human brain and nervous system’s structure and function. </span><span class="koboSpan" id="kobo.1420.2">Similar to the brain’s neurons, artificial neural networks consist of units called neurons or nodes, interconnected with one another. </span><span class="koboSpan" id="kobo.1420.3">These connections </span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.1421.1">are characterized by weights and biases, activating subsequent nodes once input values surpass predefined thresholds. </span><span class="koboSpan" id="kobo.1421.2">Visualize a neural network as a series of nodes organized in layers, where each node connects to several others in the neighboring layer, with the output of each node affecting the inputs of nodes in the next layer, kind of like a 3D flowchart where each condition and action can connect to other conditions and actions. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1422.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1423.1">.10</span></em><span class="koboSpan" id="kobo.1424.1"> depicts an example of a simple neural </span><span class="No-Break"><span class="koboSpan" id="kobo.1425.1">network design:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<span class="koboSpan" id="kobo.1426.1"><img alt="Figure 3.10 – Example of a simple neural network" src="image/B22207_03_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1427.1">Figure 3.10 – Example of a simple neural network</span></p>
<p><span class="koboSpan" id="kobo.1428.1">Each input layer</span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.1429.1"> node is connected to two nodes in the hidden layer and the resultant data is connected to the output layer. </span><span class="koboSpan" id="kobo.1429.2">The input layer of a neural network processes raw data and passes it to the nodes of hidden layers, which classify the data points according to the target criteria. </span><span class="koboSpan" id="kobo.1429.3">As data progresses through successive hidden layers, the target value’s focus becomes more refined, leading to more </span><span class="No-Break"><span class="koboSpan" id="kobo.1430.1">precise assumptions.</span></span></p>
<p><span class="koboSpan" id="kobo.1431.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.1432.1">loss function</span></strong><span class="koboSpan" id="kobo.1433.1"> is used to</span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.1434.1"> compare the predicted </span><em class="italic"><span class="koboSpan" id="kobo.1435.1">ŷ</span></em><span class="koboSpan" id="kobo.1436.1"> values against the known values. </span><span class="koboSpan" id="kobo.1436.2">The function compiles those differences, resulting in the aggregate variance for multiple cases. </span><span class="koboSpan" id="kobo.1436.3">This total variance is</span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.1437.1"> summarized as the </span><strong class="bold"><span class="koboSpan" id="kobo.1438.1">loss</span></strong><span class="koboSpan" id="kobo.1439.1">. </span><span class="koboSpan" id="kobo.1439.2">An optimization function may be employed to evaluate the weight of the various losses and determine how to adjust to minimize the loss. </span><span class="koboSpan" id="kobo.1439.3">These changes are </span><strong class="bold"><span class="koboSpan" id="kobo.1440.1">backpropagated</span></strong><span class="koboSpan" id="kobo.1441.1"> throughout the neural network to replace the original values and the model re-learns based on the updated values. </span><span class="koboSpan" id="kobo.1441.2">Each iteration of this process is known as an </span><strong class="bold"><span class="koboSpan" id="kobo.1442.1">epoch</span></strong><span class="koboSpan" id="kobo.1443.1">; epochs are repeated until </span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.1444.1">the loss and predictions fall</span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.1445.1"> within an </span><span class="No-Break"><span class="koboSpan" id="kobo.1446.1">acceptable threshold.</span></span></p>
<p><span class="koboSpan" id="kobo.1447.1">Finally, the output layer utilizes the information from the hidden layers to determine the most </span><span class="No-Break"><span class="koboSpan" id="kobo.1448.1">likely label.</span></span></p>
<p><span class="koboSpan" id="kobo.1449.1">When you hear people talk about </span><strong class="bold"><span class="koboSpan" id="kobo.1450.1">deep learning</span></strong><span class="koboSpan" id="kobo.1451.1">, it’s really about expanding the concept of neural networks with more hidden layers (representing more dimensions of data classification), resulting in more </span><span class="No-Break"><span class="koboSpan" id="kobo.1452.1">precise predictions.</span></span></p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor059"/><span class="koboSpan" id="kobo.1453.1">Example</span></h2>
<p><span class="koboSpan" id="kobo.1454.1">An example of deep learning in action is an image</span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.1455.1"> identification scenario</span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.1456.1"> where the goal is to predict animal types in images. </span><span class="koboSpan" id="kobo.1456.2">In this case, a deep learning model, typically a </span><strong class="bold"><span class="koboSpan" id="kobo.1457.1">Convolutional Neural Network</span></strong><span class="koboSpan" id="kobo.1458.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1459.1">CNN</span></strong><span class="koboSpan" id="kobo.1460.1">), would be trained on a large dataset of animal images. </span><span class="koboSpan" id="kobo.1460.2">Each image in the dataset is labeled with the type of animal it contains (e.g., dog, cat, </span><span class="No-Break"><span class="koboSpan" id="kobo.1461.1">tiger, etc.).</span></span></p>
<p><span class="koboSpan" id="kobo.1462.1">During training, the CNN learns to recognize patterns and features in the images, such as shapes, textures, and colors, that distinguish one animal from another. </span><span class="koboSpan" id="kobo.1462.2">The network consists of various layers, including input, hidden, and output layers. </span><span class="koboSpan" id="kobo.1462.3">The input layer receives the raw image data, while the hidden layers process the data through a series of filters, identifying increasingly complex features at each layer. </span><span class="koboSpan" id="kobo.1462.4">The output layer then uses the information extracted by the hidden layers to classify the image according to the type of animal it most </span><span class="No-Break"><span class="koboSpan" id="kobo.1463.1">likely represents.</span></span></p>
<p><span class="koboSpan" id="kobo.1464.1">Once the model is trained, it can be used to predict the type of animal in new, unseen images by processing the images through the same network and producing a prediction based on the features it has learned </span><span class="No-Break"><span class="koboSpan" id="kobo.1465.1">to recognize.</span></span></p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor060"/><span class="koboSpan" id="kobo.1466.1">Applications</span></h2>
<p><span class="koboSpan" id="kobo.1467.1">This ability to automatically and accurately classify images makes deep learning a powerful tool for tasks such as wildlife monitoring, pet identification, and even medical </span><span class="No-Break"><span class="koboSpan" id="kobo.1468.1">image analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.1469.1">Deep learning has many of the same </span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.1470.1">applications as clustering (as its goals are very similar—the identification and classification or grouping of previously unlabeled data). </span><span class="koboSpan" id="kobo.1470.2">As such, real-world applications include everything that clustering can do, as well as </span><span class="No-Break"><span class="koboSpan" id="kobo.1471.1">many others:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1472.1">Image recognition and computer vision</span></strong><span class="koboSpan" id="kobo.1473.1">: Deep learning models, especially CNNs, are widely used in image recognition tasks due to the way they are structured, mimicking human visual perception. </span><span class="koboSpan" id="kobo.1473.2">They can identify faces, objects, scenes, and actions in images and videos. </span><span class="koboSpan" id="kobo.1473.3">This technology underpins various applications, including security surveillance, medical imaging diagnosis, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1474.1">autonomous vehicles.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1475.1">Natural Language Processing</span></strong><span class="koboSpan" id="kobo.1476.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1477.1">NLP</span></strong><span class="koboSpan" id="kobo.1478.1">): Deep learning has significantly advanced the capabilities of NLP, enabling </span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.1479.1">applications such as language translation, sentiment analysis, and chatbots. </span><span class="koboSpan" id="kobo.1479.2">Models such as transformers and </span><strong class="bold"><span class="koboSpan" id="kobo.1480.1">Recurrent Neural Networks</span></strong><span class="koboSpan" id="kobo.1481.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1482.1">RNNs</span></strong><span class="koboSpan" id="kobo.1483.1">) have been pivotal in </span><span class="No-Break"><span class="koboSpan" id="kobo.1484.1">these </span></span><span class="No-Break"><a id="_idIndexMarker238"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1485.1">advancements.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1486.1">Speech recognition and generation</span></strong><span class="koboSpan" id="kobo.1487.1">: Deep learning models are at the heart of voice-activated systems such as virtual assistants (e.g., Siri, Alexa), speech-to-text transcription services, and voice-enabled customer </span><span class="No-Break"><span class="koboSpan" id="kobo.1488.1">service systems.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1489.1">Recommendation systems</span></strong><span class="koboSpan" id="kobo.1490.1">: Deep learning is used to power recommendation engines on platforms such as Netflix, YouTube, and Amazon, enhancing user experience by personalizing content, products, and services based on individual preferences and </span><span class="No-Break"><span class="koboSpan" id="kobo.1491.1">past behavior.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1492.1">Autonomous vehicles</span></strong><span class="koboSpan" id="kobo.1493.1">: Deep learning models process and interpret the complex visual environment required for autonomous navigation, including recognizing traffic signs, signals, pedestrians, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1494.1">other vehicles.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1495.1">Fraud detection</span></strong><span class="koboSpan" id="kobo.1496.1">: Financial institutions use deep learning to detect unusual patterns and prevent fraudulent activities in real-time, significantly reducing the risk of financial losses for both financial institutions </span><span class="No-Break"><span class="koboSpan" id="kobo.1497.1">and consumers.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1498.1">Drug discovery and genomics</span></strong><span class="koboSpan" id="kobo.1499.1">: In the field of biotechnology, deep learning aids in the discovery of new drugs and the understanding of genetic sequences, contributing to personalized medicine and the treatment of </span><span class="No-Break"><span class="koboSpan" id="kobo.1500.1">complex diseases.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1501.1">Content generation</span></strong><span class="koboSpan" id="kobo.1502.1">: Deep learning</span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.1503.1"> techniques are used to create realistic images, videos, text, and voice, enabling applications such as virtual reality, game development, and the creation of art </span><span class="No-Break"><span class="koboSpan" id="kobo.1504.1">and music.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1505.1">Sentiment analysis</span></strong><span class="koboSpan" id="kobo.1506.1">: Companies use deep learning to analyze customer feedback, social media comments, and reviews to gauge public sentiment, improve products, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1507.1">tailor services.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1508.1">These applications demonstrate the versatility and transformative potential of deep learning across different domains, driving innovation and improving efficiency, accuracy, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1509.1">user experience.</span></span></p>
<h1 id="_idParaDest-62"><a id="_idTextAnchor061"/><span class="koboSpan" id="kobo.1510.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1511.1">In this chapter, you learned about many different types of machine learning scenarios such as regression, classification, and deep learning. </span><span class="koboSpan" id="kobo.1511.2">You learned about both supervised and unsupervised learning, and where each of those </span><span class="No-Break"><span class="koboSpan" id="kobo.1512.1">is appropriate.</span></span></p>
<p><span class="koboSpan" id="kobo.1513.1">Machine learning is helpful in a variety of real-world scenarios, from weather forecasting to medical imaging analysis. </span><span class="koboSpan" id="kobo.1513.2">You learned about applications for each type of machine learning technology and even how to compute several metrics to determine how accurate trained </span><span class="No-Break"><span class="koboSpan" id="kobo.1514.1">models are.</span></span></p>
<p><span class="koboSpan" id="kobo.1515.1">In the next chapter, we’ll start talking about core machine </span><span class="No-Break"><span class="koboSpan" id="kobo.1516.1">learning concepts.</span></span></p>
<h1 id="_idParaDest-63"><a id="_idTextAnchor062"/><span class="koboSpan" id="kobo.1517.1">Exam Readiness Drill – Chapter Review Questions</span></h1>
<p><span class="koboSpan" id="kobo.1518.1">Apart from a solid understanding of key concepts, being able to think quickly under time pressure is a skill that will help you ace your certification exam. </span><span class="koboSpan" id="kobo.1518.2">That is why working on these skills early on in your learning journey </span><span class="No-Break"><span class="koboSpan" id="kobo.1519.1">is key.</span></span></p>
<p><span class="koboSpan" id="kobo.1520.1">Chapter review questions are designed to improve your test-taking skills progressively with each chapter you learn and review your understanding of key concepts in the chapter at the same time. </span><span class="koboSpan" id="kobo.1520.2">You’ll find these at the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.1521.1">each chapter.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.1522.1">Before You Proceed</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1523.1">If you don't have a Packt Library subscription or you haven't purchased this book from the Packt store, you will need to unlock the online resources to access the exam readiness drills. </span><span class="koboSpan" id="kobo.1523.2">Unlocking is free and needs to be done only once. </span><span class="koboSpan" id="kobo.1523.3">To learn how to do that, head over to the chapter titled </span><a href="B22207_12.xhtml#_idTextAnchor228"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1524.1">Chapter 12</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.1525.1">, Accessing the </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1526.1">Online Resources</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1527.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1528.1">To open the Chapter Review Questions for this chapter, perform the </span><span class="No-Break"><span class="koboSpan" id="kobo.1529.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1530.1">Click the link – </span><a href="https://packt.link/AI-900_CH03"><span class="No-Break"><span class="koboSpan" id="kobo.1531.1">https://packt.link/AI-900_CH03</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1532.1">.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.1533.1">Alternatively, you can scan the following QR code (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1534.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1535.1">.11</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1536.1">):</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer030">
<span class="koboSpan" id="kobo.1537.1"><img alt="Figure 3.11– QR code that opens Chapter Review Questions for logged-in users" src="image/B22207_03_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1538.1">Figure 3.11– QR code that opens Chapter Review Questions for logged-in users</span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.1539.1">Once you log in, you’ll see a page similar to the one shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1540.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1541.1">.12</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1542.1">:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer031">
<span class="koboSpan" id="kobo.1543.1"><img alt="Figure 3.12 – Chapter Review Questions for Chapter 3" src="image/B22207_03_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1544.1">Figure 3.12 – Chapter Review Questions for Chapter 3</span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.1545.1">Once ready, start the following practice drills, re-attempting the quiz </span><span class="No-Break"><span class="koboSpan" id="kobo.1546.1">multiple times.</span></span></li>
</ol>
<h2 id="_idParaDest-64"><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.1547.1">Exam Readiness Drill</span></h2>
<p><span class="koboSpan" id="kobo.1548.1">For the first three attempts, don’t worry about the </span><span class="No-Break"><span class="koboSpan" id="kobo.1549.1">time limit.</span></span></p>
<h3><span class="koboSpan" id="kobo.1550.1">ATTEMPT 1</span></h3>
<p><span class="koboSpan" id="kobo.1551.1">The first time, aim for at least </span><strong class="bold"><span class="koboSpan" id="kobo.1552.1">40%</span></strong><span class="koboSpan" id="kobo.1553.1">. </span><span class="koboSpan" id="kobo.1553.2">Look at the answers you got wrong and read the relevant sections in the chapter again to fix your </span><span class="No-Break"><span class="koboSpan" id="kobo.1554.1">learning gaps.</span></span></p>
<h3><span class="koboSpan" id="kobo.1555.1">ATTEMPT 2</span></h3>
<p><span class="koboSpan" id="kobo.1556.1">The second time, aim for at least </span><strong class="bold"><span class="koboSpan" id="kobo.1557.1">60%</span></strong><span class="koboSpan" id="kobo.1558.1">. </span><span class="koboSpan" id="kobo.1558.2">Look at the answers you got wrong and read the relevant sections in the chapter again to fix any remaining </span><span class="No-Break"><span class="koboSpan" id="kobo.1559.1">learning gaps.</span></span></p>
<h3><span class="koboSpan" id="kobo.1560.1">ATTEMPT 3</span></h3>
<p><span class="koboSpan" id="kobo.1561.1">The third time, aim for at least </span><strong class="bold"><span class="koboSpan" id="kobo.1562.1">75%</span></strong><span class="koboSpan" id="kobo.1563.1">. </span><span class="koboSpan" id="kobo.1563.2">Once you score 75% or more, you start working on </span><span class="No-Break"><span class="koboSpan" id="kobo.1564.1">your timing.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.1565.1">Tip</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1566.1">You may take more than </span><strong class="bold"><span class="koboSpan" id="kobo.1567.1">three</span></strong><span class="koboSpan" id="kobo.1568.1"> attempts to reach 75%. </span><span class="koboSpan" id="kobo.1568.2">That’s okay. </span><span class="koboSpan" id="kobo.1568.3">Just review the relevant sections in the chapter till you </span><span class="No-Break"><span class="koboSpan" id="kobo.1569.1">get there.</span></span></p>
<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/><span class="koboSpan" id="kobo.1570.1">Working On Timing</span></h2>
<p><span class="koboSpan" id="kobo.1571.1">Your aim is to keep the score the same while trying to answer these questions as quickly as possible. </span><span class="koboSpan" id="kobo.1571.2">Here’s an example of how your next attempts should </span><span class="No-Break"><span class="koboSpan" id="kobo.1572.1">look like:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table010">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1573.1">Attempt</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1574.1">Score</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1575.1">Time Taken</span></strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1576.1">Attempt 5</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1577.1">77%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1578.1">21 mins </span><span class="No-Break"><span class="koboSpan" id="kobo.1579.1">30 seconds</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1580.1">Attempt 6</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1581.1">78%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1582.1">18 mins </span><span class="No-Break"><span class="koboSpan" id="kobo.1583.1">34 seconds</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1584.1">Attempt 7</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1585.1">76%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1586.1">14 mins </span><span class="No-Break"><span class="koboSpan" id="kobo.1587.1">44 seconds</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1588.1">Table 3.10 – Sample timing practice drills on the online platform</span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.1589.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1590.1">The time limits shown in the above table are just examples. </span><span class="koboSpan" id="kobo.1590.2">Set your own time limits with each attempt based on the time limit of the quiz on </span><span class="No-Break"><span class="koboSpan" id="kobo.1591.1">the website.</span></span></p>
<p><span class="koboSpan" id="kobo.1592.1">With each new attempt, your score should stay above </span><strong class="bold"><span class="koboSpan" id="kobo.1593.1">75%</span></strong><span class="koboSpan" id="kobo.1594.1"> while your “time taken” to complete should “decrease”. </span><span class="koboSpan" id="kobo.1594.2">Repeat as many attempts as you want till you feel confident dealing with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1595.1">time pressure.</span></span></p>
</div>
</body></html>