<html><head></head><body>
		<div id="_idContainer146">
			<h1 id="_idParaDest-174"><a id="_idTextAnchor222"/>Chapter 12: Machine Learning Automated Workflows</h1>
			<p>For <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models that are deployed to production environments, it's important to establish a consistent and repeatable process to retrain, deploy, and operate these models. This becomes increasingly important as you scale the number of ML models running in production. The <strong class="bold">machine learning development lifecycle</strong> (<strong class="bold">ML Lifecycle)</strong> brings with it some unique challenges in operationalizing ML workflows. This will be discussed in this chapter. We will also discuss common patterns to not only automate your ML workflows, but also implement <strong class="bold">continuous integration</strong> <a id="_idTextAnchor223"/>(<strong class="bold">CI<a id="_idTextAnchor224"/></strong>) and <strong class="bold">continuous delivery</strong>/<strong class="bold">deployment</strong> (<strong class="bold">CD</strong>) practices for your ML pipelines. </p>
			<p>Although we will cover various options for automating your ML workflows and building CI/CD pipelines for ML in this chapter, we will focus particularly on detailed implementation patterns using Amazon SageMaker Pipelines and Amazon SageMaker projects. SageMaker Pipelines is purpose-built for activities that include the automation of the steps needed to build a model, such as <strong class="bold">data preparation</strong>, <strong class="bold">model training</strong>, and <strong class="bold">model evaluation</strong> tasks. SageMaker projects build on SageMaker Pipelines by incorporating CI/CD practices into your ML pipelines. SageMaker projects utilize SageMaker Pipelines in combination with the SageMaker model registry to build out end-to-end ML pipelines that also incorporate CI/CD practices such as source control, version management, and automated deployments. </p>
			<p> In this chapter, we'll cover the following topics:</p>
			<ul>
				<li>Considerations for automating your SageMaker ML workflows</li>
				<li>Building ML workflows with Amazon SageMaker Pipelines</li>
				<li>Creating CI/CD ML pipelines using Amazon SageMaker projects</li>
			</ul>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor225"/>Considerations for automating your SageMaker ML workflows</h1>
			<p>In this section, we'll <a id="_idIndexMarker551"/>review a typical ML workflow that includes the basic steps for model building and deploy activities. Understanding the key SageMaker inputs and artifacts for each step is important in building automated workflows, regardless of the automation or workflow tooling you choose to employ. </p>
			<p>This information was covered in <a href="B17249_08_Final_JM_ePub.xhtml#_idTextAnchor151"><em class="italic">Chapter 8</em></a>, <em class="italic">Manage Models at Scale Using a Model Registry</em>. Therefore, if you have not yet read that chapter it's recommended to do so prior to continuing with this chapter. We'll build on that information and cover high-level considerations and guidance for building out automated workflows and CI/CD pipelines for SageMaker workflows. We'll also briefly cover the common AWS native service options when building automated workflows and CI/CD ML pipelines. <a id="_idTextAnchor226"/></p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor227"/>Typical ML workflows</h2>
			<p>An ML workflow contains all the steps required to build an ML model for an ML use case, followed by the steps needed to deploy and operate the model in production. <em class="italic">Figure 12.1</em> shows a typical ML workflow that includes model build and model deploy steps. Each step <a id="_idIndexMarker552"/>within the workflow often has a number of associated tasks. As an example, data preparation can include multiple tasks needed to transform data into a format that is consistent with your ML algorithm. </p>
			<p>When we look at automating the end-to-end ML workflow, we look to automate the tasks included within a step, as well as how to orchestrate the sequence and timing of steps into an end-to-end pipeline. As a result, knowing the key inputs for each step, as well as the expected output or artifact of a step, is key in building end-to-end pipelines. </p>
			<p>Additionally, model development is an iterative process. It may therefore take many experiments until you're able to find a candidate model that meets your model performance criteria. As a result, it's common to continue to experiment in a data science sandbox environment until you find a candidate model to register into a model registry. This would indicate that the model is ready to deploy to one or more target environments for additional testing, followed by deployment to a production environment. </p>
			<p>Refer to the following figure for an example of a typical workflow: </p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B17249_12_01.jpg" alt="Figure 12.1 – Typical ML workflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – Typical ML workflow</p>
			<p>After the model is deployed, there may also be additional tasks required to integrate the model with existing <a id="_idIndexMarker553"/>client applications. There may also be tasks required to create a more complex inference workflow that includes multiple models and tasks required for inference. Finally, there would still be tasks required to operate that model. Although the <em class="italic">Operate</em> step comes at the end, the activities that need to be performed for the ongoing operation of that model need to be considered early on in the process. This is in order to include all necessary tasks within your automated workflow, as well as ensure key metrics are captured, and available for key personas. In addition, this allows you to set up alerts as needed. This includes activities such as the following: </p>
			<ul>
				<li><strong class="bold">Model monitoring</strong>: This includes the tasks required to ensure your model performance does not <a id="_idIndexMarker554"/>degrade over time. This topic is covered in detail in <a href="B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210"><em class="italic">Chapter 11</em></a>, <em class="italic">Monitoring Production Models with Amazon SageMaker Model Monitor and Clarify.</em> However, when building your automated deployment workflows, it's important to consider the additional tasks that may need to be included and automated within your pipeline. As an example, SageMaker Model Monitor for data drift requires tasks such as baselining of your training data, enabling data capture on your endpoints, and scheduling a SageMaker monitoring job. All of these tasks should be automated and included in your automated workflow. You can also utilize <em class="italic">Human in the Loop </em>reviews with <strong class="bold">Amazon Augmented AI</strong> (<strong class="bold">Amazon A2I</strong>) to check <a id="_idIndexMarker555"/>low-confidence predictions that can be implemented along with, or complementary to, SageMaker Model Monitor.</li>
				<li><strong class="bold">System monitoring</strong>: System monitoring includes capturing and alerting on metrics that are key to <a id="_idIndexMarker556"/>the resources hosting your model, as well as the other resources supporting the deployed ML solution. As an example, Amazon SageMaker will automatically capture key metrics about an endpoint, such as CPU/GPU utilization or the number of invocations. Setting thresholds and creating alerts in Amazon CloudWatch helps ensure the overall health of resources hosting models, as well as other solution components. </li>
				<li><strong class="bold">Model retraining</strong>: To set up automatic model retraining, the tasks that are performed across your <a id="_idIndexMarker557"/>model build steps should be captured as code that can be executed as part of a model build pipeline. This pipeline would include automation of all of the tasks within each step, as well as orchestration of those steps. </li>
				<li><strong class="bold">Pipeline monitoring</strong>: If you have automated pipelines set up for your model build and <a id="_idIndexMarker558"/>model deploy activities, it's key to also have monitoring in place on your pipeline to ensure you are notified in the event of a step failure in your pipeline. </li>
			</ul>
			<p>We have covered the general steps in an ML workflow. However, each automated workflow and CI/CD pipeline can vary due to a number of factors. In the next section, we'll cover some of the considerations that are common across ML use c<a id="_idTextAnchor228"/>ases. </p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor229"/>Considerations and guidance for building SageMaker workflows and CI/CD pipelines</h2>
			<p>The steps <a id="_idIndexMarker559"/>and tasks performed as part of an ML workflow <a id="_idIndexMarker560"/>can vary depending on the use case; however, the following high-level practices are recommended when building an automated workflow for your ML use case:</p>
			<ul>
				<li><strong class="bold">Implement a model registry</strong>: A <strong class="bold">model registry</strong> helps bridge the steps between the phases of<a id="_idIndexMarker561"/> model building experimentation and deploying your models to higher-level environments. A model registry captures key metadata, such as <strong class="bold">model metrics</strong>. It also ensures you're able to track key inputs and<a id="_idIndexMarker562"/> artifacts for traceability, as well as manage multiple model versions across environments.</li>
				<li><strong class="bold">Version inputs and artifacts</strong>: The ability to roll back or recreate a specific model version or deployable artifact is dependent on knowing the specific versions of inputs and artifacts used to create that resource. As an example, to recreate a SageMaker endpoint, you need to know key version information, such as the model artifact and the inference container image. These inputs and artifacts should be protected from inadvertent deletion. They should also be tracked through an end-to-end pipeline to be able to confidently recreate resources as part of an automate<a id="_idTextAnchor230"/>d workflow.</li>
			</ul>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor231"/>AWS-native options for automated workflow and CI/CD pipelines</h2>
			<p>In this chapter, we focus primarily on the SageMaker-native options for creating automated workflows, as well as layering on CI/CD practices in end-to-end pipelines. However, there are other options that can also be used for creating automated <a id="_idIndexMarker563"/>workflows that contain SageMaker tasks for model building and model deployment. There are also third-party options that contain operators or integrations with SageMaker. However, they are not covered in this book. </p>
			<p>First, we'll cover a few <a id="_idIndexMarker564"/>of the AWS services and features that can be used to build automated workflows that include SageMaker tasks: </p>
			<ul>
				<li><strong class="bold">AWS Step Functions</strong>: AWS Step<a id="_idIndexMarker565"/> Functions (<a href="https://aws.amazon.com/step-functions/?step-functions.sort-by=item.additionalFields.postDateTime&amp;step-functions.sort-order=desc">https://aws.amazon.com/step-functions/?step-functions.sort-by=item.additionalFields.postDateTime&amp;step-functions.sort-order=desc</a>) allows you to create automated serverless workflows that include integration with a number of AWS services, as well as giving you the capability to integrate third-party tasks into your workflows. AWS Step Functions also has native support for SageMaker tasks, such as SageMaker processing jobs, SageMaker training jobs, and SageMaker hosting options. <p>In addition, ML builders <a id="_idIndexMarker566"/>can choose to take advantage of the AWS Step Functions Data Science SDK (<a href="https://docs.aws.amazon.com/step-functions/latest/dg/concepts-python-sdk.html">https://docs.aws.amazon.com/step-functions/latest/dg/concepts-python-sdk.html</a>) to create ML workflows using Python instead of through Amazon States Language. Amazon States Language is the native pipeline syntax for AWS Step Functions. AWS Step Functions offers extensibility across AWS services with native integrations for the AWS services most commonly used in ML workflows, such as AWS Lambda, Amazon EMR, or AWS Glue. </p></li>
				<li><strong class="bold">Amazon Managed Workflows for Apache Airflow</strong>: Amazon Managed Workflows for Apache Airflow (<a href="https://aws.amazon.com/managed-workflows-for-apache-airflow/">https://aws.amazon.com/managed-workflows-for-apache-airflow/</a>) allows<a id="_idIndexMarker567"/> you to create automated ML workflows by using native integration with SageMaker among other AWS services that are commonly used. Many organizations and teams already use or have invested in Airflow, so this service provides a way to take advantage of those existing investments using a managed service that includes native integrations with SageMaker for model building and deployment steps. </li>
				<li><strong class="bold">Amazon SageMaker Operators for Kubernetes</strong>: SageMaker Operators for<a id="_idIndexMarker568"/> Kubernetes (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/amazon-sagemaker-operators-for-kubernetes.html">https://docs.aws.amazon.com/sagemaker/latest/dg/amazon-sagemaker-operators-for-kubernetes.html</a>) allows teams to create SageMaker tasks natively using the Kubernetes API and command-line Kubernetes tools, such<a id="_idIndexMarker569"/> as <strong class="bold">kubectl</strong>. </li>
				<li><strong class="bold">Amazon SageMaker Components for Kubeflow Pipelines</strong>: SageMaker Components for Kubeflow <a id="_idIndexMarker570"/>Pipelines allows teams to still utilize Kubeflow for workflow orchestration, while providing integrations with SageMaker so that you can create and run SageMaker jobs in managed environments without running them directly on your Kubernetes clusters. This is useful for taking advantage <a id="_idIndexMarker571"/>of end-to-end managed SageMaker features, but also for cases where you do not want to perform those tasks directly on your cluster. </li>
			</ul>
			<p>Next, we'll cover a few of the AWS services and features that can be used to incorporate CI/CD <a id="_idIndexMarker572"/>practices into your ML pipelines. These services are not unique to ML and <a id="_idIndexMarker573"/>can also be substituted for third-party tools offering similar capabilities: </p>
			<ul>
				<li><strong class="bold">AWS CodeCommit</strong>: AWS CodeCommit (<a href="https://aws.amazon.com/codecommit/">https://aws.amazon.com/codecommit/</a>) is a private Git-based source code repository. For ML <a id="_idIndexMarker574"/>pipelines, AWS CodeCommit can store any related source code, such as <strong class="bold">infrastructure as code</strong> (<strong class="bold">IaC</strong>)/<strong class="bold">configuration as code </strong>(<strong class="bold">CaC</strong>), data processing code, training code, model evaluation <a id="_idIndexMarker575"/>code, pipeline code, and model deployment code. The structure of your repositories may vary, but in general, it's recommended to at least separate your model build and model deploy code. </li>
				<li><strong class="bold">AWS CodeBuild</strong>: AWS CodeBuild (<a href="https://aws.amazon.com/codebuild/">https://aws.amazon.com/codebuild/</a>) is a fully managed build service that can be used for multiple purposes. These include compiling <a id="_idIndexMarker576"/>source code, running tests, and running custom scripts as part of a pipeline. For ML pipelines, AWS CodeBuild can be used for tasks such as testing through custom scripts and packaging AWS CloudFormation templates. </li>
				<li><strong class="bold">AWS CodePipeline</strong>: AWS CodePipeline (<a href="https://aws.amazon.com/codepipeline/">https://aws.amazon.com/codepipeline/</a>) is a fully managed CD <a id="_idIndexMarker577"/>service that can be used to orchestrate the steps of your ML pipeline. AWS CodePipeline can be used to orchestrate the steps for model build tasks, as well as model deploy tasks. </li>
			</ul>
			<p>The preceding list of AWS services can be used to incorporate CI/CD practices for your ML pipelines. You can also optionally substitute the services above for third-party options, such as GitHub, BitBucket, or Jenkins. </p>
			<p>In this section, we covered a high-level ML workflow in the context of automating the tasks within key steps, as well as providing overall orchestration to automate those steps. We also discussed some of the key considerations when building your ML workflows. We reviewed the AWS-native options for creating automated ML workflows. We then looked at the AWS services that can be used to incorporate CI/CD practices. </p>
			<p>All of these, as well as many third-party options, are valid options when selecting the right tooling for automating your SageMaker workflows. The decision to custom build workflows using the services mentioned in the preceding list, or the decision to substitute the services above with third-party options, typically comes from either personal preference or having organizational standards or requirements to utilize existing tooling. </p>
			<p>For the remainder of this chapter, we'll focus on the SageMaker-native capabilities for automating your ML workflows and incorp<a id="_idTextAnchor232"/>orating CI/CD practices. </p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor233"/>Building ML workflows with Amazon SageMaker Pipelines</h1>
			<p>Model build workflows cover all of the steps performed when developing your model, including data preparation, model training, model tuning, and model deployment. In this <a id="_idIndexMarker578"/>case, model deployment can include the tasks necessary to evaluate your model, as well as <a id="_idIndexMarker579"/>batch use cases that do not need to be deployed to higher environments. SageMaker Pipelines is a fully managed service that allows you to create automated model build workflows using the SageMaker Python SDK. </p>
			<p>SageMaker Pipelines includes built-in step types (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html">https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html</a>) for executing <a id="_idIndexMarker580"/>SageMaker tasks, such as SageMaker Processing for data pre-processing, and SageMaker Training for model training. Pipelines also include steps for controlling how your pipeline works. For example, the pipeline could include conditional steps that could be used to evaluate the output of a previous step to determine whether to proceed to the next step in the pipeline. </p>
			<p>To include steps that perform tasks using other AWS services or non-AWS tasks, you must use the <strong class="bold">callback step</strong>. This is useful <a id="_idIndexMarker581"/>if you are using another AWS service for a task in your pipeline. One example could be if you are using AWS Glue <a id="_idIndexMarker582"/>for data preprocessing. <em class="italic">Figure 12.2</em> builds on the previous workflow illustration to indicate where SageMaker Pipelines fits into the end-to-end workflow, as well as providing <a id="_idIndexMarker583"/>examples of the supported SageMaker features for each model build workflow step:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B17249_12_02.jpg" alt="Figure 12.2 – SageMaker Pipelines model building workflows&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – SageMaker Pipelines model building workflows</p>
			<p>In this section, you'll build out a SageMaker pipeline for your ML use case. The pipeline will include all of the steps necessary for data preparation, model training, and model evaluation. Because we don't need every SageMaker feature to build our pipeline, you'll only be using the features noted in the following diagram: </p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B17249_12_03.jpg" alt="Figure 12.3 – SageMaker Pipelines example pipeline &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 – SageMaker Pipelines example pipeline </p>
			<p>For each step in your SageMaker pipeline, you first need to configure the task that you will execute (for example, a training job) and then configure the SageMaker Pipelines step <a id="_idIndexMarker584"/>for that task. After all, steps have been configured, you chain the steps together and then execute <a id="_idIndexMarker585"/>the pipeline. The following sections will walk you through the steps in building your SageMaker pipe<a id="_idTextAnchor234"/>line for your example use case.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor235"/>Building your SageMaker pipeline </h2>
			<p>In this section, we'll walk through the steps needed to configure each step in your SageMaker pipeline, as well as how to chain those steps together and finally execute your model <a id="_idIndexMarker586"/>build pipeline. For each step in your pipeline, there are two steps to follow: </p>
			<ol>
				<li>Configure the SageMaker job. </li>
				<li>Configure the SageMaker Pipelines step. </li>
			</ol>
			<p><em class="italic">Figure 12.4</em> illustrates the steps that we will use to build the pipeline:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B17249_12_04.jpg" alt="Figure 12.4 – Pipeline use case with SageMaker steps&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.4 – Pipeline use case with SageMaker steps</p>
			<p>We'll start with the data preparation step, where we'll use SageMaker Processing to transform our raw data into the for<a id="_idTextAnchor236"/>mat expected by the algorithm. </p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor237"/>Data preparation step</h2>
			<p>In <a id="_idIndexMarker587"/>this step, you'll configure the SageMaker processing job that will be used to transform your data into a format expected by the algorithm. For this, we'll use the same configuration from <a href="B17249_04_Final_JM_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 4</em></a>, <em class="italic">Data Preparation at Scale Using Amazon SageMaker Data Wrangler and Processing</em>: </p>
			<ol>
				<li value="1">First, we'll configure the SageMaker processing job, as follows: <p class="source-code">from sagemaker.spark.processing import PySparkProcessor </p><p class="source-code">spark_processor = PySparkProcessor( </p><p class="source-code">    base_job_name="spark-preprocessor", </p><p class="source-code">    framework_version="3.0", </p><p class="source-code">    role=role, </p><p class="source-code">    instance_count=15, </p><p class="source-code">    instance_type="ml.m5.4xlarge", </p><p class="source-code">    max_runtime_in_seconds=7200, ) </p><p class="source-code">configuration = [ </p><p class="source-code">    { </p><p class="source-code">    "Classification": "spark-defaults", </p><p class="source-code">    "Properties": {"spark.executor.memory": "18g",</p><p class="source-code">              "spark.yarn.executor.memoryOverhead": "3g", </p><p class="source-code">                   "spark.driver.memory": "18g", </p><p class="source-code">                "spark.yarn.driver.memoryOverhead": "3g", </p><p class="source-code">                   "spark.executor.cores": "5", </p><p class="source-code">                   "spark.driver.cores": "5", </p><p class="source-code">                 "spark.executor.instances": "44",      </p><p class="source-code">                 "spark.default.parallelism": "440", </p><p class="source-code">               "spark.dynamicAllocation.enabled": "false" </p><p class="source-code">                }, </p><p class="source-code">     }, </p><p class="source-code">     { </p><p class="source-code">     "Classification": "yarn-site", </p><p class="source-code">      "Properties": {"yarn.nodemanager.vmem-check-enabled": "false", </p><p class="source-code">      "yarn.nodemanager.mmem-check-enabled": "false"}, </p><p class="source-code">      } </p><p class="source-code">]</p></li>
				<li>Next, we'll <a id="_idIndexMarker588"/>configure the SageMaker Pipelines step that will be used to execute your data preparation tasks. For this, we'll use the built-in processing step (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing">https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing</a>) that tells Pipelines this step will be a SageMaker processing job. <em class="italic">Figure 12.5</em> shows the high-level inputs and outputs/artifacts that <strong class="source-inline">ProcessingStep</strong> used for data preprocessing will expect: </li>
			</ol>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B17249_12_05.jpg" alt="Figure 12.5 – Data preparation pipeline step&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.5 – Data preparation pipeline step</p>
			<p>We previously <a id="_idIndexMarker589"/>configured the processor, so we will now use that processor (combined with the other inputs shown in <em class="italic">Figure 12.4</em>) to set up our Pipelines step, as follows: </p>
			<ol>
				<li value="1">First, we'll enable <strong class="bold">step caching</strong>. Step caching tells SageMaker to check for a previous execution of a step that was called with the same arguments. This is so that it can use the previous step values of a successful run instead of re-executing a step with the exact same arguments. You should consider using step caching to avoid unnecessary tasks and costs. As an example, if the second step (model training) in your pipeline fails, you can start the pipeline again without re-executing the <a id="_idIndexMarker590"/>data preparation step if that step has not changed, as follows: <p class="source-code">from sagemaker.workflow.steps import CacheConfig</p><p class="source-code">cache_config = CacheConfig(enable_caching=True, expire_after="T360m")</p></li>
				<li>Next, we'll define the runtime arguments using the <strong class="source-inline">get_run_args</strong> method. In this case, we are passing the Spark processor that was previously configured, in combination with the parameters identifying the inputs (raw weather data), the outputs (train, test, and validation datasets), and additional arguments the data processing script accepts as input. The data processing script, <strong class="source-inline">preprocess.py</strong>, is a slightly modified version of the processing script used in <a href="B17249_04_Final_JM_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 4</em></a>, <em class="italic">Data Preparation at Scale Using Amazon SageMaker Data Wrangler and Processing</em>. Refer to the following script: <p class="source-code">from sagemaker.processing import ProcessingInput, ProcessingOutput</p><p class="source-code">run_args = pyspark_processor.get_run_args(</p><p class="source-code">    "preprocess.py",</p><p class="source-code">    submit_jars=["s3://crawler-public/json/serde/json-serde.jar"],</p><p class="source-code">    spark_event_logs_s3_uri=spark_event_logs_s3_uri,</p><p class="source-code">    configuration=configuration,</p><p class="source-code">    outputs=[ \ </p><p class="source-code">        ProcessingOutput(output_name="validation", destination=validation_data_out, source="/opt/ml/processing/validation"),</p><p class="source-code">        ProcessingOutput(output_name="train", destination=train_data_out, source="/opt/ml/processing/train"),</p><p class="source-code">        ProcessingOutput(output_name="test", destination=test_data_out, source="/opt/ml/processing/test"),</p><p class="source-code">     ],</p><p class="source-code">    arguments=[</p><p class="source-code">        '--s3_input_bucket', s3_bucket,</p><p class="source-code">        '--s3_input_key_prefix', s3_prefix_parquet,</p><p class="source-code">        '--s3_output_bucket', s3_bucket,</p><p class="source-code">        '--s3_output_key_prefix', s3_output_prefix+'/prepared-data/'+timestamp</p><p class="source-code">    ]</p><p class="source-code">)</p></li>
				<li>Next, we'll use the runtime parameters to configure the actual SageMaker Pipelines step for our data preprocessing tasks. You'll notice we're using all of the parameters we<a id="_idIndexMarker591"/> configured previously to build the step that will execute as part of the pipeline: <p class="source-code">from sagemaker.workflow.steps import ProcessingStep</p><p class="source-code">step_process = ProcessingStep(</p><p class="source-code">    name="DataPreparation",</p><p class="source-code">    processor=pyspark_processor,</p><p class="source-code">    inputs=run_args.inputs,</p><p class="source-code">    outputs=run_args.outputs,</p><p class="source-code">    job_arguments=run_args.arguments,</p><p class="source-code">    code="mod<a id="_idTextAnchor238"/>elbuild/pipelines/preprocess.py",</p><p class="source-code">)</p></li>
			</ol>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor239"/>Model build step</h2>
			<p>In this step, you'll configure the SageMaker training job that will be used to train your model. You'll use the training data <a id="_idIndexMarker592"/>produced from the data preparation step, in combination with your training code and configuration parameters. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Although we do not cover it in this chapter specifically, it is important to note that SageMaker Pipelines now integrates with SageMaker Experiments, allowing you to capture extra metrics, as well as view corresponding plots in SageMaker Pipelines.</p>
			<p>For this, we'll use the same configuration from <a href="B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>, <em class="italic">Training and Tuning at Scale</em>. Refer to the following steps: </p>
			<ol>
				<li value="1">First, we'll configure the SageMaker training job, as follows: <p class="source-code"># initialize hyperparameters</p><p class="source-code">hyperparameters = {</p><p class="source-code">        "max_depth":"5",</p><p class="source-code">        "eta":"0.2",</p><p class="source-code">        "gamma":"4",</p><p class="source-code">        "min_child_weight":"6",</p><p class="source-code">        "subsample":"0.7",</p><p class="source-code">        "objective":"reg:squarederror",</p><p class="source-code">        "num_round":"5"}</p><p class="source-code"># set an output path where the trained model will be saved</p><p class="source-code">m_prefix = 'pipeline/model'</p><p class="source-code">output_path = 's3://{}/{}/{}/output'.format(s3_bucket, m_prefix, 'xgboost')</p><p class="source-code"># this line automatically looks for the XGBoost image URI and builds an XGBoost container.</p><p class="source-code"># specify the repo_version depending on your preference.</p><p class="source-code">image_uri = sagemaker.image_uris.retrieve("xgboost", region, "1.2-1")</p><p class="source-code"># construct a SageMaker estimator that calls the xgboost-container</p><p class="source-code">xgb_estimator = sagemaker.estimator.Estimator(image_uri=image_uri, </p><p class="source-code">                         hyperparameters=hyperparameters,</p><p class="source-code">                     role=sagemaker.get_execution_role(),</p><p class="source-code">                         instance_count=1, </p><p class="source-code">                         instance_type='ml.m5.12xlarge', </p><p class="source-code">                         volume_size=200, # 5 GB </p><p class="source-code">                         output_path=output_path)</p></li>
				<li>Next, we'll <a id="_idIndexMarker593"/>configure the SageMaker Pipelines step that will be used to execute your model training task. For this, we'll use the built-in <strong class="source-inline">training step</strong> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training">https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training</a>). This tells Pipelines this step will be a SageMaker <a id="_idIndexMarker594"/>training job. <em class="italic">Figure 12.6</em> shows the high-level inputs and outputs/artifacts that a <strong class="bold">Training step</strong> will expect:</li>
			</ol>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B17249_12_06.jpg" alt="Figure 12.6 – Model build pipeline step&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.6 – Model build pipeline step</p>
			<p>We previously configured the estimator, so we will now use that estimator combined with the other inputs shown in <em class="italic">Figure 12.6</em> to set up our Pipelines step: </p>
			<p class="source-code">from sagemaker.inputs import TrainingInput</p>
			<p class="source-code">from sagemaker.workflow.steps import TrainingStep</p>
			<p class="source-code">step_train = TrainingStep(</p>
			<p class="source-code">    name="ModelTrain",</p>
			<p class="source-code">    estimator=xgb_estimator,</p>
			<p class="source-code">    cache_config=cache_config,</p>
			<p class="source-code">    inputs={</p>
			<p class="source-code">        "train": TrainingInput(</p>
			<p class="source-code">            s3_data=step_process.properties.ProcessingOutputConfig.Outputs["train"].S3Output.S3Uri,</p>
			<p class="source-code">            content_type="text/csv",</p>
			<p class="source-code">        ),</p>
			<p class="source-code">        "validation": TrainingInput(</p>
			<p class="source-code">            s3_data=step_process.properties.ProcessingOutputConfig.Outputs["validation"].S3Output.S3Uri,</p>
			<p class="source-code">            content<a id="_idTextAnchor240"/>_type="text/csv",</p>
			<p class="source-code">        ),</p>
			<p class="source-code">    },</p>
			<p class="source-code">)</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor241"/>Model evaluation step</h2>
			<p>In this step, you'll <a id="_idIndexMarker595"/>configure a SageMaker processing job that will be used to evaluate your trained model using the model artifact produced from the training step in combination with your processing code and configuration: </p>
			<ol>
				<li value="1">First, we'll configure the SageMaker processing job starting with <strong class="source-inline">ScriptProcessor</strong>. We will use this to execute a simple evaluation script, as follows:<p class="source-code">from sagemaker.processing import ScriptProcessor</p><p class="source-code">script_eval = ScriptProcessor(</p><p class="source-code">    image_uri=image_uri,</p><p class="source-code">    command=["python3"],</p><p class="source-code">    instance_type=processing_instance_type,</p><p class="source-code">    instance_count=1,</p><p class="source-code">    base_job_name="script-weather-eval",</p><p class="source-code">    role=role,</p><p class="source-code">)</p></li>
				<li>Next, we'll <a id="_idIndexMarker596"/>configure the SageMaker Pipelines step that will be used to execute your model evaluation tasks. For this, we'll use the built-in Processing step (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing">https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing</a>). This tells Pipelines this step will be a SageMaker processing job. <em class="italic">Figure 12.7</em> shows the high-level inputs and outputs/artifacts that a Processing step used for model evaluation will expect: </li>
			</ol>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B17249_12_07.jpg" alt="Figure 12.7 – Model evaluation pipeline step&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.7 – Model evaluation pipeline step</p>
			<p>We previously configured the processor, so we will now use that processor combined with <a id="_idIndexMarker597"/>the other inputs shown in <em class="italic">Figure 12.7</em> to set up our Pipelines step. To do this, we'll first set up the property file that will be used to store the output, in this case, model evaluation metrics, of our processing job. Then, we'll configure the <strong class="source-inline">ProcessingStep</strong> definition as follows:</p>
			<p class="source-code">from sagemaker.workflow.properties import PropertyFile</p>
			<p class="source-code">evaluation_report = PropertyFile(</p>
			<p class="source-code">    name="EvaluationReport", output_name="evaluation", path="evaluation.json"</p>
			<p class="source-code">)</p>
			<p class="source-code">step_eval = ProcessingStep(</p>
			<p class="source-code">    name="WeatherEval",</p>
			<p class="source-code">    processor=script_eval,</p>
			<p class="source-code">    cache_config = cache_config,</p>
			<p class="source-code">    inputs=[</p>
			<p class="source-code">        ProcessingInput(</p>
			<p class="source-code">            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,</p>
			<p class="source-code">            destination="/opt/ml/processing/model",</p>
			<p class="source-code">        ),</p>
			<p class="source-code">        ProcessingInput(</p>
			<p class="source-code">          source=step_process.properties.ProcessingOutputConfig.Outputs["test"].S3Output.S3Uri,  destination="/opt/ml/processing/test",</p>
			<p class="source-code">        ),</p>
			<p class="source-code">    ],</p>
			<p class="source-code">    outputs=[</p>
			<p class="source-code">        ProcessingOutput(output_name="evaluation", source="/opt/ml/processing/evaluation"),</p>
			<p class="source-code">    ],</p>
			<p class="source-code">    code="modelbuild/pipelines/evaluation.py",</p>
			<p class="source-code">  <a id="_idTextAnchor242"/>  property_files=[evaluation_report],</p>
			<p class="source-code">)</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor243"/>Conditional step</h2>
			<p>In this step, you'll configure a built-in conditional step that will determine whether <a id="_idIndexMarker598"/>to proceed to the next step in the pipeline based on the results of your previous model evaluation step. Setting up a conditional step requires a list of conditions or items that must be true. This is in combination with instructions on the list of steps to execute based on that condition. <em class="italic">Figure 12.8</em> illustrates the inputs and outputs required for a conditional step:</p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B17249_12_08.jpg" alt="Figure 12.8 – Conditional pipeline step&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.8 – Conditional pipeline step</p>
			<p>In this case, we're going to set up a condition using the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) metric. If the metric is less than or equal to <em class="italic">nn</em>, then we will indicate the steps to proceed with using the <strong class="source-inline">if_steps</strong> parameter. In this case, the next steps if the condition <a id="_idIndexMarker599"/>were true would be to register the model and then create the model that packages your model for deployment. You can optionally specify <strong class="source-inline">else_steps</strong> to indicate the next steps to perform if the condition is not true. In this case, we will simply terminate the pipeline if the condition is not true:</p>
			<p class="source-code">from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo</p>
			<p class="source-code">from sagemaker.workflow.condition_step import (</p>
			<p class="source-code">    ConditionStep,</p>
			<p class="source-code">    JsonGet</p>
			<p class="source-code">)</p>
			<p class="source-code">cond_lte = ConditionLessThanOrEqualTo(</p>
			<p class="source-code">    left=JsonGet(</p>
			<p class="source-code">        step=step_eval,</p>
			<p class="source-code">        property_file=evaluation_report,</p>
			<p class="source-code">        json_path="regression_metrics.mse.value"</p>
			<p class="source-code">    ),</p>
			<p class="source-code">    right=6.0</p>
			<p class="source-code">)</p>
			<p class="source-code">step_cond = ConditionStep(</p>
			<p class="source-code">    name="MSECondition",</p>
			<p class="source-code">    conditions=[cond_lte],</p>
			<p class="source-code">    if_steps=[step_register,<a id="_idTextAnchor244"/> step_create_model],</p>
			<p class="source-code">    else_steps=[]</p>
			<p class="source-code">)</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor245"/>Register model step(s)</h2>
			<p>In this final step, you'll package the model and configure a built-in register model (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-register-model">https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-register-model</a>) step that will register your model to a model package group in SageMaker model registry. As seen in <em class="italic">Figure 12.9</em>, the inputs we'll use to register the <a id="_idIndexMarker600"/>model contain information about the packaged model, such as the model version, estimator, and S3 location of the model artifact. This information, when combined with additional information such as model metrics and inference specifications, is used to register the model version:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B17249_12_09.jpg" alt="Figure 12.9 – Conditional pipeline step&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.9 – Conditional pipeline step</p>
			<p>This step will use data from the prior steps in the pipeline to register the model and centrally store <a id="_idIndexMarker601"/>key metadata about this specific model version. In addition, you'll see an <strong class="source-inline">approval_status parameter</strong>. This parameter can be used to trigger downstream deployment processes (these will be discussed in more detail under SageMaker Projects):</p>
			<p class="source-code">from sagemaker.model_metrics import MetricsSource, ModelMetrics</p>
			<p class="source-code">from sagemaker.workflow.step_collections import RegisterModel</p>
			<p class="source-code">model_metrics = ModelMetrics(</p>
			<p class="source-code">    model_statistics=MetricsSource(</p>
			<p class="source-code">        s3_uri="{}/evaluation.json".format(</p>
			<p class="source-code">step_eval.arguments["ProcessingOutputConfig"]["Outputs"][0]["S3Output"]["S3Uri"]</p>
			<p class="source-code">        ),</p>
			<p class="source-code">        content_type="application/json",</p>
			<p class="source-code">    )</p>
			<p class="source-code">)</p>
			<p class="source-code">step_register = RegisterModel(</p>
			<p class="source-code">    name="RegisterModel",</p>
			<p class="source-code">    estimator=xgb_train,</p>
			<p class="source-code">    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,</p>
			<p class="source-code">    content_types=["text/csv"],</p>
			<p class="source-code">    response_types=["text/csv"],</p>
			<p class="source-code">    inference_instances=["ml.t2.medium", "ml.m5.xlarge"],</p>
			<p class="source-code">    transform_instances=["ml.m5.xlarge"],</p>
			<p class="source-code">    model_package_group_name=model_package_group_name,</p>
			<p class="source-code">    approval_status=model_approval_s<a id="_idTextAnchor246"/>tatus,</p>
			<p class="source-code">    model_metrics=model_metrics,</p>
			<p class="source-code">)</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor247"/>Creating the pipeline</h2>
			<p>In the preceding steps, we configured the tasks and steps that will be used as part of the model build pipeline. We now need to chain those steps together to create the SageMaker Pipeline. </p>
			<p>When configuring pipeline steps and creating a SageMaker pipeline, it is important to identify the <a id="_idIndexMarker602"/>parameters that could vary per pipeline execution and may be more dynamic. For example, the instance type for processing or training may be something you want to be able to change with each execution of your pipeline without directly modifying your pipeline code. This is where parameters become important in being able to dynamically pass in parameters at execution time. This allows you to change configurations (such as changing the instance type parameters) with each execution of your pipeline, based on different environments or as your data grows. </p>
			<p>The following code shows the chaining together of our previously configured pipeline steps, as well as identifying the parameters we want to be able to pass in on each execution: </p>
			<p class="source-code">from sagemaker.workflow.pipeline import Pipeline</p>
			<p class="source-code">pipeline_name = f"WeatherPipeline"</p>
			<p class="source-code">pipeline = Pipeline(</p>
			<p class="source-code">    name=pipeline_name,</p>
			<p class="source-code">    parameters=[</p>
			<p class="source-code">        processing_instance_type,</p>
			<p class="source-code">        processing_instance_count,</p>
			<p class="source-code">        training_instance_type,</p>
			<p class="source-code">        model_approval_status,</p>
			<p class="source-code">        input_data</p>
			<p class="source-code">    ],</p>
			<p class="source-code">    steps=[step_pro<a id="_idTextAnchor248"/>cess, step_train, step_eval, step_cond],</p>
			<p class="source-code">)</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor249"/>Executing the pipeline</h2>
			<p>Now that we've defined and configured our steps and the pipeline itself, we want to be able to execute the pipeline. To do this, you'll need to perform a few steps. These steps need to be performed for each pipeline execution. A pipeline can be started in multiple ways: </p>
			<ul>
				<li>Programmatically within a notebook (as shown in the example notebook for this chapter)</li>
				<li>Under Pipelines in the SageMaker Studio UI</li>
				<li>Programmatically via another resource</li>
				<li>Through an EventBridge source triggered by an event or schedule</li>
			</ul>
			<p>In this section, we'll <a id="_idIndexMarker603"/>focus on the steps required to execute your pipeline from your example notebook. First, you need to submit the pipeline definition to the SageMaker Pipelines service. This is done through an <strong class="source-inline">upsert</strong> that passes in the IAM role as an argument. Keep in mind that an <strong class="source-inline">upsert</strong> will create a pipeline definition if it doesn't exist or update the pipeline if it does. Also, the role that is passed is used by SageMaker Pipelines to create and launch all of the tasks defined in the steps. Therefore, you need to ensure that the role is scoped to the API permissions you need for your pipeline. It's a best practice to only include the API permissions that are actually needed so as to avoid overly permissive roles.</p>
			<p>In the following code, you need to load the pipeline definition and then submit that definition through <strong class="source-inline">upsert</strong>:</p>
			<p class="source-code">import json</p>
			<p class="source-code">json.loads(pipeline.definition())</p>
			<p class="source-code">pipeline.upsert(role_arn=role)</p>
			<p>Once your pipeline definition is submitted, you're ready to start the pipeline using the following code: </p>
			<p class="source-code">execution = pipeline.start()</p>
			<p>There are multiple ways to check the status and progress of your pipeline steps. You can view your <a id="_idIndexMarker604"/>pipeline in the Studio console and click on each step to get metadata about each step, including the step logs. In addition, you can programmatically check the status of your pipeline execution. To do this, you can run <strong class="source-inline">execution.describe()</strong> to view the pipeline execution status, or <strong class="source-inline">execution.list_steps()</strong> to view the execution status and each step.</p>
			<p>Running your pipelines ad hoc from a notebook is often acceptable during your model-building activities. However, when you're ready to move your models to production, it's common at that stage to find the most consistent and repeatable ways to trigger or schedule your model-building pipelines for model retraining. </p>
			<p>To do this, you can utilize the integration between SageMaker Pipelines and Amazon EventBridge (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html">https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html</a>). This integration allows you to trigger the execution of your SageMaker pipeline through event rules. These rules can be based on an event, such as the completion of<a id="_idTextAnchor250"/> an AWS Glue job, or they can be scheduled. </p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor251"/>Pipeline recommended practices</h2>
			<p>In this section, we covered how to set up a SageMaker pipeline using your example weather use case. As you build your own pipelines, they will likely vary in terms of the configuration <a id="_idIndexMarker605"/>required and the steps that should be included. However, the following general recommendations apply across use cases (unique considerations are highlighted where applicable):</p>
			<ol>
				<li value="1">SageMaker Pipelines has built-in steps supporting a variety of SageMaker jobs and the ability to utilize callback for custom steps. The built-in integrations with SageMaker steps simplify building and managing the pipeline. It is therefore recommended to <strong class="bold">utilize SageMaker-native steps for the tasks in your pipeline</strong> when possible. </li>
				<li><strong class="bold">Utilize runtime parameters</strong> for job arguments that are more likely to change between executions or environments, such as the size or number of ML instances running your training or processing jobs. This allows you to pass values in when you start the execution of the pipeline, as opposed to modifying your pipeline code every time. </li>
				<li><strong class="bold">Enable step caching</strong> to take advantage of eliminating unnecessary execution of steps <a id="_idIndexMarker606"/>in your pipeline. This will reduce costs, as well as reducing pipeline time when a previous pipeline step has already been successfully executed with the same parameters. </li>
			</ol>
			<p>In this section, we covered automating your model build ML workflows using SageMaker Pipelines. In the next section, we'll cover creating an end-to-end ML pipeline that goes beyon<a id="_idTextAnchor252"/>d automation and incorporates CI/CD practices.</p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor253"/>Creating CI/CD pipelines using Amazon SageMaker Projects</h1>
			<p>In this section, we'll discuss using Amazon SageMaker Projects to incorporate CI/CD practices <a id="_idIndexMarker607"/>into your ML pipelines. SageMaker Projects is a service that uses SageMaker Pipelines and the SageMaker model registry, in combination with CI/CD tools, to automatically provision and configure CI/CD pipelines for ML. <em class="italic">Figure 12.10</em> illustrates <a id="_idIndexMarker608"/> the core components of SageMaker Projects. With Projects, you have the advantage of a CD pipeline, source code versioning, and automatic triggers for pipeline execution: </p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B17249_12_10.jpg" alt=" Figure 12.10 – SageMaker Projects&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 12.10 – SageMaker Projects</p>
			<p>Projects are made available through built-in SageMaker MLOps project templates or by creating <a id="_idIndexMarker609"/>your own organization's MLOps templates. The underlying templates are offered through AWS Service Catalog, via SageMaker Studio, and contain CloudFormation templates that <a id="_idIndexMarker610"/>preconfigure CI/CD pipelines for the selected template. Because projects rely on CloudFormation to provision pipelines, this ensures the practice of IaC/CaC to be able to consistently and reliably create CI/CD ML pipelines. </p>
			<p>There are three core types of built-in SageMaker MLOps project templates. <em class="italic">Figure 12.11</em> shows the three primary types: 1. <strong class="bold">Build and Train Pipeline</strong>, 2. <strong class="bold">Deploy Pipeline</strong>, 3. <strong class="bold">Build, Train, and Deploy Pipeline</strong>.</p>
			<p>Refer to the following figure:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B17249_12_11_new.jpg" alt="Figure 12.11 – SageMaker Projects&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.11 – SageMaker Projects</p>
			<p>First, there is a build and train template. This covers the tasks required in data preparation, feature engineering, model training, and evaluation. This template is useful <a id="_idIndexMarker611"/>when you are performing model build activities on SageMaker but deploying your model somewhere else. It is also useful if you have batch-only use cases. In this case, Projects <a id="_idIndexMarker612"/>will automatically provision and seed a source code repository for a model build pipeline, set up pipeline triggers for changes to that code repository, and create a model group in the model registry. You are then responsible for going in and modifying that pipeline code to match your use case.</p>
			<p>Second, there is a model deployment template. This template is useful when you are looking to standardize SageMaker for hosting. In this case, Projects will automatically provision and seed a source code repository for a model deploy pipeline that deploys to a SageMaker endpoint based on triggers and information pulled from the model registry. </p>
			<p>Finally, there are end-to-end templates that cover all phases, including build, train, and deploy. These templates cover AWS Developer Services (AWS CodePipeline, AWS CodeCommit, AWS CodeBuild), or allow the option to utilize third-party source code repositories (GitHub, GitHub Enterprise, BitBucket, or Jenkins) for orchestration. In this case, Projects will automatically provision and seed source code for <a id="_idIndexMarker613"/>both model build and model deploy activities. Projects will also set up the triggers for both <a id="_idIndexMarker614"/>model build, and model deploy activities. Again, you are then responsible for going in and modifying seed code to meet your use case. </p>
			<p>In this section, we examined SageMaker projects. We concluded that it is a service that can be used to incorporate CI/CD practices into your ML pipelines. We'll now cover some of the <a id="_idTextAnchor254"/>recommended practices when using SageMaker projects.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor255"/>SageMaker projects recommended practices</h2>
			<p>In the preceding section, we covered SageMaker projects as a way to incorporate CI/CD practices <a id="_idIndexMarker615"/>into your ML pipelines by using a managed AWS service that will automatically provision and configure the integrations that are required. We'll now cover some of the general recommended practices when using SageMaker projects. </p>
			<p>As you use SageMaker projects, the customizations for your use case can vary between customizing the code within the built-in MLOps project templates or creating your own fully custom MLOps project templates. As a result, there can be a lot of variance between pipelines in order to meet the requirements of your organization and use case. However, there are some general recommendations that apply across use cases, as follows:</p>
			<ul>
				<li>Utilize built-in MLOps project templates when they meet your requirements. </li>
				<li>When you have unique requirements, such as additional deployment quality gates, create custom MLOps project templates. </li>
				<li>When creating custom MLOps project templates, it is often easier to use the AWS CloudFormation templates used for the built-in MLOps project templates as a starting point and then modify accordingly. All of the built-in MLOps project templates are available and visible in AWS Service Catalog. </li>
			</ul>
			<p>In this section, we covered adding CI/CD practices to your automated workflows using <a id="_idIndexMarker616"/>SageMaker projects. We also discussed the MLOps project template options that are available. Finally, we discussed additional considerati<a id="_idTextAnchor256"/>ons and best practices when using SageMaker projects. </p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor257"/>Summary</h1>
			<p>In this chapter, we first covered general considerations for automating your SageMaker workflows. We then discussed automating your SageMaker model build workflows, specifically through using SageMaker Pipelines. The steps required to build out a pipeline for your weather use case were highlighted in order to illustrate SageMaker Pipeline usage. Finally, we discussed how you can enhance that automated model build workflow by using SageMaker projects to incorporate CI/CD practices, in addition to the automation offered by SageMaker Pipelines. </p>
			<p>In the next chapter, we'll discuss the AWS Well-Architected Framework, specifically looking at how best practices across each Well-Architected pillar map to SageMaker workloads. </p>
		</div>
	</body></html>