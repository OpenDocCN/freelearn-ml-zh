<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Video Surveillance, Background Modeling, and Morphological Operations</h1>
                </header>
            
            <article>
                
<p>In this chapter, we are going to learn how to detect a moving object in a video taken from a static camera. This is used extensively in video surveillance systems. We will discuss the different characteristics that can be used to build this system. We will learn about background modeling and see how we can use it to build a model of the background in a live video. Once we do this, we will combine all the blocks to detect the object of interest in the video.</p>
<p>By the end of this chapter, you should be able to answer the following questions:</p>
<ul>
<li>What is naive background subtraction?</li>
<li>What is frame differencing?</li>
<li>How do we build a background model?</li>
<li>How do we identify a new object in a static video?</li>
<li>What is morphological image processing and how is it related to background modeling?</li>
<li>How do we achieve different effects using morphological operators?</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter requires familiarity with the basics of the C++ programming language. All the code used in this chapter can be downloaded from the following GitHub link: <a href="https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_08">https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_08</a>. The code can be executed on any operating system, though it is only tested on Ubuntu.</p>
<p>Check out the following video to see the Code in Action:</p>
<p><a href="http://bit.ly/2SfqzRo">http://bit.ly/2SfqzRo</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding background subtraction</h1>
                </header>
            
            <article>
                
<p>Background subtraction is very useful in video surveillance. Basically, the background subtraction technique performs really well in cases where we have to detect moving objects in a static scene. How is this useful for video surveillance? The process of video surveillance involves dealing with constant data flow. The data stream keeps coming in and we need to analyze it to recognize any suspicious activity. Let's consider the example of a hotel lobby. All the walls and furniture have a fixed location. If we build a background model, we can use it to identify suspicious activity in the lobby. We are taking advantage of the fact that the background scene remains static (which happens to be true in this case). This helps us avoid any unnecessary computational overhead. As the name indicates, this algorithm works by detecting and assigning each pixel of an image to two classes, either the background (assumed static and stable) or the foreground, and subtracting it from the current frame to obtain the foreground image part, which includes moving objects such as persons, cars, and so on. With the static assumption, the foreground objects will naturally correspond to objects or people moving in front of the background.</p>
<p>In order to detect moving objects, we need to build a model of the background. This is not the same as direct frame differencing, because we are actually modeling the background and using this model to detect moving objects. When we say that we are modeling the background, we are basically building a mathematical formula that can be used to represent the background. This is much better than the simple frame-differencing technique. This technique tries to detect static parts of the scene and then include small updates in the build statistic formula of the background model. This background model is then used to detect background pixels. So, it's an adaptive technique that can adjust according to the scene.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive background subtraction</h1>
                </header>
            
            <article>
                
<p>Let's start the discussion from the beginning. What does a background subtraction process look like? Consider the following image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-549 image-border" src="assets/3cbf0d44-4e2a-4214-a7ab-899166d16f4c.png" style="width:32.92em;height:21.92em;"/></div>
<p>The previous image represents the background scene. Now, let's introduce a new object into this scene:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-550 image-border" src="assets/86da43cc-6484-44cc-b145-0e022c5da798.png" style="width:33.50em;height:22.33em;"/></div>
<p>As we can see, there is a new object in the scene. So, if we compute the difference between this image and our background model, you should be able to identify the location of the TV remote:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-551 image-border" src="assets/f5c47d1f-cfbd-4860-a7fa-e1ed8ccf8e22.png" style="width:34.17em;height:22.67em;"/></div>
<p>The overall process looks like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-552 image-border" src="assets/e067d24c-4808-41e4-bead-526939174095.png" style="width:40.33em;height:20.50em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Does it work well?</h1>
                </header>
            
            <article>
                
<p>There's a reason we call it the <strong>naive</strong> approach! It works under ideal conditions and, as we know, nothing is ideal in the real world. It does a reasonably good job of computing the shape of the given object, but it does so under some constraints. One of the main requirements of this approach is that the color and intensity of the object should be sufficiently different from that of the background. Some of the factors that affect this kind of algorithm are image noise, lighting conditions, and autofocus in cameras.</p>
<p>Once a new object enters our scene and stays there, it will be difficult to detect new objects that are in front of it. This is because we are not updating our background model, and the new object is now a part of our background. Consider the following image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-553 image-border" src="assets/93f3155d-4cf6-4c86-86f9-fd03569dbfb2.png" style="width:24.83em;height:16.58em;"/></div>
<p>Now, let's say a new object enters our scene:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-554 image-border" src="assets/948d01ff-ce36-45ff-842f-bfd3c6c61f82.png" style="width:24.92em;height:16.58em;"/></div>
<p>We detect this to be a new object, which is fine! Let's say another object comes into the scene:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-555 image-border" src="assets/0665a410-2e58-4614-9565-c1682e249b9e.png" style="width:25.33em;height:16.92em;"/></div>
<p>It will be difficult to identify the location of these two different objects because their locations are overlapping. Here's what we get after subtracting the background and applying the threshold:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-556 image-border" src="assets/33e38517-b173-4bcb-8a68-0c38527fab88.png" style="width:48.67em;height:16.42em;"/></div>
<p>In this approach, we assume that the background is static. If some parts of our background start moving, those parts will start getting detected as new objects. So, even movements that are minor, say a waving flag, will cause problems in our detection algorithm. This approach is also sensitive to changes in illumination and it cannot handle any camera movement. Needless to say, it's a delicate approach! We need something that can handle all these things in the real world.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Frame differencing</h1>
                </header>
            
            <article>
                
<p>We know that we cannot keep a static background image pattern that can be used to detect objects. One of the ways to fix this would be by using frame differencing. It is one of the simplest techniques we can use to see what parts of the video are moving. When we consider a live video stream, the difference between successive frames gives a lot of information. The concept is fairly straightforward! We just take the difference between successive frames and display the differences between them.</p>
<p>If I move my laptop rapidly, we can see something like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-557 image-border" src="assets/d1bfa52d-7bf1-44bb-a853-2f9678f49599.png" style="width:28.17em;height:16.83em;"/></div>
<p>Instead of the laptop, let's move the object and see what happens. If I rapidly shake my head, it will look something like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-558 image-border" src="assets/a604d5b7-cb16-4cdd-833f-a498254df1df.png" style="width:28.25em;height:16.08em;"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As you can see from the previous images, only the moving parts of the video get highlighted. This gives us a good starting point to see what areas are moving in the video. Let's look at the function to compute the frame differences<span>:</span></p>
<pre>Mat frameDiff(Mat prevFrame, Mat curFrame, Mat nextFrame)<br/>{<br/>    Mat diffFrames1, diffFrames2, output;<br/>    <br/>    // Compute absolute difference between current frame and the next<br/>    absdiff(nextFrame, curFrame, diffFrames1);<br/>    <br/>    // Compute absolute difference between current frame and the previous <br/>    absdiff(curFrame, prevFrame, diffFrames2);<br/>    <br/>    // Bitwise "AND" operation between the previous two diff images<br/>    bitwise_and(diffFrames1, diffFrames2, output);<br/>    <br/>    return output;<br/>}</pre>
<p>Frame differencing is fairly straightforward! You compute the absolute differences between the current frame and the previous frame, and between the current frame and the next frame. We then take these frame differences and apply a bitwise <strong>AND</strong> operator. This will highlight the moving parts in the image. If you just compute the difference between the current frame and the previous frame, it tends to be noisy. Hence, we need to use the bitwise AND operator between successive frame differences to get some stability when we see the moving objects.</p>
<p>Let's look at the function that can extract and return a frame from the webcam:</p>
<pre>Mat getFrame(VideoCapture cap, float scalingFactor)<br/>{<br/>    Mat frame, output;<br/><br/>    // Capture the current frame<br/>    cap &gt;&gt; frame;<br/><br/>    // Resize the frame<br/>    resize(frame, frame, Size(), scalingFactor, scalingFactor, INTER_AREA);<br/><br/>    // Convert to grayscale<br/>    cvtColor(frame, output, COLOR_BGR2GRAY);<br/><br/>    return output;<br/>}</pre>
<p>As we can see, it's pretty straightforward. We just need to resize the frame and convert it to grayscale. Now that we have the helper functions ready, let's look at the main function and see how it all comes together:</p>
<pre>int main(int argc, char* argv[])<br/>{<br/>    Mat frame, prevFrame, curFrame, nextFrame;<br/>    char ch;<br/><br/>    // Create the capture object<br/>    // 0 -&gt; input arg that specifies it should take the input from the webcam<br/>    VideoCapture cap(0);<br/><br/>    // If you cannot open the webcam, stop the execution!<br/>    if(!cap.isOpened())<br/>        return -1;<br/><br/>    //create GUI windows<br/>    namedWindow("Frame");<br/><br/><br/>    // Scaling factor to resize the input frames from the webcam<br/>    float scalingFactor = 0.75;<br/><br/>    prevFrame = getFrame(cap, scalingFactor);<br/>    curFrame = getFrame(cap, scalingFactor);<br/>    nextFrame = getFrame(cap, scalingFactor);<br/><br/>    // Iterate until the user presses the Esc key<br/>    while(true)<br/>    {<br/>        // Show the object movement<br/>        imshow("Object Movement", frameDiff(prevFrame, curFrame, nextFrame));<br/><br/>        // Update the variables and grab the next frame<br/>        prevFrame = curFrame;<br/>        curFrame = nextFrame;<br/>        nextFrame = getFrame(cap, scalingFactor);<br/><br/>        // Get the keyboard input and check if it's 'Esc'<br/>        // 27 -&gt; ASCII value of 'Esc' key<br/>        ch = waitKey( 30 );<br/>        if (ch == 27) {<br/>            break;<br/>        }<br/>    }<br/>    // Release the video capture object<br/>    cap.release();<br/><br/>    // Close all windows<br/>    destroyAllWindows();<br/><br/>    return 1;<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How well does it work?</h1>
                </header>
            
            <article>
                
<p>As we can see, frame differencing addresses a couple of important problems we faced earlier. It can quickly adapt to lighting changes or camera movement. If an object comes in to the frame and stays there, it will not be detected in future frames. One of the main concerns of this approach is about detecting uniformly colored objects. It can only detect the edges of a uniformly colored object. The reason is that a large portion of this object will result in very low pixel differences:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-559 image-border" src="assets/2f8d809a-d3a8-4829-ad24-ef555b51ccb0.png" style="width:12.75em;height:10.92em;"/></div>
<p>Let's say this object moved slightly. If we compare this with the previous frame, it will look like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-560 image-border" src="assets/e8abbc81-a938-4c4c-b582-c1547bf10bb7.png" style="width:14.58em;height:13.00em;"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Hence, we have very few pixels that are labeled on that object. Another concern is that it is difficult to detect whether an object moving toward the camera or away from it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Mixture of Gaussians approach</h1>
                </header>
            
            <article>
                
<p>Before we talk about <strong>Mixture of Gaussians</strong> (<strong>MOG</strong>), let's see what a <strong>mixture model</strong> is. A mixture model is just a statistical model that can be used to represent the presence of subpopulations within our data. We don't really care about what category each data point belongs to. All we need to do is identify that the data has multiple groups inside it. If we represent each subpopulation using the Gaussian function, then it's called Mixture of Gaussians. Let's consider the following photograph:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-561 image-border" src="assets/8ceed2f7-c122-4df1-804e-7803c4343330.png" style="width:29.75em;height:19.83em;"/></div>
<p>Now, as we gather more frames in this scene, every part of the image will gradually become a part of the background model. This is what we discussed earlier in the <em>Frame differencing</em> section as well. If a scene is static, the model adapts itself to make sure the background model is updated. The foreground mask, which is supposed to represent the foreground object, looks like a black image at this point because every pixel is part of the background model.</p>
<div class="packt_infobox">OpenCV has multiple algorithms implemented for the Mixture of Gaussians approach. One of them is called <strong>MOG</strong> and the other is called <strong>MOG2:</strong> <span>refer to</span> this link for a detailed explanation: <a href="http://docs.opencv.org/master/db/d5c/tutorial_py_bg_subtraction.html#gsc.tab=0"><span class="URLPACKT">http://docs.opencv.org/master/db/d5c/tutorial_py_bg_subtraction.html#gsc.tab=0</span></a>. You will also be able check out the original research papers that were used to implement these algorithms.</div>
<p>Let's wait for some time and then introduce a new object into the scene. Let's look at what the new foreground mask looks like, using the MOG2 approach:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-563 image-border" src="assets/5ee38a6f-6e03-4193-9f5d-3ced8ffc8ead.png" style="width:43.92em;height:25.50em;"/></div>
<p>As you can see, the new objects are being identified correctly. Let's look at the interesting part of the code (you can get the full code in the <kbd>.cpp</kbd> files):</p>
<pre>int main(int argc, char* argv[])<br/>{<br/><br/>    // Variable declaration and initialization<br/>    ....<br/>    // Iterate until the user presses the Esc key<br/>    while(true)<br/>    {<br/>        // Capture the current frame<br/>        cap &gt;&gt; frame;<br/><br/>        // Resize the frame<br/>        resize(frame, frame, Size(), scalingFactor, scalingFactor, INTER_AREA);<br/><br/>        // Update the MOG2 background model based on the current frame<br/>        pMOG2-&gt;apply(frame, fgMaskMOG2);<br/><br/>        // Show the MOG2 foreground mask<br/>        imshow("FG Mask MOG 2", fgMaskMOG2);<br/><br/>        // Get the keyboard input and check if it's 'Esc'<br/>        // 27 -&gt; ASCII value of 'Esc' key<br/>        ch = waitKey( 30 );<br/>        if (ch == 27) {<br/>            break;<br/>        }<br/>    }<br/><br/>    // Release the video capture object<br/>    cap.release();<br/><br/>    // Close all windows<br/>    destroyAllWindows();<br/><br/>    return 1;<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What happened in the code?</h1>
                </header>
            
            <article>
                
<p>Let's quickly go through the code and see what's happening there. We use the Mixture of Gaussians model to create a background subtractor object. This object represents the model that will be updated as and when we encounter new frames from the webcam. We initialized two background subtraction models—<kbd>BackgroundSubtractorMOG</kbd> and <kbd>BackgroundSubtractorMOG2</kbd>. They represent two different algorithms that are used for background subtraction. The first one refers to the paper by <em>P</em>. <em>KadewTraKuPong</em> and <em>R</em>. <em>Bowden,</em> titled <em>An Improved Adaptive Background Mixture Model for Real-time Tracking with Shadow Detection</em>. You can check it out at <a href="http://personal.ee.surrey.ac.uk/Personal/R.Bowden/publications/avbs01/avbs01.pdf"><span class="URLPACKT">http://personal.ee.surrey.ac.uk/Personal/R.Bowden/publications/avbs01/avbs01.pdf</span></a>. The second one refers to the paper by <em>Z</em>. <em>Zivkovic,</em> titled <em>Improved Adaptive Gaussian Mixture Model for Background Subtraction</em>. You can check it out here: <a href="http://www.zoranz.net/Publications/zivkovic2004ICPR.pdf"><span class="URLPACKT">http://www.zoranz.net/Publications/zivkovic2004ICPR.pdf</span></a>.<span><br/></span>We start an infinite <kbd><span class="CodeInTextPACKT">while</span></kbd> loop and continuously read the input frames from the webcam. With each frame, we update the background model, as indicated in the following lines:</p>
<pre><span>pMOG2-&gt;apply(frame, fgMaskMOG2);</span></pre>
<p>The background model gets updated in these steps. Now, if a new object enters the scene and stays there, it will become part of the background model. This helps us overcome one of the biggest shortcomings of the <strong>naive</strong> background subtraction model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Morphological image processing</h1>
                </header>
            
            <article>
                
<p>As we discussed earlier, background subtraction methods are affected by many factors. Their accuracy depends on how we capture the data and how it's processed. One of the biggest factors that affects these algorithms is the noise level. When we say <strong>noise</strong>, we are talking about things such as graininess in an image and isolated black/white pixels. These issues tend to affect the quality of our algorithms. This is where morphological image processing comes into play. Morphological image processing is used extensively in a lot of real-time systems to ensure the quality of the output. Morphological image processing refers to processing the shapes of features in the image; for example, you can make a shape thicker or thinner. Morphological operators rely not on how the pixels are ordered in an image, but on their values. This is why they are really well suited to manipulating shapes in binary images. Morphological image processing can be applied to grayscale images as well, but the pixel values will not matter much.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What's the underlying principle?</h1>
                </header>
            
            <article>
                
<p>Morphological operators use a structuring element to modify an image. What is a structuring element? A structuring element is basically a small shape that can be used to inspect a small region in the image. It is positioned at all the pixel locations in the image so that it can inspect that neighborhood. We basically take a small window and overlay it on a pixel. Depending on the response, we take appropriate action at that pixel location.</p>
<p>Let's consider the following input image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-564 image-border" src="assets/7f6ac4ba-93be-4e3f-8517-4b5968ced5d9.png" style="width:13.75em;height:3.33em;"/></div>
<p>We are going to apply a bunch of morphological operations to this image to see how the shape changes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Slimming the shapes</h1>
                </header>
            
            <article>
                
<p>We achieve this effect using an operation called <strong>erosion</strong>. This is the operation that makes a shape thinner by peeling the boundary layers of all the shapes in the image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-565 image-border" src="assets/2ec264b3-8600-4f74-99fb-308208b2475f.png" style="width:67.00em;height:17.25em;"/></div>
<p>Let's look at the function that performs morphological erosion:</p>
<pre>Mat performErosion(Mat inputImage, int erosionElement, int erosionSize)<br/>{<br/><br/>    Mat outputImage;<br/>    int erosionType;<br/><br/>    if(erosionElement == 0)<br/>        erosionType = MORPH_RECT;<br/>    else if(erosionElement == 1)<br/>        erosionType = MORPH_CROSS;<br/>    else if(erosionElement == 2)<br/>        erosionType = MORPH_ELLIPSE;<br/><br/>    // Create the structuring element for erosion<br/>    Mat element = getStructuringElement(erosionType, Size(2*erosionSize + 1, 2*erosionSize + 1), Point(erosionSize, erosionSize));<br/><br/>    // Erode the image using the structuring element<br/>    erode(inputImage, outputImage, element);<br/><br/>    // Return the output image<br/>    return outputImage;<br/>}</pre>
<p>You can check out the full code in the <kbd>.cpp</kbd> files to understand how to use this function. We basically build a structuring element using a built-in OpenCV function. This object is used as a probe to modify each pixel based on certain conditions. These conditions refer to what's happening around that particular pixel in the image. For example, is it surrounded by white pixels? Or is it surround by black pixels? Once we have an answer, we take the appropriate action.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thickening the shapes</h1>
                </header>
            
            <article>
                
<p>We use an operation called <strong>dilation</strong> to achieve thickening. This is the operation that makes a shape thicker by adding boundary layers to all the shapes in the image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-566 image-border" src="assets/d747f0d4-9b2d-4ed4-b07e-f7b8ebe05435.png" style="width:162.50em;height:41.42em;"/></div>
<p>Here is the code to do it:</p>
<pre>Mat performDilation(Mat inputImage, int dilationElement, int dilationSize)<br/>{<br/>    Mat outputImage;<br/>    int dilationType;<br/><br/>    if(dilationElement == 0)<br/>        dilationType = MORPH_RECT;<br/>    else if(dilationElement == 1)<br/>        dilationType = MORPH_CROSS;<br/>    else if(dilationElement == 2)<br/>        dilationType = MORPH_ELLIPSE;<br/><br/>    // Create the structuring element for dilation<br/>    Mat element = getStructuringElement(dilationType, Size(2*dilationSize + 1, 2*dilationSize + 1), Point(dilationSize, dilationSize));<br/><br/>    // Dilate the image using the structuring element<br/>    dilate(inputImage, outputImage, element);<br/><br/>    // Return the output image<br/>    return outputImage;<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other morphological operators</h1>
                </header>
            
            <article>
                
<p>Here are some other interesting morphological operators. Let's look at the output image first. We can look at the code at the end of this section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Morphological opening</h1>
                </header>
            
            <article>
                
<p>This is the operation that <strong>opens</strong> a shape. This operator is frequently used for noise removal in images. It's basically erosion followed by dilation. Morphological opening removes small objects from the foreground in the image by placing them in the background:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-567 image-border" src="assets/33df80c3-af99-436a-ac63-dec98b9d9492.png" style="width:162.50em;height:41.92em;"/></div>
<p>Here is the function to perform morphological opening:</p>
<pre>Mat performOpening(Mat inputImage, int morphologyElement, int morphologySize)<br/>{<br/><br/>    Mat outputImage, tempImage;<br/>    int morphologyType;<br/><br/>    if(morphologyElement == 0)<br/>        morphologyType = MORPH_RECT;<br/>    else if(morphologyElement == 1)<br/>        morphologyType = MORPH_CROSS;<br/>    else if(morphologyElement == 2)<br/>        morphologyType = MORPH_ELLIPSE;<br/><br/>    // Create the structuring element for erosion<br/>    Mat element = getStructuringElement(morphologyType, Size(2*morphologySize + 1, 2*morphologySize + 1), Point(morphologySize, morphologySize));<br/><br/>    // Apply morphological opening to the image using the structuring element<br/>    erode(inputImage, tempImage, element);<br/>    dilate(tempImage, outputImage, element);<br/><br/>    // Return the output image<br/>    return outputImage;<br/>}</pre>
<p>As we can see here, we apply <strong>erosion</strong> and <strong>dilation</strong> on the image to perform morphological opening.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Morphological closing</h1>
                </header>
            
            <article>
                
<p>This is the operation that <strong>closes</strong> a shape by filling the gaps, as shown in the following screenshot. This operation is also used for noise removal. It's basically dilation followed by erosion. This operation removes tiny holes in the foreground by changing small objects in the background into the foreground:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-568 image-border" src="assets/e089e366-df33-45b3-a766-5bdd5b8194c3.png" style="width:162.50em;height:42.58em;"/></div>
<p>Let's quickly look at the function to perform morphological closing:</p>
<pre>Mat performClosing(Mat inputImage, int morphologyElement, int morphologySize)<br/>{<br/><br/>    Mat outputImage, tempImage;<br/>    int morphologyType;<br/><br/>    if(morphologyElement == 0)<br/>        morphologyType = MORPH_RECT;<br/>    else if(morphologyElement == 1)<br/>        morphologyType = MORPH_CROSS;<br/>    else if(morphologyElement == 2)<br/>        morphologyType = MORPH_ELLIPSE;<br/><br/>    // Create the structuring element for erosion<br/>    Mat element = getStructuringElement(morphologyType, Size(2*morphologySize + 1, 2*morphologySize + 1), Point(morphologySize, morphologySize));<br/><br/>    // Apply morphological opening to the image using the structuring element<br/>    dilate(inputImage, tempImage, element);<br/>    erode(tempImage, outputImage, element);<br/>    <br/>    // Return the output image<br/>    return outputImage;<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Drawing the boundary</h1>
                </header>
            
            <article>
                
<p>We achieve this using a morphological gradient. This is the operation that draws the boundary around a shape by taking the difference between the dilation and erosion of an image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-569 image-border" src="assets/6c644866-9615-48d3-83a2-e7a56e5c51b7.png" style="width:162.50em;height:42.00em;"/></div>
<p>Let's look at the function to perform morphological gradient:</p>
<pre>Mat performMorphologicalGradient(Mat inputImage, int morphologyElement, int morphologySize)<br/>{<br/>    Mat outputImage, tempImage1, tempImage2;<br/>    int morphologyType;<br/><br/>    if(morphologyElement == 0)<br/>        morphologyType = MORPH_RECT;<br/>    else if(morphologyElement == 1)<br/>        morphologyType = MORPH_CROSS;<br/>    else if(morphologyElement == 2)<br/>        morphologyType = MORPH_ELLIPSE;<br/><br/>    // Create the structuring element for erosion<br/>    Mat element = getStructuringElement(morphologyType, Size(2*morphologySize + 1, 2*morphologySize + 1), Point(morphologySize, morphologySize));<br/>    <br/>    // Apply morphological gradient to the image using the structuring element<br/>    dilate(inputImage, tempImage1, element);<br/>    erode(inputImage, tempImage2, element);<br/><br/>    // Return the output image<br/>    return tempImage1 - tempImage2;<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Top Hat transform</h1>
                </header>
            
            <article>
                
<p>This transform extracts finer details from the images. This is the difference between the input image and its morphological opening. This gives us the objects in the image that are smaller than the structuring element and brighter than the surroundings. Depending on the size of the structuring element, we can extract various objects in the given image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-570 image-border" src="assets/bc19cfb9-b944-406d-8c56-1f397c02d459.png" style="width:162.50em;height:40.83em;"/></div>
<p>If you look at the output image carefully, you can see those black rectangles. It means that the structuring element was able to fit in there, and so those regions are blackened out. Here is the function:</p>
<pre>Mat performTopHat(Mat inputImage, int morphologyElement, int morphologySize)<br/>{<br/><br/>    Mat outputImage;<br/>    int morphologyType;<br/><br/>    if(morphologyElement == 0)<br/>        morphologyType = MORPH_RECT;<br/>    else if(morphologyElement == 1)<br/>        morphologyType = MORPH_CROSS;<br/>    else if(morphologyElement == 2)<br/>        morphologyType = MORPH_ELLIPSE;<br/><br/>    // Create the structuring element for erosion<br/>    Mat element = getStructuringElement(morphologyType, Size(2*morphologySize + 1, 2*morphologySize + 1), Point(morphologySize, morphologySize));<br/><br/>    // Apply top hat operation to the image using the structuring element<br/>    outputImage = inputImage - performOpening(inputImage, morphologyElement, morphologySize);<br/><br/>    // Return the output image<br/>    return outputImage;<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Black Hat transform</h1>
                </header>
            
            <article>
                
<p>This transform extract finer details from the image as well. This is the difference between the morphological closing of an image and the image itself. This gives us the objects in the image that are smaller than the structuring element and darker than its surroundings:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-571 image-border" src="assets/aad1cad8-45b7-4ece-8b18-10f973bc1d45.png" style="width:162.50em;height:39.33em;"/></div>
<p>Let's look at the function to perform a Black Hat transform:</p>
<pre>Mat performBlackHat(Mat inputImage, int morphologyElement, int morphologySize)<br/>{<br/>    Mat outputImage;<br/>    int morphologyType;<br/><br/>    if(morphologyElement == 0)<br/>        morphologyType = MORPH_RECT;<br/>    else if(morphologyElement == 1)<br/>        morphologyType = MORPH_CROSS;<br/>    else if(morphologyElement == 2)<br/>        morphologyType = MORPH_ELLIPSE;<br/><br/>    // Create the structuring element for erosion<br/>    Mat element = getStructuringElement(morphologyType, Size(2*morphologySize + 1, 2*morphologySize + 1), Point(morphologySize, morphologySize));<br/><br/>    // Apply black hat operation to the image using the structuring element<br/>    outputImage = performClosing(inputImage, morphologyElement, morphologySize) - inputImage;<br/><br/>    // Return the output image<br/>    return outputImage;<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about the algorithms that are used for background modeling and morphological image processing. We discussed naive background subtraction and its limitations. We looked at how to get motion information using frame differencing and how it can be limiting when we want to track different types of objects. This led to our discussion about the Mixture of Gaussians. We discussed the formula and how we can implement it. We then discussed morphological image processing, which can be used for various purposes, and different operations were covered to show the use cases.</p>
<p>In the next chapter, we are going to discuss object tracking and the various techniques that can be used to do it.</p>


            </article>

            
        </section>
    </body></html>