- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracking and Visualizing ML Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of **machine learning** (**ML**), **visualization** and **experiment
    tracking systems** play a crucial role. These tools provide a way to understand
    complex data, track experiments, and make informed decisions about model development.
  prefs: []
  type: TYPE_NORMAL
- en: In ML, visualizing data is essential for understanding patterns, relationships,
    and trends. Data visualization tools allow engineers to create charts, graphs,
    and plots that help them explore and analyze their data. With the right visualization
    tool, engineers can quickly identify patterns and anomalies, which can be used
    to improve model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment tracking systems are designed to keep track of the progress of multiple
    experiments. They allow engineers to compare results, identify best practices,
    and avoid repeating mistakes. Experiment tracking tools also help with reproducibility,
    ensuring that experiments can be repeated accurately and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right tools for visualization and experiment tracking is critical.
    There are many open source and commercial options available, each with its strengths
    and weaknesses. It’s important to consider factors such as ease of use, integration
    with other tools, and the specific needs of your project when selecting a tool.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll briefly discuss **TensorBoard**, one of the most widespread
    experiment visualization systems available. We’ll also learn what type of visualizations
    it can provide and the challenges of using it with C++. As for the tracking system,
    we’ll discuss the **MLflow framework** and provide a hands-on example of how to
    use it with C++. This example covers setting up a project, defining experiments,
    logging metrics, and visualizing the training process and showcases the power
    of experiment tracking tools in enhancing the ML development process.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have a clear understanding of why these
    tools are essential for ML engineers and how they can help you achieve better
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding visualization and experiment tracking systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment tracking with MLflow’s REST API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Flashlight library 0.4.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow 2.5.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cpp-httplib` v0.16.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nlohmann` `json` v3.11.2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A modern C++ compiler with C++20 support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CMake build system version >= 3.22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be found in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter13/flashlight](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter13/flashlight)'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding visualization and tracking systems for experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualization and tracking systems for ML experiments are essential components
    of the ML development process. Together, these systems enable engineers to build
    more robust and effective ML models. They also help ensure reproducibility and
    transparency in the development process, which is crucial for scientific rigor
    and collaboration.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization tools provide a graphical representation of data, allowing engineers
    to see patterns, trends, and relationships that might be difficult to detect in
    raw data. This can help engineers gain insights into the behavior of their models,
    identify areas for improvement, and make informed decisions about model design
    and hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment tracking systems allow engineers to log and organize experiments,
    including model architectures, hyperparameters, and training data. These systems
    provide an overview of the entire experimentation process, making it easier to
    compare different models and determine which ones perform best.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at some of the key features of TensorBoard, a powerful visualization
    tool, and understand the essential components of MLflow, an effective experiment
    tracking system.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorBoard is a visualization tool for ML models that provides insights into
    model performance and training progress. It also provides an interactive dashboard
    where users can explore graphs, histograms, scatter plots, and other visualizations
    related to their experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key features of TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '`loss`, `accuracy`, `precision`, `recall`, and `F1 score`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Histogram plots**: TensorBoard also provides histogram plots for a better
    understanding of model performance. These plots can help users understand the
    distribution of layer weight and gradient values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graphs**: Graphs in TensorBoard provide a visual representation of model
    architecture. Users can create graphs to analyze the correlation between inputs
    and outputs or to compare different models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Images**: TensorBoard allows you to display image data and connect such a
    visualization to a training timeline. This can help users analyze input data,
    intermediate outputs, or convolutional filter result visualizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedding projector**: The embedding projector in TensorBoard allows users
    to explore high-dimensional data in lower dimensions using techniques such as
    **principal component analysis** (**PCA**). This feature helps in visualizing
    complex datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comparison**: In TensorBoard, comparison enables users to compare the performance
    of multiple models side by side, making it easy to identify the best-performing
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, TensorBoard doesn’t integrate easily with C++ ML frameworks.
    The native C++ support only exists in the TensorFlow framework. Also, there’s
    only one third-party open source library that allows us to use TensorBoard, and
    it isn’t actively maintained.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard can be integrated with various Python-based deep learning frameworks,
    including TensorFlow, PyTorch, and others. So, if you train your models in Python,
    it makes sense to consider it as an instrument that can help you understand how
    models are performing, identify potential issues, and make informed decisions
    about hyperparameters, data preprocessing, and model design.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, to visualize training data, you can use `gnuplot`-based libraries
    such as `CppPlot`, as we did in the previous chapters. See [*Chapter 3*](B19849_03.xhtml#_idTextAnchor152)
    for the 2D scatter and line plot visualization examples.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLflow is an open source framework that’s designed for **machine learning operations**
    (**MLOps**) and helps teams manage, track, and scale their ML projects. It provides
    a set of tools and features for building, training, and deploying models, as well
    as for monitoring their performance and experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main components of MLflow are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment tracking**: MLflow allows users to track their experiments, including
    hyperparameters, code versions, and metrics. This helps in understanding the impact
    of different configurations on model performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code reproducibility**: With MLflow, users can easily reproduce their experiments
    by tracking code versions and dependencies. This ensures consistency across experiments
    and makes it easier to identify issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Registry**: MLflow provides a Model Registry component where users
    can store, version, and manage their models. This allows for easy collaboration
    and model sharing within teams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with other tools**: MLflow integrates with popular data science
    and ML tools, such as Jupyter Notebook, TensorFlow, PyTorch, and more. This enables
    seamless integration with existing workflows. For non-Python environments, MLflow
    provides the REST API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment options**: MLflow offers various options for deploying models,
    including Docker containers, Kubernetes, and cloud platforms. This flexibility
    allows users to choose the best deployment strategy based on their needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internally, MLflow uses a database to store metadata about experiments, models,
    and parameters. By default, it uses SQLite, but other databases such as PostgreSQL
    and MySQL are also supported. This allows for scalability and flexibility in terms
    of storage requirements. MLflow uses unique identifiers to track objects and operations
    within the platform. These identifiers are used to link different components of
    an experiment together, such as a run and its associated parameters. This makes
    it easy to reproduce experiments and understand the relationships between different
    parts of a workflow. It also provides a REST API for programmatic access to features
    such as model registration, tracking, and model life cycle management. It uses
    YAML configuration files for customizing and configuring MLflow behavior, and
    Python APIs for easy integration with MLflow components and workflows.
  prefs: []
  type: TYPE_NORMAL
- en: So, we can summarize that visualization and experiment tracking systems are
    essential tools for data scientists and engineers to understand, analyze, and
    optimize their ML models. These systems allow users to track the performance of
    different models, compare results, identify patterns, and make informed decisions
    about model development and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how experiment tracking tools can be integrated into ML workflows,
    we’ll provide a concrete example in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment tracking with MLflow’s REST API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s consider an example of an experiment involving a regression model. We’ll
    use MLflow to log performance metrics and the parameters of a model for several
    experiments. While training the model, we’ll visualize the results using a plot
    to show the accuracy and loss curves over time. Finally, we’ll compare the results
    of different experiments using the tracking system so that we can select the best-performing
    model and optimize it further.
  prefs: []
  type: TYPE_NORMAL
- en: This example will demonstrate how experiment tracking can be seamlessly integrated
    into a C++ ML workflow, providing valuable insights and improving the overall
    quality of research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can use MLflow, you need to install it. You can install MLflow using
    `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you’ll need to start a server, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This command starts the local tracking server at `http://localhost:5000`, which
    saves tracking data to the `/samples/Chapter13/mlruns` directory. If you need
    to access the MLflow server from remote machines, you can start the command with
    the `--host` and `--``port` arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Having started tracking the server, we can communicate with it using the REST
    API. The access point to this API is hosted at `http://localhost:5000/api/2.0/mlflow/`.
    MLflow uses JSON as its data representation for the REST API.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement a `REST` client for communicating with the tracking server, we’ll
    use two additional libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cpp-httplib`: For implementing HTTP communication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nlohmann json`: For implementing REST requests and responses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the basic linear regression model will be implemented using the `Flashlight`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll learn how to connect all these pieces. The first part we’ll cover
    is implementing the REST client.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing MLflow’s REST C++ client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two main concepts in MLflow: **experiments** and **runs**. Together,
    they provide a structured approach to managing and tracking ML workflows. They
    help us organize our projects, ensure reproducibility, and facilitate collaboration
    among team members.'
  prefs: []
  type: TYPE_NORMAL
- en: In MLflow, we can organize and track our ML experiments. An experiment can be
    thought of as a container for all the runs related to a specific project or goal.
    It allows you to keep track of different versions of your models, compare their
    performance, and identify the best one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the key features of an experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name**: Each experiment has a unique name that identifies it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tags**: You can add tags to an experiment to categorize it based on different
    criteria.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifacts location**: Artifacts are files that are generated throughout the
    experiment, such as images, logs, and more. MLflow allows you to store and version
    these artifacts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A run represents a single execution of an experiment or a specific task within
    an experiment. Runs are used to record the details of each execution, such as
    its start time, end time, parameters, and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key features of a run are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Start time**: The time when the run started'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**End time**: The time when the run finished'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameters**: Model parameters such as batch size, learning rate, and more'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model**: The model code that was executed during the run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: The results that were produced by the run, including metrics, artifacts,
    and more'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within a run, users can log parameters, metrics, and model representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the main concepts of MLflow’s tracking structure, let’s
    implement the MLflow REST client:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’re going to put all the implementation details for the `REST` client
    in a single `MLFlow` class. The header file should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We made the constructor take a host and the port of the tracking server to communicate
    with. Then, we defined methods to start a named experiment and run inside it,
    as well as methods to log named metrics and parameters. After, we declared an
    instance of the `httplib::Client` class, which will be used for HTTP communication
    with the tracking server. Finally, we provided member variables, which are the
    IDs of the current experiment and run.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s learn how to implement these methods. The constructor implementation
    is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we initialized the `httplib::Client` instance with the host and port values
    to initialize a connection with a tracking server.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code shows the `set_experiment` method’s implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This method initializes an experiment for the following runs. There are two
    parts to this method—one for a new experiment and another for the existing one:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, we checked whether the experiment with the given name existed on the
    server using the following code:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By comparing the results with the `404` and `202` codes, we identified that
    there’s no such experiment or that it already exists.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Because there’s no existing experiment, we created a JSON-based request to
    create a new experiment, as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we passed it as the body of the HTTP request to the server, as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `dump` method of the `nlohmann::json` object was used to convert the JSON
    into a string representation. After we got the result, we used the `handle_result`
    function to check for errors (this function will be discussed in more detail later).
    With the answer in the `res` variable, we took the `experiment_id` value, as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we parsed the string that was returned by the server with the `nlohmann::json::parse`
    function and read the `experiment_id` value from the JSON object into our class
    member variable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the second part of the method, which works when an experiment exists on the
    server, we parsed the response in a JSON object and took the `experiment_id` value.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are two functions named `handle_result` that are used to check response
    codes and report errors if needed. The first one is used to check whether a response
    has some particular code and is implemented as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we checked whether the `httplib::Result` object has a valid response by
    using its Boolean cast operator. If there was a communication error, we threw
    the runtime exception. Otherwise, we returned the response code’s comparison result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The second `handle_result` function is used to check that we got the successful
    answer from the server. The following code snippet shows how it’s implemented:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We used the previous `handle_result` function to check whether the response
    was valid and we got a `200` response code. If it’s true, we’re OK. However, in
    the case of a failure, we must make a detailed report and throw a runtime exception.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These functions help to simplify response error handling code and make it easier
    to debug communications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next two methods we’re going to discuss are `start_run` and `end_run`. These
    methods mark a single run’s bounds, within which we can log metrics, parameters,
    and artifacts. In production code, it makes sense to wrap such functionality into
    some RAII abstraction, but we made two methods for simplicity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `start_run` method can be implemented as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we made a JSON-based request to create a run. This request was filled
    with the current `experiment_id` value and the run’s start time. Then, we sent
    a request to the server and got a response that we checked with the `handle_result`
    function. If we receive an answer, we parse it in the `nlohmann::json` object
    and take the `run_id` value. The `run_id` value is stored in the object member
    and will be used in the following requests. After we call this method, the tracking
    server will write all metrics and parameters into this new run.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To complete the run, we have to tell the server about it. The `end_run` method
    does just this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we made a JSON-based request that includes the `run_id` value, the finished
    status, and the end time. Then, we sent this request to the tracking server and
    checked the response. Notice that we sent the start and end times for a run, at
    which point the server used them to calculate the run duration. Due to this, you’ll
    be able to see how the run duration time depends on its parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have methods for setting an experiment and defining a run, we need
    methods so that we can log metrics values and run parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Logging metric values and running parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The difference between metrics and parameters is that metrics are sequences
    of values within a run. You can log as many values of a single metric as you need.
    Usually, this number equals epochs or batches and MLflow will show live plots
    for these metrics. However, a single parameter can only be logged once per run,
    and it’s typically a training characteristic such as the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'A metric is usually a numeric value, so we’ve made our `log_metric` method
    take a float and an argument for a value. Note that this method takes the metric
    name and the epoch index to make several distinct values for the same metric.
    The method’s implementation is shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we made a JSON-based request that includes the `run_id` value, the metric’s
    name as the `key` field, the metric’s value, the epoch index as the `step` field,
    and the timestamp value. Then, we sent the request to the tracking server and
    checked the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'A parameter value can have an arbitrary value type, so we used C++ templates
    to write a single method to process different value types. There are two `log_param`
    functions here—the first is a template function that converts any suitable parameter
    value into a string, whereas the second only takes a parameter name and a string
    value as arguments. The template can be implemented like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This template simply redirects a call to the second function after the value
    is converted into a string with the `std::to_string` function. So, if the value’s
    type can’t be converted into a string, a compilation error will occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second `log_param` function’s implementation can be seen in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we made a JSON-based request that includes the current `run_id` value,
    the parameter name as the `key` field, and the value. Then, we just sent the request
    and checked the response.
  prefs: []
  type: TYPE_NORMAL
- en: The REST API in MLflow is much richer than this; we only covered the basic functions
    here. For example, it’s also capable of accepting model architectures in JSON
    format, logging input datasets, managing experiments and models, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the basic functionality for communicating with the MLflow
    server, let’s learn how to implement an experiments tracking session for a regression
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating experiment tracking into linear regression training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll be using the `Flashlight` library to implement a linear
    regression model and train it. Our code starts with initializing Flashlight and
    connecting to an MLflow server, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we assumed that the tracking server has already been started on localhost.
    After, we set the experiment’s name to `Linear regression`. Now, we can define
    the necessary parameters and start the run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Having configured the run, we can load datasets for training and testing, define
    a model, and create an optimizer and loss function according to the parameters
    we defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we used all the previously defined parameters except for the epoch
    number. Now, we’re ready to define the training cycle, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The main part of the training cycle looks normal as we implemented this in
    the previous chapters. Note that we have two nested cycles—one for epochs and
    another for batches. At the beginning of the training epoch, we cleared the meter
    that’s used for averaging the training loss metric and put the model into the
    training mode. Then, we cleared the gradients, made a forward pass, calculated
    the loss value, made a backward pass, updated the model weights with the optimizer
    step, and added the loss value to the averaging meter object. After the internal
    cycle, training was completed. At this point, we can log the average training
    loss metric value to the tracking server, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, we logged the train loss value for the epoch with the `epoch_i` index
    and used `train loss` as its name. For every epoch, this logging will add a new
    value for the metric and we’ll be able to see the live plot of how the training
    loss changes over epochs in the MLflow UI. This plot will be shown in the following
    subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the training cycle, for every 10th epoch, we calculate the test loss
    metric, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we’ve checked that the current epoch is the 10th one, we defined an additional
    averaging meter object for the test loss metric and implemented evaluation mode.
    Then, we calculated the loss value for every batch and added these values to the
    averaging meter. At this point, we can implement a loss calculation for the test
    dataset and log the test metric to the tracking server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here, we logged the test loss value for the epoch with the `epoch_i` index and
    used `test loss` as its name. MLflow will provide a plot for this metric too.
    We’ll be able to overlap this plot with the train metric plot to check whether
    there are issues such as overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve finished using the training cycle, we can end the run and log
    its parameters, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, we logged the run parameters with the `end_run` call. This is a requirement
    when using the MLflow API. Note that parameter values can have different types
    and that they were only logged once.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how MLflow will display the program runs with different training
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments tracking process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following figure shows the MLflow UI after the tracking server has been
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Overview of the MLflow UI without experiments and runs](img/B19849_13_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Overview of the MLflow UI without experiments and runs
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see there are no experiments and there’s run information. After executing
    a program run with a set of parameters, the UI will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Overview of the MLflow UI with a single experiment and one
    run](img/B19849_13_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – Overview of the MLflow UI with a single experiment and one run
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, **Linear regression** appeared in the left panel. Also, there’s
    a new record for the run in the right-hand side table. Notice that the run name
    of **peaceful-ray-50** was automatically generated. Here, we can see the start
    time and how much time the run takes. Clicking on the run’s name will open the
    run details page, which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Overview of the run details in the MLflow UI](img/B19849_13_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – Overview of the run details in the MLflow UI
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the start time and date, the experiment ID that this run is
    associated with, the run ID, and its duration. Note that additional information
    might be provided here, such as a username, what datasets were used, tags, and
    the model source. These additional attributes can be also configured with the
    REST API.
  prefs: []
  type: TYPE_NORMAL
- en: At the bottom, we can see the **Parameters** table, where we can find the parameters
    we logged from our code. There’s also the **Metrics** table, which shows the final
    values for our train and test loss value metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we click on the **Model metrics** tab, the following page will be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – The Model metrics page in the MLflow UI](img/B19849_13_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 – The Model metrics page in the MLflow UI
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see the train and test loss metrics plots. These plots show how
    the loss values changed over epochs. Usually, it’s useful to overlap the train
    and test loss plots to see some dependencies. We can do this by clicking on the
    metric’s name on the page displayed in *Figure 13**.3*. The following page will
    be displayed upon clicking **train loss**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – The train loss metric plot](img/B19849_13_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 – The train loss metric plot
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see the plot for the single metric. On this page, we can configure
    some visualization parameters for the plot, such as the smoothness and the step.
    However, in this case, we’re interested in the **Y-axis** field, which allows
    us to add additional metrics to the same plot. If we add the **test loss** metric,
    we’ll see the following page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6 – Overlapping the metric plots](img/B19849_13_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 – Overlapping the metric plots
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have two overlapped plots for the train and test metrics. In this visualization,
    we can see that for the first few epochs, the test loss was greater than the train
    loss, but after the 15th epoch, the loss values were pretty similar. This means
    that there’s no model overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In this instance, we looked at the main regimes of the MLflow UI for the single
    train run. For more advanced cases, there will be pages that consist of artifacts
    and model sources, but we’ve skipped them here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s learn how to work with several runs for an experiment. We ran our
    application again but with a different value for momentum. MLflow shows us that
    we have two runs for the same experiment, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7 – The experiment with two runs](img/B19849_13_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 – The experiment with two runs
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, there are two runs for the experiment. Also, there are only
    two minor differences between them—the names and their duration. To compare the
    runs, we must click on both checkboxes that precede the run names, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8 – Selecting both runs](img/B19849_13_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 – Selecting both runs
  prefs: []
  type: TYPE_NORMAL
- en: 'After selecting both runs, the **Compare** button appears at the top of the
    **Runs** table. Clicking this button opens the following page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_13_09.jpg)![](img/B19849_13_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 – Overview of the runs comparison page
  prefs: []
  type: TYPE_NORMAL
- en: This page shows two runs side by side and shows different visualizations of
    the difference between various metrics and parameters. Note that the run parameter
    differences will also be highlighted. From the left top panel, you can select
    the parameters and metrics you wish to compare. By doing this, we can see that
    the new run with a lower momentum value performs worse. This is indicated by the
    top plot, where lines connect parameters with metrics and there are scales with
    values. This can also be seen at the bottom in the metrics rows, where you can
    compare final metric values.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to use the MLflow UI to explore experiment run
    behavior, as well as how to view metrics visualizations and how to compare different
    runs. All tracked information is saved by the tracking server and can be used
    later, after a server restart, so it’s quite a useful tool for ML practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualization and experiment tracking systems are essential tools for ML engineers.
    They allow us to understand the performance of models, analyze results, and improve
    the overall process.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard is a popular visualization system that provides detailed information
    about model training, including metrics, loss curves, histograms, and more. It
    supports multiple frameworks, including TensorFlow, and allows us to easily compare
    different runs.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow is an open source framework that offers end-to-end solutions for model
    life cycle management. It includes features such as experiment tracking, Model
    Registry, artifact management, and deployment. MLflow helps teams collaborate,
    reproduce experiments, and ensure reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Both TensorBoard and MLflow are powerful tools that can be used together or
    separately, depending on your needs.
  prefs: []
  type: TYPE_NORMAL
- en: After understanding both TensorBoard and MLflow, we implemented a linear regression
    training example with experiment tracking. By doing so, we learned how to implement
    the REST API client for the MLflow server and how to use it to log metrics and
    parameters for an experiment. Then, we explored the MLflow UI, where we learned
    how to view an experiment and its run details, as well as metrics plots, and learned
    how to compare different runs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll learn how to use ML models for computer vision on
    the Android mobile platform using C++.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**MLflow REST** **API**: [https://mlflow.org/docs/latest/rest-api.html](https://mlflow.org/docs/latest/rest-api.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLflow** **documentation**: [https://mlflow.org/docs/latest/index.html](https://mlflow.org/docs/latest/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorBoard** **documentation**: [https://www.tensorflow.org/tensorboard/get_started](https://www.tensorflow.org/tensorboard/get_started)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How to use TensorBoard with* *PyTorch*: [https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
