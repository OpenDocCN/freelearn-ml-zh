<html><head></head><body>
		<div id="_idContainer178" class="Content">
			<h1 id="_idParaDest-205"><em class="italics"><a id="_idTextAnchor207"/>Chapter 5:</em></h1>
		</div>
		<div id="_idContainer179" class="Content">
			<h1 id="_idParaDest-206"><a id="_idTextAnchor208"/>Classification</h1>
		</div>
		<div id="_idContainer180" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Define binary classification in supervised machine learning</li>
				<li class="bullets">Perform binary classification using white-box models: logistic regression and decision trees</li>
				<li class="bullets">Evaluate the performance of supervised classification models</li>
				<li class="bullets">Perform binary classification using black-box ensemble models – Random Forest and XGBoost</li>
				<li class="bullets">Design and develop deep neural networks for classification</li>
				<li class="bullets">Select the best model for a given classification use case</li>
			</ul>
			<p>In this chapter, we will focus on solving classification use cases for supervised learning. We will use a dataset designed for a classification use case, frame a business problem around it, and explore a few popular techniques to solve the problem.</p>
		</div>
		<div id="_idContainer198" class="Content">
			<h2 id="_idParaDest-207"><a id="_idTextAnchor209"/>Introduction</h2>
			<p>Let's quickly brush up on the topics we learned in <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em>. Supervised learning, as you already know by now, is the branch of machine learning and artificial intelligence that helps machines learn without explicit programming. A more simplified way of describing supervised learning would be developing algorithms that learn from labeled data. The broad categories in supervised learning are classification and regression, differentiated fundamentally by the type of label, that is, <strong class="bold">continuous</strong> or <strong class="bold">categorical</strong>. Algorithms that deal with continuous variables are known as <strong class="bold">regression algorithms</strong>, and those with categorical variables are called <strong class="bold">classification algorithms</strong>.</p>
			<p>In classification algorithms, our target, dependent, or criterion variable is a <strong class="bold">categorical variable</strong>. Based on the number of classes, we can further divide them into the following groups:</p>
			<ul>
				<li>Binary classification</li>
				<li>Multinomial classification</li>
				<li>Multi-label classification</li>
			</ul>
			<p>In this chapter, we will focus on <strong class="bold">binary classification</strong>. Discussing the specifics and practical examples of multinomial and multi-class classification is beyond the scope of this chapter; however, a few additional reading references for advanced topics will be listed before wrapping up the chapter.</p>
			<p>Binary classification algorithms are the most popular class of algorithms within machine learning and have numerous applications in business, research, and academia. Simple models that classify a student's chances of passing a future exam based on their past performance as pass or fail, predict whether it will rain or not, predict whether a customer will default on a loan or not, predict whether a patient has cancer or not, and so on are all common use cases that are solved by classification algorithms.</p>
			<p>Before diving deeper into algorithms, we will first get started with a use case that will help us solve a supervised learning classification problem with hands-on exercises.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor210"/>Getting Started with the Use Case</h2>
			<p>In this chapter, we will refer to the <strong class="bold">rainfall prediction problem</strong> using the <strong class="inline">weather</strong> dataset, obtained from the Australian Commonwealth Bureau of Meteorology and made available through R. The dataset has two target variables, <strong class="inline">RainTomorrow</strong>, a flag indicating whether it will rain tomorrow, and <strong class="inline">RISK_MM</strong>, which measures the amount of rainfall for the following day.</p>
			<p>In a nutshell, we can use this dataset for <strong class="bold">regression</strong> as well as <strong class="bold">classification</strong>, since we have two target variables. However, we will drop the continuous target variable and only consider the categorical target variable, <strong class="inline">RainTomorrow</strong>, for our classification exercise. The metadata and additional details about the dataset are available to explore at https://www.rdocumentation.org/packages/rattle/versions/5.2.0/topics/weather. Since the dataset is readily available through R, we don't need to separately download it; instead, we can directly use the R function within the <strong class="inline">rattle</strong> library to load the data into system memory.</p>
			<h3 id="_idParaDest-209"><a id="_idTextAnchor211"/>Some Background on the Use Case</h3>
			<p>Several weather parameters, such as temperature, direction, pressure, cloud cover, humidity, and sunshine, were recorded daily for one year. The rainfall for the next day is already engineered in the dataset as the target variable, <strong class="inline">RainTomorrow</strong>. We can leverage this data to define a machine learning model that learns from the present day's weather parameters and predicts the chances of rain for the next day.</p>
			<p>Rainfall prediction is of paramount importance to many industries. Long-haul journeys by train and buses usually look at changing weather patterns, primarily rainfall, to estimate the arrival time and journey length. Similarly, most brick and mortar stores, small restaurants and food joints, and others are all heavily impacted by rainfall. Gaining visibility of the weather conditions for the next day can help businesses better prepare in several ways, to combat business losses and, in some cases, maximize business outcomes.</p>
			<p>To build nice intuition around the problem-solving exercise, let's frame a business problem using the dataset and develop the problem statement for the use case. Since the data is about rainfall prediction, we will choose a popular business problem faced by today's hyper-local food-delivery services. Start-ups such as DoorDash, Skip the Dishes, FoodPanda, Swiggy, Foodora, and many others offer hyper-local food delivery services to customers in different countries. A common trend observed in most countries is the rise in food delivery orders with the onset of rain. In general, most delivery companies expect around a 30%-40% increase in the total number of deliveries on a given day. Given the limited number of delivery agents, the delivery time is impacted immensely due to increased orders on rainy days. To keep costs optimal, it is not viable for these companies to increase the number of full-time agents; therefore, a common strategy is to dynamically hire more agents for days when demand for the service is expected to be high. To plan better, visibility of rainfall predictions for the next day is of paramount importance.</p>
			<h3 id="_idParaDest-210"><a id="_idTextAnchor212"/>Defining the Problem Statement</h3>
			<p>With the context of the problem set up, let's try to define our problem statement for a hyper-local food-delivery service company to predict the rainfall for the next day. To keep things simple and consistent, let's frame the problem statement using the frameworks we studied previously, in <em class="italics">Chapter 2</em>, <em class="italics">Exploratory Analysis of Data</em>. This will help us distill the end goal we want to solve in a business-first approach while keeping the machine learning perspective at the forefront.</p>
			<p>The following figure creates a simple visual for the <strong class="keyword">Situation</strong> - <strong class="keyword">Complication</strong> - <strong class="keyword">Question</strong> (<strong class="keyword">SCQ</strong>) framework for the previously defined use case:</p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="image/C12624_05_01.jpg" alt="Figure 5.1: SCQ for the classification use case&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.1: SCQ for the classification use case</h6>
			<p>We can clearly answer the question from the SCQ: we would need a predictive model to predict the chances of rain for the next day as a solution to the problem. Let's move on to the next step – gathering data to build a predictive model that will help us solve the business problem.</p>
			<h3 id="_idParaDest-211"><a id="_idTextAnchor213"/>Data Gathering</h3>
			<p>The <strong class="inline">rattle.data</strong> package provides us with the data for the use case, which can be accessed using the internal dataset methods of R. In case you have not already installed the packages, you can easily install them using the <strong class="inline">install.packages("rattle.data")</strong> command.</p>
			<h3 id="_idParaDest-212"><a id="_idTextAnchor214"/>Exercise 63: Exploring Data for the Use Case</h3>
			<p>In this exercise, we will perform the initial exploration of the dataset we have gathered for the use case. We will explore the shape of the data, that is, the number of rows and columns, and study the content within each column.</p>
			<p>To explore the shape (rows x columns) and content of the data, perform the following steps:</p>
			<ol>
				<li>First, load the <strong class="inline">rattle</strong> package using the following command:<p class="snippet">library(rattle.data)</p></li>
				<li>Load the data for our use case, which is available from the <strong class="inline">rattle</strong> package:<p class="snippet">data(weatherAUS)</p><h4>Note</h4><p class="callout">The <strong class="inline">weatherAUS</strong> dataset is a DataFrame containing more than 1,40,000 daily observations from over 45 Australian weather stations.</p></li>
				<li>Now, load the weather data directly into a DataFrame called <strong class="inline">df</strong>:<p class="snippet">df &lt;- weatherAUS</p></li>
				<li>Explore the DataFrame's content using the <strong class="inline">str</strong> command:<p class="snippet">str(df)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="image/C12624_05_02.jpg" alt="Figure 5.2: Final output&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.2: Final output</h6>
			<p>We have almost 1,50,000 rows of data and 24 variables. We would need to drop the <strong class="inline">RISK_MM</strong> variable, as it will be the target variable for the regression use case (that is, predicting how much it will rain the next day). Therefore, we are left with 22 independent variables and 1 dependent variable, <strong class="inline">RainTomorrow</strong>, for our use case. We can also see a good mix of continuous and categorical variables. The <strong class="inline">Location</strong>, <strong class="inline">WindDir</strong>, <strong class="inline">RainToday</strong>, and many more variables are categorical, and the remainder are continuous.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/2Vwgu8Q.</p>
			<p>In the next exercise, we will calculate the total percentage of the null values in each column.</p>
			<h3 id="_idParaDest-213"><a id="_idTextAnchor215"/>Exercise 64: Calculating the Null Value Percentage in All Columns</h3>
			<p>The dataset we explored in <em class="italics">Exercise 1</em>, <em class="italics">Exploring Data for the Use Case</em> has quite a few null values. In this exercise, we will write a script to calculate the percentage of null values within each column.</p>
			<p>We can see the presence of null values in a few variables. Let's check the percentage of null values in each column within the <strong class="inline">df</strong> dataset.</p>
			<p>Perform the following steps to calculate the percentage of null values in each column of the dataset:</p>
			<ol>
				<li value="1">First, remove the column named <strong class="inline">RISK_MM</strong>, since it is supposed to be used as a target variable for regression use. (Adding this to our model will result in data leakage.):<p class="snippet">df$RISK_MM &lt;- NULL</p></li>
				<li>Create a <strong class="inline">temp_df</strong> DataFrame object and store the value in it:<p class="snippet">temp_df&lt;-as.data.frame(</p><p class="snippet">  sort(</p><p class="snippet">  round(</p><p class="snippet">  sapply(df, function(y) sum(length(which(is.na(y)))))/dim(df)[1],2)</p><p class="snippet">  )</p><p class="snippet">)</p><p class="snippet">colnames(temp_df) &lt;- "NullPerc"</p></li>
				<li>Now, use the <strong class="inline">print</strong> function to display the percentage null values in each column using the following command:<p class="snippet">print(temp_df)</p><p>The output is as follows:</p><p class="snippet">              NullPerc</p><p class="snippet">Date              0.00</p><p class="snippet">Location          0.00</p><p class="snippet">MinTemp           0.01</p><p class="snippet">MaxTemp           0.01</p><p class="snippet">WindSpeed9am      0.01</p><p class="snippet">Temp9am           0.01</p><p class="snippet">Rainfall          0.02</p><p class="snippet">WindSpeed3pm      0.02</p><p class="snippet">Humidity9am       0.02</p><p class="snippet">Temp3pm           0.02</p><p class="snippet">RainToday         0.02</p><p class="snippet">RainTomorrow      0.02</p><p class="snippet">WindDir3pm        0.03</p><p class="snippet">Humidity3pm       0.03</p><p class="snippet">WindGustDir       0.07</p><p class="snippet">WindGustSpeed     0.07</p><p class="snippet">WindDir9am        0.07</p><p class="snippet">Pressure9am       0.10</p><p class="snippet">Pressure3pm       0.10</p><p class="snippet">Cloud9am          0.38</p><p class="snippet">Cloud3pm          0.41</p><p class="snippet">Evaporation       0.43</p><p class="snippet">Sunshine          0.48</p></li>
			</ol>
			<p>We can see that the last four variables have more than <em class="italics">30%</em> missing or null values. This is a significantly huge drop. It would be best to drop these variables from our analysis. Also, we can see that there are a few other variables that have roughly <em class="italics">1%</em>-<em class="italics">2%</em>, and in some cases, up to <em class="italics">10%</em> missing or null values. We can treat these variables using various missing value treatment techniques, such as replacing them with mean or mode. In some important cases, we can also use additional techniques, such as clustering-based mean and mode replacement, for improved treatment. Additionally, in very critical scenarios, we can use a regression model to estimate the remainder of the missing values by defining a model where the column with the required missing value is treated as a function of the remaining variables.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/2ViZEp1.</p>
			<p>In the following exercise, we will remove null values. We will revisit data if we do not have a good model in place.</p>
			<h3 id="_idParaDest-214"><a id="_idTextAnchor216"/>Exercise 65: Removing Null Values from the Dataset</h3>
			<p>John is working on the newly created dataset, and while doing analysis, he has found out that the dataset contains significant null values. To make the dataset useful for further analysis, he must remove the null values from it.</p>
			<p>Perform the following steps to remove the null values from the <strong class="inline">df</strong> dataset:</p>
			<ol>
				<li value="1">First, select the last four columns to drop that have more than <em class="italics">30%</em> null values using the following command:<p class="snippet">cols_to_drop &lt;-tail(rownames(temp_df),4)</p></li>
				<li>Remove all the rows from the DataFrame that will have one or more columns with null values using the <strong class="inline">na.omit</strong> command, which removes all of the null rows from the DataFrame:<p class="snippet">df_new&lt;- na.omit(df[,!names(df) %in% cols_to_drop])</p></li>
				<li>Now, print the newly formatted data using the following <strong class="inline">print</strong> commands:<p class="snippet">print("Shape of data after dropping columns:")</p><p class="snippet">print(dim(df_new))</p><p>The output is as follows:</p><p class="snippet">Shape of data after dropping columns:</p><p class="snippet">112925     19</p></li>
				<li>Using the following command, verify whether the newly created dataset contains null values or not:<p class="snippet">temp_df&lt;-as.data.frame(sort(round(sapply(df_new, function(y) sum(length(which(is.na(y)))))/dim(df)[1],2)))</p><p class="snippet">colnames(temp_df) &lt;- "NullPerc"</p></li>
				<li>Now, print the dataset using the following <strong class="inline">print</strong> command:<p class="snippet">print(temp_df)</p><p>The output is as follows:</p><p class="snippet">              NullPerc</p><p class="snippet">Date                 0</p><p class="snippet">Location             0</p><p class="snippet">MinTemp              0</p><p class="snippet">MaxTemp              0</p><p class="snippet">Rainfall             0</p><p class="snippet">WindGustDir          0</p><p class="snippet">WindGustSpeed        0</p><p class="snippet">WindDir9am           0</p><p class="snippet">WindDir3pm           0</p><p class="snippet">WindSpeed9am         0</p><p class="snippet">WindSpeed3pm         0</p><p class="snippet">Humidity9am          0</p><p class="snippet">Humidity3pm          0</p><p class="snippet">Pressure9am          0</p><p class="snippet">Pressure3pm          0</p><p class="snippet">Temp9am              0</p><p class="snippet">Temp3pm              0</p><p class="snippet">RainToday            0</p><p class="snippet">RainTomorrow         0</p></li>
			</ol>
			<p>We can now double check and see that the new dataset has no more missing values and the overall number of rows in the dataset also reduced to 112,000, which is around a <em class="italics">20%</em> loss of training data. We should use missing value treatment techniques such as replacing missing values with the mean, mode, or median to combat such high losses due to the omission of missing values. A rule of thumb would be to safely ignore anything less than a <em class="italics">5%</em> loss. Since, we have more than 1,00,000 records (a reasonably high number of records for a simple use case), we are ignoring this rule of thumb.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/2Q3HIgT.</p>
			<p>Additionally, we can also engineer date- and time-related features using the <strong class="inline">Date</strong> column. The following exercise creates numeric features such as day, month, day of the week, and quarter of the year as additional time-related features and drops the original <strong class="inline">Date</strong> variable.</p>
			<p>We will use the <strong class="inline">lubridate</strong> library in R to work with date and time-related features. It provides us with extremely easy-to-use functions to perform date and time operations. If you have not already installed the package, please install the library using the <strong class="inline">install.packages('lubridate')</strong> command.</p>
			<h3 id="_idParaDest-215"><a id="_idTextAnchor217"/>Exercise 66: Engineer Time-Based Features from the Date Variable</h3>
			<p>Time- and date-related attributes cannot be directly used in a supervised classification model. To extract meaningful properties from date- and time-related variables, it is a common practice to create month, year, week, and quarter from the date as features.</p>
			<p>Perform the following steps to work with the data and time function in R:</p>
			<ol>
				<li value="1">Import the <strong class="inline">lubridate</strong> library into RStudio using the following command:<p class="snippet">library(lubridate)</p><h4>Note</h4><p class="callout">The <strong class="inline">lubridate</strong> library provides handy date- and time-related functions.</p></li>
				<li>Extract <strong class="inline">day</strong>, <strong class="inline">month</strong>, <strong class="inline">dayofweek</strong>, and <strong class="inline">quarter</strong> as new features from the <strong class="inline">Date</strong> variable using the <strong class="inline">lubridate</strong> function:<p class="snippet">df_new$day &lt;- day(df_new$Date)</p><p class="snippet">df_new$month &lt;- month(df_new$Date)</p><p class="snippet">df_new$dayofweek &lt;- wday(df_new$Date)</p><p class="snippet">df_new$quarter &lt;- quarter(df_new$Date)</p></li>
				<li>Examine the newly created variables:<p class="snippet">str(df_new[,c("day","month","dayofweek","quarter")])</p></li>
				<li>Now that we have created all of the date- and time-related features, we won't need the actual <strong class="inline">Date</strong> variable. Therefore, delete the previous <strong class="inline">Date</strong> column:<p class="snippet">df_new$Date &lt;- NULL</p><p>The output is as follows:</p><p class="snippet">'data.frame':	112925 obs. of  4 variables:</p><p class="snippet"> $ day      : int  1 2 3 4 5 6 7 8 9 10 ...</p><p class="snippet"> $ month    : num  12 12 12 12 12 12 12 12 12 12 ...</p><p class="snippet"> $ dayofweek: num  2 3 4 5 6 7 1 2 3 4 ...</p><p class="snippet"> $ quarter  : int  4 4 4 4 4 4 4 4 4 4 ...</p></li>
			</ol>
			<p>In this exercise, we have extracted meaningful features from date- and time-related attributes from the data and removed the actual date-related columns.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/2E4hOEU.</p>
			<p>Next, we need to process or clean another feature within the DataFrame: <strong class="inline">location</strong>.</p>
			<h3 id="_idParaDest-216"><a id="_idTextAnchor218"/>Exercise 67: Exploring the Location Frequency</h3>
			<p>The <strong class="inline">Location</strong> variable defines the actual location where the weather data was captured for the specified time. Let's do a quick check on the number of distinct values that are captured within this variable and see whether there are any interesting patterns that might be of importance.</p>
			<p>In the following exercise, we will be using the <strong class="inline">Location</strong> variable to define the actual location where the weather data was captured for the specified time.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">Calculate the frequency of rain across each location using the grouping functions from the <strong class="inline">dplyr</strong> package:<p class="snippet">location_dist &lt;- df_new %&gt;%    group_by(Location) %&gt;%     summarise(Rain  = sum(ifelse(RainTomorrow =="Yes",1,0)), cnt=n()) %&gt;%    mutate(pct = Rain/cnt) %&gt;%    arrange(desc(pct))</p></li>
				<li>Examine the number of distinct locations for sanity:<p class="snippet">print(paste("#Distinct locations:",dim(location_dist)[1]))</p><p>The output is as follows:</p><p class="snippet">"#Distinct locations: 44"</p></li>
				<li>Print <strong class="inline">summary</strong> to examine the aggregation performed:<p class="snippet">print(summary(location_dist))</p><p>The output is as follows:</p><p class="snippet">    Location        Rain             cnt            pct         </p><p class="snippet"> Adelaide     : 1   Min.   : 102.0   Min.   : 670   Min.   :0.06687  </p><p class="snippet"> Albury       : 1   1st Qu.: 427.8   1st Qu.:2330   1st Qu.:0.18380  </p><p class="snippet"> AliceSprings : 1   Median : 563.5   Median :2742   Median :0.21833  </p><p class="snippet"> BadgerysCreek: 1   Mean   : 568.6   Mean   :2566   Mean   :0.21896  </p><p class="snippet"> Ballarat     : 1   3rd Qu.: 740.5   3rd Qu.:2884   3rd Qu.:0.26107  </p><p class="snippet"> Bendigo      : 1   Max.   :1031.0   Max.   :3117   Max.   :0.36560  </p><p class="snippet"> (Other)      :38  </p></li>
			</ol>
			<p>We can see that there are 44 distinct locations in the data. The <strong class="inline">cnt</strong> variable, which defines the number of records (in the previous transformed data) for each location, has an average 2,566 records. The similar number distribution between the first quartile, median, and third quartile denote that the locations are evenly distributed in the data. However, if we investigate the percentage of records where rain was recorded (<strong class="inline">pct</strong>), we see an interesting trend. Here, we have locations with around a <em class="italics">6%</em> chance of rain and some with around a <em class="italics">36%</em> chance of rain. There is a huge difference in the possibility of rain, based on the location.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/30aKUMx.</p>
			<p>Since we have around 44 distinct locations, it is difficult to utilize this variable directly as a categorical feature. In R, most supervised learning algorithms internally convert the categorical column into a numerical form that can be interpreted by the model. However, with an increased number of classes within the categorical variable, the complexity of the model increases with no additional value. To keep things simple, we can transform the <strong class="inline">Location</strong> variable as a new variable with a reduced number of levels. We will select the top five and the bottom five locations with chances of rain and tag all other locations as <strong class="inline">Others</strong>. This will reduce the number of distinct levels in the variable as 10+1 and will be more suitable for the model.</p>
			<h3 id="_idParaDest-217"><a id="_idTextAnchor219"/>Exercise 68: Engineering the New Location with Reduced Levels</h3>
			<p>The <strong class="inline">location</strong> variable has too many distinct values (44 locations), and machine learning models in general do not perform well with categorical variables with a high frequency of distinct classes. We therefore need to prune the variable by reducing the number of distinct classes within it. We will select the top five and the bottom five locations with chances of rain and tag all other locations as <strong class="inline">Others</strong>. This will reduce the number of distinct levels in the variable as 10+1 and will be more suitable for the model.</p>
			<p>Perform the following steps to engineer a new variable for location with a reduced number of distinct levels:</p>
			<ol>
				<li value="1">Convert the <strong class="inline">location</strong> variable from a factor into a character:<p class="snippet">location_dist$Location &lt;- as.character(location_dist$Location)</p></li>
				<li>Create a list with the top five and the bottom five locations with respect to the chances of rain. We can do this by using the <strong class="inline">head</strong> command for the top five and the <strong class="inline">tail</strong> command for the bottom five locations after ordering the DataFrame in ascending order:<p class="snippet">location_list &lt;- c(head(location_dist$Location,5),tail(location_dist$Location,5))</p></li>
				<li>Print the list to double-check that we have the locations correctly stored:<p class="snippet">print("Final list of locations - ")</p><p class="snippet">print(location_list)</p><p>The output is as follows:</p><p class="snippet">[1] "Final list of locations - "</p><p class="snippet"> [1] "Portland"      "Walpole"       "Dartmoor"      "Cairns"       </p><p class="snippet"> [5] "NorfolkIsland" "Moree"         "Mildura"       "AliceSprings" </p><p class="snippet"> [9] "Uluru"         "Woomera" </p></li>
				<li>Convert the <strong class="inline">Location</strong> variable in the main <strong class="inline">df_new</strong> DataFrame into a <strong class="inline">character</strong>:<p class="snippet">df_new$Location &lt;- as.character(df_new$Location)</p></li>
				<li>Reduce the number of distinct locations in the variable. This can be done by tagging all the locations that are not a part of the <strong class="inline">location_list</strong> list as <strong class="inline">Others</strong>:<p class="snippet">df_new$new_location &lt;- factor(ifelse(df_new$Location %in% location_list,df_new$Location,"Others"))</p></li>
				<li>Delete the old <strong class="inline">Location</strong> variable using the following command:<p class="snippet">df_new$Location &lt;- NULL</p></li>
				<li>To ensure that the fifth step was correctly performed, we can create a temporary DataFrame and summarize the frequency of records against the new <strong class="inline">location</strong> variable we created:<p class="snippet">temp &lt;- df_new %&gt;% mutate(loc = as.character(new_location)) %&gt;%    group_by(as.character(loc)) %&gt;%    summarise(Rain  = sum(ifelse(RainTomorrow =="Yes",1,0)), cnt=n()) %&gt;%    mutate(pct = Rain/cnt) %&gt;%    arrange(desc(pct))</p></li>
				<li>Print the temporary test DataFrame and observe the results. We should see only 11 distinct location values:<p class="snippet">print(temp)</p><p>The output is as follows:</p><p class="snippet"># A tibble: 11 x 4</p><p class="snippet">   `as.character(loc)`  Rain   cnt    pct</p><p class="snippet">   &lt;chr&gt;               &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;</p><p class="snippet">  Portland             1031  2820 0.366 </p><p class="snippet">  Walpole               864  2502 0.345 </p><p class="snippet">  Dartmoor              770  2294 0.336 </p><p class="snippet">  Cairns                910  2899 0.314 </p><p class="snippet">  NorfolkIsland         883  2864 0.308 </p><p class="snippet">  Others              19380 86944 0.223 </p><p class="snippet">  Moree                 336  2629 0.128 </p><p class="snippet">  Mildura               315  2897 0.109 </p><p class="snippet">  AliceSprings          227  2744 0.0827</p><p class="snippet">  Uluru                 110  1446 0.0761</p><p class="snippet">  Woomera               193  2886 0.0669</p></li>
			</ol>
			<p>We first convert the <strong class="inline">Location</strong> variable from a factor to a character to ease the string operation's tasks. The DataFrame is sorted in descending order according to the percentage chance of rain. The <strong class="inline">head</strong> and the <strong class="inline">tail</strong> commands are used to extract the top and bottom five locations in a list. This list is then used as a reference check to reduce the number of levels in the new feature. Finally, after engineering the new feature with the reduced levels, we do a simple check to ensure that our feature has been engineered in the way we expect.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/30fnR31.</p>
			<p>Let's now get into the most interesting topic of the chapter and explore classification techniques for supervised learning.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor220"/>Classification Techniques for Supervised Learning</h2>
			<p>To approach a <strong class="bold">supervised classification algorithm</strong>, we first need to understand the basic functioning of the algorithm, explore a bit of the math in an abstract way, and then develop the algorithm using readily available packages in R. We will cover a few basic algorithms, such as white-box algorithms such as Logistic Regression and Decision Trees, and then we will move on to advanced modeling techniques, such as black-box models such as Random Forest, XGBoost, and neural networks. The list of algorithms we plan to cover is not exhaustive, but these five algorithms will help you gain a broad understanding of the topic.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor221"/>Logistic Regression</h2>
			<p><strong class="bold">Logistic regression</strong> is the most favorable white-box model used for binary classification. White-box models are defined as models where we have visibility of the entire reasoning used for the prediction. For each prediction made, we can leverage the model's mathematical equation and decode the reasons for the prediction made. There are also a set of classification models that are completely black-box, that is, by no means can we understand the reasoning for the prediction leveraged by the model. In situations where we want to focus on only the end outcome, we should prefer black-box models, as they are more powerful.</p>
			<p>Though the name ends with <em class="italics">regression</em>, logistic regression is a technique used to predict binary categorical outcomes. We would need a different approach to model for a categorical outcome. This can be done by transforming the outcome into a log of odds ratio or the probability of the event happening.</p>
			<p>Let's distill this approach into simpler constructs. Assume the probability of success for an event is 0.8. Then, the probability of failure for the same event would be defined as <em class="italics">(1-0.8) = 0.2</em>. The odds of success are defined as the ratio of the probability of success over the probability of failure.</p>
			<p>In the following example, the odds of success would be <em class="italics">(0.8/0.2) = 4</em>. That is, the odds of success are four to one. If the probability of success is 0.5, that is, a 50-50 percent chance, then the odds of success are 0.5 to 1. The logistic regression model can be mathematically represented as follows:</p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/C12624_05_10.jpg" alt=""/>
				</div>
			</div>
			<p>Where, <img src="image/C12624_05_11.png" alt=""/> is the log of odds ratio.</p>
			<p>Solving the math further, we can deduce the probability of the outcome as follows:</p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/C12624_05_12.jpg" alt=""/>
				</div>
			</div>
			<p>Discussing the mathematical background and derivation of the equations is beyond the scope of the chapter. To summarize, the <strong class="inline">logit</strong> function, that is, the link function, helps logistic regression reframe the problem (predicted outcome) intuitively as the log of odds ratio. When solved, it helps us predict the probability of a binary dependent variable.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor222"/>How Does Logistic Regression Work?</h2>
			<p>Just like linear regression, where the beta coefficients for the variables are estimated using the <strong class="bold">Ordinary Least Squares</strong> (<strong class="bold">OLS</strong>) method, a logistic regression model leverages the <strong class="bold">maximum-likelihood estimation</strong> (<strong class="bold">MLE</strong>). The MLE function estimates the best set of values of the model parameters or beta coefficients such that it maximizes the likelihood function, that is, the probability estimates, which can be also defined as the <em class="italics">agreement</em> of the selected model with the observed data. When the best set of parameter values are estimated, plugging these values or beta coefficients into the model equation as previously defined would help in estimating the probability of the outcome for a given sample. Akin to OLS, MLE is also an iterative process.</p>
			<p>Let's see a logistic regression model in action on our dataset. To get started, we will use only a small subset of variables for the model. Ideally, it is recommended to start with the most important variables based on the EDA exercise and then incrementally add remainder variables. For now, we will start with a temperature-related variable for the maximum and minimum values, a wind speed-related variable, pressure and humidity at 3 P.M., and the rainfall for the current day.</p>
			<p>We will divide the entire dataset into train (70%) and test (30%). While fitting the data to the model, we will only use the train dataset and will later evaluate the performance of the model on the train, as well as the unseen test data. This approach will help us understand whether our model is overfitting and provide a more realistic model performance on unseen data.</p>
			<h3 id="_idParaDest-221"><a id="_idTextAnchor223"/>Exercise 69: Build a Logistic Regression Model</h3>
			<p>We will build a binary classification model using logistic regression and the dataset we explored in the Exercises 1-6. We will divide the data into train and test (70% and 30%, respectively) and leverage the training data to fit the model and the test data to evaluate the model's performance on unseen data.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">First, set <strong class="inline">seed</strong> for reproducibility using the following command:<p class="snippet">set.seed(2019)</p></li>
				<li>Next, create a list of indexes for the training dataset (70%):<p class="snippet">train_index &lt;- sample(seq_len(nrow(df_new)),floor(0.7 * nrow(df_new)))</p></li>
				<li>Now, split the data into test and train datasets using the following commands:<p class="snippet">train &lt;- df_new[train_index,]</p><p class="snippet">test &lt;- df_new[-train_index,]</p></li>
				<li>Build the logistic regression model with <strong class="inline">RainTomorrow</strong> as the dependent variable and a few independent variables (we selected <strong class="inline">MinTemp</strong>, <strong class="inline">Rainfall</strong>, <strong class="inline">WindGustSpeed</strong>, <strong class="inline">WindSpeed3pm</strong>, <strong class="inline">Humidity3pm</strong>. <strong class="inline">Pressure3pm</strong>, <strong class="inline">RainToday</strong>, <strong class="inline">Temp3pm</strong>, and <strong class="inline">Temp9am</strong>). We can add all the available independent variables in the DataFrame too:<p class="snippet">model &lt;- glm(RainTomorrow ~ MinTemp + Rainfall + WindGustSpeed +         WindSpeed3pm +Humidity3pm + Pressure3pm +        RainToday +  Temp3pm + Temp9am,         data=train,        family=binomial(link='logit'))</p></li>
				<li>Print the summary of the dataset using the <strong class="inline">summary</strong> function:<p class="snippet">summary(model)</p><p>The output is as follows:</p><p class="snippet">Call:</p><p class="snippet">glm(formula = RainTomorrow ~ MinTemp + Rainfall + WindGustSpeed + </p><p class="snippet">    WindSpeed3pm + Humidity3pm + Pressure3pm + RainToday + Temp3pm + </p><p class="snippet">    Temp9am, family = binomial(link = "logit"), data = train)</p><p class="snippet">Deviance Residuals: </p><p class="snippet">    Min       1Q   Median       3Q      Max  </p><p class="snippet">-2.9323  -0.5528  -0.3235  -0.1412   3.2047  </p><p class="snippet">Coefficients:</p><p class="snippet">                Estimate Std. Error z value Pr(&gt;|z|)    </p><p class="snippet">(Intercept)    6.543e+01  1.876e+00  34.878  &lt; 2e-16 ***</p><p class="snippet">MinTemp        9.369e-05  5.056e-03   0.019    0.985    </p><p class="snippet">Rainfall       7.496e-03  1.404e-03   5.337 9.44e-08 ***</p><p class="snippet">WindGustSpeed  5.817e-02  1.153e-03  50.434  &lt; 2e-16 ***</p><p class="snippet">WindSpeed3pm  -4.331e-02  1.651e-03 -26.234  &lt; 2e-16 ***</p><p class="snippet">Humidity3pm    7.363e-02  9.868e-04  74.614  &lt; 2e-16 ***</p><p class="snippet">Pressure3pm   -7.162e-02  1.821e-03 -39.321  &lt; 2e-16 ***</p><p class="snippet">RainTodayYes   4.243e-01  2.751e-02  15.425  &lt; 2e-16 ***</p><p class="snippet">Temp3pm        3.930e-02  5.171e-03   7.599 2.98e-14 ***</p><p class="snippet">Temp9am       -4.605e-02  6.270e-03  -7.344 2.07e-13 ***</p><p class="snippet">---</p><p class="snippet">Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</p><p class="snippet">(Dispersion parameter for binomial family taken to be 1)</p><p class="snippet">    Null deviance: 83718 on 79046 degrees of freedom</p><p class="snippet">Residual deviance: 56557 on 79037 degrees of freedom</p><p class="snippet">AIC: 56577</p><p class="snippet">Number of Fisher Scoring iterations: 5</p></li>
			</ol>
			<p>The <strong class="inline">set.seed</strong> command ensures that the random selections used for the train and test data split can be reproduced. We divide the data into 70% train and 30% test. The set seed function ensures that, for the same seed, we get the same split every time. The <strong class="inline">glm</strong> function is used in R to build generalized linear models. Logistic regression is defined in the model using the <strong class="inline">family</strong> parameter value set to <strong class="inline">binomial(link ='logit')</strong>. The <strong class="inline">glm</strong> function can be used to build several other models too (such as gamma, Poisson, and binomial). The formula defines the dependent, as well as the set of independent, variables. It takes the general form <em class="italics">Var1 ~ Var2 + Var3 + …</em>, which denotes <strong class="inline">Var1</strong> as the dependent or target variable and the remainder as the independent variables. If we want to use all of the variables in the DataFrame as independent variables, we can instead use <strong class="inline">formula = Var1 ~ .</strong>, which would indicate that <strong class="inline">Var1</strong> is the dependent variable and the rest are all independent variables.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/2HwwUUX.</p>
			<h3 id="_idParaDest-222"><a id="_idTextAnchor224"/>Interpreting the Results of Logistic Regression</h3>
			<p>We previously had a glimpse of logistic regression in <em class="italics">Chapter 2</em>, <em class="italics">Exploratory Analysis of Data</em>, but we didn't get into the specifics of the model results. The results demonstrated in the previous output snippet will look like what you observed in linear regression, but with some differences. Let's explore and interpret the results part by part.</p>
			<p>Firstly, we have the <strong class="keyword">Deviance Residuals</strong> displayed right after the formula. Like linear regression, a deviance residual is a measure of goodness of fit. The <strong class="inline">glm</strong> function calculates two types of residuals, that is, <strong class="bold">Null Deviance</strong> and <strong class="bold">Residual Deviance</strong>. The difference between the two is that one reports the goodness of fit when only the intercept (that is, no dependent variables) is used and the other reports when all the provided independent variables are used. The reduction in deviance between null and residual deviance helps us understand the quantified value added by the independent variables in defining the variance or the predictive correctness. The distribution of deviance residuals is reported right after the formula.</p>
			<p>Next, we have the <strong class="bold">beta coefficients</strong> and the associated <strong class="bold">standard error</strong>, the <em class="italics">z-value</em> and the <em class="italics">p-value</em>, which is the probability of significance. For each variable provided, R internally calculates the coefficients and, along with the parameter value, it also reports additional test results to help us interpret how effective these coefficients are. The absolute value of the coefficient is a simple way to understand how important that variable is to the final predictive power, that is, how impactful the variable is in determining the end outcome of the prediction. We can see that all variables have a low value for the coefficient.</p>
			<p>Next, the standard error helps us quantify how stable the value will be. A lower value for the standard error would indicate more consistent or stable values for the beta coefficients. The standard errors for all the variables in our exercise are low. The <em class="italics">z-value</em> and the probability of significance together help us take a call as to whether the results are statistically significant or just appear as they are due to random chance. This idea follows on from the same principle we learned about the null and alternate hypothesis in <em class="italics">Chapter 2</em>, <em class="italics">Exploratory Analysis of Data</em>, and is akin to linear regression parameter significance, which we learned about in <em class="italics">Chapter 4</em>, <em class="italics">Regression</em>.</p>
			<p>The easiest way to interpret the significance would be to study the <em class="italics">asterix </em>besides each independent variable, that is, <strong class="inline">*</strong>. The number of <strong class="inline">*</strong> is defined by the actual probability value, as defined below the parameter values. In our exercise, notice that the <strong class="inline">MinTemp</strong> variable is not statistically significant, that is, <em class="italics">p-value &gt; 0.05</em>. The rest are all statistically significant variables.</p>
			<p>The <strong class="keyword">Akaike Information Criterion</strong> (<strong class="keyword">AIC</strong>) is again a metric reported by R to assess the goodness of fit of the model or the quality of the model. This number comes in handy to compare different models for the same use case. Say you fit several models using a combination of independent variables but the same dependent variable, the AIC can be used to study the best model by way of a simple comparison of the value in all models. The calculation of the metric is derived from the deviance between the model's prediction and the actual labels, but factors in the presence of variables that are not adding any value. Therefore, akin to <strong class="bold">R Squared</strong> and <strong class="bold">adjusted R Squared</strong> in linear regression, the AIC helps us to avoid building complicated models. To select the best model from a list of candidate models, we should select the model with the lowest AIC.</p>
			<p>Toward the end of the previous output, we can see the results from <strong class="bold">Fisher's Scoring</strong> algorithm, which is a derivative of Newton's method for solving maximum likelihood problems numerically. We see that it required five iterations to fit the data to the model, but beyond that, this information is not of much value to us. It is a simple indication for us to conclude that the model did converge.</p>
			<p>We now understand how logistic regression works and have interpreted the results reported by the model in R. However, we still need to evaluate the model results using our train and test dataset and ensure that the model performs well on unseen data. To study the performance of a classification model, we would need to leverage various metrics, such as accuracy, precision, and recall. Though we already explored them in <em class="italics">Chapter 4</em>, <em class="italics">Regression</em>, let's now study them in more detail.</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor225"/>Evaluating Classification Models</h2>
			<p>Classification models require a bunch of different metrics to be thoroughly evaluated, unlike regression models. Here, we don't have something as intuitive as <strong class="bold">R Squared</strong>. Moreover, the performance requirements completely change based on a specific use case. Let's take a brief look at the various metrics that we already studied in <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em>, for classification.</p>
			<h3 id="_idParaDest-224"><a id="_idTextAnchor226"/>Confusion Matrix and Its Derived Metrics</h3>
			<p>The first basis for studying model performance for classification algorithms starts with a <strong class="bold">confusion matrix</strong>. A confusion matrix is a simple representation of the distribution of predictions of each class across the actuals of each class:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/C12624_05_03.jpg" alt="Figure 5.3: Confusion matrix&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.3: Confusion matrix</h6>
			<p>The previous table is a simple representation of a confusion matrix. Here, we assume that the <strong class="bold">Yes</strong> class is labelled <strong class="bold">Positive</strong>. When the actual value of a given sample is <strong class="bold">Yes</strong> and it is correctly predicted as <strong class="bold">Positive</strong>, we define it as <strong class="bold">True Positive</strong>, whereas, if the actual value is <strong class="bold">Yes</strong>, but was predicted by the model as <strong class="bold">No</strong>, then we define it as <strong class="bold">False Positive</strong>. The same story holds true for <strong class="bold">True Negative</strong> and <strong class="bold">False Negative</strong>. Here's a simple rule: to avoid confusion in interpreting the names, consider the value of the label <strong class="bold">Positive</strong> for (<strong class="inline">1</strong>) and <strong class="bold">Negative</strong> for (<strong class="inline">0</strong>); when the result is correctly predicted, then we assign <strong class="bold">True</strong> to the label, otherwise <strong class="bold">False</strong>. Therefore, <strong class="bold">True Positive</strong> would be indicative of value <strong class="inline">1</strong> correctly predicted as <strong class="inline">1</strong> and so on for the remaining outcomes.</p>
			<p>Based on the confusion matrix and the values defined from it, we can further define a couple of metrics that will help us better understand the model's performance. We will now use the abbreviations <strong class="bold">TP</strong> for <strong class="bold">True Positive</strong>, <strong class="bold">FP</strong> for <strong class="bold">False Positive</strong>, <strong class="bold">TN</strong> for <strong class="bold">True Negative</strong>, and <strong class="bold">FN</strong> for <strong class="bold">False Negative</strong> going forward:</p>
			<ul>
				<li><strong class="bold">Overall accuracy</strong>: Overall accuracy is defined as the ratio of total correct predictions to the total number of predictions in the entire test sample. So, this would be simply the sum of <strong class="bold">True Positives</strong> and <strong class="bold">True Negatives</strong> divided by all the metrics in the confusion matrix:</li>
			</ul>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/C12624_05_13.jpg" alt=""/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">Precision</strong> or <strong class="keyword">Positive Predictive Value</strong> (<strong class="keyword">PPV</strong>): Precision is defined as the ratio of correctly predicted positive labels to the total number of positively predicted labels:</li>
			</ul>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/C12624_05_14.jpg" alt=""/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">Recall</strong> or <strong class="keyword">Sensitivity</strong>: Recall measures how sensitive your model is by representing the ratio of the number of correctly predicted positive labels to the total number of actual positive labels:</li>
			</ul>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/C12624_05_15.jpg" alt=""/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">Specificity</strong> or <strong class="keyword">True Negative Rate</strong> (<strong class="keyword">TNR</strong>): Specificity defines the ratio of correctly predicted negative labels to the total number of actual negative labels:</li>
			</ul>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/C12624_05_16.jpg" alt=""/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">F1 Score</strong>: The F1 score is the harmonic mean between precision and recall. It is a better metric to consider than overall accuracy for most cases:</li>
			</ul>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/C12624_05_17.jpg" alt=""/>
				</div>
			</div>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor227"/>What Metric Should You Choose?</h2>
			<p>Another important aspect to consider on a serious note, is which metric we should consider while evaluating a model. There is no straightforward answer, as the best combination of metrics completely depend on the type of classification use case we are dealing with. One situation that commonly arises in classification use cases is imbalanced classes. It is not necessary for us to always have an equal distribution of positive and negative labels in data. In fact, in most cases, we would be dealing with a scenario where the positive class would be less than <em class="italics">30%</em> of the data. In such cases, the overall accuracy would not be the ideal metric to consider.</p>
			<p>Let's take a simple example to understand this better. Consider the example of predicting fraud in credit card transactions. In a realistic scenario, for every 100 transactions there may be just one or two fraud transactions. Now, if we use overall accuracy as the only metric to evaluate a model, even if we predict all the labels as <strong class="bold">No</strong>, that is, <strong class="bold">Not Fraud</strong>, we would have approximately <em class="italics">99%</em> accuracy, <em class="italics">0%</em> precision, and <em class="italics">0%</em> recall. The <em class="italics">99%</em> accuracy might seem a great number for model performance; however, in this case, it would not be the ideal metric to evaluate. </p>
			<p>To deal with such a situation, there is often additional business context required to make a tangible call, but in most cases (for this type of a scenario), the business would want a higher recall with a bit of compromise on the overall accuracy and precision. The rationale to use high recall as the metric for model evaluation is that it would still be fine to predict a transaction as fraud even if it is authentic; however, it would be a mistake to predict a fraud transaction as authentic; the business losses would be colossal.</p>
			<p>Often, the evaluation of a model is taken with a combination of metrics based on business demands. The biggest decision maker would be the trade-off between precision and recall. As indicated by the confusion matrix, whenever we try to improve precision, it hurts recall and vice versa.</p>
			<p>Here are some business situations in which we prioritize different metrics:</p>
			<ul>
				<li><strong class="bold">Predicting a rare event with catastrophic consequences</strong>: When predicting whether a patient has cancer or not, whether a transaction is fraud, and so on, it is OK to predict a person without cancer as having cancer, but the other way around would result in the loss of life. Such scenarios demand high recall by compromising <em class="italics">precision</em> and <em class="italics">overall accuracy</em>.</li>
				<li><strong class="bold">Predicting a rare event with not such catastrophic consequences</strong>: When predicting whether a customer will churn or whether a customer will positively respond to a marketing campaign, the business outcome is not jeopardized by an incorrect prediction, but would be the campaign. In such cases, based on the situation, it would make sense to have high precision with a bit of compromise on recall.</li>
				<li><strong class="bold">Predicting a regular (non-rare) event with not such catastrophic consequences</strong>: This would deal with most classification use cases, where the cost of correctly predicting a class is almost equal to the cost of incorrectly predicting the class. In such cases, we can use the F1 score, which represents a harmonic mean between precision and recall. It would be ideal to use overall accuracy in conjunction with the F1 score, as accuracy is more easily interpretable.</li>
			</ul>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor228"/>Evaluating Logistic Regression</h2>
			<p>Let's now evaluate the logistic regression model that we built previously.</p>
			<h3 id="_idParaDest-227"><a id="_idTextAnchor229"/>Exercise 70: Evaluate a Logistic Regression Model</h3>
			<p>Machine learning models fitted on a training dataset cannot be evaluated using the same dataset. We would need to leverage a separate test dataset and compare the model's performance on a train as well as a test dataset. The <strong class="inline">caret</strong> package has some handy functions to compute the model evaluation metrics previously discussed.</p>
			<p>Perform the following steps to evaluate the logistic regression model we built in <em class="italics">Exercise 7</em>, <em class="italics">Build a Logistic Regression Model</em>:</p>
			<ol>
				<li value="1">Compute the distribution of records for the <strong class="inline">RainTomorrow</strong> target variable in the <strong class="inline">df_new</strong> DataFrame:<p class="snippet">print("Distribution of labels in the data-")</p><p class="snippet">print(table(df_new$RainTomorrow)/dim(df_new)[1])</p><p>The output is as follows:</p><p class="snippet">"Distribution of labels in the data-"</p><p class="snippet">       No       Yes </p><p class="snippet">0.7784459 0.2215541 </p></li>
				<li>Predict the <strong class="inline">RainTomorrow</strong> target variable on the train data using the <strong class="inline">predict</strong> function and cast observations with values (probability &gt;0.5) as <strong class="inline">Yes</strong>, else <strong class="inline">No</strong>:<p class="snippet">print("Training data results -")</p><p class="snippet">pred_train &lt;-factor(ifelse(predict(model,</p><p class="snippet">                           newdata=train, type="response")&gt; 0.5,"Yes","No"))</p></li>
				<li>Create the confusion matrix and print the results for the train data:<p class="snippet">train_metrics &lt;- confusionMatrix(pred_train,  </p><p class="snippet">                                     train$RainTomorrow,positive="Yes")</p><p class="snippet">print(train_metrics)</p><p>The output is as follows:</p><p class="snippet">[1] "Training data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  58233  8850</p><p class="snippet">       Yes  3258  8706</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8468          </p><p class="snippet">                 95% CI : (0.8443, 0.8493)</p><p class="snippet">    No Information Rate : 0.7779          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.4998          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.4959          </p><p class="snippet">            Specificity : 0.9470          </p><p class="snippet">         Pos Pred Value : 0.7277          </p><p class="snippet">         Neg Pred Value : 0.8681          </p><p class="snippet">             Prevalence : 0.2221          </p><p class="snippet">         Detection Rate : 0.1101          </p><p class="snippet">   Detection Prevalence : 0.1514          </p><p class="snippet">      Balanced Accuracy : 0.7215          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes             </p></li>
				<li>Predict the results on the test data, similar to the second step:<p class="snippet">print("Test data results -")</p><p class="snippet">pred_test &lt;-factor(ifelse(predict(model,newdata=test,type = "response") &gt; 0.5,"Yes","No"))</p></li>
				<li>Create a confusion matrix for the test data predictions and print the results:<p class="snippet">test_metrics &lt;- confusionMatrix(pred_test, </p><p class="snippet">                                test$RainTomorrow,positive="Yes")</p><p class="snippet">print(test_metrics)</p><p>The output is as follows:</p><p class="snippet">[1] "Test data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  25066  3754</p><p class="snippet">       Yes  1349  3709</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8494          </p><p class="snippet">                 95% CI : (0.8455, 0.8532)</p><p class="snippet">    No Information Rate : 0.7797          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.5042          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.4970          </p><p class="snippet">            Specificity : 0.9489          </p><p class="snippet">         Pos Pred Value : 0.7333          </p><p class="snippet">         Neg Pred Value : 0.8697          </p><p class="snippet">             Prevalence : 0.2203          </p><p class="snippet">         Detection Rate : 0.1095          </p><p class="snippet">   Detection Prevalence : 0.1493          </p><p class="snippet">      Balanced Accuracy : 0.7230          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes    </p></li>
			</ol>
			<p>We first load the necessary <strong class="inline">caret</strong> library, which will provide the functions to compute the desired metrics, as discussed. We then use the <strong class="inline">predict</strong> function in R to predict the results using the previously fitted model on the train as well as the test data (separately). The <strong class="inline">predict</strong> function for logistic regression returns the value of the <strong class="inline">link</strong> function, by default. Using the <strong class="inline">type= 'response'</strong> parameter, we can override the function to return probabilities for the target. For simplicity, we use <strong class="inline">0.5</strong> as a threshold on the predictions. Therefore, anything above 0.5 would be <strong class="bold">Yes</strong>, else <strong class="bold">No</strong>. The <strong class="inline">confusionMatrix</strong> function from the <strong class="inline">caret</strong> library provides us with a simple way to construct the confusion matrix and calculate an exhaustive list of metrics. We would need to pass the actual, as well the predicted labels, to the function.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/2Q6mYW0.</p>
			<p>The distribution of the target label is imbalanced: <em class="italics">77%</em> no and <em class="italics">23%</em> yes. In such a scenario, we cannot rely only on the overall accuracy as a metric to evaluate the model's performance. Also, the confusion matrix, as shown in the output for steps 3 and 5, is inverted when compared to the illustration shown in the previous section, <em class="italics">Confusion Matrix and Its Derived Metrics</em>. We have the predictions as rows and actual values as columns. However, the interpretation and results will remain the same. The next set of output reports the metrics of interest, along with a few others we have not explored. We have covered the most important ones (sensitivity and precision, that is, positive predictive value); however, it is recommended to explore the remaining metrics, such as negative predicted value, prevalence and detection rate. We can see that we are getting precision of around <em class="italics">73%</em> and <em class="italics">50%</em> recall and overall accuracy of <em class="italics">85%</em>. The results are similar on the train and test datasets; therefore, we can conclude that the model doesn't overfit.</p>
			<h4>Note</h4>
			<p class="callout">The results are not bad overall. Please don't be surprised to see the low recall rate; in scenarios where we have imbalanced datasets, the metrics that are used to assess model performance are business-driven.</p>
			<p>We can conclude that we would correctly predict at least half of the time whenever there is a possibility of rain, and whenever we predict, we are <em class="italics">73%</em> correct. From a business perspective, if we try to contemplate whether we should strive for high recall or precision, we would need to estimate the cost of misclassification.</p>
			<p>In our use case, whenever we predict that there is rainfall predicted for the next day, the operations management team would prepare the team with a higher number of agents to deliver faster. Since there isn't a pre-existing technique to combat rainfall-related problems, we have an opportunity to cover even if we recall only 50% of the times when there is rain. In this problem, since the cost of incorrectly predicting rain will be more expensive for the business, that is, if the chances of rainfall are predicted, the team would invest in pooling more agents for delivery, which comes at an additional cost. Therefore, we would want higher precision, while we are OK to compromise on recall.</p>
			<h4>Note</h4>
			<p class="callout">The ideal scenario is to have high precision and high recall. However, there is always a trade-off in achieving one over the other. In most real-life machine learning use cases, a business-driven decision finalizes the priority to choose either precision or recall.</p>
			<p>The previous model developed in <em class="italics">Exercise 8</em>, <em class="italics">Evaluate a Logistic Regression Model</em>, was developed only using a few variables that were available in the <strong class="inline">df_new</strong> dataset. Let's build an improved model with all the available variables in the dataset and check the performance on the test dataset.</p>
			<p>The best way to iterate for model improvements would be with feature selection and hyperparameter tuning. Feature selection involves selecting the best set of features from the available list through various validation approaches and finalizing a model with the best performance and the least number of features. Hyperparameter tuning deals with building generalized models that will not overfit, that is, a model that performs well on training as well as unseen test data. These topics will be covered in detail in <em class="italics">Chapter 6</em>, <em class="italics">Feature Selection and Dimensionality Reduction</em>, and <em class="italics">Chapter 7</em>, <em class="italics">Model Improvements</em>. For now, the scope of the chapter will be restricted to demonstrate model evaluation only. We will touch on the same use case for hyperparameter tuning and feature selection in upcoming chapters.</p>
			<h3 id="_idParaDest-228"><a id="_idTextAnchor230"/>Exercise 71: Develop a Logistic Regression Model with All of the Independent Variables Available in Our Use Case</h3>
			<p>In the previous exercise, we limited the number of independent variables to only a few. In this example, we will use all the available independent variables in our <strong class="inline">df_new</strong> dataset and create an improved model. We will again use the train dataset to fit the model and test to evaluate the model's performance.</p>
			<p>Perform the following steps to build a logistic regression model with all of the independent variables available within the use case:</p>
			<ol>
				<li value="1">Fit the logistic regression model with all the available independent variables:<p class="snippet">model &lt;- glm(RainTomorrow~., data=train ,family=binomial(link='logit'))</p></li>
				<li>Predict on the train dataset:<p class="snippet">print("Training data results-")</p><p class="snippet">pred_train &lt;-factor(ifelse(predict(model,newdata=train,type = "response") &gt;= 0.5,"Yes","No"))</p></li>
				<li>Create the confusion matrix:<p class="snippet">train_metrics &lt;- confusionMatrix(pred_train, train$RainTomorrow,positive="Yes")</p><p class="snippet">print(train_metrics)</p><p>The output is as follows:</p><p class="snippet">"Training data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  58189  8623</p><p class="snippet">       Yes  3302  8933</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8491          </p><p class="snippet">                 95% CI : (0.8466, 0.8516)</p><p class="snippet">    No Information Rate : 0.7779          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.5104          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.5088          </p><p class="snippet">            Specificity : 0.9463          </p><p class="snippet">         Pos Pred Value : 0.7301          </p><p class="snippet">         Neg Pred Value : 0.8709          </p><p class="snippet">             Prevalence : 0.2221          </p><p class="snippet">         Detection Rate : 0.1130          </p><p class="snippet">   Detection Prevalence : 0.1548          </p><p class="snippet">      Balanced Accuracy : 0.7276          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes             </p></li>
				<li>Predict the results on the test data:<p class="snippet">print("Test data results -")</p><p class="snippet">pred_test &lt;-factor(ifelse(predict(model,newdata=test,type = "response") &gt; 0.5,"Yes","No"))</p></li>
				<li>Create the confusion matrix:<p class="snippet">test_metrics &lt;- confusionMatrix(pred_test, test$RainTomorrow,positive="Yes")</p><p class="snippet">print(test_metrics)</p><p>The output is as follows:</p><p class="snippet">"Test data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  25057  3640</p><p class="snippet">       Yes  1358  3823</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8525          </p><p class="snippet">                 95% CI : (0.8486, 0.8562)</p><p class="snippet">    No Information Rate : 0.7797          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.5176          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.5123          </p><p class="snippet">            Specificity : 0.9486          </p><p class="snippet">         Pos Pred Value : 0.7379          </p><p class="snippet">         Neg Pred Value : 0.8732          </p><p class="snippet">             Prevalence : 0.2203          </p><p class="snippet">         Detection Rate : 0.1128          </p><p class="snippet">   Detection Prevalence : 0.1529          </p><p class="snippet">      Balanced Accuracy : 0.7304          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes</p></li>
			</ol>
			<p>We leverage all the variables within the dataset to create a logistic regression model using the <strong class="inline">glm</strong> function. We then use the <strong class="bold">fitted</strong> model to predict the outcomes for the train and the test datasets; akin to the previous exercise.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/2HgwjaU.</p>
			<p>Notice how the overall accuracy, precision, and recall has improved a bit (though marginally). The results are fair and we can iterate with logistic regression to improving them further. For now, let's explore a few other classification techniques and study the performance of the model.</p>
			<h4>Note</h4>
			<p class="callout">In this exercise, we have not printed the model's summary statistics, akin to the first model, with a few variables. If printed, the results would consume less than two pages of the chapter. For now, we will ignore that since we are not exploring the model characteristics that are reported by R; instead, we are evaluating a model purely from the accuracy, precision, and recall metrics on the train and test dataset.</p>
			<p class="callout">The ideal way to get the best model would be to eliminate all statistically insignificant variables, remove multicollinearity, and treat the data for outliers, and so on. All these steps have been ignored for now, given the scope of the chapter.</p>
			<h3 id="_idParaDest-229"><a id="_idTextAnchor231"/>Activity 8: Building a Logistic Regression Model with Additional Features</h3>
			<p>We built a simple model with few features in <em class="italics">Exercise 8</em>, <em class="italics">Evaluate a Logistic Regression Model</em>, and then with all the features in <em class="italics">Exercise 9</em>, <em class="italics">Develop a Logistic Regression Model with All of the Independent Variables Available in Our Use Case</em>. In this activity, we will build a logistic regression model with additional features that we can generate using simple mathematical transformations. It is good practice to add additional transformations of numeric features with log transformations, square and cube power transformations, square root transformations, and so on.</p>
			<p>Perform the following steps to develop a logistic regression model with additional features engineered:</p>
			<ol>
				<li value="1">Create a copy of the <strong class="inline">df_new</strong> dataset in <strong class="inline">df_copy</strong> for the activity and select any three numeric features (for example, <strong class="inline">MaxTemp</strong>, <strong class="inline">Rainfall</strong> and <strong class="inline">Humidity3pm</strong>).</li>
				<li>Engineer new features with square and cube power and square root transformations for each of the selected features.</li>
				<li>Divide the <strong class="inline">df_copy</strong> dataset into train and test in a 70:30 ratio.</li>
				<li>Fit the model with the new train data, evaluate it on test data, and finally, compare the results.<p>The output is as follows:</p><p class="snippet">"Test data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  25057  3640</p><p class="snippet">       Yes  1358  3823</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8525          </p><p class="snippet">                 95% CI : (0.8486, 0.8562)</p><p class="snippet">    No Information Rate : 0.7797          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.5176          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.5123          </p><p class="snippet">            Specificity : 0.9486          </p><p class="snippet">         Pos Pred Value : 0.7379          </p><p class="snippet">         Neg Pred Value : 0.8732          </p><p class="snippet">             Prevalence : 0.2203          </p><p class="snippet">         Detection Rate : 0.1128          </p><p class="snippet">   Detection Prevalence : 0.1529          </p><p class="snippet">      Balanced Accuracy : 0.7304          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes             </p><h4>Note</h4><p class="callout">You can find the solution for this activity on page 451.</p></li>
			</ol>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor232"/>Decision Trees</h2>
			<p>Like logistic regression, there is another popular classification technique that is very popular due to its simplicity and white-box nature. A decision tree is a simple flowchart that is represented in the form of a tree (an inverted tree). It starts with a root node and branches into several nodes, which can be traversed based on a decision, and ends with a leaf node where the <em class="italics">final outcome</em> is determined. Decision trees can be used for regression, as well as classification use cases. There are several variations of decision trees implemented in machine learning. A few popular choices are listed here:</p>
			<ul>
				<li><strong class="bold">Iterative Dichotomiser 3</strong> (<strong class="bold">ID3</strong>)</li>
				<li><strong class="bold">Successor to ID3</strong> (<strong class="bold">C4.5</strong>)</li>
				<li><strong class="bold">Classification and Regression Tree</strong> (<strong class="bold">CART</strong>) </li>
				<li><strong class="bold">CHi-squared Automatic Interaction Detector</strong> (<strong class="bold">CHAID</strong>)</li>
				<li><strong class="bold">Conditional Inference Trees</strong> (<strong class="bold">C Trees</strong>)</li>
			</ul>
			<p>The preceding list is not exhaustive. There are other alternatives, and each of them has small variations in how they approach the tree creation process. In this chapter, we will limit our exploration to <strong class="keyword">CART Decision Trees</strong>, which are the most widely used. R provides a few packages that house the implementation of the CART algorithm. Before we delve into the implementation, let's explore a few important aspects of decision trees in the following sections.</p>
			<h3 id="_idParaDest-231"><a id="_idTextAnchor233"/>How Do Decision Trees Work?</h3>
			<p>Each variation of decision trees has a slightly different approach. Overall, if we try to simplify the pseudocode for a generic decision tree, it can be summarized as follows:</p>
			<ol>
				<li value="1">Select the root node (the node corresponds to a variable).</li>
				<li>Partition the data into groups.</li>
				<li>For each group from the previous step:<p>Create a decision node or leaf node (based on the splitting criteria).</p><p>Repeat until node size &lt;= threshold or features = empty.</p></li>
			</ol>
			<p>Variations between different forms of tree implementations include the way categorical and numerical variables are handled, the approach used to select the root node and consecutive nodes in the tree, the rules to branch each decision node, and so on.</p>
			<p>The following visual is a sample decision tree. The root node and the decision nodes are the independent variables we provide to the algorithm. The leaf nodes denote the final outcome, whereas the root node and the intermediate decision nodes help in traversing the data to the leaf node. The simplicity of a decision tree is what makes it so effective and easy to interpret. This helps in easily identifying rules for a prediction task. Often, many research and business initiatives leverage decision trees to design a set of rules for a simple classification system:</p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/C12624_05_04.jpg" alt="Figure 5.4: Sample decision tree&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.4: Sample decision tree</h6>
			<p>In a general sense, given a combination of dependent and several independent variables, the decision tree algorithm calculates a metric that represents the goodness of fit between the dependent target variable and all independent variables. For classification use cases, entropy and information gain are commonly used metrics in CART decision trees. The variable with the best fit for the metric is chosen as the root node and the next best is used as the decision nodes in the descending order of fit. The nodes are terminated into leaf nodes based on a defined threshold. The tree keeps growing till it exhausts the number of variables for decision nodes or when a predefined threshold for the number of nodes is reached.</p>
			<p>To improve tree performance and reduce overfitting, a few strategies, such as restricting the depth or breadth of the tree or additional rules for leaf nodes or decision nodes help in generalizing a tree for prediction.</p>
			<p>Let's implement the same use case using CART decision trees in R. The CART model is available through the <strong class="inline">rpart</strong> package in R. This algorithm was developed by Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone in 1984 and has been widely adopted in the industry.</p>
			<h3 id="_idParaDest-232"><a id="_idTextAnchor234"/>Exercise 72: Create a Decision Tree Model in R</h3>
			<p>In this exercise, we will create a decision tree model in R using the same data and the use case we leveraged in <em class="italics">Exercise 9</em>, <em class="italics">Develop a Logistic Regression Model with All of the Independent Variables Available in Our Use Case</em>. We will try to study whether there are any differences in the performance of a decision tree model over a logistic regression model.</p>
			<p>Perform the following steps to create a decision tree model in R:</p>
			<ol>
				<li value="1">Import the <strong class="inline">rpart</strong> and <strong class="inline">rpart.plot</strong> packages using the following command:<p class="snippet">library(rpart)</p><p class="snippet">library(rpart.plot)</p></li>
				<li>Build the CART model with all of the variables:<p class="snippet">tree_model &lt;- rpart(RainTomorrow~.,data=train)</p></li>
				<li>Plot the cost parameter:<p class="snippet">plotcp(tree_model)</p><p>The output is as follows:</p><div id="_idContainer193" class="IMG---Figure"><img src="image/C12624_05_05.jpg" alt="Figure 5.5: Decision tree model&#13;&#10;"/></div><h6>Figure 5.5: Decision tree model</h6></li>
				<li>Plot the tree using the following command:<p class="snippet">rpart.plot(tree_model,uniform=TRUE, main="Predicting RainFall")</p><p>The output is as follows:</p><div id="_idContainer194" class="IMG---Figure"><img src="image/C12624_05_06.jpg" alt="Figure 5.6: Predicting rainfall&#13;&#10;"/></div><h6>Figure 5.6: Predicting rainfall</h6></li>
				<li>Make predictions on the train data:<p class="snippet">print("Training data results -")</p><p class="snippet">pred_train &lt;- predict(tree_model,newdata = train,type = "class")</p><p class="snippet">confusionMatrix(pred_train, train$RainTomorrow,positive="Yes")</p><p>The output is as follows:</p><p class="snippet">"Training data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  59667 11215</p><p class="snippet">       Yes  1824  6341</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.835           </p><p class="snippet">                 95% CI : (0.8324, 0.8376)</p><p class="snippet">    No Information Rate : 0.7779          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.4098          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.36119         </p><p class="snippet">            Specificity : 0.97034         </p><p class="snippet">         Pos Pred Value : 0.77661         </p><p class="snippet">         Neg Pred Value : 0.84178         </p><p class="snippet">             Prevalence : 0.22210         </p><p class="snippet">         Detection Rate : 0.08022         </p><p class="snippet">   Detection Prevalence : 0.10329         </p><p class="snippet">      Balanced Accuracy : 0.66576         </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes             </p></li>
				<li>Make predictions on the test data:<p class="snippet">print("Test data results -")</p><p class="snippet">pred_test &lt;- predict(tree_model,newdata = test,type = "class")</p><p class="snippet">confusionMatrix(pred_test, test$RainTomorrow,positive="Yes")</p><p>The output is as follows:</p><p class="snippet">[1] "Test data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  25634  4787</p><p class="snippet">       Yes   781  2676</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8356          </p><p class="snippet">                 95% CI : (0.8317, 0.8396)</p><p class="snippet">    No Information Rate : 0.7797          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.4075          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.35857         </p><p class="snippet">            Specificity : 0.97043         </p><p class="snippet">         Pos Pred Value : 0.77408         </p><p class="snippet">         Neg Pred Value : 0.84264         </p><p class="snippet">             Prevalence : 0.22029         </p><p class="snippet">         Detection Rate : 0.07899         </p><p class="snippet">   Detection Prevalence : 0.10204         </p><p class="snippet">      Balanced Accuracy : 0.66450         </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes</p></li>
			</ol>
			<p>The <strong class="inline">rpart</strong> library provides us with the CART implementation of decision trees. There are additional libraries that help us visualize the decision tree in R. We have used <strong class="inline">rpart.plot</strong> here. If the package is not already installed, please install it using the <strong class="inline">install.packages</strong> command. We use the <strong class="inline">rpart</strong> function to create the tree model and we use all the available independent variables. We then use the <strong class="inline">plotcp</strong> function to visualize the complexity parameter's corresponding validation error on different iterations. We also use the <strong class="inline">plot.rpart</strong> function to plot the decision tree. </p>
			<p>Finally, we make predictions on the train as well as the test data and build the confusion matrix and calculate the metrics of interest using the <strong class="inline">confusionMatrix</strong> function for the train and test datasets individually.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/2WECLgZ.</p>
			<p>The CART decision tree implemented in R has several optimizations already in place. The function, by default, sets a ton of parameters for optimum results. In a decision tree, there are several parameters that we can manually set to tune the performance based on our requirements. However, the R implementation does a great job of setting a wide number of parameters with a relatively good value by default. These additional settings can be added to the <strong class="inline">rpart</strong> tree with the <strong class="inline">control</strong> parameter.</p>
			<p>We can add the following parameter to the tree model:</p>
			<p class="snippet">control = rpart.control(</p>
			<p class="snippet">    minsplit = 20, </p>
			<p class="snippet">    minbucket = round(minsplit/3), </p>
			<p class="snippet">    cp = 0.01, </p>
			<p class="snippet">    maxcompete = 4, </p>
			<p class="snippet">    maxsurrogate = 5, </p>
			<p class="snippet">    usesurrogate = 2, xval = 10, </p>
			<p class="snippet">    surrogatestyle = 0, </p>
			<p class="snippet">    maxdepth = 30</p>
			<p class="snippet">)</p>
			<p>One parameter of interest would be the <strong class="bold">complexity parameter</strong>. The complexity parameter restricts a split in the decision node if it does not decrease the overall lack of fit by the factor of the defined value. The default value is set to <strong class="inline">0.01</strong>. We can further change this to a lower number that would make the tree grow deeper and become more complicated. The <strong class="inline">plotcp</strong> function visualizes the relative validation error for different values of <strong class="inline">cp</strong>, that is, the complexity parameter. The most ideal value for <strong class="inline">cp</strong> is the leftmost value below the dotted line in the plot in <em class="italics">Figure 5.4</em>. In this case (as shown in the plot), the best value is 0.017. Since this value is not very different from the default value, we don't change it further.</p>
			<p>The next plot in <em class="italics">Figure 5.5</em> helps us visualize the actual decision tree constructed by the algorithm. We can see the simple set of rules being constructed using the available data. As you can see, only two independent variables, that is, <strong class="inline">Humidity3pm</strong> and <strong class="inline">WindGustSpeed</strong>, have been selected for the tree. If we change the complexity parameter to <em class="italics">0.001</em> instead of <em class="italics">0.01</em>, we can see a much deeper tree (which could overfit the model) would have been constructed. Finally, we can see the results from the confusion matrix (step 6) along with additional metrics of interest for the train and test dataset.</p>
			<p>We can see that the results are similar for the train and test dataset. We can therefore conclude that the model doesn't overfit. However, there is a significant drop in accuracy (<em class="italics">83%</em>) and recall (<em class="italics">35%</em>), while the precision has increased to a slightly higher value (<em class="italics">77%</em>).</p>
			<p>We have now worked with a few white-box modeling techniques. Given the simplicity and ease of interpretation of white-box models, they are the most preferred technique for classification use cases in business, where reasoning and driver analysis is of paramount importance. However, there are a few scenarios where a business might be more interested in the <em class="italics">net outcome </em>of the model rather than the entire interpretation of the outcome. In such cases, the end model performance is of more interest. In our use case, we want to achieve high precision. Let's explore a few black-box models that are superior (in most cases) to white-box models in terms of model performance and that can be achieved with far less effort and more training data.</p>
			<h3 id="_idParaDest-233"><a id="_idTextAnchor235"/>Activity 9: Create a Decision Tree Model with Additional Control Parameters</h3>
			<p>The decision tree model we created in <em class="italics">Exercise 10</em>, <em class="italics">Create a Decision Tree Model in R</em>, used the default control parameters for the tree. In this activity, we will override a few control parameters and study its impact on the overall tree-fitting process.</p>
			<p>Perform the following steps to create a decision tree model with additional control parameters:</p>
			<ol>
				<li value="1">Load the <strong class="inline">rpart</strong> library.</li>
				<li>Create the control object for the decision tree with new values: <strong class="inline">minsplit =15</strong> and <strong class="inline">cp = 0.00</strong>.</li>
				<li>Fit the tree model with the train data and pass the control object to the <strong class="inline">rpart</strong> function.</li>
				<li>Plot the complexity parameter plot to see how the tree performs at different values of <strong class="inline">CP</strong>.</li>
				<li>Use the fitted model to make predictions on the train data and create the confusion matrix.</li>
				<li>Use the fitted model to make predictions on the test data and create the confusion matrix.<p>The output is as follows:</p><p class="snippet">"Test data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  25068  3926</p><p class="snippet">       Yes  1347  3537</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8444          </p><p class="snippet">                 95% CI : (0.8404, 0.8482)</p><p class="snippet">    No Information Rate : 0.7797          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.4828          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.4739          </p><p class="snippet">            Specificity : 0.9490          </p><p class="snippet">         Pos Pred Value : 0.7242          </p><p class="snippet">         Neg Pred Value : 0.8646          </p><p class="snippet">             Prevalence : 0.2203          </p><p class="snippet">         Detection Rate : 0.1044          </p><p class="snippet">   Detection Prevalence : 0.1442          </p><p class="snippet">      Balanced Accuracy : 0.7115          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes </p><h4>Note</h4><p class="callout">You can find the solution for this activity on page 454.</p></li>
			</ol>
			<h3 id="_idParaDest-234"><a id="_idTextAnchor236"/>Ensemble Modelling</h3>
			<p>Ensemble modeling is one of the most popular approaches used in classification and regression modeling techniques when there is a need for improved performance with a larger training sample. In simple words, ensemble modeling can be defined by breaking down the name into individual terms: <strong class="bold">ensemble</strong> and <strong class="bold">modeling</strong>. We have already studied modeling in this book; an ensemble in simple terms is a <strong class="bold">group</strong>. Therefore, the process of building several models for the same task instead of just one model and then combining the results into a single outcome through any means, such as averaging or voting, and many others, is called <strong class="bold">ensemble modeling</strong>.</p>
			<p>We can build ensembles of any models, such as linear models or tree models, and in fact can even build an ensemble of ensemble models. However, the most popular approach is using tree models as the base for ensembles. There are two broad types of ensemble models:</p>
			<ul>
				<li><strong class="bold">Bagging</strong>: Here, each model is built in parallel with some randomization introduced within each model, and the results of all models are combined using a simple voting mechanism. Say we built 100 tree models and 60 models predicted the outcome as <em class="italics">Yes</em> and 40 predicted it as <em class="italics">No</em>. The end result would be a <em class="italics">Yes</em>.</li>
				<li><strong class="bold">Boosting</strong>: Here, models are built sequentially and the results of the first model are used to tune the next model. Each model iteratively learns from errors made by the previous model and tries to improve with successive iterations. The result is usually a weighted average of all the individual outcomes.</li>
			</ul>
			<p>There are several implementations available in bagging as well as boosting. <strong class="bold">Bagging</strong> itself is an ensemble model available in R. By far the most popular bagging technique used is random forest. Another bagging technique along similar lines as random forest is <strong class="bold">extra trees</strong>. Similarly, a few examples of boosting techniques are AdaBoost, Stochastic Gradient Boosting, BrownBoost, and many others. However, the most popular boosting technique is <strong class="bold">XGBoost</strong>, which is derived from the name <strong class="bold">EXtreme Gradient Boosting</strong>. In most cases, for classification as well as regression use cases, data scientists prefer using random forests or XGBoost models. A recent survey on Kaggle (an online data science community) revealed the most popular technique used for most machine learning competitions were always random forest and XGBoost. In this chapter, we will take a closer look at both models.</p>
			<h3 id="_idParaDest-235"><a id="_idTextAnchor237"/>Random Forest</h3>
			<p><strong class="bold">Random forest</strong> is the most popular bagging technique used in machine learning. It was developed by Leo Brieman, the author of CART. This simple technique is so effective that it is almost always the first choice of algorithm for a data scientist given a supervised use case. Random forest is a good choice for classification as well as regression use cases. It is a highly effective method for reducing overfitting with a bare minimum amount of effort. Let's have a deeper understanding of how random forests work.</p>
			<p>As we already know, random forest is an ensemble modeling technique, where we build several models and combine their results using a simple voting technique. In random forests, we use decision trees as the base model. The inner workings of the algorithm can be fairly guessed from the name itself, that is, random (since it induces a layer of randomization in every model that is built) and forest (since there are several <em class="italics">tree</em> models we build). Before we get into the actual workings of the algorithm, we first need to understand the story of its predecessor, <strong class="bold">bagging</strong>, and study why we need ensembles.</p>
			<h3 id="_idParaDest-236"><a id="_idTextAnchor238"/>Why Are Ensemble Models Used?</h3>
			<p>The first question that would have surfaced in your thoughts may be, why do we need to build several models for the same task in the first place? Is it necessary? Well, yes! When we build ensembles, we don't build the exact same model several times; instead, every model we build will be different from the others in some way. The intuition behind this can be understood using a simple example from our day-to-day lives. It is built on the principle that several weak learners combined together build a stronger and more robust model.</p>
			<p>Let's understand this idea using a simple example. Say you reach a new city and want to know the chances of there being rain in the city the next day. Assuming technology is not an available option, the easiest way you could find this out would be to ask someone in the neighborhood who has been a dweller of the place for a while. Maybe the answer would not always be correct; if someone said that there was a very high chance of rain the next day, it doesn't necessarily mean that it would certainly rain. Therefore, to make an improved guess, you ask several people in the neighborhood. Now, if 7 out of the 10 people you asked mentioned that there was a high chance of rain the next day, then it almost certainly would rain the very next day. The reason this works effectively is because every person you reached out to would have some understanding about rain patterns and also every person's understanding about those patterns would be a bit different. Though the differences are not miles apart, some level of randomness among the people's understanding when aggregated for a collective answer would yield a better answer.</p>
			<h3 id="_idParaDest-237"><a id="_idTextAnchor239"/>Bagging – Predecessor to Random Forest</h3>
			<p>Ensemble modeling works on the same principle. Here, in each model, we induce some level of randomness. The bagging algorithm brings in this randomness for each model on the training data. The name bagging is derived from <strong class="bold">Bootstrap Aggregation</strong>; a process where we sample two-thirds of the available data with replacement data for training and the remainder for testing and validation. Here, each model, that is, a decision tree model, trains on a slightly different dataset and therefore might have a slightly different outcome for the same test sample. Bagging, in a way, mimics the real-world example that we discussed and therefore combines several weak learners (decision tree models) into a strong learner.</p>
			<h3 id="_idParaDest-238"><a id="_idTextAnchor240"/>How Does Random Forest Work?</h3>
			<p><strong class="bold">Random forest</strong> is basically a successor to bagging. Here, apart from the randomness in the training data, random forest adds an additional layer of randomness with the feature set. Therefore, each decision tree not only has bootstrap aggregation, that is, two thirds of the training data with replacement, but also a subset of features randomly selected from the available list. Thus, each individual decision tree in the ensemble has a slightly different training dataset and a slightly different set of features to train. This additional layer of randomness works effectively in generalizing the model and reduces variance.</p>
			<h3 id="_idParaDest-239"><a id="_idTextAnchor241"/>Exercise 73: Building a Random Forest Model in R</h3>
			<p>In this exercise, we will build a random forest model on the same dataset we leveraged in Exercises 8, 9, and 10. We will leverage ensemble modelling and test whether the overall model performance improves compared to decision trees and logistic regression.</p>
			<h4>Note</h4>
			<p class="callout">To get started, we can quickly build a random forest model using the same dataset we used earlier. The <strong class="inline">randomForest</strong> package in R provides the implementation for the model, along with a few additional functions to optimize the model.</p>
			<p>Let's look at a basic random forest model. Perform the following steps:</p>
			<ol>
				<li value="1">First, import the <strong class="inline">randomForest</strong> library using the following command:<p class="snippet">library(randomForest)</p></li>
				<li>Build a random forest model with all of the independent features available:<p class="snippet">rf_model &lt;- randomForest(RainTomorrow ~ . , data = train, ntree = 100,                                             importance = TRUE, </p><p class="snippet">                                            maxnodes=60)</p></li>
				<li>Evaluate on the training data:<p class="snippet">print("Training data results -")</p><p class="snippet">pred_train &lt;- predict(rf_model,newdata = train,type = "class")</p><p class="snippet">confusionMatrix(pred_train, train$RainTomorrow,positive="Yes")</p></li>
				<li>Evaluate on the test data:<p class="snippet">print("Test data results -")</p><p class="snippet">pred_test &lt;- predict(rf_model,newdata = test,type = "class")</p><p class="snippet">confusionMatrix(pred_test, test$RainTomorrow,positive="Yes")</p></li>
				<li>Plot the feature importance:<p class="snippet">varImpPlot(rf_model)</p><p>The output is as follows:</p><p class="snippet">[1] "Training data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  59630 10133</p><p class="snippet">       Yes  1861  7423</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8483          </p><p class="snippet">                 95% CI : (0.8457, 0.8508)</p><p class="snippet">    No Information Rate : 0.7779          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.472           </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.42282         </p><p class="snippet">            Specificity : 0.96974         </p><p class="snippet">         Pos Pred Value : 0.79955         </p><p class="snippet">         Neg Pred Value : 0.85475         </p><p class="snippet">             Prevalence : 0.22210         </p><p class="snippet">         Detection Rate : 0.09391         </p><p class="snippet">   Detection Prevalence : 0.11745         </p><p class="snippet">      Balanced Accuracy : 0.69628         </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes             </p><p class="snippet">                                          </p><p class="snippet">[1] "Test data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction    No   Yes</p><p class="snippet">       No  25602  4369</p><p class="snippet">       Yes   813  3094</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.847           </p><p class="snippet">                 95% CI : (0.8432, 0.8509)</p><p class="snippet">    No Information Rate : 0.7797          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.4629          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.41458         </p><p class="snippet">            Specificity : 0.96922         </p><p class="snippet">         Pos Pred Value : 0.79191         </p><p class="snippet">         Neg Pred Value : 0.85423         </p><p class="snippet">             Prevalence : 0.22029         </p><p class="snippet">         Detection Rate : 0.09133         </p><p class="snippet">   Detection Prevalence : 0.11533         </p><p class="snippet">      Balanced Accuracy : 0.69190         </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : Yes             </p><p class="snippet">                              </p></li>
			</ol>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/C12624_05_07.jpg" alt="Figure 5.7: Random Forest model&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.7: Random Forest model</h6>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/2Q2xKwd.</p>
			<h3 id="_idParaDest-240"><a id="_idTextAnchor242"/>Activity 10: Build a Random Forest Model with a Greater Number of Trees</h3>
			<p>In <em class="italics">Exercise 11</em>, <em class="italics">Building a Random Forest Model in R</em>, we created a random forest model with just 100 trees; we can build a more robust model with a higher number of trees. In this activity, we will create a random forest model with 500 trees and study the impact of the model having only 100 trees. In general, we expect the model's performance to improve (at least marginally with an increased number of trees). This comes with higher computational time for the model to converge.</p>
			<p>Perform the following steps to build a random forest model with 500 trees:</p>
			<ol>
				<li value="1">Develop a random forest model with a higher number of trees; say, 500. Readers are encouraged to try higher numbers such as 1,000, 2,000, and so on, and study the incremental improvements in each version. </li>
				<li>Leverage the fitted model to predict estimates on the train-and-test data and study whether there was any improvement compared to the model with 100 trees.<h4>Note</h4><p class="callout">You can find the solution for this activity on page 457.</p></li>
			</ol>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor243"/>XGBoost</h2>
			<p><strong class="bold">XGBoost</strong> is the most popular boosting technique in recent times. Although there have been various new versions that have been developed by large corporations, XGBoost still remains the undisputed king. Let's look at a brief history of boosting.</p>
			<h3 id="_idParaDest-242"><a id="_idTextAnchor244"/>How Does the Boosting Process Work?</h3>
			<p>Boosting differs from bagging in its core principles; the learning process is, in fact, sequential. Every model built in an ensemble is ideally an improved version of the previous model. To understand boosting in simple terms, imagine you are playing a game where you must remember all the objects placed on the table that you are shown just once for 30 seconds. The moderator of the game arranges around 50-100 different objects on a table, such as a bat, ball, clock, die, coins, and so on, and covers them with a large piece of cloth. When the game begins, he withdraws the cloth from the table and gives you exactly 30 seconds to see them and puts the curtain back. You now must recollect all the objects you can remember. The participant who can recollect the most, aces the game.</p>
			<p>In this game, let's add one new dimension. Assume you are a team and the players take turns one by one to announce all the objects they can recollect, while the others listen to them. Say there are 10 participants; each participant steps forward and announces out loud the objects they can recollect from the table. By the time the second player steps forward, they have heard all the objects called out by the first player. They would have mentioned a few objects that the second player might not have recollected. To improve on the first player, the second player learns a few new objects from the first player, adds them to his list, and then announces them out loud. By the time the last player steps forward, they have already learned several objects that other players recollected, which they failed to recollect themselves. </p>
			<p>Putting those together, that player creates the most exhaustive list and aces the competition. The fact that each player announces the list sequentially helps the next player learn from their mistakes and improvise on it.</p>
			<p>Boosting works in the same way. Each model trained sequentially is imparted with additional knowledge, such that the errors of the first model are learned better in the second model. Say the first model learns to classify well for most cases of a specific independent variable; however, it fails to correctly predict for just one specific category. The next model is imparted with a different training sample, such that the model learns better for the category where the previous model fails. A simple example would be oversampling based on the variable or category of interest. Boosting effectively reduces bias and therefore improves the model's performance.</p>
			<h3 id="_idParaDest-243"><a id="_idTextAnchor245"/>What Are Some Popular Boosting Techniques?</h3>
			<p>The boosting techniques introduced earlier were not very popular, because they were easily overfit and often required, relatively, a lot of effort in tuning to achieve great performance. AdaBoost, BrownBoost, Gradient Boosting, and Stochastic Gradient Boosting are all boosting techniques that were popular for a long time. However, in 2014, when T Chen and others introduced XGBoost (<strong class="bold">Extreme Gradient Boosting</strong>), it ushered in a new height in the boosting performance.</p>
			<h3 id="_idParaDest-244"><a id="_idTextAnchor246"/>How Does XGBoost Work?</h3>
			<p>XGBoost natively introduced regularization, which helps models combat overfitting and thus delivered high performance. Compared to other available boosting techniques at the time, XGBoost reduced the overfitting problem significantly and with the least amount of effort. With current implementations of the model in R or any other language, XGBoost almost always performs great with the default parameter setting. (Though, this is not always true; in many cases, random forest outperforms XGBoost). XGBoost has been among the most popular choice of algorithms used in data science hackathons and enterprise projects.</p>
			<p>In a nutshell, XGBoost has regularization introduced in the objective function, which penalizes the model when it gets more complicated in a training iteration. Discussing the depth of mathematical constructs that goes into XGBoosting is beyond the scope of this chapter. You can refer to T Chen's paper here (https://arxiv.org/abs/1603.02754) for further notes. Also, this blog will help you to understand the mathematical differences between GBM and XGBoost in a simple way: https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d.</p>
			<h3 id="_idParaDest-245"><a id="_idTextAnchor247"/>Implementing XGBoost in R</h3>
			<p>We can leverage the XGBoost package, which provides a neat implementation of the algorithm. There are a few differences in the implementation approach that we will need to take care of before getting started. Unlike other implementations of algorithms in R, XGBoost does not handle categorical data (others take care of converting it into numeric data internally). The internal functioning of XGBoost in R doesn't handle the automatic conversion of categorical columns into numeric columns. Therefore, we manually convert categorical columns into numeric or one-hot encoded form. </p>
			<p>A one-hot encoded form basically represents a single categorical column as a binary encoded form. Say we have a categorical column with values such as <strong class="bold">Yes</strong>/<strong class="bold">No</strong>/<strong class="bold">Maybe</strong>; then, we transform this single variable, where we have an individual variable for each value of the categorical variable indicating its value as <strong class="bold">0</strong> or <strong class="bold">1</strong>. So, the values for the columns <strong class="bold">Yes</strong>, <strong class="bold">No</strong>, and <strong class="bold">Maybe</strong> will take <strong class="bold">0</strong> and <strong class="bold">1</strong> based on the original value.</p>
			<p>One-hot encoding is demonstrated in the following table:</p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/C12624_05_08.jpg" alt="Figure 5.8: One-hot encoding&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.8: One-hot encoding</h6>
			<p>Let's transform the data into the required form and build an XGBoost model on the dataset.</p>
			<h3 id="_idParaDest-246"><a id="_idTextAnchor248"/>Exercise 74: Building an XGBoost Model in R</h3>
			<p>Just as we did in <em class="italics">Exercise 11</em>, <em class="italics">Building a Random Forest Model in R</em>, we will try to improve the performance of the classification model by building an XGBoost model for the same use case and dataset as in <em class="italics">Exercise 11</em>, <em class="italics">Building a Random Forest Model in R</em>.</p>
			<p>Perform the following steps to build an XGBoost model in R.</p>
			<ol>
				<li value="1">Create list placeholders for the target, categorical, and numeric variables:<p class="snippet">target&lt;- "RainTomorrow"</p><p class="snippet">categorical_columns &lt;- c("RainToday","WindGustDir","WindDir9am",</p><p class="snippet">"WindDir3pm", "new_location")</p><p class="snippet">numeric_columns &lt;- setdiff(colnames(train),c(categorical_columns,target))</p></li>
				<li>Convert the categorical factor variables into character. This will be useful for converting them into one-hot-encoded forms:<p class="snippet">df_new &lt;- df_new %&gt;% mutate_if(is.factor, as.character)</p></li>
				<li>Convert the categorical variables into one-hot encoded forms using the <strong class="inline">dummyVars</strong> function from the <strong class="inline">caret</strong> package:<p class="snippet">dummies &lt;- dummyVars(~ RainToday + WindGustDir + WindDir9am + </p><p class="snippet">                     WindDir3pm + new_location, data = df_new)</p><p class="snippet">df_all_ohe &lt;- as.data.frame(predict(dummies, newdata = df_new))</p></li>
				<li>Combine numeric variables and the one-hot encoded variables from the third step into a single DataFrame named <strong class="inline">df_final</strong>:<p class="snippet">df_final &lt;- cbind(df_new[,numeric_columns],df_all_ohe)</p></li>
				<li>Convert the target variable into numeric form, as the XGBoost implementation in R doesn't accept factor or character forms:<p class="snippet">y &lt;- ifelse(df_new[,target] == "Yes",1,0)</p></li>
				<li>Split the <strong class="inline">df_final</strong> dataset into train (70%) and test (30%) datasets:<p class="snippet">set.seed(2019)</p><p class="snippet">train_index &lt;- sample(seq_len(nrow(df_final)),floor(0.7 * nrow(df_final)))</p><p class="snippet">xgb.train &lt;- df_final[train_index,]</p><p class="snippet">y_train&lt;- y[train_index]</p><p class="snippet">xgb.test &lt;- df_final[-train_index,]</p><p class="snippet">y_test &lt;- y[-train_index]</p></li>
				<li>Build an XGBoost model using the <strong class="inline">xgboost</strong> function. Pass the train data and <strong class="inline">y_train</strong> target variable and define the <strong class="inline">eta = 0.01</strong>, <strong class="inline">max_depth = 6</strong>, <strong class="inline">nrounds = 200</strong>, and <strong class="inline">colsample_bytree = 1</strong> hyperparameters, define the evaluation metric as <strong class="inline">logloss,</strong> and the <strong class="inline">objective</strong> function as <strong class="inline">binary:logistic</strong>, since we are dealing with binary classification:<p class="snippet">xgb &lt;- xgboost(data = data.matrix(xgb.train), </p><p class="snippet">               label = y_train, </p><p class="snippet">               eta = 0.01,</p><p class="snippet">               max_depth = 6, </p><p class="snippet">               nround=200, </p><p class="snippet">               subsample = 1,</p><p class="snippet">               colsample_bytree = 1,</p><p class="snippet">               seed = 1,</p><p class="snippet">               eval_metric = "logloss",</p><p class="snippet">               objective = "binary:logistic",</p><p class="snippet">               nthread = 4</p><p class="snippet">)</p></li>
				<li>Make a prediction using the fitted model on the train dataset and create the confusion matrix to evaluate the model's performance on the train data:<p class="snippet">print("Training data results -")</p><p class="snippet">pred_train &lt;- factor(ifelse(predict(xgb,data.matrix(xgb.train),type="class")&gt;0.5,1,0))</p><p class="snippet">confusionMatrix(pred_train,factor(y_train),positive='1')</p><p>The output is as follows:</p><p class="snippet">"Training data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction     0     1</p><p class="snippet">         0 58967  8886</p><p class="snippet">         1  2524  8670</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8557          </p><p class="snippet">                 95% CI : (0.8532, 0.8581)</p><p class="snippet">    No Information Rate : 0.7779          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.5201          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.4938          </p><p class="snippet">            Specificity : 0.9590          </p><p class="snippet">         Pos Pred Value : 0.7745          </p><p class="snippet">         Neg Pred Value : 0.8690          </p><p class="snippet">             Prevalence : 0.2221          </p><p class="snippet">         Detection Rate : 0.1097          </p><p class="snippet">   Detection Prevalence : 0.1416          </p><p class="snippet">      Balanced Accuracy : 0.7264          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : 1               </p></li>
				<li>Now, as in the previous step, make predictions using the fitted model on the test dataset and create the confusion matrix to evaluate the model's performance on the test data:<p class="snippet">print("Test data results -")</p><p class="snippet">pred_test &lt;- factor(ifelse(predict(xgb,data.matrix(xgb.test),</p><p class="snippet">type="class")&gt;0.5,1,0))</p><p class="snippet">confusionMatrix(pred_test,factor(y_test),positive='1')</p><p>The output is as follows:</p><p class="snippet">[1] "Test data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction     0     1</p><p class="snippet">         0 25261  3884</p><p class="snippet">         1  1154  3579</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8513          </p><p class="snippet">                 95% CI : (0.8475, 0.8551)</p><p class="snippet">    No Information Rate : 0.7797          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.5017          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.4796          </p><p class="snippet">            Specificity : 0.9563          </p><p class="snippet">         Pos Pred Value : 0.7562          </p><p class="snippet">         Neg Pred Value : 0.8667          </p><p class="snippet">             Prevalence : 0.2203          </p><p class="snippet">         Detection Rate : 0.1056          </p><p class="snippet">   Detection Prevalence : 0.1397          </p><p class="snippet">      Balanced Accuracy : 0.7179          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : 1</p></li>
			</ol>
			<p>If we take a closer look at the results from the model, we can see a slight improvement in the performance compared to random forest model results. The <strong class="bold">recall</strong> is around <em class="italics">48%</em> on the test dataset and <strong class="bold">Precision</strong> is around <em class="italics">76%</em>. We can see that the recall for the XGBoost model has improved from 41% but the precision has decreased from 79% to 76%. If we tweak the model predictions (as mentioned earlier) by shifting the probability cut-off threshold a notch higher, say <strong class="inline">0.54</strong> instead of <strong class="inline">0.5</strong>, we can increase the precision (to match random forest) while still having slightly higher recall than random forest. The increase in recall for XGBoost is significantly higher than the decrease in precision. The threshold value for the probability cutoff is not a defined, hard cutoff. We can tweak the threshold based on our use case. The best number can be studied with empirical experiments or by studying the sensitivity, specificity distribution.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/30gzSW0.</p>
			<p>The following exercise uses 0.54 instead of 0.5 as the probability cutoff to study the improvement in precision at the cost of recall.</p>
			<h3 id="_idParaDest-247"><a id="_idTextAnchor249"/>Exercise 75: Improving the XGBoost Model's Performance</h3>
			<p>We can tweak the model performance of binary classification models by adjusting the threshold value of the output. By default, we select 0.5 as the default probability cutoff. So, all responses above 0.5 are tagged as <strong class="inline">Yes</strong>, else <strong class="inline">No</strong>. Adjusting the threshold can help us achieve more sensitive or more precise models.</p>
			<p>Perform the following steps to improve the XGBoost model's performance by adjusting the threshold for the probability cutoff:</p>
			<ol>
				<li value="1">Increase the probability cutoff for the prediction on the train dataset from 0.5 to 0.53 and print the results:<p class="snippet">print("Training data results -")</p><p class="snippet">pred_train &lt;- factor(ifelse(predict(xgb,data.matrix(xgb.train),</p><p class="snippet">type="class")&gt;0.53,1,0))</p><p class="snippet">confusionMatrix(pred_train,factor(y_train),positive='1')</p><p>The output is as follows:</p><p class="snippet">[1] "Training data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction     0     1</p><p class="snippet">         0 59626  9635</p><p class="snippet">         1  1865  7921</p><p class="snippet">                                        </p><p class="snippet">               Accuracy : 0.8545        </p><p class="snippet">                 95% CI : (0.852, 0.857)</p><p class="snippet">    No Information Rate : 0.7779        </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16     </p><p class="snippet">                                        </p><p class="snippet">                  Kappa : 0.4999        </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16     </p><p class="snippet">                                        </p><p class="snippet">            Sensitivity : 0.4512        </p><p class="snippet">            Specificity : 0.9697        </p><p class="snippet">         Pos Pred Value : 0.8094        </p><p class="snippet">         Neg Pred Value : 0.8609        </p><p class="snippet">             Prevalence : 0.2221        </p><p class="snippet">         Detection Rate : 0.1002        </p><p class="snippet">   Detection Prevalence : 0.1238        </p><p class="snippet">      Balanced Accuracy : 0.7104        </p><p class="snippet">                                        </p><p class="snippet">       'Positive' Class : 1             </p></li>
				<li>Increase the probability cutoff for the prediction on the test dataset from 0.5 to 0.53 and print the results:<p class="snippet">print("Test data results -")</p><p class="snippet">pred_test &lt;- factor(ifelse(predict(xgb,data.matrix(xgb.test),</p><p class="snippet">type="class")&gt;0.53,1,0))</p><p class="snippet">confusionMatrix(pred_test,factor(y_test),positive='1')</p><p>The output is as follows:</p><p class="snippet">1] "Test data results -"</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction     0     1</p><p class="snippet">         0 25551  4210</p><p class="snippet">         1   864  3253</p><p class="snippet">                                         </p><p class="snippet">               Accuracy : 0.8502         </p><p class="snippet">                 95% CI : (0.8464, 0.854)</p><p class="snippet">    No Information Rate : 0.7797         </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      </p><p class="snippet">                                         </p><p class="snippet">                  Kappa : 0.4804         </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16      </p><p class="snippet">                                         </p><p class="snippet">            Sensitivity : 0.43588        </p><p class="snippet">            Specificity : 0.96729        </p><p class="snippet">         Pos Pred Value : 0.79014        </p><p class="snippet">         Neg Pred Value : 0.85854        </p><p class="snippet">             Prevalence : 0.22029        </p><p class="snippet">         Detection Rate : 0.09602        </p><p class="snippet">   Detection Prevalence : 0.12152        </p><p class="snippet">      Balanced Accuracy : 0.70159        </p><p class="snippet">                                         </p><p class="snippet">       'Positive' Class : 1  </p></li>
			</ol>
			<p>We see that, at 44% recall, we have 80% precision on the test dataset, and the difference in performance between the train and test datasets is also negligible. We can therefore conclude that the model performance of XGBoost is a bit better than random forest, though only a bit.</p>
			<h4>Note</h4>
			<p class="callout">You can find the complete code on GitHub: http://bit.ly/30c5DQ9.</p>
			<p>Before wrapping up our chapter, let's experiment with the last supervised technique for classification, that is, deep neural networks.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor250"/>Deep Neural Networks</h2>
			<p>The last type of technique that we will be discussing before wrapping up our chapter is deep neural networks or deep learning. This is a long and complicated topic, which by no means will we be able to do justice in a short section of this chapter. A complete book may not even suffice to cover the surface of the topic! We will explore the topic from 100 feet and quickly study an easy implementation in R. </p>
			<p>Deep neural networks, which are primarily used in the field of computer vision and natural language processing, have also found significance in machine learning use cases for regression and classification on tabular cross-sectional data. With large amounts of data, deep neural networks have been proved to be very effective at learning latent patterns and thus training models with better performance.</p>
			<h3 id="_idParaDest-249"><a id="_idTextAnchor251"/>A Deeper Look into Deep Neural Networks</h3>
			<p>Deep neural networks were inspired by the neural structure of the human brain. The field of deep learning became popular for solving computer vision problems, that is, the area of problems that were easily solved by humans, but computers struggled with for a long time. The motivation for designing deep neural networks akin to a miniature and highly simplified human brain was to solve problems that were specifically easy for humans. Later, with the success of deep learning in the field of computer vision, it was embraced in several other fields, including traditional machine learning supervised use cases.</p>
			<p>A neural network is organized as a hierarchy of neurons, just like the neurons in the human brain. Each neuron is connected to other neurons, which enables communication between them that traverses as a signal to other neurons and results in a large complex network that can learn with a feedback mechanism.</p>
			<p>The following figure demonstrates a simple neural network:</p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/C12624_05_09.jpg" alt="Figure 5.9: Simple neural network"/>
				</div>
			</div>
			<h6>Figure 5.9: Simple neural network</h6>
			<p>The input data forms the 0th layer in the network. This layer then connects to the neurons within the next layer, which is hidden. It is called <strong class="bold">hidden</strong> as the network can be perceived as a black box where we provide input to the network and directly see the output. The intermediate layers are hidden. In a neural network, a layer can have any number of neurons and each network can have any number of layers. The larger the number of layers, the 'deeper' the network will be. Hence the name deep learning and deep neural networks. Every neuron in each hidden layer computes a mathematical function, which is called the activation function in deep learning. This function helps in mimicking the signal between two neurons. If the function (activation) computes a value greater than a threshold, it sends a signal to the immediate connected neuron in the next layer. The connection between these two neurons is moderated by a weight. The weight decides how important the incoming neuron's signal is for the receiving neuron. The learning method in the deep learning model updates the weights between neurons such that the end prediction, akin to machine learning models, is the most accurate one.</p>
			<h3 id="_idParaDest-250"><a id="_idTextAnchor252"/>How Does the Deep Learning Model Work?</h3>
			<p>To understand how a neural network works and learns to make predictions on data, let's consider a simple task that is, relatively, very easy for humans. Consider the task of learning to identify different people by their faces. Most of us meet a few different people every day; say, at work, school, or on the street. Every person we meet is different from each other in some dimension. Though everyone would have a ton of similar features, such as two eyes, two ears, lips, two hands, and so on, our brain easily distinguishes between two individuals. The second time we meet a person, we would most probably recognize them and distinguish them as someone we met previously. Given the scale at which this happens and the fact that our brain effectively works to solve this mammoth problem with ease, it makes us wonder how exactly this happens.</p>
			<p>To understand this and appreciate the beauty of our brain, we need to understand how the brain fundamentally learns. The brain is a large, complex structure of interconnected neurons. Each neuron gets activated when it senses something essential and passes a message or signal to other neurons it is connected to. The connection between neurons is strengthened by constant learning from the feedback they receive. Here, when we see a new face, rather than learning the structure of the face to identify people, the brain learns how different the given face is from a generic baseline face. This can be further simplified as calculating the difference between important facial features, such as eye shape, nose, lips, ears, and lip structure, color deviations of the skin and hair, and other attributes. These differences, which are quantified by different neurons, are then orchestrated in a systematic fashion for the brain to distinguish one face from another and recall a face from memory. This entire computation happens subconsciously, and we barely realize this as the results are instant for us to notice anything specific.</p>
			<p>A neural network essentially tries to mimic the learning functionality of the brain in an extremely simplified form. Neurons are connected to each other in a layer-wise fashion and initialized with random weights. A mathematical calculation across the network combines the inputs from all neurons layer-wise and finally reaches the end outcome. The deviation of the end outcome (the predicted value) is then quantified as an error and is given as feedback to the network. Based on the error, the network tries updating the weights of the connections and tries to reduce the error in the prediction iteratively. With several iterations, the network updates its weights in an ordered fashion and thus learns to recognize patterns to make a correct prediction.</p>
			<h3 id="_idParaDest-251"><a id="_idTextAnchor253"/>What Framework Do We Use for Deep Learning Models?</h3>
			<p>For now, we will experiment with deep neural networks for our classification use case, using Keras for R. For deep learning model development, we would need to write a ton of code, which would render the building blocks for the network. To speed up our process, we can leverage Keras, a deep learning framework that provides neat abstraction for deep learning components. Keras has an R interface and works on top of a low-level deep learning framework. </p>
			<p>The deep learning frameworks available in today's AI community are either low-level or high-level. Frameworks such as TensorFlow, Theano, PyTorch, PaddlePaddle, and mxnet are low-level frameworks that provide the basic building blocks for deep learning models. Using low-level frameworks offers a ton of flexibility and customization to the end network design. However, we would still need to write quite a lot of code to get a relatively large network working. To simplify this further, there are a few high-level frameworks available that work on top of the low-level frameworks and provide a second layer of abstraction in the process of building deep learning models. Keras, Gluon, and Lasagne are a few frameworks that leverage the aforementioned low-level framework as a backend and provide a new API that makes the overall development process far easier. This reduces the flexibility when compared to directly using a low-level framework such as TensorFlow, and offers a robust solution for most networks. For our use case, we can directly leverage Keras with the R interface.</p>
			<p>Using the <strong class="inline">install.packages('keras')</strong> command would install the R interface to Keras and would also automatically install TensorFlow as the low-level backend for Keras.</p>
			<h3 id="_idParaDest-252"><a id="_idTextAnchor254"/>Building a Deep Neural Network in Keras</h3>
			<p>To leverage Keras in R, we would need additional data augmentations to our existing training dataset. In most machine learning functions available under R, we can pass the categorical column directly coded as a factor. However, we saw that XGBoost had a mandate that the data needs to be rendered into one-hot encoded form, as it does not internally transform the data into the required format. We therefore used the <strong class="inline">dummyVars</strong> function in R to transform the training and test dataset into a one-hot encoded version, such that we have only numerical data in the dataset. In Keras, we would need to feed a matrix instead of a DataFrame as the training dataset. Therefore, in addition to transforming the data into a one-hot encoded form, we would also need to convert the dataset into a matrix. </p>
			<p>Moreover, it is also recommended that we standardize, normalize, or scale all our input dimensions. The process of normalization rescales data values into the range 0 to 1. Similarly, standardization rescales data to have a mean (<em class="italics">μ</em>) of 0 and standard deviation (<em class="italics">σ</em>) of 1 (unit vari<a id="_idTextAnchor255"/>ance). This transformation is a good feature to have in machine learning, as some algorithms tend to benefit and learn better. However, in deep learning, this transformation becomes crucial, as the model learning process suffers if we provide an input training dataset such that all dimensions are in a different range or scale. The reason behind this issue is the type of activation function used in neurons. </p>
			<p>The following code snippet implements a basic neural network in Keras. Here, we use an architecture that has three layers with 250 neurons each. Finding the right architecture is an empirical process and does not have a definitive guide. The deeper network is designed, the more computation it will need to fit the data. The dataset used in the following snippet is the same as was used for XGBoost and already has the one-hot encoded forms.</p>
			<h3 id="_idParaDest-253"><a id="_idTextAnchor256"/>Exercise 76: Build a Deep Neural Network in R using R Keras</h3>
			<p>In this exercise, we will leverage deep neural networks to build a classification model for the same use case as <em class="italics">Exercise 13</em>, <em class="italics">Improving XGBoost Model Performance</em>, and try to improve the performance. Deep neural networks will not always perform better than ensemble models. They are usually a preferred choice when we have a very high number of training samples, say 10 million. However, we will experiment and check whether we can achieve any better performance than the models we built in exercises 10-13.</p>
			<p>Perform the following steps to build a deep neural network in R.</p>
			<ol>
				<li value="1">Scale the input dataset in the range 0 to 1. We would first need to initiate a <strong class="inline">preProcess</strong> object on the training data. This will be later used to scale the train as well as the test data. Neural networks perform better with scaled data. The train data alone is used for creating the object to scale:<p class="snippet">standardizer &lt;- preProcess(x_train, method='range',rangeBounds=c(0,1))</p></li>
				<li>Use the <strong class="inline">standardizer</strong> object created in the previous step to scale the train and test data:<p class="snippet">x_train_scaled &lt;- predict(standardizer, newdata=x_train)</p><p class="snippet">x_test_scaled &lt;- predict(standardizer, newdata=x_test)</p></li>
				<li>Store the number of predictor variables in a variable called <strong class="bold">predictors</strong>. We will use this information to construct the network:<p class="snippet">predictors &lt;-  dim(x_train_scaled)[2]</p></li>
				<li>Define the structure for a deep neural network. We will use the <strong class="inline">keras_model_sequential</strong> method. We will create a network with three hidden layers, having 250 neurons each and <strong class="inline">relu</strong> as the activation function. The output layer will have one neuron with the <strong class="inline">sigmoid</strong> activation function (since we are developing a binary classification mode):<p class="snippet">dl_model &lt;-  keras_model_sequential()  %&gt;% </p><p class="snippet">  layer_dense(units = 250, activation = 'relu', </p><p class="snippet">input_shape =c(predictors)) %&gt;% </p><p class="snippet">  layer_dense(units = 250, activation = 'relu' ) %&gt;% </p><p class="snippet">  layer_dense(units = 250, activation = 'relu') %&gt;% </p><p class="snippet">  layer_dense(units = 1, activation = 'sigmoid') </p></li>
				<li>Define the model optimizer as <strong class="inline">adam</strong>, loss function, and the metrics to capture for the model's training iteration:<p class="snippet">dl_model %&gt;% compile(</p><p class="snippet">  loss = 'binary_crossentropy',</p><p class="snippet">  optimizer = optimizer_adam(),</p><p class="snippet">  metrics = c('accuracy')</p><p class="snippet">)</p><p class="snippet">summary(dl_model)</p><p>The output is as follows:</p><p class="snippet">_____________________________________________________________</p><p class="snippet">Layer (type)                 Output Shape               Param #</p><p class="snippet">=============================================================</p><p class="snippet">dense_34 (Dense)             (None, 250)                16750</p><p class="snippet">_____________________________________________________________</p><p class="snippet">dense_35 (Dense)             (None, 250)                62750</p><p class="snippet">_____________________________________________________________</p><p class="snippet">dense_36 (Dense)             (None, 250)                62750</p><p class="snippet">_____________________________________________________________</p><p class="snippet">dense_37 (Dense)             (None, 1)                  251  </p><p class="snippet">=============================================================</p><p class="snippet">Total params: 142,501</p><p class="snippet">Trainable params: 142,501</p><p class="snippet">Non-trainable params: 0</p></li>
				<li>Fit the model structure we created in steps 4-5 with the training and test data from steps 1-2:<p class="snippet">history &lt;- dl_model %&gt;% fit(</p><p class="snippet">  as.matrix(x_train_scaled), as.matrix(y_train), </p><p class="snippet">  epochs = 10, batch_size = 32, </p><p class="snippet">  validation_split = 0.2</p><p class="snippet">)</p><p>The output is as follows:</p><p class="snippet">Train on 63237 samples, validate on 15810 samples</p><p class="snippet">Epoch 1/10</p><p class="snippet">63237/63237 [==============================] - 7s 104us/step – </p><p class="snippet">loss: 0.3723 - acc: 0.8388 - val_loss: 0.3639 - val_acc: 0.8426</p><p class="snippet">Epoch 2/10</p><p class="snippet">63237/63237 [==============================] - 6s 102us/step – </p><p class="snippet">loss: 0.3498 - acc: 0.8492 - val_loss: 0.3695 - val_acc: 0.8380</p><p class="snippet">Epoch 3/10</p><p class="snippet">63237/63237 [==============================] - 6s 97us/step – </p><p class="snippet">loss: 0.3434 - acc: 0.8518 - val_loss: 0.3660 - val_acc: 0.8438</p><p class="snippet">Epoch 4/10</p><p class="snippet">63237/63237 [==============================] - 6s 99us/step – </p><p class="snippet">loss: 0.3390 - acc: 0.8527 - val_loss: 0.3628 - val_acc: 0.8395</p><p class="snippet">Epoch 5/10</p><p class="snippet">63237/63237 [==============================] - 6s 97us/step – </p><p class="snippet">loss: 0.3340 - acc: 0.8551 - val_loss: 0.3556 - val_acc: 0.8440</p><p class="snippet">Epoch 6/10</p><p class="snippet">63237/63237 [==============================] - 7s 119us/step – </p><p class="snippet">loss: 0.3311 - acc: 0.8574 - val_loss: 0.3612 - val_acc: 0.8414</p><p class="snippet">Epoch 7/10</p><p class="snippet">63237/63237 [==============================] - 7s 107us/step – </p><p class="snippet">loss: 0.3266 - acc: 0.8573 - val_loss: 0.3536 - val_acc: 0.8469</p><p class="snippet">Epoch 8/10</p><p class="snippet">63237/63237 [==============================] - 7s 105us/step – </p><p class="snippet">loss: 0.3224 - acc: 0.8593 - val_loss: 0.3575 - val_acc: 0.8471</p><p class="snippet">Epoch 9/10</p><p class="snippet">63237/63237 [==============================] - 7s 105us/step – </p><p class="snippet">loss: 0.3181 - acc: 0.8607 - val_loss: 0.3755 - val_acc: 0.8444</p><p class="snippet">Epoch 10/10</p><p class="snippet">63237/63237 [==============================] - 7s 104us/step – </p><p class="snippet">loss: 0.3133 - acc: 0.8631 - val_loss: 0.3601 - val_acc: 0.8468</p></li>
				<li>Predict the responses using the fitted model on the train dataset:<p class="snippet">print("Training data results - ")</p><p class="snippet">pred_train &lt;- factor(ifelse(predict(dl_model, </p><p class="snippet">as.matrix(x_train_scaled))&gt;0.5,1,0))</p><p class="snippet">confusionMatrix(pred_train,factor(y_train),positive='1')</p><p>The output is as follows:</p><p class="snippet">"Training data results - "</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction     0     1</p><p class="snippet">         0 59281  8415</p><p class="snippet">         1  2351  9000</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8638          </p><p class="snippet">                 95% CI : (0.8614, 0.8662)</p><p class="snippet">    No Information Rate : 0.7797          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.547           </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.5168          </p><p class="snippet">            Specificity : 0.9619          </p><p class="snippet">         Pos Pred Value : 0.7929          </p><p class="snippet">         Neg Pred Value : 0.8757          </p><p class="snippet">             Prevalence : 0.2203          </p><p class="snippet">         Detection Rate : 0.1139          </p><p class="snippet">   Detection Prevalence : 0.1436          </p><p class="snippet">      Balanced Accuracy : 0.7393          </p><p class="snippet">                                          </p><p class="snippet">       'Positive' Class : 1               </p></li>
				<li>Predict the responses using the fitted model on the test dataset:<p class="snippet">#Predict on Test Data</p><p class="snippet">pred_test &lt;- factor(ifelse(predict(dl_model,   </p><p class="snippet">                           as.matrix(x_test_scaled))&gt;0.5,1,0))</p><p class="snippet">confusionMatrix(pred_test,factor(y_test),positive='1')</p><p>The output is as follows:</p><p class="snippet">"Test data results - "</p><p class="snippet">Confusion Matrix and Statistics</p><p class="snippet">          Reference</p><p class="snippet">Prediction     0     1</p><p class="snippet">         0 25028  3944</p><p class="snippet">         1  1246  3660</p><p class="snippet">                                          </p><p class="snippet">               Accuracy : 0.8468          </p><p class="snippet">                 95% CI : (0.8429, 0.8506)</p><p class="snippet">    No Information Rate : 0.7755          </p><p class="snippet">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">                  Kappa : 0.4965          </p><p class="snippet"> Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">                                          </p><p class="snippet">            Sensitivity : 0.4813          </p><p class="snippet">            Specificity : 0.9526          </p><p class="snippet">         Pos Pred Value : 0.7460          </p><p class="snippet">         Neg Pred Value : 0.8639          </p><p class="snippet">             Prevalence : 0.2245          </p><p class="snippet">         Detection Rate : 0.1080          </p><p class="snippet">   Detection Prevalence : 0.1448          </p><p class="snippet">      Balanced Accuracy : 0.7170          </p><p class="snippet">       'Positive' Class : 1               </p><h4>Note</h4><p class="callout">You can find the complete code on GitHub: http://bit.ly/2Vz8Omb.</p></li>
			</ol>
			<p>The preprocessor function helps to transform the data into the required scale or range. Here, we scale the data to a 0 to 1 scale. We should only consider using the train data as the input to the function generator and use the fitted method to scale the test data. This is essential, as we won't have access to the test data in a real-time scenario. Once the <strong class="inline">preProcess</strong> method is fit, we use it to transform the train and test data. We then define the architecture for the deep neural network model. R provides the easy to extend pipe operator with <strong class="inline">%&gt;%</strong>, which enables the easy concatenation of the operators. We design a network with three layers and 250 neurons each. The input data will form the 0th layer and the last layer will be the predicted outcome. The activation function used in the network for the hidden layers is <strong class="inline">relu</strong>, the most recommended activation function for any deep learning use case. The final layer has the <strong class="inline">sigmoid</strong> activation function, as we have a binary classification use case. There are a ton of activation functions to choose from in Keras, such as <strong class="inline">prelu</strong>, <strong class="inline">tanh</strong>, <strong class="inline">swish</strong>, and so on. Once, the model architecture is defined, we define the loss function, <strong class="inline">binary_crossentropy</strong>, which is analogous to binary <strong class="inline">logloss</strong> (akin to XGBoost), the optimizer, that is, technique, used by the model to learn and backpropagate. The errors in the prediction are backpropagated to the network so that it can adjust the weights in the right direction and iteratively reduce the error. </p>
			<p>The mathematical intuitiveness of this functionality can take various approaches. Adam optimization, which is based on adaptive estimates of lower-order moments, is the most popular choice, which we can almost blindly experiment with for most use cases in deep learning. Some of the other options are <strong class="inline">rmsprop</strong>, stochastic gradient descent, and <strong class="inline">Adagrad</strong>. We also define the metrics to calculate on the validation dataset after each epoch, that is, one complete presentation of training samples to the network. The <strong class="inline">summary</strong> function displays the resultant architecture we defined in the preceding section using the Keras constructs. The <strong class="inline">summary</strong> function gives us a brief idea of the number of parameters in each layer and additionally represents the network in a hierarchical structure to help us visualize the model architecture. Lastly, we use the <strong class="inline">fit</strong> function which trains or 'fits' the data to the network. We also define the number of epochs the model should iterate; the higher the number of epochs, the longer the training process will take to compute. </p>
			<p>The batch size indicates the number of training samples the network consumes in one single pass before updating the weights of the network; a lower number for the batch indicates more frequent weight updates and helps the RAM memory to be effectively utilized. The validation split defines the percentage of training samples to be used for validation at the end of each epoch. Finally, we validate the model's performance on the train and test data.</p>
			<h4>Note</h4>
			<p class="callout">This explanation in the code snippet will by no means be a justification for the topic. A deep neural network is an extremely vast and complex topic that might need a complete book for a basic introduction. We have wrapped the context into a short paragraph for you to understand the constructs used in the model development process. Exploring the depth of the topic would be beyond the scope of this book. </p>
			<p>Looking at the results, we can see similar results as for the previous models. The results are almost comparable with the XGBoost model we developed previously. We have around 48% recall and 75% precision on the test dataset. The results can be further tweaked to reduce recall and enhance precision (if necessary).</p>
			<p>We can therefore conclude that we got fairly good results from our simple logistic regression model, XGBoost, and the deep neural network model. The differences between all three models were relatively slight. This might bring important questions into your mind: Is it worth iterating for various models on the same use case? Which model will ideally give the best results? Though there are no straightforward answers to these questions, we can say that, overall, simple models always do great; ensemble models perform better with lots of data; and deep learning models perform better with a ton of data. In the use case that we experimented with in this chapter, we will get improved results from all the models with hyperparameter tuning and; most importantly; feature engineering. We will explore hyperparameter tuning in <em class="italics">Chapter 7</em>, <em class="italics">Model Improvements</em>, and feature engineering on a light node in <em class="italics">Chapter 6</em>, <em class="italics">Feature Selection and Dimensionality Reduction</em>. The process of feature engineering is very domain-specific and can only be generalized to a certain extent. We will have a look at this in more detail in the next chapter. The primary agenda for this chapter was to introduce the range of modeling techniques that cover a substantial area in the field and can help you build the foundations for any machine learning technique to be developed for a classification use case.</p>
			<h2 id="_idParaDest-254"><a id="_idTextAnchor257"/>Choosing the Right Model for Your Use Case</h2>
			<p>So far, we have explored a set of white-box models and a couple of black-box machine learning models for the same classification use case. We also extended the same use case with a deep neural network in Keras and studied its performance. With the results from several models and various iterations, we need to decide which model would be the best for a classification use case. There isn't a simple and straightforward answer to this. In a more general sense, we can say that the best model would be a Random Forest or XGBoost for most use cases. However, this is not true for all types of data. There will be numerous scenarios where ensemble modeling may not be the right fit and a linear model would outperform it and vice versa. In most experiments conducted by data scientists for classification use cases, the approach would be an exploratory and iterative one. There is no one-size-fits-all model in machine learning. The process of designing and training a machine learning model is arduous and extremely iterative and will always depend on the type of data used to train it. </p>
			<p>The best approach to proceed, given the task of building a supervised machine learning model, would be as follows:</p>
			<ul>
				<li><strong class="bold">Step 0</strong>: <strong class="bold">EDA, Data Treatment and Feature Engineering</strong>: Study the data extensively using a combination of visualization techniques and then treat the data for missing values, remove outliers, engineer new features, and build the train and test datasets. (If necessary, create a validation dataset too.)</li>
				<li><strong class="bold">Step 1</strong>: <strong class="bold">Start with a simple white-box model such as logistic regression</strong>: The best starting point in the modeling iterations is a simple white-box model that helps us study the impact of each predictor on the dependent variable in an easy-to-quantify way. A couple of model iterations will help with feature selection and getting a clear understanding of the best predictors and a model benchmark. </li>
				<li><strong class="bold">Step 2</strong>: <strong class="bold">Repeat the modeling experiments with a decision tree model</strong>: Leveraging decision tree models will always help us get a new perspective on the model and feature patterns. It might give us simple rules and thereby new ideas to engineer features for an improved model. </li>
				<li><strong class="bold">Step 3</strong>: If there is enough data, experiment with ensemble modeling; otherwise, try alternative approaches, such as support vector machines.<p>Ensemble modeling with Random Forest and XGBoost is almost always a safe option to experiment with. But in cases where there is a scarcity of data to train, ensemble modeling might not be an effective approach to proceed. In such cases, a black box kernel-based model would be more effective at learning data patterns and, thus, would improve model performance. We have not covered <strong class="bold">Support Vector Machines</strong> (<strong class="bold">SVM</strong>) in this chapter, given the scope. However, with the wide range of topics covered in the chapter, getting started with SVMs would be a straightforward task for you. This blog provides a simple and easy to understand guide to SVMs: https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/.</p><p>Additionally, to understand whether the number of training samples is less or more, you can use a simple rule of thumb. If there are at least 100 rows of training samples for every feature in the dataset, then there is enough data for ensemble models; if the number of samples is lower than that, then ensemble models might not always be effective. It is still worth a try, though. For example, if there are 15 features (independent variables) and 1 dependent variable, and then if we have <em class="italics">15 x 100 = 1500</em> training samples, ensemble models might have better performance on a white-box model.</p></li>
				<li><strong class="bold">Step 4</strong>: If there is more than enough data, try deep neural networks. If there are at least 10,000 samples for every feature in the dataset, experimenting with deep neural networks might be a good idea. The problem with neural networks is mainly the huge training data and large number of iterations required to get good performance. In most generic cases for classification using tabular cross-sectional data (the type of use case we solved in this book), deep neural networks are just as effective as ensemble models but require significantly more effort in training and tuning to achieve the same results. They do outperform ensemble models when there is a significantly large number of samples to train. Investing the effort in deep neural networks only returns favorable results when there is a significantly higher number of training samples.</li>
			</ul>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor258"/>Summary</h2>
			<p>In this chapter, we explored different types of classification algorithms for supervised machine learning. We leveraged the Australian weather data, designed a business problem around it, and explored various machine learning techniques on the same use case. We studied how to develop these models in R and studied the functioning of these algorithms in depth with mathematical abstractions. We summarized the results from each technique and studied a generalized approach to tackle common classification use cases. </p>
			<p>In the next chapter, we will study feature selection, dimensionality reduction, and feature engineering for machine learning models.</p>
		</div>
	</body></html>