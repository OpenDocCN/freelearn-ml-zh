<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Implementing Machine Learning Algorithms</h1>
                </header>
            
            <article>
                
<p>Learning has been a matter of study for many years. How human beings acquire new knowledge, from basic survival skills to advanced abstract subjects, is difficult to understand and reproduce in the computer world. Machines learn by comparing examples and by finding similarities in them.</p>
<p>The easiest way for a machine (and also for a human being) to learn is to simplify the problem that needs to be solved. A simplified version of reality, called a model, is useful for this task. <span>Some of the relevant issues to be studied are t</span>he minimum number of samples, underfitting and overfitting, relevant features, and how well a model can learn. Different types of target variables require different algorithms.</p>
<p>In this chapter, the following topics will be covered:</p>
<ul>
<li>Understanding learning and models</li>
<li>Focusing on model features</li>
<li>Studying machine learning models in practice</li>
<li>Evaluating models</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>There are no technical requirements for this chapter, since it is introductory. The data shown in the sections should be input into an Excel spreadsheet in order to be able to follow the examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding learning and models</h1>
                </header>
            
            <article>
                
<p>The way that humans learn has been studied for many decades now. There are a handful of psychological theories that try to explain how we acquire knowledge, use it, and generalize it in order to apply what we know to completely new scenarios. Taking one step back, we could ask ourselves: what does it mean to learn? We could say that, once we learn something, we are able to repeat it in a more or less detailed way. In reality, learning implies much more than just copying a behavior or memorizing a piece of poetry. In fact, we understand what we learn and are able to generalize that knowledge, which helps us to react correctly to new people, places, and situations.</p>
<p>The need to create a machine that somehow mimics our human behavior and intelligence has been desired for a very long time. Hundreds of years ago, kings were amazed by chess-playing machines, musical instruments that did not require a human player, and mysterious boxes that answered all kinds of questions. These, many times fake, inventions show that one of the greatest dreams of humans is to create an intelligent being, which is able to replicate easy or difficult tasks that are usually performed by people, even when intelligence is an elusive and not easily-defined thing.</p>
<p>Many years have passed, and technology has evolved in such a way that we can now create machines that <em>think</em>, or at least seem to. In fact, most of the systems that we call <em>intelligent</em> are just able to perform repetitive tasks or react to external inputs according to whatever we have showed them by way of example. As we progress through the chapter, we will see that some of the defined characteristics of human learning and intelligence are already part of modern machine learning systems and some are still the subject of science fiction novels.</p>
<p><span>By definition, m</span>achine learning means to teach a machine or an algorithm to perform tasks. We have been doing this for many years now – it is called <strong>programming</strong>. We give a computer a set of instructions, the order in which they should be executed, and a number of options of how to react to a limited number of inputs. If the input is not known, or if we ask the computer to do something that is not contained in the program, then it will fail, showing an error. The difference between this traditional paradigm and machine learning is that we will never tell the computer exactly what to do. We will either let it discover patterns or show it samples of what we want. We will use programming, of course, but just to define algorithms that <em>learn</em> in the sense that was described previously. From finding the straight line that better represents a set of points to driving a car, everything a machine can do is learned in this way.</p>
<p>As babies, we start exploring the world around us. Since we are too young to understand words or examples, we basically experience the world through our senses. We learn the difference between hard and soft, rough and smooth, hot and cold. We can call for attention when we need something, and we can even gain an understanding of the patience levels of our parents and pets. In most cases, nobody sits next to us to explain what we see, hear, feel, taste, and smell. This is an example of what we call <em>unsupervised learning</em>.</p>
<p>In unsupervised learning, the training data is "unlabeled". Without our help or intervention, the algorithm/s (or program/s) will find the required connections or unsuspected patterns in the data and learn the details and properties of the dataset.</p>
<p>Later, as we grow up, we understand words and start naming things. Our parents tell us when we see a dog or a cat, we learn our names and theirs, and we learn to identify our toys from among other children's toys (and fight over them). Without even realizing it, we relate some characteristics of objects, animals, and people to their names. These are examples of what is known as <em>supervised learning</em>. In the case of a computer, the algorithm is shown as a set of variables that are representative of the properties of the problem and then it learns how these features relate to the name of the label.</p>
<p>Science has shown us the immense complexity of the world that surrounds us. Every branch of scientific knowledge needs advanced mathematical calculations and even completely new ways of looking at data. However, the vast majority of what we can explain is only a fraction of the real world. Whenever we describe a physical phenomenon, an economic or financial event, or try to understand the behavior of individuals and groups, we rely on simplified versions of the real problem. These are called <strong>models</strong> and they make it possible for us to build a mental representation of whatever we are trying to explain. If the model is accurate enough, we will be able to <em>predict</em> some future event, or get some approximate value for a certain outcome. As you should have realized by now, this is incredibly powerful. For example, if an artillery soldier is capable of calculating, with accurate precision, where the cannonball is going to hit, then his army has a clear advantage over the enemy in battle. A <span>model is a simplified version of reality that is used to understand a problem and eventually make predictions.</span> Understanding something that your opponent ignores always represents an advantage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning by example – the linear regression model</h1>
                </header>
            
            <article>
                
<p>Imagine that you and a friend own a small ice cream shop. You are discussing how many <strong>kilograms</strong> (<strong>kg</strong>) of ice cream to produce each day and you both agree on the fact that the hotter the weather is, the more ice cream will be sold. You add that this is not the only factor to take into account, but there are other variables that can also affect the number of sales. As rational people and good analysts, you decide to run a small experiment by recording the mean temperature during the shop opening hours and the amount of ice cream that is sold. The summer turns out to be particularly rainy, and the temperature variation is high, which helps you to achieve a good range for the variables. The final dataset looks like the following table:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Mean temperature (°C)</strong></td>
<td><strong>Ice cream sold (kg)</strong></td>
</tr>
<tr>
<td>26</td>
<td>45</td>
</tr>
<tr>
<td>23</td>
<td>42.5</td>
</tr>
<tr>
<td>29</td>
<td>53.5</td>
</tr>
<tr>
<td>23</td>
<td>35.5</td>
</tr>
<tr>
<td>15</td>
<td>32.5</td>
</tr>
<tr>
<td>19</td>
<td>34.5</td>
</tr>
<tr>
<td>21</td>
<td>33.5</td>
</tr>
<tr>
<td>18</td>
<td>35</td>
</tr>
<tr>
<td>15</td>
<td>32.5</td>
</tr>
<tr>
<td>25</td>
<td>40.5</td>
</tr>
<tr>
<td>25</td>
<td>39.5</td>
</tr>
<tr>
<td>16</td>
<td>32</td>
</tr>
<tr>
<td>23</td>
<td>44.5</td>
</tr>
<tr>
<td>23</td>
<td>39.5</td>
</tr>
<tr>
<td>20</td>
<td>33</td>
</tr>
<tr>
<td>17</td>
<td>26.5</td>
</tr>
<tr>
<td>21</td>
<td>37.5</td>
</tr>
<tr>
<td>29</td>
<td>49.5</td>
</tr>
<tr>
<td>25</td>
<td>40.5</td>
</tr>
<tr>
<td>24</td>
<td>44</td>
</tr>
</tbody>
</table>
<p class="mce-root"><span>Your model states that the amount of ice cream sold is (directly) proportional to the mean temperature. In order to test this hypothesis, we can make a scatter plot of the collected data:</span></p>
<ol>
<li>Select the full range of cells containing the table, click on <span class="packt_screen">Insert menu</span>, and select <span class="packt_screen">Charts</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/02346ee9-a54d-4382-b82d-b50a275fd3f4.png" style="width:25.08em;height:8.75em;"/></p>
<p class="CDPAlignCenter CDPAlign"/>
<ol start="2">
<li><span>Now, click on</span> <span class="packt_screen">Scatter</span>, as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1ce4865a-687b-4bf5-b563-2961bf1d36ed.png" style="width:12.58em;height:17.92em;"/></p>
<p style="padding-left: 60px">After writing the names of the axis titles, you should get a chart that is similar to the following chart:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/69dca98a-1ef2-449a-860b-8c6ba69ee3e4.png" style="width:30.00em;height:19.92em;"/></p>
<p style="padding-left: 60px">We see that there is indeed a linear correlation and that it is positive (the larger the temperature value, the more ice cream you sell). We can then represent the model using a linear equation, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>IC = a * T + b</em> (1)</p>
<p style="padding-left: 60px">Here, <em>IC</em> is the amount of ice cream sold, <em>T</em> is the mean temperature, and <em>a</em> and <em>b</em> are constant values to be calculated by a linear regression.</p>
<p style="padding-left: 60px">To obtain the values of <em>a</em> and <em>b</em>, we can use Excel's <span class="packt_screen">Analysis ToolPak</span> <span>data analysis add-in</span><em>.</em> If you have not enabled it, refer link <a href="https://support.office.com/en-ie/article/use-the-analysis-toolpak-to-perform-complex-data-analysis-6c67ccf0-f4a9-487c-8dec-bdb5a2cefab6">https://support.office.com/en-ie/article/use-the-analysis-toolpak-to-perform-complex-data-analysis-6c67ccf0-f4a9-487c-8dec-bdb5a2cefab6</a> for instructions on how to do it.</p>
<ol start="3">
<li>Select the data range in your worksheet, go to <span class="packt_screen">Data</span> in the main menu and then select <span class="packt_screen">Data Analysis</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/856925f7-bf15-4cdf-8e85-5487c52cdf25.png"/></p>
<ol start="4">
<li>In the pop-up menu, select <span class="packt_screen">Regression</span> and click on <span class="packt_screen">OK</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b709d8ff-f462-4297-8e70-ac6a6a1a8b79.png" style="width:30.25em;height:14.17em;"/></p>
<ol start="5">
<li>Make sure that the <em>x</em> and <em>y</em> ranges are correct (<em>x</em> is temperature and <em>y</em> is ice cream amount). Select <span class="packt_screen">Line Fit Plots</span> to see the regression line on top of the data points in a new diagram:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1012d61f-557f-47ea-ad17-52aa327f4d46.png" style="width:29.08em;height:25.25em;"/></p>
<p>Looking at the output, we see that the line that best fits the data can be written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>IC = 1.5* T + 6</em> (2)</p>
<p>There is a standard error for <em>a</em> of <em>±0.2</em>, <span>and for <em>b</em> of <em>±4</em>. The <em>R<sup>2</sup></em> value is</span> <em>0.78</em><span>, which means that the fit is not very good and only 78% of the variation in ice cream sales can be explained by the mean temperature. So, you and your friend were both right!</span></p>
<p class="CDPAlignLeft CDPAlign">The following diagram shows the fitted line:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c7cfc8a5-d159-4fec-88c1-2daa6f6ee4c5.png" style="width:36.92em;height:27.92em;"/></p>
<p>It is clear that the line represents the data quite well, but some points are a little bit off, showing that you need to take other factors into consideration when predicting ice cream consumption. In any case, given the mean forecasted temperature for one day, you can use equation <em>(2)</em> to have a rough estimation of how much ice cream to produce to cover the possible demand.</p>
<p>Keep the rest of the linear regression results to hand, as we are going to use some of them in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Focusing on model features</h1>
                </header>
            
            <article>
                
<p>As a simplified representation of reality, a model also includes a set of variables that contain the relevant information that describes the different parts of the problem we are representing. These variables can be something as concrete as 1 kg of ice cream, as we saw in our previous example, or as abstract as a numerical value that represents how similar the meaning is of two words in a text document.</p>
<p>In the particular case of a machine learning model, these variables are called <strong>features</strong><em>.</em> Choosing significant features that provide relevant information about the phenomenon that we try to explain or predict is of paramount importance. If we consider unsupervised learning, then the relevant features are those that better represent the clustering or association of information in the dataset. For supervised learning, the most important features are those that highly correlate with the target variable – that is, the value that we want to predict or explain.</p>
<p>The quality of the insights that can be obtained from a machine learning model depends on the features used as input to the model. <strong>Feature selection</strong> and <strong>feature engineering</strong> are regularly-used techniques to improve a model's input. <span>Feature selection is the process of selecting a subset of relevant features for use in any identified model construction. It can also be termed as variable selection or attribute selection. While building any machine learning model, feature selection and data cleaning should be the first and most important steps. Feature engineering is defined as the process of using the domain knowledge of the identified data to create features that make the machine learning algorithm(s) to work. If this is done correctly, then it will increase the predictive power of machine learning algorithms by creating features from new data that is fed into this model or system.</span></p>
<p>In our previous example, the model features are the mean temperature and the amount of ice cream sold. Since we have already proved that there are more variables involved, we could add some additional features to better explain the daily ice cream consumption. For example, we could take into account which day of the week we are recording data for and include this information as another feature. Additionally, any other relevant information can be represented, more or less accurately, into a feature. In supervised learning, it is customary to call the input variables <em>features</em>, and the target or predicted variable <em>label.</em></p>
<p>Features can be numerical (such as the temperature in our previous example), or categorical (such as the day of the week). Since everything in computers is represented as numerical data, categorical data should be converted into numerical form by assigning categories to numbers. One-hot encoding is a process by which categorical variables are converted into a numerical form (or <em>encoded</em>) so that they can be input into machine learning algorithms.</p>
<p>Following our example, we could translate the day-of-the-week into day number, as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Day-of-the-week</strong></td>
<td><strong>Day number</strong></td>
</tr>
<tr>
<td>Monday</td>
<td>1</td>
</tr>
<tr>
<td>Tuesday</td>
<td>2</td>
</tr>
<tr>
<td>Wednesday</td>
<td>3</td>
</tr>
<tr>
<td>Thursday</td>
<td>4</td>
</tr>
<tr>
<td>Friday</td>
<td>5</td>
</tr>
<tr>
<td>Saturday</td>
<td>6</td>
</tr>
<tr>
<td>Sunday</td>
<td>7</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="CDPAlignLeft CDPAlign">This encoding reflects the order of the days and reserves the highest values for the weekend.</p>
<p>Let's say that you want to be more specific and predict the amount of ice cream for each flavor that you sell. For ease, let's say that you produce four different flavors: chocolate, strawberry, lemon, and vanilla. Could you just assign one number to each flavor, in the same way that you did in the day-of-the-week encoding? The answer, as we shall see, is negative. Let's try it and see what is wrong:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>Flavor</td>
<td>Flavor number</td>
</tr>
<tr>
<td>Chocolate</td>
<td>1</td>
</tr>
<tr>
<td>Strawberry</td>
<td>2</td>
</tr>
<tr>
<td>Lemon</td>
<td>3</td>
</tr>
<tr>
<td>Vanilla</td>
<td>4</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="CDPAlignLeft CDPAlign">By using this encoding, we are implicitly saying that chocolate is closer to strawberry than to vanilla (1 unit versus 3 units), which is not a real property of the flavors. The right way of translating to numbers is to create binary variables. This approach is known as one-hot encoding and looks like the following table:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Flavor</strong></td>
<td><strong>Is it chocolate?</strong></td>
<td><strong>Is it strawberry?</strong></td>
<td><strong>Is it lemon?</strong></td>
<td><strong>Is it vanilla?</strong></td>
</tr>
<tr>
<td>Chocolate</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Strawberry</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Lemon</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>Vanilla</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>This method creates some overhead, since it increases the number of features by creating one binary variable for each possible value of the original variable. On the positive side, it correctly calculates the properties of the feature. We will see some examples of this in the next chapter.</p>
<p>Depending on the type of target variable, we can classify them into <em>regression models</em> (that is, continuous target variables) or <em>classification models</em> (that is, discrete target variables). For example, if we want to predict a real number or an integer number, we use regression, whereas if we are trying to predict a tag with a finite number of options, we use classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Studying machine learning models in practice</h1>
                </header>
            
            <article>
                
<p>We have already seen a very simple example and used it to explain some basic concepts. In the next chapter, we are going to explore more complex models. We restricted ourselves to a very small dataset, just for clarity and to start our journey towards mastering machine learning with an easy task. There are some general considerations that we need to be aware of when working with machine learning models to solve real problems:</p>
<ul>
<li>The amount of data is usually very large. In fact, a larger dataset helps to get a more accurate model and a more reliable prediction. Extremely large datasets, usually called <em>big data,</em> can present storage and manipulation challenges.</li>
<li>Data is never clean and ready to use, so data cleansing is extremely important and takes a lot of time.</li>
<li>The number of features required to correctly represent a real-life problem is often large. The feature engineering techniques previously mentioned are impossible to perform by hand, so automatic methods must be devised and applied.</li>
<li>It is far more important to assess the predictive power of a combination of input features than the significance of each individual one. Some simple examples of how to select features are given in <a href="0da64bd8-0bc9-491b-875c-7ec7c35c6165.xhtml">Chapter 5</a>, <em>Correlations and the Importance of Variables</em>.</li>
<li>It is very unlikely that we will get a very good result with the first model that we apply. Testing and evaluating many different machine learning models implies repeating the same steps several times and usually requires automation as well.</li>
</ul>
<ul>
<li>The dataset should be large enough to use a percentage of the data for training purposes (usually 80%) and the rest for testing. Evaluating the accuracy of a model only on the training data is misleading. A model can be very precise at explaining and predicting the training dataset, but it can fail to generalize and deliver wrong results when presented with new, previously unseen data values.</li>
<li>Training and test data should be selected, usually at random, from the same full dataset. Trying to make a prediction based on input that lies far away from the training range is unlikely to give good results.</li>
</ul>
<p>Supervised machine learning models are usually trained using a fraction of the input data and tested on the remaining part. The model can be then used to predict the outcome when fed with new and unknown feature values, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d4756867-e08c-4225-8e33-e27a64ec0109.png" style="width:57.08em;height:40.92em;"/></p>
<p class="mce-root">A typical supervised machine learning project includes the following steps:</p>
<ol>
<li>Obtaining the data and merging different data sources (there is more on this in <a href="146c3aff-32a3-4008-8985-c1fd7db22739.xhtml">Chapter 3</a>, <em>Importing Data into Excel from Different Data Sources</em>)</li>
<li>Cleansing the data (you can refer to <a href="f93bc229-5658-466c-a7e2-ad082617bca9.xhtml">Chapter 4</a>, <em>Data Cleansing and Preliminary Data Analysis</em>)</li>
<li>Preliminary analysis and feature engineering (you can refer to <a href="0da64bd8-0bc9-491b-875c-7ec7c35c6165.xhtml">Chapter 5</a>, <em>Correlations and the Importance of Variables</em>)</li>
<li>Trying different models and parameters for each of them, and training them by using a percentage of the full dataset and using the rest for testing</li>
<li>Deploying the model so that it can be used in a continuous analysis flow and not only in small, isolated tests</li>
<li>Predicting values for new input data</li>
</ol>
<p>This procedure will become clear in the examples shown in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing underfitting and overfitting</h1>
                </header>
            
            <article>
                
<p>In the preceding list, step <em>4</em> implies an iterative process where we try models, parameters, and features until we get the best result that we can. Let's now think about a classification problem, where we want to separate squares from circles, as shown in the following diagram. At the beginning of the process, we will probably be in a situation that is similar to the first chart (on the left-hand side). The model fails to efficiently separate the two shapes and both sides are a mixture of both squares and circles. This is called <strong>underfitting</strong> and refers to a model that fails to represent the characteristics of the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bb28af32-321e-44b7-b54c-c7f06a4e2d44.jpg" style="width:31.17em;height:9.25em;"/></p>
<p>As we continue tuning parameters and adjusting the model to the training dataset, we might find ourselves in a situation that is similar to the third chart (on the right-hand side). The model accurately splits the dataset, leaving only one shape on each side of the border line. Even if this seems correct, it completely lacks generalization. The result adjusts so well to the training data that it will be completely wrong to we test it against a different dataset. This problem is called <strong>overfitting</strong>.</p>
<p><span>To solve the problem of overfitting in our model, we need to increase its adaptability. However, making it too flexible can also make it bad at predicting. To avoid this, the usual solution is to use <em>regularization</em> techniques. There are many similar techniques that can be found in specialized literature, but they are beyond the scope of this book.</span></p>
<p>The center chart shows a more flexible model; it represents the dataset, but is general enough to deal with new, previously unseen data. It is often time-consuming and it can be difficult to get the right balance in order to build a good machine learning model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating models</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">Whenever we obtain a result, it is is only as accurate as the model that represents the real problem. It is, therefore, extremely important to understand which methods can be used to evaluate the performance of our models.</p>
<p>When dealing with <em>classification</em> <em>models</em> we can use the following methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing classification accuracy</h1>
                </header>
            
            <article>
                
<p><span>This is the ratio of the number of <strong>correct predictions</strong> (<strong>CP</strong>) to the</span> total number of samples<span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e43d024f-97c4-4cb1-9dc4-e0357b2e7ef2.png" style="width:6.83em;height:2.17em;"/></p>
<p>Here, <em>CP</em> is the number of accurate or correct predictions, and <em>TP</em> is the total count of all the predictions that have been made.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the confusion matrix</h1>
                </header>
            
            <article>
                
<p><span>Let's now think about a binary classification problem. We have a set of samples belonging to two classes: <em>YES</em> or <em>NO</em>. We can build a machine learning model that outputs a class for each input set of variables. By testing our model on 200 samples, we will get the following results:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>N=200</strong></td>
<td><strong>Predicted NO</strong></td>
<td><strong>Predicted YES</strong></td>
</tr>
<tr>
<td><strong>Actual NO</strong></td>
<td>60</td>
<td>15</td>
</tr>
<tr>
<td><strong>Actual YES</strong></td>
<td>25</td>
<td>100</td>
</tr>
</tbody>
</table>
<p class="graf graf--p graf-after--figure">There are four elements to the confusion matrix:</p>
<ul class="postList">
<li class="graf graf--li graf-after--p"><strong>True positives (TP)</strong>: The number of times that the model predicts YES and the actual value is YES. In our example, this is 100 times.</li>
<li class="graf graf--li graf-after--li"><strong>True negatives (TN)</strong>: <span>The number of times that t</span>he model predicts NO and the actual value is NO. In our example, this is 60 times.</li>
<li class="graf graf--li graf-after--li"><strong>False positives (FP)</strong>: <span>The number of times that t</span>he model predicts YES and the actual value is NO. In our example, this is 15 times.</li>
<li class="graf graf--li graf-after--li"><strong>False negatives (FN)</strong>: <span>The number of times that t</span>he model predicts NO and the actual value is YES. In this example, this is 25 times.</li>
</ul>
<p>Then, we calculate the confusion matrix in the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/51beb0b2-3b62-4f8e-a087-ba9830df55b0.png" style="width:18.42em;height:2.25em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating the Area Under Curve (AUC)</h1>
                </header>
            
            <article>
                
<p><span>The AUC of a classification model is defined as the probability that the model will rank a random positive example above a random negative example.</span></p>
<p>Using the confusion matrix, we can define other quantities as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/be372292-b22f-486f-a734-9725a417db80.png" style="width:14.17em;height:2.33em;"/></p>
<p>The <strong>True Positive Rate</strong> (<strong>TPR</strong>) or sensitivity is the the <span>ratio of data points correctly predicted as positive, with respect to all the data points that have a true value of <em>YES</em></span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7de4c40c-9d05-4c8f-9334-000805b7b8ff.png" style="width:13.83em;height:2.33em;"/></p>
<p><span>The</span> <strong>False Positive Rate</strong> (<strong>FPR</strong>) <span>or</span> specificity is <span>the ratio of <em>NO</em> data points incorrectly predicted as <em>YES</em>, with respect to all <em>NO</em> data points.</span></p>
<p><span>Both quantities have values in the [0, 1] range. FPR and TPR are both computed at different threshold values and a graph is constructed. The curve is known as <strong>Receiving Operating Characteristic</strong> (<strong>ROC</strong>); AUC is the area under that curve, as shown in the following figure:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/393c0bcf-36a6-4bfa-95e3-6426e56129ea.png" style="width:29.58em;height:23.92em;"/></p>
<p>If we instead want to <span>evaluate</span> regression models, we can use the following techniques.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating the Mean Absolute Error (MAE)</h1>
                </header>
            
            <article>
                
<p><span>MAE is the mean value of the absolute difference between the real values (<em>y<sub>j</sub></em>) and the predicted values (<em>ŷ<sub>j</sub></em>). It cannot tell us the direction of the error, meaning that the prediction could be above or below the true value. If we have a total of <em>N</em> data points, we can calculate MAE as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/666440a4-f39b-4049-ba24-b597f8d7dce9.png" style="width:20.42em;height:3.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating the Mean Squared Error (MSE)</h1>
                </header>
            
            <article>
                
<p class="graf graf--p graf-after--h3">MSE takes the average of the square of the difference between the actual values and predicted values:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/de88ba04-619c-4baf-a5f4-e21fb32afdba.png" style="width:19.08em;height:3.42em;"/></p>
<p class="aspectRatioPlaceholder is-locked">No matter what evaluation method we choose, it is extremely important to take into account the business part of the problem. The optimal solution is not always to have the most accurate model, but the one that better satisfies your business needs. It may be the case that a not-so-accurate model that can be built quickly is better than a perfect one that takes a year to produce. Taking into account the dataset imbalance and business needs is important for fine-tuning the model in order to improve the confusion matrix values:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/72e10596-4454-4a1d-afbe-1be65a1a3d4e.png" style="width:44.00em;height:32.17em;"/></p>
<p>Another important factor to consider is whether we have, in the case of a classification problem, a balanced dataset. A dominant class will lead to a model that mostly predicts the same result every time. For example, a dataset with 99% <em>YES</em> labels will produce a machine learning model after training that predicts <em>YES</em> for 99% of the input (and it will be right!). There are many known techniques used to balance a dataset and find the problems in our data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we briefly discussed the learning process for machines, which, to some extent, mimics that of human beings. We described how a model, which is a simplified representation of the problem that we want to solve, can be used to apply machine learning to find a solution.</p>
<p>Using a linear regression model, we built a simple supervised predictive model and explained how to use it. <span>We then discussed the difference between regression and classification, and showed the properties of the input variables and features.</span></p>
<p>Underfitting and overfitting are two of the main concerns when training a machine learning model. We explained what they are and suggested methods to avoid them.</p>
<p><span>Finally, different types of target variables require different algorithms and evaluation methods to test the quality of the model – we discussed this in detail in the final sections.</span></p>
<p><span>In the next chapter, we are going to solve some real-life problems using machine learning and explore how some supervised and unsupervised models are built.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the main difference between classical computer programming and machine learning?</li>
<li>How are models classified, considering the type of target variable?</li>
<li>What are the different types of models, depending on how they learn?</li>
<li>What are the main steps when creating and using a machine learning model?</li>
<li>The output of the regression performed in Excel contains information about the residuals. What are they and how are they related to the MAE and MSE?</li>
<li>Explain underfitting and overfitting.</li>
<li>How can categorical features be used to feed machine learning models?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li class="graf graf--h3 graf--leading graf--title"><em>Machine Learning For Beginners</em>: <a href="https://towardsdatascience.com/machine-learning-for-beginners-d247a9420dab">https://towardsdatascience.com/machine-learning-for-beginners-d247a9420dab</a></li>
<li class="graf graf--h3 graf--leading graf--title"><em>Machine Learning basics — It's your cup of tea</em>: <a href="https://hackernoon.com/machine-learning-basics-its-your-cup-of-tea-af4baf060ace">https://hackernoon.com/machine-learning-basics-its-your-cup-of-tea-af4baf060ace</a></li>
</ul>


            </article>

            
        </section>
    </body></html>