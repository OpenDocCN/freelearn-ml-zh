- en: Creating a Machine Learning Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to summarize many of the concepts discussed in
    the book with the purpose of defining a complete machine learning architecture
    that is able to preprocess the input data, decompose/augment it, classify/cluster
    it, and eventually, show the results using graphical tools. We're also going to
    show how scikit-learn manages complex pipelines and how it's possible to fit them,
    and search for the optimal parameters in the global context of a complete architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now we have discussed single methods that could be employed to solve specific
    problems. However, in real contexts, it's very unlikely to have well-defined datasets
    that can be immediately fed into a standard classifier or clustering algorithm.
    A machine learning engineer often has to design a full architecture that a non-expert
    could consider like a black-box where the raw data enters and the outcomes are
    automatically produced. All the steps necessary to achieve the final goal must
    be correctly organized and seamlessly joined together in a processing chain similar
    to a computational graph (indeed, it's very often a direct acyclic graph). Unfortunately,
    this is a non-standard process, as every real-life problem has its own peculiarities.
    However, there are some common steps which are normally included in almost any
    ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following picture, there''s a schematic representation of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/988b11a0-9eff-4559-9150-14856f4a30c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we will briefly explain the details of each phase with some possible solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is always the most generic because it depends on each single
    context. However, before working with any data, it's necessary to collect it from
    all the sources where it's stored. The ideal situation is to have a **comma separated
    values** (**CSV**) (or another suitable format) dump that can be immediately loaded,
    but more often, the engineer has to look for all the database tables, define the
    right SQL query to collect all the pieces of information, and manage data type
    conversion and encoding. We're not going to discuss this topic, but it's important
    not to under-evaluate this stage because it can be much more difficult than expected.
    I suggest, whenever possible, to extract flattened tables, where all the fields
    are placed on the same row, because it's easier to manipulate a large amount of
    data using a DBMS or a big data tool, but it can be very time and memory consuming
    if done on a normal PC directly with Python tools. Moreover, it's important to
    use a standard character encoding for all text fields. The most common choice
    is UTF-8, but it's also possible to find DB tables encoded with other charsets
    and normally it's a good practice to convert all the documents before starting
    with the other operations. A very famous and powerful Python library for data
    manipulation is pandas (part of SciPy). It's based on the concept of DataFrame
    (an abstraction of SQL table) and implements many methods that allow the selection,
    joining, grouping, and statistical processing of datasets that can fit in memory.
    In *Heydt M., Learning pandas - Python Data Discovery and Analysis Made Easy*,
    Packt, the reader can find all the information needed to use this library to solve
    many real-life problems. A common problem that must be managed during this phase,
    is imputing the missing features. In [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml),
    *Feature Selection and Feature Engineering*, we discussed some practical methods
    that can be employed automatically before starting with the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normalizing a numeric dataset is one of the most important steps, particularly
    when different features have different scales. In [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml),
    *Feature Selection and Feature Engineering*,we discussed several methods that
    can be employed to solve this problem. Very often, it's enough to use a `StandardScaler`
    to whiten the data, but sometimes it's better to consider the impact of noisy
    features on the global trend and use a `RobustScaler` to filter them out without
    the risk of conditioning the remaining features. The reader can easily verify
    the different performances of the same classifier (in particular, SVMs and neural
    networks) when working with normalized and unnormalized datasets. As we're going
    to see in the next section, it's possible to include the normalization step in
    the processing pipeline as one of the first actions and include the `C` parameter
    in grid search in order to impose an *L1*/*L2* weight normalization during the
    training phase (see the importance of regularization in [Chapter 4](3905e421-e623-457f-bd52-13d69cf88467.xhtml),
    *Linear Regression*, when discussing about Ridge, Lasso and ElasticNet).
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This step is not always mandatory, but, in many cases, it can be a good solution
    to memory leaks or long computational times. When the dataset has many features,
    the probability of some hidden correlation is relatively high. For example, the
    final price of a product is directly influenced by the price of all materials
    and, if we remove one secondary element, the value changes slightly (more generally
    speaking, we can say that the total variance is almost preserved). If you remember
    how PCA works, you know that this process decorrelates the input data too. Therefore,
    it's useful to check whether a PCA or a Kernel PCA (for non-linear datasets) can
    remove some components while keeping the explained variance close to 100 percent
    (this is equivalent to compressing the data with minimum information loss). There
    are also other methods discussed in [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml),
    *Feature Selection and Feature Engineering* (like `NMF` or `SelectKBest`), that
    can be useful for selecting only the best features according to various criteria
    (like ANOVA or chi-squared). Testing the impact of each factor during the initial
    phases of the project can save time that can be useful when it's necessary to
    evaluate slower and more complex algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes the original dataset has only a few non-linear features and it's quite
    difficult for a standard classifier to capture the dynamics. Moreover, forcing
    an algorithm on a complex dataset can result in overfitting the model because
    all the capacity is exhausted in trying to minimize the error considering only
    the training set, and without taking into account the generalization ability.
    For this reason, it's sometimes useful to enrich the dataset with derived features
    that are obtained through functions of the existing ones. `PolynomialFeatures`
    is an example of data augmentation that can really improve the performances of
    standard algorithms and avoid overfitting. In other cases, it can be useful to
    introduce trigonometric functions (like *sin(x)* or *cos(x)*) or correlating features
    (like *x[1]x[2]*). The former allows a simpler management of radial datasets,
    while the latter can provide the classifier with information about the cross-correlation
    between two features. In general, data augmentation can be employed before trying
    a more complex algorithm; for example, a logistic regression (that is a linear
    method) can be successfully applied to augmented non-linear datasets (we saw a
    similar situation in [Chapter 4](3905e421-e623-457f-bd52-13d69cf88467.xhtml),
    *Linear Regression*, when we had discussed the polynomial regression). The choice
    to employ a more complex (with higher capacity) model or to try to augment the
    dataset is up to the engineer and must be considered carefully, taking into account
    both the pros and the cons. In many cases, for example, it's preferable not to
    modify the original dataset (which could be quite large), but to create a scikit-learn
    interface to augment the data in real time. In other cases, a neural model can
    provide faster and more accurate results without the need for data augmentation.
    Together with parameter selection, this is more of an art than a real science,
    and the experiments are the only way to gather useful knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Data conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This step is probably the simplest and, at the same time, the most important
    when handling categorical data. We have discussed several methods to encode labels
    using numerical vectors and it's not necessary to repeat the concepts already
    explained. A general rule concerns the usage of integer or binary values (one-hot
    encoding). The latter is probably the best choice when the output of the classifier
    is the value itself, because, as discussed in [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml),
    *Feature Selection and Feature Engineering*, it's much more robust to noise and
    prediction errors. On the other hand, one-hot encoding is quite memory-consuming.
    Therefore, whenever it's necessary to work with probability distributions (like
    in NLP), an integer label (representing a dictionary entry or a frequency/count
    value) can be much more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling/Grid search/Cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modeling implies the choice of the classification/clustering algorithm that
    best suits every specific task. We have discussed different methods and the reader
    should be able to understand when a set of algorithms is a reasonable candidate,
    and when it's better to look for another strategy. However, the success of a machine
    learning technique often depends on the right choice of each parameter involved
    in the model as well. As already discussed, when talking about data augmentation,
    it's very difficult to find a precise method to determine the optimal values to
    assign, and the best approach is always based on a grid search. scikit-learn provides
    a very flexible mechanism to investigate the performance of a model with different
    parameter combinations, together with cross-validation (that allows a robust validation
    without reducing the number of training samples), and this is indeed a more reasonable
    approach, even for experts engineers. Moreover, when performing different transformations,
    the effect of a choice can impact the whole pipeline, and, therefore, (we're going
    to see a few examples in the next section) I always suggest for application of
    the grid search to all components at the same time, to be able to evaluate the
    cross-influence of each possible choice.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, it's useful/necessary to visualize the results of intermediate and
    final steps. In this book, we have always shown plots and diagrams using matplotlib,
    which is part of SciPy and provides a flexible and powerful graphics infrastructure.
    Even if it's not part of the book, the reader can easily modify the code in order
    to get different results; for a deeper understanding, refer to Mcgreggor D., *Mastering
    matplotlib*, Packt. As this is an evolving sector, many new projects are being
    developed, offering new and more stylish plotting functions. One of them is Bokeh
    ([http://bokeh.pydata.org](http://bokeh.pydata.org)), that works using some JavaScript
    code to create interactive graphs that can be embedded into web pages too.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn tools for machine learning architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we're going to present two very important scikit-learn classes that can
    help the machine learning engineer to create complex processing structures including
    all the steps needed to generate the desired outcomes from the raw datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'scikit-learn provides a flexible mechanism for creating pipelines made up of
    subsequent processing steps. This is possible thanks to a standard interface implemented
    by the majority of classes therefore most of the components (both data processors/transformers
    and classifiers/clustering tools) can be exchanged seamlessly. The class `Pipeline`
    accepts a single parameter `steps`, which is a list of tuples in the form (name
    of the component—instance), and creates a complex object with the standard fit/transform
    interface. For example, if we need to apply a PCA, a standard scaling, and then
    we want to classify using a SVM, we could create a pipeline in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: At this point, the pipeline can be fitted like a single classifier (using the
    standard methods `fit()` and `fit_transform()`), even if the the input samples
    are first passed to the `PCA` instance, the reduced dataset is normalized by the
    `StandardScaler` instance, and finally, the resulting samples are passed to the
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pipeline is also very useful together with `GridSearchCV`, to evaluate different
    combinations of parameters, not limited to a single step but considering the whole
    process. Considering the previous example, we can create a dummy dataset and try
    to find the optimal parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset is quite redundant. Therefore, we need to find the optimal number
    of components for PCA and the best kernel for the SVM. When working with a pipeline,
    the name of the parameter must be specified using the component ID followed by
    a double underscore and then the actual name, for example, `classifier__kernel`
    (if you want to check all the acceptable parameters with the right name, it''s
    enough to execute: `print(pipeline.get_params().keys())`). Therefore, we can perform
    a grid search with the following parameter dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the best estimator (which is a complete pipeline) has 15 principal
    components (that means they are uncorrelated) and a radial-basis function SVM
    with a relatively high `gamma` value (0.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding score is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s also possible to use a `Pipeline` together with `GridSearchCV` to evaluate
    different combinations. For example, it can be useful to compare some decomposition
    methods, mixed with various classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to compare **principal component analysis** (**PCA**), **non-negative
    matrix factorization** (**NMF**), and k-best feature selection based on the ANOVA
    criterion, together with logistic regression and kernelized SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Performing a grid search, we get the pipeline made up of PCA with 20 components
    (the original dataset 64 features) and an RBF SVM with a very small `gamma` value
    (0.05) and  a medium (5.0) *L2* penalty parameter `C` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Considering the need to capture small details in the digit representations,
    these values are an optimal choice. The score for this pipeline is indeed very
    high:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Feature unions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another interesting class provided by scikit-learn is `FeatureUnion`, which
    allows concatenating different feature transformations into a single output matrix.
    The main difference with a pipeline (which can also include a feature union) is
    that the pipeline selects from alternative scenarios, while a feature union creates
    a unified dataset where different preprocessing outcomes are joined together.
    For example, considering the previous results, we could try to optimize our dataset
    by performing a PCA with 10 components joined with the selection of the best 5
    features chosen according to the ANOVA metric. In this way, the dimensionality
    is reduced to 15 instead of 20:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We already know that a RBF SVM is a good choice, and, therefore, we keep the
    remaining part of the architecture without modifications. Performing a cross-validation,
    we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The score is slightly lower than before (< 0.002) but the number of features
    has been considerably reduced and therefore also the computational time. Joining
    the outputs of different data preprocessors is a form of data augmentation and
    it must always be taken into account when the original number of features is too
    high or redundant/noisy and a single decomposition method doesn't succeed in capturing
    all the dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mcgreggor D., *Mastering matplotlib*, Packt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heydt M., *Learning pandas - Python Data Discovery and Analysis Made Easy*,
    Packt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, we discussed the main elements of machine learning architecture,
    considering some common scenarios and the procedures that are normally employed
    to prevent issues and improve the global performance. None of these steps should
    be discarded without a careful evaluation because the success of a model is determined
    by the joint action of many parameter, and hyperparameters, and finding the optimal
    final configuration starts with considering all possible preprocessing steps.
  prefs: []
  type: TYPE_NORMAL
- en: We saw that a grid search is a powerful investigation tool and that it's often
    a good idea to use it together with a complete set of alternative pipelines (with
    or without feature unions), so as to find the best solution in the context of
    a global scenario. Modern personal computers are fast enough to test hundreds
    of combinations in a few hours, and when the datasets are too large, it's possible
    to provision a cloud server using one of the existing providers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I'd like to repeat that till now (also considering the research in
    the deep learning field), creating an up-and-running machine learning architecture
    needs a continuous analysis of alternative solutions and configurations, and there's
    no silver bullet for any but the simplest cases. This is a science that still
    keeps an artistic heart!
  prefs: []
  type: TYPE_NORMAL
