- en: Creating a Machine Learning Architecture
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建机器学习架构
- en: In this chapter, we're going to summarize many of the concepts discussed in
    the book with the purpose of defining a complete machine learning architecture
    that is able to preprocess the input data, decompose/augment it, classify/cluster
    it, and eventually, show the results using graphical tools. We're also going to
    show how scikit-learn manages complex pipelines and how it's possible to fit them,
    and search for the optimal parameters in the global context of a complete architecture.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将总结书中讨论的许多概念，目的是定义一个完整的机器学习架构，该架构能够预处理输入数据，分解/增强它，分类/聚类它，并最终使用图形工具展示结果。我们还将展示scikit-learn如何管理复杂的流程，以及如何在完整架构的全局范围内拟合它们并搜索最佳参数。
- en: Machine learning architectures
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习架构
- en: Until now we have discussed single methods that could be employed to solve specific
    problems. However, in real contexts, it's very unlikely to have well-defined datasets
    that can be immediately fed into a standard classifier or clustering algorithm.
    A machine learning engineer often has to design a full architecture that a non-expert
    could consider like a black-box where the raw data enters and the outcomes are
    automatically produced. All the steps necessary to achieve the final goal must
    be correctly organized and seamlessly joined together in a processing chain similar
    to a computational graph (indeed, it's very often a direct acyclic graph). Unfortunately,
    this is a non-standard process, as every real-life problem has its own peculiarities.
    However, there are some common steps which are normally included in almost any
    ML pipeline.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了可以用来解决特定问题的单一方法。然而，在现实情况下，很难有定义明确的可以立即输入到标准分类器或聚类算法中的数据集。机器学习工程师通常必须设计一个完整的架构，非专家可能会将其视为一个黑盒，其中原始数据进入，结果自动产生。实现最终目标所需的所有步骤都必须正确组织，并在类似于计算图的处理链中无缝连接（实际上，它通常是一个直接无环图）。不幸的是，这是一个非标准过程，因为每个现实生活中的问题都有其独特的特性。然而，有一些常见的步骤通常包含在几乎任何机器学习流程中。
- en: 'In the following picture, there''s a schematic representation of this process:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图片中，展示了该过程的示意图：
- en: '![](img/988b11a0-9eff-4559-9150-14856f4a30c5.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/988b11a0-9eff-4559-9150-14856f4a30c5.png)'
- en: Now we will briefly explain the details of each phase with some possible solutions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将简要解释每个阶段的细节以及一些可能的解决方案。
- en: Data collection
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集
- en: The first step is always the most generic because it depends on each single
    context. However, before working with any data, it's necessary to collect it from
    all the sources where it's stored. The ideal situation is to have a **comma separated
    values** (**CSV**) (or another suitable format) dump that can be immediately loaded,
    but more often, the engineer has to look for all the database tables, define the
    right SQL query to collect all the pieces of information, and manage data type
    conversion and encoding. We're not going to discuss this topic, but it's important
    not to under-evaluate this stage because it can be much more difficult than expected.
    I suggest, whenever possible, to extract flattened tables, where all the fields
    are placed on the same row, because it's easier to manipulate a large amount of
    data using a DBMS or a big data tool, but it can be very time and memory consuming
    if done on a normal PC directly with Python tools. Moreover, it's important to
    use a standard character encoding for all text fields. The most common choice
    is UTF-8, but it's also possible to find DB tables encoded with other charsets
    and normally it's a good practice to convert all the documents before starting
    with the other operations. A very famous and powerful Python library for data
    manipulation is pandas (part of SciPy). It's based on the concept of DataFrame
    (an abstraction of SQL table) and implements many methods that allow the selection,
    joining, grouping, and statistical processing of datasets that can fit in memory.
    In *Heydt M., Learning pandas - Python Data Discovery and Analysis Made Easy*,
    Packt, the reader can find all the information needed to use this library to solve
    many real-life problems. A common problem that must be managed during this phase,
    is imputing the missing features. In [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml),
    *Feature Selection and Feature Engineering*, we discussed some practical methods
    that can be employed automatically before starting with the following steps.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步总是最通用的，因为它依赖于每个单独的上下文。然而，在处理任何数据之前，有必要从所有存储数据的地方收集它。理想的情况是有一个**逗号分隔值**（**CSV**）或其他合适的格式文件，可以立即加载，但更常见的情况是工程师必须查找所有的数据库表，定义正确的SQL查询来收集所有信息片段，并管理数据类型转换和编码。我们不会讨论这个话题，但重要的是不要低估这个阶段，因为它可能比预期的要困难得多。我建议，在可能的情况下，提取扁平化的表格，其中所有字段都放在同一行上，因为使用数据库管理系统或大数据工具处理大量数据更容易，但如果直接在普通的PC上使用Python工具进行，可能会非常耗费时间和内存。此外，对于所有文本字段，使用标准的字符编码也很重要。最常见的选择是UTF-8，但也可以找到使用其他字符集编码的数据库表，通常在开始其他操作之前将所有文档转换为标准编码是一个好的做法。一个非常著名且功能强大的Python数据操作库是pandas（SciPy的一部分）。它基于DataFrame的概念（SQL表的抽象）并实现了许多方法，允许选择、连接、分组和统计处理适合内存的数据集。在*Heydt
    M.，《学习pandas - Python数据发现与分析简化》*，Packt这本书中，读者可以找到使用这个库解决许多实际问题所需的所有信息。在这个阶段必须管理的常见问题之一是填充缺失的特征。在[第3章](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml)，“特征选择与特征工程”中，我们讨论了一些可以在开始下一步之前自动采用的实际方法。
- en: Normalization
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化
- en: Normalizing a numeric dataset is one of the most important steps, particularly
    when different features have different scales. In [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml),
    *Feature Selection and Feature Engineering*,we discussed several methods that
    can be employed to solve this problem. Very often, it's enough to use a `StandardScaler`
    to whiten the data, but sometimes it's better to consider the impact of noisy
    features on the global trend and use a `RobustScaler` to filter them out without
    the risk of conditioning the remaining features. The reader can easily verify
    the different performances of the same classifier (in particular, SVMs and neural
    networks) when working with normalized and unnormalized datasets. As we're going
    to see in the next section, it's possible to include the normalization step in
    the processing pipeline as one of the first actions and include the `C` parameter
    in grid search in order to impose an *L1*/*L2* weight normalization during the
    training phase (see the importance of regularization in [Chapter 4](3905e421-e623-457f-bd52-13d69cf88467.xhtml),
    *Linear Regression*, when discussing about Ridge, Lasso and ElasticNet).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对数值数据集进行归一化是其中一个最重要的步骤，尤其是当不同的特征具有不同的尺度时。在[第3章](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml)，*特征选择与特征工程*中，我们讨论了几种可以用来解决这个问题的方法。通常情况下，使用`StandardScaler`来白化数据就足够了，但有时考虑噪声特征对全局趋势的影响，并使用`RobustScaler`来过滤它们，而不必担心会条件化剩余的特征会更好。读者可以很容易地验证在处理归一化和未归一化的数据集时，相同的分类器（特别是SVM和神经网络）的不同性能。正如我们将在下一节中看到的，将归一化步骤包含在处理管道中作为第一个动作之一，并在网格搜索中包含`C`参数，以便在训练阶段强制执行*L1*/*L2*权重归一化（参见[第4章](3905e421-e623-457f-bd52-13d69cf88467.xhtml)，*线性回归*中讨论正则化的重要性，当讨论岭回归、Lasso和ElasticNet时）。
- en: Dimensionality reduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度降低
- en: This step is not always mandatory, but, in many cases, it can be a good solution
    to memory leaks or long computational times. When the dataset has many features,
    the probability of some hidden correlation is relatively high. For example, the
    final price of a product is directly influenced by the price of all materials
    and, if we remove one secondary element, the value changes slightly (more generally
    speaking, we can say that the total variance is almost preserved). If you remember
    how PCA works, you know that this process decorrelates the input data too. Therefore,
    it's useful to check whether a PCA or a Kernel PCA (for non-linear datasets) can
    remove some components while keeping the explained variance close to 100 percent
    (this is equivalent to compressing the data with minimum information loss). There
    are also other methods discussed in [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml),
    *Feature Selection and Feature Engineering* (like `NMF` or `SelectKBest`), that
    can be useful for selecting only the best features according to various criteria
    (like ANOVA or chi-squared). Testing the impact of each factor during the initial
    phases of the project can save time that can be useful when it's necessary to
    evaluate slower and more complex algorithms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤并非总是必需的，但在许多情况下，它可以是一个解决内存泄漏或长时间计算的好方法。当数据集具有许多特征时，某些隐藏相关性的概率相对较高。例如，产品的最终价格直接受所有材料价格的影响，如果我们移除一个次要元素，其价值会略有变化（更普遍地说，我们可以说总方差几乎保持不变）。如果你记得PCA是如何工作的，你就会知道这个过程也会使输入数据去相关。因此，检查PCA或核PCA（用于非线性数据集）是否可以去除一些组件，同时保持解释方差接近100%（这相当于以最小信息损失压缩数据）是有用的。在[第3章](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml)，*特征选择与特征工程*中，也讨论了其他方法（如`NMF`或`SelectKBest`），这些方法可以根据各种标准（如ANOVA或卡方检验）选择最佳特征。在项目的初始阶段测试每个因素的影响可以节省时间，这在需要评估较慢且更复杂的算法时可能是有用的。
- en: Data augmentation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: Sometimes the original dataset has only a few non-linear features and it's quite
    difficult for a standard classifier to capture the dynamics. Moreover, forcing
    an algorithm on a complex dataset can result in overfitting the model because
    all the capacity is exhausted in trying to minimize the error considering only
    the training set, and without taking into account the generalization ability.
    For this reason, it's sometimes useful to enrich the dataset with derived features
    that are obtained through functions of the existing ones. `PolynomialFeatures`
    is an example of data augmentation that can really improve the performances of
    standard algorithms and avoid overfitting. In other cases, it can be useful to
    introduce trigonometric functions (like *sin(x)* or *cos(x)*) or correlating features
    (like *x[1]x[2]*). The former allows a simpler management of radial datasets,
    while the latter can provide the classifier with information about the cross-correlation
    between two features. In general, data augmentation can be employed before trying
    a more complex algorithm; for example, a logistic regression (that is a linear
    method) can be successfully applied to augmented non-linear datasets (we saw a
    similar situation in [Chapter 4](3905e421-e623-457f-bd52-13d69cf88467.xhtml),
    *Linear Regression*, when we had discussed the polynomial regression). The choice
    to employ a more complex (with higher capacity) model or to try to augment the
    dataset is up to the engineer and must be considered carefully, taking into account
    both the pros and the cons. In many cases, for example, it's preferable not to
    modify the original dataset (which could be quite large), but to create a scikit-learn
    interface to augment the data in real time. In other cases, a neural model can
    provide faster and more accurate results without the need for data augmentation.
    Together with parameter selection, this is more of an art than a real science,
    and the experiments are the only way to gather useful knowledge.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有时原始数据集只有少数非线性特征，对于标准分类器来说捕捉动态变化相当困难。此外，将算法强加于复杂数据集可能会导致模型过拟合，因为所有能力都耗尽在尝试最小化仅考虑训练集的错误上，而没有考虑到泛化能力。因此，有时通过现有特征的函数获得派生特征来丰富数据集是有用的。`PolynomialFeatures`
    是数据增强的一个例子，它可以真正提高标准算法的性能并避免过拟合。在其他情况下，引入三角函数（如 *sin(x)* 或 *cos(x)*）或相关特征（如 *x[1]x[2]*）可能很有用。前者允许更简单地管理径向数据集，而后者可以为分类器提供关于两个特征之间交叉相关性的信息。通常，数据增强可以在尝试更复杂的算法之前使用；例如，逻辑回归（这是一种线性方法）可以成功地应用于增强的非线性数据集（我们在[第4章](3905e421-e623-457f-bd52-13d69cf88467.xhtml)，*线性回归*，讨论多项式回归时看到了类似的情况）。选择使用更复杂（具有更高容量）的模型或尝试增强数据集取决于工程师，并且必须仔细考虑，权衡利弊。在许多情况下，例如，最好是不要修改原始数据集（这可能相当大），而是创建一个scikit-learn接口以实时增强数据。在其他情况下，神经网络模型可以提供更快、更准确的结果，而无需数据增强。与参数选择一样，这更多的是一种艺术而不是真正的科学，实验是唯一收集有用知识的方法。
- en: Data conversion
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换
- en: This step is probably the simplest and, at the same time, the most important
    when handling categorical data. We have discussed several methods to encode labels
    using numerical vectors and it's not necessary to repeat the concepts already
    explained. A general rule concerns the usage of integer or binary values (one-hot
    encoding). The latter is probably the best choice when the output of the classifier
    is the value itself, because, as discussed in [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml),
    *Feature Selection and Feature Engineering*, it's much more robust to noise and
    prediction errors. On the other hand, one-hot encoding is quite memory-consuming.
    Therefore, whenever it's necessary to work with probability distributions (like
    in NLP), an integer label (representing a dictionary entry or a frequency/count
    value) can be much more efficient.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理分类数据时，这一步可能是最简单同时也是最重要的。我们已经讨论了几种使用数值向量编码标签的方法，没有必要重复已经解释的概念。一个一般规则是关于使用整数或二进制值（one-hot编码）。当分类器的输出是值本身时，后者可能是最佳选择，因为，如[第3章](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml)，*特征选择和特征工程*中讨论的那样，它对噪声和预测误差的鲁棒性更强。另一方面，one-hot编码相当消耗内存。因此，当需要处理概率分布（如NLP中）时，整数标签（代表字典条目或频率/计数值）可能更有效。
- en: Modeling/Grid search/Cross-validation
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型/网格搜索/交叉验证
- en: Modeling implies the choice of the classification/clustering algorithm that
    best suits every specific task. We have discussed different methods and the reader
    should be able to understand when a set of algorithms is a reasonable candidate,
    and when it's better to look for another strategy. However, the success of a machine
    learning technique often depends on the right choice of each parameter involved
    in the model as well. As already discussed, when talking about data augmentation,
    it's very difficult to find a precise method to determine the optimal values to
    assign, and the best approach is always based on a grid search. scikit-learn provides
    a very flexible mechanism to investigate the performance of a model with different
    parameter combinations, together with cross-validation (that allows a robust validation
    without reducing the number of training samples), and this is indeed a more reasonable
    approach, even for experts engineers. Moreover, when performing different transformations,
    the effect of a choice can impact the whole pipeline, and, therefore, (we're going
    to see a few examples in the next section) I always suggest for application of
    the grid search to all components at the same time, to be able to evaluate the
    cross-influence of each possible choice.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 建模意味着选择最适合每个特定任务的分类/聚类算法。我们已经讨论了不同的方法，读者应该能够理解一组算法何时是一个合理的候选者，何时最好寻找另一种策略。然而，机器学习技术的成功往往还取决于模型中每个参数的正确选择。正如已经讨论过的，当谈到数据增强时，很难找到一个精确的方法来确定要分配的最佳值，最佳方法始终基于网格搜索。scikit-learn提供了一个非常灵活的机制来调查不同参数组合的模型性能，以及交叉验证（这允许在没有减少训练样本数量的情况下进行稳健的验证），这确实是一个更合理的做法，即使是专家工程师也是如此。此外，当执行不同的转换时，选择的影响可能会影响整个流程，因此，（我们将在下一节中看到一些例子）我总是建议同时应用网格搜索到所有组件，以便能够评估每个可能选择的交叉影响。
- en: Visualization
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化
- en: Sometimes, it's useful/necessary to visualize the results of intermediate and
    final steps. In this book, we have always shown plots and diagrams using matplotlib,
    which is part of SciPy and provides a flexible and powerful graphics infrastructure.
    Even if it's not part of the book, the reader can easily modify the code in order
    to get different results; for a deeper understanding, refer to Mcgreggor D., *Mastering
    matplotlib*, Packt. As this is an evolving sector, many new projects are being
    developed, offering new and more stylish plotting functions. One of them is Bokeh
    ([http://bokeh.pydata.org](http://bokeh.pydata.org)), that works using some JavaScript
    code to create interactive graphs that can be embedded into web pages too.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '有时，可视化中间步骤和最终步骤的结果是有用/必要的。在这本书中，我们始终使用matplotlib来展示图表和图解，matplotlib是SciPy的一部分，提供了一个灵活且强大的图形基础设施。即使它不是本书的一部分，读者也可以轻松修改代码以获得不同的结果；为了更深入的理解，请参阅Mcgreggor
    D.的《精通matplotlib》，Packt出版社。由于这是一个不断发展的领域，许多新的项目正在开发中，提供了新的、更时尚的绘图功能。其中之一是Bokeh（[http://bokeh.pydata.org](http://bokeh.pydata.org)），它使用一些JavaScript代码创建可以嵌入到网页中的交互式图表。 '
- en: scikit-learn tools for machine learning architectures
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: scikit-learn工具用于机器学习架构
- en: Now we're going to present two very important scikit-learn classes that can
    help the machine learning engineer to create complex processing structures including
    all the steps needed to generate the desired outcomes from the raw datasets.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将介绍两个非常重要的scikit-learn类，这些类可以帮助机器学习工程师创建复杂的数据处理结构，包括从原始数据集中生成所需结果所需的所有步骤。
- en: Pipelines
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道
- en: 'scikit-learn provides a flexible mechanism for creating pipelines made up of
    subsequent processing steps. This is possible thanks to a standard interface implemented
    by the majority of classes therefore most of the components (both data processors/transformers
    and classifiers/clustering tools) can be exchanged seamlessly. The class `Pipeline`
    accepts a single parameter `steps`, which is a list of tuples in the form (name
    of the component—instance), and creates a complex object with the standard fit/transform
    interface. For example, if we need to apply a PCA, a standard scaling, and then
    we want to classify using a SVM, we could create a pipeline in the following way:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn提供了一个灵活的机制来创建由后续处理步骤组成的管道。这是由于大多数类实现了标准接口，因此大多数组件（包括数据处理/转换器和分类器/聚类工具）可以无缝交换。类`Pipeline`接受一个参数`steps`，它是一个元组列表，形式为（组件名称—实例），并创建一个具有标准fit/transform接口的复杂对象。例如，如果我们需要应用PCA、标准缩放，然后我们想使用SVM进行分类，我们可以按以下方式创建一个管道：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: At this point, the pipeline can be fitted like a single classifier (using the
    standard methods `fit()` and `fit_transform()`), even if the the input samples
    are first passed to the `PCA` instance, the reduced dataset is normalized by the
    `StandardScaler` instance, and finally, the resulting samples are passed to the
    classifier.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，管道可以像单个分类器一样拟合（使用标准的`fit()`和`fit_transform()`方法），即使输入样本首先通过`PCA`实例，通过`StandardScaler`实例对减少的数据集进行归一化，最后，将得到的样本传递给分类器。
- en: 'A pipeline is also very useful together with `GridSearchCV`, to evaluate different
    combinations of parameters, not limited to a single step but considering the whole
    process. Considering the previous example, we can create a dummy dataset and try
    to find the optimal parameters:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 管道与`GridSearchCV`一起也非常有用，可以评估不同参数组合的不同组合，不仅限于单个步骤，而是考虑整个流程。考虑到前面的例子，我们可以创建一个虚拟数据集并尝试找到最佳参数：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The dataset is quite redundant. Therefore, we need to find the optimal number
    of components for PCA and the best kernel for the SVM. When working with a pipeline,
    the name of the parameter must be specified using the component ID followed by
    a double underscore and then the actual name, for example, `classifier__kernel`
    (if you want to check all the acceptable parameters with the right name, it''s
    enough to execute: `print(pipeline.get_params().keys())`). Therefore, we can perform
    a grid search with the following parameter dictionary:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集相当冗余。因此，我们需要找到PCA的最佳组件数量和SVM的最佳核。当使用管道工作时，必须使用组件ID后跟两个下划线，然后是实际名称来指定参数的名称，例如`classifier__kernel`（如果您想检查所有具有正确名称的可接受参数，只需执行：`print(pipeline.get_params().keys())`）。因此，我们可以执行以下参数字典的网格搜索：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As expected, the best estimator (which is a complete pipeline) has 15 principal
    components (that means they are uncorrelated) and a radial-basis function SVM
    with a relatively high `gamma` value (0.2):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，最佳估计量（这是一个完整的管道）有15个主成分（这意味着它们是不相关的）和一个具有相对较高`gamma`值（0.2）的径向基函数SVM：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The corresponding score is:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的分数是：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It''s also possible to use a `Pipeline` together with `GridSearchCV` to evaluate
    different combinations. For example, it can be useful to compare some decomposition
    methods, mixed with various classifiers:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用`Pipeline`与`GridSearchCV`一起评估不同的组合。例如，比较一些分解方法与各种分类器混合可能很有用：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We want to compare **principal component analysis** (**PCA**), **non-negative
    matrix factorization** (**NMF**), and k-best feature selection based on the ANOVA
    criterion, together with logistic regression and kernelized SVM:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想比较**主成分分析**（**PCA**）、**非负矩阵分解**（**NMF**）和基于ANOVA准则的k最佳特征选择，以及逻辑回归和核化SVM：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Performing a grid search, we get the pipeline made up of PCA with 20 components
    (the original dataset 64 features) and an RBF SVM with a very small `gamma` value
    (0.05) and  a medium (5.0) *L2* penalty parameter `C` :'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 执行网格搜索，我们得到由PCA（原始数据集有64个特征）和具有非常小的`gamma`值（0.05）和中等（5.0）*L2*惩罚参数`C`的RBF SVM组成的管道：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Considering the need to capture small details in the digit representations,
    these values are an optimal choice. The score for this pipeline is indeed very
    high:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到需要捕捉数字表示中的小细节，这些值是最佳选择。这个管道的分数确实非常高：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Feature unions
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征组合
- en: 'Another interesting class provided by scikit-learn is `FeatureUnion`, which
    allows concatenating different feature transformations into a single output matrix.
    The main difference with a pipeline (which can also include a feature union) is
    that the pipeline selects from alternative scenarios, while a feature union creates
    a unified dataset where different preprocessing outcomes are joined together.
    For example, considering the previous results, we could try to optimize our dataset
    by performing a PCA with 10 components joined with the selection of the best 5
    features chosen according to the ANOVA metric. In this way, the dimensionality
    is reduced to 15 instead of 20:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供的另一个有趣类别是 `FeatureUnion`，它允许将不同的特征转换连接到一个单一的输出矩阵中。与管道（也可以包括特征联合）的主要区别在于，管道从不同的场景中选择，而特征联合创建了一个统一的数据集，其中不同的预处理结果被合并在一起。例如，考虑到之前的结果，我们可以尝试通过执行具有
    10 个组件的 PCA 并选择根据 ANOVA 指标选择的最佳 5 个特征来优化我们的数据集。这样，维度从 20 减少到 15：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We already know that a RBF SVM is a good choice, and, therefore, we keep the
    remaining part of the architecture without modifications. Performing a cross-validation,
    we get:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道径向基函数支持向量机（RBF SVM）是一个不错的选择，因此我们保持架构的其余部分不变。进行交叉验证后，我们得到：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The score is slightly lower than before (< 0.002) but the number of features
    has been considerably reduced and therefore also the computational time. Joining
    the outputs of different data preprocessors is a form of data augmentation and
    it must always be taken into account when the original number of features is too
    high or redundant/noisy and a single decomposition method doesn't succeed in capturing
    all the dynamics.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 得分略低于之前（< 0.002），但特征数量已显著减少，因此计算时间也有所减少。将不同数据预处理器的输出合并是一种数据增强形式，当原始特征数量过高或冗余/噪声时，必须始终考虑这一点，因为单一分解方法可能无法成功捕捉所有动态。
- en: References
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Mcgreggor D., *Mastering matplotlib*, Packt
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mcgreggor D.，《精通 matplotlib》，Packt
- en: Heydt M., *Learning pandas - Python Data Discovery and Analysis Made Easy*,
    Packt
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heydt M.，《学习 pandas - Python 数据发现与分析变得简单》，Packt
- en: Summary
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this final chapter, we discussed the main elements of machine learning architecture,
    considering some common scenarios and the procedures that are normally employed
    to prevent issues and improve the global performance. None of these steps should
    be discarded without a careful evaluation because the success of a model is determined
    by the joint action of many parameter, and hyperparameters, and finding the optimal
    final configuration starts with considering all possible preprocessing steps.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们讨论了机器学习架构的主要元素，考虑了一些常见的场景和通常用于防止问题和提高整体性能的程序。在仔细评估之前，不应丢弃任何这些步骤，因为模型的成功取决于许多参数和超参数的联合作用，而找到最佳最终配置的起点是考虑所有可能的预处理步骤。
- en: We saw that a grid search is a powerful investigation tool and that it's often
    a good idea to use it together with a complete set of alternative pipelines (with
    or without feature unions), so as to find the best solution in the context of
    a global scenario. Modern personal computers are fast enough to test hundreds
    of combinations in a few hours, and when the datasets are too large, it's possible
    to provision a cloud server using one of the existing providers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到网格搜索是一个强大的调查工具，并且通常与一套完整的替代管道（包括或不包括特征联合）一起使用是一个好主意，以便在全局场景中找到最佳解决方案。现代个人计算机足够快，可以在几小时内测试数百种组合，而当数据集太大时，可以使用现有的提供商之一提供云服务器。
- en: Finally, I'd like to repeat that till now (also considering the research in
    the deep learning field), creating an up-and-running machine learning architecture
    needs a continuous analysis of alternative solutions and configurations, and there's
    no silver bullet for any but the simplest cases. This is a science that still
    keeps an artistic heart!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想再次强调，到目前为止（也考虑到深度学习领域的研究），创建一个运行良好的机器学习架构需要持续分析替代解决方案和配置，而对于任何但最简单的情况，都没有一劳永逸的解决方案。这是一门仍然保持着艺术之心的科学！
