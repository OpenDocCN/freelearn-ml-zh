["```py\nTransformers library, specifically the T5Tokenizer and T5ForConditionalGeneration classes.\n```", "```py\n    pip install transformers\n    ```", "```py\n!pip install transformers\nfrom transformers import T5Tokenizer,\nT5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\nmodel =\nT5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\ninput_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```", "```py\ninput_text = \"Who is the prime minister of India?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```", "```py\n<pad> narendra modi</s>\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n# Define a simple  generative model\nclass SampleGenModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super(SampleGenModel, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, latent_dim * 2)\n# Two times latent_dim for mean and log-variance\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    def forward(self, x):\n        x = self.encoder(x)\n        mu, log_var = x[:, :latent_dim], x[:, latent_dim:]\n        z = self.reparameterize(mu, log_var)\n        reconstructed = self.decoder(z)\n        return reconstructed, mu, log_var\n# Generate synthetic data for demonstration\nnum_samples = 1000\ndata_dim = 20\ndata = torch.tensor(np.random.randint(2, size=(num_samples, data_dim)), dtype=torch.float32)\nprint(data)\n# Initialize the SampleGenModel\ninput_dim = data_dim\nhidden_dim = 64\nlatent_dim = 16\nvae = SampleGenModel(input_dim, hidden_dim, latent_dim)\n# Define an adversary model (a simple feedforward neural network)\nclass Adversary(nn.Module):\n    def __init__(self, input_dim):\n        super(Adversary, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return self.fc(x)\n# Train the SampleGenModel\n# Train the adversary model\nadversary = Adversary(latent_dim)\noptimizer = optim.Adam(adversary.parameters(), lr=0.001)\ncriterion = nn.BCELoss()\n# Prepare target data for the membership inference attack\ntarget_data_point = torch.tensor(np.random.randint(2, size=data_dim), dtype=torch.float32)\n# Membership inference attack function\ndef membership_inference_attack(vae, adversary, target_data_point):\n# Encode the target data point using the VAE\n    with torch.no_grad():\n        target_data_point = target_data_point.unsqueeze(0)  # Add batch dimension\n        reconstructed, mu, log_var = vae(target_data_point)\n# Use the adversary to predict membership\n    prediction = adversary(mu)\n# If the prediction is close to 1, the target data point is likely a member\n    if prediction.item() > 0.5:\n        return \"Member\"\n    else:\n        return \"Non-Member\"\n# Perform the membership inference attack\nresult = membership_inference_attack(vae, adversary, target_data_point)\n# Output the result\nprint(\"Membership Inference Result:\", result)\n```", "```py\ntensor([[0., 0., 1.,  ..., 1., 0., 1.],\n        [0., 1., 1.,  ..., 0., 0., 1.],\n        [1., 0., 1.,  ..., 1., 0., 1.],\n        ...,\n        [0., 0., 0.,  ..., 1., 0., 0.],\n        [0., 1., 0.,  ..., 0., 1., 1.],\n        [1., 0., 1.,  ..., 0., 0., 0.]])\nMembership Inference Result: Member\n```", "```py\n!pip install torch\n!pip install transformers\nimport torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n# Load a pretrained GPT-2 model and tokenizer\nmodel_name = \"gpt2\"\n# You can choose other pretrained models as well\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\n# Generate text samples from the model\nprompt = \"Once upon a time\"\nnum_samples = 5\nfor _ in range(num_samples):\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n    output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    print(\"Generated Text:\\n\", generated_text)\n    print(\"=\"*80)\n```", "```py\nclass SimpleModel:\n    def __init__(self):\n        self.data={\n            'Unique ID':'123-45-6789',\n            'email':'example@example.com',\n            'password':'mypassword'\n        }\n    def generate_text(self,prompt):\n        return self.data.get(prompt,'Sorry,I don\\'t have the data')\nmodel=SimpleModel()\n## Normal Request\nprint(model.generate_text('favorite_color'))\n## Malicious request , simulating an attempt to a prompt injection attack\nprint(model.generate_text('Unique ID'))\n```", "```py\nSorry,I don't have the data\n123-45-6789\n```", "```py\n!pip install textattack\n```", "```py\nimport pandas as pd\nimport os\nfrom transformers import BertTokenizer, BertModel\nfrom torch import nn\nimport torch\nimport math\nimport textattack\nimport random\n#from train_bert import Model\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n#torch.cuda.is_available = lambda : False\ntextattack.shared.utils.device = \"cpu\"\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n        #self.bert_model.parallelize()\n        self.drop = torch.nn.Dropout(p=0.1)\n        self.l1 = torch.nn.Linear(768,2)\n    def forward(self, text):\n        tokenized_text = tokenizer(text , max_length=512, truncation=True, return_tensors='pt').input_ids#.to('cuda:3')\n        text_rep = self.drop(self.bert_model(tokenized_text).pooler_output)\n        out = self.l1(text_rep)\n        print(out)\n        return out.squeeze().tolist()\nmodel = Model()\nmodel.load_state_dict(torch.load('bert-base-uncased'))\nmodel = model.to('cpu')\nmodel.eval()\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nclass CustomWrapper(textattack.models.wrappers.ModelWrapper):\n    def __init__(self, model):\n        self.model = model#.to('cuda:3')\n        self.model.eval()\n    def __call__(self, list_of_texts):\n        results = []\n        self.model.requires_grad = False\n        for text in list_of_texts:\n          results.append(self.model(text))\n        return results\nclass_model = CustomWrapper(model)\nfrom textattack.datasets import Dataset\nfrom textattack.attack_recipes.textfooler_jin_2019 import TextFoolerJin2019\nfrom textattack import Attacker, AttackArgs\nattack = TextFoolerJin2019.build(class_model)\nattack#.cuda_()\ndataset = [\n    [\"This film is a masterpiece! The story is incredibly moving, and the performances are outstanding. It's a true classic.\", 1],\n    [\"The Godfather is a cinematic gem. The storytelling and performances are top-notch. A true classic in every sense.\", 1],\n    [\"The Emoji Movie is a complete disappointment. The plot is weak, and it feels like one big advertisement. A waste of time.\", 0],\n    [\"Mind-bending and visually stunning! Inception keeps you guessing from start to finish. Christopher Nolan at his best.\", 1],\n    [\"Twilight is a guilty pleasure for some, but the acting and dialogue are cringe-worthy. Not a cinematic masterpiece.\", 0],\n    [\"Forrest Gump is a heartwarming journey through history. Tom Hanks delivers an unforgettable performance.\", 1],\n    [\"Explosions and CGI can't make up for the lackluster story in Transformers: The Last Knight. Disappointing.\", 0],\n    [\"The Dark Knight is a dark and gripping superhero film. Heath Ledger's Joker is iconic. A must-see.\", 1],\n    [\"Avatar is visually breathtaking, but the story is somewhat predictable. Still, it's a cinematic experience.\", 1],\n    [\"The Room is so bad that it's almost good. The unintentional humor makes it a cult classic.\", 1]\n]\nrandom.shuffle(dataset)\nattacker = Attacker(attack, textattack.datasets.Dataset(dataset[:10]), AttackArgs(num_examples=10))\nattacker.attack_dataset()\n```", "```py\n+-------------------------------+--------+\n| Attack Results                |        |\n+-------------------------------+--------+\n| Number of successful attacks: | 1      |\n| Number of failed attacks:     | 2      |\n| Number of skipped attacks:    | 7      |\n| Original accuracy:            | 30.0%  |\n| Accuracy under attack:        | 20.0%  |\n| Attack success rate:          | 33.33% |\n| Average perturbed word %:     | 40.91% |\n| Average num. words per input: | 17.3   |\n| Avg num queries:              | 213.33 |\n+-------------------------------+--------+\n```", "```py\nclass ClassificationModel(nn.Module):\n    def __init__(self, model, pos_prompt, neg_prompt):\n        super(ClassificationModel, self).__init__()\n        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n        self.model = GPT2LMHeadModel.from_pretrained(model)\n        self.model.eval()\n        self.pos_prompt = pos_prompt\n        self.neg_prompt = neg_prompt\n    def score(self, prompt, sentence, model):\n        tokenized_prompt = self.tokenizer.encode(prompt , max_length=1024, truncation=True, return_tensors='pt').to('cpu')\n        tokenized_all = self.tokenizer.encode(prompt + ' ' + sentence, max_length=1024, truncation=True, return_tensors='pt').to('cpu')\n        loss1=model(tokenized_all, labels=tokenized_all).loss\n        loss2 = model(tokenized_prompt, labels=tokenized_prompt).loss*len(tokenized_prompt[0])/len(tokenized_all[0])\n        loss = loss1-loss2\n        return math.exp(loss)\n    def forward(self, sentence):\n        pos = 0\n        neg = 0\n        for prompt in self.pos_prompt:\n             pos += self.score(prompt, sentence, self.model)#.cpu()\n        for prompt in self.neg_prompt:\n             neg += self.score(prompt, sentence, self.model)#.cpu()\n        result = torch.FloatTensor([5000-neg/10.0e+52, 5000-pos/10.0e+52])\n        result = torch.softmax(result, 0)\n        if abs(result[0].item()+result[1].item()-1) >= 1e-6:\n            print('detected something')\n            result = torch.FloatTensor([1,0])\n        return torch.softmax(result, 0)\nmodel = ClassificationModel('gpt2', ['Positive:'], ['Negative:'])\nclass_model = CustomWrapper(model)\nattacker = Attacker(attack, textattack.datasets.Dataset(dataset[:10]), AttackArgs(num_examples=10))\n```", "```py\n-------------------------------+-------+\n| Attack Results                |       |\n+-------------------------------+-------+\n| Number of successful attacks: | 0     |\n| Number of failed attacks:     | 3     |\n| Number of skipped attacks:    | 7     |\n| Original accuracy:            | 30.0% |\n| Accuracy under attack:        | 30.0% |\n| Attack success rate:          | 0.0%  |\n| Average perturbed word %:     | nan%  |\n| Average num. words per input: | 17.3  |\n| Avg num queries:              | 250.0 |\n+-------------------------------+-------+\n```", "```py\n!pip install transformers\n!pip install git+https://github.com/lxuechen/private-transformers.git\n!pip install tqdm\nfrom tqdm import tqdm\nimport transformers\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom private_transformers import PrivacyEngine\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, texts, labels, eos_token):\n       self.texts = texts\n       self.y = labels\n       self.eos_token = eos_token\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, index):\n        text = self.texts[index] + ' ' + self.eos_token\n        label = self.y[index]\n        return text, label\ndef get_data_from_txt(path: str):\n    texts = []\n    labels = []\n    with open(path, 'r') as f:\n        for line in f:\n            texts.append(' '.join(line.split(' ')[1:]).replace('\\n', ''))\n            labels.append(int(line.split(' ')[0]))\n    return texts, labels\ndef forward_step(texts,tokenizer, model):\n    tokenized_texts = tokenizer(texts, truncation=True, max_length=500, return_tensors='pt', padding=True).input_ids.to('cpu')\n    lm_loss = model(tokenized_texts, labels=tokenized_texts).loss.unsqueeze(dim=0)\n    return lm_loss\ndef train_llm(train_data, train_loader, ):\n    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n    #model.parallelize()\n    model.train()\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n    tokenizer.pad_token = tokenizer.eos_token\n    optimizer = torch.optim.Adam(model.parameters(),lr = 8e-6)\n    args_epochs=2\n    print(args_epochs)\n    epsilon=0.5\n    privacy_engine = PrivacyEngine(\n            model,\n            batch_size=1,\n            sample_size=10,\n            epochs=args_epochs,\n            max_grad_norm=0.1,\n            target_epsilon=epsilon,\n        )\n    privacy_engine.attach(optimizer)\n    for epoch in range(args_epochs):\n        total_loss = 0\n        for texts, labels in tqdm(train_loader):\n            lm_loss = forward_step(texts,tokenizer, model)\n            optimizer.step(loss=lm_loss)\n            total_loss += lm_loss.item()\n    return model\ntrain_texts, train_labels = get_data_from_txt('imdb_train.txt')\ntrain_texts = train_texts[0:100]\ntrain_labels =train_labels[0:100]\ntrain_data = Dataset(train_texts, train_labels, '<|endoftext|>')\ntrain_loader = torch.utils.data.DataLoader(train_data, shuffle=False, batch_size=1)\npmodel = train_llm(train_data,train_loader)\nprint(pmodel)\nOutput of this program as follows:\n2\n0.5\ntraining epoch 0\n```"]