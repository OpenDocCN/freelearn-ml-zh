<html><head></head><body>
		<div id="_idContainer115">
			<h1 id="_idParaDest-118" class="chapter-number"><a id="_idTextAnchor185"/>6</h1>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor186"/>Data Imbalance in Deep Learning</h1>
			<p>Class imbalanced data is a common issue for deep learning models. When one or more classes have significantly fewer samples, the performance of deep learning models can suffer as they tend to prioritize learning from the majority class, resulting in poor generalization for the <span class="No-Break">minority class(es).</span></p>
			<p>A lot of real-world data is imbalanced, which presents challenges to deep learning classification tasks. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em> shows some common categories of imbalanced data problems in various deep <span class="No-Break">learning applications:</span></p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B17259_06_01.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Some common categories of imbalanced data problems</p>
			<p>We will cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>A brief introduction to <span class="No-Break">deep learning</span></li>
				<li>Data imbalance in <span class="No-Break">deep learning</span></li>
				<li>Overview of deep learning techniques to handle <span class="No-Break">data imbalance</span></li>
				<li><span class="No-Break">Multi-label classification</span></li>
			</ul>
			<p>By the end of this chapter, we’ll have a foundational understanding of deep learning and neural networks. We’ll have also grasped the impact of data imbalance on these models and gained a high-level overview of various strategies to address the challenges of <span class="No-Break">imbalanced data.</span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor187"/>Technical requirements</h1>
			<p>In this chapter, we will utilize common libraries such as <strong class="source-inline">numpy</strong>, <strong class="source-inline">scikit-learn</strong>, and <strong class="source-inline">PyTorch</strong>. <strong class="source-inline">PyTorch</strong> is an open source machine learning library that’s used for deep learning tasks and has grown in popularity recently because of its flexibility and ease <span class="No-Break">of use.</span></p>
			<p>You can install <strong class="source-inline">PyTorch</strong> using <strong class="source-inline">pip</strong> or <strong class="source-inline">conda</strong>. Visit the official PyTorch website (<a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a>) to get the appropriate command for your <span class="No-Break">system configuration.</span></p>
			<p>The code and notebooks for this chapter are available on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06"><span class="No-Break">https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor188"/>A brief introduction to deep learning</h1>
			<p>Deep learning is a subfield of machine learning that focuses on artificial neural networks with multiple <a id="_idIndexMarker393"/>layers (deep models typically have three or more layers, including input, output, and hidden layers). These models have demonstrated remarkable capabilities in various applications, including image and speech recognition, natural language processing, and <span class="No-Break">autonomous driving.</span></p>
			<p>The prevalence of “big data” (large volumes of structured or unstructured data, often challenging to manage with traditional data processing software) problems greatly benefited from <a id="_idIndexMarker394"/>the development of <strong class="bold">Graphical Processing Units</strong> (<strong class="bold">GPUs</strong>), which were initially designed for <span class="No-Break">graphics processing.</span></p>
			<p>In this section, we will provide a concise introduction to the foundational elements of deep learning, discussing <a id="_idIndexMarker395"/>only what is necessary for the problems associated with data imbalance in deep learning. For a more in-depth introduction, we recommend referring to a more dedicated book on deep learning (please refer to the resources listed as [1] and [2] in the <span class="No-Break"><em class="italic">References</em></span><span class="No-Break"> section).</span></p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor189"/>Neural networks</h2>
			<p>Neural networks are the foundation of deep learning. Inspired by the structure and function of <a id="_idIndexMarker396"/>the human brain, neural networks consist of interconnected nodes or artificial neurons organized <span class="No-Break">in layers.</span></p>
			<p>The core data <a id="_idIndexMarker397"/>structure in <strong class="source-inline">PyTorch</strong> is tensors. Tensors are multi-dimensional arrays, similar to NumPy arrays, but with GPU acceleration support and capabilities for automatic differentiation. Tensors can be created, manipulated, and operated using <strong class="source-inline">PyTorch</strong> functions, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
import torch
# Create a tensor with specific data
t1 = torch.tensor([[1, 2], [3, 4]])
# Create a 2x3 tensor filled with zeros
t2 = torch.zeros(2, 3)
# Create a 2x3 tensor filled with random values
t3 = torch.randn(2, 3)</pre>			<p>Mixing CUDA tensors with CPU-bound tensors will lead <span class="No-Break">to errors:</span></p>
			<pre class="source-code">
# moving the tensor to GPU (assuming a GPU with CUDA support is available)
x = torch.rand(2,2).to("cuda") 
y = torch.rand(2,2) # this tensor remains on the CPU
x+y     # adding a GPU tensor &amp; a CPU tensor will lead to an error</pre>			<p>Here’s <span class="No-Break">the output:</span></p>
			<pre class="source-code">
--------------------------------------------------------------------------
RuntimeError Traceback (most recent call last)
----&gt; 1 x+y
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</pre>			<p><strong class="source-inline">autograd</strong> is a powerful <a id="_idIndexMarker398"/>feature in <strong class="source-inline">PyTorch</strong> that allows automatic <a id="_idIndexMarker399"/>differentiation for tensor operations. This is particularly useful for backpropagation (discussed later) in <span class="No-Break">neural networks:</span></p>
			<pre class="source-code">
# Create a tensor with autograd enabled
x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)
# Perform operations on the tensor
y = x + 2
z = y * y * 3
# Compute gradients with respect to the input tensor x
z.backward(torch.ones_like(x))
# Display the gradients
print(x.grad)</pre>			<p>We will see the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
tensor([[18., 24.],
        [30., 36.]])</pre>			<p>Let’s review how this code works. It creates a PyTorch tensor, <strong class="source-inline">x</strong>, performs some operations to compute <strong class="source-inline">y</strong> and <strong class="source-inline">z</strong>, and then computes the gradient of <strong class="source-inline">z</strong> concerning <strong class="source-inline">x</strong> using the <strong class="source-inline">backward()</strong> method. The gradients are stored in <strong class="source-inline">x.grad</strong>. The operations are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">y = x + 2</strong> increases each element in <strong class="source-inline">x</strong> <span class="No-Break">by 2</span></li>
				<li><strong class="source-inline">z = y * y * 3</strong> squares each element in <strong class="source-inline">y</strong> and then multiplies <span class="No-Break">by 3</span></li>
			</ul>
			<p>The gradient <a id="_idIndexMarker400"/>calculation, in this case, involves applying the chain rule to these operations to compute <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dz</span><span class="_-----MathTools-_Math_Variable"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Base">_</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">dx</span></span><span class="No-Break">:</span></p>
			<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dz</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dx</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dz</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dy</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dy</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dx</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<p>Ok, let’s calculate <a id="_idIndexMarker401"/>each piece in <span class="No-Break">this equation:</span></p>
			<ol>
				<li>First, let’s compute <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dz</span><span class="_-----MathTools-_Math_Variable"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Base">_</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">dy</span></span><span class="No-Break">:</span><p class="list-inset"><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dz</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dy</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">3</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">6</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">6</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p></li>
				<li><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dy</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dx</span><span class="_-----MathTools-_Math_Space"> </span>= 1 because <em class="italic">y</em> is a linear function <span class="No-Break">of </span><span class="No-Break"><em class="italic">x</em></span><span class="No-Break">:</span></li>
				<li>Finally, let’s compute <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dz</span><span class="_-----MathTools-_Math_Variable"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Base">_</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">dx</span></span><span class="No-Break">:</span><p class="list-inset"><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dz</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dx</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dz</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dy</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dy</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">dx</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">6</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">6</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">)</span></span></p></li>
			</ol>
			<p>Given <strong class="source-inline">x = [[1., 2.], [3., 4.]]</strong>, the output when we print <strong class="source-inline">x.grad</strong> should be <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
tensor([[18., 24.],
        [30., 36.]])</pre>			<p>The output tensor corresponds to the evaluated gradients of <strong class="source-inline">z</strong> concerning <strong class="source-inline">x</strong> at the specific values <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">x</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor190"/>Perceptron</h2>
			<p>The perceptron is <a id="_idIndexMarker402"/>the most basic unit of a neural network. It is a simple, linear classifier that takes a <a id="_idIndexMarker403"/>set of input values, multiplies them by their corresponding weights, adds a bias term, sums the results, and applies an activation function to produce <span class="No-Break">an output:</span></p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B17259_06_02.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – A simple perceptron</p>
			<p>So, what’s an activation <span class="No-Break">function, then?</span></p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor191"/>Activation functions</h2>
			<p>Activation functions <a id="_idIndexMarker404"/>introduce non-linearity into neural <a id="_idIndexMarker405"/>networks and help determine the output of a neuron. Common <a id="_idIndexMarker406"/>activation functions include sigmoid, tanh, <strong class="bold">Rectified Linear Unit</strong> (<strong class="bold">ReLU</strong>), and softmax. These functions enable the network to learn complex, non-linear patterns in <span class="No-Break">the data.</span></p>
			<p>Let’s get into the various components of an artificial <span class="No-Break">neural network.</span></p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor192"/>Layers</h2>
			<p>A neural <a id="_idIndexMarker407"/>network typically consists of an input layer, one or more <a id="_idIndexMarker408"/>hidden (intermediate) layers, and an output layer. The input layer receives the raw data, while the hidden layers perform various transformations, and the output layer produces the <span class="No-Break">final result.</span></p>
			<p>The number of layers <a id="_idIndexMarker409"/>in a neural network is called the <strong class="bold">depth</strong> of the network, while the <a id="_idIndexMarker410"/>number of neurons in each layer is called the <strong class="bold">width</strong> of <span class="No-Break">the network.</span></p>
			<p>Counterintuitively, deeper and wider neural networks, though they have more capacity to learn complex <a id="_idIndexMarker411"/>patterns and representations in the training data, are not <a id="_idIndexMarker412"/>necessarily more robust to imbalanced datasets than shallower and <span class="No-Break">narrower networks.</span></p>
			<p>Deeper and wider networks are more prone to overfitting, especially in the context of imbalanced datasets, because large networks can memorize the patterns of the majority class(es), which can hamper the performance of <span class="No-Break">minority class(es).</span></p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor193"/>Feedforward neural networks</h2>
			<p>A feedforward neural network is a kind of neural network that has a unidirectional flow of information, starting from the input layer, progressing through any hidden layers, and ending at the <span class="No-Break">output layer.</span></p>
			<p>There are no <a id="_idIndexMarker413"/>feedback loops or connections between layers that cycle back to previous layers, hence the name feedforward. These <a id="_idIndexMarker414"/>networks are widely used for tasks such as image classification, regression, <span class="No-Break">and others.</span></p>
			<p><strong class="source-inline">PyTorch</strong> provides the <strong class="source-inline">nn</strong> module for creating and training neural networks – the <strong class="source-inline">nn.Module</strong> class is the base class for all neural network modules. The following code snippet defines a simple feedforward <span class="No-Break">neural network:</span></p>
			<pre class="source-code">
import torch
import torch.optim as optim
class Net(torch.nn.Module):
     def __init__(self):
           super(Net, self).__init__()
           self.fc1 = torch.nn.Linear(4, 10)
           self.fc2 = torch.nn.Linear(10, 3)
     def forward (self, x):
           x = torch.relu(self.fc1(x))
           x = self.fc2(x)
           return x
# Create an instance of the network
net = Net()
# Define a loss function and an optimizer
criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)</pre>			<p><strong class="bold">Loss functions</strong> quantify the <a id="_idIndexMarker415"/>difference between the predicted output and <a id="_idIndexMarker416"/>target values. Common loss functions include <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>), cross-entropy, and <span class="No-Break">hinge loss.</span></p>
			<p>The most <a id="_idIndexMarker417"/>commonly used loss function, <strong class="source-inline">CrossEntropyLoss</strong>, which we used in the previous snippet, tends to favor the majority <a id="_idIndexMarker418"/>class examples in imbalanced datasets. This occurs because the majority class examples significantly outnumber those of the minority class. As a result, the loss function becomes biased toward the majority class and fails to account for the error in the minority <span class="No-Break">classes adequately.</span></p>
			<p>We will learn about several loss function modifications more suited to class imbalance problems in <a href="B17259_08.xhtml#_idTextAnchor235"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Algorithm-Level Deep </em><span class="No-Break"><em class="italic">Learning Techniques</em></span><span class="No-Break">.</span></p>
			<h3>MultiLayer Perceptron</h3>
			<p>A MultiLayer Perceptron (MLP) is <a id="_idIndexMarker419"/>a feedforward neural network consisting of an input layer, one or more hidden layers, and an output layer. Each layer is fully connected, meaning every neuron in one layer is connected to all neurons in the next layer. MLPs can be used for various tasks, including classification, regression, and feature extraction. They are particularly suited for problems where the input data can be represented as a fixed-size vector, such as tabular data or flattened images. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em> shows a multilayer perceptron with two input nodes, two hidden nodes, and one output node, using <a id="_idIndexMarker420"/>the specified weights w1 <span class="No-Break">to w6:</span></p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B17259_06_03.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – A multilayer perceptron</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor194"/>Training neural networks</h2>
			<p>Training neural <a id="_idIndexMarker421"/>networks involves finding the optimal weights <a id="_idIndexMarker422"/>and biases that minimize a loss function. Two essential algorithms are used in this process – gradient descent <span class="No-Break">and backpropagation:</span></p>
			<ul>
				<li><strong class="bold">Gradient descent</strong>: Gradient <a id="_idIndexMarker423"/>descent is an optimization algorithm that minimizes the loss function by iteratively <a id="_idIndexMarker424"/>updating the weights <span class="No-Break">and biases.</span></li>
				<li><strong class="bold">Backpropagation</strong>: Backpropagation is a critical algorithm for training neural networks. It computes <a id="_idIndexMarker425"/>the gradient of <a id="_idIndexMarker426"/>the loss function concerning each weight using the chain rule (a method for finding the derivative of composite functions), efficiently calculating the gradients from the output layer back to the <span class="No-Break">input layer.</span></li>
			</ul>
			<p>Training a neural network involves iterating through a dataset, feeding the data into the network, computing the loss, and updating the weights using backpropagation, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
# Example dataset
inputs = torch.randn(100, 4)
targets = torch.randint(0, 3, (100,))
# Train for 100 epochs
for epoch in range(100):
     # Forward pass
     outputs = net(inputs)
     # Compute loss
     loss = criterion(outputs, targets)
     # Backward pass and optimization
     # reset gradients of model parameters
     optimizer.zero_grad()
     # compute the gradient of loss w.r.t model parameters
     loss.backward()
     # update model parameters based on computed gradients
     optimizer.step()
     # Print the loss for this epoch
     print(f"Epoch {epoch + 1}, Loss: {loss.item()}")</pre>			<p>This code <a id="_idIndexMarker427"/>produces the following output, which shows the <a id="_idIndexMarker428"/>loss for each epoch (note that the loss goes down as the <span class="No-Break">training progresses):</span></p>
			<pre class="source-code">
Epoch 1, Loss: 1.106634497642517
Epoch 2, Loss: 1.1064313650131226
…
Epoch 100, Loss: 1.026018738746643</pre>			<p>In the <a id="_idIndexMarker429"/>previous code snippet, the line containing <strong class="source-inline">loss.backward()</strong> maps to the backpropagation process. The <strong class="source-inline">optimizer.zero_grad()</strong> and <strong class="source-inline">optimizer.step()</strong> lines both represent one step of the <a id="_idIndexMarker430"/>gradient descent process. <strong class="source-inline">optimizer.zero_grad()</strong> clears old gradients from the last step (otherwise, they will accumulate), while <strong class="source-inline">optimizer.step()</strong> performs the actual update of the parameters (weights and biases) in the direction that reduces the loss <span class="No-Break">the most.</span></p>
			<p>The following flowchart depicts the training logic <span class="No-Break">in PyTorch:</span></p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B17259_06_04.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – PyTorch training logic flowchart</p>
			<p><strong class="bold">Overfitting</strong> is a common <a id="_idIndexMarker431"/>problem in machine learning, where a model <a id="_idIndexMarker432"/>performs <a id="_idIndexMarker433"/>well on the training data but fails to generalize to <span class="No-Break">unseen data.</span></p>
			<p><strong class="bold">Underfitting</strong> happens when <a id="_idIndexMarker434"/>the model is too simple (think about when we used a linear regression model when we needed a decision tree regressor model) and does not capture the underlying patterns in <span class="No-Break">the data.</span></p>
			<p>Both overfitting and underfitting lead to poor performance on <span class="No-Break">unseen data.</span></p>
			<h3>Regularization</h3>
			<p>Regularization <a id="_idIndexMarker435"/>techniques help mitigate <a id="_idIndexMarker436"/>overfitting by adding additional constraints the model must meet beyond merely optimizing the loss function. If L is the loss function, the most commonly used types of regularization are L1 and L2, which add a penalty term to the loss function, discouraging overly <span class="No-Break">complex </span><span class="No-Break">models:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">-</span><span class="_-----MathTools-_Math_Variable">loss</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">w</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">|</span></span></p>
			<p><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Operator">-</span><span class="_-----MathTools-_Math_Variable">loss</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
			<p>Here, <span class="_-----MathTools-_Math_Variable">λ</span> is the regularization strength and <span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> are the model <span class="No-Break">parameters (weights).</span></p>
			<p>There are a few other regularization techniques <span class="No-Break">as well:</span></p>
			<ul>
				<li><strong class="bold">Dropout</strong>: Dropout <a id="_idIndexMarker437"/>is another regularization <a id="_idIndexMarker438"/>technique to prevent overfitting in neural networks. During training, dropout randomly “drops” or deactivates a fraction of neurons in a layer, preventing them from contributing to the output. This forces the network to learn more robust and generalized features as it cannot rely on any single neuron. In <strong class="source-inline">PyTorch</strong>, the <strong class="source-inline">torch.nn.Dropout</strong> class implements dropout and takes the dropout rate (the probability of the neuron being zeroed) as <span class="No-Break">a parameter.</span></li>
				<li><strong class="bold">Batch normalization</strong>: Batch <a id="_idIndexMarker439"/>normalization (implemented in PyTorch using <strong class="source-inline">torch.nn.BatchNorm1d</strong> and <strong class="source-inline">torch.nn.BatchNorm2d</strong>) is a technique that’s used <a id="_idIndexMarker440"/>to improve the training of deep neural networks. It normalizes the inputs to each layer by adjusting their mean and standard deviation, which helps stabilize and accelerate the training process, allowing the use of higher learning rates and reducing sensitivity to <span class="No-Break">weight initialization.</span></li>
				<li><strong class="bold">Early stopping</strong>: Early stopping is a technique that’s used to prevent overfitting during the <a id="_idIndexMarker441"/>training of neural networks. It involves monitoring the model’s performance on a validation set and stopping <a id="_idIndexMarker442"/>the training process when the performance stops improving or starts to degrade. This helps with finding the point at which the model generalizes well to new data without the need to memorize the <span class="No-Break">training set:</span></li>
			</ul>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B17259_06_05.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Applying the early stopping technique when validation loss starts to increase with more epochs</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor195"/>The effect of the learning rate on data imbalance</h2>
			<p>We have seen that gradient descent takes a step in the direction of the negative gradient – the size <a id="_idIndexMarker443"/>of that step is called the <span class="No-Break"><strong class="bold">learning rate</strong></span><span class="No-Break">.</span></p>
			<p>Choosing the <a id="_idIndexMarker444"/>right learning rate is crucial for models operating on <span class="No-Break">imbalanced datasets.</span></p>
			<p>The learning rate should be selected so that the model learns patterns from both majority and minority classes effectively. Monitoring training and validation loss and other evaluation metrics such as precision, recall, and F1-score can help fine-tune the <span class="No-Break">learning rate.</span></p>
			<p>A high learning rate means that the model’s weights are updated more drastically during each iteration of training. When applied to the minority class, these rapid, large updates can cause the model to skip over the optimal set of weights that minimize the loss for that class. It’s <a id="_idIndexMarker445"/>often beneficial to use techniques such as adaptive <a id="_idIndexMarker446"/>learning rates [3][4] or even class-specific learning rates [5] to ensure that the model learns effectively from both the majority and <span class="No-Break">minority classes.</span></p>
			<p>Now, let’s review some particular kinds of neural networks that have been quite useful for image and <span class="No-Break">text domains.</span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor196"/>Image processing using Convolutional Neural Networks</h2>
			<p><strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>) <a id="_idIndexMarker447"/>is a deep learning model designed for image and video processing tasks. It consists <a id="_idIndexMarker448"/>of convolutional <a id="_idIndexMarker449"/>layers, pooling layers, and fully <span class="No-Break">connected layers:</span></p>
			<ul>
				<li>Convolutional layers apply filters to the input data, learning to detect local features such as edges or textures. These filters slide across the input data, performing element-wise multiplication and summing the results, which creates a feature map representing the presence of <span class="No-Break">specific features.</span></li>
				<li>Pooling layers, such as max-pooling or average pooling, reduce the spatial dimensions of the data, aggregating information and reducing computational complexity. These layers help build invariance to small translations and distortions in the <span class="No-Break">input data.</span></li>
				<li>Fully connected layers process the high-level features extracted by the convolutional and pooling layers to make predictions. Fully connected layers are traditional neural network layers where each neuron is connected to every neuron in the <span class="No-Break">previous layer.</span></li>
			</ul>
			<p>CNNs tend to overfit minority classes, which is in contrast with traditional machine learning algorithms, which usually underfit these minority classes [6]. The intuition here is that CNNs, with their multiple layers and a large number of parameters, are designed to capture complex patterns. Additionally, being data-hungry, CNNs can learn intricate details from the data. When faced with imbalanced data, CNNs may focus excessively on the minority class, essentially “memorizing” the minority <span class="No-Break">class instances.</span></p>
			<p>We will <a id="_idIndexMarker450"/>discuss this in more detail in <a href="B17259_08.xhtml#_idTextAnchor235"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Algorithm-Level Deep Learning Techniques</em>, in the <em class="italic">Class-Dependent Temperature (CDT) </em><span class="No-Break"><em class="italic">loss</em></span><span class="No-Break"> section.</span></p>
			<p>The imbalance <a id="_idIndexMarker451"/>problems in the <a id="_idIndexMarker452"/>computer vision domain can be categorized as shown in <span class="No-Break"><em class="italic">Figure </em></span><span class="No-Break"><em class="italic">6</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break"> [7]:</span></p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B17259_06_06.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Categorization of imbalance problems in computer vision (adapted from [7])</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor197"/>Text analysis using Natural Language Processing</h2>
			<p><strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) is another branch of AI that helps computers understand and analyze human language <a id="_idIndexMarker453"/>for extracting <a id="_idIndexMarker454"/>insights and organizing <a id="_idIndexMarker455"/>information. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em> shows the categorization of imbalance problems in text based on data complexity levels, while <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.8</em> shows the categorization based on some of the popular NLP <span class="No-Break">application areas:</span></p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B17259_06_07.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Categorization of imbalance problems in NLP based on the form of textual data</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B17259_06_08.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Categorization of imbalance problems in NLP based on applications</p>
			<p>With the basics out of the way, let’s see how data imbalance affects deep <span class="No-Break">learning models.</span></p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor198"/>Data imbalance in deep learning</h1>
			<p>While many classical machine learning problems that use tabular data are limited to binary classes <a id="_idIndexMarker456"/>and are interested in predicting the minority class, this is not the norm in domains where deep learning is often applied, especially <a id="_idIndexMarker457"/>computer vision or <span class="No-Break">NLP problems.</span></p>
			<p>Even benchmark datasets such as MNIST (a collection of handwritten digits containing grayscale images from 0 to 9) and CIFAR10 (color images with 10 different classes) have 10 classes to <a id="_idIndexMarker458"/>predict. So, we can say that <strong class="bold">multi-class classification</strong> is typical in problems that use deep <span class="No-Break">learning models.</span></p>
			<p>This data skew or imbalance can severely impact the model performance. We should review what we discussed about the typical kinds of imbalance in datasets in <a href="B17259_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Introduction to Data Imbalance in Machine Learning</em>. To simulate real-world data imbalance scenarios, two types of imbalance are usually investigated in <span class="No-Break">the literature:</span></p>
			<ul>
				<li><strong class="bold">Step imbalance</strong>: All the <a id="_idIndexMarker459"/>minority classes have <a id="_idIndexMarker460"/>the same or almost the same number of examples, as do all the <span class="No-Break">majority classes:</span></li>
			</ul>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B17259_06_09.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Step imbalance</p>
			<ul>
				<li><strong class="bold">Long-tailed imbalance</strong>: The <a id="_idIndexMarker461"/>number of <a id="_idIndexMarker462"/>examples across different classes follows an exponential decay. The plot usually has a long tail toward the left or <span class="No-Break">the right:</span></li>
			</ul>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B17259_06_10.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – Long-tailed imbalance</p>
			<p>Generally, when comparing two datasets, the one with a higher imbalance ratio is likely to yield a lower ROC-AUC score [8]. There can be issues of class overlap, noisy labels, and concept complexity in imbalanced datasets [9][10]. These issues can significantly impact the performance of deep <span class="No-Break">learning models:</span></p>
			<ul>
				<li><strong class="bold">Class overlap</strong>: Class overlap <a id="_idIndexMarker463"/>occurs when instances from different classes are close to each other in the feature space. Deep learning models often rely on complex decision boundaries to separate classes. When instances from different classes are close in the feature space, as is common in imbalanced datasets, the majority class can dominate these decision boundaries. This makes it especially challenging for deep learning models to accurately classify minority <span class="No-Break">class instances.</span></li>
				<li><strong class="bold">Noisy labels</strong>: Noisy labels <a id="_idIndexMarker464"/>refers to instances in a dataset that have incorrect or ambiguous class labels. In imbalanced datasets, noisy labels can disproportionately affect the minority class as the model has fewer instances to learn from. Deep learning models are data-hungry and highly sensitive to the quality of the training data. This can lead to poor generalization in deep learning models, affecting their performance on new, <span class="No-Break">unseen data.</span></li>
				<li><strong class="bold">Concept complexity</strong>: Concept <a id="_idIndexMarker465"/>complexity is the inherent difficulty in distinguishing between classes based on the given features. Deep learning models excel at capturing intricate patterns in the data. However, the complexity of the relationships between features and class labels in imbalanced datasets can make it difficult for these models to effectively learn the minority class. The limited number of instances available for the minority class often compounds <span class="No-Break">this issue.</span></li>
			</ul>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor199"/>The impact of data imbalance on deep learning models</h2>
			<p>Let’s also <a id="_idIndexMarker466"/>review how data imbalance affects deep learning models compared to classical machine <span class="No-Break">learning models:</span></p>
			<ul>
				<li><strong class="bold">Model sensitivity</strong>: The performance of deep learning models can be significantly <a id="_idIndexMarker467"/>impacted as the imbalance ratio of a dataset increases (<span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">):</span></li>
			</ul>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B17259_06_11.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11– Model performance on an imbalanced version of CIFAR-10 with a fixed number of minority classes (adapted from [8])</p>
			<ul>
				<li><strong class="bold">Feature engineering</strong>: Classical machine learning models usually require manual <a id="_idIndexMarker468"/>feature engineering, which can help address data imbalance by creating new features or transforming existing ones to highlight the minority class. In deep learning models, feature engineering is typically performed automatically through the learning process, making it less reliant on human intervention to address <span class="No-Break">the imbalance.</span></li>
				<li><strong class="bold">Techniques to handle imbalance</strong>: In classical machine learning, standard techniques <a id="_idIndexMarker469"/>for handling data imbalance include resampling (oversampling the minority class or undersampling the majority class), generating <a id="_idIndexMarker470"/>synthetic samples (for example, using the <strong class="bold">Synthetic Minority Over-Sampling Technique</strong> (<strong class="bold">SMOTE</strong>)), and using cost-sensitive learning (assigning different misclassification costs to <span class="No-Break">different classes).</span><p class="list-inset">Some techniques from classical machine learning can also be applied in deep learning, but additional methods have been developed specifically for deep learning models. These include transfer learning (leveraging pre-trained models to learn from imbalanced data), using focal loss (a loss function that focuses on hard-to-classify examples), and employing data augmentation techniques to generate more varied and balanced training data. These data augmentation techniques will be discussed in detail in <a href="B17259_07.xhtml#_idTextAnchor205"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Data-Level Deep </em><span class="No-Break"><em class="italic">Learning Methods</em></span><span class="No-Break">.</span></p></li>
				<li><strong class="bold">Model interpretability</strong>: Classical machine learning models are often more interpretable, which can help us understand the impact of data imbalance on the model’s <a id="_idIndexMarker471"/>decision-making process. Deep learning models, on the other hand, are often referred to as “black boxes” due to their lack of interpretability. This lack of interpretability can make it harder to understand how the model handles imbalanced data and whether it is learning meaningful patterns or simply memorizing the <span class="No-Break">majority class.</span></li>
				<li><strong class="bold">Training data size</strong>: Deep <a id="_idIndexMarker472"/>learning models typically require large amounts of training data to achieve optimal performance. In cases of severe data imbalance, gathering sufficient data for the minority class may be more challenging, hindering the performance of deep learning models. Additionally, if a large dataset is available, it is more likely that instances of the minority class will <a id="_idIndexMarker473"/>be found within that vast amount of data. In contrast, in a smaller dataset, the minority class might never appear at all! On the other hand, classical machine learning algorithms can often achieve decent performance with smaller datasets, which is an advantage when dealing with <span class="No-Break">imbalanced data.</span></li>
				<li><strong class="bold">The impact of depth (number of layers) on deep learning models trained on imbalanced data problems</strong>: The pros of more layers are the improved capacity to learn complex patterns and features in the data and improved <a id="_idIndexMarker474"/>generalization of the model. The cons of adding more layers can be the model overfitting and the problem of vanishing or exploding gradients worsening (as depth increases, the gradients during backpropagation can become very small (vanish) or very large (explode), making it challenging to train the model). This is summarized in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">:</span></li>
			</ul>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B17259_06_12.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Summarizing the impact of depth on deep learning models</p>
			<p class="list-inset">Overall, the influence of depth on a deep learning model varies with the data and model architecture requiring <span class="No-Break">empirical evaluation.</span></p>
			<ul>
				<li><strong class="bold">Oversampling and undersampling</strong>: The study titled <em class="italic">A systematic study of the </em><em class="italic"><a id="_idIndexMarker475"/></em><em class="italic">class imbalance </em><em class="italic"><a id="_idIndexMarker476"/></em><em class="italic">problem in convolutional neural networks</em>, by Mateusz Buda [8], thoroughly examined how class imbalance affects the performance of CNNs. The research utilized three well-known datasets – MNIST, CIFAR-10, and ImageNet – and employed models such as LeNet-5, All-CNN, and ResNet-10 for the experiments. Their key findings were <span class="No-Break">as follows:</span><ul><li>In almost all analyzed cases, oversampling proved to be the most effective technique for mitigating <span class="No-Break">class imbalance</span></li><li>Oversampling was more effective when the imbalance was eliminated while undersampling yielded better results when only reducing the <span class="No-Break">imbalance partially</span></li><li>Contrary <a id="_idIndexMarker477"/>to some traditional machine learning algorithms, CNNs did not overfit when oversampling <span class="No-Break">was applied</span></li><li>In some cases, undersampling performs on par with oversampling, even when using <span class="No-Break">less data</span></li><li>Undersampling <a id="_idIndexMarker478"/>generally showed poor performance compared to the baseline and never showed a notable advantage <span class="No-Break">over oversampling</span></li><li>If the focus is on correctly classifying examples from minority classes, undersampling may be preferable <span class="No-Break">to oversampling</span></li></ul><p class="list-inset">If there’s too much data, undersampling might be the preferred or only option to save time, resources, and the cost <span class="No-Break">of training.</span></p></li>
				<li><strong class="bold">Threshold adjustment</strong>: We discussed threshold adjustment in detail in <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Cost-Sensitive Learning</em>. As a refresher, a decision threshold is a value <a id="_idIndexMarker479"/>that determines the boundary between different classes or outcomes in a classification problem. The paper by Johnson et al. [11] emphasized that the optimal threshold is linearly correlated with the minority class size (that is, the lower the minority class size, the lower the threshold). They trained and tested for fraud detection using deep learning models with two and four hidden layers on CMS Medicare data [12]. The default threshold often leads to poor classification, especially for the minority class, highlighting the need <a id="_idIndexMarker480"/>for threshold adjustment based on a <span class="No-Break">validation set.</span></li>
			</ul>
			<p class="callout-heading">A note about the datasets used in the second half of this book</p>
			<p class="callout">In the remainder of this book, we will primarily use imbalanced versions of the MNIST and CIFAR10-LT (“LT” stands for “long-tailed”) datasets <span class="No-Break">for training:</span></p>
			<p class="callout">•  <strong class="bold">MNIST</strong>: A small grayscale image dataset with 10 classes containing digits from 0 to 9. It consists of 60,000 training images and 10,000 test images, each 28 pixels x 28 pixels. It’s faster to train/test compared <span class="No-Break">to CIFAR-10.</span></p>
			<p class="callout">•  <strong class="bold">CIFAR-10</strong>: Used for object recognition, this color image dataset also has 10 classes. It includes 50,000 training images and 10,000 test images, each 32 pixels x <span class="No-Break">32 pixels.</span></p>
			<p class="callout">Though the training sets are imbalanced, the corresponding test sets have an equal number of examples in each class. This balanced test set approach provides <span class="No-Break">several benefits:</span></p>
			<p class="callout">•  <strong class="bold">Comparability</strong>: Balanced test sets allow unbiased comparison across various classes <span class="No-Break">and models</span></p>
			<p class="callout">•  <strong class="bold">Repeatability and reproducibility</strong>: Using simple datasets such as MNIST and CIFAR-10 ensures ease of code execution <span class="No-Break">and understanding</span></p>
			<p class="callout">•  <strong class="bold">Efficiency</strong>: Smaller datasets enable quicker iterations, allowing us to try, test, and retry running the code in a <span class="No-Break">reasonable timeframe</span></p>
			<p class="callout">•  <strong class="bold">Alignment with research</strong>: This approach is consistent with most research studies on long-tailed learning and imbalanced datasets, providing a common framework <span class="No-Break">for comparison</span></p>
			<p>The next section will give us an overview of deep learning strategies for managing data imbalance. We will also see how various techniques for handling imbalanced datasets that were initially developed for classical machine learning techniques can be easily extended to deep <span class="No-Break">learning models.</span></p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor200"/>Overview of deep learning techniques to handle data imbalance</h1>
			<p>Much like <a id="_idIndexMarker481"/>the first half of this book, where we focused on classical machine learning techniques, the major categories typically include sampling techniques, cost-sensitive techniques, threshold adjustment techniques, or a combination <span class="No-Break">of these:</span></p>
			<ul>
				<li>The sampling techniques comprise either undersampling the majority class or oversampling the minority class data. Data augmentation is a fundamental technique in computer vision problems that’s used to increase the diversity of the training set. While not directly an oversampling method aimed at addressing class imbalance, data augmentation does have the effect of expanding the training data. We will discuss these techniques in more detail in <a href="B17259_07.xhtml#_idTextAnchor205"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Data-Level Deep </em><span class="No-Break"><em class="italic">Learning Methods</em></span><span class="No-Break">.</span></li>
				<li>Cost-sensitive techniques usually involve changing the model loss function in some way to accommodate the higher cost of misclassifying the minority class examples. Some standard loss functions, such as <strong class="source-inline">CrossEntropyLoss</strong> in PyTorch, support the weight parameter to accommodate such costs. We will cover many of those, including several custom loss functions, in detail in <a href="B17259_08.xhtml#_idTextAnchor235"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Algorithm-Level Deep </em><span class="No-Break"><em class="italic">Learning Techniques</em></span><span class="No-Break">.</span></li>
				<li>Hybrid deep learning techniques integrate the data-level and algorithm-level approaches. This fusion allows for more nuanced and effective solutions to tackle class imbalance. We’ll discuss this in <a href="B17259_09.xhtml#_idTextAnchor256"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Hybrid Deep </em><span class="No-Break"><em class="italic">Learning Methods</em></span><span class="No-Break">.</span></li>
				<li>Threshold adjustment techniques are applied to the scores produced from the model after the model has been trained. These techniques can help adjust the threshold so that the model metric, say, F1-score or geometric mean, gets optimized. This was discussed in <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">, </em><span class="No-Break"><em class="italic">Cost-Sensitive Learning</em></span><span class="No-Break">:</span></li>
			</ul>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B17259_06_13.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – Categorization of deep learning techniques covered</p>
			<p>We will not use deep learning methods for tabular data from this chapter onwards because <a id="_idIndexMarker482"/>classical models such as XGBoost, LightGBM, and CatBoost tend to perform well on such structured data. Several studies ([13] and [14]) have shown that traditional machine learning models often outperform deep learning models in supervised learning tasks involving tabular data. However, an ensemble of an XGBoost model and a deep learning model can outperform the XGBoost model alone [13]. Therefore, tasks using tabular datasets can still benefit from deep learning models. It’s likely only a matter of time before deep learning models catch up to the performance of classical models on tabular data. Nevertheless, we will focus our implementation and examples on vision and NLP problems when using deep <span class="No-Break">learning models.</span></p>
			<p>This concludes our high-level discussion of various techniques for dealing with imbalanced datasets when using deep <span class="No-Break">learning models.</span></p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor201"/>Multi-label classification</h1>
			<p>Multi-label classification is a classification task where each instance can be assigned to multiple <a id="_idIndexMarker483"/>classes or labels simultaneously. In other words, an instance can belong to more than one category or have multiple attributes. For example, a movie can belong to multiple genres, such as action, comedy, and romance. Similarly, an image can have multiple objects in it (<span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">):</span></p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B17259_06_14.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – Multi-label image classification with prediction probabilities shown</p>
			<p>But how is it different from multi-class classification? Multi-class classification is a classification task where each instance can be assigned to only one class or label. In this case, the classes or categories are mutually exclusive, meaning an instance can belong to just one category. For example, a handwritten digit recognition task would be multi-class since each digit can belong to only one <span class="No-Break">class (0-9).</span></p>
			<p>In summary, the main difference between multi-label and multi-class classification is that in multi-label classification, instances can have multiple labels. In contrast, in multi-class classification, instances can have only one label. This is summarized in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.15</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B17259_06_15.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.15 – Distinction between multi-label and multi-class classification</p>
			<p>Many real-world <a id="_idIndexMarker484"/>problems are inherently multi-labeled and experience class imbalance. Deep learning models are particularly useful here, especially when the data involved is unstructured, such as images, videos, <span class="No-Break">or text.</span></p>
			<p>In <strong class="bold">Multi-Label Datasets</strong> (<strong class="bold">MLDs</strong>), there can be tens or hundreds of labels, and each instance can <a id="_idIndexMarker485"/>be associated with a subset of those labels. The more different labels exist, the more possibilities there are that some have a very low presence, leading to a <span class="No-Break">significant imbalance.</span></p>
			<p>The data imbalance in multi-label classification can occur at many <span class="No-Break">levels [15]:</span></p>
			<ul>
				<li><strong class="bold">Imbalance within labels</strong>: A large disparity between negative and positive instances <a id="_idIndexMarker486"/>in each label can happen, causing classification models to struggle with <span class="No-Break">minority classes</span></li>
				<li><strong class="bold">Imbalance between labels</strong>: Unequal distribution of positive instances among labels, leading <a id="_idIndexMarker487"/>to poor performance for <span class="No-Break">minority classes</span></li>
				<li><strong class="bold">Imbalance among label sets</strong>: Sparse <a id="_idIndexMarker488"/>frequency of label sets (a combination of various labels) due to label sparseness, making it challenging for classification models to <span class="No-Break">learn effectively</span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.16</em> sums up these kinds of imbalances when classifying <span class="No-Break">multi-label datasets:</span></p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B17259_06_16.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.16 – Kinds of imbalances in multi-label datasets</p>
			<p>The imbalance <a id="_idIndexMarker489"/>handling approaches and metrics are similar to those used for imbalanced multi-class classification methods. Typical approaches include resampling methods, ensemble approaches, and <span class="No-Break">cost-sensitive methods.</span></p>
			<p>The most straightforward approach is to convert the multi-label dataset into the multi-class dataset and then apply various re-sampling methods such as random oversampling, random <a id="_idIndexMarker490"/>undersampling, <strong class="bold">Edited Nearest Neighbors</strong> (<strong class="bold">ENN</strong>), and others. Similar strategies have been used in the literature to adapt the cost-sensitive approaches to fit multi-label <span class="No-Break">classification problems.</span></p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor202"/>Summary</h1>
			<p>Deep learning has become essential in many fields, from computer vision and natural language processing to healthcare and finance. This chapter has provided a brief introduction to the core concepts and techniques in deep learning. We talked about PyTorch, the fundamentals of deep learning, activation functions, and data imbalance challenges. We also got a bird’s-eye view of the various techniques we will discuss in the following <span class="No-Break">few chapters.</span></p>
			<p>Understanding these fundamentals will equip you with the knowledge necessary to explore more advanced topics and applications and ultimately contribute to the ever-evolving world of <span class="No-Break">deep learning.</span></p>
			<p>In the next chapter, we will look at data-level deep <span class="No-Break">learning methods.</span></p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor203"/>Questions</h1>
			<ol>
				<li>What are some challenges in porting data imbalance handling methods from classical machine learning models to deep <span class="No-Break">learning models?</span></li>
				<li>How could an imbalanced version of the MNIST dataset <span class="No-Break">be created?</span></li>
				<li>Use the MNIST dataset to train a CNN model with varying degrees of imbalance in the data. Record the model’s overall accuracy on a fixed test set. Plot how the overall accuracy changes as the imbalance in the training data increases. Observe whether the overall accuracy declines as the training data becomes <span class="No-Break">more imbalanced.</span></li>
				<li>What is the purpose of using random oversampling with deep <span class="No-Break">learning models?</span></li>
				<li>What are some of the data augmentation techniques that can be applied when dealing with limited or <span class="No-Break">imbalanced data?</span></li>
				<li>How does undersampling work in handling data imbalance, and what are <span class="No-Break">its limitations?</span></li>
				<li>Why is it important to ensure that the data augmentation techniques preserve the original labels of <span class="No-Break">the dataset?</span></li>
			</ol>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor204"/>References</h1>
			<ol>
				<li value="1">A. W. Trask, Grokking Deep Learning (Manning, Shelter Island, <span class="No-Break">NY, 2019).</span></li>
				<li>F. Chollet, <em class="italic">Deep Learning with Python</em>. Manning <span class="No-Break">Publications, 2021.</span></li>
				<li>Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, “<em class="italic">Class-Balanced Loss Based on Effective Number of Samples</em>,” <span class="No-Break">p. 10.</span></li>
				<li>K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, <em class="italic">Learning Imbalanced Datasets with Label- Distribution-Aware Margin Loss </em>[Online]. Available <span class="No-Break">at </span><a href="https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf"><span class="No-Break">https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf</span></a><span class="No-Break">.</span></li>
				<li>R. Jantanasukon and A. Thammano, <em class="italic">Adaptive Learning Rate for Dealing with Imbalanced Data in Classification Problems</em>. In 2021 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunication Engineering, Cha-am, Thailand: IEEE, Mar. 2021, pp. 229–232, <span class="No-Break">doi: </span><span class="No-Break">10.1109/ECTIDAMTNCON51128.2021.9425715</span><span class="No-Break">.</span></li>
				<li>H.-J. Ye, H.-Y. Chen, D.-C. Zhan, and W.-L. Chao, <em class="italic">Identifying and Compensating for Feature Deviation in Imbalanced Deep Learning</em>. arXiv, Jul. 10, 2022. Accessed: Dec. 14, 2022. [Online]. Available <span class="No-Break">at </span><a href="http://arxiv.org/abs/2001.01385"><span class="No-Break">http://arxiv.org/abs/2001.01385</span></a><span class="No-Break">.</span></li>
				<li>V. Sampath, I. Maurtua, J. J. Aguilar Martín, and A. Gutierrez, <em class="italic">A survey on generative adversarial networks for imbalance problems in computer vision tasks</em>. J Big Data, vol. 8, no. 1, p. 27, Dec. 2021, <span class="No-Break">doi: </span><span class="No-Break">10.1186/s40537-021-00414-0</span><span class="No-Break">.</span></li>
				<li>M. Buda, A. Maki, and M. A. Mazurowski, <em class="italic">A systematic study of the class imbalance problem in convolutional neural networks</em>. Neural Networks, vol. 106, pp. 249–259, Oct. 2018, <span class="No-Break">doi: </span><span class="No-Break">10.1016/j.neunet.2018.07.011</span><span class="No-Break">.</span></li>
				<li>K. Ghosh, C. Bellinger, R. Corizzo, B. Krawczyk, and N. Japkowicz, <em class="italic">On the combined effect of class imbalance and concept complexity in deep learning</em>. arXiv, Jul. 29, 2021. Accessed: Mar. 28, 2023. [Online]. Available <span class="No-Break">at </span><a href="http://arxiv.org/abs/2107.14194"><span class="No-Break">http://arxiv.org/abs/2107.14194</span></a><span class="No-Break">.</span></li>
				<li>K. Ghosh, C. Bellinger, R. Corizzo, B. Krawczyk, and N. Japkowicz, <em class="italic">On the combined effect of class imbalance and concept complexity in deep learning</em>. arXiv, Jul. 29, 2021. Accessed: Mar. 28, 2023. [Online]. Available <span class="No-Break">at </span><a href="http://arxiv.org/abs/2107.14194"><span class="No-Break">http://arxiv.org/abs/2107.14194</span></a><span class="No-Break">.</span></li>
				<li>J. M. Johnson and T. M. Khoshgoftaar, <em class="italic">Medicare fraud detection using neural networks</em>. J Big Data, vol. 6, no. 1, p. 63, Dec. 2019, <span class="No-Break">doi: </span><span class="No-Break">10.1186/s40537-019-0225-0</span><span class="No-Break">.</span></li>
				<li><em class="italic">Medicare fraud &amp; abuse: prevention, detection, and reporting</em>. Centers for Medicare &amp; Medicaid Services. <span class="No-Break">2017. </span><a href="https://www.cms.gov/Outreach-and-Education/Medicare-Learning-Network-MLN/MLNProducts/MLN-Publications-Items/MLN4649244"><span class="No-Break">https://www.cms.gov/Outreach-and-Education/Medicare-Learning-Network-MLN/MLNProducts/MLN-Publications-Items/MLN4649244</span></a><span class="No-Break">.</span></li>
				<li>R. Shwartz-Ziv and A. Armon, <em class="italic">Tabular Data: Deep Learning is Not All You Need</em>. arXiv, Nov. 23, 2021. Accessed: Apr. 10, 2023. [Online]. Available <span class="No-Break">at </span><a href="http://arxiv.org/abs/2106.03253"><span class="No-Break">http://arxiv.org/abs/2106.03253</span></a><span class="No-Break">.</span></li>
				<li>V. Borisov, T. Leemann, K. Sessler, J. Haug, M. Pawelczyk, and G. Kasneci, <em class="italic">Deep Neural Networks and Tabular Data: A Survey</em>. IEEE Trans. Neural Netw. Learning Syst., pp. 1–21, 2022, <span class="No-Break">doi: </span><span class="No-Break">10.1109/TNNLS.2022.3229161</span><span class="No-Break">.</span></li>
				<li>A. N. Tarekegn, M. Giacobini, and K. Michalak, <em class="italic">A review of methods for imbalanced multi-label classification</em>. Pattern Recognition, vol. 118, p. 107965, Oct. 2021, doi: <span class="No-Break">10.1016/j. patcog.2021.107965</span><span class="No-Break">.</span></li>
			</ol>
		</div>
	</body></html>