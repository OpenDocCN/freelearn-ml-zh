<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer103">
			<h1 id="_idParaDest-102" class="chapter-number"><a id="_idTextAnchor102"/>8</h1>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor103"/>ML Model Explainability</h1>
			<p>In the rapidly <a id="_idIndexMarker464"/>evolving world of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) and <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>), developing models capable of delivering accurate predictions is no <a id="_idIndexMarker465"/>longer the sole objective. As organizations increasingly rely on data-driven decision-making, understanding the rationale behind a model’s predictions becomes paramount. The growing need for explainability in ML models stems from ethical, regulatory, and practical concerns, and it is here that <a id="_idIndexMarker466"/>the concept of <strong class="bold">Explainable AI</strong> (<strong class="bold">XAI</strong>) comes <span class="No-Break">into play.</span></p>
			<p>This chapter delves into the intricacies of Explainable ML models, a critical component in the MLOps landscape, with a focus on their implementation in the Google Cloud ecosystem. Although a comprehensive exploration of XAI techniques and tools is beyond this chapter’s scope, we aim to equip you with the knowledge and skills to build transparent, interpretable, and accountable ML models using the Explainable ML tools available <span class="No-Break">on GCP.</span></p>
			<p>The following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>What is Explainable AI and why is it important for <span class="No-Break">MLOps practitioners?</span></li>
				<li>Overview of Explainable <span class="No-Break">AI techniques</span></li>
				<li>Explainable AI features available in Google Cloud <span class="No-Break">Vertex AI</span></li>
				<li>Hands-on exercises for using Vertex AI’s <span class="No-Break">explainability features</span></li>
			</ul>
			<p>As we journey through this chapter, we will establish the importance of explainability and its role in enhancing trust, accountability, and fairness in ML models. Next, we will discuss various techniques for achieving explainability in ML, ranging from traditional interpretable models to explanation techniques for more complex models such as deep learning. We will then dive into Google Cloud’s XAI offerings, which facilitate the development and evaluation of Explainable <span class="No-Break">ML models.</span></p>
			<p>In addition to providing an understanding of Explainable ML models, this chapter will guide you through practical, hands-on examples, illustrating the application of these concepts in <span class="No-Break">real-world scenarios.</span></p>
			<p>By the end of this chapter, you will be well-equipped to design, deploy, and evaluate Explainable ML models on Google Cloud, ensuring that your organization stays ahead in the race toward ethical and responsible <span class="No-Break">AI adoption.</span></p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor104"/>What is Explainable AI and why is it important for MLOps practitioners?</h1>
			<p>XAI refers to methods and techniques that are used in the domain of AI that aim to make the decision-making processes of AI models transparent, interpretable, and understandable to humans. Instead of acting as black boxes where input data goes in and a decision <a id="_idIndexMarker467"/>or prediction comes out without clarity on how the decision was reached, XAI seeks to provide insights into the inner workings of models. This transparency allows users, developers, and stakeholders to trust and validate the system’s decisions, ensuring they align with ethical, legal, and <span class="No-Break">practical considerations.</span></p>
			<p>As ML <a id="_idIndexMarker468"/>continues to advance and its applications permeate various industries, the need for transparent and interpretable models has become a pressing concern. XAI aims to address this by developing techniques for understanding, interpreting, and explaining ML models. For MLOps practitioners working with Google Cloud, incorporating XAI into their workflows can lead to several benefits, including improved trust, regulatory compliance, and enhanced <span class="No-Break">model performance.</span></p>
			<p>First, let’s look at the key reasons for the importance of XAI for MLOps practitioners and its impact on the development and deployment of <span class="No-Break">ML models.</span></p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor105"/>Building trust and confidence</h2>
			<p>XAI <a id="_idIndexMarker469"/>can help MLOps practitioners build trust in their models by providing clear and understandable explanations of how these models make decisions. This is particularly important when dealing with stakeholders who may not have a technical background as the ability to explain model behavior can lead to increased confidence in its predictions. Additionally, having a deeper understanding of the inner workings of a model allows practitioners <a id="_idIndexMarker470"/>to better communicate the limitations and strengths of their solutions, which, in turn, can foster trust among collaborators and <span class="No-Break">end users.</span></p>
			<h3>Regulatory compliance</h3>
			<p>As ML <a id="_idIndexMarker471"/>models become more widely adopted, regulatory bodies around the world are increasingly calling for more transparency and accountability in AI systems. XAI techniques can help MLOps practitioners ensure compliance with these regulations by providing insights into the decision-making process of their models. This can be especially important in industries such as healthcare, finance, and human resources, where the consequences of biased or unfair decisions can be significant. By incorporating XAI into their workflows, practitioners can demonstrate that their models adhere to relevant laws and <span class="No-Break">ethical guidelines.</span></p>
			<h3>Model debugging and improvement</h3>
			<p>XAI can be invaluable to MLOps practitioners during the model development and debugging process. By providing insights into how a model is making its predictions, XAI can help <a id="_idIndexMarker472"/>identify areas where the model may be underperforming, overfitting, or exhibiting bias. With this information, practitioners can make targeted adjustments to their models, leading to improved performance and more robust solutions. This iterative process can save both time and resources, allowing practitioners to focus on addressing the most critical issues impacting <span class="No-Break">their models.</span></p>
			<h3>Ethical considerations</h3>
			<p>As the <a id="_idIndexMarker473"/>power and influence of ML models grow, so too does the responsibility of MLOps practitioners to ensure that these models are used ethically. XAI can help practitioners identify and address any unintended consequences or biases that may arise from their models. By understanding how a model makes its decisions, practitioners can better ensure that their solutions are fair, unbiased, and aligned with <span class="No-Break">ethical principles.</span></p>
			<p>Incorporating XAI into the MLOps workflow within the Google Cloud ecosystem can lead to <a id="_idIndexMarker474"/>numerous benefits for practitioners. From building trust with stakeholders and ensuring regulatory compliance to improving model performance and addressing ethical concerns, the importance of XAI cannot be understated. As the field of ML continues to evolve, integrating XAI into MLOps practices will become increasingly essential for the development and deployment of transparent, interpretable, and responsible <span class="No-Break">AI solutions.</span></p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor106"/>Explainable AI techniques</h1>
			<p>Different techniques are available to cater to various types of data, including tabular, image, and text data. Each data type presents its own set of challenges and complexities, requiring <a id="_idIndexMarker475"/>tailored methods to provide meaningful insights into the decision-making processes of ML models. This subsection will list various XAI techniques applicable to tabular, image, and text data. The following section will dive into the ones available as out-of-the-box features in <span class="No-Break">Google Cloud.</span></p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor107"/>Global versus local explainability</h2>
			<p>Explainability <a id="_idIndexMarker476"/>can be <a id="_idIndexMarker477"/>categorized into two categories: local <a id="_idIndexMarker478"/>and global explainability. These terms are sometimes referred to as local and global <span class="No-Break">feature importance:</span></p>
			<ul>
				<li><strong class="bold">Global explainability</strong> focuses on the overall impact of a feature on the model. This is <a id="_idIndexMarker479"/>usually obtained by calculating the average feature attribution values over the entire dataset. A feature with a high absolute value indicates that it significantly influenced the <span class="No-Break">model’s predictions.</span></li>
				<li><strong class="bold">Local explainability</strong> provides <a id="_idIndexMarker480"/>insight into how much each feature contributed to the prediction for a specific instance. The feature attribution values give information about the effect of a particular feature on the prediction relative to a <span class="No-Break">baseline prediction.</span></li>
			</ul>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor108"/>Techniques for image data</h2>
			<p>XAI techniques <a id="_idIndexMarker481"/>for image data often focus on visualizing the areas within an image that contribute most significantly to the model’s predictions. Here are some <span class="No-Break">key techniques:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Integrated Gradients</strong></span><p class="list-inset">Integrated Gradients <a id="_idIndexMarker482"/>is an attribution <a id="_idIndexMarker483"/>technique specifically designed for deep learning models, such as neural networks. It computes the gradients of the model’s output concerning the input features (pixels in the case of image data) and integrates these gradients over a straight path from a baseline input to the instance of interest. This process assigns an importance value to each pixel in the image, reflecting its contribution to the model’s prediction. Integrated Gradients provide insights into the importance of each pixel and can help identify potential biases or shortcomings in the <span class="No-Break">model’s predictions:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B17792_08_01.jpg" alt="Figure 8.1 – Integrated Gradients explanation" width="1393" height="400"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Integrated Gradients explanation</p>
			<p class="list-inset">The preceding figure showcases the Integrated Gradients approach to image explanation, which highlights pixels in the image that the model gives high importance to <span class="No-Break">during prediction.</span></p>
			<ul>
				<li><strong class="bold">eXtended Relevance-weighted Attribution of </strong><span class="No-Break"><strong class="bold">Importance</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">XRAI</strong></span><span class="No-Break">)</span><p class="list-inset">XRAI <a id="_idIndexMarker484"/>is an XAI method for visualizing the most important regions in <a id="_idIndexMarker485"/>an image for a given model’s prediction. It is an extension of the Integrated Gradients method, which combines pixel-wise attributions with segmentation techniques to generate more coherent and interpretable visualizations. By identifying the most important segments within an image, XRAI provides insights into the model’s decision-making process and can help identify potential biases or issues in the <span class="No-Break">model’s predictions:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B17792_08_02.jpg" alt="Figure 8.2 – XRAI" width="1114" height="314"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – XRAI</p>
			<p class="list-inset">The preceding figure showcases the XRAI approach to an image explanation, which highlights areas of the image that the model gives high importance <span class="No-Break">during prediction.</span></p>
			<ul>
				<li><strong class="bold">Local Interpretable Model-agnostic </strong><span class="No-Break"><strong class="bold">Explanations</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LIME</strong></span><span class="No-Break">)</span><p class="list-inset">LIME is an XAI technique that provides local explanations for individual predictions of any classifier. In the context of image data, LIME generates synthetic data points (perturbed images) around a specific instance, obtains predictions from the model, and fits an interpretable model (for example, linear regression) to these data points, weighted by their proximity to the instance. The <a id="_idIndexMarker486"/>resulting model provides insights into the most important regions <a id="_idIndexMarker487"/>that contribute to the prediction for the specific instance. By visualizing these regions, practitioners can better understand the model’s decision-making process and identify potential biases or issues in the <span class="No-Break">model’s predictions.</span></p></li>
				<li><strong class="bold">Gradient-weighted Class Activation </strong><span class="No-Break"><strong class="bold">Mapping</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">Grad-CAM</strong></span><span class="No-Break">)</span><p class="list-inset">Grad-CAM <a id="_idIndexMarker488"/>is a visualization technique for deep learning models, specifically <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>). It generates heatmap-like visualizations of the most important regions in an image for a given model’s prediction. Grad-CAM computes <a id="_idIndexMarker489"/>the gradients of the predicted class score concerning the feature maps of the last convolutional layer and then uses these gradients to compute <a id="_idIndexMarker490"/>a weighted sum of the feature maps. The resulting heatmap highlights the regions in the image that contribute most to the model’s prediction. Grad-CAM provides insights into the model’s decision-making process and can help identify potential biases or shortcomings in the <span class="No-Break">model’s predictions.</span></p></li>
			</ul>
			<p>These techniques provide insights into the model’s decision-making process and can help identify potential biases or shortcomings in the <span class="No-Break">model’s predictions.</span></p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor109"/>Techniques for tabular data</h2>
			<p>Tabular data, which is <a id="_idIndexMarker491"/>composed of structured <a id="_idIndexMarker492"/>rows and columns, is one of the most common data types encountered in ML. Various XAI techniques can be employed to interpret models trained on <span class="No-Break">tabular data:</span></p>
			<ul>
				<li><strong class="bold">Local Interpretable Model-agnostic </strong><span class="No-Break"><strong class="bold">Explanations</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LIME</strong></span><span class="No-Break">)</span><p class="list-inset">As its name suggests, LIME is an XAI technique that provides <em class="italic">local</em> explanations for <a id="_idIndexMarker493"/>individual predictions of any classifier. It does so by approximating the complex model with <a id="_idIndexMarker494"/>a simpler, interpretable model (for example, linear regression) within the vicinity of a specific instance. LIME generates synthetic data points around the instance, obtains predictions from the complex model, and fits an interpretable model to these data points, weighted by their proximity to the instance. The resulting model provides insights into the most important features contributing to the prediction for the <span class="No-Break">specific instance.</span></p><p class="list-inset">In the following figure, we’re using a LIME report to explain the decision of an ML model trained to predict the survival probabilities of Titanic’s passengers based on their attributes, such as their gender, the fare they paid, the passenger class they were traveling in, and so on. The Titanic survival dataset is a common publically available dataset that’s used as an example for classification models. Let’s see if LIME can help us get some insights into the <span class="No-Break">model’s behavior:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B17792_08_03.jpg" alt="Figure 8.3 – Explaining classification models with LIME" width="1099" height="357"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Explaining classification models with LIME</p>
			<p class="list-inset">The leftmost chart shows the predicted survival probability for the chosen passenger. In this example, the model predicts that this passenger had a 40% probability of survival based on their key attributes. The chart in the middle shows us the ranked feature importance list generated by using LIME. Based on this chart, it seems that the top three most important features that dictated whether a Titanic passenger survived or not were their sex/gender, fare paid, and the passenger class they were traveling in. This makes sense because we know <a id="_idIndexMarker495"/>women were evacuated first, giving them a higher overall chance of survival. We also know that passengers who paid lower fares and had lower-class tickets had their cabins on the lower decks/floors of the ship, which got flooded first and the passengers with higher fares had their cabins on the upper decks, giving them a better chance of survival. So, you can see how LIME and similar techniques can help decipher black box ML models and help us better understand why a particular prediction was made. You will also be glad to know that the Titanic passenger we used for the preceding example was a 26-year-old lady named Miss. Laina Heikkinen paid $7.925 for a ticket in the third-class passenger section and despite our model giving her a less than 40% chance of survival, <span class="No-Break">she survived.</span></p>
			<ul>
				<li><strong class="bold">Shapley Additive exPlanations</strong> (<strong class="bold">SHAP</strong>) (native support on <span class="No-Break">Vertex AI)</span><p class="list-inset">SHAP is a unified measure of feature importance based on cooperative game theory, providing a consistent <a id="_idIndexMarker496"/>and fair way to allocate feature importance. By calculating the Shapley value for each feature, SHAP assigns an importance value that reflects its contribution to the prediction for a specific instance. The Shapley value is calculated by averaging the marginal contributions of a feature across all possible feature combinations. SHAP values provide insights into the most influential features driving a model’s predictions and can be used with a variety of models. Recommended model types include non-differentiable models such as ensembles of trees. They can also be used for neural networks, where SHAP can provide insights into the contribution of each input feature to the final prediction made by the network. By analyzing the SHAP values, you <a id="_idIndexMarker497"/>can determine which <a id="_idIndexMarker498"/>features have the most impact on the network’s output and understand the relationship between the inputs <span class="No-Break">and outputs:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B17792_08_04.jpg" alt="Figure 8.4 – Feature importance graph in Vertex AI based on the Shapley method" width="459" height="762"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Feature importance graph in Vertex AI based on the Shapley method</p>
			<ul>
				<li><strong class="bold">Permutation </strong><span class="No-Break"><strong class="bold">feature importance</strong></span><p class="list-inset">Permutation <a id="_idIndexMarker499"/>feature importance <a id="_idIndexMarker500"/>is a model-agnostic technique that estimates the importance of each feature by measuring the change in the model’s performance when the feature’s values are randomly shuffled. This process is repeated several times, and the average decrease in performance is used as an estimate of the feature’s importance. By disrupting the relationship <a id="_idIndexMarker501"/>between the feature and the <a id="_idIndexMarker502"/>target variable, permutation importance helps identify the features that have the greatest impact on the <span class="No-Break">model’s predictions.</span></p></li>
				<li><strong class="bold">Partial dependence </strong><span class="No-Break"><strong class="bold">plots</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">PDP</strong></span><span class="No-Break">)</span><p class="list-inset">PDP is a visualization technique that depicts the relationship between a specific feature <a id="_idIndexMarker503"/>and the model’s predicted outcome while holding all other features constant. By illustrating how a single <a id="_idIndexMarker504"/>feature influences the prediction, PDP can help practitioners better understand the behavior of their models and identify potential biases <span class="No-Break">or inconsistencies.</span></p></li>
				<li><strong class="bold">Feature importance</strong> (for example, GINI importance and coefficients in <span class="No-Break">linear models)</span><p class="list-inset">Feature <a id="_idIndexMarker505"/>importance is a set of techniques that quantify the impact of input features on the model’s predictions. These methods <a id="_idIndexMarker506"/>can help practitioners identify the most relevant features, enabling them to focus on the most significant variables during model development and debugging. Some common approaches to feature importance are <span class="No-Break">as follows:</span></p><ul><li><strong class="bold">GINI Importance</strong>: Used in decision trees and random forests, GINI importance <a id="_idIndexMarker507"/>measures the average <a id="_idIndexMarker508"/>reduction in impurity (<strong class="bold">GINI index</strong>) attributable to a specific feature across all the trees in <span class="No-Break">the forest.</span></li><li><strong class="bold">Coefficients in linear models</strong>: In linear regression and logistic regression, the <a id="_idIndexMarker509"/>coefficients of the model can be <a id="_idIndexMarker510"/>used as a measure of feature importance, indicating the magnitude and direction of the relationship between each feature and the target variable. Larger absolute coefficient values indicate a stronger relationship between the feature and the <span class="No-Break">target variable.</span></li></ul></li>
			</ul>
			<p>These techniques help practitioners understand the relationships between input features and model predictions, identify the most influential features, and assess the impact of specific features on <span class="No-Break">individual predictions.</span></p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor110"/>Techniques for text data</h2>
			<p>In the <a id="_idIndexMarker511"/>case of natural language processing models that <a id="_idIndexMarker512"/>use text data, the aim is to identify the most significant words or phrases that contribute to the model’s predictions. Here are some XAI techniques for <span class="No-Break">text data:</span></p>
			<ul>
				<li><strong class="bold">Text-specific LIME</strong>: This is an adaptation of LIME specifically designed for text data <a id="_idIndexMarker513"/>that provides explanations <a id="_idIndexMarker514"/>for individual predictions by highlighting the most important words <span class="No-Break">or phrases.</span><p class="list-inset">In the following example, we’re using LIME to explain why the ML model we built to classify movie reviews as positive or negative arrives at a <span class="No-Break">particular conclusion:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B17792_08_05.jpg" alt="Figure 8.5 – LIME-based explanation for text classification" width="1076" height="714"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – LIME-based explanation for text classification</p>
			<p>As we can see, the predicted probability of the movie review being positive is 0.77 (77%). The words highlighted in orange contributed toward the positive probability, while the <a id="_idIndexMarker515"/>words highlighted in blue had a significant contribution toward moving the final prediction toward the “negative” label. The chart at the top <a id="_idIndexMarker516"/>right shows the respective contribution of each of the highlighted words toward the final decision. For example, if we remove the word “amazing” from the review text, the positive probability will go down <span class="No-Break">by 0.03:</span></p>
			<ul>
				<li><strong class="bold">Text-specific SHAP</strong>: This is an adaptation of SHAP tailored for text data, attributing <a id="_idIndexMarker517"/>importance values <a id="_idIndexMarker518"/>to individual words or phrases within a <span class="No-Break">given text</span></li>
				<li><strong class="bold">Attention mechanisms</strong>: In <a id="_idIndexMarker519"/>deep learning <a id="_idIndexMarker520"/>models such as Transformers, attention mechanisms can provide insights into the relationships between words and the model’s predictions by visualizing the <span class="No-Break">attention weights</span></li>
			</ul>
			<p>Now that we have familiarized ourselves with the various popular XAI techniques, let’s look at the different features available within Google Cloud Vertex AI that can help us build XAI solutions using <span class="No-Break">these techniques.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor111"/>Explainable AI features available in Google Cloud 
Vertex AI</h1>
			<p><strong class="bold">Google Cloud Vertex AI</strong> offers <a id="_idIndexMarker521"/>a suite of tools and options tailored to make AI systems more understandable. This section delves into the various <a id="_idIndexMarker522"/>XAI options available in Vertex AI, showcasing how this platform is advancing the frontier of <span class="No-Break">transparent ML.</span></p>
			<p>Broadly, the XAI options available in Vertex AI can be divided into <span class="No-Break">two types:</span></p>
			<ul>
				<li><strong class="bold">Feature-based</strong>: Feature attributions refer to the degree to which each feature in <a id="_idIndexMarker523"/>a model contributes to the predictions for a specific instance. When making a prediction request, you receive the predicted values that are generated by your model. However, when requesting an explanation, you receive not only the predictions but also the feature <span class="No-Break">attribution information.</span></li>
			</ul>
			<p>It is important to note that feature attributions are primarily applicable to tabular data but also include built-in visualization capabilities for image data. This makes it easier to understand and interpret the attributions <span class="No-Break">more intuitively.</span></p>
			<ul>
				<li><strong class="bold">Example-based</strong>: Vertex AI utilizes nearest neighbor search to provide example-based explanations. This method involves finding the closest examples (usually from the training data) to the input and returning a list of the most similar <a id="_idIndexMarker524"/>examples. This approach leverages the principle that similar inputs are likely to produce similar predictions, allowing us to gain insight into the behavior of our model. By examining these similar examples, we can better understand and explain our <span class="No-Break">model’s output.</span></li>
			</ul>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor112"/>Feature-based explanation techniques available on Vertex AI</h2>
			<p>The <a id="_idIndexMarker525"/>following table shows different methods of feature-based <a id="_idIndexMarker526"/>explanations available <span class="No-Break">in GCP.</span></p>
			<table id="table001-5" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Method</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Compatible Vertex AI </strong><span class="No-Break"><strong class="bold">Model Resources</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Example </strong><span class="No-Break"><strong class="bold">Use Cases</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Sampled <span class="No-Break">Shapley (SHAP)</span></p>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li class="Table-Bullet-style">Any custom-trained model (running in any <span class="No-Break">prediction container)</span></li>
								<li class="Table-Bullet-style">AutoML <span class="No-Break">tabular models</span></li>
							</ul>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li>Classification and regression on <span class="No-Break">tabular data</span></li>
							</ul>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Integrated Gradients</span></p>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li class="Table-Bullet-style">Custom-trained TensorFlow models using TensorFlow prebuilt containers for <span class="No-Break">prediction serving</span></li>
								<li class="Table-Bullet-style">AutoML <span class="No-Break">image models</span></li>
							</ul>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li>Classification and regression on <span class="No-Break">tabular data</span></li>
								<li>Classification of <span class="No-Break">image data</span></li>
							</ul>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>XRAI (eXplanation with Ranked <span class="No-Break">Area Integrals)</span></p>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li class="Table-Bullet-style">Custom-trained TensorFlow models using TensorFlow prebuilt containers for <span class="No-Break">prediction serving</span></li>
								<li class="Table-Bullet-style">AutoML <span class="No-Break">image models</span></li>
							</ul>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li>Classification of <span class="No-Break">image data</span></li>
							</ul>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – Feature attribution methods available in GCP. Source: <a href="https://cloud.google.com/vertex-ai/docs/explainable-ai/overview">https://cloud.google.com/vertex-ai/docs/explainable-ai/overview</a></p>
			<p>Next, we will learn how to use these Vertex AI features to generate explanations for <span class="No-Break">model output.</span></p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor113"/>Using the model feature importance (SHAP-based) capability with AutoML for tabular data</h2>
			<p>In <a id="_idIndexMarker527"/>the following <a id="_idIndexMarker528"/>exercise, we will <a id="_idIndexMarker529"/>learn how to use XAI features in Vertex AI to evaluate feature importance in ML models for <span class="No-Break">structured data.</span></p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor114"/>Exercise 1</h2>
			<p>Objective: Use <a id="_idIndexMarker530"/>Vertex AutoML Tables’ <strong class="bold">Feature importance</strong> feature to explain global (model-level) and local (<span class="No-Break">sample-level) behavior</span></p>
			<p>Dataset to be used: <em class="italic">Bank Marketing Dataset</em> (Available in the <em class="italic">Chapter-8 </em>folder within this book’s <span class="No-Break">GitHub repository)</span></p>
			<p>Model objective: Predict whether the customer will open a new term deposit or not (<strong class="bold">Feature Label – </strong><span class="No-Break"><strong class="bold">deposit(Yes/No)</strong></span><span class="No-Break">)</span></p>
			<p>Follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Following <a id="_idIndexMarker531"/>the steps shown in <a href="B17792_05.xhtml#_idTextAnchor066"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, create an AutoML classification model to predict the probability of a customer opening the <span class="No-Break">term deposit.</span></li>
				<li>Once the model has been trained, navigate to <strong class="bold">Model Registry</strong> | <strong class="bold">Your model</strong> | <strong class="bold">Your model version</strong> (<em class="italic">1 for new models</em>) | the <span class="No-Break"><strong class="bold">Evaluate</strong></span><span class="No-Break"> tab.</span></li>
				<li>Scroll down to the <strong class="bold">Feature </strong><span class="No-Break"><strong class="bold">importance</strong></span><span class="No-Break"> section.</span></li>
				<li>The following <strong class="bold">Feature importance</strong> graph shows the relative feature importance of different <span class="No-Break">model features:</span></li>
			</ol>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B17792_08_06.jpg" alt="Figure 8.6 – Feature importance graph in Vertex AI AutoML Tables" width="784" height="862"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Feature importance graph in Vertex AI AutoML Tables</p>
			<p>As shown <a id="_idIndexMarker532"/>in the previous graph, the most important features in our model to predict whether a customer will respond to the outreach and open a new term deposit are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Duration</strong>: How long the customer has been with <span class="No-Break">the bank</span></li>
				<li><strong class="bold">Month</strong>: This could be because of the seasonality of <span class="No-Break">the business</span></li>
				<li><strong class="bold">Contact</strong> (Method – Cellular/Landline): This could be because of the different communication preferences of different types <span class="No-Break">of customers</span></li>
				<li><strong class="bold">Outcome</strong>: The outcome of the last promotional outreach to <span class="No-Break">this customer</span></li>
			</ul>
			<p>The following are the least important features as per the <span class="No-Break">preceding graph:</span></p>
			<ul>
				<li><strong class="bold">Default</strong>: Has the customer <span class="No-Break">defaulted before?</span></li>
				<li><span class="No-Break"><strong class="bold">Marital (Status)</strong></span></li>
				<li><strong class="bold">Education</strong>: Level of <span class="No-Break">education attained</span></li>
				<li><strong class="bold">Previous</strong>: The number of times contact was made before <span class="No-Break">this campaign</span></li>
			</ul>
			<p>The preceding information can help the data science team better understand the model’s behavior and possibly uncover additional insights into their data and customer behavior and provide important guidance for <span class="No-Break">future experiments.</span></p>
			<p>Although <a id="_idIndexMarker533"/>performing exploratory data analysis on the training dataset is beyond the scope of this book, for anyone interested, you can look at the notebook (<a href="B17792_08.xhtml#_idTextAnchor102"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic"> – ML Model Explainability – Exercise 1 Addendum</em>) for the correlation analysis between features and <span class="No-Break">predicted labels.</span></p>
			<p>Here are some insights we can draw from this feature <span class="No-Break">importance information:</span></p>
			<ul>
				<li><strong class="bold">Duration</strong>: As per correlation analysis in the notebook mentioned previously, duration has the strongest correlation with a predicted label (<strong class="source-inline">Deposit_Signup</strong>), which aligns with the duration being high in terms of feature importance in the <span class="No-Break">preceding graph.</span></li>
				<li><strong class="bold">Contact</strong>: Similarly, correlation analysis also shows that someone owning a cellular phone has a strong correlation with someone opening a deposit in response to <span class="No-Break">the campaign.</span></li>
				<li><strong class="bold">Outcome</strong>: Correlation analysis also shows that “Success” has a strong correlation with someone opening a deposit. This means that if someone responded positively to the last campaign, there is a stronger chance of that person being influenced by the <span class="No-Break">current campaign.</span></li>
				<li><strong class="bold">Months</strong>: In correlation analysis, we can also see that some months (March and May specifically) have a strong correlation with the positive outcome of the campaign. This could relate to AutoML surfacing months as an <span class="No-Break">important feature.</span></li>
			</ul>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor115"/>Exercise 2</h2>
			<p>Objective: Use Vertex AI’s feature attribution features to explain image <span class="No-Break">classification predictions</span></p>
			<p>Dataset <a id="_idIndexMarker534"/>to be <span class="No-Break">used: </span><span class="No-Break"><strong class="source-inline">Fast_Food_Classification_Dataset</strong></span></p>
			<p>Follow these steps to create the image <span class="No-Break">classification model:</span></p>
			<ol>
				<li>Download and unzip the dataset from <span class="No-Break">Kaggle: </span><a href="https://www.kaggle.com/datasets/utkarshsaxenadn/fast-food-classification-dataset"><span class="No-Break">https://www.kaggle.com/datasets/utkarshsaxenadn/fast-food-classification-dataset</span></a></li>
				<li>Following the steps shown in <a href="B17792_05.xhtml#_idTextAnchor066"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, create an AutoML image classification dataset using <strong class="source-inline">Fast_Food_Classification_Dataset</strong>. Make sure to select data type and objective as <strong class="bold">Image classification (Single-label)</strong>, <span class="No-Break">as shown.</span></li>
			</ol>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B17792_08_07.jpg" alt="Figure 8.7 – Model objective – Image classification (Single-label)" width="697" height="574"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Model objective – Image classification (Single-label)</p>
			<ol>
				<li value="3">Once the empty dataset is created, go to the <strong class="bold">Browse</strong> tab and add new labels for each food type you plan to include in your model. In this example, we uploaded <a id="_idIndexMarker535"/>our favorite fast foods, including burgers, donuts, hot dogs, and pizza, but feel free to use whatever food types you want <span class="No-Break">to use.</span></li>
			</ol>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B17792_08_08.jpg" alt="Figure 8.8 – Creating label names" width="894" height="685"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Creating label names</p>
			<ol>
				<li value="4">Now let’s upload the images of different fast-food types and annotate/label them. You don’t need all the available images in the dataset. Just 50 or so images for each food type <span class="No-Break">should suffice.</span><p class="list-inset">Repeat the following steps for each <span class="No-Break">food type:</span></p><ul><li>Upload images – Labeling images one at a time is difficult. To make labeling the images a bit easier, upload images for one food type at <span class="No-Break">a time.</span></li></ul></li>
			</ol>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B17792_08_09.jpg" alt="Figure 8.9 – Adding labels to images" width="1028" height="641"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – Adding labels to images</p>
			<p class="list-inset">As shown, once you have uploaded the images for a particular food item, you can <a id="_idIndexMarker536"/>click on <strong class="bold">Unlabeled</strong> and then <strong class="bold">Select All </strong>to select all the images that need to be labeled. If you upload and label one food type at a time, it ensures that you only select images of one type of food. If you upload all images at once, then clicking on the <strong class="bold">Unlabeled</strong> tab would end up selecting ALL unlabeled images, requiring you to manually select images of <span class="No-Break">one type.</span></p>
			<ol>
				<li value="5">Once the images have been selected, click on <strong class="bold">ASSIGN LABELS</strong> and select the right food type label. Then <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Save.</strong></span><p class="list-inset"><em class="italic">Do this process for all different </em><span class="No-Break"><em class="italic">food types</em></span><span class="No-Break">.</span></p></li>
				<li>Once all images are uploaded and labeled, navigate to the dataset in Vertex AI and go to the <span class="No-Break"><strong class="bold">Browse</strong></span><span class="No-Break"> tab.</span></li>
				<li>Click <strong class="bold">TRAIN </strong><span class="No-Break"><strong class="bold">NEW MODEL</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B17792_08_10.jpg" alt="Figure 8.10 – Initiating model training" width="942" height="681"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Initiating model training</p>
			<ol>
				<li value="8">On the <a id="_idIndexMarker537"/>next screen, select the dataset and annotation set you want to use for training the new model. Then, select the following options and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">CONTINUE</strong></span><span class="No-Break">:</span><p class="list-inset">Objective: <strong class="bold">Image </strong><span class="No-Break"><strong class="bold">classification</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">Single-label</strong></span><span class="No-Break">)</span></p><p class="list-inset">Model training <span class="No-Break">method: </span><span class="No-Break"><strong class="bold">AutoML</strong></span></p><p class="list-inset">Choose where to use the <span class="No-Break">model: </span><span class="No-Break"><strong class="bold">Cloud</strong></span></p></li>
			</ol>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B17792_08_11.jpg" alt="Figure 8.11 – Training configuration/options" width="816" height="446"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Training configuration/options</p>
			<ol>
				<li value="9">On the <a id="_idIndexMarker538"/>next screen, select <strong class="bold">TRAIN NEW MODEL</strong> and enter the name of the new model. You can leave all other options as is and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">CONTINUE</strong></span><span class="No-Break">.</span></li>
				<li>On the next screen, select <strong class="bold">Default</strong> as the goal and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">CONTINUE</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B17792_08_12.jpg" alt="Figure 8.12– Training configuration/options" width="1204" height="766"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12– Training configuration/options</p>
			<ol>
				<li value="11">On the <a id="_idIndexMarker539"/>next screen (the <strong class="bold">Explainability</strong> tab), check the <strong class="bold">Generate Explainable </strong><span class="No-Break"><strong class="bold">Bitmaps</strong></span><span class="No-Break"> option.</span></li>
				<li>On the <strong class="bold">Explainability</strong> tab, you can configure key parameters around the two available explainability options shown here (for this exercise, leave the default options as is and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">CONTINUE</strong></span><span class="No-Break">):</span><ul><li><span class="No-Break"><strong class="bold">Integrated Gradients</strong></span></li><li><strong class="bold"> </strong><span class="No-Break"><strong class="bold">XRAI</strong></span></li></ul><p class="list-inset">The following are the <span class="No-Break">available configurations:</span></p><ul><li><strong class="bold">Type</strong>: The type of visualization used, which can be either <strong class="bold">OUTLINES</strong> or <strong class="bold">PIXELS</strong>. This field is specific to Integrated Gradients and cannot be specified if you are using XRAI. By default, the type is set to <strong class="bold">OUTLINES</strong> for Integrated Gradients, which displays regions of attribution. To show per-pixel attribution, set the type <span class="No-Break">to </span><span class="No-Break"><strong class="bold">PIXELS</strong></span><span class="No-Break">.</span></li><li><strong class="bold">Polarity</strong> [<em class="italic">Not available in the UI. Can be used through the API.</em>]: Polarity indicates the orientation of the spotlighted attributions. By default, it’s set to <strong class="source-inline">positive,</strong> emphasizing regions with the highest positive influences. Essentially, it illuminates pixels that significantly contribute to a positive prediction by the model. Changing the polarity to <strong class="source-inline">negative</strong> will underscore areas, persuading the model away from predicting the positive class and aiding in pinpointing areas responsible for false negatives. There’s also an option to choose <strong class="source-inline">both,</strong> which provides a comprehensive view by displaying both positive and <span class="No-Break">negative attributions.</span></li><li><strong class="bold">Upper bound clip percentage</strong>: This setting omits attributions that exceed the defined percentile from the emphasized regions. When combined with other clip parameters, it helps in filtering out distractions, enabling clearer identification of areas with <span class="No-Break">strong attribution.</span></li><li><strong class="bold">Lower bound clip percentage</strong>: This configuration leaves out attributions falling below the indicated percentile, ensuring only significant regions <span class="No-Break">are highlighted.</span></li><li><strong class="bold">Color map</strong>: This refers to the color palette chosen for distinguishing highlighted zones. Integrated gradients typically use the <strong class="source-inline">pink_green</strong> default, where green signifies positive attributions and pink denotes negative ones. On the other hand, XRAI visualizations employ a gradient color scheme, with <strong class="source-inline">Viridis</strong> as the default. In this setup, the most impactful regions are <a id="_idIndexMarker540"/>bathed in yellow, while the less influential areas are shaded in blue. For an exhaustive list of available palettes, consult the <strong class="bold">Visualization</strong> message within the <span class="No-Break">API documentation.</span></li><li><strong class="bold">Overlay type</strong>: This setting defines how the original image is showcased within the visualization. Tweaking the overlay enhances visibility, especially when the inherent properties of the initial image obscure the <span class="No-Break">visualization details.</span></li><li><strong class="bold">Steps</strong>: The number of steps used to approximate the path integral can be specified here. It is recommended to start with a value of 50 and gradually increase it until the “sum to diff” property falls within the desired error range. The valid range for this value is inclusive between 1 <span class="No-Break">and 100:</span></li></ul></li>
			</ol>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B17792_08_13.jpg" alt="Figure 8.13 – Explainability configurations" width="543" height="557"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – Explainability configurations</p>
			<ul>
				<li>In the <strong class="bold">Compute and pricing</strong> tab, set the budget to 8 hours. This specifies the maximum amount of time the training <span class="No-Break">will run.</span></li>
			</ul>
			<ol>
				<li value="13">Click <span class="No-Break"><strong class="bold">START TRAINING</strong></span><span class="No-Break">.</span><p class="list-inset">(A coffee break would be too short, so maybe go prepare a 7-course meal and come back to check on the training status in a <span class="No-Break">few hours!)</span></p><p class="list-inset">Once the <a id="_idIndexMarker541"/>model training is complete, we need to deploy the model to a Vertex AI endpoint by following the <span class="No-Break">next steps.</span></p></li>
				<li>Navigate to <strong class="bold">Model Registry</strong> | <strong class="bold">Your model</strong> | <strong class="bold">Your model version</strong> (<em class="italic">1 for </em><span class="No-Break"><em class="italic">new models</em></span><span class="No-Break">).</span></li>
				<li>Navigate to the <strong class="bold">Deploy &amp; Test</strong> tab and click <strong class="bold">Deploy </strong><span class="No-Break"><strong class="bold">to endpoint</strong></span><span class="No-Break">.</span></li>
				<li>Enter the name of the endpoint and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">CONTINUE</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B17792_08_14.jpg" alt="Figure 8.14 – Model deployment options" width="523" height="490"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Model deployment options</p>
			<ol>
				<li value="17">In the <strong class="bold">Model settings</strong> tab, check <strong class="bold">Enable feature attributions for this model</strong> and then <a id="_idIndexMarker542"/>click the <strong class="bold">EDIT</strong> button underneath to open the <strong class="bold">Explainability </strong><span class="No-Break"><strong class="bold">options</strong></span><span class="No-Break"> menu:</span></li>
			</ol>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B17792_08_15.jpg" alt="Figure 8.15 – Deployment configuration for explainability" width="463" height="411"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – Deployment configuration for explainability</p>
			<ol>
				<li value="18">In the <strong class="bold">Explainability options</strong> menu, select the <strong class="bold">Integrated gradients</strong> option since we <a id="_idIndexMarker543"/>are first creating an endpoint to test the Integrated Gradients technique. <span class="No-Break">Click </span><span class="No-Break"><strong class="bold">DONE</strong></span><span class="No-Break">.</span></li>
				<li>Now, repeat these steps to create an endpoint for the <strong class="bold">XRAI</strong> explainability option. This time, suffix the name of the endpoint with XRAI, and on the <strong class="bold">Explainability options</strong> screen, <span class="No-Break">pick </span><span class="No-Break"><strong class="bold">XRAI</strong></span><span class="No-Break">.</span></li>
				<li>With that, two endpoints should have been created for <span class="No-Break">the model.</span></li>
			</ol>
			<p>Now, let’s test the model by uploading a sample image of donuts and evaluate the prediction and the explanation for the prediction returned by <span class="No-Break">the model:</span></p>
			<ol>
				<li>In the model’s <strong class="bold">Deploy &amp; Test</strong> tab, select the <strong class="bold">Integrated Gradients</strong> endpoint by clicking on <strong class="bold">Endpoint ID</strong>. Do not click on the endpoint’s name as that will take you to the endpoint’s <span class="No-Break">settings screen.</span></li>
				<li>Click <strong class="bold">Upload &amp; Explain</strong> and select an image you want <span class="No-Break">to test.</span></li>
				<li>Vertex AI will process the image and present the resulting classification of the image, along with <a id="_idIndexMarker544"/>an explanation (the image overlay will show areas of high importance for <span class="No-Break">the image):</span></li>
			</ol>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B17792_08_16.jpg" alt="Figure 8.16 – Uploaded image" width="974" height="548"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Uploaded image</p>
			<p class="list-inset">The following screenshot shows the class prediction based on the ML model, along with the Integrated Gradients-based explanation generated by Vertex AI. The explanation image showcases the key areas/pixels in the image that helped the model make the final decision that this is an image of <span class="No-Break">a donut:</span></p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B17792_08_17.jpg" alt=" Figure 8.17 – Resulting Integrated Gradients explanation and predicted class" width="949" height="541"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 8.17 – Resulting Integrated Gradients explanation and predicted class</p>
			<ol>
				<li value="4">You can <a id="_idIndexMarker545"/>repeat this step with the XRAI endpoint to get explanations using the <span class="No-Break">XRAI technique:</span></li>
			</ol>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B17792_08_18.jpg" alt="Figure 8.18 – XRAI explanation" width="1232" height="347"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.18 – XRAI explanation</p>
			<p class="list-inset">As you can see, with both the explanation images, which were generated through the Integrated Gradients technique and XRAI technique, the areas/pixels close to the location of donuts in the image are highlighted, and the model seems to be focusing on the <span class="No-Break">correct areas.</span></p>
			<p>Now let’s look at example-based explanations where instead of explaining the results based <a id="_idIndexMarker546"/>on the features of the input instance, we instead try to explain the results by looking at the examples in the dataset that are similar to the <span class="No-Break">input instance.</span></p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor116"/>Example-based explanations</h2>
			<p>Vertex AI’s example-based explanation feature uses nearest-neighbor search algorithms to find the <a id="_idIndexMarker547"/>closest match to a sample. Essentially, when given an input, Vertex AI identifies and provides a set of examples, typically originating from the training data, that closely resemble the given input. This feature is rooted in the common expectation that inputs with similar attributes will lead to corresponding predictions. Therefore, these identified examples serve as an intuitive way to comprehend and elucidate the workings and decisions of <span class="No-Break">our model.</span></p>
			<p>This method can be extremely helpful in the <span class="No-Break">following scenarios:</span></p>
			<ul>
				<li><strong class="bold">Identifying mislabeled examples</strong>: If the solution locates data samples or embeddings that are close together in the vector space, but have different labels, then there is a possibility that the data sample <span class="No-Break">is mislabeled.</span></li>
				<li><strong class="bold">Decision support</strong>: If the predicted labels for new data points are similar to the ground truth labels of other data points appearing close to the new data points in the vector space, then that can help confirm the validity <span class="No-Break">of predictions.</span></li>
				<li><strong class="bold">Active learning</strong>: In the vector space, you can identify the unlabeled samples appearing close to the labeled samples and add them to the training data with the label of the <span class="No-Break">nearby samples.</span></li>
			</ul>
			<p>The functionality of example-based explanations in Vertex AI can be leveraged by any model offering an embedding – a latent representation – for its inputs. This means that the model should be able to convert the input data into a set of relevant features or vectors in a latent space. This excludes certain types of models, such as tree-based models such as decision trees, from being supported due to their inherent nature of not creating these <span class="No-Break">latent spaces.</span></p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor117"/>Key steps to use example-based explanations</h2>
			<p>Here are <a id="_idIndexMarker548"/>the <span class="No-Break">key steps:</span></p>
			<ol>
				<li>Enable explanations during model creation: Begin by creating a model with explanations enabled and uploading it to Vertex AI. When you create/import a model, you can set a default configuration for all its explanations using the model’s <span class="No-Break"><strong class="source-inline">explanationSpec</strong></span><span class="No-Break"> field.</span><p class="list-inset">To facilitate the generation of example-based explanations, certain criteria should be met by your model. Two potential scenarios exist <span class="No-Break">for this:</span></p><ul><li>You can <a id="_idIndexMarker549"/>implement a <strong class="bold">deep neural network</strong> (<strong class="bold">DNN</strong>) model, in which case the name of a specific layer or signature should be provided. The output of this layer or signature is then utilized as the <span class="No-Break">latent space.</span></li><li>Alternatively, the model could be designed to directly output embeddings, thus serving as a representation of the <span class="No-Break">latent space.</span></li></ul><p class="list-inset">This latent space is integral to the process as it houses the example representations that are instrumental in <span class="No-Break">generating explanations.</span></p></li>
				<li>Deploy the model to an endpoint: Next, create an endpoint resource and deploy your model to it, establishing an accessible channel <span class="No-Break">for interaction.</span></li>
				<li>Explore the generated explanations: Finally, issue explanation requests to the deployed model and scrutinize the provided explanations to understand your model’s <span class="No-Break">decision-making processes.</span></li>
			</ol>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor118"/>Exercise 3</h2>
			<p>Custom <a id="_idIndexMarker550"/>train an image classification model to generate real-time predictions and provide example-based explanations – see <em class="italic">Notebook 8.3 – Implementing example-based explanations with Vertex </em><span class="No-Break"><em class="italic">AI</em></span><span class="No-Break"> (</span><a href="https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter08/Chapter8_Explainable_AI_example_based.ipynb"><span class="No-Break">https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter08/Chapter8_Explainable_AI_example_based.ipynb</span></a><span class="No-Break">)</span></p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor119"/>Summary</h1>
			<p>In this chapter, we delved into the world of XAI and its relevance in modern MLOps. We discussed how XAI aids in building trust, ensuring regulatory compliance, debugging and improving models, and addressing <span class="No-Break">ethical considerations.</span></p>
			<p>We explored different explanation techniques for various types of data, including tabular, image, and text data. Techniques such as LIME, SHAP, permutation feature importance, and others were discussed for tabular data. For image data, methods such as Integrated Gradients and XRAI were explained, while text-specific LIME was presented for <span class="No-Break">text data.</span></p>
			<p>This chapter also provided an overview of the XAI features available in GCP, including both feature-based and <span class="No-Break">example-based explanations.</span></p>
			<p>At this point, you should have gained a good understanding of XAI, its importance, various techniques, and practical applications in the context of Vertex AI. As the field of AI continues to evolve, the role of XAI in creating transparent, trustworthy, and fair ML models will only grow. As MLOps practitioners, having these skills will be crucial in leading ethical and responsible <span class="No-Break">AI adoption.</span></p>
			<p>In the next chapter, we will go over various Vertex AI tools that can help you iterate through model hyperparameters to improve the performance of your <span class="No-Break">ML solutions.</span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor120"/>References</h1>
			<p><a href="https://cloud.google.com/vertex-ai/docs/explainable-ai/overview"><span class="No-Break">https://cloud.google.com/vertex-ai/docs/explainable-ai/overview</span></a></p>
			<p>Munn, Michael; Pitman, David. <em class="italic">Explainable AI for Practitioners</em>. <span class="No-Break">O’Reilly Media.</span></p>
		</div>
	</div>
</div>
</body></html>