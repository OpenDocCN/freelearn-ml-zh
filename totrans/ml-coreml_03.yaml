- en: Recognizing Objects in the World
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will immerse ourselves in the world of **machine learning**
    (**ML**) and Core ML by working through what could be considered the 101 Core
    ML application. We will be using an image classification model to allow the user
    to point their iPhone at anything and have the app classify the most dominant
    object in the view.
  prefs: []
  type: TYPE_NORMAL
- en: We will start off by first discussing the concept of **convolutional neural
    networks** (**ConvNets** or **CNNs**), a category of neural networks well suited
    to image classification, before jumping into implementation. Starting from a skeleton
    project, you will soon discover just how easy it is to integrate ML into your
    apps with the help of Core ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Gaining some intuition on how machines understand images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building out the example application for this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capturing photo frames and preprocessing them before passing them to the Core
    ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Core ML model to perform inference and interpreting the result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks are commonly referred to as either CNNs or ConvNets,
    and these terms are used interchangeably throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, it's not my intention to give you a theoretical or
    deep understanding of any particular ML algorithm, but rather gently introduce
    you to some of the main concepts. This will help you to gain an intuitive understanding
    of how they work so that you know where and how to apply them, as well as give
    you a platform to dive deeper into the particular subject, which I strongly encourage
    you to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a good introductory text on deep learning, I strongly recommend Andrew
    Trask''s book *Grokking Deep Learning*. For a general introduction to ML, I would
    recommend Toby Segaran''s book *Programming Collective Intelligence: Building
    Smart Web 2.0 Applications*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will be introducing CNNs, specifically introducing what
    they are and why they are well suited for spatial data, that is, images. But before
    discussing CNNs, we will start by inspecting the data; then we'll see why CNNs
    perform better than their counterpart, fully connected neural networks (or just neural
    networks).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of illustrating these concepts, consider the task of classifying
    the following digits, where each digit is represented as a 5 x 5 matrix of pixels.
    The dark gray pixels have a value of 1 and light gray pixels have a value of 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b143785-9e1f-4f17-b091-8bffa35774ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using a fully connected neural network (single hidden layer), our model would
    learn the joint probability of each pixel with respect to their associated label;
    that is, the model will assign positive weights to pixels that correlate with
    the label and using the output with the highest likelihood to be the most probable
    label. During training, we take each image and flatten it before feeding into
    our network, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c2c3b67-caff-406a-94c2-042feea11f35.png)'
  prefs: []
  type: TYPE_IMG
- en: This works remarkably well, and if you have experience with ML, particularly
    deep learning, you would have likely come across the MNIST dataset. It's a dataset
    consisting of labeled handwritten digits, where each digit is centrally rendered
    to a 28 x 28 gray scale (single channel with the pixel value ranging from 0-255)
    image. Using a single-layer fully connected network will likely result in a validation
    accuracy close to 90%. But what happens if we introduce some complexities such
    as moving the image around a larger space, as illustrated in the following diagram?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36c99f3d-9e48-4a1c-8eba-eba8cda49949.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The fully connected network has no concept of space or local relationships;
    in this case, the model would need to learn all variants of each digit at each
    possible location. To further emphasize the importance of being able to capture
    the relationship of spatial data, consider the need to learn more complex images,
    such as classifying dogs and cats using a network that discards 2D information.
    Individual pixels alone are unable to portray complex shapes such as eyes, a nose,
    or ears; it''s only when you consider neighboring pixels that you can describe
    these more complex shapes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb9e8cad-f0c3-4b80-988d-ffaa5431a611.png)'
  prefs: []
  type: TYPE_IMG
- en: Images taken from the Kaggle competition cats vs dogs (https://www.kaggle.com/c/dogs-vs-cats)
  prefs: []
  type: TYPE_NORMAL
- en: 'We need something that can abstract away from the raw pixels, something that
    can describe images using high-level features. Let''s return to our digits dataset
    and investigate how we might go about extracting higher-level features for the
    task of classification. As alluded to in an earlier example, we need a set of
    features that abstracts away from the raw pixels, is unaffected by position, and
    preserves 2D spatial information. If you''re familiar with image processing, or
    even image processing tools, you would have most probably come across the idea
    and results of **edge detection** or **edge filters**; in simplest terms, these
    work by passing a set of kernels across the whole image, where the output is the
    image with its edges emphasized. Let''s see how this looks diagrammatically. First,
    we have our set of kernels; each one extracts a specific feature of the image,
    such as the presence of horizontal edges, vertical edges, or edges at a 45 degree
    angle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8861c305-175e-4535-ba4e-b42c9f240a96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For each of these filters, we pass them over our image, extracting each of
    the features; to help illustrate this, let''s take one digit and pass the vertical
    kernel over it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c152549c-5150-4a50-a20d-f08a457765c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As illustrated in the previous diagram, we slide the horizontal kernel across
    the image, producing a new image using the values of the image and kernel. We
    continue until we have reached the bounds of the image, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2d32e91-c97a-42ac-901c-9d35a04a902a.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of this is a map showing the presence of vertical lines detected
    within the image. Using this and the other kernels, we can now describe each class
    by its dominant gradients rather than using pixel positions. This higher level
    abstraction allows us to recognize classes independent of their location as well
    as describe more complex objects.
  prefs: []
  type: TYPE_NORMAL
- en: Two useful things to be aware of when dealing with kernels are the **stride**
    **value** and **padding**. Strides determines how large your step size is when
    sliding your kernel across the image. In the preceding example, our stride is
    set to 1; that is, we're sliding only by a single value. Padding refers to how
    you deal with the boundaries; here, we are using **valid**, where we only process
    pixels within valid ranges. **same **would mean adding a border around the image
    to ensure that the output remains the same size as the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we have performed here is known as **feature engineering** and something
    neural networks perform automatically; in particular, this is what CNNs do. They
    create a series of kernels (or convolution matrices) that are used to convolve
    the image to extract local features from neighboring pixels. Unlike our previous
    engineered example, these kernels are learned during training. Because they are
    learned automatically, we can afford to create many filters that can extract granular nuances
    of the image as well, allowing us to effectively stack convolution layers on top
    of each other. This allows for increasingly higher levels of abstraction to learn. For
    example, your first layer may learn to detect simple edges, and your second layer
    (operating on the previous extracted features) may learn to extract simple shapes.
    The deeper we go, the higher the level achieved by our features, as illustrated
    in the diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89d3eafc-e01f-4b41-91fe-cc6d32977f97.png)'
  prefs: []
  type: TYPE_IMG
- en: And there we have it! An architecture capable of understanding the world by
    learning features and layers of abstraction to efficiently describe it. Let's
    now put this into practice using a pretrained model and Core ML to get our phone
    to recognize the objects it sees.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing objects in the world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To recap, our goal in this chapter is to create an application that will recognize what
    it sees. We will start by first capturing video frames, prepare these frames for
    our model, and finally feed them into a Core ML model to perform inference. Let's
    get started.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you haven''t done it already, download the latest code from the accompanying
    repository: [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter3/Start/ObjectRecognition/`
    and open the project `ObjectRecognition.xcodeproj`. Once loaded, you will see
    the skeleton project for this chapter, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f875374a-88c0-44b2-a401-c61345016010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To help you navigate around the project, here is a list of core files/classes
    and their main functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`VideoCapture` will be responsible for the management and handling of the camera,
    including capturing video frames'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CaptureVideoPreviewView.swift` contains the class `CapturePreviewView`, which
    will be used to present the captured frames'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CIImage` provides convenient extensions to the class `CIImage`, used for preparing
    the frame for the Core ML model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VideoController`, as you would expect, is the controller for the application
    and is responsible for interfacing with the imported Core ML model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be making changes to each of these in the following sections in order
    to realize the desired functionality. Our first task will be to get access to
    the camera and start capturing frames; to do this, we will be making use of Apple's
    iOS frameworks **AVFoundation** and **CoreVideo**.
  prefs: []
  type: TYPE_NORMAL
- en: The AVFoundation framework encompasses classes for handing capturing, processing,
    synthesizing, controlling, importing, and exporting of audiovisual media on iOS
    and other platforms. In this chapter, we are most interested in a subset of this
    framework for dealing with cameras and media capture, but you can learn more about
    the AVFoundation framework on Apple's official documentation site at [https://developer.apple.com/documentation/avfoundation](https://developer.apple.com/documentation/avfoundation).
  prefs: []
  type: TYPE_NORMAL
- en: CoreVideo provides a pipeline-based API for manipulating digital videos, capable
    of accelerating the process using support from both Metal and OpenGL.
  prefs: []
  type: TYPE_NORMAL
- en: We will designate the responsibility of setting up and capturing frames from
    the camera to the class `VideoCapture`; let's jump into the code now. Select `VideoCapture.swift`
    from the left-hand side panel to open in the editing window. Before making amendments,
    let's inspect what is already there and what's left to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top of the class, we have the protocol `VideoCaptureDelegate` defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`VideoCapture` will pass through the captured frames to a registered delegate,
    thus allowing the `VideoCapture` class to focus solely on the task of capturing
    the frames. What we pass to the delegate is a reference to itself, the image data
    (captured frame) of type `CVPixelBuffer` and the timestamp as type `CMTime`. `CVPixelBuffer`
    is a CoreVideo data structure specifically for holding pixel data, and the data
    structure our Core ML model is expecting (which we''ll see in a short while).
    `CMTTime` is just a struct for encapsulating a timestamp, which we''ll obtain
    directly from the video frame.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the protocol, we have the skeleton of our `VideoCapture` class. We will
    be walking through it in this section, along with an extension to implement the
    `AVCaptureVideoDataOutputSampleBufferDelegate` protocol, which we will use to
    capture frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Most of this should be self-explanatory, so I will only highlight the not-so-obvious
    parts, starting with the variables `fps` and `lastTimestamp`. We use these together
    to throttle how quickly we pass frames back to the delegate; we do this as it's
    our assumption that we capture frames far quicker than we can process them. And
    to avoid having our camera lag or jump, we explicitly limit how quickly we pass
    frames to the delegate. **Frames per second** (**fps**) sets this frequency while
    `lastTimestamp` is used in conjunction to calculate the elapsed time since the
    last processing of a frame.
  prefs: []
  type: TYPE_NORMAL
- en: The only other part of the code I will highlight here is the `asyncStartCapturing`
    and `asyncStopCapturing` methods; these methods, as the names imply, are responsible
    for starting and stopping the capture session respectively. Because they both
    will be using blocking methods, which can take some time, we will dispatch the
    task off the main thread to avoid blocking it and affecting the user's experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have the extension; it implements the `AVCaptureVideoDataOutputSampleBufferDelegate`
    protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will discuss the details shortly, but essentially it is the delegate that
    we assign to the camera for handling incoming frames of the camera. We will then
    proxy it through to the `VideoCaptureDelegate` delegate assigned to this class.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now walk through implementing the methods of this class, starting with
    `initCamera`. In this method, we want to set up the pipeline that will grab the
    frames from the physical camera of the device and pass them onto our delegate
    method. We do this by first getting a reference to the physical camera and then
    wrapping it in an instance of the `AVCaptureDeviceInput` class, which takes care
    of managing the connection and communication with the physical camera. Finally,
    we add a destination for the frames, which is where we use an instance of `AVCaptureVideoDataOutput`,
    assigning ourselves as the delegate for receiving these frames. This pipeline
    is wrapped in something called `AVCaptureSession`, which is responsible for coordinating
    and managing this pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now define some instance variables we''ll need; inside the class `VideoCapture`,
    add the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We mentioned the purpose of `captureSession` previously, but also introduced
    a `DispatchQueue`. When adding a delegate to `AVCaptureVideoDataOutput` (for handling
    the arrival of new frames), you also pass in a `DispatchQueue`; this allows you
    to control which queue the frames are managed on. For our example, we will be
    handling the processing of the images off the main thread so as to avoid impacting
    the performance of the user interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our instance variables now declared, we will turn our attention to the
    `initCamera` method, breaking it down into small snippets of code. Add the following
    within the body of the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We signal to the `captureSession` that we want to batch multiple configurations
    by calling the method `beginConfiguration`; these changes won''t be made until
    we commit them by calling the session''s `commitConfiguration` method. Then, in
    the next line of code, we set the desired quality level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the next snippet, we obtain the physical device; here, we are obtaining the
    default device capable of recording video, but you can just as easily search for
    one with specific capabilities, such as the front camera. After successfully obtaining
    the device, we wrap it in an instance of `AVCaptureDeviceInput` that will be responsible
    for capturing data from the physical camera and finally adding it to the session.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have to add the destination for these frames; again, add the following
    snippet to the `initCamera` method where you left off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code snippet, we create, set up, and added our output. We start
    by instantiating an instance of `AVCaptureVideoDataOutput`, before defining what
    data we want. Here, we are requesting full color (`kCVPixelFormatType_32BGRA`),
    but depending on your model, it may be more efficient to request images in grayscale
    (`kCVPixelFormatType_8IndexedGray_WhiteIsZero`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting `alwaysDiscardsLateVideoFrames` to true means any frames that arrive
    while the dispatch queue is busy will be discarded—a desirable feature for our
    example. We then assign ourselves along with our dedicated dispatch queue as the
    delegate for handing incoming frames using the method `videoOutput.setSampleBufferDelegate(self,
    queue: sessionQueue)`. Once we have configured our output, we are ready to add
    it to our session as part of our configuration request. To prevent our images
    from being rotated by 90 degrees, we then request that our images are in portrait
    orientation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the final statement to commit these configurations; it''s only after we
    do this that these changes will take effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This now completes our `initCamera` method; let''s swiftly (excuse the pun)
    move onto the methods responsible for starting and stopping this session. Add
    the following code to the body of the `asyncStartCapturing` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned previously, the `startRunning` and `stopRunning` methods both block
    the main thread and can take some time to complete; for this reason, we execute
    them off the main thread, again to avoid affecting the responsiveness of the user
    interface. Invoking `startRunning` will start the flow of data from the subscribed
    inputs (camera) to the subscribed outputs (delegate).
  prefs: []
  type: TYPE_NORMAL
- en: Errors, if any, are reported through the notification `AVCaptureSessionRuntimeError`.
    You can subscribe to listen to it using the default `NotificationCenter`. Similarly,
    you can subscribe to listen when the session starts and stops with the notifications
    `AVCaptureSessionDidStartRunning` and `AVCaptureSessionDidStopRunning`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, add the following code to the method `asyncStopCapturing`, which
    will be responsible for stopping the current session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Within the `initCamera` method, we subscribed ourselves as the delegate to
    handle arriving frames using the statement `videoOutput.setSampleBufferDelegate(self,
    queue: sessionQueue)`; let''s now turn our attention to handling this. As you
    may recall, we included an extension of the `VideoCapture` class to implement
    the `AVCaptureVideoDataOutputSampleBufferDelegate` protocol within the `captureOutput`
    method. Add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Before walking through this code snippet, it's worth mentioning what parameters
    this method is passed and how we use them. The first parameter, `output`, is of the
    type `AVCaptureVideoDataOutput` and references the associated output that this
    frame originated from. The next parameter, `sampleBuffer`, is of the type `CMSampleBuffer` and
    this is what we will use to access data of the current frame. Along with the frames,
    the duration, format, and timestamp associated with each frame can also be obtained.
    The final parameter, `connection`, is of the type `AVCaptureConnection` and provides
    a reference to the connection associated with the received frame.
  prefs: []
  type: TYPE_NORMAL
- en: Now, walking through the code, we start by guarding against any occurrences
    where no delegate is assigned, and returning early if so. Then we determine whether
    enough time has elapsed since the last time we processed a frame, remembering
    that we are throttling how frequently we process a frame to ensure a seamless
    experience. Here, instead of using the systems clock, we obtain the time associated
    with the latest frame via the statement `let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)`;
    this ensures that we are measuring against the relative time with respect to the
    frame rather than absolute time of the system. Given that enough time has passed,
    we proceed to get a reference to the sample's image buffer via the statement `CMSampleBufferGetImageBuffer(sampleBuffer)`,
    finally passing it over to the assigned delegate.
  prefs: []
  type: TYPE_NORMAL
- en: 'This now completes our `VideoCapture` class; let''s move on to hooking it up
    to our view using the `ViewController`. But before jumping into the code, let''s
    inspect the interface via the storyboard to better understand where we''ll be
    presenting the video stream. Within Xcode, select `Main.storyboard` from the Project
    Navigator panel on the left to open up interface builder; when opened, you will
    be presented with a layout similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8c13e02-eba6-4a0c-8ea0-92e3cf37c079.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Nothing complicated; we have a label to present our results and a view to render
    our video frames onto. If you select the VideoPreview view and inspect the class
    assigned to it, you will see we have a custom class to handle the rendering called,
    appropriately, CapturePreviewView. Let''s jump into the code for this class and
    make the necessary changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Fortunately, `AVFoundation` makes available a subclass of `CALayer` specifically
    for rendering frames from the camera; all that remains for us to do is to override
    the view''s `layerClass` property and return the appropriate class. Add the following
    code to the `CapturePreviewView` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This method is called early during the creation of the view and is used to
    determine what `CALayer` to instantiate and associate with this view. As previously
    mentioned, the `AVCaptureVideoPreviewLayer` is—as the name suggests—specifically
    for handling video frames. In order to get the frames rendered, we simply assign
    `AVCaptureSession` with the `AVCaptureVideoPreviewLayer.session` property. Let''s
    do that now; first open up the `ViewController` class in Xcode and add the following
    variable (in bold):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `previewView` and `classifiedLabel` are existing variables associated with
    the interface via the Interface Builder. Here, we are creating an instance of
    `VideoCapture`, which we had implemented earlier. Next, we will set up and start
    the camera using the `VideoCapture` instance, before assigning the session to
    our `previewView` layer. Add the following code within the `ViewDidLoad` method
    under the statement `super.viewDidLoad()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Most of the code should look familiar to you as a lot of it is using the methods
    we have just implemented. First we initialize the camera, calling the `initCamera`
    method of the `VideoCamera` class. Then, if successful, we assign the created
    `AVCaptureSession` to the layer's session. We also hint to the layer how we want
    it to handle the content, in this case filling the screen whilst respecting its
    aspect ratio. Finally, we start the camera by calling `videoCapture.asyncStartCapturing()`.
  prefs: []
  type: TYPE_NORMAL
- en: With that now completed, it's a good time to test that everything is working
    correctly. If you build and deploy on an iOS 11+ device, you should see the video
    frames being rendered on your phone's screen.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will walk through how to capture and process them for
    our model before performing inference (recognition).
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this stage, we have the app rendering the frames from the camera, but we
    are not yet receiving any frames. To do this, we will assign ourselves to receive
    these frames, as implemented in the previous section. The existing `ViewController`
    class already has an extension implementing the `VideoCaptureDelegate` protocol.
    What''s left to do is to assign ourselves as the delegate of the `VideoCapture`
    instance and implement the details of the callback method; the following is the
    code for `extension`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Depending on your coding style, you can just as easily implement the protocols
    inside the main class. I tend to make use of extensions to implement the protocols—a
    personal preference.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s assign ourselves as the delegate to start receiving the frames;
    within the `ViewDidLoad` method of the `ViewController` class, we add the following
    statement just before we initialize the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have assigned ourselves as the delegate, we will receive frames
    (at the defined frame rate) via the callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It's within this method that we will prepare and feed the data to the model
    to classify the dominant object within the frame. What the model is expecting
    is dependent on the model, so to get a better idea of what we need to pass it,
    let's download the trained model we will be using for this example and import
    it into our project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trained models can be obtained from a variety of sources; in some instances,
    you will need to convert them, and in other cases, you will need to train the
    model yourself. But in this instance, we can make use of the models Apple has
    made available; open up your web browser and navigate to [https://developer.apple.com/machine-learning/](https://developer.apple.com/machine-learning/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b03ed353-ba5c-4d37-aa90-c54468fc8831.png)'
  prefs: []
  type: TYPE_IMG
- en: You will be taken to a web page where Apple has made available a range of pretrained
    and converted models. Conveniently, most of the available models are specifically
    for object classification; given our use case, we're particularly interested in
    the models trained on a large array of objects. Our options include MobileNet, SqueezeNet, ResNet50, Inception
    v3, and VGG16\. Most of these have been trained on the ImageNet dataset, a dataset
    with reference to over 10 million URLs' images that have been manually assigned
    to one of 1,000 classes. References to the original research papers and performance
    can be obtained via the View original model details link. For this example, we'll
    use Inception v3, a good balance between size and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are using the Inception v3 model, but the effort to swap the model
    is minimal; it requires updating the references as the generated classes are prefixed
    with the model's name, as you will soon see, and ensuring that you are conforming
    to the expected inputs of the model (which can be alleviated by using the Vision
    framework, as you will see in future chapters).
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the Download Core ML Model link to proceed to download and, once downloaded,
    drag the `Inceptionv3.mlmodel` file onto the Project Navigator panel on the left
    of Xcode, checking Copy items if needed if desired or else leaving everything
    as default. Select the `Inceptionv3.mlmodel` file from the Project Navigator panel
    on the left to bring up the details within the Editor area, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7672f9b-67a1-4755-befe-e69d230b3937.png)'
  prefs: []
  type: TYPE_IMG
- en: It is important to ensure that the model is correctly assigned to the appropriate
    target; in this example, this means verifying that the ObjectRecognition target
    is checked, as seen here on the Utilities panel to the right. Also worth noting
    are the expected inputs and outputs of the model. Here, the model is expecting
    a color image of size 299 x 299 for its input, and it returns a single class label
    as a string and a dictionary of string-double pairs of probabilities of all the
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a `.mlmodel` file is imported, Xcode will generate a wrapper for the model
    itself and the input and output parameters to interface with the model; this is
    illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db3c0a61-38ff-4a49-9f2c-66eb98f65aa9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can easily access this by tapping on the arrow button next to the `Inceptionv3`
    label within the Model Class section; when tapped, you will see the following
    code (separated into three distinct blocks to make it more legible):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The first block of the preceding code is the input for our model. This class
    implements the `MLFeatureProvider` protocol, a protocol representing a collection
    of feature values for the model, in this case, the image feature. Here, you can
    see the expected data structure, `CVPixelBuffer`, along with the specifics declared
    (handily) in the comments. Let''s continue on with our inspection of the generated
    classes by looking at the binding for the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As previously mentioned, the output exposes a directory of probabilities and
    a string for the dominated class, each exposed as properties or accessible using
    the getter method `featureValue(for featureName: String)` by passing in the feature''s
    name. Our final extract for the generated code is the model itself; let''s inspect
    that now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This class wraps the model class and provides strongly typed methods for performing
    inference via the `prediction(input: Inceptionv3Input)` and `prediction(image:
    CVPixelBuffer)` methods, each returning the output class we saw previously—`Inceptionv3Output`.
    Now, knowing what our model is expecting, let''s continue to implement the preprocessing
    functionality required for the captured frames in order to feed them into the
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Core ML 2 introduced a the ability to work with batches; if your model was
    compiled with Xcode 10+ then you will also see the additional method `<CODE>func
    predictions(from: MLBatchProvider, options: MLPredictionOptions)</CODE>` allowing
    you to perform inference on a batch of inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we know that we are receiving the correct data type (`CVPixelBuffer`)
    and image format (explicitly defined in the settings when configuring the capture
    video output instance `kCVPixelFormatType_32BGRA`) from the camera. But we are
    receiving an image significantly larger than the expected size of 299 x 299\.
    Our next task will be to create some utility methods to perform resizing and cropping.
  prefs: []
  type: TYPE_NORMAL
- en: For this, we will be extending `CIImage` to wrap and process the pixel data
    we receive along with making use of `CIContext` to obtain the raw pixels again.
    If you're unfamiliar with the CoreImage framework, then it suffices to say that
    it is a framework dedicated to efficiently processing and analyzing images. `CIImage` can
    be considered the base data object of this framework that is often used in conjunction
    with other CoreImage classes such as `CIFilter`, `CIContext`, `CIVector`, and
    `CIColor`. Here, we are interested in `CIImage` as it provides convenient methods
    for manipulating images along with `CIContext` to extract the raw pixel data from
    `CIImage` (`CVPixelBuffer`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Back in Xcode, select the `CIImage.swift` file from the Project navigator to
    open it up in the Editor area. In this file, we have extended the `CIImage` class
    with a method responsible for rescaling and another for returning the raw pixels (`CVPixelBuffer`),
    a format required for our Core ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start by implementing the `resize` method; this method is passed in
    the desired size, which we''ll use to calculate the relative scale; then we''ll
    use this to scale the image uniformly. Add the following code snippet to the `resize`
    method, replacing the `fatalError("Not implemented")` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Unless the image is a square, we are likely to have an overflow either vertically
    or horizontally. To handle this, we will simply center the image and crop it to
    the desired size; do this by appending the following code to the `resize` method
    (beneath the code written in the preceding snippet):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have the functionality to rescale the image; our next piece of functionality
    is to obtain a `CVPixelBuffer` from the `CIImage`. Let''s do that by implementing
    the body of the `toPixelBuffer` method. Let''s first review the method''s signature
    and then briefly talk about the functionality required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This method is expecting a `CIContext` and flag indicating whether the image
    should be grayscale (single channel) or full color; `CIContext` will be used to
    render the image to a pixel buffer (our `CVPixelBuffer`). Let's now flesh out
    the implementation for `toPixelBuffer` piece by piece.
  prefs: []
  type: TYPE_NORMAL
- en: The preprocessing required on the image (resizing, grayscaling, and normalization)
    is dependent on the Core ML model and the data it was trained on. You can get
    a sense of these parameters by inspecting the Core ML model in Xcode. If you recall,
    the expected input to our model is (image color 299 x 299); this tells us that
    the Core ML model is expecting the image to be color (three channels) and 299
    x 299 in size.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating the pixel buffer we will be rendering our image to; add
    the following code snippet to the body of the `toPixelBuffer` method, replacing
    the `fatalError("Not implemented")` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We first create an array to hold the attributes defining the compatibility requirements
    for our pixel buffer; here, we specify that we want our pixel buffer to be compatible
    with `CGImage` types (`kCVPixelBufferCGImageCompatibilityKey`) and compatible
    with CoreGraphics bitmap contexts (`kCVPixelBufferCGBitmapContextCompatibilityKey`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We then proceed to create a pixel buffer, passing in our compatibility attributes,
    the format (either grayscale or full color depending on the value of `gray`),
    width, height, and pointer to the variable. Next, we unwrap the nullable pixel
    buffer as well as ensure that the call was successful; if either of these is `false`,
    we return `NULL`. Otherwise, we''re ready to render our `CIImage` into the newly
    created pixel buffer. Append the following code to the `toPixelBuffer` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Before drawing, we lock the address of the pixel buffer via `CVPixelBufferLockBaseAddress`
    and then unlock once we've finished using the `CVPixelBufferUnlockBaseAddress`
    method. We are required to do this when accessing pixel data from the CPU, which
    we are doing here.
  prefs: []
  type: TYPE_NORMAL
- en: Once locked, we simply use the `CIContext` to render the scaled image to the
    buffer, passing in the destination rectangle (in this case, the full size of the
    pixel buffer) and destination color space, which is full color or grayscale depending
    on the value of `gray` as mentioned previously. After unlocking the pixel buffer,
    as described earlier, we return our newly created pixel buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now extended the `CIImage` with two convenient methods, one responsible
    for rescaling and the other for creating a pixel buffer representation of itself.
    We will now return to the `ViewController` class to handle the preprocessing steps
    required before passing our data into the model. Select the `ViewController.swift`
    file from the Projector navigator panel within Xcode to bring up the source code,
    and within the body of the `ViewController` class, add the following variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As previously discussed, we will be passing this to our `CIImage.toPixelBuffer`
    method for rendering the image to the pixel buffer. Now return to the `onFrameCaptured`
    method and add the following code, to make use of the methods we''ve just created
    for preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We first unwrap the `pixelBuffer`, returning if it is `NULL`; then we create
    an instance of `CIImage`, passing in the current frame and then chaining our extension
    methods to perform rescaling (299 x 299) and rendering out to a pixel buffer (setting
    the gray parameter to false as the model is expecting full color images). If successful,
    we are returned a image ready to be passed to our model for inference, the focus
    of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Performing inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This may come as a bit of an anticlimax for someone expecting some hardcore
    coding, but its simplicity definitely pays tribute to the effort of Apple's engineers
    in making this framework one of the most accessible ways to work with a ML model.
    Without further ado, let's put the final pieces together; we start by instantiating
    an instance of our model we had imported in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Near the top, but within the body of the `ViewController` class, add the following
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model is now ready; we return to the `onFrameCaptured` method, starting
    from where we previously left off, and add the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In case you have missed it, I have made the statement performing inference in
    bold. That's it!
  prefs: []
  type: TYPE_NORMAL
- en: After performing inference, we simply assign the `classLabel` property (the
    class with the highest probability) to our `UILabel`, `classifiedLabel`.
  prefs: []
  type: TYPE_NORMAL
- en: With the final piece put in place, we build and deploy. And see how well our
    app performs, recognizing some objects we have lying nearby. Once you're done
    surveying your space, return here, where we will wrap up this chapter and move
    on to greater and more impressive examples.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced object recognition, the 101 project for ML with
    Core ML. We spent some time introducing CNNs or ConvNets, a category of neural
    networks well suited for extracting patterns from images. We discussed how they
    build increasing levels of abstraction with each convolutional layer. We then
    proceeded to make use of our newfound knowledge by implementing the functionality
    that allowed our application to recognize the physical world through its camera.
    We saw firsthand that the majority of the work wasn't performing inference but
    rather implementing the functionality to facilitate and make use of it. This is
    the take-away; intelligence by itself is not useful. What we are interested in
    exploring in this book is the application of trained models to deliver intuitive and
    intelligent experiences. For instance, this example can easily be turned into
    a language tutor assistant, allowing the user to learn a new language by observing
    the world around them.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue our journey into the world of computer
    vision with Core ML by looking at how we can infer the emotional state of someone
    by recognizing their facial expressions. Let's get to it.
  prefs: []
  type: TYPE_NORMAL
