- en: Recognizing Objects in the World
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will immerse ourselves in the world of **machine learning**
    (**ML**) and Core ML by working through what could be considered the 101 Core
    ML application. We will be using an image classification model to allow the user
    to point their iPhone at anything and have the app classify the most dominant
    object in the view.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: We will start off by first discussing the concept of **convolutional neural
    networks** (**ConvNets** or **CNNs**), a category of neural networks well suited
    to image classification, before jumping into implementation. Starting from a skeleton
    project, you will soon discover just how easy it is to integrate ML into your
    apps with the help of Core ML.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Gaining some intuition on how machines understand images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building out the example application for this chapter
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capturing photo frames and preprocessing them before passing them to the Core
    ML model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Core ML model to perform inference and interpreting the result
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks are commonly referred to as either CNNs or ConvNets,
    and these terms are used interchangeably throughout this book.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Understanding images
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, it's not my intention to give you a theoretical or
    deep understanding of any particular ML algorithm, but rather gently introduce
    you to some of the main concepts. This will help you to gain an intuitive understanding
    of how they work so that you know where and how to apply them, as well as give
    you a platform to dive deeper into the particular subject, which I strongly encourage
    you to do.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'For a good introductory text on deep learning, I strongly recommend Andrew
    Trask''s book *Grokking Deep Learning*. For a general introduction to ML, I would
    recommend Toby Segaran''s book *Programming Collective Intelligence: Building
    Smart Web 2.0 Applications*.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will be introducing CNNs, specifically introducing what
    they are and why they are well suited for spatial data, that is, images. But before
    discussing CNNs, we will start by inspecting the data; then we'll see why CNNs
    perform better than their counterpart, fully connected neural networks (or just neural
    networks).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of illustrating these concepts, consider the task of classifying
    the following digits, where each digit is represented as a 5 x 5 matrix of pixels.
    The dark gray pixels have a value of 1 and light gray pixels have a value of 0:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b143785-9e1f-4f17-b091-8bffa35774ab.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: 'Using a fully connected neural network (single hidden layer), our model would
    learn the joint probability of each pixel with respect to their associated label;
    that is, the model will assign positive weights to pixels that correlate with
    the label and using the output with the highest likelihood to be the most probable
    label. During training, we take each image and flatten it before feeding into
    our network, as shown in the following diagram:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c2c3b67-caff-406a-94c2-042feea11f35.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7c2c3b67-caff-406a-94c2-042feea11f35.png)'
- en: This works remarkably well, and if you have experience with ML, particularly
    deep learning, you would have likely come across the MNIST dataset. It's a dataset
    consisting of labeled handwritten digits, where each digit is centrally rendered
    to a 28 x 28 gray scale (single channel with the pixel value ranging from 0-255)
    image. Using a single-layer fully connected network will likely result in a validation
    accuracy close to 90%. But what happens if we introduce some complexities such
    as moving the image around a larger space, as illustrated in the following diagram?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这效果非常好，如果你有机器学习，尤其是深度学习的经验，你很可能已经遇到过MNIST数据集。这是一个包含标记的手写数字数据集，每个数字都中心渲染成28 x
    28的灰度图（单通道，像素值范围从0-255）。使用单层全连接网络很可能会得到接近90%的验证准确率。但是，如果我们引入一些复杂性，比如将图像移动到一个更大的空间中，如图所示，会发生什么呢？
- en: '![](img/36c99f3d-9e48-4a1c-8eba-eba8cda49949.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/36c99f3d-9e48-4a1c-8eba-eba8cda49949.png)'
- en: 'The fully connected network has no concept of space or local relationships;
    in this case, the model would need to learn all variants of each digit at each
    possible location. To further emphasize the importance of being able to capture
    the relationship of spatial data, consider the need to learn more complex images,
    such as classifying dogs and cats using a network that discards 2D information.
    Individual pixels alone are unable to portray complex shapes such as eyes, a nose,
    or ears; it''s only when you consider neighboring pixels that you can describe
    these more complex shapes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接网络没有空间或局部关系的概念；在这种情况下，模型需要学习每个数字在每个可能位置的所有变体。为了进一步强调能够捕捉空间数据关系的重要性，考虑需要学习更复杂的图像，例如使用丢弃2D信息的网络来对猫和狗进行分类。单独的像素无法描绘出眼睛、鼻子或耳朵等复杂形状；只有当你考虑相邻像素时，你才能描述这些更复杂的形状：
- en: '![](img/bb9e8cad-f0c3-4b80-988d-ffaa5431a611.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bb9e8cad-f0c3-4b80-988d-ffaa5431a611.png)'
- en: Images taken from the Kaggle competition cats vs dogs (https://www.kaggle.com/c/dogs-vs-cats)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自Kaggle竞赛猫狗对比（https://www.kaggle.com/c/dogs-vs-cats）
- en: 'We need something that can abstract away from the raw pixels, something that
    can describe images using high-level features. Let''s return to our digits dataset
    and investigate how we might go about extracting higher-level features for the
    task of classification. As alluded to in an earlier example, we need a set of
    features that abstracts away from the raw pixels, is unaffected by position, and
    preserves 2D spatial information. If you''re familiar with image processing, or
    even image processing tools, you would have most probably come across the idea
    and results of **edge detection** or **edge filters**; in simplest terms, these
    work by passing a set of kernels across the whole image, where the output is the
    image with its edges emphasized. Let''s see how this looks diagrammatically. First,
    we have our set of kernels; each one extracts a specific feature of the image,
    such as the presence of horizontal edges, vertical edges, or edges at a 45 degree
    angle:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种能够从原始像素中抽象出来的东西，能够使用高级特征来描述图像。让我们回到我们的数字数据集，并研究我们如何可能提取用于分类任务的高级特征。正如在先前的例子中提到的，我们需要一组从原始像素中抽象出来的特征，不受位置影响，并保留2D空间信息。如果你熟悉图像处理，或者甚至图像处理工具，你很可能已经遇到过**边缘检测**或**边缘滤波器**的概念和结果；最简单的说法，这些是通过在整个图像上传递一组核来实现，输出是强调边缘的图像。让我们看看这图示上是如何的。首先，我们有我们的核集；每个核提取图像的特定特征，例如水平边缘、垂直边缘或45度角的边缘：
- en: '![](img/8861c305-175e-4535-ba4e-b42c9f240a96.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8861c305-175e-4535-ba4e-b42c9f240a96.png)'
- en: 'For each of these filters, we pass them over our image, extracting each of
    the features; to help illustrate this, let''s take one digit and pass the vertical
    kernel over it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些滤波器中的每一个，我们将它们应用到我们的图像上，提取每个特征；为了帮助说明这一点，让我们取一个数字并将垂直核应用到它上面：
- en: '![](img/c152549c-5150-4a50-a20d-f08a457765c8.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c152549c-5150-4a50-a20d-f08a457765c8.png)'
- en: 'As illustrated in the previous diagram, we slide the horizontal kernel across
    the image, producing a new image using the values of the image and kernel. We
    continue until we have reached the bounds of the image, as shown in the following
    diagram:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们将水平核在图像上滑动，使用图像和核的值生成一个新的图像。我们继续这样做，直到达到图像的边界，如下图所示：
- en: '![](img/e2d32e91-c97a-42ac-901c-9d35a04a902a.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e2d32e91-c97a-42ac-901c-9d35a04a902a.png)'
- en: The output of this is a map showing the presence of vertical lines detected
    within the image. Using this and the other kernels, we can now describe each class
    by its dominant gradients rather than using pixel positions. This higher level
    abstraction allows us to recognize classes independent of their location as well
    as describe more complex objects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这的输出是一个显示图像内检测到的垂直线的映射。使用这个和其他内核，我们现在可以用每个类的主导梯度来描述每个类，而不是使用像素位置。这个更高层次的抽象使我们能够独立于位置识别类别，以及描述更复杂的对象。
- en: Two useful things to be aware of when dealing with kernels are the **stride**
    **value** and **padding**. Strides determines how large your step size is when
    sliding your kernel across the image. In the preceding example, our stride is
    set to 1; that is, we're sliding only by a single value. Padding refers to how
    you deal with the boundaries; here, we are using **valid**, where we only process
    pixels within valid ranges. **same **would mean adding a border around the image
    to ensure that the output remains the same size as the input.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理内核时，有两个需要注意的有用事项是**步长**值和**填充**。步长决定了你在滑动内核跨过图像时步长的大小。在上一个例子中，我们的步长设置为1；也就是说，我们只滑动一个值。填充指的是你如何处理边界；在这里，我们使用**valid**，这意味着我们只处理有效范围内的像素。**same**则意味着在图像周围添加一个边界，以确保输出大小与输入相同。
- en: 'What we have performed here is known as **feature engineering** and something
    neural networks perform automatically; in particular, this is what CNNs do. They
    create a series of kernels (or convolution matrices) that are used to convolve
    the image to extract local features from neighboring pixels. Unlike our previous
    engineered example, these kernels are learned during training. Because they are
    learned automatically, we can afford to create many filters that can extract granular nuances
    of the image as well, allowing us to effectively stack convolution layers on top
    of each other. This allows for increasingly higher levels of abstraction to learn. For
    example, your first layer may learn to detect simple edges, and your second layer
    (operating on the previous extracted features) may learn to extract simple shapes.
    The deeper we go, the higher the level achieved by our features, as illustrated
    in the diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的是所谓的**特征工程**，这是神经网络自动执行的事情；特别是，这正是CNN所做的事情。它们创建一系列内核（或卷积矩阵），用于通过卷积图像从相邻像素中提取局部特征。与我们的先前工程示例不同，这些内核是在训练期间学习的。因为它们是自动学习的，我们可以创建许多过滤器，这些过滤器可以提取图像的细微差别，从而允许我们有效地堆叠卷积层。这允许我们学习更高层次的抽象。例如，第一层可能学会检测简单的边缘，而第二层（在先前提取的特征上操作）可能学会提取简单的形状。我们走得越深，我们的特征达到的层次就越高，如图所示：
- en: '![](img/89d3eafc-e01f-4b41-91fe-cc6d32977f97.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/89d3eafc-e01f-4b41-91fe-cc6d32977f97.png)'
- en: And there we have it! An architecture capable of understanding the world by
    learning features and layers of abstraction to efficiently describe it. Let's
    now put this into practice using a pretrained model and Core ML to get our phone
    to recognize the objects it sees.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！一个能够通过学习特征和抽象层来高效描述世界的架构。现在，让我们通过使用预训练模型和Core ML来实践，让我们的手机能够识别它所看到的物体。
- en: Recognizing objects in the world
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别世界中的物体
- en: To recap, our goal in this chapter is to create an application that will recognize what
    it sees. We will start by first capturing video frames, prepare these frames for
    our model, and finally feed them into a Core ML model to perform inference. Let's
    get started.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾，我们本章的目标是创建一个能够识别它所看到的应用程序。我们将首先捕获视频帧，为我们的模型准备这些帧，最后将它们输入到Core ML模型中进行推理。让我们开始吧。
- en: Capturing data
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据采集
- en: 'If you haven''t done it already, download the latest code from the accompanying
    repository: [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter3/Start/ObjectRecognition/`
    and open the project `ObjectRecognition.xcodeproj`. Once loaded, you will see
    the skeleton project for this chapter, as shown in the following screenshot:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，请从配套仓库下载最新代码：[https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml)。下载后，导航到目录`Chapter3/Start/ObjectRecognition/`并打开项目`ObjectRecognition.xcodeproj`。加载后，你将看到本章的骨架项目，如下面的截图所示：
- en: '![](img/f875374a-88c0-44b2-a401-c61345016010.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f875374a-88c0-44b2-a401-c61345016010.png)'
- en: 'To help you navigate around the project, here is a list of core files/classes
    and their main functions:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您在项目中导航，以下是一个核心文件/类及其主要功能的列表：
- en: '`VideoCapture` will be responsible for the management and handling of the camera,
    including capturing video frames'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VideoCapture` 将负责管理和处理摄像头，包括捕获视频帧'
- en: '`CaptureVideoPreviewView.swift` contains the class `CapturePreviewView`, which
    will be used to present the captured frames'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CaptureVideoPreviewView.swift` 包含 `CapturePreviewView` 类，它将被用来展示捕获的帧'
- en: '`CIImage` provides convenient extensions to the class `CIImage`, used for preparing
    the frame for the Core ML model'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CIImage` 为 `CIImage` 类提供了便利的扩展，用于准备帧以供 Core ML 模型使用'
- en: '`VideoController`, as you would expect, is the controller for the application
    and is responsible for interfacing with the imported Core ML model'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如您所期望的，`VideoController` 是应用程序的控制器，负责与导入的 Core ML 模型进行接口交互
- en: We will be making changes to each of these in the following sections in order
    to realize the desired functionality. Our first task will be to get access to
    the camera and start capturing frames; to do this, we will be making use of Apple's
    iOS frameworks **AVFoundation** and **CoreVideo**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中对这些进行修改，以实现所需的功能。我们的第一个任务将是获取对摄像头的访问权限并开始捕获帧；为此，我们将利用 Apple 的 iOS
    框架 **AVFoundation** 和 **CoreVideo**。
- en: The AVFoundation framework encompasses classes for handing capturing, processing,
    synthesizing, controlling, importing, and exporting of audiovisual media on iOS
    and other platforms. In this chapter, we are most interested in a subset of this
    framework for dealing with cameras and media capture, but you can learn more about
    the AVFoundation framework on Apple's official documentation site at [https://developer.apple.com/documentation/avfoundation](https://developer.apple.com/documentation/avfoundation).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: AVFoundation 框架包含用于在 iOS 和其他平台上处理捕获、处理、合成、控制、导入和导出音视频媒体类的集合。在本章中，我们最感兴趣的是该框架的一个子集，用于处理摄像头和媒体捕获，但您可以在
    Apple 的官方文档网站上了解更多关于 AVFoundation 框架的信息：[https://developer.apple.com/documentation/avfoundation](https://developer.apple.com/documentation/avfoundation)。
- en: CoreVideo provides a pipeline-based API for manipulating digital videos, capable
    of accelerating the process using support from both Metal and OpenGL.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CoreVideo 提供了一个基于管道的 API，用于操作数字视频，能够通过 Metal 和 OpenGL 的支持来加速处理过程。
- en: We will designate the responsibility of setting up and capturing frames from
    the camera to the class `VideoCapture`; let's jump into the code now. Select `VideoCapture.swift`
    from the left-hand side panel to open in the editing window. Before making amendments,
    let's inspect what is already there and what's left to do.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指定将设置和从摄像头捕获帧的责任分配给 `VideoCapture` 类；现在让我们直接进入代码。从左侧面板中选择 `VideoCapture.swift`
    以在编辑窗口中打开。在做出修改之前，让我们检查已经存在的内容以及还需要完成的工作。
- en: 'At the top of the class, we have the protocol `VideoCaptureDelegate` defined:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在类的顶部，我们定义了 `VideoCaptureDelegate` 协议：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`VideoCapture` will pass through the captured frames to a registered delegate,
    thus allowing the `VideoCapture` class to focus solely on the task of capturing
    the frames. What we pass to the delegate is a reference to itself, the image data
    (captured frame) of type `CVPixelBuffer` and the timestamp as type `CMTime`. `CVPixelBuffer`
    is a CoreVideo data structure specifically for holding pixel data, and the data
    structure our Core ML model is expecting (which we''ll see in a short while).
    `CMTTime` is just a struct for encapsulating a timestamp, which we''ll obtain
    directly from the video frame.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`VideoCapture` 将将捕获的帧传递给已注册的代理，从而使 `VideoCapture` 类能够专注于捕获帧的任务。我们传递给代理的是对自身的引用、类型为
    `CVPixelBuffer` 的图像数据（捕获帧）以及类型为 `CMTime` 的时间戳。`CVPixelBuffer` 是 CoreVideo 专门用于存储像素数据的数据结构，也是我们的
    Core ML 模型所期望的数据结构（我们将在稍后看到）。`CMTime` 是一个用于封装时间戳的结构体，我们将直接从视频帧中获取它。'
- en: 'Under the protocol, we have the skeleton of our `VideoCapture` class. We will
    be walking through it in this section, along with an extension to implement the
    `AVCaptureVideoDataOutputSampleBufferDelegate` protocol, which we will use to
    capture frames:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在协议下，我们有 `VideoCapture` 类的骨架。在本节中，我们将逐步介绍它，以及一个扩展来实现 `AVCaptureVideoDataOutputSampleBufferDelegate`
    协议，我们将使用它来捕获帧：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Most of this should be self-explanatory, so I will only highlight the not-so-obvious
    parts, starting with the variables `fps` and `lastTimestamp`. We use these together
    to throttle how quickly we pass frames back to the delegate; we do this as it's
    our assumption that we capture frames far quicker than we can process them. And
    to avoid having our camera lag or jump, we explicitly limit how quickly we pass
    frames to the delegate. **Frames per second** (**fps**) sets this frequency while
    `lastTimestamp` is used in conjunction to calculate the elapsed time since the
    last processing of a frame.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分内容应该是显而易见的，所以我只会强调不那么明显的地方，从变量`fps`和`lastTimestamp`开始。我们使用这些变量一起控制我们向代理传递帧的速率；我们这样做是因为我们假设我们捕获帧的速度远快于我们处理它们的速度。为了避免我们的摄像头出现延迟或跳跃，我们明确限制向代理传递帧的速率。**每秒帧数**（**fps**）设置这个频率，而`lastTimestamp`则与计算自上次处理帧以来经过的时间一起使用。
- en: The only other part of the code I will highlight here is the `asyncStartCapturing`
    and `asyncStopCapturing` methods; these methods, as the names imply, are responsible
    for starting and stopping the capture session respectively. Because they both
    will be using blocking methods, which can take some time, we will dispatch the
    task off the main thread to avoid blocking it and affecting the user's experience.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里要强调的代码的另一部分是`asyncStartCapturing`和`asyncStopCapturing`方法；正如其名称所暗示的，这些方法分别负责启动和停止捕获会话。因为它们都将使用阻塞方法，这可能会花费一些时间，所以我们将任务从主线程派发出去，以避免阻塞它并影响用户体验。
- en: 'Finally, we have the extension; it implements the `AVCaptureVideoDataOutputSampleBufferDelegate`
    protocol:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有扩展；它实现了`AVCaptureVideoDataOutputSampleBufferDelegate`协议：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will discuss the details shortly, but essentially it is the delegate that
    we assign to the camera for handling incoming frames of the camera. We will then
    proxy it through to the `VideoCaptureDelegate` delegate assigned to this class.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后讨论细节，但基本上是我们分配给摄像头以处理摄像头传入帧的代理。然后我们将它代理到分配给这个类的`VideoCaptureDelegate`代理。
- en: Let's now walk through implementing the methods of this class, starting with
    `initCamera`. In this method, we want to set up the pipeline that will grab the
    frames from the physical camera of the device and pass them onto our delegate
    method. We do this by first getting a reference to the physical camera and then
    wrapping it in an instance of the `AVCaptureDeviceInput` class, which takes care
    of managing the connection and communication with the physical camera. Finally,
    we add a destination for the frames, which is where we use an instance of `AVCaptureVideoDataOutput`,
    assigning ourselves as the delegate for receiving these frames. This pipeline
    is wrapped in something called `AVCaptureSession`, which is responsible for coordinating
    and managing this pipeline.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来逐步实现这个类的各个方法，从`initCamera`方法开始。在这个方法中，我们想要设置一个管道，该管道将从设备的物理摄像头捕获帧并将它们传递到我们的代理方法。我们通过首先获取物理摄像头的引用，然后将其包装在`AVCaptureDeviceInput`类的一个实例中来实现这一点，该类负责管理与物理摄像头的连接和通信。最后，我们为帧添加一个目的地，这是我们在其中使用`AVCaptureVideoDataOutput`类的一个实例的地方，我们将自己指定为接收这些帧的代理。这个管道被包裹在称为`AVCaptureSession`的东西中，它负责协调和管理这个管道。
- en: 'Let''s now define some instance variables we''ll need; inside the class `VideoCapture`,
    add the following variables:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义一些我们将需要的实例变量；在类`VideoCapture`内部添加以下变量：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We mentioned the purpose of `captureSession` previously, but also introduced
    a `DispatchQueue`. When adding a delegate to `AVCaptureVideoDataOutput` (for handling
    the arrival of new frames), you also pass in a `DispatchQueue`; this allows you
    to control which queue the frames are managed on. For our example, we will be
    handling the processing of the images off the main thread so as to avoid impacting
    the performance of the user interface.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到了`captureSession`的目的，但也介绍了一个`DispatchQueue`。当将代理添加到`AVCaptureVideoDataOutput`（用于处理新帧的到来）时，你也会传递一个`DispatchQueue`；这允许你控制帧在哪个队列上被管理。在我们的例子中，我们将处理图像的操作从主线程上移除，以避免影响用户界面的性能。
- en: 'With our instance variables now declared, we will turn our attention to the
    `initCamera` method, breaking it down into small snippets of code. Add the following
    within the body of the method:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经声明了实例变量，我们将把注意力转向`initCamera`方法，将其分解成小的代码片段。在方法体内添加以下内容：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We signal to the `captureSession` that we want to batch multiple configurations
    by calling the method `beginConfiguration`; these changes won''t be made until
    we commit them by calling the session''s `commitConfiguration` method. Then, in
    the next line of code, we set the desired quality level:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过调用方法 `beginConfiguration` 向 `captureSession` 信号我们想要批量配置多个配置；这些更改不会在我们通过调用会话的
    `commitConfiguration` 方法提交之前生效。然后，在下一行代码中，我们设置所需的品质级别：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the next snippet, we obtain the physical device; here, we are obtaining the
    default device capable of recording video, but you can just as easily search for
    one with specific capabilities, such as the front camera. After successfully obtaining
    the device, we wrap it in an instance of `AVCaptureDeviceInput` that will be responsible
    for capturing data from the physical camera and finally adding it to the session.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个代码片段中，我们获取物理设备；在这里，我们获取默认的能够录制视频的设备，但您也可以轻松地搜索具有特定功能（如前置摄像头）的设备。在成功获取设备后，我们将它包装在一个
    `AVCaptureDeviceInput` 的实例中，该实例将负责从物理摄像头捕获数据并将其最终添加到会话中。
- en: 'We now have to add the destination for these frames; again, add the following
    snippet to the `initCamera` method where you left off:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须添加这些帧的目的地；再次，将以下片段添加到 `initCamera` 方法中，您之前停止的地方：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the previous code snippet, we create, set up, and added our output. We start
    by instantiating an instance of `AVCaptureVideoDataOutput`, before defining what
    data we want. Here, we are requesting full color (`kCVPixelFormatType_32BGRA`),
    but depending on your model, it may be more efficient to request images in grayscale
    (`kCVPixelFormatType_8IndexedGray_WhiteIsZero`).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，我们创建、设置并添加了我们的输出。我们首先实例化 `AVCaptureVideoDataOutput` 的一个实例，然后定义我们想要的数据。在这里，我们请求全彩（`kCVPixelFormatType_32BGRA`），但根据您的模型，请求灰度图像（`kCVPixelFormatType_8IndexedGray_WhiteIsZero`）可能更有效率。
- en: 'Setting `alwaysDiscardsLateVideoFrames` to true means any frames that arrive
    while the dispatch queue is busy will be discarded—a desirable feature for our
    example. We then assign ourselves along with our dedicated dispatch queue as the
    delegate for handing incoming frames using the method `videoOutput.setSampleBufferDelegate(self,
    queue: sessionQueue)`. Once we have configured our output, we are ready to add
    it to our session as part of our configuration request. To prevent our images
    from being rotated by 90 degrees, we then request that our images are in portrait
    orientation.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '将 `alwaysDiscardsLateVideoFrames` 设置为 true 意味着在调度队列忙碌时到达的任何帧都将被丢弃——这对于我们的示例来说是一个理想的功能。然后，我们使用方法
    `videoOutput.setSampleBufferDelegate(self, queue: sessionQueue)` 将我们自己以及我们的专用调度队列指定为处理传入帧的代理。一旦我们配置了输出，我们就可以将其作为配置请求的一部分添加到会话中。为了防止我们的图像被旋转
    90 度，我们随后请求图像以竖直方向显示。'
- en: 'Add the final statement to commit these configurations; it''s only after we
    do this that these changes will take effect:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 添加最终语句以提交这些配置；只有在我们这样做之后，这些更改才会生效：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This now completes our `initCamera` method; let''s swiftly (excuse the pun)
    move onto the methods responsible for starting and stopping this session. Add
    the following code to the body of the `asyncStartCapturing` method:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这现在完成了我们的 `initCamera` 方法；让我们迅速（请原谅这个双关语）转到负责启动和停止此会话的方法。将以下代码添加到 `asyncStartCapturing`
    方法的主体中：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As mentioned previously, the `startRunning` and `stopRunning` methods both block
    the main thread and can take some time to complete; for this reason, we execute
    them off the main thread, again to avoid affecting the responsiveness of the user
    interface. Invoking `startRunning` will start the flow of data from the subscribed
    inputs (camera) to the subscribed outputs (delegate).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`startRunning` 和 `stopRunning` 方法都会阻塞主线程，并且可能需要一些时间来完成；因此，我们将它们在主线程之外执行，再次避免影响用户界面的响应性。调用
    `startRunning` 将启动从订阅的输入（摄像头）到订阅的输出（代理）的数据流。
- en: Errors, if any, are reported through the notification `AVCaptureSessionRuntimeError`.
    You can subscribe to listen to it using the default `NotificationCenter`. Similarly,
    you can subscribe to listen when the session starts and stops with the notifications
    `AVCaptureSessionDidStartRunning` and `AVCaptureSessionDidStopRunning`, respectively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有任何错误，将通过通知 `AVCaptureSessionRuntimeError` 报告。您可以使用默认的 `NotificationCenter`
    订阅以监听它。同样，您可以通过通知 `AVCaptureSessionDidStartRunning` 和 `AVCaptureSessionDidStopRunning`
    分别订阅以监听会话开始和停止。
- en: 'Similarly, add the following code to the method `asyncStopCapturing`, which
    will be responsible for stopping the current session:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，将以下代码添加到 `asyncStopCapturing` 方法中，该方法将负责停止当前会话：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Within the `initCamera` method, we subscribed ourselves as the delegate to
    handle arriving frames using the statement `videoOutput.setSampleBufferDelegate(self,
    queue: sessionQueue)`; let''s now turn our attention to handling this. As you
    may recall, we included an extension of the `VideoCapture` class to implement
    the `AVCaptureVideoDataOutputSampleBufferDelegate` protocol within the `captureOutput`
    method. Add the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '在`initCamera`方法中，我们使用语句`videoOutput.setSampleBufferDelegate(self, queue: sessionQueue)`将自己注册为代理以处理到达的帧；现在让我们关注如何处理它。如您所回忆的那样，我们在`captureOutput`方法中为`VideoCapture`类添加了一个扩展来实现`AVCaptureVideoDataOutputSampleBufferDelegate`协议。添加以下代码：'
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Before walking through this code snippet, it's worth mentioning what parameters
    this method is passed and how we use them. The first parameter, `output`, is of the
    type `AVCaptureVideoDataOutput` and references the associated output that this
    frame originated from. The next parameter, `sampleBuffer`, is of the type `CMSampleBuffer` and
    this is what we will use to access data of the current frame. Along with the frames,
    the duration, format, and timestamp associated with each frame can also be obtained.
    The final parameter, `connection`, is of the type `AVCaptureConnection` and provides
    a reference to the connection associated with the received frame.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览这段代码片段之前，值得提一下这个方法接收了哪些参数以及我们如何使用它们。第一个参数，`output`，其类型为`AVCaptureVideoDataOutput`，它引用了与此帧相关的输出。下一个参数，`sampleBuffer`，其类型为`CMSampleBuffer`，这是我们用来访问当前帧数据的。与帧一起，每个帧关联的持续时间、格式和时间戳也可以获得。最后一个参数，`connection`，其类型为`AVCaptureConnection`，它提供了与接收到的帧相关联的连接的引用。
- en: Now, walking through the code, we start by guarding against any occurrences
    where no delegate is assigned, and returning early if so. Then we determine whether
    enough time has elapsed since the last time we processed a frame, remembering
    that we are throttling how frequently we process a frame to ensure a seamless
    experience. Here, instead of using the systems clock, we obtain the time associated
    with the latest frame via the statement `let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)`;
    this ensures that we are measuring against the relative time with respect to the
    frame rather than absolute time of the system. Given that enough time has passed,
    we proceed to get a reference to the sample's image buffer via the statement `CMSampleBufferGetImageBuffer(sampleBuffer)`,
    finally passing it over to the assigned delegate.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐行分析代码，我们首先检查是否有未分配的代理，并在必要时提前返回。然后我们确定自上次处理帧以来是否已经过去了足够的时间，记住我们正在限制处理帧的频率以确保流畅的体验。在这里，我们不是使用系统时钟，而是通过语句`let
    timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)`获取与最新帧相关的时间；这确保了我们是在相对于帧的相对时间而不是系统的绝对时间进行测量。鉴于已经过去了足够的时间，我们继续通过语句`CMSampleBufferGetImageBuffer(sampleBuffer)`获取样本的图像缓冲区引用，最后将其传递给指定的代理。
- en: 'This now completes our `VideoCapture` class; let''s move on to hooking it up
    to our view using the `ViewController`. But before jumping into the code, let''s
    inspect the interface via the storyboard to better understand where we''ll be
    presenting the video stream. Within Xcode, select `Main.storyboard` from the Project
    Navigator panel on the left to open up interface builder; when opened, you will
    be presented with a layout similar to the following screenshot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经完成了`VideoCapture`类的编写；让我们继续使用`ViewController`将其连接到我们的视图。但在跳入代码之前，让我们通过故事板检查接口，以便更好地理解我们将在哪里展示视频流。在Xcode中，从左侧的项目导航器面板中选择`Main.storyboard`以打开界面构建器；打开后，您将看到一个类似于以下截图的布局：
- en: '![](img/f8c13e02-eba6-4a0c-8ea0-92e3cf37c079.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f8c13e02-eba6-4a0c-8ea0-92e3cf37c079.png)'
- en: 'Nothing complicated; we have a label to present our results and a view to render
    our video frames onto. If you select the VideoPreview view and inspect the class
    assigned to it, you will see we have a custom class to handle the rendering called,
    appropriately, CapturePreviewView. Let''s jump into the code for this class and
    make the necessary changes:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么复杂的；我们有一个标签来显示我们的结果，还有一个视图来渲染我们的视频帧。如果您选择VideoPreview视图并检查分配给它的类，您将看到我们有一个名为CapturePreviewView的自定义类来处理渲染。让我们跳入这个类的代码并进行必要的修改：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Fortunately, `AVFoundation` makes available a subclass of `CALayer` specifically
    for rendering frames from the camera; all that remains for us to do is to override
    the view''s `layerClass` property and return the appropriate class. Add the following
    code to the `CapturePreviewView` class:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`AVFoundation` 提供了一个专门用于从摄像头渲染帧的 `CALayer` 子类；我们剩下的工作就是重写视图的 `layerClass`
    属性并返回适当的类。将以下代码添加到 `CapturePreviewView` 类中：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This method is called early during the creation of the view and is used to
    determine what `CALayer` to instantiate and associate with this view. As previously
    mentioned, the `AVCaptureVideoPreviewLayer` is—as the name suggests—specifically
    for handling video frames. In order to get the frames rendered, we simply assign
    `AVCaptureSession` with the `AVCaptureVideoPreviewLayer.session` property. Let''s
    do that now; first open up the `ViewController` class in Xcode and add the following
    variable (in bold):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在视图创建的早期阶段就被称作，用于确定要实例化哪个 `CALayer` 并将其与该视图关联。正如之前提到的，`AVCaptureVideoPreviewLayer`
    ——正如其名称所暗示的那样——专门用于处理视频帧。为了获取渲染的帧，我们只需将 `AVCaptureSession` 赋值为 `AVCaptureVideoPreviewLayer.session`
    属性。现在让我们来做这件事；首先在 Xcode 中打开 `ViewController` 类，并添加以下变量（粗体）：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `previewView` and `classifiedLabel` are existing variables associated with
    the interface via the Interface Builder. Here, we are creating an instance of
    `VideoCapture`, which we had implemented earlier. Next, we will set up and start
    the camera using the `VideoCapture` instance, before assigning the session to
    our `previewView` layer. Add the following code within the `ViewDidLoad` method
    under the statement `super.viewDidLoad()`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`previewView` 和 `classifiedLabel` 是通过 Interface Builder 与界面关联的现有变量。在这里，我们创建了一个
    `VideoCapture` 的实例，这是我们之前实现的。接下来，我们将使用 `VideoCapture` 实例设置并启动摄像头，然后将会话分配给我们的 `previewView`
    层。在 `ViewDidLoad` 方法下的 `super.viewDidLoad()` 语句中添加以下代码：'
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Most of the code should look familiar to you as a lot of it is using the methods
    we have just implemented. First we initialize the camera, calling the `initCamera`
    method of the `VideoCamera` class. Then, if successful, we assign the created
    `AVCaptureSession` to the layer's session. We also hint to the layer how we want
    it to handle the content, in this case filling the screen whilst respecting its
    aspect ratio. Finally, we start the camera by calling `videoCapture.asyncStartCapturing()`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分的代码应该对你来说都很熟悉，因为其中很多都是使用我们刚刚实现的方法。首先我们初始化摄像头，调用 `VideoCamera` 类的 `initCamera`
    方法。然后，如果成功，我们将创建的 `AVCaptureSession` 分配给层的会话。我们还向层暗示了我们希望它如何处理内容，在这种情况下是填充屏幕同时尊重其宽高比。最后，我们通过调用
    `videoCapture.asyncStartCapturing()` 启动摄像头。
- en: With that now completed, it's a good time to test that everything is working
    correctly. If you build and deploy on an iOS 11+ device, you should see the video
    frames being rendered on your phone's screen.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切完成，是时候测试一切是否正常工作了。如果你在 iOS 11+ 设备上构建和部署，你应该能在手机屏幕上看到渲染的视频帧。
- en: In the next section, we will walk through how to capture and process them for
    our model before performing inference (recognition).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍如何在执行推理（识别）之前，如何捕获和处理这些帧以供我们的模型使用。
- en: Preprocessing the data
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'At this stage, we have the app rendering the frames from the camera, but we
    are not yet receiving any frames. To do this, we will assign ourselves to receive
    these frames, as implemented in the previous section. The existing `ViewController`
    class already has an extension implementing the `VideoCaptureDelegate` protocol.
    What''s left to do is to assign ourselves as the delegate of the `VideoCapture`
    instance and implement the details of the callback method; the following is the
    code for `extension`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们的应用正在渲染来自摄像头的帧，但我们还没有接收到任何帧。为了做到这一点，我们将自己设置为接收这些帧，正如前一个章节中实现的那样。现有的
    `ViewController` 类已经有一个扩展实现了 `VideoCaptureDelegate` 协议。剩下要做的就是将我们自己设置为 `VideoCapture`
    实例的代理并实现回调方法的细节；以下是为 `extension` 编写的代码：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Depending on your coding style, you can just as easily implement the protocols
    inside the main class. I tend to make use of extensions to implement the protocols—a
    personal preference.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的编码风格，你同样可以在主类内部实现协议。我倾向于使用扩展来实现协议——这是一个个人偏好。
- en: 'First, let''s assign ourselves as the delegate to start receiving the frames;
    within the `ViewDidLoad` method of the `ViewController` class, we add the following
    statement just before we initialize the camera:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将自己设置为代理以开始接收帧；在 `ViewController` 类的 `ViewDidLoad` 方法中，在我们初始化摄像头之前添加以下语句：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we have assigned ourselves as the delegate, we will receive frames
    (at the defined frame rate) via the callback:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It's within this method that we will prepare and feed the data to the model
    to classify the dominant object within the frame. What the model is expecting
    is dependent on the model, so to get a better idea of what we need to pass it,
    let's download the trained model we will be using for this example and import
    it into our project.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Trained models can be obtained from a variety of sources; in some instances,
    you will need to convert them, and in other cases, you will need to train the
    model yourself. But in this instance, we can make use of the models Apple has
    made available; open up your web browser and navigate to [https://developer.apple.com/machine-learning/](https://developer.apple.com/machine-learning/):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b03ed353-ba5c-4d37-aa90-c54468fc8831.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: You will be taken to a web page where Apple has made available a range of pretrained
    and converted models. Conveniently, most of the available models are specifically
    for object classification; given our use case, we're particularly interested in
    the models trained on a large array of objects. Our options include MobileNet, SqueezeNet, ResNet50, Inception
    v3, and VGG16\. Most of these have been trained on the ImageNet dataset, a dataset
    with reference to over 10 million URLs' images that have been manually assigned
    to one of 1,000 classes. References to the original research papers and performance
    can be obtained via the View original model details link. For this example, we'll
    use Inception v3, a good balance between size and accuracy.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are using the Inception v3 model, but the effort to swap the model
    is minimal; it requires updating the references as the generated classes are prefixed
    with the model's name, as you will soon see, and ensuring that you are conforming
    to the expected inputs of the model (which can be alleviated by using the Vision
    framework, as you will see in future chapters).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the Download Core ML Model link to proceed to download and, once downloaded,
    drag the `Inceptionv3.mlmodel` file onto the Project Navigator panel on the left
    of Xcode, checking Copy items if needed if desired or else leaving everything
    as default. Select the `Inceptionv3.mlmodel` file from the Project Navigator panel
    on the left to bring up the details within the Editor area, as shown in the following
    screenshot:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7672f9b-67a1-4755-befe-e69d230b3937.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: It is important to ensure that the model is correctly assigned to the appropriate
    target; in this example, this means verifying that the ObjectRecognition target
    is checked, as seen here on the Utilities panel to the right. Also worth noting
    are the expected inputs and outputs of the model. Here, the model is expecting
    a color image of size 299 x 299 for its input, and it returns a single class label
    as a string and a dictionary of string-double pairs of probabilities of all the
    classes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'When a `.mlmodel` file is imported, Xcode will generate a wrapper for the model
    itself and the input and output parameters to interface with the model; this is
    illustrated here:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db3c0a61-38ff-4a49-9f2c-66eb98f65aa9.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'You can easily access this by tapping on the arrow button next to the `Inceptionv3`
    label within the Model Class section; when tapped, you will see the following
    code (separated into three distinct blocks to make it more legible):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The first block of the preceding code is the input for our model. This class
    implements the `MLFeatureProvider` protocol, a protocol representing a collection
    of feature values for the model, in this case, the image feature. Here, you can
    see the expected data structure, `CVPixelBuffer`, along with the specifics declared
    (handily) in the comments. Let''s continue on with our inspection of the generated
    classes by looking at the binding for the output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As previously mentioned, the output exposes a directory of probabilities and
    a string for the dominated class, each exposed as properties or accessible using
    the getter method `featureValue(for featureName: String)` by passing in the feature''s
    name. Our final extract for the generated code is the model itself; let''s inspect
    that now:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This class wraps the model class and provides strongly typed methods for performing
    inference via the `prediction(input: Inceptionv3Input)` and `prediction(image:
    CVPixelBuffer)` methods, each returning the output class we saw previously—`Inceptionv3Output`.
    Now, knowing what our model is expecting, let''s continue to implement the preprocessing
    functionality required for the captured frames in order to feed them into the
    model.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Core ML 2 introduced a the ability to work with batches; if your model was
    compiled with Xcode 10+ then you will also see the additional method `<CODE>func
    predictions(from: MLBatchProvider, options: MLPredictionOptions)</CODE>` allowing
    you to perform inference on a batch of inputs.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we know that we are receiving the correct data type (`CVPixelBuffer`)
    and image format (explicitly defined in the settings when configuring the capture
    video output instance `kCVPixelFormatType_32BGRA`) from the camera. But we are
    receiving an image significantly larger than the expected size of 299 x 299\.
    Our next task will be to create some utility methods to perform resizing and cropping.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: For this, we will be extending `CIImage` to wrap and process the pixel data
    we receive along with making use of `CIContext` to obtain the raw pixels again.
    If you're unfamiliar with the CoreImage framework, then it suffices to say that
    it is a framework dedicated to efficiently processing and analyzing images. `CIImage` can
    be considered the base data object of this framework that is often used in conjunction
    with other CoreImage classes such as `CIFilter`, `CIContext`, `CIVector`, and
    `CIColor`. Here, we are interested in `CIImage` as it provides convenient methods
    for manipulating images along with `CIContext` to extract the raw pixel data from
    `CIImage` (`CVPixelBuffer`).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Back in Xcode, select the `CIImage.swift` file from the Project navigator to
    open it up in the Editor area. In this file, we have extended the `CIImage` class
    with a method responsible for rescaling and another for returning the raw pixels (`CVPixelBuffer`),
    a format required for our Core ML model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s start by implementing the `resize` method; this method is passed in
    the desired size, which we''ll use to calculate the relative scale; then we''ll
    use this to scale the image uniformly. Add the following code snippet to the `resize`
    method, replacing the `fatalError("Not implemented")` statement:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Unless the image is a square, we are likely to have an overflow either vertically
    or horizontally. To handle this, we will simply center the image and crop it to
    the desired size; do this by appending the following code to the `resize` method
    (beneath the code written in the preceding snippet):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We now have the functionality to rescale the image; our next piece of functionality
    is to obtain a `CVPixelBuffer` from the `CIImage`. Let''s do that by implementing
    the body of the `toPixelBuffer` method. Let''s first review the method''s signature
    and then briefly talk about the functionality required:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This method is expecting a `CIContext` and flag indicating whether the image
    should be grayscale (single channel) or full color; `CIContext` will be used to
    render the image to a pixel buffer (our `CVPixelBuffer`). Let's now flesh out
    the implementation for `toPixelBuffer` piece by piece.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: The preprocessing required on the image (resizing, grayscaling, and normalization)
    is dependent on the Core ML model and the data it was trained on. You can get
    a sense of these parameters by inspecting the Core ML model in Xcode. If you recall,
    the expected input to our model is (image color 299 x 299); this tells us that
    the Core ML model is expecting the image to be color (three channels) and 299
    x 299 in size.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating the pixel buffer we will be rendering our image to; add
    the following code snippet to the body of the `toPixelBuffer` method, replacing
    the `fatalError("Not implemented")` statement:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We first create an array to hold the attributes defining the compatibility requirements
    for our pixel buffer; here, we specify that we want our pixel buffer to be compatible
    with `CGImage` types (`kCVPixelBufferCGImageCompatibilityKey`) and compatible
    with CoreGraphics bitmap contexts (`kCVPixelBufferCGBitmapContextCompatibilityKey`).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'We then proceed to create a pixel buffer, passing in our compatibility attributes,
    the format (either grayscale or full color depending on the value of `gray`),
    width, height, and pointer to the variable. Next, we unwrap the nullable pixel
    buffer as well as ensure that the call was successful; if either of these is `false`,
    we return `NULL`. Otherwise, we''re ready to render our `CIImage` into the newly
    created pixel buffer. Append the following code to the `toPixelBuffer` method:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Before drawing, we lock the address of the pixel buffer via `CVPixelBufferLockBaseAddress`
    and then unlock once we've finished using the `CVPixelBufferUnlockBaseAddress`
    method. We are required to do this when accessing pixel data from the CPU, which
    we are doing here.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Once locked, we simply use the `CIContext` to render the scaled image to the
    buffer, passing in the destination rectangle (in this case, the full size of the
    pixel buffer) and destination color space, which is full color or grayscale depending
    on the value of `gray` as mentioned previously. After unlocking the pixel buffer,
    as described earlier, we return our newly created pixel buffer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now extended the `CIImage` with two convenient methods, one responsible
    for rescaling and the other for creating a pixel buffer representation of itself.
    We will now return to the `ViewController` class to handle the preprocessing steps
    required before passing our data into the model. Select the `ViewController.swift`
    file from the Projector navigator panel within Xcode to bring up the source code,
    and within the body of the `ViewController` class, add the following variable:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As previously discussed, we will be passing this to our `CIImage.toPixelBuffer`
    method for rendering the image to the pixel buffer. Now return to the `onFrameCaptured`
    method and add the following code, to make use of the methods we''ve just created
    for preprocessing:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We first unwrap the `pixelBuffer`, returning if it is `NULL`; then we create
    an instance of `CIImage`, passing in the current frame and then chaining our extension
    methods to perform rescaling (299 x 299) and rendering out to a pixel buffer (setting
    the gray parameter to false as the model is expecting full color images). If successful,
    we are returned a image ready to be passed to our model for inference, the focus
    of the next section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Performing inference
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This may come as a bit of an anticlimax for someone expecting some hardcore
    coding, but its simplicity definitely pays tribute to the effort of Apple's engineers
    in making this framework one of the most accessible ways to work with a ML model.
    Without further ado, let's put the final pieces together; we start by instantiating
    an instance of our model we had imported in the previous section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Near the top, but within the body of the `ViewController` class, add the following
    line:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Our model is now ready; we return to the `onFrameCaptured` method, starting
    from where we previously left off, and add the following code snippet:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In case you have missed it, I have made the statement performing inference in
    bold. That's it!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: After performing inference, we simply assign the `classLabel` property (the
    class with the highest probability) to our `UILabel`, `classifiedLabel`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: With the final piece put in place, we build and deploy. And see how well our
    app performs, recognizing some objects we have lying nearby. Once you're done
    surveying your space, return here, where we will wrap up this chapter and move
    on to greater and more impressive examples.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced object recognition, the 101 project for ML with
    Core ML. We spent some time introducing CNNs or ConvNets, a category of neural
    networks well suited for extracting patterns from images. We discussed how they
    build increasing levels of abstraction with each convolutional layer. We then
    proceeded to make use of our newfound knowledge by implementing the functionality
    that allowed our application to recognize the physical world through its camera.
    We saw firsthand that the majority of the work wasn't performing inference but
    rather implementing the functionality to facilitate and make use of it. This is
    the take-away; intelligence by itself is not useful. What we are interested in
    exploring in this book is the application of trained models to deliver intuitive and
    intelligent experiences. For instance, this example can easily be turned into
    a language tutor assistant, allowing the user to learn a new language by observing
    the world around them.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue our journey into the world of computer
    vision with Core ML by looking at how we can infer the emotional state of someone
    by recognizing their facial expressions. Let's get to it.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
