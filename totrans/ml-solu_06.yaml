- en: Chapter 6. Job Recommendation Engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already seen how to develop a recommendation system for the e-commerce
    product in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems for E-Commerce"),
    *Recommendation Systems for e-Commerce*, Now, we will apply the same concepts
    that you learned in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems
    for E-Commerce"), *Recommendation Systems for e-Commerce* but the type and format
    of the dataset is different. Basically, we will build a job recommendation engine.
    For this application, we have taken into account the text dataset. The main concept
    of building the recommendation engine will not change, but this chapter gives
    you a fair idea of how to apply the same concepts to different types of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the baseline approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the baseline approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the baseline approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the baseline approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the revised approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to improve the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let's discuss the problem statement.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will build an engine that can recommend jobs to any user.
    This is the simplest goal we want to achieve. How we are going to build it? In
    order to answer this question, let me give you an idea about what kind of approaches
    we will take in order to build a job recommendation system.
  prefs: []
  type: TYPE_NORMAL
- en: For our baseline approach, we will scrape resumes of dummy users and try to
    build a job recommendation engine based on the scraped dataset. The reason we
    are scraping the dataset is that, most of the time, there will not be any dataset
    available for many data science applications. Suppose you are in a position where
    you have not found any dataset. What you will do then? I want to provide a solution
    for these kinds of scenarios. So, you will learn how to scrape the data and build
    the baseline solution.
  prefs: []
  type: TYPE_NORMAL
- en: In the revised approach, we will be using a dataset hosted by Kaggle. Using
    the content-based approach, we will be building a job recommendation engine. For
    the best approach, we will be using the concept of user-based collaborative filtering
    for this domain, and build the final job recommendation system.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look into the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we are using two datasets. The two datasets are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The scraped dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The job recommendation challenge dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with the scraped dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Scraped dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this dataset, we have scraped the dummy resume from indeed.com (we are
    using this data just for learning and research purposes). We will download the
    resumes of users in PDF format. These will become our dataset. The code for this
    is given at this GitHub link: [https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the code given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scraped dataset](img/B08394_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Code snippet for scraping the data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the preceding code, we can download the resumes. We have used the `requests`
    library and `urllib` to scrape the data. All these downloaded resumes are in PDF
    form, so we need to parse them. To parse the PDF document, we will use a Python
    library called `PDFminer`. We need to extract the following data attributes from
    the PDF documents:'
  prefs: []
  type: TYPE_NORMAL
- en: Work experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Education
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skills
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Awards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can take a look at the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scraped dataset](img/B08394_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Code snippet to parse PDF documents'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, `PDFminer` is converting the content of PDF into text. Once we have
    the text data using regular expressions, we can fetch the necessary details. You
    can see the entire code by using this GitHub link: [https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we fetch all the necessary information, we will save the data in a pickle
    format. Now, you don''t need to scrap the data and fetch all necessary information.
    I have uploaded the data into a pickle file format at this GitHub link: [https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl)'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `resume_data.pkl` file for our baseline approach.
  prefs: []
  type: TYPE_NORMAL
- en: Job recommendation challenge dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This dataset is provided by [www.careerbuilder.com](http://www.careerbuilder.com)
    and is hosted on Kaggle. You can download the dataset using this link: [https://www.kaggle.com/c/job-recommendation/data](https://www.kaggle.com/c/job-recommendation/data).
    These are the data files that we are going to use for our revised and best approach.
    All the values given in these datafiles are tab-separated:'
  prefs: []
  type: TYPE_NORMAL
- en: '`apps.tsv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`users.tsv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jobs.zip`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user_history.tsv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: apps.tsv
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This datafile contains the records of users'' job applications. It indicates
    job positions that a particular user has applied for. The job position is described
    by the JobID column. All the necessary information about this datafile is given
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![apps.tsv](img/B08394_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Data information about apps.tsv'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are five data columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`UserId`: This indicates the unique ID for a given user. By using this ID,
    we can access the user''s profile.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WindowsID`: This is the mask data attribute with the constant value of 1\.
    This data attribute is not important for us.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Split`: This data attribute indicates which data records we should consider
    for training and testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Application date`: This is the timestamp at which the user applied for the
    job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JobID`: This attribute indicates the `JobIds` for which the user nominates
    themselves. Using this `JobId`, we can access other information about a particular
    job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: users.tsv
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This datafile contains the user profile and all user-related information. You
    can find all the necessary information displayed in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![users.tsv](img/B08394_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 Data information about users.tsv
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the data attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`UserID`: This data attribute indicates the user''s unique identification number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WindowID`: This is the mask data attribute with a constant value of 1\. This
    data attribute is not important for us.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Split`: This data attribute indicates which data records we should consider
    for training and testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`City`: This attribute indicates the user''s current city.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`State`: This attribute indicates the user''s state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Country: This attribute indicates the user''s country.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZipCode`: This data attribute indicates the user''s ZIP code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DegreeType`: This data column indicates the user''s degree; whether the user
    is a high school pass-out or has a bachelor''s degree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Major`: This data attribute indicates the major subject in which the user
    has a degree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GraduationDate`: This data attribute indicates the graduation date of the
    user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WorkHistoryCount`: This data attribute indicates the number of companies the
    user has worked for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TotalYearsExperience`: This data column indicates the user''s total years
    of experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CurrentlyEmployed`: This data attribute has a binary value. If the user is
    currently employed, then the value is *Yes*; if not, then the value is *No*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ManagedOthers`: This data attribute has a binary value as well. If the user
    is managing other people, then the value of this column is *Yes*; if the user
    is not managing other people, then the value of this column is *No*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ManagedHowMany`: This data attribute has a numerical value. The value of this
    column indicates the number of people that are managed by the user. If the user
    is not managing anyone, then the value is 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jobs.zip
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you extract this ZIP file, you can get the `jobs.tsv` file. There is more
    information available in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jobs.zip](img/B08394_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Data information about jobs.tsv'
  prefs: []
  type: TYPE_NORMAL
- en: '`JobID`: This is the unique ID for each job present in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WindowID`: This is the mask data attribute that has a constant value of 1\.
    This data attribute is not important for us.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Title`: This data attribute indicates the job title.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Description`: This data attribute indicates the job description.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Requirements`: This data attribute indicates the job requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`City`: This data field indicates the job location in terms of the city.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`State`: This data field indicates the job location in terms of the state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Country`: This data field indicates the job location in terms of the country.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Zip5`: This data field indicates the ZIP code of the job location.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StartDate`: This date indicates when the job is posted or is open for applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EndDate`: This date is the deadline for the job application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: user_history.tsv
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `user_history.tsv` file contains the user''s job history. There is more
    information available on this in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![user_history.tsv](img/B08394_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Data information about user_history.tsv'
  prefs: []
  type: TYPE_NORMAL
- en: There are only two new columns for this datafile.
  prefs: []
  type: TYPE_NORMAL
- en: '`Sequence`: This sequence is a numerical field. The number indicates the sequential
    order of the user''s job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JobTitle`: This data field indicates the job title of the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have covered all the attributes in our datafiles; now let's start building
    the baseline approach.
  prefs: []
  type: TYPE_NORMAL
- en: Building the baseline approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be building the baseline approach. We will use the
    scraped dataset. The main approach we will be using is TF-IDF (Term-frequency,
    Inverse Document Frequency) and cosine similarity. Both of these concepts have
    already been described in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems
    for E-Commerce"), *Recommendation System for e-commerce.* The name of the pertinent
    sections are *Generating features using TF-IDF* and *Building the cosine similarity
    matrix*.
  prefs: []
  type: TYPE_NORMAL
- en: As this application has more textual data, we can use TF-IDF, CountVectorizers,
    cosine similarity, and so on. There are no ratings available for any job. Because
    of this, we are not using other matrix decomposition methods, such as SVD, or
    correlation coefficient-based methods, such as Pearsons'R correlation.
  prefs: []
  type: TYPE_NORMAL
- en: For the baseline approach, we are trying to find out the similarity between
    the resumes, because that is how we will know how similar the user profiles are.
    By using this fact, we can recommend jobs to all the users who share a similar
    kind of professional profile. For the baseline model, our context is to generate
    the similarity score between the resumes.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the baseline approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to develop a simple job recommendation system, we need to perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining constants
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the helper function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating TF-IDF vectors and cosine similarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining constants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will define some constant values. These values are based on the dataset
    we have scraped. In our dataset, we have scraped the dummy resumes for seven companies,
    and there are seven data attributes that we have generated by parsing the resumes.
    We consider 100 resumes as our first training dataset and 50 resumes as our testing
    dataset. The size of our second training dataset is 50\. You can refer to the
    code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining constants](img/B08394_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Code snippet for defining constants'
  prefs: []
  type: TYPE_NORMAL
- en: After this step, we will load the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you know, we have already parsed the resumes that are in the PDF file format.
    We store the parsed data into the pickle format, and we need to load that pickle
    file. We will use the `dill` library to load the pickle file. You can refer to
    the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the dataset](img/B08394_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Code snippet for loading the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We have restored the dataset. As the next step, we need to define the functions
    so that we can build a basic job recommendation system.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the helper function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are various helper functions that will be useful for us. There are a
    total of three helper functions for this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '`my_normalize`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_sim_vector`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_class`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first function is used to normalize the testing score. We will get the
    testing score in the form of a matrix. You can take a look at the code snippet
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining the helper function](img/B08394_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: Code snippet for helper function my_normalize'
  prefs: []
  type: TYPE_NORMAL
- en: This normalization is nothing but the weighted average of the testing score
    matrix. So, it takes the testing score matrix and generates the normalized testing
    score matrix. Bear with me for a while; we will see what the testing score matrix
    looks like when we generate the result of this baseline approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second function basically takes the TF-IDF vector matrix and dataset as
    an input. As an output, it generates the cosine similarity score. You can refer
    to the code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining the helper function](img/B08394_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: Code snippet for helper function get_sim_vector'
  prefs: []
  type: TYPE_NORMAL
- en: 'The third function basically takes the cosine similarity array as an input
    and iterates through it in order to get the maximum cosine value from the cosine
    similarity array. You can find the code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining the helper function](img/B08394_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Code snippet for helper function get_class'
  prefs: []
  type: TYPE_NORMAL
- en: We have understood the input, output, and work of our helper functions. Now,
    it's time to see their usage when we generate TF-IDF vectors and the cosine similarity.
    So, let's move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Generating TF-IDF vectors and cosine similarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will be developing the core logic of the baseline approach.
    We will be using a simple TF-IDF concept. In order to build the job recommendation
    engine using simple TF-IDF, we need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Building the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating IF-IDF vectors for the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the testing dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the similarity score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's build the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Building the training dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Basically, we have not divided our dataset into training and testing. So, for
    training, we need to generate the training dataset by using the code snippet that
    is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the training dataset](img/B08394_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: Code snippet for generating the training dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The code is simple to understand. As you can see, we have used the `train1_size`
    constant value, which we have defined earlier, so that we can generate 100 resumes
    that can be used for training purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Generating IF-IDF vectors for the training dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In order to generate TF-IDF vectors, we will be using scikit-learn''s `TfidfVectorizer`
    API. This basically converts our text data into a numerical format. You can take
    a look at the code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating IF-IDF vectors for the training dataset](img/B08394_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: Code snippet for generating TF-IDF'
  prefs: []
  type: TYPE_NORMAL
- en: By using the preceding code, we can convert our textual training dataset into
    a vectorized format. The matrix of the TF-IDF is used when we generate the predictions
    for testing the dataset. Now, let's build the testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Building the testing dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have trained the model. Now, we need to build the test dataset so that we
    can check how well or how badly our trained model is performing on the test dataset.
    We have used 100 resumes from our dataset for training purposes, so now, we need
    to use the resumes that are not the part of the training dataset. In order to
    generate the testing dataset, we will execute the following code so that we can
    generate the test dataset. You can refer to the code snippet shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the testing dataset](img/B08394_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: Code snippet for generating the testing dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have generated the test dataset using the index of the resume,
    and have taken only those documents that are not a part of the training.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the similarity score
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, first, we will take the test dataset as an input and generate
    the TF-IDF vectors for them. Once the TF-IDF vector matrix has been generated,
    we will use the cosine similarity API in order to generate the similarity score.
    For this API, we will pass the two TF-IDF matrices. One matrix is what we recently
    generated using the testing dataset, and the second matrix is what we generated
    using the training dataset. When we pass these two matrices, we will get the cosine
    similarity array as the output. You can refer to the code snippet given in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the similarity score](img/B08394_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Code snippet for generating a cosine similarity for the testing
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an output, we can generate the cosine similarity array displayed in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the similarity score](img/B08394_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Cosine similarity array'
  prefs: []
  type: TYPE_NORMAL
- en: The array shown in the preceding screenshot has seven elements. Each element
    indicates the similarity of that resume for seven companies. So, if the highest
    cosine value appears in the 0th index, then it means that the given resume is
    more similar to resumes of other users who are working at Amazon. So, we will
    recommend a job opening at Amazon to that particular user, as their resume is
    more similar to other employees who are working at Amazon.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's explore some facts related to the testing matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we build a recommendation engine using TF-IDF, count vectorizer, and cosine
    similarity, we are actually building the content-based recommendation engine.
    There is no predefined testing matrix available for generating the accuracy score.
    In this case, either we need to check our recommendations relevance manually,
    or we can take a heuristic to get the basic intuitive score. In [Chapter 4](ch04.xhtml
    "Chapter 4. Recommendation Systems for E-Commerce"), *Recommendation Systems for
    E-Commerce*, for the baseline approach, we implemented some basic threshold-based
    heuristics to get a basic idea of how well the recommendation engine was working.
    I suggest that you refer to the *Test the result of baseline approach* section
    in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems for E-Commerce"),
    *Recommendation Engine for e-commerce*.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the baseline approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a number of problems with the baseline approach. I will list all
    of them one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: There are not enough data attributes available in the dataset to build a good
    job recommendation system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The baseline approach can''t really provide accurate job recommendations, because
    we have the dataset of user resumes only, and based on that, we can just say something
    like "your resume will look like other employees at Amazon, so please apply for
    job openings at Amazon". Now, the problem is identifying the kind of jobs we need
    to recommend to the user: whether we should recommend all job openings at Amazon,
    or some of them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In my opinion, the baseline solution is not able to provide us the complete
    picture, because of the quality and quantity of the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution for these problems will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the baseline approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we listed the shortcomings of the baseline approach.
    In this section, we will look at how we can overcome these shortcomings. We are
    facing a major problem because we did not use appropriate quality and quantity
    for the dataset. So, first of all, we need to use the dataset in which we have
    information about users' profiles as well as information about the job openings.
    Here, we are not scraping more resumes or posting information about jobs anymore.
    We are using the dataset released by the career builder. We have already seen
    basic information about this dataset earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To build the revised approach, we will use this new dataset. Now, let's start
    building the revised approach.
  prefs: []
  type: TYPE_NORMAL
- en: Building the revised approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be using the readily available job recommendation
    challenge dataset. We have already covered the data attributes of this dataset.
    We will be using a context-based approach to build the recommendation engine.
    In order to build the revised approach, we need to perform the following steps.
    The code for the revised approach is given at this GitHub link: [https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb](https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting the training and testing datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the recommendation engine using the jobs datafile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you know, the dataset is in various files. We need to load all these files.
    Remember that all the datafiles are in a `.tsv` format, so we need to use the
    `\t` delimiter as a parameter. You can refer to the code snippet shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the dataset](img/B08394_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.17: Code snippet for loading the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have used the pandas `read_csv` method with the delimiter
    as a parameter, and loaded the dataset in the form of five different dataframes.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the training and testing datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three data files in which training and testing both types of data
    records is present. These dataframes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: apps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: user_history
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the preceding dataframes, some records are tagged as `Train` and some records
    are tagged as `Test`. The data attribute `Split` indicates which data records
    are considered a part of the training dataset and which ones are used for testing.
    So, we need to filter our dataset. You can take a look at the code snippet given
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting the training and testing datasets](img/B08394_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.18: Code snippet for splitting the training and testing datasets'
  prefs: []
  type: TYPE_NORMAL
- en: We have applied a simple filter operation for all three dataframes and stored
    their output in new dataframes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the **Exploratory Data Analysis** (**EDA**) section.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory Data Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be performing some basic analysis so that we can find
    out what kind of data is present in our dataset. For the revised approach, we
    are building the recommendation system using data attributes given in the jobs
    dataframe. So, before using it to build the recommendation engine, we will check
    the quality of the data records. We need to check whether any blank values are
    present in the dataset. Apart from that, we also need to check the data distribution
    of this dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will perform EDA specifically on geo-location data attributes. Here, we
    have performed grouping by operation on three data columns: City, State, and Country.
    You can take a look at the code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploratory Data Analysis](img/B08394_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: Grouping by operation on City, State, and Country'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the code snippet, there are many records where the state name
    is not present. We need to take care of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from this, we also need to count the data records country-wise so that
    we can find out how many data records are present for each country. You can refer
    to the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploratory Data Analysis](img/B08394_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.20: Code snippet for counting data records country-wise'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding code snippet, there approximately 1 million
    jobs from the US region. We can say that in our dataset, the country location
    for most of the jobs is the US. To make our life easy, we are just considering
    jobs where the country is the US. You can refer to the code snippet given in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploratory Data Analysis](img/B08394_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.21: Code snippet for all the data records where the country is the
    US'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we need to check whether there is an empty data value present for the
    city or state data columns. After observing the output of the preceding code,
    we can see that there are no data records where the city or state name is missing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the state for which we have maximum job openings. Remember
    that we have considered only those jobs where the country location is the US.
    In order to find out the number of jobs state-wise, you can refer to the code
    snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploratory Data Analysis](img/B08394_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.22: Code snippet for generating state-wise number of jobs'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also refer to the graph shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploratory Data Analysis](img/B08394_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.23: Graph for state-wise number of jobs'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, maximum job opportunities are available in California, Texas,
    Florida, Illinois, and New York. We have done enough EDA for the revised approach.
    Now, we are going to start building the recommendation engine.
  prefs: []
  type: TYPE_NORMAL
- en: Building the recommendation engine using the jobs datafile
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explore the code to see how we can build a job recommendation
    engine. We will use TF-IDF and cosine similarity concepts in order to build the
    recommendation engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have taken the `jobs_US dataframe` into account here. This dataframe contains
    jobs where the country is the US. So, we don''t have any junk data records. We
    will be considering only 10,000 data records for training because training for
    1 million data records is time consuming. You can refer to the code shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the recommendation engine using the jobs datafile](img/B08394_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.24: Code snippet of the jobs dataset to build the revised approach'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will be focusing on the job title and job description in order to
    build the recommendation engine. As we are using the metadata of jobs, this is
    the content-based approach. We apply concatenation operation to the job title
    and job descriptions, as well as replace `nan value` with an empty string value.
    You can refer to the code given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the recommendation engine using the jobs datafile](img/B08394_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.25: Code snippet for applying the concatenation operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will generate the TF-IDF vectors for the concatenated string. We will
    use the TF-IDF vector matrix in order to generate the cosine similarity score.
    We will be using the `linear_kernel` function from scikit-learn in order to generate
    the cosine similarity. This function can generate the cosine similarity in less
    time compared to the `cosine_similarity` function of scikit-learn, which takes
    longer. You can refer to the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the recommendation engine using the jobs datafile](img/B08394_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.26: Code snippet for generating TF-IDF and cosine similarity'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have generated a high-dimensional TF-IDF matrix here. By
    using `linear_kernel`, we have generated the cosine similarity score as well.
  prefs: []
  type: TYPE_NORMAL
- en: As we are done with the implementation of the revised approach, we need to test
    the recommendation now.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will generate a similar kind of job recommendation based
    on any given job title. We are passing the job title as the input here, and with
    the help of the cosine similarity score, we can generate the top 10 similar kinds
    of jobs that any user can apply for.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose a person is an SAP business analyst. That person may want
    to apply to a similar kind of job, so here, our function will take the job title
    as the input and generate the top 10 similar kinds of jobs for that particular
    user. The code for generating the top 10 job recommendations is given in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the revised approach](img/B08394_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.27: Code snippet for generating the top 10 job recommendations'
  prefs: []
  type: TYPE_NORMAL
- en: When we see the output, the recommendations start making sense. The person who
    is an SAP business analyst will get jobs recommendations, such as SAP FI/ Co-business
    analyst. The result of the revised approach is satisfying for us, and the recommendations
    seem relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be discussing the problems with the revised approach.
    In the best approach, we can resolve this problem. In the revised approach, we
    have used only the jobs data attribute. We haven't considered the user's profile
    or the user's preferences. During the implementation of the best approach, we
    will also consider the user's profile, and based on the user's profile, we will
    suggest the jobs to them.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will take a look at an intuitive idea for how to optimize
    the revised approach.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how to improve the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we have used data attributes given in the jobs datafile, but we haven't
    used the data attributes from the `users` datafile and the `apps` datafile. The
    `users` datafile contains the user's profile information, and the `apps` datafile
    contains information about which user applied for which jobs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best approach has three simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, with the help of user's profile, we will find and generate the top 10
    similar users.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will try to find out the jobs these 10 people applied for. We can then generate
    `JobIDs`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we will generate the job title using `JobIDs`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, we have taken the user's profile into account, so the recommendations
    are more specific to the particular user base. Now, let's start implementing it.
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already seen the intuitive approach for how we will build the best
    possible approach. Here, we will use the same techniques as the ones we used in
    the revised approach. In this approach, we are adding more data attributes to
    make the recommendation engine more accurate. You can refer to the code by using
    this GitHub link: [https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb](https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the steps we need to take in order to implement the best possible
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Filtering the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the concatenation operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the TF-IDF and cosine similarity score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating recommendations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start implementing each of these listed steps.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this step, we need to filter the user''s dataframe. We are applying the
    filter on the country data column. We need to consider the US-based users because
    there are around 300K users based outside of the US, and other users are from
    elsewhere in the world. The code to filter the user dataframe is given in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filtering the dataset](img/B08394_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.28: Code snippet to filter the user''s dataframe'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's prepare the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the training dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are 300K users, but we are not considering all of them because of the
    limited training time and computational power. Here, we are considering only 10,000
    users. If you have more computational resources, then you can consider a higher
    number of users. You can refer to the code snipp.et shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preparing the training dataset](img/B08394_06_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.29: Code snippet for selecting data records for training'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the concatenation operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this step, we are basically performing the concatenation operation. In order
    to find a similar user profile, we will concatenate the user''s degree type, major,
    and years of experience. We will generate the TF-IDF and cosine similarity for
    this concatenated data value. You can refer to the code snippet given in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying the concatenation operation](img/B08394_06_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.30: Code snippet for applying the concatenation operation'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will generate the TF-IDF and cosine similarity score using this concatenated
    data value.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the TF-IDF and cosine similarity score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will generate the TF-IDF and cosine similarity score using
    the scikit-learn API. We are using the same API that we used in the revised approach.
    Here, we haven''t changed the technique, but we will change the data attributes.
    You can refer to the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the TF-IDF and cosine similarity score](img/B08394_06_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.31: Code snippet for generating TF-IDF and cosine similarity'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have generated the cosine similarity score, so based on that,
    we can generate a similar user profile and give them a job recommendation based
    on their job-application track records.
  prefs: []
  type: TYPE_NORMAL
- en: Generating recommendations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to generate the job recommendation, we need to perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: In order to generate the top 10 similar user profiles, we need
    to pass the UserID, and as an output, we get the 10 UserIDs that are the most
    similar with respect to the input UserID. You can refer to the following screenshot:![Generating
    recommendations](img/B08394_06_32.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 6.32: Code snippet for generating the top 10 similar users'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 2**: We will take the list of `userIDs` that we generated in step 1
    and try to find out the same `UserIDs` in the apps dataframe. The purpose of this
    kind of search operation is that we need to know which user applied for which
    job position. By using the apps data frame, we get `JobIDs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 3**: Once we obtain `JobIDs`, we will obtain job titles using the jobs
    dataframe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code snippet for step 2 and step 3 is given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating recommendations](img/B08394_06_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.33: Code snippet to obtain JobIDs and Job title'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have obtained similar users for `UserID 47`, and as we can
    see in the job recommendations, we get fairly relevant jobs based on the users'
    profile and their educational qualification. In the recommendation, we can see
    medical domain jobs in the Florida location. That is because, in our user base,
    a majority of the users' profiles are from a medical background. As we have considered
    both the user profile and job profile, we are able to get the most relevant job
    recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this entire chapter, we used a content-based approach in order to develop
    a job recommendation engine, and you learned how to scrap the dataset and build
    the baseline job recommendation engine. After that, we explored another dataset.
    For the revised and best approach, we used the readily available dataset. During
    the course of the development of the revised approach, we considered the metadata
    of jobs, and built a recommendation system that works quite well. For the best
    approach, we tried to find out similar user profiles. Based on the user's profile,
    we suggested jobs to the group of users.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be building a summarization application. There,
    we will take a look at documents for the medical domain and try to summarize them.
    We will use deep-learning algorithms in order to build an application. So, keep
    reading!
  prefs: []
  type: TYPE_NORMAL
