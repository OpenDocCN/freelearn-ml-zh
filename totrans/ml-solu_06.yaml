- en: Chapter 6. Job Recommendation Engine
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。职位推荐引擎
- en: We have already seen how to develop a recommendation system for the e-commerce
    product in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems for E-Commerce"),
    *Recommendation Systems for e-Commerce*, Now, we will apply the same concepts
    that you learned in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems
    for E-Commerce"), *Recommendation Systems for e-Commerce* but the type and format
    of the dataset is different. Basically, we will build a job recommendation engine.
    For this application, we have taken into account the text dataset. The main concept
    of building the recommendation engine will not change, but this chapter gives
    you a fair idea of how to apply the same concepts to different types of datasets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第4章](ch04.xhtml "第4章。电子商务推荐系统")中看到了如何开发电子商务产品的推荐系统，*电子商务推荐系统*，现在，我们将应用你在[第4章](ch04.xhtml
    "第4章。电子商务推荐系统")中学习到的相同概念，但数据集的类型和格式不同。基本上，我们将构建一个职位推荐引擎。对于这个应用，我们已经考虑了文本数据集。构建推荐引擎的主要概念不会改变，但本章将给你一个很好的想法，了解如何将相同的概念应用于不同类型的数据集。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing the problem statement
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: Understanding the datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集
- en: Building the baseline approach
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建基线方法
- en: Implementing the baseline approach
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现基线方法
- en: Understanding the testing matrix
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解测试矩阵
- en: Problems with the baseline approach
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线方法的问题
- en: Optimizing the baseline approach
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化基线方法
- en: Building the revised approach
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建修订方法
- en: Implementing the revised approach
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现修订方法
- en: Testing the revised approach
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试修订方法
- en: Problems with the revised approach
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修订方法存在的问题
- en: Understanding how to improve the revised approach
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何改进修订方法
- en: The best approach
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳方法
- en: Implementing the best approach
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现最佳方法
- en: Summary
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述
- en: So, let's discuss the problem statement.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们来讨论问题陈述。
- en: Introducing the problem statement
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: In this chapter, we will build an engine that can recommend jobs to any user.
    This is the simplest goal we want to achieve. How we are going to build it? In
    order to answer this question, let me give you an idea about what kind of approaches
    we will take in order to build a job recommendation system.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个可以向任何用户推荐工作的引擎。这是我们想要实现的最简单目标。我们将如何构建它？为了回答这个问题，让我给你一个关于我们将采取什么方法来构建职位推荐系统的大致想法。
- en: For our baseline approach, we will scrape resumes of dummy users and try to
    build a job recommendation engine based on the scraped dataset. The reason we
    are scraping the dataset is that, most of the time, there will not be any dataset
    available for many data science applications. Suppose you are in a position where
    you have not found any dataset. What you will do then? I want to provide a solution
    for these kinds of scenarios. So, you will learn how to scrape the data and build
    the baseline solution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的基线方法，我们将抓取模拟用户的简历，并尝试基于抓取的数据集构建一个职位推荐引擎。我们抓取数据集的原因是，在许多数据科学应用中，大多数时候都不会有任何数据集可用。假设你处于一个没有找到任何数据集的位置。那时你会怎么做？我想为这类场景提供解决方案。所以，你将学习如何抓取数据并构建基线解决方案。
- en: In the revised approach, we will be using a dataset hosted by Kaggle. Using
    the content-based approach, we will be building a job recommendation engine. For
    the best approach, we will be using the concept of user-based collaborative filtering
    for this domain, and build the final job recommendation system.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在修订方法中，我们将使用Kaggle托管的数据集。使用基于内容的策略，我们将构建一个职位推荐引擎。对于最佳方法，我们将使用基于用户的协同过滤概念来构建这个领域的最终职位推荐系统。
- en: Now, let's look into the datasets.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看数据集。
- en: Understanding the datasets
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据集
- en: 'Here, we are using two datasets. The two datasets are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了两个数据集。这两个数据集如下：
- en: The scraped dataset
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抓取的数据集
- en: The job recommendation challenge dataset
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 职位推荐挑战数据集
- en: Let's start with the scraped dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从抓取的数据集开始。
- en: Scraped dataset
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抓取的数据集
- en: 'For this dataset, we have scraped the dummy resume from indeed.com (we are
    using this data just for learning and research purposes). We will download the
    resumes of users in PDF format. These will become our dataset. The code for this
    is given at this GitHub link: [https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，我们从indeed.com抓取了模拟简历（我们使用这些数据仅用于学习和研究目的）。我们将下载用户的PDF格式的简历。这些将成为我们的数据集。这个代码可以在以下GitHub链接中找到：[https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py)。
- en: 'Take a look at the code given in the following screenshot:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下截图给出的代码：
- en: '![Scraped dataset](img/B08394_06_01.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![抓取的数据集](img/B08394_06_01.jpg)'
- en: 'Figure 6.1: Code snippet for scraping the data'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：抓取数据的代码片段
- en: 'Using the preceding code, we can download the resumes. We have used the `requests`
    library and `urllib` to scrape the data. All these downloaded resumes are in PDF
    form, so we need to parse them. To parse the PDF document, we will use a Python
    library called `PDFminer`. We need to extract the following data attributes from
    the PDF documents:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们可以下载简历。我们使用了`requests`库和`urllib`来抓取数据。所有这些下载的简历都是以PDF形式，因此我们需要解析它们。为了解析PDF文档，我们将使用一个名为`PDFminer`的Python库。我们需要从PDF文档中提取以下数据属性：
- en: Work experience
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作经验
- en: Education
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教育
- en: Skills
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技能
- en: Awards
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖项
- en: Certifications
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证书
- en: Additional information
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他信息
- en: 'You can take a look at the code snippet shown in the following screenshot:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看以下截图所示的代码片段：
- en: '![Scraped dataset](img/B08394_06_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![抓取的数据集](img/B08394_06_02.jpg)'
- en: 'Figure 6.2: Code snippet to parse PDF documents'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：解析PDF文档的代码片段
- en: 'Basically, `PDFminer` is converting the content of PDF into text. Once we have
    the text data using regular expressions, we can fetch the necessary details. You
    can see the entire code by using this GitHub link: [https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，`PDFminer`将PDF的内容转换为文本。一旦我们使用正则表达式获取了文本数据，我们就可以获取必要的详细信息。您可以通过使用以下GitHub链接查看整个代码：[https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py)。
- en: 'Once we fetch all the necessary information, we will save the data in a pickle
    format. Now, you don''t need to scrap the data and fetch all necessary information.
    I have uploaded the data into a pickle file format at this GitHub link: [https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获取了所有必要的信息，我们将以pickle格式保存数据。现在，您不需要抓取数据并获取所有必要的信息。我已经将数据上传到pickle文件格式，您可以通过以下GitHub链接查看：[https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl)
- en: We will use the `resume_data.pkl` file for our baseline approach.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`resume_data.pkl`文件作为我们的基线方法。
- en: Job recommendation challenge dataset
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 职业推荐挑战数据集
- en: 'This dataset is provided by [www.careerbuilder.com](http://www.careerbuilder.com)
    and is hosted on Kaggle. You can download the dataset using this link: [https://www.kaggle.com/c/job-recommendation/data](https://www.kaggle.com/c/job-recommendation/data).
    These are the data files that we are going to use for our revised and best approach.
    All the values given in these datafiles are tab-separated:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集由[www.careerbuilder.com](http://www.careerbuilder.com)提供，并在Kaggle上托管。您可以通过此链接下载数据集：[https://www.kaggle.com/c/job-recommendation/data](https://www.kaggle.com/c/job-recommendation/data)。这些是我们将要用于我们修订和最佳方法的数据文件。这些数据文件中的所有值都是用制表符分隔的：
- en: '`apps.tsv`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apps.tsv`'
- en: '`users.tsv`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`users.tsv`'
- en: '`jobs.zip`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jobs.zip`'
- en: '`user_history.tsv`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user_history.tsv`'
- en: apps.tsv
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: apps.tsv
- en: 'This datafile contains the records of users'' job applications. It indicates
    job positions that a particular user has applied for. The job position is described
    by the JobID column. All the necessary information about this datafile is given
    in the following screenshot:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据文件包含用户求职申请的记录。它表示特定用户申请的职位。职位由JobID列描述。有关此数据文件的必要信息在以下截图给出：
- en: '![apps.tsv](img/B08394_06_03.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![apps.tsv](img/B08394_06_03.jpg)'
- en: 'Figure 6.3: Data information about apps.tsv'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：关于apps.tsv的数据信息
- en: 'There are five data columns:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有五个数据列：
- en: '`UserId`: This indicates the unique ID for a given user. By using this ID,
    we can access the user''s profile.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UserId`: 这表示给定用户的唯一标识符。通过使用此ID，我们可以访问用户的个人资料。'
- en: '`WindowsID`: This is the mask data attribute with the constant value of 1\.
    This data attribute is not important for us.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WindowsID`: 这是一个具有常量值1的掩码数据属性。此数据属性对我们来说并不重要。'
- en: '`Split`: This data attribute indicates which data records we should consider
    for training and testing.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Split`: 此数据属性表示我们应该考虑哪些数据记录用于训练和测试。'
- en: '`Application date`: This is the timestamp at which the user applied for the
    job.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Application date`: 这是用户申请工作的时间戳。'
- en: '`JobID`: This attribute indicates the `JobIds` for which the user nominates
    themselves. Using this `JobId`, we can access other information about a particular
    job.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JobID`: 此属性表示用户提名的工作的`JobIds`。使用此`JobId`，我们可以访问特定工作的其他信息。'
- en: users.tsv
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: users.tsv
- en: 'This datafile contains the user profile and all user-related information. You
    can find all the necessary information displayed in the following screenshot:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据文件包含用户个人资料和所有相关用户信息。您可以在以下屏幕截图中找到所有必要的信息：
- en: '![users.tsv](img/B08394_06_04.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![users.tsv](img/B08394_06_04.jpg)'
- en: Figure 6.4 Data information about users.tsv
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 关于users.tsv的数据信息
- en: 'These are the data attributes:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是数据属性：
- en: '`UserID`: This data attribute indicates the user''s unique identification number.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UserID`: 此数据属性表示用户的唯一识别号码。'
- en: '`WindowID`: This is the mask data attribute with a constant value of 1\. This
    data attribute is not important for us.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WindowID`: 这是一个具有常量值1的掩码数据属性。此数据属性对我们来说并不重要。'
- en: '`Split`: This data attribute indicates which data records we should consider
    for training and testing.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Split`: 此数据属性表示我们应该考虑哪些数据记录用于训练和测试。'
- en: '`City`: This attribute indicates the user''s current city.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`City`: 此属性表示用户的当前城市。'
- en: '`State`: This attribute indicates the user''s state.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`State`: 此属性表示用户的州。'
- en: 'Country: This attribute indicates the user''s country.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国家：此属性表示用户的国籍。
- en: '`ZipCode`: This data attribute indicates the user''s ZIP code.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ZipCode`: 此数据属性表示用户的邮政编码。'
- en: '`DegreeType`: This data column indicates the user''s degree; whether the user
    is a high school pass-out or has a bachelor''s degree.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DegreeType`: 此数据列表示用户的学位；用户是高中毕业生还是拥有学士学位。'
- en: '`Major`: This data attribute indicates the major subject in which the user
    has a degree.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Major`: 此数据属性表示用户学位的主修科目。'
- en: '`GraduationDate`: This data attribute indicates the graduation date of the
    user.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GraduationDate`: 此数据属性表示用户的毕业日期。'
- en: '`WorkHistoryCount`: This data attribute indicates the number of companies the
    user has worked for.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WorkHistoryCount`: 此数据属性表示用户工作过的公司数量。'
- en: '`TotalYearsExperience`: This data column indicates the user''s total years
    of experience.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TotalYearsExperience`: 此数据列表示用户的总工作经验年数。'
- en: '`CurrentlyEmployed`: This data attribute has a binary value. If the user is
    currently employed, then the value is *Yes*; if not, then the value is *No*.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CurrentlyEmployed`: 此数据属性具有二进制值。如果用户目前有工作，则值为*是*；如果没有，则值为*否*。'
- en: '`ManagedOthers`: This data attribute has a binary value as well. If the user
    is managing other people, then the value of this column is *Yes*; if the user
    is not managing other people, then the value of this column is *No*.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ManagedOthers`: 此数据属性也有二进制值。如果用户正在管理其他人，则此列的值为*是*；如果用户没有管理其他人，则此列的值为*否*。'
- en: '`ManagedHowMany`: This data attribute has a numerical value. The value of this
    column indicates the number of people that are managed by the user. If the user
    is not managing anyone, then the value is 0.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ManagedHowMany`: 此数据属性具有数值。此列的值表示由用户管理的员工人数。如果用户没有管理任何人，则此值为0。'
- en: Jobs.zip
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jobs.zip
- en: 'When you extract this ZIP file, you can get the `jobs.tsv` file. There is more
    information available in the following screenshot:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当您解压此ZIP文件时，您可以获取`jobs.tsv`文件。以下屏幕截图中有更多信息：
- en: '![Jobs.zip](img/B08394_06_05.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![Jobs.zip](img/B08394_06_05.jpg)'
- en: 'Figure 6.5: Data information about jobs.tsv'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：关于jobs.tsv的数据信息
- en: '`JobID`: This is the unique ID for each job present in the dataset.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JobID`: 这是数据集中每个工作唯一的标识符。'
- en: '`WindowID`: This is the mask data attribute that has a constant value of 1\.
    This data attribute is not important for us.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WindowID`: 这是一个具有常量值1的掩码数据属性。此数据属性对我们来说并不重要。'
- en: '`Title`: This data attribute indicates the job title.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Title`: 此数据属性表示工作标题。'
- en: '`Description`: This data attribute indicates the job description.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Description`: 此数据属性表示工作描述。'
- en: '`Requirements`: This data attribute indicates the job requirements.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Requirements`：这个数据属性表示工作要求。'
- en: '`City`: This data field indicates the job location in terms of the city.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`City`：这个数据字段表示工作地点的城市。'
- en: '`State`: This data field indicates the job location in terms of the state.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`State`：这个数据字段表示工作地点的州。'
- en: '`Country`: This data field indicates the job location in terms of the country.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Country`：这个数据字段表示工作地点的国家。'
- en: '`Zip5`: This data field indicates the ZIP code of the job location.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Zip5`：这个数据字段表示工作地点的ZIP代码。'
- en: '`StartDate`: This date indicates when the job is posted or is open for applications.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StartDate`：这个日期表示工作发布或开放申请的时间。'
- en: '`EndDate`: This date is the deadline for the job application.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EndDate`：这个日期是工作申请的截止日期。'
- en: user_history.tsv
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: user_history.tsv
- en: 'The `user_history.tsv` file contains the user''s job history. There is more
    information available on this in the following screenshot:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`user_history.tsv` 文件包含用户的职业历史。关于此信息，以下截图提供了更多内容：'
- en: '![user_history.tsv](img/B08394_06_06.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![user_history.tsv](img/B08394_06_06.jpg)'
- en: 'Figure 6.6: Data information about user_history.tsv'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：关于user_history.tsv的数据信息
- en: There are only two new columns for this datafile.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据文件只有两个新列。
- en: '`Sequence`: This sequence is a numerical field. The number indicates the sequential
    order of the user''s job.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sequence`：这个序列是一个数值字段。数字表示用户职业的顺序。'
- en: '`JobTitle`: This data field indicates the job title of the user.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JobTitle`：这个数据字段表示用户的职位名称。'
- en: We have covered all the attributes in our datafiles; now let's start building
    the baseline approach.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了我们数据文件中的所有属性；现在让我们开始构建基线方法。
- en: Building the baseline approach
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基线方法
- en: In this section, we will be building the baseline approach. We will use the
    scraped dataset. The main approach we will be using is TF-IDF (Term-frequency,
    Inverse Document Frequency) and cosine similarity. Both of these concepts have
    already been described in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems
    for E-Commerce"), *Recommendation System for e-commerce.* The name of the pertinent
    sections are *Generating features using TF-IDF* and *Building the cosine similarity
    matrix*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建基线方法。我们将使用抓取的数据集。我们将使用的主要方法是TF-IDF（词频，逆文档频率）和余弦相似度。这两个概念已经在[第4章](ch04.xhtml
    "第4章。电子商务推荐系统")，*电子商务推荐系统*中描述过。相关章节的名称是*使用TF-IDF生成特征*和*构建余弦相似度矩阵*。
- en: As this application has more textual data, we can use TF-IDF, CountVectorizers,
    cosine similarity, and so on. There are no ratings available for any job. Because
    of this, we are not using other matrix decomposition methods, such as SVD, or
    correlation coefficient-based methods, such as Pearsons'R correlation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个应用程序有更多的文本数据，我们可以使用TF-IDF、CountVectorizers、余弦相似度等。没有任何工作有评分。因此，我们没有使用其他矩阵分解方法，如SVD，或基于相关系数的方法，如皮尔逊R相关系数。
- en: For the baseline approach, we are trying to find out the similarity between
    the resumes, because that is how we will know how similar the user profiles are.
    By using this fact, we can recommend jobs to all the users who share a similar
    kind of professional profile. For the baseline model, our context is to generate
    the similarity score between the resumes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基线方法，我们试图找出简历之间的相似度，因为这将告诉我们用户配置文件有多相似。通过使用这个事实，我们可以向所有具有类似专业配置文件的用户推荐工作。对于基线模型，我们的上下文是生成简历之间的相似度分数。
- en: Implementing the baseline approach
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施基线方法
- en: 'In order to develop a simple job recommendation system, we need to perform
    the following steps:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发一个简单的职位推荐系统，我们需要执行以下步骤：
- en: Defining constants
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义常量
- en: Loading the dataset
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据集
- en: Defining the helper function
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义辅助函数
- en: Generating TF-IDF vectors and cosine similarity
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成TF-IDF向量和余弦相似度
- en: Defining constants
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义常量
- en: 'We will define some constant values. These values are based on the dataset
    we have scraped. In our dataset, we have scraped the dummy resumes for seven companies,
    and there are seven data attributes that we have generated by parsing the resumes.
    We consider 100 resumes as our first training dataset and 50 resumes as our testing
    dataset. The size of our second training dataset is 50\. You can refer to the
    code snippet shown in the following screenshot:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一些常量值。这些值基于我们抓取的数据集。在我们的数据集中，我们抓取了七家公司的模拟简历，并且通过解析简历生成了七个数据属性。我们将100份简历视为我们的第一个训练数据集，50份简历作为我们的测试数据集。我们的第二个训练数据集的大小是50。您可以参考以下截图所示的代码片段：
- en: '![Defining constants](img/B08394_06_07.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![定义常量](img/B08394_06_07.jpg)'
- en: 'Figure 6.7: Code snippet for defining constants'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：定义常量的代码片段
- en: After this step, we will load the dataset.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤之后，我们将加载数据集。
- en: Loading the dataset
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'As you know, we have already parsed the resumes that are in the PDF file format.
    We store the parsed data into the pickle format, and we need to load that pickle
    file. We will use the `dill` library to load the pickle file. You can refer to
    the code snippet shown in the following screenshot:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，我们已解析了PDF文件格式的简历。我们将解析的数据存储为pickle格式，并需要加载该pickle文件。我们将使用`dill`库来加载pickle文件。您可以在以下屏幕截图所示的代码片段中参考：
- en: '![Loading the dataset](img/B08394_06_08.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![加载数据集](img/B08394_06_08.jpg)'
- en: 'Figure 6.8: Code snippet for loading the dataset'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：加载数据集的代码片段
- en: We have restored the dataset. As the next step, we need to define the functions
    so that we can build a basic job recommendation system.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经恢复了数据集。作为下一步，我们需要定义函数，以便我们可以构建一个基本的职位推荐系统。
- en: Defining the helper function
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义辅助函数
- en: 'There are various helper functions that will be useful for us. There are a
    total of three helper functions for this approach:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种辅助函数对我们很有用。对于这种方法，总共有三个辅助函数：
- en: '`my_normalize`'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`my_normalize`'
- en: '`get_sim_vector`'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_sim_vector`'
- en: '`get_class`'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_class`'
- en: 'The first function is used to normalize the testing score. We will get the
    testing score in the form of a matrix. You can take a look at the code snippet
    shown in the following screenshot:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数用于标准化测试分数。我们将以矩阵的形式获得测试分数。您可以在以下屏幕截图所示的代码片段中查看：
- en: '![Defining the helper function](img/B08394_06_09.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![定义辅助函数](img/B08394_06_09.jpg)'
- en: 'Figure 6.9: Code snippet for helper function my_normalize'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：辅助函数my_normalize的代码片段
- en: This normalization is nothing but the weighted average of the testing score
    matrix. So, it takes the testing score matrix and generates the normalized testing
    score matrix. Bear with me for a while; we will see what the testing score matrix
    looks like when we generate the result of this baseline approach.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这种标准化实际上就是测试分数矩阵的加权平均值。因此，它接受测试分数矩阵并生成标准化的测试分数矩阵。请稍等片刻；当我们生成基线方法的结果时，我们将看到测试分数矩阵的样子。
- en: 'The second function basically takes the TF-IDF vector matrix and dataset as
    an input. As an output, it generates the cosine similarity score. You can refer
    to the code snippet given in the following screenshot:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数基本上将TF-IDF向量矩阵和数据集作为输入。作为输出，它生成余弦相似度分数。您可以在以下屏幕截图中找到提供的代码片段：
- en: '![Defining the helper function](img/B08394_06_10.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![定义辅助函数](img/B08394_06_10.jpg)'
- en: 'Figure 6.10: Code snippet for helper function get_sim_vector'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10：辅助函数get_sim_vector的代码片段
- en: 'The third function basically takes the cosine similarity array as an input
    and iterates through it in order to get the maximum cosine value from the cosine
    similarity array. You can find the code snippet given in the following screenshot:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个函数基本上将余弦相似度数组作为输入，并按顺序遍历它以从余弦相似度数组中获得最大余弦值。您可以在以下屏幕截图中找到提供的代码片段：
- en: '![Defining the helper function](img/B08394_06_11.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![定义辅助函数](img/B08394_06_11.jpg)'
- en: 'Figure 6.11: Code snippet for helper function get_class'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：辅助函数get_class的代码片段
- en: We have understood the input, output, and work of our helper functions. Now,
    it's time to see their usage when we generate TF-IDF vectors and the cosine similarity.
    So, let's move on to the next section.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经理解了辅助函数的输入、输出和工作原理。现在，让我们看看在生成TF-IDF向量和余弦相似度时它们的用法。因此，让我们继续到下一节。
- en: Generating TF-IDF vectors and cosine similarity
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成TF-IDF向量和余弦相似度
- en: 'In this section, we will be developing the core logic of the baseline approach.
    We will be using a simple TF-IDF concept. In order to build the job recommendation
    engine using simple TF-IDF, we need to perform the following steps:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开发基线方法的核心理念。我们将使用简单的TF-IDF概念。为了使用简单的TF-IDF构建工作推荐引擎，我们需要执行以下步骤：
- en: Building the training dataset
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建训练数据集
- en: Generating IF-IDF vectors for the training dataset
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练数据集生成IF-IDF向量
- en: Building the testing dataset
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建测试数据集
- en: Generating the similarity score
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成相似度分数
- en: Let's build the training dataset.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建训练数据集。
- en: Building the training dataset
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建训练数据集
- en: 'Basically, we have not divided our dataset into training and testing. So, for
    training, we need to generate the training dataset by using the code snippet that
    is shown in the following screenshot:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们并没有将数据集划分为训练集和测试集。因此，对于训练，我们需要使用以下截图中的代码片段来生成训练数据集：
- en: '![Building the training dataset](img/B08394_06_12.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![构建训练数据集](img/B08394_06_12.jpg)'
- en: 'Figure 6.12: Code snippet for generating the training dataset'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12：生成训练数据集的代码片段
- en: The code is simple to understand. As you can see, we have used the `train1_size`
    constant value, which we have defined earlier, so that we can generate 100 resumes
    that can be used for training purposes.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 代码易于理解。如您所见，我们使用了之前定义的`train1_size`常量值，以便我们可以生成100份可用于训练的简历。
- en: Now, let's move on to the next step.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续下一步。
- en: Generating IF-IDF vectors for the training dataset
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为训练数据集生成IF-IDF向量
- en: 'In order to generate TF-IDF vectors, we will be using scikit-learn''s `TfidfVectorizer`
    API. This basically converts our text data into a numerical format. You can take
    a look at the code snippet given in the following screenshot:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成TF-IDF向量，我们将使用scikit-learn的`TfidfVectorizer` API。这基本上将我们的文本数据转换为数值格式。您可以查看以下截图中的代码片段：
- en: '![Generating IF-IDF vectors for the training dataset](img/B08394_06_13.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![为训练数据集生成IF-IDF向量](img/B08394_06_13.jpg)'
- en: 'Figure 6.13: Code snippet for generating TF-IDF'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13：生成TF-IDF的代码片段
- en: By using the preceding code, we can convert our textual training dataset into
    a vectorized format. The matrix of the TF-IDF is used when we generate the predictions
    for testing the dataset. Now, let's build the testing dataset.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用前面的代码，我们可以将我们的文本训练数据集转换为向量格式。当我们为测试数据集生成预测时，使用TF-IDF矩阵。现在，让我们构建测试数据集。
- en: Building the testing dataset
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建测试数据集
- en: 'We have trained the model. Now, we need to build the test dataset so that we
    can check how well or how badly our trained model is performing on the test dataset.
    We have used 100 resumes from our dataset for training purposes, so now, we need
    to use the resumes that are not the part of the training dataset. In order to
    generate the testing dataset, we will execute the following code so that we can
    generate the test dataset. You can refer to the code snippet shown in the following
    screenshot:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练了模型。现在，我们需要构建测试数据集，以便我们可以检查我们的训练模型在测试数据集上的表现是好是坏。我们已从数据集中使用了100份简历进行训练，因此现在我们需要使用那些不属于训练数据集的简历。为了生成测试数据集，我们将执行以下代码，以便我们可以生成测试数据集。您可以参考以下截图中的代码片段：
- en: '![Building the testing dataset](img/B08394_06_14.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![构建测试数据集](img/B08394_06_14.jpg)'
- en: 'Figure 6.14: Code snippet for generating the testing dataset'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14：生成测试数据集的代码片段
- en: As you can see, we have generated the test dataset using the index of the resume,
    and have taken only those documents that are not a part of the training.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们使用简历的索引生成了测试数据集，并仅选取了那些不属于训练集的文档。
- en: Generating the similarity score
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成相似度分数
- en: 'In this section, first, we will take the test dataset as an input and generate
    the TF-IDF vectors for them. Once the TF-IDF vector matrix has been generated,
    we will use the cosine similarity API in order to generate the similarity score.
    For this API, we will pass the two TF-IDF matrices. One matrix is what we recently
    generated using the testing dataset, and the second matrix is what we generated
    using the training dataset. When we pass these two matrices, we will get the cosine
    similarity array as the output. You can refer to the code snippet given in the
    following screenshot:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，首先，我们将以测试数据集作为输入并为其生成TF-IDF向量。一旦生成了TF-IDF向量矩阵，我们将使用余弦相似度API来生成相似度分数。对于此API，我们将传递两个TF-IDF矩阵。一个矩阵是我们最近使用测试数据集生成的，另一个矩阵是我们使用训练数据集生成的。当我们传递这两个矩阵时，我们将得到余弦相似度数组作为输出。您可以参考以下截图中的代码片段：
- en: '![Generating the similarity score](img/B08394_06_15.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![生成相似度分数](img/B08394_06_15.jpg)'
- en: 'Figure 6.15: Code snippet for generating a cosine similarity for the testing
    dataset'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15：生成测试数据集余弦相似度的代码片段
- en: 'As an output, we can generate the cosine similarity array displayed in the
    following screenshot:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输出，我们可以生成以下截图显示的余弦相似度数组：
- en: '![Generating the similarity score](img/B08394_06_16.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![生成相似度分数](img/B08394_06_16.jpg)'
- en: 'Figure 6.16: Cosine similarity array'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16：余弦相似度数组
- en: The array shown in the preceding screenshot has seven elements. Each element
    indicates the similarity of that resume for seven companies. So, if the highest
    cosine value appears in the 0th index, then it means that the given resume is
    more similar to resumes of other users who are working at Amazon. So, we will
    recommend a job opening at Amazon to that particular user, as their resume is
    more similar to other employees who are working at Amazon.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 前一截图显示的数组包含七个元素。每个元素表示该简历与七个公司的相似度。因此，如果最高的余弦值出现在0号索引，那么这意味着给定的简历与其他在亚马逊工作的用户的简历更相似。所以，我们将向该特定用户推荐亚马逊的工作机会，因为他们的简历与其他在亚马逊工作的员工更相似。
- en: Now, let's explore some facts related to the testing matrix.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探讨一些与测试矩阵相关的事实。
- en: Understanding the testing matrix
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解测试矩阵
- en: When we build a recommendation engine using TF-IDF, count vectorizer, and cosine
    similarity, we are actually building the content-based recommendation engine.
    There is no predefined testing matrix available for generating the accuracy score.
    In this case, either we need to check our recommendations relevance manually,
    or we can take a heuristic to get the basic intuitive score. In [Chapter 4](ch04.xhtml
    "Chapter 4. Recommendation Systems for E-Commerce"), *Recommendation Systems for
    E-Commerce*, for the baseline approach, we implemented some basic threshold-based
    heuristics to get a basic idea of how well the recommendation engine was working.
    I suggest that you refer to the *Test the result of baseline approach* section
    in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems for E-Commerce"),
    *Recommendation Engine for e-commerce*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用TF-IDF、计数向量化器和余弦相似度构建推荐引擎时，我们实际上是在构建基于内容的推荐引擎。没有预定义的测试矩阵可用于生成准确度分数。在这种情况下，我们可能需要手动检查推荐的相关性，或者我们可以采用启发式方法来获得基本的直观分数。在[第4章](ch04.xhtml
    "第4章。电子商务推荐系统")，*电子商务推荐系统*中，对于基线方法，我们实现了一些基于阈值的启发式方法来了解推荐引擎的工作效果。我建议您参考[第4章](ch04.xhtml
    "第4章。电子商务推荐系统")中的*测试基线方法的结果*部分。
- en: Problems with the baseline approach
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基线方法的问题
- en: 'There are a number of problems with the baseline approach. I will list all
    of them one by one:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 基线方法存在许多问题。我将逐一列出：
- en: There are not enough data attributes available in the dataset to build a good
    job recommendation system.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集中可用的数据属性不足以构建一个良好的职位推荐系统。
- en: 'The baseline approach can''t really provide accurate job recommendations, because
    we have the dataset of user resumes only, and based on that, we can just say something
    like "your resume will look like other employees at Amazon, so please apply for
    job openings at Amazon". Now, the problem is identifying the kind of jobs we need
    to recommend to the user: whether we should recommend all job openings at Amazon,
    or some of them.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线方法实际上无法提供准确的职位推荐，因为我们只有用户简历的数据集，基于这些数据，我们只能说“你的简历看起来像亚马逊的其他员工，所以请申请亚马逊的工作机会”。现在的问题是确定我们需要向用户推荐哪种类型的工作：我们应该推荐亚马逊的所有职位空缺，还是其中的一些。
- en: In my opinion, the baseline solution is not able to provide us the complete
    picture, because of the quality and quantity of the dataset.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我看来，由于数据集的质量和数量，基线解决方案无法提供完整的图景。
- en: The solution for these problems will be discussed in the next section.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的解决方案将在下一节讨论。
- en: Optimizing the baseline approach
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化基线方法
- en: In the previous section, we listed the shortcomings of the baseline approach.
    In this section, we will look at how we can overcome these shortcomings. We are
    facing a major problem because we did not use appropriate quality and quantity
    for the dataset. So, first of all, we need to use the dataset in which we have
    information about users' profiles as well as information about the job openings.
    Here, we are not scraping more resumes or posting information about jobs anymore.
    We are using the dataset released by the career builder. We have already seen
    basic information about this dataset earlier in this chapter.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们列出了基线方法的不足。在本节中，我们将探讨如何克服这些不足。我们面临一个主要问题，因为我们没有使用适当的质量和数量来处理数据集。因此，首先，我们需要使用包含用户资料信息以及职位空缺信息的数据库。在这里，我们不再抓取更多简历或发布关于工作的信息。我们正在使用由职业建设者发布的数据库。我们已经在本章前面看到了关于这个数据库的基本信息。
- en: To build the revised approach, we will use this new dataset. Now, let's start
    building the revised approach.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建修订的方法，我们将使用这个新的数据集。现在，让我们开始构建修订的方法。
- en: Building the revised approach
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建修订的方法
- en: 'In this section, we will be using the readily available job recommendation
    challenge dataset. We have already covered the data attributes of this dataset.
    We will be using a context-based approach to build the recommendation engine.
    In order to build the revised approach, we need to perform the following steps.
    The code for the revised approach is given at this GitHub link: [https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb](https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用现成的职位推荐挑战数据集。我们已经覆盖了该数据集的数据属性。我们将使用基于上下文的方法来构建推荐引擎。为了构建修订的方法，我们需要执行以下步骤。修订方法的代码在以下GitHub链接中给出：[https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb](https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb)
- en: 'Let''s implement the following steps:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下步骤：
- en: Loading the dataset
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据集
- en: Splitting the training and testing datasets
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割训练和测试数据集
- en: Exploratory data analysis
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: Building the recommendation engine using the jobs datafile
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用工作数据文件构建推荐引擎
- en: Loading the dataset
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'As you know, the dataset is in various files. We need to load all these files.
    Remember that all the datafiles are in a `.tsv` format, so we need to use the
    `\t` delimiter as a parameter. You can refer to the code snippet shown in the
    following screenshot:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，数据集在多个文件中。我们需要加载所有这些文件。请记住，所有数据文件都是`.tsv`格式，因此我们需要使用`\t`作为参数。您可以参考以下屏幕截图中显示的代码片段：
- en: '![Loading the dataset](img/B08394_06_17.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![加载数据集](img/B08394_06_17.jpg)'
- en: 'Figure 6.17: Code snippet for loading the dataset'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17：加载数据集的代码片段
- en: As you can see, we have used the pandas `read_csv` method with the delimiter
    as a parameter, and loaded the dataset in the form of five different dataframes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们使用了pandas的`read_csv`方法，并将分隔符作为参数，以五种不同的数据框形式加载数据集。
- en: Splitting the training and testing datasets
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割训练和测试数据集
- en: 'There are three data files in which training and testing both types of data
    records is present. These dataframes are as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个数据文件，其中包含训练和测试两种类型的数据记录。以下数据框如下：
- en: apps
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序
- en: user_history
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: user_history
- en: users
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: users
- en: 'In the preceding dataframes, some records are tagged as `Train` and some records
    are tagged as `Test`. The data attribute `Split` indicates which data records
    are considered a part of the training dataset and which ones are used for testing.
    So, we need to filter our dataset. You can take a look at the code snippet given
    in the following screenshot:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的数据框中，一些记录被标记为`Train`，一些记录被标记为`Test`。数据属性`Split`指示哪些数据记录被认为是训练数据集的一部分，哪些用于测试。因此，我们需要过滤我们的数据集。您可以查看以下屏幕截图中给出的代码片段：
- en: '![Splitting the training and testing datasets](img/B08394_06_18.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![分割训练和测试数据集](img/B08394_06_18.jpg)'
- en: 'Figure 6.18: Code snippet for splitting the training and testing datasets'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18：分割训练和测试数据集的代码片段
- en: We have applied a simple filter operation for all three dataframes and stored
    their output in new dataframes.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对三个数据框都应用了一个简单的过滤操作，并将它们的输出存储在新数据框中。
- en: Now, let's move on to the **Exploratory Data Analysis** (**EDA**) section.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续到**探索性数据分析**（**EDA**）部分。
- en: Exploratory Data Analysis
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: In this section, we will be performing some basic analysis so that we can find
    out what kind of data is present in our dataset. For the revised approach, we
    are building the recommendation system using data attributes given in the jobs
    dataframe. So, before using it to build the recommendation engine, we will check
    the quality of the data records. We need to check whether any blank values are
    present in the dataset. Apart from that, we also need to check the data distribution
    of this dataframe.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将执行一些基本分析，以便我们可以找出数据集中存在哪些类型的数据。对于修订的方法，我们正在使用工作数据框中给出的数据属性构建推荐系统。因此，在将其用于构建推荐引擎之前，我们将检查数据记录的质量。我们需要检查数据集中是否存在任何空白值。除此之外，我们还需要检查这个数据框的数据分布。
- en: 'We will perform EDA specifically on geo-location data attributes. Here, we
    have performed grouping by operation on three data columns: City, State, and Country.
    You can take a look at the code snippet given in the following screenshot:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对地理定位数据属性执行EDA。在这里，我们对三个数据列：城市、州和国家进行了分组操作。您可以查看以下屏幕截图给出的代码片段：
- en: '![Exploratory Data Analysis](img/B08394_06_19.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![探索性数据分析](img/B08394_06_19.jpg)'
- en: 'Figure 6.19: Grouping by operation on City, State, and Country'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19：按城市、州和国家进行分组
- en: As you can see in the code snippet, there are many records where the state name
    is not present. We need to take care of them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在代码片段中看到的，有许多记录中缺少州名称。我们需要注意这些记录。
- en: 'Apart from this, we also need to count the data records country-wise so that
    we can find out how many data records are present for each country. You can refer
    to the code snippet shown in the following screenshot:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个之外，我们还需要按国家计数数据记录，以便我们可以找出每个国家有多少数据记录。您可以参考以下屏幕截图所示的代码片段：
- en: '![Exploratory Data Analysis](img/B08394_06_20.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![探索性数据分析](img/B08394_06_20.jpg)'
- en: 'Figure 6.20: Code snippet for counting data records country-wise'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20：按国家计数数据记录的代码片段
- en: 'As you can see in the preceding code snippet, there approximately 1 million
    jobs from the US region. We can say that in our dataset, the country location
    for most of the jobs is the US. To make our life easy, we are just considering
    jobs where the country is the US. You can refer to the code snippet given in the
    following screenshot:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前面的代码片段中看到的，大约有100万个来自美国地区的职位。我们可以说，在我们的数据集中，大多数职位的国家位置是美国。为了使我们的工作更简单，我们只考虑国家为美国的职位。您可以参考以下屏幕截图给出的代码片段：
- en: '![Exploratory Data Analysis](img/B08394_06_21.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![探索性数据分析](img/B08394_06_21.jpg)'
- en: 'Figure 6.21: Code snippet for all the data records where the country is the
    US'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21：所有国家为美国的记录的代码片段
- en: Here, we need to check whether there is an empty data value present for the
    city or state data columns. After observing the output of the preceding code,
    we can see that there are no data records where the city or state name is missing.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要检查城市或州数据列中是否存在空数据值。观察前面代码的输出后，我们可以看到没有数据记录中缺少城市或州名称。
- en: 'Now, let''s look at the state for which we have maximum job openings. Remember
    that we have considered only those jobs where the country location is the US.
    In order to find out the number of jobs state-wise, you can refer to the code
    snippet given in the following screenshot:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们拥有最多职位空缺的州。请记住，我们只考虑了国家位置为美国的职位。为了找出按州划分的职位数量，您可以参考以下屏幕截图给出的代码片段：
- en: '![Exploratory Data Analysis](img/B08394_06_22.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![探索性数据分析](img/B08394_06_22.jpg)'
- en: 'Figure 6.22: Code snippet for generating state-wise number of jobs'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22：按州生成就业数量代码片段
- en: 'You can also refer to the graph shown in the following screenshot:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以参考以下屏幕截图所示的图形：
- en: '![Exploratory Data Analysis](img/B08394_06_23.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![探索性数据分析](img/B08394_06_23.jpg)'
- en: 'Figure 6.23: Graph for state-wise number of jobs'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23：按州划分的就业数量图
- en: As you can see, maximum job opportunities are available in California, Texas,
    Florida, Illinois, and New York. We have done enough EDA for the revised approach.
    Now, we are going to start building the recommendation engine.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，最多的工作机会在加利福尼亚州、德克萨斯州、佛罗里达州、伊利诺伊州和纽约州。我们已经为改进的方法做了足够的EDA。现在，我们将开始构建推荐引擎。
- en: Building the recommendation engine using the jobs datafile
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用工作数据文件构建推荐引擎
- en: In this section, we will explore the code to see how we can build a job recommendation
    engine. We will use TF-IDF and cosine similarity concepts in order to build the
    recommendation engine.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索代码，看看我们如何构建一个职位推荐引擎。我们将使用TF-IDF和余弦相似度概念来构建推荐引擎。
- en: 'We have taken the `jobs_US dataframe` into account here. This dataframe contains
    jobs where the country is the US. So, we don''t have any junk data records. We
    will be considering only 10,000 data records for training because training for
    1 million data records is time consuming. You can refer to the code shown in the
    following screenshot:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经考虑了`jobs_US dataframe`。这个dataframe包含国家为美国的职位。因此，我们没有任何垃圾数据记录。我们将只考虑10,000条数据记录进行训练，因为训练1百万条数据记录是耗时的。您可以参考以下屏幕截图所示的代码：
- en: '![Building the recommendation engine using the jobs datafile](img/B08394_06_24.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![使用工作数据文件构建推荐引擎](img/B08394_06_24.jpg)'
- en: 'Figure 6.24: Code snippet of the jobs dataset to build the revised approach'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24：构建修订后方法的职位数据集代码片段
- en: 'Here, we will be focusing on the job title and job description in order to
    build the recommendation engine. As we are using the metadata of jobs, this is
    the content-based approach. We apply concatenation operation to the job title
    and job descriptions, as well as replace `nan value` with an empty string value.
    You can refer to the code given in the following screenshot:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将关注职位名称和职位描述，以构建推荐引擎。由于我们使用的是职位的元数据，这是基于内容的方法。我们对职位名称和职位描述应用连接操作，并将`nan值`替换为空字符串值。您可以参考以下截图中的代码：
- en: '![Building the recommendation engine using the jobs datafile](img/B08394_06_25.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![使用职位数据文件构建推荐引擎](img/B08394_06_25.jpg)'
- en: 'Figure 6.25: Code snippet for applying the concatenation operation'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25：应用连接操作的代码片段
- en: 'Now, we will generate the TF-IDF vectors for the concatenated string. We will
    use the TF-IDF vector matrix in order to generate the cosine similarity score.
    We will be using the `linear_kernel` function from scikit-learn in order to generate
    the cosine similarity. This function can generate the cosine similarity in less
    time compared to the `cosine_similarity` function of scikit-learn, which takes
    longer. You can refer to the code snippet shown in the following screenshot:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将为连接字符串生成TF-IDF向量。我们将使用TF-IDF向量矩阵来生成余弦相似度分数。我们将使用scikit-learn中的`linear_kernel`函数来生成余弦相似度。这个函数比scikit-learn中的`cosine_similarity`函数生成余弦相似度所需的时间更短。您可以参考以下截图中的代码片段：
- en: '![Building the recommendation engine using the jobs datafile](img/B08394_06_26.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![使用职位数据文件构建推荐引擎](img/B08394_06_26.jpg)'
- en: 'Figure 6.26: Code snippet for generating TF-IDF and cosine similarity'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26：生成TF-IDF和余弦相似度的代码片段
- en: As you can see, we have generated a high-dimensional TF-IDF matrix here. By
    using `linear_kernel`, we have generated the cosine similarity score as well.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们在这里生成了一个高维的TF-IDF矩阵。通过使用`linear_kernel`，我们还生成了余弦相似度分数。
- en: As we are done with the implementation of the revised approach, we need to test
    the recommendation now.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完成修订后方法的实现后，现在我们需要测试推荐。
- en: Testing the revised approach
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试修订后的方法
- en: In this section, we will generate a similar kind of job recommendation based
    on any given job title. We are passing the job title as the input here, and with
    the help of the cosine similarity score, we can generate the top 10 similar kinds
    of jobs that any user can apply for.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将基于任何给定的职位名称生成类似类型的职位推荐。在这里，我们将职位名称作为输入传递，借助余弦相似度分数，我们可以生成任何用户可以申请的10个类似类型的职位。
- en: 'For example, suppose a person is an SAP business analyst. That person may want
    to apply to a similar kind of job, so here, our function will take the job title
    as the input and generate the top 10 similar kinds of jobs for that particular
    user. The code for generating the top 10 job recommendations is given in the following
    screenshot:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一个人是SAP商业分析师。那个人可能想申请类似类型的职位，所以在这里，我们的函数将职位名称作为输入，并为该特定用户生成前10个类似类型的职位。生成前10个职位推荐的代码如下截图所示：
- en: '![Testing the revised approach](img/B08394_06_27.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![测试修订后的方法](img/B08394_06_27.jpg)'
- en: 'Figure 6.27: Code snippet for generating the top 10 job recommendations'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.27：生成前10个职位推荐的代码片段
- en: When we see the output, the recommendations start making sense. The person who
    is an SAP business analyst will get jobs recommendations, such as SAP FI/ Co-business
    analyst. The result of the revised approach is satisfying for us, and the recommendations
    seem relevant.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看到输出时，推荐开始变得有意义。作为SAP商业分析师的人将获得如SAP FI/ Co商业分析师的职位推荐。修订后方法的结果对我们来说令人满意，推荐看起来是相关的。
- en: Problems with the revised approach
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修订后方法的问题
- en: In this section, we will be discussing the problems with the revised approach.
    In the best approach, we can resolve this problem. In the revised approach, we
    have used only the jobs data attribute. We haven't considered the user's profile
    or the user's preferences. During the implementation of the best approach, we
    will also consider the user's profile, and based on the user's profile, we will
    suggest the jobs to them.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论修订后方法的问题。在最佳方法中，我们可以解决这个问题。在修订后的方法中，我们只使用了职位数据属性。我们没有考虑用户的个人资料或用户的偏好。在实施最佳方法时，我们也将考虑用户的个人资料，并根据用户的个人资料向他们推荐职位。
- en: In the next section, we will take a look at an intuitive idea for how to optimize
    the revised approach.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何优化改进后的方法的直观想法。
- en: Understanding how to improve the revised approach
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解如何改进改进后的方法
- en: Until now, we have used data attributes given in the jobs datafile, but we haven't
    used the data attributes from the `users` datafile and the `apps` datafile. The
    `users` datafile contains the user's profile information, and the `apps` datafile
    contains information about which user applied for which jobs.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用了工作数据文件中给出的数据属性，但我们还没有使用`users`数据文件和`apps`数据文件中的数据属性。`users`数据文件包含用户的个人资料信息，而`apps`数据文件包含关于哪些用户申请了哪些工作的信息。
- en: 'The best approach has three simple steps:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳方法有三个简单的步骤：
- en: First, with the help of user's profile, we will find and generate the top 10
    similar users.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，借助用户的个人资料，我们将找到并生成前10个相似用户。
- en: We will try to find out the jobs these 10 people applied for. We can then generate
    `JobIDs`.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将尝试找出这10个人申请的工作。然后我们可以生成`JobIDs`。
- en: Now, we will generate the job title using `JobIDs`.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用`JobIDs`生成工作标题。
- en: Here, we have taken the user's profile into account, so the recommendations
    are more specific to the particular user base. Now, let's start implementing it.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经考虑了用户的个人资料，因此推荐更加具体地针对特定的用户群体。现在，让我们开始实施它。
- en: The best approach
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳方法
- en: 'We have already seen the intuitive approach for how we will build the best
    possible approach. Here, we will use the same techniques as the ones we used in
    the revised approach. In this approach, we are adding more data attributes to
    make the recommendation engine more accurate. You can refer to the code by using
    this GitHub link: [https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb](https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何构建最佳可能方法的直观方法。在这里，我们将使用与改进方法中相同的技巧。在这个方法中，我们添加了更多的数据属性，以使推荐引擎更加准确。您可以通过使用此GitHub链接来参考代码：[https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb](https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb)。
- en: Implementing the best approach
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施最佳方法
- en: 'These are the steps we need to take in order to implement the best possible
    approach:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们需要采取的步骤，以实现最佳可能的方法：
- en: Filtering the dataset
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤数据集
- en: Preparing the training dataset
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备训练数据集
- en: Applying the concatenation operation
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用连接操作
- en: Generating the TF-IDF and cosine similarity score
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成TF-IDF和余弦相似度分数
- en: Generating recommendations
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成推荐
- en: Let's start implementing each of these listed steps.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始实施这些列出的步骤。
- en: Filtering the dataset
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤数据集
- en: 'In this step, we need to filter the user''s dataframe. We are applying the
    filter on the country data column. We need to consider the US-based users because
    there are around 300K users based outside of the US, and other users are from
    elsewhere in the world. The code to filter the user dataframe is given in the
    following screenshot:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们需要过滤用户数据框。我们正在对国家数据列应用过滤器。我们需要考虑基于美国的用户，因为大约有30万用户基于美国之外，其他用户来自世界各地的其他地方。过滤用户数据框的代码在以下截图中给出：
- en: '![Filtering the dataset](img/B08394_06_28.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![过滤数据集](img/B08394_06_28.jpg)'
- en: 'Figure 6.28: Code snippet to filter the user''s dataframe'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.28：过滤用户数据框的代码片段
- en: Now, let's prepare the training dataset.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们准备训练数据集。
- en: Preparing the training dataset
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备训练数据集
- en: 'There are 300K users, but we are not considering all of them because of the
    limited training time and computational power. Here, we are considering only 10,000
    users. If you have more computational resources, then you can consider a higher
    number of users. You can refer to the code snipp.et shown in the following screenshot:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 有30万用户，但由于训练时间和计算能力的限制，我们并没有考虑所有用户。在这里，我们只考虑了10,000个用户。如果您有更多的计算资源，那么您可以考虑更多的用户数量。您可以参考以下截图中的代码片段：
- en: '![Preparing the training dataset](img/B08394_06_29.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![准备训练数据集](img/B08394_06_29.jpg)'
- en: 'Figure 6.29: Code snippet for selecting data records for training'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.29：用于选择训练数据记录的代码片段
- en: Now, let's move on to the next step.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续下一步。
- en: Applying the concatenation operation
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用连接操作
- en: 'In this step, we are basically performing the concatenation operation. In order
    to find a similar user profile, we will concatenate the user''s degree type, major,
    and years of experience. We will generate the TF-IDF and cosine similarity for
    this concatenated data value. You can refer to the code snippet given in the following
    screenshot:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们基本上是在执行连接操作。为了找到一个相似的用户个人资料，我们将连接用户的学位类型、专业和经验年数。我们将为这个连接的数据值生成TF-IDF和余弦相似度。您可以参考以下截图给出的代码片段：
- en: '![Applying the concatenation operation](img/B08394_06_30.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![应用连接操作](img/B08394_06_30.jpg)'
- en: 'Figure 6.30: Code snippet for applying the concatenation operation'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.30：应用连接操作代码片段
- en: Now, we will generate the TF-IDF and cosine similarity score using this concatenated
    data value.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用这个连接的数据值生成TF-IDF和余弦相似度分数。
- en: Generating the TF-IDF and cosine similarity score
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成TF-IDF和余弦相似度分数
- en: 'In this section, we will generate the TF-IDF and cosine similarity score using
    the scikit-learn API. We are using the same API that we used in the revised approach.
    Here, we haven''t changed the technique, but we will change the data attributes.
    You can refer to the code snippet shown in the following screenshot:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用scikit-learn API生成TF-IDF和余弦相似度分数。我们使用的是我们在改进方法中使用的相同API。在这里，我们没有改变技术，但我们将更改数据属性。您可以参考以下截图所示的代码片段：
- en: '![Generating the TF-IDF and cosine similarity score](img/B08394_06_31.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![生成TF-IDF和余弦相似度分数](img/B08394_06_31.jpg)'
- en: 'Figure 6.31: Code snippet for generating TF-IDF and cosine similarity'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.31：生成TF-IDF和余弦相似度代码片段
- en: As you can see, we have generated the cosine similarity score, so based on that,
    we can generate a similar user profile and give them a job recommendation based
    on their job-application track records.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们已经生成了余弦相似度分数，因此基于这个分数，我们可以生成一个相似的用户个人资料，并根据他们的求职记录给出职位推荐。
- en: Generating recommendations
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成推荐
- en: 'In order to generate the job recommendation, we need to perform the following
    steps:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成职位推荐，我们需要执行以下步骤：
- en: '**Step 1**: In order to generate the top 10 similar user profiles, we need
    to pass the UserID, and as an output, we get the 10 UserIDs that are the most
    similar with respect to the input UserID. You can refer to the following screenshot:![Generating
    recommendations](img/B08394_06_32.jpg)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤1**：为了生成前10个相似的用户个人资料，我们需要传递UserID，作为输出，我们得到与输入UserID最相似的10个UserID。您可以参考以下截图：![生成推荐](img/B08394_06_32.jpg)'
- en: 'Figure 6.32: Code snippet for generating the top 10 similar users'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.32：生成前10个相似用户代码片段
- en: '**Step 2**: We will take the list of `userIDs` that we generated in step 1
    and try to find out the same `UserIDs` in the apps dataframe. The purpose of this
    kind of search operation is that we need to know which user applied for which
    job position. By using the apps data frame, we get `JobIDs`.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤2**：我们将取步骤1中生成的`userIDs`列表，并尝试在apps数据框中找到相同的`UserIDs`。这种搜索操作的目的在于我们需要知道哪个用户申请了哪个职位。通过使用apps数据框，我们得到`JobIDs`。'
- en: '**Step 3**: Once we obtain `JobIDs`, we will obtain job titles using the jobs
    dataframe.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤3**：一旦我们获得`JobIDs`，我们将使用jobs数据框获取职位名称。'
- en: 'The code snippet for step 2 and step 3 is given in the following screenshot:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2和步骤3的代码片段如下截图所示：
- en: '![Generating recommendations](img/B08394_06_33.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![生成推荐](img/B08394_06_33.jpg)'
- en: 'Figure 6.33: Code snippet to obtain JobIDs and Job title'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.33：获取JobIDs和职位名称代码片段
- en: As you can see, we have obtained similar users for `UserID 47`, and as we can
    see in the job recommendations, we get fairly relevant jobs based on the users'
    profile and their educational qualification. In the recommendation, we can see
    medical domain jobs in the Florida location. That is because, in our user base,
    a majority of the users' profiles are from a medical background. As we have considered
    both the user profile and job profile, we are able to get the most relevant job
    recommendations.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们已经为`UserID 47`找到了相似用户，并且正如我们在工作推荐中看到的那样，我们根据用户的个人资料和教育资格得到了相当相关的职位。在推荐中，我们可以看到佛罗里达地区的医疗领域职位。这是因为，在我们的用户基础中，大多数用户的个人资料来自医疗背景。因为我们同时考虑了用户个人资料和职位个人资料，所以我们能够得到最相关的职位推荐。
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: For this entire chapter, we used a content-based approach in order to develop
    a job recommendation engine, and you learned how to scrap the dataset and build
    the baseline job recommendation engine. After that, we explored another dataset.
    For the revised and best approach, we used the readily available dataset. During
    the course of the development of the revised approach, we considered the metadata
    of jobs, and built a recommendation system that works quite well. For the best
    approach, we tried to find out similar user profiles. Based on the user's profile,
    we suggested jobs to the group of users.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的整个过程中，我们使用基于内容的方法来开发一个职位推荐引擎，并且你学习了如何抓取数据集并构建基线职位推荐引擎。之后，我们探索了另一个数据集。为了改进和最佳的方法，我们使用了现成的数据集。在改进方法的发展过程中，我们考虑了职位的元数据，并构建了一个表现相当不错的推荐系统。对于最佳方法，我们试图找出相似的用户档案。基于用户的档案，我们向用户群体推荐了职位。
- en: In the next chapter, we will be building a summarization application. There,
    we will take a look at documents for the medical domain and try to summarize them.
    We will use deep-learning algorithms in order to build an application. So, keep
    reading!
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将构建一个摘要应用。在那里，我们将查看医学领域的文档并尝试对它们进行总结。我们将使用深度学习算法来构建这个应用。所以，继续阅读吧！
