- en: Chapter 6. Job Recommendation Engine
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already seen how to develop a recommendation system for the e-commerce
    product in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems for E-Commerce"),
    *Recommendation Systems for e-Commerce*, Now, we will apply the same concepts
    that you learned in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems
    for E-Commerce"), *Recommendation Systems for e-Commerce* but the type and format
    of the dataset is different. Basically, we will build a job recommendation engine.
    For this application, we have taken into account the text dataset. The main concept
    of building the recommendation engine will not change, but this chapter gives
    you a fair idea of how to apply the same concepts to different types of datasets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the baseline approach
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the baseline approach
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the baseline approach
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the baseline approach
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the revised approach
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the revised approach
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the revised approach
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the revised approach
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to improve the revised approach
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The best approach
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best approach
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let's discuss the problem statement.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will build an engine that can recommend jobs to any user.
    This is the simplest goal we want to achieve. How we are going to build it? In
    order to answer this question, let me give you an idea about what kind of approaches
    we will take in order to build a job recommendation system.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: For our baseline approach, we will scrape resumes of dummy users and try to
    build a job recommendation engine based on the scraped dataset. The reason we
    are scraping the dataset is that, most of the time, there will not be any dataset
    available for many data science applications. Suppose you are in a position where
    you have not found any dataset. What you will do then? I want to provide a solution
    for these kinds of scenarios. So, you will learn how to scrape the data and build
    the baseline solution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: In the revised approach, we will be using a dataset hosted by Kaggle. Using
    the content-based approach, we will be building a job recommendation engine. For
    the best approach, we will be using the concept of user-based collaborative filtering
    for this domain, and build the final job recommendation system.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look into the datasets.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the datasets
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we are using two datasets. The two datasets are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The scraped dataset
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The job recommendation challenge dataset
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with the scraped dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Scraped dataset
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this dataset, we have scraped the dummy resume from indeed.com (we are
    using this data just for learning and research purposes). We will download the
    resumes of users in PDF format. These will become our dataset. The code for this
    is given at this GitHub link: [https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，我们从indeed.com抓取了模拟简历（我们使用这些数据仅用于学习和研究目的）。我们将下载用户的PDF格式的简历。这些将成为我们的数据集。这个代码可以在以下GitHub链接中找到：[https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py)。
- en: 'Take a look at the code given in the following screenshot:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下截图给出的代码：
- en: '![Scraped dataset](img/B08394_06_01.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![抓取的数据集](img/B08394_06_01.jpg)'
- en: 'Figure 6.1: Code snippet for scraping the data'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：抓取数据的代码片段
- en: 'Using the preceding code, we can download the resumes. We have used the `requests`
    library and `urllib` to scrape the data. All these downloaded resumes are in PDF
    form, so we need to parse them. To parse the PDF document, we will use a Python
    library called `PDFminer`. We need to extract the following data attributes from
    the PDF documents:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们可以下载简历。我们使用了`requests`库和`urllib`来抓取数据。所有这些下载的简历都是以PDF形式，因此我们需要解析它们。为了解析PDF文档，我们将使用一个名为`PDFminer`的Python库。我们需要从PDF文档中提取以下数据属性：
- en: Work experience
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作经验
- en: Education
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教育
- en: Skills
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技能
- en: Awards
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖项
- en: Certifications
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证书
- en: Additional information
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他信息
- en: 'You can take a look at the code snippet shown in the following screenshot:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看以下截图所示的代码片段：
- en: '![Scraped dataset](img/B08394_06_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![抓取的数据集](img/B08394_06_02.jpg)'
- en: 'Figure 6.2: Code snippet to parse PDF documents'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：解析PDF文档的代码片段
- en: 'Basically, `PDFminer` is converting the content of PDF into text. Once we have
    the text data using regular expressions, we can fetch the necessary details. You
    can see the entire code by using this GitHub link: [https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，`PDFminer`将PDF的内容转换为文本。一旦我们使用正则表达式获取了文本数据，我们就可以获取必要的详细信息。您可以通过使用以下GitHub链接查看整个代码：[https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py)。
- en: 'Once we fetch all the necessary information, we will save the data in a pickle
    format. Now, you don''t need to scrap the data and fetch all necessary information.
    I have uploaded the data into a pickle file format at this GitHub link: [https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获取了所有必要的信息，我们将以pickle格式保存数据。现在，您不需要抓取数据并获取所有必要的信息。我已经将数据上传到pickle文件格式，您可以通过以下GitHub链接查看：[https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl](https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl)
- en: We will use the `resume_data.pkl` file for our baseline approach.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`resume_data.pkl`文件作为我们的基线方法。
- en: Job recommendation challenge dataset
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 职业推荐挑战数据集
- en: 'This dataset is provided by [www.careerbuilder.com](http://www.careerbuilder.com)
    and is hosted on Kaggle. You can download the dataset using this link: [https://www.kaggle.com/c/job-recommendation/data](https://www.kaggle.com/c/job-recommendation/data).
    These are the data files that we are going to use for our revised and best approach.
    All the values given in these datafiles are tab-separated:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集由[www.careerbuilder.com](http://www.careerbuilder.com)提供，并在Kaggle上托管。您可以通过此链接下载数据集：[https://www.kaggle.com/c/job-recommendation/data](https://www.kaggle.com/c/job-recommendation/data)。这些是我们将要用于我们修订和最佳方法的数据文件。这些数据文件中的所有值都是用制表符分隔的：
- en: '`apps.tsv`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apps.tsv`'
- en: '`users.tsv`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`users.tsv`'
- en: '`jobs.zip`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jobs.zip`'
- en: '`user_history.tsv`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user_history.tsv`'
- en: apps.tsv
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: apps.tsv
- en: 'This datafile contains the records of users'' job applications. It indicates
    job positions that a particular user has applied for. The job position is described
    by the JobID column. All the necessary information about this datafile is given
    in the following screenshot:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据文件包含用户求职申请的记录。它表示特定用户申请的职位。职位由JobID列描述。有关此数据文件的必要信息在以下截图给出：
- en: '![apps.tsv](img/B08394_06_03.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![apps.tsv](img/B08394_06_03.jpg)'
- en: 'Figure 6.3: Data information about apps.tsv'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：关于apps.tsv的数据信息
- en: 'There are five data columns:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有五个数据列：
- en: '`UserId`: This indicates the unique ID for a given user. By using this ID,
    we can access the user''s profile.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UserId`: 这表示给定用户的唯一标识符。通过使用此ID，我们可以访问用户的个人资料。'
- en: '`WindowsID`: This is the mask data attribute with the constant value of 1\.
    This data attribute is not important for us.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WindowsID`: 这是一个具有常量值1的掩码数据属性。此数据属性对我们来说并不重要。'
- en: '`Split`: This data attribute indicates which data records we should consider
    for training and testing.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Split`: 此数据属性表示我们应该考虑哪些数据记录用于训练和测试。'
- en: '`Application date`: This is the timestamp at which the user applied for the
    job.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Application date`: 这是用户申请工作的时间戳。'
- en: '`JobID`: This attribute indicates the `JobIds` for which the user nominates
    themselves. Using this `JobId`, we can access other information about a particular
    job.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JobID`: 此属性表示用户提名的工作的`JobIds`。使用此`JobId`，我们可以访问特定工作的其他信息。'
- en: users.tsv
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: users.tsv
- en: 'This datafile contains the user profile and all user-related information. You
    can find all the necessary information displayed in the following screenshot:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据文件包含用户个人资料和所有相关用户信息。您可以在以下屏幕截图中找到所有必要的信息：
- en: '![users.tsv](img/B08394_06_04.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![users.tsv](img/B08394_06_04.jpg)'
- en: Figure 6.4 Data information about users.tsv
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 关于users.tsv的数据信息
- en: 'These are the data attributes:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是数据属性：
- en: '`UserID`: This data attribute indicates the user''s unique identification number.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UserID`: 此数据属性表示用户的唯一识别号码。'
- en: '`WindowID`: This is the mask data attribute with a constant value of 1\. This
    data attribute is not important for us.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WindowID`: 这是一个具有常量值1的掩码数据属性。此数据属性对我们来说并不重要。'
- en: '`Split`: This data attribute indicates which data records we should consider
    for training and testing.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Split`: 此数据属性表示我们应该考虑哪些数据记录用于训练和测试。'
- en: '`City`: This attribute indicates the user''s current city.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`City`: 此属性表示用户的当前城市。'
- en: '`State`: This attribute indicates the user''s state.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`State`: 此属性表示用户的州。'
- en: 'Country: This attribute indicates the user''s country.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国家：此属性表示用户的国籍。
- en: '`ZipCode`: This data attribute indicates the user''s ZIP code.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ZipCode`: 此数据属性表示用户的邮政编码。'
- en: '`DegreeType`: This data column indicates the user''s degree; whether the user
    is a high school pass-out or has a bachelor''s degree.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DegreeType`: 此数据列表示用户的学位；用户是高中毕业生还是拥有学士学位。'
- en: '`Major`: This data attribute indicates the major subject in which the user
    has a degree.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Major`: 此数据属性表示用户学位的主修科目。'
- en: '`GraduationDate`: This data attribute indicates the graduation date of the
    user.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GraduationDate`: 此数据属性表示用户的毕业日期。'
- en: '`WorkHistoryCount`: This data attribute indicates the number of companies the
    user has worked for.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WorkHistoryCount`: 此数据属性表示用户工作过的公司数量。'
- en: '`TotalYearsExperience`: This data column indicates the user''s total years
    of experience.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TotalYearsExperience`: 此数据列表示用户的总工作经验年数。'
- en: '`CurrentlyEmployed`: This data attribute has a binary value. If the user is
    currently employed, then the value is *Yes*; if not, then the value is *No*.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CurrentlyEmployed`: 此数据属性具有二进制值。如果用户目前有工作，则值为*是*；如果没有，则值为*否*。'
- en: '`ManagedOthers`: This data attribute has a binary value as well. If the user
    is managing other people, then the value of this column is *Yes*; if the user
    is not managing other people, then the value of this column is *No*.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ManagedOthers`: 此数据属性也有二进制值。如果用户正在管理其他人，则此列的值为*是*；如果用户没有管理其他人，则此列的值为*否*。'
- en: '`ManagedHowMany`: This data attribute has a numerical value. The value of this
    column indicates the number of people that are managed by the user. If the user
    is not managing anyone, then the value is 0.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ManagedHowMany`: 此数据属性具有数值。此列的值表示由用户管理的员工人数。如果用户没有管理任何人，则此值为0。'
- en: Jobs.zip
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jobs.zip
- en: 'When you extract this ZIP file, you can get the `jobs.tsv` file. There is more
    information available in the following screenshot:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当您解压此ZIP文件时，您可以获取`jobs.tsv`文件。以下屏幕截图中有更多信息：
- en: '![Jobs.zip](img/B08394_06_05.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![Jobs.zip](img/B08394_06_05.jpg)'
- en: 'Figure 6.5: Data information about jobs.tsv'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：关于jobs.tsv的数据信息
- en: '`JobID`: This is the unique ID for each job present in the dataset.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JobID`: 这是数据集中每个工作唯一的标识符。'
- en: '`WindowID`: This is the mask data attribute that has a constant value of 1\.
    This data attribute is not important for us.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WindowID`: 这是一个具有常量值1的掩码数据属性。此数据属性对我们来说并不重要。'
- en: '`Title`: This data attribute indicates the job title.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Title`: 此数据属性表示工作标题。'
- en: '`Description`: This data attribute indicates the job description.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Description`: 此数据属性表示工作描述。'
- en: '`Requirements`: This data attribute indicates the job requirements.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`City`: This data field indicates the job location in terms of the city.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`State`: This data field indicates the job location in terms of the state.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Country`: This data field indicates the job location in terms of the country.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Zip5`: This data field indicates the ZIP code of the job location.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StartDate`: This date indicates when the job is posted or is open for applications.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EndDate`: This date is the deadline for the job application.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: user_history.tsv
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `user_history.tsv` file contains the user''s job history. There is more
    information available on this in the following screenshot:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![user_history.tsv](img/B08394_06_06.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Data information about user_history.tsv'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: There are only two new columns for this datafile.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '`Sequence`: This sequence is a numerical field. The number indicates the sequential
    order of the user''s job.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JobTitle`: This data field indicates the job title of the user.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have covered all the attributes in our datafiles; now let's start building
    the baseline approach.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Building the baseline approach
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be building the baseline approach. We will use the
    scraped dataset. The main approach we will be using is TF-IDF (Term-frequency,
    Inverse Document Frequency) and cosine similarity. Both of these concepts have
    already been described in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems
    for E-Commerce"), *Recommendation System for e-commerce.* The name of the pertinent
    sections are *Generating features using TF-IDF* and *Building the cosine similarity
    matrix*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: As this application has more textual data, we can use TF-IDF, CountVectorizers,
    cosine similarity, and so on. There are no ratings available for any job. Because
    of this, we are not using other matrix decomposition methods, such as SVD, or
    correlation coefficient-based methods, such as Pearsons'R correlation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: For the baseline approach, we are trying to find out the similarity between
    the resumes, because that is how we will know how similar the user profiles are.
    By using this fact, we can recommend jobs to all the users who share a similar
    kind of professional profile. For the baseline model, our context is to generate
    the similarity score between the resumes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the baseline approach
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to develop a simple job recommendation system, we need to perform
    the following steps:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Defining constants
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the dataset
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the helper function
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating TF-IDF vectors and cosine similarity
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining constants
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will define some constant values. These values are based on the dataset
    we have scraped. In our dataset, we have scraped the dummy resumes for seven companies,
    and there are seven data attributes that we have generated by parsing the resumes.
    We consider 100 resumes as our first training dataset and 50 resumes as our testing
    dataset. The size of our second training dataset is 50\. You can refer to the
    code snippet shown in the following screenshot:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining constants](img/B08394_06_07.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Code snippet for defining constants'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: After this step, we will load the dataset.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you know, we have already parsed the resumes that are in the PDF file format.
    We store the parsed data into the pickle format, and we need to load that pickle
    file. We will use the `dill` library to load the pickle file. You can refer to
    the code snippet shown in the following screenshot:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the dataset](img/B08394_06_08.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Code snippet for loading the dataset'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: We have restored the dataset. As the next step, we need to define the functions
    so that we can build a basic job recommendation system.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Defining the helper function
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are various helper functions that will be useful for us. There are a
    total of three helper functions for this approach:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '`my_normalize`'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_sim_vector`'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_class`'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first function is used to normalize the testing score. We will get the
    testing score in the form of a matrix. You can take a look at the code snippet
    shown in the following screenshot:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining the helper function](img/B08394_06_09.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: Code snippet for helper function my_normalize'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: This normalization is nothing but the weighted average of the testing score
    matrix. So, it takes the testing score matrix and generates the normalized testing
    score matrix. Bear with me for a while; we will see what the testing score matrix
    looks like when we generate the result of this baseline approach.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'The second function basically takes the TF-IDF vector matrix and dataset as
    an input. As an output, it generates the cosine similarity score. You can refer
    to the code snippet given in the following screenshot:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining the helper function](img/B08394_06_10.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: Code snippet for helper function get_sim_vector'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The third function basically takes the cosine similarity array as an input
    and iterates through it in order to get the maximum cosine value from the cosine
    similarity array. You can find the code snippet given in the following screenshot:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![Defining the helper function](img/B08394_06_11.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Code snippet for helper function get_class'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: We have understood the input, output, and work of our helper functions. Now,
    it's time to see their usage when we generate TF-IDF vectors and the cosine similarity.
    So, let's move on to the next section.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Generating TF-IDF vectors and cosine similarity
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will be developing the core logic of the baseline approach.
    We will be using a simple TF-IDF concept. In order to build the job recommendation
    engine using simple TF-IDF, we need to perform the following steps:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Building the training dataset
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating IF-IDF vectors for the training dataset
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the testing dataset
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the similarity score
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's build the training dataset.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Building the training dataset
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Basically, we have not divided our dataset into training and testing. So, for
    training, we need to generate the training dataset by using the code snippet that
    is shown in the following screenshot:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the training dataset](img/B08394_06_12.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: Code snippet for generating the training dataset'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The code is simple to understand. As you can see, we have used the `train1_size`
    constant value, which we have defined earlier, so that we can generate 100 resumes
    that can be used for training purposes.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the next step.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Generating IF-IDF vectors for the training dataset
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In order to generate TF-IDF vectors, we will be using scikit-learn''s `TfidfVectorizer`
    API. This basically converts our text data into a numerical format. You can take
    a look at the code snippet given in the following screenshot:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating IF-IDF vectors for the training dataset](img/B08394_06_13.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: Code snippet for generating TF-IDF'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: By using the preceding code, we can convert our textual training dataset into
    a vectorized format. The matrix of the TF-IDF is used when we generate the predictions
    for testing the dataset. Now, let's build the testing dataset.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Building the testing dataset
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have trained the model. Now, we need to build the test dataset so that we
    can check how well or how badly our trained model is performing on the test dataset.
    We have used 100 resumes from our dataset for training purposes, so now, we need
    to use the resumes that are not the part of the training dataset. In order to
    generate the testing dataset, we will execute the following code so that we can
    generate the test dataset. You can refer to the code snippet shown in the following
    screenshot:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the testing dataset](img/B08394_06_14.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: Code snippet for generating the testing dataset'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have generated the test dataset using the index of the resume,
    and have taken only those documents that are not a part of the training.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Generating the similarity score
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, first, we will take the test dataset as an input and generate
    the TF-IDF vectors for them. Once the TF-IDF vector matrix has been generated,
    we will use the cosine similarity API in order to generate the similarity score.
    For this API, we will pass the two TF-IDF matrices. One matrix is what we recently
    generated using the testing dataset, and the second matrix is what we generated
    using the training dataset. When we pass these two matrices, we will get the cosine
    similarity array as the output. You can refer to the code snippet given in the
    following screenshot:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the similarity score](img/B08394_06_15.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Code snippet for generating a cosine similarity for the testing
    dataset'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'As an output, we can generate the cosine similarity array displayed in the
    following screenshot:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the similarity score](img/B08394_06_16.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Cosine similarity array'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The array shown in the preceding screenshot has seven elements. Each element
    indicates the similarity of that resume for seven companies. So, if the highest
    cosine value appears in the 0th index, then it means that the given resume is
    more similar to resumes of other users who are working at Amazon. So, we will
    recommend a job opening at Amazon to that particular user, as their resume is
    more similar to other employees who are working at Amazon.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's explore some facts related to the testing matrix.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we build a recommendation engine using TF-IDF, count vectorizer, and cosine
    similarity, we are actually building the content-based recommendation engine.
    There is no predefined testing matrix available for generating the accuracy score.
    In this case, either we need to check our recommendations relevance manually,
    or we can take a heuristic to get the basic intuitive score. In [Chapter 4](ch04.xhtml
    "Chapter 4. Recommendation Systems for E-Commerce"), *Recommendation Systems for
    E-Commerce*, for the baseline approach, we implemented some basic threshold-based
    heuristics to get a basic idea of how well the recommendation engine was working.
    I suggest that you refer to the *Test the result of baseline approach* section
    in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems for E-Commerce"),
    *Recommendation Engine for e-commerce*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the baseline approach
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a number of problems with the baseline approach. I will list all
    of them one by one:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: There are not enough data attributes available in the dataset to build a good
    job recommendation system.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The baseline approach can''t really provide accurate job recommendations, because
    we have the dataset of user resumes only, and based on that, we can just say something
    like "your resume will look like other employees at Amazon, so please apply for
    job openings at Amazon". Now, the problem is identifying the kind of jobs we need
    to recommend to the user: whether we should recommend all job openings at Amazon,
    or some of them.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In my opinion, the baseline solution is not able to provide us the complete
    picture, because of the quality and quantity of the dataset.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution for these problems will be discussed in the next section.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the baseline approach
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we listed the shortcomings of the baseline approach.
    In this section, we will look at how we can overcome these shortcomings. We are
    facing a major problem because we did not use appropriate quality and quantity
    for the dataset. So, first of all, we need to use the dataset in which we have
    information about users' profiles as well as information about the job openings.
    Here, we are not scraping more resumes or posting information about jobs anymore.
    We are using the dataset released by the career builder. We have already seen
    basic information about this dataset earlier in this chapter.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: To build the revised approach, we will use this new dataset. Now, let's start
    building the revised approach.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建修订的方法，我们将使用这个新的数据集。现在，让我们开始构建修订的方法。
- en: Building the revised approach
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建修订的方法
- en: 'In this section, we will be using the readily available job recommendation
    challenge dataset. We have already covered the data attributes of this dataset.
    We will be using a context-based approach to build the recommendation engine.
    In order to build the revised approach, we need to perform the following steps.
    The code for the revised approach is given at this GitHub link: [https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb](https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用现成的职位推荐挑战数据集。我们已经覆盖了该数据集的数据属性。我们将使用基于上下文的方法来构建推荐引擎。为了构建修订的方法，我们需要执行以下步骤。修订方法的代码在以下GitHub链接中给出：[https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb](https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb)
- en: 'Let''s implement the following steps:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下步骤：
- en: Loading the dataset
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据集
- en: Splitting the training and testing datasets
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割训练和测试数据集
- en: Exploratory data analysis
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: Building the recommendation engine using the jobs datafile
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用工作数据文件构建推荐引擎
- en: Loading the dataset
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'As you know, the dataset is in various files. We need to load all these files.
    Remember that all the datafiles are in a `.tsv` format, so we need to use the
    `\t` delimiter as a parameter. You can refer to the code snippet shown in the
    following screenshot:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，数据集在多个文件中。我们需要加载所有这些文件。请记住，所有数据文件都是`.tsv`格式，因此我们需要使用`\t`作为参数。您可以参考以下屏幕截图中显示的代码片段：
- en: '![Loading the dataset](img/B08394_06_17.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![加载数据集](img/B08394_06_17.jpg)'
- en: 'Figure 6.17: Code snippet for loading the dataset'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17：加载数据集的代码片段
- en: As you can see, we have used the pandas `read_csv` method with the delimiter
    as a parameter, and loaded the dataset in the form of five different dataframes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们使用了pandas的`read_csv`方法，并将分隔符作为参数，以五种不同的数据框形式加载数据集。
- en: Splitting the training and testing datasets
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割训练和测试数据集
- en: 'There are three data files in which training and testing both types of data
    records is present. These dataframes are as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个数据文件，其中包含训练和测试两种类型的数据记录。以下数据框如下：
- en: apps
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序
- en: user_history
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: user_history
- en: users
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: users
- en: 'In the preceding dataframes, some records are tagged as `Train` and some records
    are tagged as `Test`. The data attribute `Split` indicates which data records
    are considered a part of the training dataset and which ones are used for testing.
    So, we need to filter our dataset. You can take a look at the code snippet given
    in the following screenshot:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的数据框中，一些记录被标记为`Train`，一些记录被标记为`Test`。数据属性`Split`指示哪些数据记录被认为是训练数据集的一部分，哪些用于测试。因此，我们需要过滤我们的数据集。您可以查看以下屏幕截图中给出的代码片段：
- en: '![Splitting the training and testing datasets](img/B08394_06_18.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![分割训练和测试数据集](img/B08394_06_18.jpg)'
- en: 'Figure 6.18: Code snippet for splitting the training and testing datasets'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18：分割训练和测试数据集的代码片段
- en: We have applied a simple filter operation for all three dataframes and stored
    their output in new dataframes.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对三个数据框都应用了一个简单的过滤操作，并将它们的输出存储在新数据框中。
- en: Now, let's move on to the **Exploratory Data Analysis** (**EDA**) section.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续到**探索性数据分析**（**EDA**）部分。
- en: Exploratory Data Analysis
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: In this section, we will be performing some basic analysis so that we can find
    out what kind of data is present in our dataset. For the revised approach, we
    are building the recommendation system using data attributes given in the jobs
    dataframe. So, before using it to build the recommendation engine, we will check
    the quality of the data records. We need to check whether any blank values are
    present in the dataset. Apart from that, we also need to check the data distribution
    of this dataframe.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将执行一些基本分析，以便我们可以找出数据集中存在哪些类型的数据。对于修订的方法，我们正在使用工作数据框中给出的数据属性构建推荐系统。因此，在将其用于构建推荐引擎之前，我们将检查数据记录的质量。我们需要检查数据集中是否存在任何空白值。除此之外，我们还需要检查这个数据框的数据分布。
- en: 'We will perform EDA specifically on geo-location data attributes. Here, we
    have performed grouping by operation on three data columns: City, State, and Country.
    You can take a look at the code snippet given in the following screenshot:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploratory Data Analysis](img/B08394_06_19.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: Grouping by operation on City, State, and Country'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the code snippet, there are many records where the state name
    is not present. We need to take care of them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from this, we also need to count the data records country-wise so that
    we can find out how many data records are present for each country. You can refer
    to the code snippet shown in the following screenshot:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploratory Data Analysis](img/B08394_06_20.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.20: Code snippet for counting data records country-wise'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding code snippet, there approximately 1 million
    jobs from the US region. We can say that in our dataset, the country location
    for most of the jobs is the US. To make our life easy, we are just considering
    jobs where the country is the US. You can refer to the code snippet given in the
    following screenshot:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploratory Data Analysis](img/B08394_06_21.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.21: Code snippet for all the data records where the country is the
    US'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Here, we need to check whether there is an empty data value present for the
    city or state data columns. After observing the output of the preceding code,
    we can see that there are no data records where the city or state name is missing.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the state for which we have maximum job openings. Remember
    that we have considered only those jobs where the country location is the US.
    In order to find out the number of jobs state-wise, you can refer to the code
    snippet given in the following screenshot:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploratory Data Analysis](img/B08394_06_22.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.22: Code snippet for generating state-wise number of jobs'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also refer to the graph shown in the following screenshot:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploratory Data Analysis](img/B08394_06_23.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.23: Graph for state-wise number of jobs'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, maximum job opportunities are available in California, Texas,
    Florida, Illinois, and New York. We have done enough EDA for the revised approach.
    Now, we are going to start building the recommendation engine.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Building the recommendation engine using the jobs datafile
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explore the code to see how we can build a job recommendation
    engine. We will use TF-IDF and cosine similarity concepts in order to build the
    recommendation engine.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'We have taken the `jobs_US dataframe` into account here. This dataframe contains
    jobs where the country is the US. So, we don''t have any junk data records. We
    will be considering only 10,000 data records for training because training for
    1 million data records is time consuming. You can refer to the code shown in the
    following screenshot:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the recommendation engine using the jobs datafile](img/B08394_06_24.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.24: Code snippet of the jobs dataset to build the revised approach'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will be focusing on the job title and job description in order to
    build the recommendation engine. As we are using the metadata of jobs, this is
    the content-based approach. We apply concatenation operation to the job title
    and job descriptions, as well as replace `nan value` with an empty string value.
    You can refer to the code given in the following screenshot:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the recommendation engine using the jobs datafile](img/B08394_06_25.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.25: Code snippet for applying the concatenation operation'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will generate the TF-IDF vectors for the concatenated string. We will
    use the TF-IDF vector matrix in order to generate the cosine similarity score.
    We will be using the `linear_kernel` function from scikit-learn in order to generate
    the cosine similarity. This function can generate the cosine similarity in less
    time compared to the `cosine_similarity` function of scikit-learn, which takes
    longer. You can refer to the code snippet shown in the following screenshot:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the recommendation engine using the jobs datafile](img/B08394_06_26.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.26: Code snippet for generating TF-IDF and cosine similarity'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have generated a high-dimensional TF-IDF matrix here. By
    using `linear_kernel`, we have generated the cosine similarity score as well.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: As we are done with the implementation of the revised approach, we need to test
    the recommendation now.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Testing the revised approach
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will generate a similar kind of job recommendation based
    on any given job title. We are passing the job title as the input here, and with
    the help of the cosine similarity score, we can generate the top 10 similar kinds
    of jobs that any user can apply for.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose a person is an SAP business analyst. That person may want
    to apply to a similar kind of job, so here, our function will take the job title
    as the input and generate the top 10 similar kinds of jobs for that particular
    user. The code for generating the top 10 job recommendations is given in the following
    screenshot:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the revised approach](img/B08394_06_27.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.27: Code snippet for generating the top 10 job recommendations'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: When we see the output, the recommendations start making sense. The person who
    is an SAP business analyst will get jobs recommendations, such as SAP FI/ Co-business
    analyst. The result of the revised approach is satisfying for us, and the recommendations
    seem relevant.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the revised approach
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be discussing the problems with the revised approach.
    In the best approach, we can resolve this problem. In the revised approach, we
    have used only the jobs data attribute. We haven't considered the user's profile
    or the user's preferences. During the implementation of the best approach, we
    will also consider the user's profile, and based on the user's profile, we will
    suggest the jobs to them.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will take a look at an intuitive idea for how to optimize
    the revised approach.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how to improve the revised approach
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we have used data attributes given in the jobs datafile, but we haven't
    used the data attributes from the `users` datafile and the `apps` datafile. The
    `users` datafile contains the user's profile information, and the `apps` datafile
    contains information about which user applied for which jobs.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'The best approach has three simple steps:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: First, with the help of user's profile, we will find and generate the top 10
    similar users.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will try to find out the jobs these 10 people applied for. We can then generate
    `JobIDs`.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we will generate the job title using `JobIDs`.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, we have taken the user's profile into account, so the recommendations
    are more specific to the particular user base. Now, let's start implementing it.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already seen the intuitive approach for how we will build the best
    possible approach. Here, we will use the same techniques as the ones we used in
    the revised approach. In this approach, we are adding more data attributes to
    make the recommendation engine more accurate. You can refer to the code by using
    this GitHub link: [https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb](https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the steps we need to take in order to implement the best possible
    approach:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Filtering the dataset
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the training dataset
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the concatenation operation
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the TF-IDF and cosine similarity score
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating recommendations
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start implementing each of these listed steps.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Filtering the dataset
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this step, we need to filter the user''s dataframe. We are applying the
    filter on the country data column. We need to consider the US-based users because
    there are around 300K users based outside of the US, and other users are from
    elsewhere in the world. The code to filter the user dataframe is given in the
    following screenshot:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![Filtering the dataset](img/B08394_06_28.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.28: Code snippet to filter the user''s dataframe'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's prepare the training dataset.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the training dataset
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are 300K users, but we are not considering all of them because of the
    limited training time and computational power. Here, we are considering only 10,000
    users. If you have more computational resources, then you can consider a higher
    number of users. You can refer to the code snipp.et shown in the following screenshot:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![Preparing the training dataset](img/B08394_06_29.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.29: Code snippet for selecting data records for training'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the next step.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Applying the concatenation operation
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this step, we are basically performing the concatenation operation. In order
    to find a similar user profile, we will concatenate the user''s degree type, major,
    and years of experience. We will generate the TF-IDF and cosine similarity for
    this concatenated data value. You can refer to the code snippet given in the following
    screenshot:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying the concatenation operation](img/B08394_06_30.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.30: Code snippet for applying the concatenation operation'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will generate the TF-IDF and cosine similarity score using this concatenated
    data value.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Generating the TF-IDF and cosine similarity score
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will generate the TF-IDF and cosine similarity score using
    the scikit-learn API. We are using the same API that we used in the revised approach.
    Here, we haven''t changed the technique, but we will change the data attributes.
    You can refer to the code snippet shown in the following screenshot:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating the TF-IDF and cosine similarity score](img/B08394_06_31.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.31: Code snippet for generating TF-IDF and cosine similarity'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have generated the cosine similarity score, so based on that,
    we can generate a similar user profile and give them a job recommendation based
    on their job-application track records.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Generating recommendations
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to generate the job recommendation, we need to perform the following
    steps:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: In order to generate the top 10 similar user profiles, we need
    to pass the UserID, and as an output, we get the 10 UserIDs that are the most
    similar with respect to the input UserID. You can refer to the following screenshot:![Generating
    recommendations](img/B08394_06_32.jpg)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 6.32: Code snippet for generating the top 10 similar users'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 2**: We will take the list of `userIDs` that we generated in step 1
    and try to find out the same `UserIDs` in the apps dataframe. The purpose of this
    kind of search operation is that we need to know which user applied for which
    job position. By using the apps data frame, we get `JobIDs`.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 3**: Once we obtain `JobIDs`, we will obtain job titles using the jobs
    dataframe.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code snippet for step 2 and step 3 is given in the following screenshot:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating recommendations](img/B08394_06_33.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.33: Code snippet to obtain JobIDs and Job title'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have obtained similar users for `UserID 47`, and as we can
    see in the job recommendations, we get fairly relevant jobs based on the users'
    profile and their educational qualification. In the recommendation, we can see
    medical domain jobs in the Florida location. That is because, in our user base,
    a majority of the users' profiles are from a medical background. As we have considered
    both the user profile and job profile, we are able to get the most relevant job
    recommendations.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this entire chapter, we used a content-based approach in order to develop
    a job recommendation engine, and you learned how to scrap the dataset and build
    the baseline job recommendation engine. After that, we explored another dataset.
    For the revised and best approach, we used the readily available dataset. During
    the course of the development of the revised approach, we considered the metadata
    of jobs, and built a recommendation system that works quite well. For the best
    approach, we tried to find out similar user profiles. Based on the user's profile,
    we suggested jobs to the group of users.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be building a summarization application. There,
    we will take a look at documents for the medical domain and try to summarize them.
    We will use deep-learning algorithms in order to build an application. So, keep
    reading!
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
