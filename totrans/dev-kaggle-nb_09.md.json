["```py\nimport os\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nfrom base64 import b64encode\ndef display_image_from_video(video_path):\n    '''\n    Display image from video\n    Process\n        1\\. perform a video capture from the video\n        2\\. read the image\n        3\\. display the image\n    Args:\n        video_path - path for video\n    Returns:\n        None\n    '''\n    capture_image = cv.VideoCapture(video_path)\n    ret, frame = capture_image.read()\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111)\n    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n    ax.imshow(frame) \n```", "```py\ndef display_images_from_video_list(video_path_list, data_folder, video_folder):\n    '''\n    Display images from video list\n    Process:\n        0\\. for each video in the video path list\n            1\\. perform a video capture from the video\n            2\\. read the image\n            3\\. display the image\n    Args:\n        video_path_list: path for video list\n        data_folder: path for data\n        video_folder: path for video folder\n    Returns:\n        None\n    '''\n    plt.figure()\n    fig, ax = plt.subplots(2,3,figsize=(16,8))\n    # we only show images extracted from the first 6 videos\nfor i, video_file in enumerate(video_path_list[0:6]):\n        video_path = os.path.join(data_folder, video_folder,video_file)\n        capture_image = cv.VideoCapture(video_path)\n        ret, frame = capture_image.read()\n        frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n        ax[i//3, i%3].imshow(frame)\n        ax[i//3, i%3].set_title(f\"Video: {video_file}\")\n        ax[i//3, i%3].axis('on') \n```", "```py\ndef play_video(video_file, data_folder, subset):\n    '''\n    Display video given by composed path\n    Args\n        video_file: the name of the video file to display\n        data_folder: data folder\n        subset: the folder where the video file is located\n    Returns:\n        a HTML objects running the video\n    '''\n    video_url = open(os.path.join(data_folder, subset,video_file),'rb').read()\n    data_url = \"data:video/mp4;base64,\" + b64encode(video_url).decode()\n    return HTML(\"\"\"<video width=500 controls><source src=\"img/%s\" type=\"video/mp4\"></video>\"\"\" % data_url) \n```", "```py\nimport os\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nclass CascadeObjectDetector():\n    '''\n    Class for Cascade Object Detection\n    '''\ndef __init__(self,object_cascade_path):\n        '''\n        Args:\n        object_cascade_path: path for the *.xml defining the parameters\n                for {face, eye, smile, profile} detection algorithm\n                source of the haarcascade resource is:\n                https://github.com/opencv/opencv/tree/master/data/haarcascades\n        Returns:\n            None\n        '''\n        self.object_cascade=cv.CascadeClassifier(object_cascade_path) \ndetect function of the CascadeObjectDetector class. This function returns the rectangle coordinates of the object detected in the image:\n```", "```py\ndef detect(self, image, scale_factor=1.3,\n           min_neighbors=5,\n           min_size=(20,20)):\n    '''\n    Function return rectangle coordinates of object for given image\n    Args:\n        image: image to process\n        scale_factor: scale factor used for object detection\n        min_neighbors: minimum number of parameters considered during object detection\n        min_size: minimum size of bounding box for object detected\n    Returns:\n        rectangle with detected object\n\n    '''\n    rects=self.object_cascade.detectMultiScale(image,\n                                            scaleFactor=scale_factor,\n                                            minNeighbors=min_neighbors,\n                                            minSize=min_size)\n    return rects \n```", "```py\nclass FaceObjectDetector():\n    '''\n    Class for Face Object Detection\n    '''\ndef __init__(self, face_detection_folder):\n        '''\n        Args:\n        face_detection_folder: path for folder where the *.xmls\n                for {face, eye, smile, profile} detection algorithm\n        Returns:\n            None\n        '''\n        self.path_cascade=face_detection_folder\n        self.frontal_cascade_path= os.path.join(self.path_cascade,'haarcascade_frontalface_default.xml')\n        self.eye_cascade_path= os.path.join(self.path_cascade,'haarcascade_eye.xml')\n        self.profile_cascade_path= os.path.join(self.path_cascade,'haarcascade_profileface.xml')\n        self.smile_cascade_path= os.path.join(self.path_cascade,'haarcascade_smile.xml')\n        #Detector object created\n# frontal face\n        self.face_detector=CascadeObjectDetector(self.frontal_cascade_path)\n        # eye\n        self.eyes_detector=CascadeObjectDetector(self.eye_cascade_path)\n        # profile face\n        self.profile_detector=CascadeObjectDetector(self.profile_cascade_path)\n        # smile\n        self.smile_detector=CascadeObjectDetector(self.smile_cascade_path) \n```", "```py\ndetect_object function of the FaceObjectDetector class, and the detect function of the CascadeObjectDetector object initialized with the eyes Haar cascade object. Then, we use the OpenCV Circle function to mark on the initial image, with a circle, the position of the eyes detected in the image:\n```", "```py\ndef detect_objects(self,\n                   image,\n                   scale_factor,\n                   min_neighbors,\n                   min_size,\n                   show_smile=False):\n    '''\n    Objects detection function\n    Identify frontal face, eyes, smile and profile face and display the detected objects over the image\n    Args:\n        image: the image extracted from the video\n        scale_factor: scale factor parameter for `detect` function of CascadeObjectDetector object\n        min_neighbors: min neighbors parameter for `detect` function of CascadeObjectDetector object\n        min_size: minimum size parameter for f`detect` function of CascadeObjectDetector object\n        show_smile: flag to activate/deactivate smile detection; set to False due to many false positives\n    Returns:\n        None\n    '''\n    image_gray=cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n    eyes=self.eyes_detector.detect(image_gray,\n                   scale_factor=scale_factor,\n                   min_neighbors=min_neighbors,\n                   min_size=(int(min_size[0]/2), int(min_size[1]/2)))\n    for x, y, w, h in eyes:\n        #detected eyes shown in color image\n        cv.circle(image,(int(x+w/2),int(y+h/2)),(int((w + h)/4)),(0, 0,255),3) \n```", "```py\n # deactivated by default due to many false positive\nif show_smile:\n            smiles=self.smile_detector.detect(image_gray,\n                          scale_factor=scale_factor,\n                          min_neighbors=min_neighbors,\n                          min_size=(int(min_size[0]/2), int(min_size[1]/2)))\n            for x, y, w, h in smiles:\n               #detected smiles shown in color image\n               cv.rectangle(image,(x,y),(x+w, y+h),(0, 0,255),3) \n```", "```py\n profiles=self.profile_detector.detect(image_gray,\n                       scale_factor=scale_factor,\n                       min_neighbors=min_neighbors,\n                       min_size=min_size)\n        for x, y, w, h in profiles:\n            #detected profiles shown in color image\n            cv.rectangle(image,(x,y),(x+w, y+h),(255, 0,0),3)\n        faces=self.face_detector.detect(image_gray,\n                       scale_factor=scale_factor,\n                       min_neighbors=min_neighbors,\n                       min_size=min_size)\n        for x, y, w, h in faces:\n            #detected faces shown in color image\n            cv.rectangle(image,(x,y),(x+w, y+h),(0, 255,0),3)\n        # image\n        fig = plt.figure(figsize=(10,10))\n        ax = fig.add_subplot(111)\n        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n        ax.imshow(image) \n```", "```py\n def extract_image_objects(self,\n                              video_file,\n                              data_folder,\n                              video_set_folder,\n                              show_smile=False\n ):\n        '''\n        Extract one image from the video and then perform face/eyes/smile/profile detection on the image\n        Args:\n            video_file: the video from which to extract the image from which we extract the face\n            data_folder: folder with the data\n            video_set_folder: folder with the video set\n            show_smile: show smile (False by default)\n        Returns:\n            None\n        '''\n        video_path = os.path.join(data_folder, video_set_folder,video_file)\n        capture_image = cv.VideoCapture(video_path)\n        ret, frame = capture_image.read()\n        #frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n        self.detect_objects(image=frame,\n                scale_factor=1.3,\n                min_neighbors=5,\n                min_size=(50, 50),\n                show_smile=show_smile) \n```", "```py\nclass MTCNNFaceDetector():\n    '''\n    Class for MTCNN Face Detection\n    Detects the face and the face keypoints: right & left eye,\n    nose, right and left lips limits\n    Visualize a image capture from a video and marks the\n    face boundingbox and the features\n    On top of the face boundingbox shows the confidence score\n    '''\ndef __init__(self, mtcnn_model):\n        '''\n        Args:\n            mtcnn_model: mtcnn model instantiated already\n        Returns:\n            None\n        '''\n        self.detector = mtcnn_model\n        self.color_face = (255,0,0)\n        self.color_keypoints = (0, 255, 0)\n        self.font = cv.FONT_HERSHEY_SIMPLEX\n        self.color_font = (255,0,255) \n```", "```py\n def detect(self, video_path):\n        '''\n        Function plot image\n        Args:\n            video_path: path to the video from which to capture\n            image and then apply detector\n        Returns:\n            rectangle with detected object\n\n        '''\n        capture_image = cv.VideoCapture(video_path)\n        ret, frame = capture_image.read()\n        image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n\n        results = self.detector.detect_faces(image)\n        if results:\n            for result in results:\n                print(f\"Extracted features: {result}\")\n                x, y, w, h = bounding_box = result['box']\n                keypoints = result['keypoints']\n                confidence = f\"{round(result['confidence'], 4)}\"\n                cv.rectangle(image, (x, y),(x+w,y+h), self.color_face, 3)\n                # add all the internal features\nfor key in keypoints:\n                    xk, yk = keypoints[key]\n                    cv.rectangle(image, (xk-2, yk-2), (xk+2, yk+2), self.color_keypoints, 3)\n                image = cv.putText(image, confidence, (x, y-2),\n                                   self.font, 1,\n                                   self.color_font, 2,\n                                   cv.LINE_AA)\n        fig = plt.figure(figsize=(15, 15))\n        ax = fig.add_subplot(111)\n        ax.imshow(image)\n        plt.show() \n```", "```py\n{\n   'box': [906, 255, 206, 262],\n   'confidence': 0.9999821186065674,\n   'keypoints':\n{\n       'left_eye': (965, 351),\n       'right_eye': (1064, 354),\n       'nose': (1009, 392),\n       'mouth_left': (966, 453),\n       'mouth_right': (1052, 457)\n     }\n} \n```", "```py\nfrom data_quality_stats import missing_data, unique_values, most_frequent_values\nfrom plot_style_utils import set_color_map, plot_count\nfrom video_utils import display_image_from_video, display_images_from_video_list, play_video\nfrom face_object_detection import CascadeObjectDetector, FaceObjectDetector\nfrom face_detection_mtcnn import MTCNNFaceDetector \n```", "```py\ntrain_list = list(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))\next_dict = []\nfor file in train_list:\n    file_ext = file.split('.')[1]\n    if (file_ext not in ext_dict):\n        ext_dict.append(file_ext)\nprint(f\"Extensions: {ext_dict}\") \n```", "```py\njson_file = [file for file in train_list if  file.endswith('json')][0]\ndef get_meta_from_json(path):\n    df = pd.read_json(os.path.join(DATA_FOLDER, path, json_file))\n    df = df.T\n    return df\nmeta_train_df = get_meta_from_json(TRAIN_SAMPLE_FOLDER)\nmeta_train_df.head() \n```", "```py\ncolor_list = ['#4166AA', '#06BDDD', '#83CEEC', '#EDE8E4', '#C2AFA8']\ncmap_custom = set_color_map(color_list) \n```", "```py\nfake_train_sample_video = list(meta_train_df.loc[meta_train_df.label=='FAKE'].sample(3).index)\nfor video_file in fake_train_sample_video:\n    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file)) \n```", "```py\nreal_train_sample_video = list(meta_train_df.loc[meta_train_df.label=='REAL'].sample(3).index)\nfor video_file in real_train_sample_video:\n    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file)) \n```", "```py\nsame_original_fake_train_sample_video = \\\n        list(meta_train_df.loc[meta_train_df.original=='meawmsgiti.mp4'].index)\ndisplay_images_from_video_list(video_path_list=same_original_fake_train_sample_video,\n                               data_folder=DATA_FOLDER,\n                               video_folder=TRAIN_SAMPLE_FOLDER) \n```", "```py\ndisplay_images_from_video_list(test_videos.sample(2).video, DATA_FOLDER, TEST_FOLDER) \n```", "```py\nsame_original_fake_train_sample_video = \\\n    list(meta_train_df.loc[meta_train_df.original=='kgbkktcjxf.mp4'].index)\nfor video_file in same_original_fake_train_sample_video[1:4]:\n    print(video_file)\n    face_object_detector.extract_image_objects(video_file=video_file,\n                          data_folder=DATA_FOLDER,\n                          video_set_folder=TRAIN_SAMPLE_FOLDER,\n                          show_smile=False \n                          ) \n```", "```py\nExtracted features: {'box': [906, 255, 206, 262], 'confidence': 0.9999821186065674, 'keypoints': {'left_eye': (965, 351), 'right_eye': (1064, 354), 'nose': (1009, 392), 'mouth_left': (966, 453), 'mouth_right': (1052, 457)}}\nExtracted features: {'box': [882, 966, 77, 84], 'confidence': 0.871575653553009, 'keypoints': {'left_eye': (905, 1003), 'right_eye': (926, 985), 'nose': (919, 1002), 'mouth_left': (921, 1024), 'mouth_right': (942, 1008)}} \n```"]