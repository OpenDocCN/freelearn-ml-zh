<html><head></head><body><div class="book" title="2D scale space relation"><div class="book" id="1DOR02-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec42" class="calibre1"/>2D scale space relation</h1></div></div></div><p class="calibre8">Another large <a id="id556" class="calibre1"/>advantage that we can exploit in industrial cases is the fact that many of these setups have a fixed camera position. This is interesting when the objects that need to be detected follow a fixed ground plane, like in the case of pedestrians or objects passing by on a conveyor belt. If these conditions exist, then there is actually a possibility to model the scale of an object at each position in the image. This yields two possible advantages:</p><div class="book"><ul class="itemizedlist"><li class="listitem">First of all, you can use this knowledge to effectively reduce the number of false positive detections while still keeping your certainty threshold low enough so that low certainty and good detection still stick around. This can be done in some sort of post-processing step after the object detection phase.</li><li class="listitem">Secondly, this knowledge can be used to effectively reduce the detection time and search space for object candidates inside the image pyramid.</li></ul></div><p class="calibre8">Let's start by focusing on the following case, illustrated in the following figure. Consider the fact that we want to create a pedestrian detection and that we have an existing model for doing so. We have a 360-degree camera mounted on top of a car and are grabbing those cycloramic images at continuous intervals. The cycloramas are now passed on towards the computer vision component that needs to define if a pedestrian is actually occurring in the image. Due to the very large resolution of such a 360-degree cyclorama, the image pyramid will be huge, leading to a lot of false positive detections and a very long processing time.</p><div class="mediaobject"><img src="../images/00087.jpeg" alt="2D scale space relation" class="calibre11"/><div class="caption"><p class="calibre28">An example of the Viola and Jones cascade classifier pedestrian detection model in OpenCV 3 based on HAAR features</p></div></div><p class="calibre12"> </p><p class="calibre8">The example<a id="id557" class="calibre1"/> clearly shows that when applying the detector it is very hard to find a decent score threshold to only find pedestrians and no longer have a bunch of false positive detections. Therefore, we took a base set of 40 cycloramic images and manually annotated each pedestrian inside, using our object annotation tool. If we then visualized the annotation heights of the bounding box in function of the x position location of appearance in the image, we could derive the following relation, as shown in the following graph:</p><div class="mediaobject"><img src="../images/00088.jpeg" alt="2D scale space relation" class="calibre11"/><div class="caption"><p class="calibre28">Scale space relation between the position of the annotation in the image and the scale of the found detection</p></div></div><p class="calibre12"> </p><p class="calibre8">The red dots in this figure are all <a id="id558" class="calibre1"/>possible ground truth annotations that we retrieved from the test bench of 40 cycloramas. The red line is the linear relation that we fitted to the data and which describes more or less which scale should be detected on which location in the image. However, we do know that there could be a small variation on that specific scale as defined by the green borders, in order to contain as much of the annotations as possible. We used the rule of assigning a Gaussian distribution and thus agree that in the range [-3sigma,+3sigma] 98% of all detections should fall. We then apply the minimal and maximal value according to our ranges and define a region where objects can occur naturally, assigned with the blue borders and visualized in the following picture:</p><div class="mediaobject"><img src="../images/00089.jpeg" alt="2D scale space relation" class="calibre11"/><div class="caption"><p class="calibre28">Possible locations of pedestrians walking in the same ground plane and fully visible by the camera system</p></div></div><p class="calibre12"> </p><p class="calibre8">This means that if we run a detector on this input image, we already can eliminate more than 50% of the image because training data clearly shows that a pedestrian cannot occur in that location. This reduces the search space quite a lot! The only downside to this approach of limiting the search space with an image mask is that people on, for example, balconies will simply be ignored. But again, in this application, it was not necessary to find these people since they are not in the same ground plane.</p><p class="calibre8">We then finally combined everything we know from this chapter together. We applied a scale space relation for all possible scales that can occur, already only inside the mask area because objects cannot exist outside of it in our application. We then lowered the score threshold to have <a id="id559" class="calibre1"/>more detections and to ensure that we have detected as many pedestrians as possible before applying our filtering based on the scale-space relation. The result can be shown here. It clearly shows that there are applications where the contextual information can increase your detection rates a lot!</p><div class="mediaobject"><img src="../images/00090.jpeg" alt="2D scale space relation" class="calibre11"/><div class="caption"><p class="calibre28">The complete pipeline: 1) detection with low threshold, 2) applying the mask and removing a lot of false positives, 3) enforcing the scale space location to remove extra false positive detections</p></div></div><p class="calibre12"> </p></div>

<div id="page" style="height:0pt"/><div class="book" title="Performance evaluation and GPU optimizations" id="1ENBI1-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec43" class="calibre1"/>Performance evaluation and GPU optimizations</h1></div></div></div><p class="calibre8">We are heading <a id="id560" class="calibre1"/>towards the end of this chapter, but before we end, I would like to address two small but still important topics. Let's start by discussing the evaluation <a id="id561" class="calibre1"/>of the performance of cascade classifier object detection models by using not only a visual check but by actually looking at how good our model performs over a larger dataset.</p></div>

<div class="book" title="Performance evaluation and GPU optimizations" id="1ENBI1-940925703e144daa867f510896bffb69">
<div class="book" title="Object detection performance testing"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec58" class="calibre1"/>Object detection performance testing</h2></div></div></div><p class="calibre8">We will do this by <a id="id562" class="calibre1"/>using the concept of precision recall curves. They differ a bit from the more common ROC curves from the statistics field, which have the downside that they depend on true negative values, and with sliding windows applications, this value becomes so high that the true positive, false positive, and false negative values will disappear in relation to the true negatives. Precision-recall curves avoid this measurement and thus are better for creating an evaluation of our cascade classifier model.</p><p class="calibre8"><span class="strong"><em class="calibre10">Precision = TP / (TP + FP)</em></span> and <span class="strong"><em class="calibre10">Recall = TP / (TP + FN)</em></span> with a true positive (TP) being an annotation that is also found as detection, a false positive (FP) being a detection for which no annotation exist, and a false negative (FN) being an annotation for which no detection exists.</p><p class="calibre8">These values describe how good your model works for a certain threshold value. We use the certainty score as a threshold value. The <span class="strong"><strong class="calibre9">precision</strong></span><a id="id563" class="calibre1"/> defines how much of the found detections are actual objects, while the <span class="strong"><strong class="calibre9">recall</strong></span><a id="id564" class="calibre1"/> defines how many of the objects that are in the image are actually found.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note86" class="calibre1"/>Note</h3><p class="calibre8">Software for creating PR curves over a varying threshold can be found at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/precision_recall/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/precision_recall/</a>.</p></div><p class="calibre8">The software requires several input elements:</p><div class="book"><ul class="itemizedlist"><li class="listitem">First of all, you need to collect a validation/test set that is independent of the training set because otherwise you will never be able to decide if your model was overfitted for a set of training data and thus worse for generalizing over a set of class instances.</li><li class="listitem">Secondly, you need an annotation file of the validation set, which can be seen as a ground truth of the validation set. This can be made with the object annotation software that is supplied with this chapter.</li><li class="listitem">Third, you need a detection file created with the detection software that also outputs the score, in order to be able to vary over those retrieved scores. Also, ensure that the nonmaxima suppression is only set at 1 so that detections on the same location get merged but none of the detections get rejected.</li></ul></div><p class="calibre8">When running the <a id="id565" class="calibre1"/>software on such a validation set, you will receive a precision recall result file as shown here. Combined with a precision recall coordinate for each threshold step, you will also receive the threshold itself, so that you could select the most ideal working point for your application in the precision recall curve and then find the threshold needed for that!</p><div class="mediaobject"><img src="../images/00091.jpeg" alt="Object detection performance testing" class="calibre11"/><div class="caption"><p class="calibre28">Precision recall results for a self trained cascade classifier object model</p></div></div><p class="calibre12"> </p><p class="calibre8">This output can then be visualized by software packages such as MATLAB (<a class="calibre1" href="http://nl.mathworks.com/products/matlab/">http://nl.mathworks.com/products/matlab/</a>) or Octave (<a class="calibre1" href="http://www.gnu.org/software/octave/">http://www.gnu.org/software/octave/</a>), which have better support for graph generation than OpenCV. The result from the preceding file can be seen in the following figure. A MATLAB sample script for generating those visualizations is supplied together with the precision recall software.</p><div class="mediaobject"><img src="../images/00092.jpeg" alt="Object detection performance testing" class="calibre11"/><div class="caption"><p class="calibre28">Precision recall results on a graph</p></div></div><p class="calibre12"> </p><p class="calibre8">Looking at the <a id="id566" class="calibre1"/>graph, we see that both precision and recall have a scale of [0 1]. The most ideal point in the graph would be the upper-right corner (precision=1/recall=1), which would mean that all objects in the image are found and that no false positive detections are found. So basically, the closer the slope of your graph goes towards the upper right corner, the better your detector will be.</p><p class="calibre8">In order to add a value of accuracy to a certain curve of the precision recall graph (when comparing models with different parameters), the computer vision research community uses the principle of the area under the curve (AUC), expressed in a percentage, which can also be seen in the generated graph. Again, getting an AUC of 100% would mean that you have developed the ideal object detector.</p></div></div>

<div class="book" title="Performance evaluation and GPU optimizations" id="1ENBI1-940925703e144daa867f510896bffb69">
<div class="book" title="Optimizations using GPU code"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec59" class="calibre1"/>Optimizations using GPU code</h2></div></div></div><p class="calibre8">To be able to <a id="id567" class="calibre1"/>reconstruct the experiments done in the discussion about GPU usage, you will need to have an NVIDIA GPU, which is compatible with the OpenCV CUDA module. Furthermore, you will need to rebuild OpenCV with different configurations (which I will highlight later) to get the exact same output.</p><p class="calibre8">The tests from <a id="id568" class="calibre1"/>my end were done with a Dell Precision T7610 computer containing an Intel Xeon(R) CPU that has two processors, each supporting 12 cores and 32 GB of RAM memory. As GPU interface, I am using an NVIDIA Quadro K2000 with 1 GB of dedicated on-board memory.</p><p class="calibre8">Similar results can be achieved with a non-NVIDIA GPU through OpenCL and the newly introduced T-API in OpenCV 3. However, since this technique is fairly new and still not bug free, we will stick to the CUDA interface.</p><p class="calibre8">OpenCV 3 contains a GPU implementation of the cascade classifier detection system, which can be found under the CUDA module in. This interface could help to increase the performance when processing larger images. An example of that can be seen in the following figure:</p><div class="mediaobject"><img src="../images/00093.jpeg" alt="Optimizations using GPU code" class="calibre11"/><div class="caption"><p class="calibre28">CPU-GPU comparison without using any CPU optimizations</p></div></div><p class="calibre12"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note87" class="calibre1"/>Note</h3><p class="calibre8">These results were obtained by using the software that can be retrieved from <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/CPU_GPU_comparison/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/CPU_GPU_comparison/</a>.</p></div><p class="calibre8">For achieving<a id="id569" class="calibre1"/> this result, I built OpenCV without any CPU optimization and CUDA support. For this, you will need to disable several CMAKE flags, thus disabling the following packages: IPP, TBB, SSE, SSE2, SSE3, SSE4, OPENCL, and PTHREAD. In order to avoid any bias from a single image being loaded at a moment that the CPU is doing something in the background, I processed the image 10 times in a row.</p><p class="calibre8">The original input image has a size of 8000x4000 pixels, but after some testing, it seemed that the <code class="email">detectMultiScale</code> function on GPU would require memory larger than the dedicated 1 GB. Therefore, we only run tests starting from having the image size as 4000*2000 pixels. It is clear that, when processing images on a single core CPU, the GPU interface is way more efficient, even if you take into account that at each run, it needs to push data from memory to the GPU and get it back. We still get a speedup of about 4-6 times.</p><p class="calibre8">However, the GPU implementation is not always the best way to go, as we will prove by a second test. Let's start by summing up some reasons why the GPU could be a bad idea:</p><div class="book"><ul class="itemizedlist"><li class="listitem">If your image resolution is small, then it is possible that the time needed to initialize the GPU, parse the data towards the GPU, process the data, and grab it back to memory will be the bottleneck in your application and will actually take longer than simply processing it on the CPU. In this case, it is better to use a CPU-based implementation of the detection software.</li><li class="listitem">The GPU implementation does not provide the ability to return the stage weights and thus creating a precision recall curve based on the GPU optimized function will be difficult.</li><li class="listitem">The preceding case was tested with a single core CPU without any optimizations, which is actually a bad reference nowadays. OpenCV has been putting huge efforts into making their algorithms run efficiently on CPU with tons of optimizations. In this case, it is not for granted that a GPU with the data transfer bottleneck will still run faster.</li></ul></div><p class="calibre8">To prove the fact that a GPU implementation can be worse than a CPU implementation, we built OpenCV with the following freely available optimization parameters: IPP (free compact set provided by OpenCV), TBB, SSE2, SSE3, SSE4 (SSE instructions selected automatically by the CMAKE script for my system), pthread (for using parallel for loop structures), and of course, with the CUDA interface.</p><p class="calibre8">We will then run the same software test again, as shown here.</p><div class="mediaobject"><img src="../images/00094.jpeg" alt="Optimizations using GPU code" class="calibre11"/><div class="caption"><p class="calibre28">CPU-GPU comparison with basic CPU optimizations provided by OpenCV 3.0</p></div></div><p class="calibre12"> </p><p class="calibre8">We will clearly <a id="id570" class="calibre1"/>see now that using the optimizations on my system yield a better result on CPU than on GPU. In this case, one would make a bad decision by only looking at the fact that he/she has a GPU available. Basically, this proves that you should always pay attention to how you will optimize your algorithm. Of course, this result is a bit biased, since a normal computer does not have 24 cores and 32 GB of RAM memory, but seeing that the performance of personal computers increase every day, it will not take long before everyone has access to these kind of setups.</p><p class="calibre8">I even took it one step further, by taking the original image of 8000*4000 pixels, which has no memory limits on my system for the CPU due to the 32 GB or RAM, and performed the software again on that single size. For the GPU, this meant that I had to break down the image into two parts and process those. Again, we processed 10 images in a row. The result can be seen in the following image:</p><div class="mediaobject"><img src="../images/00095.jpeg" alt="Optimizations using GPU code" class="calibre11"/><div class="caption"><p class="calibre28">Comparison of an 8000x4000 pixel processed on GPU versus multicore CPU</p></div></div><p class="calibre12"> </p><p class="calibre8">As you can see, there <a id="id571" class="calibre1"/>is still a difference of the GPU interface taking about four times as long as the CPU interface, and thus in this case, it would be a very bad decision to select a GPU solution for the project, rather than a multicore CPU solution.</p></div></div>
<div class="book" title="Practical applications" id="1FLS41-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec44" class="calibre1"/>Practical applications</h1></div></div></div><p class="calibre8">If you are still wondering <a id="id572" class="calibre1"/>what the actual industrial applications for this object detection software could be, then take a look at the following:</p><div class="mediaobject"><img src="../images/00096.jpeg" alt="Practical applications" class="calibre11"/><div class="caption"><p class="calibre28">Examples of industrial object detection</p></div></div><p class="calibre12"> </p><p class="calibre8">This is a quick overview of the applications that I used this software for to get accurate locations of <a id="id573" class="calibre1"/>detected objects:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Dummy test cases containing rotation invariant detection of both cookies and candies on a set of different backgrounds.</li><li class="listitem">Automated detection and counting of microorganisms under a microscope instead of counting them yourself.</li><li class="listitem">Localization of strawberries for ripeness classification.</li><li class="listitem">Localization of road markings in an aerial imagery for automated creation of a GIS (Geographic Information System) based on the retrieved data.</li><li class="listitem">Rotation invariant detection of peppers (green, yellow, and red combined) on a conveyor belt combined with the detection of the stoke for effective robot gripping.</li><li class="listitem">Traffic sign detection for ADAS (Automated Driver Assist System) systems.</li><li class="listitem">Orchid detection for automated classification orchid species.</li><li class="listitem">Pedestrian detection and tracking in NIR images for security applications.</li></ul></div><p class="calibre8">So, as you can see, the possibilities are endless! Now, try to come up with your own application and conquer the world with it.</p><p class="calibre8">Let's wrap up the chapter with a critical note on object detection using the Viola and Jones object categorization framework. As long as your application is focusing on detecting one or two object classes, then this approach works fairly well. However, once you want to tackle multiclass detection problems, it might be good to look for all the other object categorization techniques out there and find a more suitable one for your application, since running a ton of cascade classifiers on top of a single image will take forever.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note88" class="calibre1"/>Note</h3><p class="calibre8">Some very promising object categorization frameworks that are in research focus at the moment, or that are a solid base for newer techniques, can be found below. They might be an interesting starting point for people wanting to go further than the OpenCV possibilities.</p><div class="book"><ul class="itemizedlist"><li class="listitem">Dollár P., Tu Z., Perona P., and Belongie S (2009, September), Integral Channel Features. In BMVC (Vol. 2, No. 3, p. 5)</li><li class="listitem">Dollár P., Appel R., Belongie S., and Perona P (2014), Fast feature pyramids for object detection. Pattern Analysis and Machine Intelligence, IEEE transactions on, 36(8), 1532-1545.</li><li class="listitem">Krizhevsky A., Sutskever I., and Hinton G. E (2012), Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105).</li><li class="listitem">Felzenszwalb P. F., Girshick R. B., McAllester D., and Ramanan D (2010), Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9), 1627-1645.</li></ul></div></div></div>
<div class="book" title="Summary" id="1GKCM1-940925703e144daa867f510896bffb69"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec45" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">This chapter brought together a wide variety of tips and tricks concerning the cascade classifier object detection interface in OpenCV 3 based on the Viola and Jones framework for face detection. We went through each step of the object detection pipeline and raised attention to points where it can go wrong. This chapter supplied you with tools to optimize the result of your cascade classifier for any desired object model, while at the same time suggesting optimal parameters to choose from.</p><p class="calibre8">Finally, some scene specific examples were used to illustrate that a weaker trained object model can also perform well if you use the knowledge of the scene to remove false positive detections.</p></div></body></html>