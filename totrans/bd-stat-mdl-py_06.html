<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer094">
<h1 class="chapter-number" id="_idParaDest-100"><a id="_idTextAnchor104"/>6</h1>
<h1 id="_idParaDest-101"><a id="_idTextAnchor105"/>Simple Linear Regression</h1>
<p>In previous chapters, we worked with distributions of single variables. Now we will discuss the relationships between variables. In this chapter and the next chapter, we will investigate the relationship between two or more variables using linear regression. In this chapter, we will discuss simple linear regression within the framework of <strong class="bold">Ordinary Least Squares</strong> (<strong class="bold">OLS</strong>) regression. Simple linear regression is a very useful tool for estimating continuous values from two linearly related variables. We will provide an overview of the intuitions and calculations behind regression errors. Next, we will provide an overview of the pertinent assumptions of linear regression. After that, we will analyze the output summary of OLS in <strong class="source-inline">statsmodels</strong>, and finally, we will address the scenarios of serial correlation and model validation. As highlighted, our main topics in this chapter follow <span class="No-Break">this framework:</span></p>
<ul>
<li>Simple linear regression <span class="No-Break">using OLS</span></li>
<li>Coefficients of correlation <span class="No-Break">and determination</span></li>
<li>Required <span class="No-Break">model assumptions</span></li>
<li>Test <span class="No-Break">for significance</span></li>
<li>Handling <span class="No-Break">model errors</span></li>
<li><span class="No-Break">Validating models</span></li>
</ul>
<h1 id="_idParaDest-102"><a id="_idTextAnchor106"/>Simple linear regression using OLS</h1>
<p>We will study<a id="_idIndexMarker459"/> one <a id="_idIndexMarker460"/>of the simplest machine learning models – simple linear regression. We will provide its overview within the context of OLS, where the objective is to minimize the sum of the square of errors. It is a straightforward concept related to a dependent variable (quantitative response) <span class="_-----MathTools-_Math_Variable">y</span> and its independent variable <span class="_-----MathTools-_Math_Variable">x</span>, where their relationship can be drawn as a straight line, approximately. Mathematically, a simple linear regression model can be written in the <span class="No-Break">following form:</span></p>
<p><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span></p>
<p>Here, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> is the intercept term and <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> is the slope of the linear model. The error term is denoted as <span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span> in the preceding linear model. We can see that in an ideal case where the error term is zero, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> represents the value of the dependent variable <span class="_-----MathTools-_Math_Variable">y</span> at <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>. Within the range of the independent variable <span class="_-----MathTools-_Math_Variable">x</span>, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> represents the increase in the outcome <span class="_-----MathTools-_Math_Variable">y</span> corresponding to a unit change in <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">.</span> In literature, the independent variable <span class="_-----MathTools-_Math_Variable">x</span> can be called the explanatory variable, predictor, input, or feature and the dependent variable <span class="_-----MathTools-_Math_Variable">y</span> can be called the response variable, output, or target. The question is raised, “If we have a sample from a dataset <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span> with <span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1,2</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span> points), how do we determine a line of best fit using the intercept term and the slope?” By estimating these terms, we will fit the best line through the data to show the linear relationship between the independent variable and the <span class="No-Break">dependent variable.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<img alt="Figure 6.1 – The relationship between x and y – Line of best fit" height="568" src="image/B18945_06_001.jpg" width="712"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – The relationship between x and y – Line of best fit</p>
<p>In the preceding plot example, the blue points are actual values representing the relationship between <a id="_idIndexMarker461"/>the<a id="_idIndexMarker462"/> independent variable <span class="_-----MathTools-_Math_Variable">x</span> and the dependent variable <span class="_-----MathTools-_Math_Variable">y</span>. The red line is the line of best fit through all these data points. Our objective now is to formulate the predicted line (the line of <span class="No-Break">best fit):</span></p>
<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">β</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">1</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span></p>
<p>Take a closer look at the relationship between the predicted values and the true values. In order to find the line of best fit, we will minimize the vertical distances, meaning that we will minimize the errors between the points and the line through the data. In other words, we will minimize the sum of square errors, which is computed by <span class="No-Break">the following:</span></p>
<p><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<img alt="Figure 6.2 – Errors between data points and the line through the data" height="682" src="image/B18945_06_002.jpg" width="962"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Errors between data points and the line through the data</p>
<p><span class="No-Break">Here,</span></p>
<p><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span></p>
<p>where <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> is the observed value and <span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> is the predicted value of the response variable <span class="_-----MathTools-_Math_Variable">y</span> for the <span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">h</span> observation. The sum of the square of errors is <span class="No-Break">given by:</span></p>
<p><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<p>To minimize S, we will take the partial derivatives with respect to <span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> and <span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">.</span> Then, <span class="No-Break">we see:</span></p>
<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span></p>
<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">∂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">.</span></span></p>
<p>Therefore, it is easy to <span class="No-Break">see that:</span></p>
<p><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span></p>
<p><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span></p>
<p><span class="No-Break">This implies:</span></p>
<p><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span></p>
<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">.</span></span></p>
<p>Because <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span>   and <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span>, we can rewrite the slope and intercept <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"> </span></span><span class="No-Break">,</span></p>
<p><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">___________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">.</span></span></p>
<p>In Python, we can <a id="_idIndexMarker463"/>implement<a id="_idIndexMarker464"/> the following code to find <span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> and <span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> <span class="No-Break">as follows:</span></p>
<pre class="source-code">
def least_squares_method(x,y):
    x_mean=x.mean()
    y_mean=y.mean()
    beta1 = ((x-x_mean)*(y-y_mean)).sum(axis=0)/ ((x-x.mean())**2).sum(axis=0)
    beta0 = y_mean-(beta1*x_mean)
    return beta0, beta1</pre>
<h1 id="_idParaDest-103"><a id="_idTextAnchor107"/>Coefficients of correlation and determination</h1>
<p>In this section, we will discuss two related notions – coefficients of correlation and coefficients <span class="No-Break">of determination.</span></p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor108"/>Coefficients of correlation</h2>
<p>A coefficient of <a id="_idIndexMarker465"/>correlation is a measure of the statistical linear relationship between two variables and can be computed using the <span class="No-Break">following formula:</span></p>
<p><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">‾</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">‾</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
<p>The reader can go here – <a href="https://shiny.rit.albany.edu/stat/corrsim/">https://shiny.rit.albany.edu/stat/corrsim/</a> – to simulate the correlation relationship between <span class="No-Break">two variables.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<img alt="Figure 6.3 – Simulated bivariate distribution" height="1665" src="image/B18945_06_003.jpg" width="1643"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Simulated bivariate distribution</p>
<p>By observing the scatter plots, we can see the direction and the strength of the linear relationship between the two variables and their outliers. If the direction is positive (r&gt;0), then both<a id="_idIndexMarker466"/> variables increase or <span class="No-Break">decrease together.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<img alt="Figure 6.4 – Simulated bivariate distribution" height="418" src="image/B18945_06_004.jpg" width="418"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Simulated bivariate distribution</p>
<p>If the direction is negative, then one variable increases while the other decreases. We illustrate this in the following <span class="No-Break">scatter plot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<img alt="Figure 6.5 – Simulated bivariate distribution" height="529" src="image/B18945_06_005.jpg" width="529"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Simulated bivariate distribution</p>
<p>The coefficient of correlation exists as a value between -1 and 1. When r =1, we have a perfect positive correlation, and when r = -1, we have a perfect negative correlation. Observe that the<a id="_idIndexMarker467"/> value of the coefficient of correlation does not change if, for example, the variable <span class="_-----MathTools-_Math_Variable">x</span> and the variable <span class="_-----MathTools-_Math_Variable">y</span> are switched or if all values of either variable are linearly scaled. In general, correlation does not <span class="No-Break">imply causation.</span></p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor109"/>Coefficients of determination</h2>
<p>The coefficient of <a id="_idIndexMarker468"/>determination is just <span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>, where <span class="_-----MathTools-_Math_Variable">r</span> is the coefficient of correlation, which is a proportion of the variation in the variable <span class="_-----MathTools-_Math_Variable">y</span>, explained by the variable <span class="_-----MathTools-_Math_Variable">x</span>. The value of <span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> is between 0 and 1, with the linear relationship between the two variables becoming stronger as its value approaches 1. The value of <span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> can also be computed using the <span class="No-Break">following formula:</span></p>
<p><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base"> </span></p>
<p>Here, <span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">s</span> is the sum of the square of errors and <span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">t</span>is the total sum of the square of the difference between the actual value and the average value. In <strong class="source-inline">statsmodels</strong>, from the output of the OLS <a id="_idIndexMarker469"/>regression results, the value of determination can be obtained. This output is discussed in the next sections of <span class="No-Break">this chapter.</span></p>
<h1 id="_idParaDest-106"><a id="_idTextAnchor110"/>Required model assumptions</h1>
<p>Like the parametric <a id="_idIndexMarker470"/>tests we discussed in <a href="B18945_04.xhtml#_idTextAnchor070"><span class="No-Break"><em class="italic">Chapter 4</em></span></a><em class="italic">, Parametric Tests</em>, linear regression is a parametric method and requires certain assumptions to be met for the results to be valid. For linear regression, there are <span class="No-Break">four assumptions:</span></p>
<ul>
<li>A linear relationship <span class="No-Break">between variables</span></li>
<li>The normality of <span class="No-Break">the residuals</span></li>
<li>The homoscedasticity of <span class="No-Break">the residuals</span></li>
<li><span class="No-Break">Independent samples</span></li>
</ul>
<p>Let’s discuss each of these <span class="No-Break">assumptions individually.</span></p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor111"/>A linear relationship between the variables</h2>
<p>When thinking about fitting a linear model to data, our first consideration should be whether the model is appropriate for the data. When working with two variables, the relationship between the variables should be assessed with a scatter plot. Let’s look at an example. Three scatter plots are shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.6</em>. The data is plotted, and the actual function used to generate the data is drawn over the data points. The leftmost plot shows data exhibiting a linear relationship. The middle plot shows data exhibiting a quadratic relationship. The rightmost plot shows two <span class="No-Break">uncorrelated variables.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<img alt="Figure 6.6 – Three scatter plots depicting three distinct relationships between two variables" height="332" src="image/B18945_06_006.jpg" width="1035"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Three scatter plots depicting three distinct relationships between two variables</p>
<p>Of the example data in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.6</em>, a linear model would only be appropriate for the leftmost data. In this example, we have the benefit of knowing the actual relationship of the two variables, but, in real problems, you will have to determine whether a linear model is appropriate. Once we have assessed whether a linear model is appropriate and fitted a linear model, we can assess the two assumptions that depend on the <span class="No-Break">model residuals.</span></p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor112"/>Normality of the residuals</h2>
<p>In the previous two <a id="_idIndexMarker471"/>chapters, we discussed normality at length and looked at several examples of data that are normally distributed and several examples that were not normally distributed. In this case, we are making the same assessment, but it is for the model residuals rather than for the variables. In fact, unlike the previous parametric methods, the <em class="italic">variables themselves do not need to be normally distributed, only the residuals</em>. Let’s look at an example. We will fit a linear model on two variables, each of which exhibits a skewed distribution. As can be seen in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em>, even though each variable has a skewed distribution, the variables are <span class="No-Break">linearly correlated.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<img alt="Figure 6.7 – Distributions of X, Y, and their scatter plots" height="359" src="image/B18945_06_007.jpg" width="1030"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Distributions of X, Y, and their scatter plots</p>
<p>Now, let’s look at the residuals from the model fit. The residuals from the model fit are shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.8</em>. As we expect, the residuals from this model fit appear to be normally distributed. This model appears to meet the assumption of normally <span class="No-Break">distributed residuals.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<img alt="Figure 6.8 – Residuals from model fit" height="290" src="image/B18945_06_008.jpg" width="429"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Residuals from model fit</p>
<p>With the assumption of normally distributed residuals verified, let’s move on to the assumption of homoscedasticity of <span class="No-Break">the residuals.</span></p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor113"/>Homoscedasticity of the residuals</h2>
<p>Not only should the<a id="_idIndexMarker472"/> residuals be normally distributed, but the residuals should also have constant variance over the linear model. <strong class="bold">Homoscedasticity</strong> refers to<a id="_idIndexMarker473"/> the residuals having equal scatter. Conversely, residuals that do exhibit equal spread are said to be <strong class="bold">heteroscedastic</strong>. Homoscedasticity is <a id="_idIndexMarker474"/>generally assessed by plotting the residuals against the model predictions with a scatter plot. The residuals should appear to be randomly distributed with a mean of 0 and equally dispersed along the <em class="italic">x</em> axis. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em> shows a scatter plot of the model residuals against the predicted value of <em class="italic">Y</em>. These residuals appear to <span class="No-Break">exhibit homoscedasticity.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<img alt="Figure 6.9 – Scatter plot of model residuals versus prediction" height="326" src="image/B18945_06_009.jpg" width="487"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Scatter plot of model residuals versus prediction</p>
<p>There are a few common ways the homoscedasticity of the residuals could <span class="No-Break">be violated:</span></p>
<ul>
<li>The residuals show a systematic change <span class="No-Break">in variance</span></li>
<li>There is an <span class="No-Break">extreme outlier</span></li>
</ul>
<p>Two examples of these violations are shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.10</em>. The left plot shows an extreme outlier, and the right plot shows non-constant variance in the residuals. In both cases, the model assumptions <span class="No-Break">are violated.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<img alt="Figure 6.10 – Example residuals that exhibit poor behavior" height="358" src="image/B18945_06_010.jpg" width="1032"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – Example residuals that exhibit poor behavior</p>
<p>When the residuals show a systematic pattern, it may be an indication that another type of model would be more appropriate, or it may be helpful to apply a transformation to one or both <a id="_idIndexMarker475"/>variables. When an extreme outlier is present, it may be worth verifying and investigating the data point to ensure it should be included in the analysis. In the next section, we will discuss how extreme outliers impact the <span class="No-Break">model fit.</span></p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor114"/>Sample independence</h2>
<p>We have discussed sample independence in previous chapters. There are no graphics or statistics that can be used to determine whether samples are independent. <em class="italic">Assessing sample independence requires careful analysis of the sampling method and the populations from which the samples are drawn</em>. For example, a common type of data that violates the independence assumption is time-series data. Time-series data is a type of data that is sampled over time, making it serially correlated. We will discuss analysis methods for time-series data in <span class="No-Break">later chapters.</span></p>
<p>In this section, we discussed the linear regression model assumptions. In the next section, we will discuss how to validate a linear model, which will again utilize the <span class="No-Break">model residuals.</span></p>
<h1 id="_idParaDest-111"><a id="_idTextAnchor115"/>Testing for significance and validating models</h1>
<p>Up to this point in the chapter, we have discussed the concepts of the OLS approach to linear regression modeling; the coefficients in a linear model; the coefficients of correlation and determination; and the assumptions required for modeling with linear regression. We will now begin our discussion on testing for significance and <span class="No-Break">model validation.</span></p>
<h3>Testing for significance</h3>
<p>To test for <a id="_idIndexMarker476"/>significance, let us load <strong class="source-inline">statsmodels</strong> macrodata data set so we can build a model that tests the relationship between real gross private domestic investment, <strong class="source-inline">realinv</strong>, and real private disposable <span class="No-Break">income, </span><span class="No-Break"><strong class="source-inline">realdpi</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
from statsmodels.nonparametric.smoothers_lowess import lowess
df = sm.datasets.macrodata.load().data</pre>
<p>Least squares regression requires a constant coefficient in order to derive the <strong class="bold">intercept</strong>. In the least squares equation, <span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span>, the first column of the <strong class="bold">design matrix</strong>, <em class="italic">X</em>, requires a set of ones. However, this <strong class="bold">constant</strong> must be added in directly, which we do with <strong class="source-inline">statsmodels’</strong> <strong class="source-inline">add_constant</strong> <span class="No-Break">function here:</span></p>
<pre class="source-code">
df = sm.add_constant(df, prepend=False)
df_mod = df[['realinv','realdpi','const']]</pre>
<p>The input data does not need to be normally distributed for least squares regression. However, it is assumed that the residuals of the model are normally distributed. However, it is useful to see the spread of the data to get an understanding of their statistics, such as the mean, median, and range, and whether there are outliers present. If the residual analysis after the modeling suggests there may be some issues, having inspected the model variables will help the analyst understand potential <span class="No-Break">root causes.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<img alt="Figure 6.11 – Visualizing the distributions of the realinv and realdpi variables" height="434" src="image/B18945_06_011.jpg" width="872"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Visualizing the distributions of the <em class="italic">realinv</em> and <em class="italic">realdpi</em> variables</p>
<p>When visualizing the relationship between <strong class="source-inline">realinv</strong> and <strong class="source-inline">realdpi</strong>, we can see a strong linear relationship, but also a potential serial correlation in the data as there appears to be a <a id="_idIndexMarker477"/>somewhat <strong class="bold">cyclical oscillation</strong> in the data, which becomes <a id="_idIndexMarker478"/>stronger toward the extreme values, seen in the upper right of <span class="No-Break">the plot.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<img alt="Figure 6.12 – Visualizing the relationship between the realinv and realdpi variables" height="627" src="image/B18945_06_012.jpg" width="894"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Visualizing the relationship between the <em class="italic">realinv</em> and <em class="italic">realdpi</em> variables</p>
<p>We will analyze the potential serial correlation seen in the preceding plot after a few more steps. First, let us fit the input variable and coefficient to <span class="No-Break">the model:</span></p>
<pre class="source-code">
ols_model = sm.OLS(df_mod['realdpi'], df_mod[['const','realinv']])
compiled_model = ols_model.fit()</pre>
<p>Next, we will want to print out a summary of the model performance so we can begin to gauge the significance of the <span class="No-Break">terms involved:</span></p>
<pre class="source-code">
print(compiled_model.summary())</pre>
<p>Referring to the results in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.13</em>, we can see that the <strong class="bold">95% confidence interval</strong> for neither the intercept, <strong class="source-inline">const</strong>, nor the input variable, <strong class="source-inline">realinv</strong>, contains zero. Therefore, in <a id="_idIndexMarker479"/>addition to the <strong class="bold">significant p-values</strong> for these variables, we can conclude they are significant contributors to the target at the coefficients provided. Now that we have confirmed there is statistical significance for both the intercept and the input variable and their coefficients, we want to know the level of correlation to the target they represent. We can<a id="_idIndexMarker480"/> see the <strong class="bold">R-squared</strong> statistic is 0.944. Because the two variables are the constant and the input variable, this R-squared statistic explains only the <span class="No-Break">input variable.</span></p>
<p class="callout-heading">R-squared versus adjusted r-squared</p>
<p class="callout">For multivariate regression, we would look at the adjusted r-squared value, which we can also see is the same – in the univariate/simple regression case – as the <span class="No-Break">r-squared value.</span></p>
<p>The <strong class="bold">correlation of determination</strong> – also called goodness of fit – of 0.944 means real gross private <a id="_idIndexMarker481"/>domestic investment explains 94.4% of the variance in real private <span class="No-Break">disposable income.</span></p>
<p>We can see based on the table that the intercept and the <strong class="source-inline">realinv</strong> variable are both significant in predicting the target with very low p-values. We can also see that neither of the 95% confidence intervals contains 0, which backs up the relevancy of their p-values. However, the confidence interval for the constant (intercept) suggests there may be a risk of <a id="_idIndexMarker482"/>uncertainty in the model and that more variables may be needed to reduce that uncertainty and improve model performance. Stated differently, we could say a higher level of uncertainty in the constant indicates the model has variance left to be explained and that the current model is <span class="No-Break">overly biased.</span></p>
<h3>Validating models</h3>
<h4>The Durbin-Watson test</h4>
<p>It is important<a id="_idIndexMarker483"/> to acknowledge that based on the fact that data was observed over a 50-year period and random sampling was not performed, it is possible the residuals will have a <strong class="bold">serial correlation</strong>. We <a id="_idIndexMarker484"/>observe a possibility of serial correlation in<a id="_idIndexMarker485"/> the <strong class="bold">Durbin-Watson test</strong> statistic of 0.095, which suggests the residuals exhibit positive autocorrelation. The Durbin-Watson statistic tests whether there is autocorrelation at the first time lag. This means the Durbin-Watson tests whether the change in values between each data point is correlated with the value of the previous data point. If that correlation exists, it is <a id="_idIndexMarker486"/>considered <span class="No-Break"><strong class="bold">lag-one autocorrelation</strong></span><span class="No-Break">.</span></p>
<p class="callout-heading">Durbin-Watson test statistic</p>
<p class="callout">As a rule <a id="_idIndexMarker487"/>of thumb, a Durbin-Watson test statistic below roughly 2.0 (many consider 1.5 to 2.0 to be reasonable) is considered a flag for positive autocorrelation, while a value above roughly 2.0 (many consider 2.0 to 2.5 to be reasonable) indicates possible negative autocorrelation. Procedurally, a table lookup is required to identify the critical values against which the test statistic should be measured. The process follows the same steps for any hypothesis test with respect to comparing the test statistic against the critical value. The Durbin-Watson test is typically interpreted as a one-tailed test as inference from a two-tailed test is not particularly useful. A version of the Durbin-Watson table can be found at <a href="https://www.real-statistics.com/statistics-tables/durbin-watson-table/">https://www.real-statistics.com/statistics-tables/durbin-watson-table/</a>. This table produces the critical<a id="_idIndexMarker488"/> values for <strong class="bold">positive autocorrelation</strong>. To find the critical<a id="_idIndexMarker489"/> values for <strong class="bold">negative autocorrelation</strong>, the positive critical values can be subtracted <span class="No-Break">from 4.</span></p>
<p>Although we have a Durbin-Watson statistic that suggests serial correlation may be present in the data, we will confirm significance by comparing the Durbin-Watson statistic to its corresponding positive autocorrelation <strong class="bold">table lookup</strong> using the table found at the preceding link. We have n=203 samples in our dataset but will use n=200 on the table lookup since this value is likely accurate enough to assess, and 203 does not exist in the table. At the 0.05 level of significance, which is used by default with the Durbin-Watson statistic produced in <strong class="source-inline">statsmodels</strong>’ OLS regression, we find the bounds for the <strong class="source-inline">k=1</strong> input variable to be <strong class="source-inline">[1.758, 1.779]</strong>. A value within this range should be interpreted as <strong class="bold">inconclusive</strong>. A value less than the lower bound indicates the presence of enough evidence to reject the null hypothesis and thus conclude there is statistically significant positive <a id="_idIndexMarker490"/>autocorrelation. A value greater than the upper bound indicates there is not enough evidence to reject the null hypothesis and thus there is no autocorrelation. Based on the results of our model, we can conclude with a 95% level of confidence that there is evidence of positive serial correlation in <span class="No-Break">our residuals.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="Figure 6.13 – OLS regression model output for realdpi" height="832" src="image/B18945_06_013.jpg" width="624"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – OLS regression model output for <em class="italic">realdpi</em></p>
<p>At this point, an analyst may want to consider addressing the serial correlation before moving forward. However, for the purpose of following the process of end-to-end model validation, we<a id="_idIndexMarker491"/> will move forward to the <span class="No-Break">next steps.</span></p>
<h4>Diagnostic plots for analyzing model errors</h4>
<p>As mentioned, <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span>, where in regression, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> is the predicted mean for <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> given <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> and <span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> is<a id="_idIndexMarker492"/> the error term – also called the residual term, which is calculated as the observed value for a data point minus the predicted value for that data point. The residuals in least squares linear regression must approximate a normal distribution centered around the mean with a standard deviation. The method for deriving the residuals for a fitted model is to subtract the predicted value from the actual value for each observation. Four common visualizations for inspecting the fit of a least squares regression model are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Residuals versus fitted plots</strong>, used to inspect the requirement of a linear relationship between the input and <span class="No-Break">target variables</span></li>
<li><strong class="bold">Quantile plots</strong>, used to inspect the assumption of residuals being <span class="No-Break">normally distributed</span></li>
<li><strong class="bold">Scale-location plots</strong>, used to inspect the assumption of homoscedasticity, or the homogenous distribution <span class="No-Break">of variation</span></li>
<li><strong class="bold">Residuals versus leverage influence plots</strong>, which help identify the impact outliers may have on <span class="No-Break">model fit</span></li>
</ul>
<p>To produce these plots, we can use the following Python <span class="No-Break">code implementation:</span></p>
<pre class="source-code">
model_residuals = compiled_model.resid
fitted_value = compiled_model.fittedvalues
standardized_residuals = compiled_model.resid_pearson # Residuals, normalized to have unit variance.
sqrt_standardized_residuals = np.sqrt(np.abs(compiled_model.get_influence().resid_studentized_internal))
influence = compiled_model.get_influence()
leverage = influence.hat_matrix_diag
cooks_distance = compiled_model.get_influence().cooks_distance[0]
fig, ax = plt.subplots(2, 2, figsize=(10,8))
# Residuals vs. Fitted
ax[0, 0].set_xlabel('Fitted Values')
ax[0, 0].set_ylabel('Residuals')
ax[0, 0].set_title('Residuals vs. Fitted')
locally_weighted_line1 = lowess(model_residuals, fitted_value)
sns.scatterplot(x=fitted_value, y=model_residuals, ax=ax[0, 0])
ax[0, 0].axhline(y=0, color='grey', linestyle='--')
ax[0,0].plot(locally_weighted_line1[:,0], locally_weighted_line1[:,1], color = 'red')
# Normal Q-Q
ax[0, 1].set_title('Normal Q-Q')
sm.qqplot(model_residuals, fit=True, line='45',ax=ax[0, 1], c='blue')
# Scale-Location
ax[1, 0].set_xlabel('Fitted Values')
ax[1, 0].set_ylabel('Square Root of Standardized Residuals')
ax[1, 0].set_title('Scale-Location')
locally_weighted_line2 = lowess(sqrt_standardized_residuals, fitted_value)
sns.scatterplot(x=fitted_value, y=sqrt_standardized_residuals, ax=ax[1, 0])
ax[1,0].plot(locally_weighted_line2[:,0], locally_weighted_line2[:,1], color = 'red')
# Residual vs. Leverage Influence
ax[1, 1].set_xlabel('Leverage')
ax[1, 1].set_ylabel('Standardized Residuals')
ax[1, 1].set_title('Residuals vs. Leverage Influence')
locally_weighted_line3 = lowess(standardized_residuals, leverage)
sns.scatterplot(x=leverage, y=standardized_residuals, ax=ax[1, 1])
ax[1, 1].plot(locally_weighted_line3[:,0], locally_weighted_line3[:,1], color = 'red')
ax[1, 1].axhline(y=0, color='grey', linestyle='--')
ax[1, 1].axhline(3, color='orange', linestyle='--', label='Outlier Demarkation')
ax[1, 1].axhline(-3, color='orange', linestyle='--')
ax[1, 1].legend(loc='upper right')
leverages = []
for i in range(len(cooks_distance)):
    if cooks_distance[i] &gt; 0.5:
        leverages.append(leverage[i])
        ax[1, 1].annotate(str(i) + " Cook's D &gt; 0.5",xy=(leverage[i], standardized_residuals[i]))
if leverages:
    ax[1, 1].axvline(min(leverages), color='red', linestyle='--', label="Cook's Distance")
for i in range(len(standardized_residuals)):
    if standardized_residuals[i] &gt; 3 or standardized_residuals[i] &lt; -3:
        ax[1, 1].annotate(i,xy=(leverage[i], standardized_residuals[i]))
fig.tight_layout()</pre>
<p>The following <a id="_idIndexMarker493"/>output contains four common <span class="No-Break">diagnostic plots.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="Figure 6.14 – Linear regression diagnostic plots" height="776" src="image/B18945_06_014.jpg" width="975"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – Linear regression diagnostic plots</p>
<h4>Residuals versus fitted</h4>
<p>One requirement <a id="_idIndexMarker494"/>of least squares regression is a <strong class="bold">linear relationship</strong> between <a id="_idIndexMarker495"/>the input and target variables. Suffice it <a id="_idIndexMarker496"/>to say a strongly correlated relationship exists between the input and target variables. This plot helps the analyst assess the linearity between <span class="No-Break">the two.</span></p>
<p>We should expect the fitted line – representing the model – to be horizontal, representing the strength of the linear relationship between the input and target variable across all data points. The residuals should be roughly evenly spread around the line in reasonable proximity. If these attributes cannot be confirmed, a non-linear relationship may exist between the two variables. A transformation of at least one of the variables – if the variable is skewed – may help resolve this type of issue. However, it could also be that a non-parametric model is more appropriate, which is often the case when categorical features – or encoded categorical features – <span class="No-Break">are present.</span></p>
<p>Our model produced a plot that does not display evenly spread residuals forming around a horizontal line. Rather, the line is concaved down, which suggests a polynomial or non-parametric regression may be more appropriate. Additionally, the residuals form a pattern toward the right end of the line that suggests there may be some serial correlation in the data as it<a id="_idIndexMarker497"/> exhibits some sinusoidal behavior. We were able to identify the presence of serial correlation using the Durbin-Watson statistic. This is likely due to the fact the data was not randomly sampled and was collected over a long period <span class="No-Break">of time.</span></p>
<h4>QQ plot of residuals</h4>
<p>Assessment of <a id="_idIndexMarker498"/>normality for the QQ plot follows the same procedure <a id="_idIndexMarker499"/>as assessing normality using the QQ plot in <a href="B18945_04.xhtml#_idTextAnchor070"><span class="No-Break"><em class="italic">Chapter 4</em></span></a><em class="italic">, Parametric Tests</em>. The primary purpose of this plot is to assess the required assumption <a id="_idIndexMarker500"/>of the <strong class="bold">normal distribution of errors</strong>. Minor deviation of the residuals from the 45-degree line in the plot is normal, but extreme skewness or deviation from the line may indicate a poor fit. Such a scenario could indicate poor statistical power. In the event of skewness, data transformation – such as a log transformation – could be useful to resolve the issue. Dropping outliers may be another suitable solution. However, rather than dropping outliers outright, it is advisable to ensure there was not an error in data handling that could be resolved to make the <span class="No-Break">outliers conform.</span></p>
<p>In the QQ plot for our model’s residuals, we can see some moderate left-side skewness and extreme right-side skewness, which suggests the three values in the right tail could be extreme <a id="_idIndexMarker501"/>outliers. Aside from this, we can assume that overall, the residual error is approximately <span class="No-Break">normally distributed.</span></p>
<h4>Scale-location</h4>
<p>The scale-location plot <a id="_idIndexMarker502"/>of the square root of the<a id="_idIndexMarker503"/> standardized residuals helps analysts use visual inspection to determine whether <a id="_idIndexMarker504"/>there is <strong class="bold">homoscedasticity</strong> – or <em class="italic">non-constant</em> variance – of the residuals, which is required for a least squares regression model. The plot visualizes the square root of the absolute value of the standardized residuals. Analyzing homoscedasticity can sometimes be useful for identifying potential issues related to <strong class="bold">sample independence</strong>, another requirement of least-squares regression. However, the Durbin-Watson test should be given more weight for testing <span class="No-Break">this assumption.</span></p>
<p>In assessing our model, we can see the line has some deviation from being perfectly horizontal. Additionally, the residuals, while seeming to have a large amount of constant variance, nonetheless at times exhibit patterns, which conflicts with the behavior of homoscedasticity. Furthermore, there appear to be three very notable outliers. Based on this information, we can reasonably assume a risk of heteroscedasticity – or <em class="italic">constant</em> variance – exists within the <span class="No-Break">model’s residuals.</span></p>
<h4>Residuals versus leverage influence</h4>
<p>Assessing the <a id="_idIndexMarker505"/>residuals versus <a id="_idIndexMarker506"/>leverage influence is another approach to assessing model fit. In addition to gauging leverage, this plot also shows when <strong class="bold">Cook’s distance</strong> is beyond a <span class="No-Break">reasonable level.</span></p>
<p><strong class="bold">Leverage</strong> is used to <a id="_idIndexMarker507"/>identify the distance of an individual point from all other points. High leverage for a residual likely means the corresponding data point is an outlier strongly influencing the model to fit less approximately to the overall data and instead give more weight to that specific value. Residuals should be between -2 and 2 to not be considered potential outliers. Values between +/-2.5 to 3 suggest data points are extreme outliers. Overall, this plot is useful for separating outliers that have no significant negative impact on the model from the ones <span class="No-Break">that do.</span></p>
<p><strong class="bold">Cook’s distance</strong> is a measurement<a id="_idIndexMarker508"/> that uses both an observation’s leverage and its residual value. As the leverage is higher, its calculated Cook’s distance is higher. In general, a Cook’s distance value greater than 0.5 means a residual has leverage that is negatively impacting the model through over-representation – essentially, an outlier. Cook’s distance is included in the code and flags the residuals that have distances greater than 0.5. Cook’s distance is particularly useful because removing outliers may not have a significant impact on the model. However, if the outlier has a high Cook’s distance value, that is a strong indication that resolving the outlier <strong class="bold">will benefit</strong> the model. Cook’s distance is calculated for each data point by removing it from the model and calculating the difference in error divided by the mean squared error multiplied by the number of model coefficients plus one. Its <span class="No-Break">formulation follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-normal">ŷ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">ŷ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">__________</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">̂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span></p>
<p>where <span class="_-----MathTools-_Math_Variable">ŷ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span> is the <em class="italic">j</em>th estimate of the response and <span class="_-----MathTools-_Math_Variable">ŷ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">)</span> is the value of the <em class="italic">j</em>th fitted value with the <em class="italic">i</em>th value removed. <em class="italic">p</em> is the number of coefficients in the model and <span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">̂</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> is the variance of the residuals for all observations, including those removed, also called the <span class="No-Break">squared error.</span></p>
<p>In the plot for our model, we can observe three extreme outliers. However, we can also see no value of Cook’s<a id="_idIndexMarker509"/> distance is greater than 0.5. As with the scale-location plot, the standardized<a id="_idIndexMarker510"/> residuals should follow even distribution around a smoothed line that is horizontal, which indicates a constant variance. We can see from our model’s output that this is not <span class="No-Break">the case.</span></p>
<h3>Addressing issues with residuals</h3>
<p>Alone, each plot can <a id="_idIndexMarker511"/>be considered enough to negate <a id="_idIndexMarker512"/>the validity of a model. However, it is useful to consider all plots since a model may still be useful if there is only some deviance from expectations in the diagnostic plots. Some approaches to resolving issues with residuals are <span class="No-Break">as follows:</span></p>
<ul>
<li>Investigating and resolving any data collection issues that may produce outliers or <span class="No-Break">data inconsistencies</span></li>
<li>Removing <span class="No-Break">outliers outright</span></li>
<li>Improving the sampling approach using methods such as using random or <span class="No-Break">stratified sampling</span></li>
<li>Performing data transformations, such as a log or square <span class="No-Break">root transformation</span></li>
<li>Including <a id="_idIndexMarker513"/>additional input variables that can <a id="_idIndexMarker514"/>help explain the target variable or any serial correlation that may <span class="No-Break">be present</span></li>
</ul>
<h3>Handling serial correlation</h3>
<p>If uncertain <a id="_idIndexMarker515"/>of the presence of serial correlation in the residuals, a useful next step is to analyze a <strong class="bold">Partial Autocorrelation Function</strong> (<strong class="bold">PACF</strong>) plot to <a id="_idIndexMarker516"/>assess whether serial correlation exists in the model at a significant level, which could explain issues with the <span class="No-Break">model’s residuals.</span></p>
<p class="callout-heading">ACF and PACF</p>
<p class="callout">The <strong class="bold">PACF</strong> provides<a id="_idIndexMarker517"/> a partial correlation between the value of a point and previous points, called lags, by controlling for the lags between. Controlling the lags between is where the term “partial” comes from. If a specific point at lag zero in a dataset is correlated strongly with the value of the third lagging point, there would be a significant correlation with a lag of 3, but not a lag of 2 or 1. In time-series modeling, the PACF is used to directly derive the autoregressive orders, denoted as <em class="italic">AR(n)</em> where <em class="italic">n</em> is the lag. The<strong class="bold"> Autocorrelation Function</strong> (<strong class="bold">ACF</strong>), which<a id="_idIndexMarker518"/> is used for determining moving average orders, denoted as <em class="italic">MA(n),</em> considers the correlation between data points at lag zero and all previous lagging points without controlling for the relationships between them. Stated alternatively, autocorrelation in an ACF plot at lag 3 will provide autocorrelation across points 0 through 3 whereas correlation in a PACF plot at lag 3 will provide autocorrelation only between point 0 and point 3, excluding the impact of values at lags 1 <span class="No-Break">and 2.</span></p>
<p>We can execute the following code to generate the PACF plot with a 95% confidence interval using the most recent 50 data points, ordered by index positioning – in this case, <span class="No-Break">by date:</span></p>
<pre class="source-code">
from statsmodels.graphics.tsaplots import plot_pacf
plot_pacf(compiled_model.resid, alpha=0.05, lags=50);</pre>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="Figure 6.15 – Partial autocorrelation plot for the model residuals" height="373" src="image/B18945_06_015.jpg" width="566"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.15 – Partial autocorrelation plot for the model residuals</p>
<p>In the PACF plot in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.15</em>, we see very strong and significant correlations at lags 0 and 1. We observe a low level of significant correlation (approximately -0.25) for lags 2 and 36, extending beyond the 95% confidence interval. What this tells us is we might be better off modeling the<a id="_idIndexMarker519"/> dataset with a first-order autoregressive (<strong class="bold">AR(1)</strong>) time-series model than a linear <span class="No-Break">regression model.</span></p>
<p>Answering regression questions with a least squares method when there is a presence of significant autocorrelation creates inherit risk. There are methods to adjust the results of a model when there is autocorrelation detected, such as taking a <strong class="bold">first-order difference</strong> – or another type of low-pass filter - or applying the Cochrane-Orcutt, Hildreth-Lu, or Yule-Walker adjustments. However, it is ideal to perform a time-series analysis, as the results of that type of modeling are more robust to time-dependent patterns. We will introduce time series in <a href="B18945_10.xhtml#_idTextAnchor160"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Introduction to Time Series</em>, where we discuss simple and complex methods for approximating time series to produce more <span class="No-Break">useful results.</span></p>
<p>Let us consider a first-order difference of the data. Regarding <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.16</em>, we first look at the ACF plot and original data for the input variable, <strong class="source-inline">realdpi</strong>. We can see an exponentially growing line (original data) that displays an exponentially dampening ACF plot. This ACF behavior is typical for <strong class="bold">trended</strong> data that often benefits from <strong class="bold">first-order differencing</strong>. The low autoregressive ordering identified in the PACF indicates this may be sufficient for a regression model to proceed. Had the ACF shown different behavior, such as sinusoidal (seasonal) autocorrelation, or the PACF shown more partial autocorrelation, we would have evidence against a first-order difference being a sufficient solution for resolving serial correlation and proceeding with least squares regression. ACFs and PACFs for least squares regression should ideally have no significantly correlated lags. However, when the ACF exhibits this behavior and the PACF shows no significance, or when the level of correlation is very low, regression may perform well. It is preferable to see no patterns or peaks in either plot. Regardless, in either case, model error should be assessed, in addition to the autocorrelation of the residuals, to determine if least squares regression performs <span class="No-Break">as needed.</span></p>
<p>Using the <strong class="source-inline">diff()</strong> <strong class="source-inline">numpy</strong> function, we take a first-order, low-pass difference and plot the ACFs, PACFs, and line plots for the original and differenced data. We can see a significant reduction in autocorrelation as a result. As an aside, the post-differencing autocorrelation suggests<a id="_idIndexMarker520"/> an <strong class="bold">autoregressive moving average </strong>(<strong class="bold">ARMA</strong>) model of AR order 1 and MA order 1 may be better suited than least-squares regression. However, we will ignore this for <a id="_idIndexMarker521"/>the chapter <span class="No-Break">and continue:</span></p>
<pre class="source-code">
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
fig, ax = plt.subplots(2,3, figsize=(15,10))
plot_acf(df_mod['realdpi'], alpha=0.05, lags=50, ax=ax[0,0])
ax[0,0].set_title('Original ACF')
plot_pacf(df_mod['realdpi'], alpha=0.05, lags=50, ax=ax[0,1])
ax[0,1].set_title('Original PACF')
ax[0,2].set_title('Original Data')
ax[0,2].plot(df_mod['realdpi'])
plot_acf(np.diff(df_mod['realdpi'], n=1), alpha=0.05, lags=50, ax=ax[1,0])
ax[1,0].set_title('Once-Differenced ACF')
plot_pacf(np.diff(df_mod['realdpi'], n=1), alpha=0.05, lags=50, ax=ax[1,1])
ax[1,1].set_title('Once-Differenced PACF')
ax[1,2].set_title('Once-Differenced Data')
ax[1,2].plot(np.diff(df_mod['realdpi'], n=1))</pre>
<p>We get the <span class="No-Break">following result:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<img alt="Figure 6.16 – Plots for the realinv variable before and after first-order differencing" height="728" src="image/B18945_06_016.jpg" width="1100"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.16 – Plots for the <em class="italic">realinv</em> variable before and after first-order differencing</p>
<p>This gives us data<a id="_idIndexMarker522"/> that we may more suitably model using regression. However, when using differencing as a method to resolve serial correlation in a least squares regression model, we must also differentiate the input variable. Observe the input variable, <strong class="source-inline">realinv</strong>, before and after transformation. Here in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.17</em>, we can see similar behavior as <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">realdpi</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<img alt="Figure 6.17 – Plots for the realdpi variable before and after first-order differencing" height="800" src="image/B18945_06_017.jpg" width="1207"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.17 – Plots for the <em class="italic">realdpi</em> variable before and after first-order differencing</p>
<p>We can now build <a id="_idIndexMarker523"/>a new regression model using the differenced data. It is important to note first-order differencing removed one data point from the dataset so one value must also be removed from the constant in the <span class="No-Break">design matrix:</span></p>
<pre class="source-code">
ols_model_1diff = sm.OLS(np.diff(df_mod['realdpi'], n=1), pd.concat([df_mod['const'].iloc[:-1], pd.Series(np.diff(df_mod['realinv'], n=1))], axis=1))
compiled_model_1diff = ols_model_1diff.fit()</pre>
<p>The output of this model is notably different. We now see an r-squared value of <strong class="source-inline">0.045</strong>. Looking at the differenced data, we can see there is a different variance in the differenced data for disposable income than there is for investment. This means that although the <strong class="source-inline">realinv</strong> variable is useful for predicting <strong class="source-inline">realdpi</strong> as it appears significant, there is a lot more influencing real disposable gross personal income. We observe that the new Durbin-Watson test statistic is now <strong class="source-inline">2.487</strong>. Using the critical values from the table lookup earlier, <strong class="source-inline">[1.758, 1.779]</strong>, we see the Durbin-Watson statistic is now above the upper limit. <strong class="bold">To check whether there is negative autocorrelation</strong>, we subtract these values from 4 to find the new limit range <strong class="source-inline">[2.221, 2.242]</strong>. Because the new statistic, <strong class="source-inline">2.487</strong>, is greater than <strong class="source-inline">2.221</strong>, we can confirm there is now negative autocorrelation in the model’s residuals. We can see based on the PACF plot, as well as the proximity to the critical value range, the autocorrelation is much less significant than before, but still present. It could be argued the autocorrelation of the residuals is now small enough to be considered resolved, however; as noted earlier, a Durbin-Watson statistic less than 2.5 can be considered normal. Nonetheless, least-squares regression remains risky compared to time-series modeling to generate our predictions. We begin an in-depth overview <a id="_idIndexMarker524"/>of this topic in <a href="B18945_10.xhtml#_idTextAnchor160"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><em class="italic">, Introduction to </em><span class="No-Break"><em class="italic">Time Series</em></span><span class="No-Break">.</span></p>
<p>Let us assume, however, that for the sake of moving through the steps of regression model validation, we have no autocorrelation in our data. The next step will be to test <span class="No-Break">our model.</span></p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor116"/>Model validation</h2>
<p>Assuming an analyst <a id="_idIndexMarker525"/>has developed a model that produces strong results and appears useful based on the previously mentioned plots, the next step is to test the model. One conventional approach is to perform a train-and-test split on the data used for the original model where we fit all data. In the following code, we run a split with the train dataset having 75% of the data and the test dataset having 25%. The purpose is to assess whether there is a significant difference in performance between the two, as well as compared to the original model. The process follows the same steps as earlier. Acceptable differences are up to the analyst to decide, but in addition to assessing the differences between the metrics already discussed, the analyst should also note the coefficients for the slope and input variable, as these will be used to predict <span class="No-Break">the target.</span></p>
<p>Here, we split the data. We use the <strong class="source-inline">shuffle</strong> argument to <em class="italic">randomly</em> shuffle the data so that if there is some order to the data, such as it being ordered by time, the data will be <span class="No-Break">split randomly:</span></p>
<pre class="source-code">
from sklearn.model_selection import train_test_split
train, test = train_test_split(df_mod, train_size=0.75, shuffle=True)</pre>
<p>Now, we build a model using the <span class="No-Break">training data:</span></p>
<pre class="source-code">
ols_model_train = sm.OLS(train['realdpi'], train[['const','realinv']])
compiled_model_train = ols_model_train.fit()
print(compiled_model_train.summary())</pre>
<p>Finally, we build a model using the <span class="No-Break">testing data:</span></p>
<pre class="source-code">
ols_model_test = sm.OLS(test['realdpi'], test[['const','realinv']])
compiled_model_test = ols_model_test.fit()
print(compiled_model_test.summary())</pre>
<p>The idea here is that the two models should produce similar results on two different partitions of <span class="No-Break">the data.</span></p>
<p>Another method for validating the model is to compare a metric against a naïve model. Let us use<a id="_idIndexMarker526"/> the <strong class="bold">mean absolute error</strong> (<strong class="bold">MAE</strong>). With this model metric, we could compare a naïve linear model’s errors where the error is each data point minus the average to the model’s <a id="_idIndexMarker527"/>predictions. A useful model MAE would need to be lower than the naïve model’s MAE. Here, we use the fit model to predict the inputs, then compare the model’s predictions to the naïve <span class="No-Break">model’s predictions:</span></p>
<p>Below we can observe the MAE from the <span class="No-Break"><em class="italic">trained</em></span><span class="No-Break"> model:</span></p>
<pre class="source-code">
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(train['realdpi'], compiled_model_train.predict(train[['const','realinv']]))
mae
#438.5872081797031</pre>
<p>Here we can see the MAE from the <em class="italic">naïve</em> model, which makes no assumptions other than that the mean will continue to hold the <span class="No-Break">true values:</span></p>
<pre class="source-code">
import numpy as np
errors = []
for i in range(len(train)):
    errors.append(abs(train['realdpi'].iloc[i] - train['realdpi'].mean()))
np.mean(errors)
#2065.440235457064</pre>
<p>We can see the model compares favorably against the <span class="No-Break">naïve model.</span></p>
<p>Another method that is popular to use is testing on a holdout dataset. For this, a model is constructed and trained on the training data. Then, using that model, we apply it to the test data. We would then compare the metric to that of the training data. Using the training model’s MAE of 438, we compare it to <span class="No-Break">the following:</span></p>
<pre class="source-code">
mae_test = mean_absolute_error(test['realdpi'], compiled_model_train.predict(test[['const','realinv']]))
mae_test
#408.0253171549187</pre>
<p>We can see that the error is lower for the test data than for the train data. However, since the train and test data are assumed to have different values and thus means, we should reassess that the naïve model still produces a higher MAE for the <span class="No-Break">test data:</span></p>
<pre class="source-code">
errors = []
for i in range(len(test)):
    errors.append(abs(test['realdpi'].iloc[i] - test['realdpi'].mean()))
np.mean(errors)
#1945.5873125720873</pre>
<p>Here we can see the model does provide better MAE than the naïve model at approximately the same <a id="_idIndexMarker528"/>rate on the test data as on the train data. Therefore, we expect that the linear regression model is better than the <span class="No-Break">naïve model.</span></p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor117"/>Summary</h1>
<p>In this chapter, we discussed an overview of simple linear regression between one explanatory variable and one response variable. The topics we covered include <span class="No-Break">the following:</span></p>
<ul>
<li>The OLS method for simple <span class="No-Break">linear regression</span></li>
<li>Coefficients of correlation and determination and their calculations <span class="No-Break">and significance</span></li>
<li>The assumptions required for least <span class="No-Break">squares regression</span></li>
<li>Methods of analysis for model and <span class="No-Break">parameter significance</span></li>
<li><span class="No-Break">Model validation</span></li>
</ul>
<p>We looked closely at the concept of the square of error and how the sum of squared errors is meaningful for building and validating linear regression models. Then, we walked through the four pertinent assumptions required to make linear regression a stable solution. After, we provided an overview of four diagnostic plots and their interpretations with respect to assessing the presence of various issues related to heteroscedasticity, linearity, outliers, and serial correlation. We then walked through an example of using the ACF and PACF to assess serial correlation and an example of using first-order differencing to remove serial correlation constraints from data and build an OLS model using the differenced data. Finally, we provided methods for testing and validating least square <span class="No-Break">regression models.</span></p>
<p>In the next chapter, we will extend this concept to include more than one explanatory variable, a technique called multiple linear regression. We will also discuss various topics related to multiple linear regression, such as variable selection <span class="No-Break">and regularization.</span></p>
</div>
</div></body></html>