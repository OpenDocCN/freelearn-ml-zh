<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-100"><a id="_idTextAnchor104"/>6</h1>
<h1 id="_idParaDest-101"><a id="_idTextAnchor105"/>Simple Linear Regression</h1>
<p>In previous chapters, we worked with distributions of single variables. Now we will discuss the relationships between variables. In this chapter and the next chapter, we will investigate the relationship between two or more variables using linear regression. In this chapter, we will discuss simple linear regression within the framework of <code>statsmodels</code>, and finally, we will address the scenarios of serial correlation and model validation. As highlighted, our main topics in this chapter follow this framework:</p>
<ul>
<li>Simple linear regression using OLS</li>
<li>Coefficients of correlation and determination</li>
<li>Required model assumptions</li>
<li>Test for significance</li>
<li>Handling model errors</li>
<li>Validating models</li>
</ul>
<h1 id="_idParaDest-102"><a id="_idTextAnchor106"/>Simple linear regression using OLS</h1>
<p>We will study<a id="_idIndexMarker459"/> one <a id="_idIndexMarker460"/>of the simplest machine learning models – simple linear regression. We will provide its overview within the context of OLS, where the objective is to minimize the sum of the square of errors. It is a straightforward concept related to a dependent variable (quantitative response) y and its independent variable x, where their relationship can be drawn as a straight line, approximately. Mathematically, a simple linear regression model can be written in the following form:</p>
<p>y = β 0 + β 1 x + ϵ</p>
<p>Here, β 0 is the intercept term and β 1 is the slope of the linear model. The error term is denoted as ϵ in the preceding linear model. We can see that in an ideal case where the error term is zero, β 0 represents the value of the dependent variable y at x = 0. Within the range of the independent variable x, β 1 represents the increase in the outcome y corresponding to a unit change in x. In literature, the independent variable x can be called the explanatory variable, predictor, input, or feature and the dependent variable y can be called the response variable, output, or target. The question is raised, “If we have a sample from a dataset (x i, y i) with i = 1,2, … , n points), how do we determine a line of best fit using the intercept term and the slope?” By estimating these terms, we will fit the best line through the data to show the linear relationship between the independent variable and the dependent variable.</p>
<div><div><img alt="Figure 6.1 – The relationship between x and y – Line of best fit" height="568" src="img/B18945_06_001.jpg" width="712"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – The relationship between x and y – Line of best fit</p>
<p>In the preceding plot example, the blue points are actual values representing the relationship between <a id="_idIndexMarker461"/>the<a id="_idIndexMarker462"/> independent variable x and the dependent variable y. The red line is the line of best fit through all these data points. Our objective now is to formulate the predicted line (the line of best fit):</p>
<p> ˆ y  = ˆ β 0 + ˆ β 1x</p>
<p>Take a closer look at the relationship between the predicted values and the true values. In order to find the line of best fit, we will minimize the vertical distances, meaning that we will minimize the errors between the points and the line through the data. In other words, we will minimize the sum of square errors, which is computed by the following:</p>
<p>Σ i=1 n  e i 2</p>
<div><div><img alt="Figure 6.2 – Errors between data points and the line through the data" height="682" src="img/B18945_06_002.jpg" width="962"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Errors between data points and the line through the data</p>
<p>Here,</p>
<p>e i = y i − ˆ y i,</p>
<p>where y i is the observed value and ˆ y i is the predicted value of the response variable y for the i th observation. The sum of the square of errors is given by:</p>
<p>S = Σ i=1 n  (y i − ˆ β 0 − ˆ β 1 x i) 2</p>
<p>To minimize S, we will take the partial derivatives with respect to ˆ β 0 and ˆ β 1. Then, we see:</p>
<p> ∂ S _ ∂ ˆ β 0  = − 2Σ(y i − ˆ β 0 − ˆ β 1 x i) = 0,</p>
<p> ∂ S _ ∂ ˆ β 1  = − 2Σ x i(y i − ˆ β 0 − ˆ β 1 x i) = 0.</p>
<p>Therefore, it is easy to see that:</p>
<p>nˆ β 0 + (Σ i=1 n  x i)ˆ β 1 = Σ i=1 n  y i,</p>
<p>(Σ i=1 n  x i)ˆ β 0 + (Σ i=1 n  x i 2)ˆ β 1 = Σ i=1 n  x i y i,</p>
<p>This implies:</p>
<p>ˆ β 0 =  Σ i=1 n  y i _ n  − ˆ β 1  Σ i=1 n  x i _ n ,</p>
<p> Σ i=1 n  x i Σ i=1 n  y i _ n  −  (Σ i=1 n  x i) 2 _ n ˆ β 1 + (Σ i=1 n  x i 2)ˆ β 1 = Σ i=1 n  x i y i.</p>
<p>Because  _ y  = Σ i=1 n  y i _ n    and  _ x  = Σ i=1 n  x i _ n , we can rewrite the slope and intercept as follows:</p>
<p>ˆ β 0 =  _ y  − ˆ β 1 _ x ,</p>
<p>ˆ β 1 =  Σ(x i −  _ x )(y i −  _ y )  ____________  Σ (x i −  _ x ) 2 .</p>
<p>In Python, we can <a id="_idIndexMarker463"/>implement<a id="_idIndexMarker464"/> the following code to find ˆ β 0 and ˆ β 1 as follows:</p>
<pre class="source-code">
def least_squares_method(x,y):
    x_mean=x.mean()
    y_mean=y.mean()
    beta1 = ((x-x_mean)*(y-y_mean)).sum(axis=0)/ ((x-x.mean())**2).sum(axis=0)
    beta0 = y_mean-(beta1*x_mean)
    return beta0, beta1</pre>
<h1 id="_idParaDest-103"><a id="_idTextAnchor107"/>Coefficients of correlation and determination</h1>
<p>In this section, we will discuss two related notions – coefficients of correlation and coefficients of determination.</p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor108"/>Coefficients of correlation</h2>
<p>A coefficient of <a id="_idIndexMarker465"/>correlation is a measure of the statistical linear relationship between two variables and can be computed using the following formula:</p>
<p>r =  1 _ n − 1 Σ i=1 n (x i −  ‾ x  _ s x )(y i −  ‾ y  _ s y )</p>
<p>The reader can go here – <a href="https://shiny.rit.albany.edu/stat/corrsim/">https://shiny.rit.albany.edu/stat/corrsim/</a> – to simulate the correlation relationship between two variables.</p>
<div><div><img alt="Figure 6.3 – Simulated bivariate distribution" height="1665" src="img/B18945_06_003.jpg" width="1643"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Simulated bivariate distribution</p>
<p>By observing the scatter plots, we can see the direction and the strength of the linear relationship between the two variables and their outliers. If the direction is positive (r&gt;0), then both<a id="_idIndexMarker466"/> variables increase or decrease together.</p>
<div><div><img alt="Figure 6.4 – Simulated bivariate distribution" height="418" src="img/B18945_06_004.jpg" width="418"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Simulated bivariate distribution</p>
<p>If the direction is negative, then one variable increases while the other decreases. We illustrate this in the following scatter plot:</p>
<div><div><img alt="Figure 6.5 – Simulated bivariate distribution" height="529" src="img/B18945_06_005.jpg" width="529"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Simulated bivariate distribution</p>
<p>The coefficient of correlation exists as a value between -1 and 1. When r =1, we have a perfect positive correlation, and when r = -1, we have a perfect negative correlation. Observe that the<a id="_idIndexMarker467"/> value of the coefficient of correlation does not change if, for example, the variable x and the variable y are switched or if all values of either variable are linearly scaled. In general, correlation does not imply causation.</p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor109"/>Coefficients of determination</h2>
<p>The coefficient of <a id="_idIndexMarker468"/>determination is just r 2, where r is the coefficient of correlation, which is a proportion of the variation in the variable y, explained by the variable x. The value of r 2 is between 0 and 1, with the linear relationship between the two variables becoming stronger as its value approaches 1. The value of r 2 can also be computed using the following formula:</p>
<p>r 2 = 1 −  S S Res _ S S Tot </p>
<p>Here, S S Res is the sum of the square of errors and S S Totis the total sum of the square of the difference between the actual value and the average value. In <code>statsmodels</code>, from the output of the OLS <a id="_idIndexMarker469"/>regression results, the value of determination can be obtained. This output is discussed in the next sections of this chapter.</p>
<h1 id="_idParaDest-106"><a id="_idTextAnchor110"/>Required model assumptions</h1>
<p>Like the parametric <a id="_idIndexMarker470"/>tests we discussed in <a href="B18945_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a><em class="italic">, Parametric Tests</em>, linear regression is a parametric method and requires certain assumptions to be met for the results to be valid. For linear regression, there are four assumptions:</p>
<ul>
<li>A linear relationship between variables</li>
<li>The normality of the residuals</li>
<li>The homoscedasticity of the residuals</li>
<li>Independent samples</li>
</ul>
<p>Let’s discuss each of these assumptions individually.</p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor111"/>A linear relationship between the variables</h2>
<p>When thinking about fitting a linear model to data, our first consideration should be whether the model is appropriate for the data. When working with two variables, the relationship between the variables should be assessed with a scatter plot. Let’s look at an example. Three scatter plots are shown in <em class="italic">Figure 6</em><em class="italic">.6</em>. The data is plotted, and the actual function used to generate the data is drawn over the data points. The leftmost plot shows data exhibiting a linear relationship. The middle plot shows data exhibiting a quadratic relationship. The rightmost plot shows two uncorrelated variables.</p>
<div><div><img alt="Figure 6.6 – Three scatter plots depicting three distinct relationships between two variables" height="332" src="img/B18945_06_006.jpg" width="1035"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Three scatter plots depicting three distinct relationships between two variables</p>
<p>Of the example data in <em class="italic">Figure 6</em><em class="italic">.6</em>, a linear model would only be appropriate for the leftmost data. In this example, we have the benefit of knowing the actual relationship of the two variables, but, in real problems, you will have to determine whether a linear model is appropriate. Once we have assessed whether a linear model is appropriate and fitted a linear model, we can assess the two assumptions that depend on the model residuals.</p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor112"/>Normality of the residuals</h2>
<p>In the previous two <a id="_idIndexMarker471"/>chapters, we discussed normality at length and looked at several examples of data that are normally distributed and several examples that were not normally distributed. In this case, we are making the same assessment, but it is for the model residuals rather than for the variables. In fact, unlike the previous parametric methods, the <em class="italic">variables themselves do not need to be normally distributed, only the residuals</em>. Let’s look at an example. We will fit a linear model on two variables, each of which exhibits a skewed distribution. As can be seen in <em class="italic">Figure 6</em><em class="italic">.7</em>, even though each variable has a skewed distribution, the variables are linearly correlated.</p>
<div><div><img alt="Figure 6.7 – Distributions of X, Y, and their scatter plots" height="359" src="img/B18945_06_007.jpg" width="1030"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Distributions of X, Y, and their scatter plots</p>
<p>Now, let’s look at the residuals from the model fit. The residuals from the model fit are shown in <em class="italic">Figure 6</em><em class="italic">.8</em>. As we expect, the residuals from this model fit appear to be normally distributed. This model appears to meet the assumption of normally distributed residuals.</p>
<div><div><img alt="Figure 6.8 – Residuals from model fit" height="290" src="img/B18945_06_008.jpg" width="429"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Residuals from model fit</p>
<p>With the assumption of normally distributed residuals verified, let’s move on to the assumption of homoscedasticity of the residuals.</p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor113"/>Homoscedasticity of the residuals</h2>
<p>Not only should the<a id="_idIndexMarker472"/> residuals be normally distributed, but the residuals should also have constant variance over the linear model. <strong class="bold">Homoscedasticity</strong> refers to<a id="_idIndexMarker473"/> the residuals having equal scatter. Conversely, residuals that do exhibit equal spread are said to be <strong class="bold">heteroscedastic</strong>. Homoscedasticity is <a id="_idIndexMarker474"/>generally assessed by plotting the residuals against the model predictions with a scatter plot. The residuals should appear to be randomly distributed with a mean of 0 and equally dispersed along the <em class="italic">x</em> axis. <em class="italic">Figure 6</em><em class="italic">.9</em> shows a scatter plot of the model residuals against the predicted value of <em class="italic">Y</em>. These residuals appear to exhibit homoscedasticity.</p>
<div><div><img alt="Figure 6.9 – Scatter plot of model residuals versus prediction" height="326" src="img/B18945_06_009.jpg" width="487"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Scatter plot of model residuals versus prediction</p>
<p>There are a few common ways the homoscedasticity of the residuals could be violated:</p>
<ul>
<li>The residuals show a systematic change in variance</li>
<li>There is an extreme outlier</li>
</ul>
<p>Two examples of these violations are shown in <em class="italic">Figure 6</em><em class="italic">.10</em>. The left plot shows an extreme outlier, and the right plot shows non-constant variance in the residuals. In both cases, the model assumptions are violated.</p>
<div><div><img alt="Figure 6.10 – Example residuals that exhibit poor behavior" height="358" src="img/B18945_06_010.jpg" width="1032"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – Example residuals that exhibit poor behavior</p>
<p>When the residuals show a systematic pattern, it may be an indication that another type of model would be more appropriate, or it may be helpful to apply a transformation to one or both <a id="_idIndexMarker475"/>variables. When an extreme outlier is present, it may be worth verifying and investigating the data point to ensure it should be included in the analysis. In the next section, we will discuss how extreme outliers impact the model fit.</p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor114"/>Sample independence</h2>
<p>We have discussed sample independence in previous chapters. There are no graphics or statistics that can be used to determine whether samples are independent. <em class="italic">Assessing sample independence requires careful analysis of the sampling method and the populations from which the samples are drawn</em>. For example, a common type of data that violates the independence assumption is time-series data. Time-series data is a type of data that is sampled over time, making it serially correlated. We will discuss analysis methods for time-series data in later chapters.</p>
<p>In this section, we discussed the linear regression model assumptions. In the next section, we will discuss how to validate a linear model, which will again utilize the model residuals.</p>
<h1 id="_idParaDest-111"><a id="_idTextAnchor115"/>Testing for significance and validating models</h1>
<p>Up to this point in the chapter, we have discussed the concepts of the OLS approach to linear regression modeling; the coefficients in a linear model; the coefficients of correlation and determination; and the assumptions required for modeling with linear regression. We will now begin our discussion on testing for significance and model validation.</p>
<h3>Testing for significance</h3>
<p>To test for <a id="_idIndexMarker476"/>significance, let us load <code>statsmodels</code> macrodata data set so we can build a model that tests the relationship between real gross private domestic investment, <code>realinv</code>, and real private disposable income, <code>realdpi</code>:</p>
<pre class="source-code">
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
from statsmodels.nonparametric.smoothers_lowess import lowess
df = sm.datasets.macrodata.load().data</pre>
<p>Least squares regression requires a constant coefficient in order to derive the <code>statsmodels’</code> <code>add_constant</code> function here:</p>
<pre class="source-code">
df = sm.add_constant(df, prepend=False)
df_mod = df[['realinv','realdpi','const']]</pre>
<p>The input data does not need to be normally distributed for least squares regression. However, it is assumed that the residuals of the model are normally distributed. However, it is useful to see the spread of the data to get an understanding of their statistics, such as the mean, median, and range, and whether there are outliers present. If the residual analysis after the modeling suggests there may be some issues, having inspected the model variables will help the analyst understand potential root causes.</p>
<div><div><img alt="Figure 6.11 – Visualizing the distributions of the realinv and realdpi variables" height="434" src="img/B18945_06_011.jpg" width="872"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Visualizing the distributions of the <em class="italic">realinv</em> and <em class="italic">realdpi</em> variables</p>
<p>When visualizing the relationship between <code>realinv</code> and <code>realdpi</code>, we can see a strong linear relationship, but also a potential serial correlation in the data as there appears to be a <a id="_idIndexMarker477"/>somewhat <strong class="bold">cyclical oscillation</strong> in the data, which becomes <a id="_idIndexMarker478"/>stronger toward the extreme values, seen in the upper right of the plot.</p>
<div><div><img alt="Figure 6.12 – Visualizing the relationship between the realinv and realdpi variables" height="627" src="img/B18945_06_012.jpg" width="894"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Visualizing the relationship between the <em class="italic">realinv</em> and <em class="italic">realdpi</em> variables</p>
<p>We will analyze the potential serial correlation seen in the preceding plot after a few more steps. First, let us fit the input variable and coefficient to the model:</p>
<pre class="source-code">
ols_model = sm.OLS(df_mod['realdpi'], df_mod[['const','realinv']])
compiled_model = ols_model.fit()</pre>
<p>Next, we will want to print out a summary of the model performance so we can begin to gauge the significance of the terms involved:</p>
<pre class="source-code">
print(compiled_model.summary())</pre>
<p>Referring to the results in <em class="italic">Figure 6</em><em class="italic">.13</em>, we can see that the <code>const</code>, nor the input variable, <code>realinv</code>, contains zero. Therefore, in <a id="_idIndexMarker479"/>addition to the <strong class="bold">significant p-values</strong> for these variables, we can conclude they are significant contributors to the target at the coefficients provided. Now that we have confirmed there is statistical significance for both the intercept and the input variable and their coefficients, we want to know the level of correlation to the target they represent. We can<a id="_idIndexMarker480"/> see the <strong class="bold">R-squared</strong> statistic is 0.944. Because the two variables are the constant and the input variable, this R-squared statistic explains only the input variable.</p>
<p class="callout-heading">R-squared versus adjusted r-squared</p>
<p class="callout">For multivariate regression, we would look at the adjusted r-squared value, which we can also see is the same – in the univariate/simple regression case – as the r-squared value.</p>
<p>The <strong class="bold">correlation of determination</strong> – also called goodness of fit – of 0.944 means real gross private <a id="_idIndexMarker481"/>domestic investment explains 94.4% of the variance in real private disposable income.</p>
<p>We can see based on the table that the intercept and the <code>realinv</code> variable are both significant in predicting the target with very low p-values. We can also see that neither of the 95% confidence intervals contains 0, which backs up the relevancy of their p-values. However, the confidence interval for the constant (intercept) suggests there may be a risk of <a id="_idIndexMarker482"/>uncertainty in the model and that more variables may be needed to reduce that uncertainty and improve model performance. Stated differently, we could say a higher level of uncertainty in the constant indicates the model has variance left to be explained and that the current model is overly biased.</p>
<h3>Validating models</h3>
<h4>The Durbin-Watson test</h4>
<p>It is important<a id="_idIndexMarker483"/> to acknowledge that based on the fact that data was observed over a 50-year period and random sampling was not performed, it is possible the residuals will have a <strong class="bold">serial correlation</strong>. We <a id="_idIndexMarker484"/>observe a possibility of serial correlation in<a id="_idIndexMarker485"/> the <strong class="bold">Durbin-Watson test</strong> statistic of 0.095, which suggests the residuals exhibit positive autocorrelation. The Durbin-Watson statistic tests whether there is autocorrelation at the first time lag. This means the Durbin-Watson tests whether the change in values between each data point is correlated with the value of the previous data point. If that correlation exists, it is <a id="_idIndexMarker486"/>considered <strong class="bold">lag-one autocorrelation</strong>.</p>
<p class="callout-heading">Durbin-Watson test statistic</p>
<p class="callout">As a rule <a id="_idIndexMarker487"/>of thumb, a Durbin-Watson test statistic below roughly 2.0 (many consider 1.5 to 2.0 to be reasonable) is considered a flag for positive autocorrelation, while a value above roughly 2.0 (many consider 2.0 to 2.5 to be reasonable) indicates possible negative autocorrelation. Procedurally, a table lookup is required to identify the critical values against which the test statistic should be measured. The process follows the same steps for any hypothesis test with respect to comparing the test statistic against the critical value. The Durbin-Watson test is typically interpreted as a one-tailed test as inference from a two-tailed test is not particularly useful. A version of the Durbin-Watson table can be found at <a href="https://www.real-statistics.com/statistics-tables/durbin-watson-table/">https://www.real-statistics.com/statistics-tables/durbin-watson-table/</a>. This table produces the critical<a id="_idIndexMarker488"/> values for <strong class="bold">positive autocorrelation</strong>. To find the critical<a id="_idIndexMarker489"/> values for <strong class="bold">negative autocorrelation</strong>, the positive critical values can be subtracted from 4.</p>
<p>Although we have a Durbin-Watson statistic that suggests serial correlation may be present in the data, we will confirm significance by comparing the Durbin-Watson statistic to its corresponding positive autocorrelation <code>statsmodels</code>’ OLS regression, we find the bounds for the <code>k=1</code> input variable to be <code>[1.758, 1.779]</code>. A value within this range should be interpreted as <strong class="bold">inconclusive</strong>. A value less than the lower bound indicates the presence of enough evidence to reject the null hypothesis and thus conclude there is statistically significant positive <a id="_idIndexMarker490"/>autocorrelation. A value greater than the upper bound indicates there is not enough evidence to reject the null hypothesis and thus there is no autocorrelation. Based on the results of our model, we can conclude with a 95% level of confidence that there is evidence of positive serial correlation in our residuals.</p>
<div><div><img alt="Figure 6.13 – OLS regression model output for realdpi" height="832" src="img/B18945_06_013.jpg" width="624"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – OLS regression model output for <em class="italic">realdpi</em></p>
<p>At this point, an analyst may want to consider addressing the serial correlation before moving forward. However, for the purpose of following the process of end-to-end model validation, we<a id="_idIndexMarker491"/> will move forward to the next steps.</p>
<h4>Diagnostic plots for analyzing model errors</h4>
<p>As mentioned, y i = β 0 + β 1 x i + e i, where in regression, β 0 + β 1 x i is the predicted mean for y i given x i and e i is<a id="_idIndexMarker492"/> the error term – also called the residual term, which is calculated as the observed value for a data point minus the predicted value for that data point. The residuals in least squares linear regression must approximate a normal distribution centered around the mean with a standard deviation. The method for deriving the residuals for a fitted model is to subtract the predicted value from the actual value for each observation. Four common visualizations for inspecting the fit of a least squares regression model are as follows:</p>
<ul>
<li><strong class="bold">Residuals versus fitted plots</strong>, used to inspect the requirement of a linear relationship between the input and target variables</li>
<li><strong class="bold">Quantile plots</strong>, used to inspect the assumption of residuals being normally distributed</li>
<li><strong class="bold">Scale-location plots</strong>, used to inspect the assumption of homoscedasticity, or the homogenous distribution of variation</li>
<li><strong class="bold">Residuals versus leverage influence plots</strong>, which help identify the impact outliers may have on model fit</li>
</ul>
<p>To produce these plots, we can use the following Python code implementation:</p>
<pre class="source-code">
model_residuals = compiled_model.resid
fitted_value = compiled_model.fittedvalues
standardized_residuals = compiled_model.resid_pearson # Residuals, normalized to have unit variance.
sqrt_standardized_residuals = np.sqrt(np.abs(compiled_model.get_influence().resid_studentized_internal))
influence = compiled_model.get_influence()
leverage = influence.hat_matrix_diag
cooks_distance = compiled_model.get_influence().cooks_distance[0]
fig, ax = plt.subplots(2, 2, figsize=(10,8))
# Residuals vs. Fitted
ax[0, 0].set_xlabel('Fitted Values')
ax[0, 0].set_ylabel('Residuals')
ax[0, 0].set_title('Residuals vs. Fitted')
locally_weighted_line1 = lowess(model_residuals, fitted_value)
sns.scatterplot(x=fitted_value, y=model_residuals, ax=ax[0, 0])
ax[0, 0].axhline(y=0, color='grey', linestyle='--')
ax[0,0].plot(locally_weighted_line1[:,0], locally_weighted_line1[:,1], color = 'red')
# Normal Q-Q
ax[0, 1].set_title('Normal Q-Q')
sm.qqplot(model_residuals, fit=True, line='45',ax=ax[0, 1], c='blue')
# Scale-Location
ax[1, 0].set_xlabel('Fitted Values')
ax[1, 0].set_ylabel('Square Root of Standardized Residuals')
ax[1, 0].set_title('Scale-Location')
locally_weighted_line2 = lowess(sqrt_standardized_residuals, fitted_value)
sns.scatterplot(x=fitted_value, y=sqrt_standardized_residuals, ax=ax[1, 0])
ax[1,0].plot(locally_weighted_line2[:,0], locally_weighted_line2[:,1], color = 'red')
# Residual vs. Leverage Influence
ax[1, 1].set_xlabel('Leverage')
ax[1, 1].set_ylabel('Standardized Residuals')
ax[1, 1].set_title('Residuals vs. Leverage Influence')
locally_weighted_line3 = lowess(standardized_residuals, leverage)
sns.scatterplot(x=leverage, y=standardized_residuals, ax=ax[1, 1])
ax[1, 1].plot(locally_weighted_line3[:,0], locally_weighted_line3[:,1], color = 'red')
ax[1, 1].axhline(y=0, color='grey', linestyle='--')
ax[1, 1].axhline(3, color='orange', linestyle='--', label='Outlier Demarkation')
ax[1, 1].axhline(-3, color='orange', linestyle='--')
ax[1, 1].legend(loc='upper right')
leverages = []
for i in range(len(cooks_distance)):
    if cooks_distance[i] &gt; 0.5:
        leverages.append(leverage[i])
        ax[1, 1].annotate(str(i) + " Cook's D &gt; 0.5",xy=(leverage[i], standardized_residuals[i]))
if leverages:
    ax[1, 1].axvline(min(leverages), color='red', linestyle='--', label="Cook's Distance")
for i in range(len(standardized_residuals)):
    if standardized_residuals[i] &gt; 3 or standardized_residuals[i] &lt; -3:
        ax[1, 1].annotate(i,xy=(leverage[i], standardized_residuals[i]))
fig.tight_layout()</pre>
<p>The following <a id="_idIndexMarker493"/>output contains four common diagnostic plots.</p>
<div><div><img alt="Figure 6.14 – Linear regression diagnostic plots" height="776" src="img/B18945_06_014.jpg" width="975"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – Linear regression diagnostic plots</p>
<h4>Residuals versus fitted</h4>
<p>One requirement <a id="_idIndexMarker494"/>of least squares regression is a <strong class="bold">linear relationship</strong> between <a id="_idIndexMarker495"/>the input and target variables. Suffice it <a id="_idIndexMarker496"/>to say a strongly correlated relationship exists between the input and target variables. This plot helps the analyst assess the linearity between the two.</p>
<p>We should expect the fitted line – representing the model – to be horizontal, representing the strength of the linear relationship between the input and target variable across all data points. The residuals should be roughly evenly spread around the line in reasonable proximity. If these attributes cannot be confirmed, a non-linear relationship may exist between the two variables. A transformation of at least one of the variables – if the variable is skewed – may help resolve this type of issue. However, it could also be that a non-parametric model is more appropriate, which is often the case when categorical features – or encoded categorical features – are present.</p>
<p>Our model produced a plot that does not display evenly spread residuals forming around a horizontal line. Rather, the line is concaved down, which suggests a polynomial or non-parametric regression may be more appropriate. Additionally, the residuals form a pattern toward the right end of the line that suggests there may be some serial correlation in the data as it<a id="_idIndexMarker497"/> exhibits some sinusoidal behavior. We were able to identify the presence of serial correlation using the Durbin-Watson statistic. This is likely due to the fact the data was not randomly sampled and was collected over a long period of time.</p>
<h4>QQ plot of residuals</h4>
<p>Assessment of <a id="_idIndexMarker498"/>normality for the QQ plot follows the same procedure <a id="_idIndexMarker499"/>as assessing normality using the QQ plot in <a href="B18945_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a><em class="italic">, Parametric Tests</em>. The primary purpose of this plot is to assess the required assumption <a id="_idIndexMarker500"/>of the <strong class="bold">normal distribution of errors</strong>. Minor deviation of the residuals from the 45-degree line in the plot is normal, but extreme skewness or deviation from the line may indicate a poor fit. Such a scenario could indicate poor statistical power. In the event of skewness, data transformation – such as a log transformation – could be useful to resolve the issue. Dropping outliers may be another suitable solution. However, rather than dropping outliers outright, it is advisable to ensure there was not an error in data handling that could be resolved to make the outliers conform.</p>
<p>In the QQ plot for our model’s residuals, we can see some moderate left-side skewness and extreme right-side skewness, which suggests the three values in the right tail could be extreme <a id="_idIndexMarker501"/>outliers. Aside from this, we can assume that overall, the residual error is approximately normally distributed.</p>
<h4>Scale-location</h4>
<p>The scale-location plot <a id="_idIndexMarker502"/>of the square root of the<a id="_idIndexMarker503"/> standardized residuals helps analysts use visual inspection to determine whether <a id="_idIndexMarker504"/>there is <strong class="bold">homoscedasticity</strong> – or <em class="italic">non-constant</em> variance – of the residuals, which is required for a least squares regression model. The plot visualizes the square root of the absolute value of the standardized residuals. Analyzing homoscedasticity can sometimes be useful for identifying potential issues related to <strong class="bold">sample independence</strong>, another requirement of least-squares regression. However, the Durbin-Watson test should be given more weight for testing this assumption.</p>
<p>In assessing our model, we can see the line has some deviation from being perfectly horizontal. Additionally, the residuals, while seeming to have a large amount of constant variance, nonetheless at times exhibit patterns, which conflicts with the behavior of homoscedasticity. Furthermore, there appear to be three very notable outliers. Based on this information, we can reasonably assume a risk of heteroscedasticity – or <em class="italic">constant</em> variance – exists within the model’s residuals.</p>
<h4>Residuals versus leverage influence</h4>
<p>Assessing the <a id="_idIndexMarker505"/>residuals versus <a id="_idIndexMarker506"/>leverage influence is another approach to assessing model fit. In addition to gauging leverage, this plot also shows when <strong class="bold">Cook’s distance</strong> is beyond a reasonable level.</p>
<p><strong class="bold">Leverage</strong> is used to <a id="_idIndexMarker507"/>identify the distance of an individual point from all other points. High leverage for a residual likely means the corresponding data point is an outlier strongly influencing the model to fit less approximately to the overall data and instead give more weight to that specific value. Residuals should be between -2 and 2 to not be considered potential outliers. Values between +/-2.5 to 3 suggest data points are extreme outliers. Overall, this plot is useful for separating outliers that have no significant negative impact on the model from the ones that do.</p>
<p><strong class="bold">Cook’s distance</strong> is a measurement<a id="_idIndexMarker508"/> that uses both an observation’s leverage and its residual value. As the leverage is higher, its calculated Cook’s distance is higher. In general, a Cook’s distance value greater than 0.5 means a residual has leverage that is negatively impacting the model through over-representation – essentially, an outlier. Cook’s distance is included in the code and flags the residuals that have distances greater than 0.5. Cook’s distance is particularly useful because removing outliers may not have a significant impact on the model. However, if the outlier has a high Cook’s distance value, that is a strong indication that resolving the outlier <strong class="bold">will benefit</strong> the model. Cook’s distance is calculated for each data point by removing it from the model and calculating the difference in error divided by the mean squared error multiplied by the number of model coefficients plus one. Its formulation follows:</p>
<p>D i =  ∑ j=1 n  (ŷ j − ŷ j(i)) 2  ___________  (p + 1)σ ̂ 2 </p>
<p>where ŷ j is the <em class="italic">j</em>th estimate of the response and ŷ j(i) is the value of the <em class="italic">j</em>th fitted value with the <em class="italic">i</em>th value removed. <em class="italic">p</em> is the number of coefficients in the model and σ ̂ 2 is the variance of the residuals for all observations, including those removed, also called the squared error.</p>
<p>In the plot for our model, we can observe three extreme outliers. However, we can also see no value of Cook’s<a id="_idIndexMarker509"/> distance is greater than 0.5. As with the scale-location plot, the standardized<a id="_idIndexMarker510"/> residuals should follow even distribution around a smoothed line that is horizontal, which indicates a constant variance. We can see from our model’s output that this is not the case.</p>
<h3>Addressing issues with residuals</h3>
<p>Alone, each plot can <a id="_idIndexMarker511"/>be considered enough to negate <a id="_idIndexMarker512"/>the validity of a model. However, it is useful to consider all plots since a model may still be useful if there is only some deviance from expectations in the diagnostic plots. Some approaches to resolving issues with residuals are as follows:</p>
<ul>
<li>Investigating and resolving any data collection issues that may produce outliers or data inconsistencies</li>
<li>Removing outliers outright</li>
<li>Improving the sampling approach using methods such as using random or stratified sampling</li>
<li>Performing data transformations, such as a log or square root transformation</li>
<li>Including <a id="_idIndexMarker513"/>additional input variables that can <a id="_idIndexMarker514"/>help explain the target variable or any serial correlation that may be present</li>
</ul>
<h3>Handling serial correlation</h3>
<p>If uncertain <a id="_idIndexMarker515"/>of the presence of serial correlation in the residuals, a useful next step is to analyze a <strong class="bold">Partial Autocorrelation Function</strong> (<strong class="bold">PACF</strong>) plot to <a id="_idIndexMarker516"/>assess whether serial correlation exists in the model at a significant level, which could explain issues with the model’s residuals.</p>
<p class="callout-heading">ACF and PACF</p>
<p class="callout">The <strong class="bold">PACF</strong> provides<a id="_idIndexMarker517"/> a partial correlation between the value of a point and previous points, called lags, by controlling for the lags between. Controlling the lags between is where the term “partial” comes from. If a specific point at lag zero in a dataset is correlated strongly with the value of the third lagging point, there would be a significant correlation with a lag of 3, but not a lag of 2 or 1. In time-series modeling, the PACF is used to directly derive the autoregressive orders, denoted as <em class="italic">AR(n)</em> where <em class="italic">n</em> is the lag. The<strong class="bold"> Autocorrelation Function</strong> (<strong class="bold">ACF</strong>), which<a id="_idIndexMarker518"/> is used for determining moving average orders, denoted as <em class="italic">MA(n),</em> considers the correlation between data points at lag zero and all previous lagging points without controlling for the relationships between them. Stated alternatively, autocorrelation in an ACF plot at lag 3 will provide autocorrelation across points 0 through 3 whereas correlation in a PACF plot at lag 3 will provide autocorrelation only between point 0 and point 3, excluding the impact of values at lags 1 and 2.</p>
<p>We can execute the following code to generate the PACF plot with a 95% confidence interval using the most recent 50 data points, ordered by index positioning – in this case, by date:</p>
<pre class="source-code">
from statsmodels.graphics.tsaplots import plot_pacf
plot_pacf(compiled_model.resid, alpha=0.05, lags=50);</pre>
<div><div><img alt="Figure 6.15 – Partial autocorrelation plot for the model residuals" height="373" src="img/B18945_06_015.jpg" width="566"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.15 – Partial autocorrelation plot for the model residuals</p>
<p>In the PACF plot in <em class="italic">Figure 6</em><em class="italic">.15</em>, we see very strong and significant correlations at lags 0 and 1. We observe a low level of significant correlation (approximately -0.25) for lags 2 and 36, extending beyond the 95% confidence interval. What this tells us is we might be better off modeling the<a id="_idIndexMarker519"/> dataset with a first-order autoregressive (<strong class="bold">AR(1)</strong>) time-series model than a linear regression model.</p>
<p>Answering regression questions with a least squares method when there is a presence of significant autocorrelation creates inherit risk. There are methods to adjust the results of a model when there is autocorrelation detected, such as taking a <strong class="bold">first-order difference</strong> – or another type of low-pass filter - or applying the Cochrane-Orcutt, Hildreth-Lu, or Yule-Walker adjustments. However, it is ideal to perform a time-series analysis, as the results of that type of modeling are more robust to time-dependent patterns. We will introduce time series in <a href="B18945_10.xhtml#_idTextAnchor160"><em class="italic">Chapter 10</em></a>, <em class="italic">Introduction to Time Series</em>, where we discuss simple and complex methods for approximating time series to produce more useful results.</p>
<p>Let us consider a first-order difference of the data. Regarding <em class="italic">Figure 6</em><em class="italic">.16</em>, we first look at the ACF plot and original data for the input variable, <code>realdpi</code>. We can see an exponentially growing line (original data) that displays an exponentially dampening ACF plot. This ACF behavior is typical for <strong class="bold">trended</strong> data that often benefits from <strong class="bold">first-order differencing</strong>. The low autoregressive ordering identified in the PACF indicates this may be sufficient for a regression model to proceed. Had the ACF shown different behavior, such as sinusoidal (seasonal) autocorrelation, or the PACF shown more partial autocorrelation, we would have evidence against a first-order difference being a sufficient solution for resolving serial correlation and proceeding with least squares regression. ACFs and PACFs for least squares regression should ideally have no significantly correlated lags. However, when the ACF exhibits this behavior and the PACF shows no significance, or when the level of correlation is very low, regression may perform well. It is preferable to see no patterns or peaks in either plot. Regardless, in either case, model error should be assessed, in addition to the autocorrelation of the residuals, to determine if least squares regression performs as needed.</p>
<p>Using the <code>diff()</code> <code>numpy</code> function, we take a first-order, low-pass difference and plot the ACFs, PACFs, and line plots for the original and differenced data. We can see a significant reduction in autocorrelation as a result. As an aside, the post-differencing autocorrelation suggests<a id="_idIndexMarker520"/> an <strong class="bold">autoregressive moving average </strong>(<strong class="bold">ARMA</strong>) model of AR order 1 and MA order 1 may be better suited than least-squares regression. However, we will ignore this for <a id="_idIndexMarker521"/>the chapter and continue:</p>
<pre class="source-code">
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
fig, ax = plt.subplots(2,3, figsize=(15,10))
plot_acf(df_mod['realdpi'], alpha=0.05, lags=50, ax=ax[0,0])
ax[0,0].set_title('Original ACF')
plot_pacf(df_mod['realdpi'], alpha=0.05, lags=50, ax=ax[0,1])
ax[0,1].set_title('Original PACF')
ax[0,2].set_title('Original Data')
ax[0,2].plot(df_mod['realdpi'])
plot_acf(np.diff(df_mod['realdpi'], n=1), alpha=0.05, lags=50, ax=ax[1,0])
ax[1,0].set_title('Once-Differenced ACF')
plot_pacf(np.diff(df_mod['realdpi'], n=1), alpha=0.05, lags=50, ax=ax[1,1])
ax[1,1].set_title('Once-Differenced PACF')
ax[1,2].set_title('Once-Differenced Data')
ax[1,2].plot(np.diff(df_mod['realdpi'], n=1))</pre>
<p>We get the following result:</p>
<div><div><img alt="Figure 6.16 – Plots for the realinv variable before and after first-order differencing" height="728" src="img/B18945_06_016.jpg" width="1100"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.16 – Plots for the <em class="italic">realinv</em> variable before and after first-order differencing</p>
<p>This gives us data<a id="_idIndexMarker522"/> that we may more suitably model using regression. However, when using differencing as a method to resolve serial correlation in a least squares regression model, we must also differentiate the input variable. Observe the input variable, <code>realinv</code>, before and after transformation. Here in <em class="italic">Figure 6</em><em class="italic">.17</em>, we can see similar behavior as with <code>realdpi</code>:</p>
<div><div><img alt="Figure 6.17 – Plots for the realdpi variable before and after first-order differencing" height="800" src="img/B18945_06_017.jpg" width="1207"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.17 – Plots for the <em class="italic">realdpi</em> variable before and after first-order differencing</p>
<p>We can now build <a id="_idIndexMarker523"/>a new regression model using the differenced data. It is important to note first-order differencing removed one data point from the dataset so one value must also be removed from the constant in the design matrix:</p>
<pre class="source-code">
ols_model_1diff = sm.OLS(np.diff(df_mod['realdpi'], n=1), pd.concat([df_mod['const'].iloc[:-1], pd.Series(np.diff(df_mod['realinv'], n=1))], axis=1))
compiled_model_1diff = ols_model_1diff.fit()</pre>
<p>The output of this model is notably different. We now see an r-squared value of <code>0.045</code>. Looking at the differenced data, we can see there is a different variance in the differenced data for disposable income than there is for investment. This means that although the <code>realinv</code> variable is useful for predicting <code>realdpi</code> as it appears significant, there is a lot more influencing real disposable gross personal income. We observe that the new Durbin-Watson test statistic is now <code>2.487</code>. Using the critical values from the table lookup earlier, <code>[1.758, 1.779]</code>, we see the Durbin-Watson statistic is now above the upper limit. <code>[2.221, 2.242]</code>. Because the new statistic, <code>2.487</code>, is greater than <code>2.221</code>, we can confirm there is now negative autocorrelation in the model’s residuals. We can see based on the PACF plot, as well as the proximity to the critical value range, the autocorrelation is much less significant than before, but still present. It could be argued the autocorrelation of the residuals is now small enough to be considered resolved, however; as noted earlier, a Durbin-Watson statistic less than 2.5 can be considered normal. Nonetheless, least-squares regression remains risky compared to time-series modeling to generate our predictions. We begin an in-depth overview <a id="_idIndexMarker524"/>of this topic in <a href="B18945_10.xhtml#_idTextAnchor160"><em class="italic">Chapter 10</em></a><em class="italic">, Introduction to </em><em class="italic">Time Series</em>.</p>
<p>Let us assume, however, that for the sake of moving through the steps of regression model validation, we have no autocorrelation in our data. The next step will be to test our model.</p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor116"/>Model validation</h2>
<p>Assuming an analyst <a id="_idIndexMarker525"/>has developed a model that produces strong results and appears useful based on the previously mentioned plots, the next step is to test the model. One conventional approach is to perform a train-and-test split on the data used for the original model where we fit all data. In the following code, we run a split with the train dataset having 75% of the data and the test dataset having 25%. The purpose is to assess whether there is a significant difference in performance between the two, as well as compared to the original model. The process follows the same steps as earlier. Acceptable differences are up to the analyst to decide, but in addition to assessing the differences between the metrics already discussed, the analyst should also note the coefficients for the slope and input variable, as these will be used to predict the target.</p>
<p>Here, we split the data. We use the <code>shuffle</code> argument to <em class="italic">randomly</em> shuffle the data so that if there is some order to the data, such as it being ordered by time, the data will be split randomly:</p>
<pre class="source-code">
from sklearn.model_selection import train_test_split
train, test = train_test_split(df_mod, train_size=0.75, shuffle=True)</pre>
<p>Now, we build a model using the training data:</p>
<pre class="source-code">
ols_model_train = sm.OLS(train['realdpi'], train[['const','realinv']])
compiled_model_train = ols_model_train.fit()
print(compiled_model_train.summary())</pre>
<p>Finally, we build a model using the testing data:</p>
<pre class="source-code">
ols_model_test = sm.OLS(test['realdpi'], test[['const','realinv']])
compiled_model_test = ols_model_test.fit()
print(compiled_model_test.summary())</pre>
<p>The idea here is that the two models should produce similar results on two different partitions of the data.</p>
<p>Another method for validating the model is to compare a metric against a naïve model. Let us use<a id="_idIndexMarker526"/> the <strong class="bold">mean absolute error</strong> (<strong class="bold">MAE</strong>). With this model metric, we could compare a naïve linear model’s errors where the error is each data point minus the average to the model’s <a id="_idIndexMarker527"/>predictions. A useful model MAE would need to be lower than the naïve model’s MAE. Here, we use the fit model to predict the inputs, then compare the model’s predictions to the naïve model’s predictions:</p>
<p>Below we can observe the MAE from the <em class="italic">trained</em> model:</p>
<pre class="source-code">
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(train['realdpi'], compiled_model_train.predict(train[['const','realinv']]))
mae
#438.5872081797031</pre>
<p>Here we can see the MAE from the <em class="italic">naïve</em> model, which makes no assumptions other than that the mean will continue to hold the true values:</p>
<pre class="source-code">
import numpy as np
errors = []
for i in range(len(train)):
    errors.append(abs(train['realdpi'].iloc[i] - train['realdpi'].mean()))
np.mean(errors)
#2065.440235457064</pre>
<p>We can see the model compares favorably against the naïve model.</p>
<p>Another method that is popular to use is testing on a holdout dataset. For this, a model is constructed and trained on the training data. Then, using that model, we apply it to the test data. We would then compare the metric to that of the training data. Using the training model’s MAE of 438, we compare it to the following:</p>
<pre class="source-code">
mae_test = mean_absolute_error(test['realdpi'], compiled_model_train.predict(test[['const','realinv']]))
mae_test
#408.0253171549187</pre>
<p>We can see that the error is lower for the test data than for the train data. However, since the train and test data are assumed to have different values and thus means, we should reassess that the naïve model still produces a higher MAE for the test data:</p>
<pre class="source-code">
errors = []
for i in range(len(test)):
    errors.append(abs(test['realdpi'].iloc[i] - test['realdpi'].mean()))
np.mean(errors)
#1945.5873125720873</pre>
<p>Here we can see the model does provide better MAE than the naïve model at approximately the same <a id="_idIndexMarker528"/>rate on the test data as on the train data. Therefore, we expect that the linear regression model is better than the naïve model.</p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor117"/>Summary</h1>
<p>In this chapter, we discussed an overview of simple linear regression between one explanatory variable and one response variable. The topics we covered include the following:</p>
<ul>
<li>The OLS method for simple linear regression</li>
<li>Coefficients of correlation and determination and their calculations and significance</li>
<li>The assumptions required for least squares regression</li>
<li>Methods of analysis for model and parameter significance</li>
<li>Model validation</li>
</ul>
<p>We looked closely at the concept of the square of error and how the sum of squared errors is meaningful for building and validating linear regression models. Then, we walked through the four pertinent assumptions required to make linear regression a stable solution. After, we provided an overview of four diagnostic plots and their interpretations with respect to assessing the presence of various issues related to heteroscedasticity, linearity, outliers, and serial correlation. We then walked through an example of using the ACF and PACF to assess serial correlation and an example of using first-order differencing to remove serial correlation constraints from data and build an OLS model using the differenced data. Finally, we provided methods for testing and validating least square regression models.</p>
<p>In the next chapter, we will extend this concept to include more than one explanatory variable, a technique called multiple linear regression. We will also discuss various topics related to multiple linear regression, such as variable selection and regularization.</p>
</div>
</div></body></html>