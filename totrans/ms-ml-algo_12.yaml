- en: Generative Adversarial Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to provide a brief introduction to a family of
    generative models based on some game theory concepts. Their main peculiarity is
    an adversarial training procedure that is aimed at learning to distinguish between
    true and fake samples, driving, at the same time, another component that generates
    samples more and more similar to the training examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will be discussing:'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial training and standard** Generative Adversarial Networks** (**GANs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Convolutional GANs** (**DCGAN**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wasserstein GANs** (**WGAN**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The brilliant idea of adversarial training, proposed by Goodfellow and others
    (in *Generative Adversarial Networks*,*Goodfellow I*. *J*., *Pouget-Abadie J*.,
    *Mirza M*., *Xu B*., *Warde-Farley D*., *Ozair S*., *Courville A*., *Bengio Y*.,
    *arXiv:1406.2661* [*stat.ML*]), ushered in a new generation of generative models
    that immediately outperformed the majority of existing algorithms. All of the
    derived models are based on the same fundamental concept of adversarial training,
    which is an approach partially inspired by game theory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose that we have a data generating process, *p[data](x)*, that represents
    an actual data distribution and a finite number of samples that we suppose are
    drawn from *p[data]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/caf2bfbb-3f87-4f34-ad30-fa5c2fa2e697.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our goal is to train a model called a generator, whose distribution must be
    as close as possible to *p[data]*. This is the trickiest part of the algorithm,
    because instead of standard methods (for example, variational autoencoders), adversarial
    training is based on a minimax game between two players (we can simply say that,
    given an objective, the goal of both players is to minimize the maximum possible
    loss; but in this case, each of them works on different parameters). One player
    is the generator, we can define as a parameterized function of a noise sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20c40179-342d-4870-83c7-10c7d02f44cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The generator is fed with a noise vector (in this case, we have employed a
    uniform distribution, but there are no particular restrictions; therefore, we
    are simply going to say that *z* is drawn from a noise distribution *p[noise]*),
    and outputs a value that has the same dimensionality of the samples drawn from
    *p[data]*. Without any further control, the generator distribution will be completely
    different from the data generating process, but this is the moment for the other
    player to enter the scene. The second model is called the *discriminator* (or
    Critic), and it has the responsibility of evaluating the samples drawn from *p[data]*
    and the ones produced by the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/975272aa-e4ee-40e0-8509-6ab22347e7d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The role of this model is to output a probability that must reflect the fact
    that the sample is drawn from *p[data]*, instead of being generated by *G(z; θ[g])*.
    What happens is very simple: the first player (the generator) outputs a sample, *x*.
    If *x* actually belongs to *p[data]*, the discriminator will output a value close
    to 1, while if it''s very different from the other true samples, *D(x; θ[d])*
    will output a very low probability. The real structure of the game is based on
    the idea of training the generator to deceive the discriminator, by producing
    samples that can potentially be drawn from *p[data]*. This result can be achieved
    by trying to maximize the log-probability, *log(D(x; θ[d]**))*, when *x* is a
    true sample (drawn from *p[data]*), while minimizing the log-probability, *log(1
    - D(G(z; θ[g]); θ[d]))*, with *z* sampled from a noise distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: The first operation forces the discriminator to become more and more aware of
    the true samples (this condition is necessary to avoid being deceived too easily).
    The second objective is a little bit more complex, because the discriminator has
    to evaluate a sample that can be acceptable or not. Let's suppose that the generator
    is not smart enough, and outputs a sample that cannot belong to *p[data]*. As
    the discriminator is learning how *p[data]* is structured, it will very soon distinguish
    the wrong sample, outputting a low probability. Hence, by minimizing *log(1 -
    D(G(z; θ[g]); θ[d]))*, we are forcing the discriminator to become more and more
    critical when the samples are quite different from the ones drawn from *p[d]*[*ata*],
    and the becomes generator more and more able to produce acceptable samples. On
    the other hand, if the generator outputs a sample that belongs to the data generating
    process, the discriminator will output a high probability, and the minimization
    falls back in the previous case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors expressed this minimax game using a shared value function, *V(G,
    D)*, that must be minimized by the generator and maximized by the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c2dfbf2-2dc2-4aa2-9776-d8789f81fba1.png)'
  prefs: []
  type: TYPE_IMG
- en: This formula represents the dynamics of a non-cooperative game between two players
    (for further information, refer to *Tadelis S*., *Game Theory, Princeton University
    Press*) that theoretically admits a special configuration, called a *Nash equilibrium*,
    that can be described by saying that if the two players know each other's strategy,
    they have no reason to change their own strategy if the other player doesn't.
    In this case, both the discriminator and generator will pursue their strategies
    until no change is needed, reaching a final, stable configuration, which is potentially
    a Nash equilibrium (even if there are many factors that can prevent reaching this
    goal). A common problem is the premature convergence of the discriminator, which
    forces the gradients to vanish because the loss function becomes flat in a region
    close to 0\. As this is a game, a fundamental condition is the possibility to
    provide information to allow the player to make corrections. If the discriminator
    learns how to separate true samples from fake ones too quickly, the generator
    convergence slows down, and the player can remain trapped in a sub-optimal configuration.
    In general, when the distributions are rather complex, the discriminator is slower
    than the generator; but, in some cases, it is necessary to update the generator
    more times after each single discriminator update. Unfortunately, there are no
    rules of thumb; but, for example, when working with images, it's possible to observe
    the samples generated after a sufficiently large number of iterations. If the
    discriminator loss has become very small and the samples appear corrupted or incoherent,
    it means that the generator did not have enough time to learn the distribution,
    and it's necessary to slow down the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors in the aforementioned paper showed that, given a generator characterized
    by a distribution *p[g](x)*, the optimal discriminator is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6905cecd-0b6b-4604-8b67-992f19fee8df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, considering the previous value function *V(G*, *D)* and using
    the optimal discriminator, we can rewrite it in a single objective (as a function
    of *G*) that must be minimized by the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85879f7c-9cf7-408c-9672-ddabf7e0989d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To better understand how a **GAN** works, we need to expand the previous expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0287e84f-5d7a-4c11-8cc3-54b8c26eb2f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Applying some simple manipulations, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3058f142-4c04-4230-94dd-422eaaec8d37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The last term represents the Jensen-Shannon divergence between *p[data]* and
    *p[g]*. This measure is similar to the Kullback-Leibler divergence, but it''s
    symmetric and bounded between *0* and *log(2)*. When the two distributions are
    identical, *D[JS] = 0*, but if their supports (the value sets where *p(x) > 0*)
    are disjoint, *D[JS]** = log(2) (*while *D[KL] = ∞)*. Therefore, the value function
    can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1856a1c4-b41c-4ca3-8d60-a82a38f6a07e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, it should be clearer that a GAN tries to minimize the Jensen-Shannon divergence
    between the data generating process and the generator distribution. In general,
    this procedure is quite effective; however, when the supports are disjointed,
    a GAN has no pieces of information about the true distance. This consideration
    (analyzed with more mathematical rigor in *Improved Techniques for Training GANs*, *Salimans
    T*., *Goodfellow I*., *Zaremba W*., *Cheung V*., *Radford A*.,and *Chen X*., *arXiv:1606.03498
    [cs.LG]*) explains why training a GAN can become quite difficult, and, consequently,
    why the Nash equilibrium cannot be found in many cases. For these reasons, we
    are going to analyze an alternative approach in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete GAN algorithm (as proposed by the authors) is:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the number of epochs, *N[epochs]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the number of discriminator iterations, *N[iter]* (in most cases, *N[iter]
    = 1*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the batch size, *k*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a noise generating process, *M* (for example, *U(-1, 1)*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *e=1* to *N[epochs]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *k* values from *X*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *k* values from *N*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N[iter]*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients, *∇[d] V(G, D)* (only with respect to the discriminator
    variables). The expected value is approximated with a sample mean.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the discriminator parameters by *Stochastic Gradient Ascent* (as we are
    working with logarithms, it's possible to minimize the negative loss).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *k* values from *N*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients, *∇[g] V[noise](G, D)* (only with respect to the generator
    variables)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the generator parameters by Stochastic Gradient Descent
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: As these models need to sample noisy vectors in order to guarantee the reproducibility,
    I suggest setting the random seed in both NumPy (`np.random.seed(...)`) and TensorFlow
    (`tf.set_random_seed(...)`). The default value chosen for all of these experiments
    is 1,000.
  prefs: []
  type: TYPE_NORMAL
- en: Example of DCGAN with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we want to build a DCGAN (proposed in *Unsupervised Representation
    Learning with Deep Convolutional Generative Adversarial Networks*, *Radford A*.,
    *Metz L*., *Chintala S*., , *arXiv:1511.06434 [cs.LG]*) with the Fashion-MNIST
    dataset (obtained through the `keras` helper function). As the training speed
    is not very high, we limit the number of samples to 5,000, but I suggest repeating
    the experiment with larger values. The first step is loading and normalizing (between
    -1 and 1) the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the original paper, the generator is based on four transpose convolutions
    with kernel sizes equal to *(4, 4)* and strides equal to *(2, 2)*. The input is
    a single multi-channel pixel (*1 × 1 × code_length*) that is expanded by subsequent
    convolutions. The number of filters is 1024, 512, 256, 128, and 1 (we are working
    with grayscale images). The authors suggest employing a symmetric-valued dataset
    (that''s why we have normalized between -1 and 1), batch normalization after each
    layer, and leaky ReLU activation (with a default negative slope set to 0.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The strides are set to work with 64 × 64 images (unfortunately, the Fashion-MNIST
    dataset has 28 × 28 samples, which cannot be generated with power-of-two modules);
    therefore, we are going to resize the samples while training. As we need to compute
    the gradients of the discriminator and generator separately, it's necessary to
    set the variable scope (using the context manager `tf.variable_scope()`) to immediately
    extract only the variables whose names have the scope as a prefix (for example, `generator/Conv_1_1/...`).
    The `is_training` parameter is necessary to disable the batch normalization during
    the generation phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The discriminator is almost the same as a generator (the only main differences
    are the inverse convolution sequence and the absence of batch normalization after
    the first layer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we have an extra parameter (`reuse_variables`) that is necessary
    when building the loss functions. In fact, we need to declare two discriminators
    (fed with real samples and with the generator output), but they are not made up
    of separate layers; hence, the second one must reuse the variables defined by
    the first one. We can now create a graph and define all of the placeholders and
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step is defining the placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_x` contains the true samples drawn from *X*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_z` contains the noise samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_training` is a Boolean indicating whether or not the batch normalization
    must be active'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we define the generator instance after reshaping the noise sample as a
    *(1 × 1 × code_length)* matrix (this is necessary to work efficiently with transpose
    convolutions). As this is a fundamental hyperparameter, I suggest testing different
    values and comparing the final performances.
  prefs: []
  type: TYPE_NORMAL
- en: As explained previously, the input images are resized before defining the two
    discriminators (the second one reuses the variables previously defined). The  `discr_1_l `instance
    is fed with the true samples, while `discr_2_l` works with the generator output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is defining the loss functions. As we are working with logarithms,
    there can be stability problems when the values become close to 0\. For this reason,
    it''s preferable to employ the TensorFlow built-in function `tf.nn.sigmoid_cross_entropy_with_logits()`,
    which guarantees numerical stability in every case. This function takes a *logit*
    as input and applies the sigmoid transformation internally. In general, the output
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/113fa46f-60de-4854-8b88-6afa017fec4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, setting the label equal to 1 forces the second term to be null, and
    vice versa. At this point, we need to create two lists containing the variables
    belonging to each scope (this can be easily achieved by using the `tf.trainable_variables()` function,
    which outputs a list of all variables). The last step consists of defining the
    optimizers. As suggested in the official TensorFlow documentation, when working
    with batch normalizations, it's necessary to wrap the training operations in a
    context manager that checks whether all dependencies (in this case, batch average
    and variance) have been computed. We have employed the Adam optimizer with *η*
    = 0.0002, and a gradient momentum forgetting factor (*μ1*) equal to 0.5 (this
    is a choice motivated by the potential instability that a high momentum can yield).
    As it's possible to see, in both cases, the minimization is limited to a specific
    subset of the variables (providing a list through the `var_list` parameter).
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can create a `Session` (we are going to use an `InteractiveSession`),
    initialize all variables, and start the training procedure (with 200 epochs and
    a batch size equal to 128):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The training step (with a single discriminator iteration) is split into two
    phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator training with a batch of true images and noise samples
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generator training with a batch of noise samples
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the training process has finished, we can generate some images (50) by
    executing the generator with a matrix of noise samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f960533-fb13-4f56-8901-1f74b65f1635.png)'
  prefs: []
  type: TYPE_IMG
- en: Samples generated by a DCGAN trained with the Fashion-MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise, I invite the reader to employ more complex convolutional architectures
    and an RGB dataset such as CIFAR-10 ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)).
  prefs: []
  type: TYPE_NORMAL
- en: The training phase of this example and the following one, even if limited to
    5,000 samples, can be quite slow (around 12-15 hours) particularly when no GPU
    is available. The reader can simplify the examples by reducing the complexity
    of the networks (paying attention to the shapes) and reducing the number of samples.
    To avoid mismatches, I suggest adding the `print(gen.shape)` command after the
    generator instance. The expected shape should be `(?, 64, 64, 1)`. Alternatively,
    it's possible to employ smaller target dimensions (like 32 × 32), setting one
    of the strides (possibly the last one) equal to `(1, 1)`.
  prefs: []
  type: TYPE_NORMAL
- en: Wasserstein GAN (WGAN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained in the previous section, one of the most difficult problems with
    standard GANs is caused by the loss function based on the Jensen-Shannon divergence,
    whose value becomes constant when two distributions have disjointed supports.
    This situation is quite common with high-dimensional, semantically structured
    datasets. For example, images are constrained to having particular features in
    order to represent a specific subject (this is a consequence of the manifold assumption
    discussed in [Chapter 2](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml), *Introduction
    to Semi-Supervised Learning*). The initial generator distribution is very unlikely
    to overlap a true dataset, and in many cases, they are also very far from each
    other. This condition increases the risk of learning a wrong representation (a
    problem known as mode collapse), even when the discriminator is able to distinguish
    between true and generated samples (such a condition arises when the discriminator
    learns too quickly, with respect to the generator). Moreover, the Nash equilibrium
    becomes harder to achieve, and the GAN can easily remain blocked in a sub-optimal
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to mitigate this problem, Arjovsky, Chintala, and Bottou (in *Wasserstein
    GAN*, *Arjovsky M*., *Chintala S*., *Bottou L.*, *arXiv:1701.07875 [stat.ML]*)
    proposed employing a different divergence, called the *Wasserstein distance* (or
    Earth Mover''s distance), which is formally defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db345cf3-03b3-4cd5-9293-99159663a83a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The term *∏(p[data], p[g])* represents the set of all possible joint probability
    distributions between *p[data]*, *p**[g]*. Hence, the Wasserstein distance is
    the infimum (considering all joint distributions) of the set of expected values
    of *||x - y||,* where *x* and *y* are sampled from the joint distribution μ. The
    main property of *D[W]* is that, even when two distributions have disjointed support,
    its value is proportional to the actual distributional distance. The formal proof
    is not very complex, but it''s easier to understand the concept intuitively. In
    fact, given two distributions with disjointed support, the infimum operator forces
    taking the shortest distance between each possible couple of samples. Clearly,
    this measure is more robust than the Jensen-Shannon divergence, but there''s a
    practical drawback: it''s extremely difficult to compute. As we cannot work with
    all possible joint distributions (nor with an approximation), a further step is
    necessary to employ this loss function. In the aforementioned paper, the authors
    proved that it''s possible to apply a transformation, thanks to the Kantorovich-Rubinstein
    theorem (the topic is quite complex, but the reader can find further information
    in *On the Kantorovich–Rubinstein Theorem*, *Edwards D*. *A*., *Expositiones Mathematicae*,
    2011):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9eee8eb-17a7-4e32-93b1-43888c8c6ee6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first element to consider is the nature of *f(•)*. The theorem imposes
    considering only L-Lipschitz functions, which means that *f(•)* (assuming a real-valued
    function of a single variable) must obey:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7dc640f-292c-4ed7-a144-4fa6ff1f6ff2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, the Wasserstein distance is proportional to the supremum (with
    respect to all L-Lipschitz functions) of the difference between two expected values,
    which are extremely easy to compute. In a WGAN, the *f(•)* function is represented
    by a neural network; therefore, we have no warranties about the Lipschitz condition.
    To solve this problem, the author suggested a very simple procedure: clipping
    the discriminator (which is normally called Critic, and whose responsibility is
    to represent the parameterized function *f(•)*) variables after applying the corrections.
    If the input is bounded, all of the transformations will yield a bounded output;
    however, the clipping factor must be small enough (0.01, or even smaller) to avoid
    the additive effect of multiple operations leading to an inversion of the Lipschitz
    condition. This is not an efficient solution (because it slows down the training
    process when it''s not necessary), but it allows for exploiting the Kantorovich-Rubinstein
    theorem, even when there are no formal constraints imposed on the function family.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a parameterized function (such as a Deep Convolutional Network), the
    Wasserstein distance becomes as follows (omitting the term *L*, which is constant):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a63cc711-96c9-4bd8-ad48-731742488753.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous expression, we explicitly extracted the generator output, and
    in the last step, separated the term that will be optimized separately. The reader
    has probably noticed that the computation is simpler than a standard GAN because,
    in this case, we have to average over only the *f(•)* values of a batch (there's
    no more need for a logarithm). However, as the Critic variables are clipped, the
    number of required iterations is normally larger, and in order to compensate the
    difference between the training speeds of the Critic and generator, it's often
    necessary to set *N[critic] > 1* (the authors suggest a value equal to 5, but
    this is a hyperparameter that must be tuned in every specific context).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete WGAN algorithm is:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the number of epochs, *N[epochs.]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the number of Critic iterations, *N[critic]* (in most cases, *N[iter]* =
    5).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the batch size, *k.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a clipping constant, c (for example, c = 0.01).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a noise generating process, M (for example, *U(-1, 1)*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *e=1* to *N[epochs]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *k* values from *X*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *k* values from *N*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N[critic]*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients, *∇[c] D[W](p[data]||p[g])* (only with respect to the
    Critic variables). The expected values are approximated by sample means.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the Critic parameters by Stochastic Gradient Ascent.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Clip the Critic parameters in the range [*-c, c*].
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *k* values from *N*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients, *∇[g] W[noise]* (only with respect to the generator variables).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the generator parameters by Stochastic Gradient Descent.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Example of WGAN with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This example can be considered a variant of the previous one because it uses
    the same dataset, generator, and discriminator. The only main difference is that
    in this case, the discriminator (together with its variable scope) has been renamed `critic()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can step directly to the creation of the `Graph` containing
    all of the placeholders, operations, and loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As it's possible to see, there are no differences in the placeholder section,
    in the definition of the generator, and in the image resizing to the target dimensions
    of 64 × 64\. In the next block, we define the two Critic instances (which are
    perfectly analogous to the ones declared in the previous example).
  prefs: []
  type: TYPE_NORMAL
- en: The two loss functions are simpler than a standard GAN, as they work directly
    with the Critic outputs, computing the sample mean over a batch. In the original
    paper, the authors suggest using RMSProp as the standard optimizer, in order to
    avoid the instabilities that a momentum-based algorithm can produce. However,
    Adam, with lower forgetting factors (*μ[1] = 0.5* and *μ[2] = 0.9*) and a learning
    rate *η = 0.00005*, is faster than RMSProp, and doesn't lead to instabilities.
    I suggest testing both options, trying to maximize the training speed while preventing
    the mode collapse. Contrary to the previous example, in this case we need to clip
    all of the Critic variables after each training step. To avoid that, the internal
    concurrency can alter the order of some operations; it's necessary to employ a
    nested dependency control context manager. In this way, the actual `training_step_c` (responsible
    for clipping and reassigning the values to each variable) will be executed only
    after the `optimizer_c` step has completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create the `InteractiveSession`, initialize the variables, and
    start the training process, which is very similar to the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The main difference is that, in this case, the Critic is trained `n_critic`
    times before each generator training step. The result of the generation of 50
    random samples is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e952ca1d-0bd6-4cdd-800d-3474c876432b.png)'
  prefs: []
  type: TYPE_IMG
- en: Samples generated by a WGAN trained with the Fashion MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: As it's possible to see, the quality is slightly higher, and the samples are
    smoother. I invite the reader to also test this model with an RGB dataset, because
    the final quality is normally excellent.
  prefs: []
  type: TYPE_NORMAL
- en: When working with these models, the training time can be very long. To avoid
    waiting to see the initial results (and to perform the required tuning), I suggest
    using Jupyter. In this way, it's possible to stop the learning process, check
    the generator ability, and restart it without any problem. Of course, the graph
    must remain the same, and the variable initialization must be performed only at
    the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed the main principles of adversarial training,
    and explained the roles of two players: the generator and discriminator. We described
    how to model and train them using a minimax approach whose double goal is to force
    the generator to learn the true data distribution *p[data]*, and get the discriminator
    to distinguish perfectly between true samples (belonging to *p[data]*) and unacceptable
    ones. In the same section, we analyzed the inner dynamics of a Generative Adversarial
    Network and some common problems that can slow down the training process and lead
    to a sub-optimal final configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most difficult problems experienced with standard GANs arises when
    the data generating process and the generator distribution have disjointed support.
    In this case, the Jensen-Shannon divergence becomes constant and doesn't provide
    precise information about the distance. An excellent alternative is provided by
    the Wasserstein measure, which is employed in a more efficient model, called WGAN.
    This method can efficiently manage disjointed distributions, but it's necessary
    to enforce the L-Lipschitz condition on the Critic. The standard approach is based
    on clipping the parameters after each gradient ascent update. This simple technique
    guarantees the L-Lipschitz condition, but it's necessary to use very small clipping
    factors, and this can lead to a slower conversion. For this reason, it's normally
    necessary to repeat the training of the Critic a fixed number of times (such as
    five) before each single generator training step.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to introduce another probabilistic generative
    neural model, based on a particular kind of neural network, called the Restricted
    Boltzmann Machine.
  prefs: []
  type: TYPE_NORMAL
