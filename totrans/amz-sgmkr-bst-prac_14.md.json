["```py\n    from sagemaker.model_monitor import DataCaptureConfig\n    data_capture_config = DataCaptureConfig(\n    enable_capture=True, \n    sampling_percentage=100,  destination_s3_uri=s3_capture_upload_path\n    )\n    ```", "```py\n    predictor = model.deploy(initial_instance_count=1,\n                    instance_type='ml.m4.xlarge',\n                    endpoint_name=endpoint_name,\n                   data_capture_config = data_capture_config)\n    ```", "```py\n    {\n      \"captureData\": {\n        \"endpointInput\": {\n          \"observedContentType\": \"text/csv\",\n          \"mode\": \"INPUT\",\n         \"data\": \"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\n          \"encoding\": \"CSV\"\n        },\n        \"endpointOutput\": {\n          \"observedContentType\": \"text/csv; charset=utf-8\",\n          \"mode\": \"OUTPUT\",\n          \"data\": \"-4.902510643005371\",\n          \"encoding\": \"CSV\"\n        }\n      },\n      \"eventMetadata\": {\n        \"eventId\": \"e68592ca-948c-44dd-a764-608934e49534\",\n        \"inferenceTime\": \"2021-06-28T18:41:16Z\"\n      },\n      \"eventVersion\": \"0\"\n    }\n    ```", "```py\n    from sagemaker.model_monitor import DefaultModelMonitor\n    from sagemaker.model_monitor.dataset_format import DatasetFormat\n    my_default_monitor = DefaultModelMonitor(\n        role=role,\n        instance_count=1,\n        instance_type=\"ml.m5.xlarge\",\n        volume_size_in_gb=20,\n        max_runtime_in_seconds=3600,\n    )\n    ```", "```py\n    my_default_monitor.suggest_baseline(\n        baseline_dataset=baseline_data_uri + \"/training-dataset-with-header.csv\",\n        dataset_format=DatasetFormat.csv(header=True),\n        output_s3_uri=baseline_results_uri,\n        wait=True\n    )\n    ```", "```py\n    with open(f\"test_data/{validate_dataset}\", \"w\") as baseline_file:\n        baseline_file.write(\"probability,prediction,label\\n\")  # Header of the file\n        for tl in t_lines[1:300]:\n            #Remove the first column since it is the label\n            test_list = tl.split(\",\")\n            label = test_list.pop(0)\n            test_string = ','.join([str(elem) for elem in test_list])\n\n            result = smrt.invoke_endpoint(EndpointName=endpoint_name,\n             ContentType=\"text/csv\", Body=test_string)   \n            rbody = StreamingBody(raw_stream=result['Body'],content_length=int(result['ResponseMetadata']['HTTPHeaders']['content-length']))\n            prediction = rbody.read().decode('utf-8')\n            baseline_file.write(f\"{prediction},{prediction},{label}\\n\")\n            #print(f\"label {label} ; prediction {prediction} \")\n            print(\".\", end=\"\", flush=True)\n            sleep(0.5)\n    ```", "```py\n    # Create the model quality monitoring object\n    model_quality_monitor = ModelQualityMonitor(\n        role=role,\n        instance_count=1,\n        instance_type=\"ml.m5.xlarge\",\n        volume_size_in_gb=20,\n        max_runtime_in_seconds=1800,\n        sagemaker_session=session,\n    )\n    ```", "```py\n    cut the baseline suggestion job.\n    # You will specify problem type, in this case Binary Classification, and provide other requirtributes.\n    job = model_quality_monitor.suggest_baseline(\n        job_name=baseline_job_name,\n        baseline_dataset=baseline_dataset_uri,\n        dataset_format=DatasetFormat.csv(header=True),\n        output_s3_uri=baseline_results_uri,\n        problem_type=\"Regression\",\n        inference_attribute=\"prediction\",\n        probability_attribute=\"probability\",\n        ground_truth_attribute=\"label\",\n    )\n    job.wait(logs=False)\n    ```", "```py\n    model_bias_data_config = DataConfig(\n        s3_data_input_path=validation_dataset,\n        s3_output_path=model_bias_baselining_job_result_uri,\n        label=label_header,\n        headers=all_headers,\n        dataset_type='CSV'\n    )\n    ```", "```py\n    model_bias_config = BiasConfig(\n        label_values_or_threshold=[1],\n        facet_name=\"City\",\n        facet_values_or_threshold=[100],\n    )\n    ```", "```py\n    model_config = ModelConfig(\n        model_name=model_name,\n        instance_count=endpoint_instance_count,\n        instance_type=endpoint_instance_type,\n        content_type=dataset_type,\n        accept_type=dataset_type,\n    )\n    ```", "```py\n    model_predicted_label_config = ModelPredictedLabelConfig(\n        probability_threshold=0.8,\n    )\n    ```", "```py\n    model_bias_monitor = ModelBiasMonitor(\n        role=role,\n        sagemaker_session=sagemaker_session,\n        max_runtime_in_seconds=1800,\n    )\n    model_bias_monitor.suggest_baseline(\n        model_config=model_config,\n        data_config=model_bias_data_config,\n        bias_config=model_bias_config,\n        model_predicted_label_config=model_predicted_label_config,\n    )\n    ```", "```py\n    {\n        \"version\": \"1.0\",\n        \"post_training_bias_metrics\": {\n            \"label\": \"value\",\n            \"facets\": {\n                \"city\": [\n                    {\n                     \"value_or_threshold\": \"(100.0, 2278.0]\",\n                        \"metrics\": [\n                            {\n                                \"name\": \"AD\",\n                   \"description\": \"Accuracy Difference (AD)\",\n                                \"value\": 0.008775168751768203\n                            },\n                           ...\n                ]\n     },\n            \"label_value_or_threshold\": \"(1.0, 130.24536736711912]\"\n        }\n    ```", "```py\n    # Here use the mean value of test dataset as SHAP baseline\n    test_dataframe = pd.read_csv(test_dataset, header=None)\n    shap_baseline = [list(test_dataframe.mean())]\n    shap_config = SHAPConfig(\n        baseline=shap_baseline,\n        num_samples=100,\n        agg_method=\"mean_abs\",\n        save_local_shap_values=False,\n    )\n    ```", "```py\n    model_explainability_monitor = ModelExplainabilityMonitor(\n        role=role,\n        sagemaker_session=sagemaker_session,\n        max_runtime_in_seconds=1800,\n    )\n    ```", "```py\n    model_explainability_monitor.suggest_baseline(\n        data_config=model_explainability_data_config,\n        model_config=model_config,\n        explainability_config=shap_config,\n    ```", "```py\n    {\n        \"version\": \"1.0\",\n        \"explanations\": {\n            \"kernel_shap\": {\n                \"label0\": {\n                    \"global_shap_values\": {\n                        \"ismobile\": 0.00404293281766823,\n                        \"year\": 0.006527703849451637,\n                         ...\n                         \"co\": 0.03389338421306029\n                    },\n                    \"expected_value\": 0.17167794704437256\n                }\n            }\n        }\n    }\n    ```"]