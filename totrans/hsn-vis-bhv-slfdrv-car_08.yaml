- en: '*Chapter 6*: Improving Your Neural Network'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091), *Deep Learning
    with Neural Networks*, we designed a network that is able to achieve almost 93%
    accuracy in the training dataset, but that translated to less than 66% accuracy
    in the validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will continue working on that neural network, with the aim
    to improve the validation accuracy significantly. Our goal is to reach at least
    80% validation accuracy. We will apply some of the knowledge acquired in [*Chapter
    5*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118), *Deep Learning Workflow*,
    and we will also learn new techniques that will help us very much, such as batch
    normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the number of parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the size of the network and the number of layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding batch normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving validation with early stopping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtually increasing the dataset size with data augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving validation accuracy with dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving validation accuracy with spatial dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The full source code for the chapter can be found here: [https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter6](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter6)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter requires the following software prerequisites, and a basic knowledge
    of the following would help to understand the chapter better:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NumPy module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Matplotlib module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TensorFlow module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCV-Python module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A recommended GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Code in Action videos for this chapter can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3dGIdJA](https://bit.ly/3dGIdJA)'
  prefs: []
  type: TYPE_NORMAL
- en: A bigger model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training your own neural network is an art; you need intuition, some luck, a
    lot of patience, and all the knowledge and help that you can find. You will also
    need money and time to either buy a faster GPU, use clusters to test more configurations,
    or pay to get a better dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there are no real recipes. That said, we will divide our journey into two
    phases, as explained in [*Chapter 5*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118),
    *Deep Learning Workflow*:'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving generalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start from where we left off in [*Chapter 4*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091),
    *Deep Learning with Neural Networks*, with our basic model reaching 66% validation
    accuracy on CIFAR-10, and then we will improve it significantly, first to make
    it faster, and then to make it more precise.
  prefs: []
  type: TYPE_NORMAL
- en: The starting point
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the model that we developed in [*Chapter 4*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091),
    *Deep Learning with Neural Networks*, a model that overfits the dataset because
    it achieves a high training accuracy value at relatively low validation accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It is a shallow but relatively big model as it has the following number of
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We previously trained it for 12 epochs, with these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The training accuracy is actually good enough for us (here, in this run, it
    is higher than in [*Chapter 5*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118),
    *Deep Learning Workflow*, mostly because of randomness), but the validation accuracy
    is low too. It is overfitting. So, we could even keep it as a starting point,
    but it would be nice to tune it a bit and see whether we can do better or make
    it faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should also keep an eye on five epochs, as we might do some tests on fewer
    epochs, to speed up the whole process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When you use fewer epochs, you are betting on being able to understand the evolution
    of the curves, so you are trading speed of development for accuracy of your choices.
    Sometimes, this is fine, but sometimes it is not.
  prefs: []
  type: TYPE_NORMAL
- en: Our model is too big, so we will start reducing its size and speeding up the
    training a bit.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the speed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our model is not just very big – in fact, it is too big. The second convolutional
    layer has 256 filters and, combined with the 512 neurons of the dense layer, they
    use a high number of parameters. We can do better. We know that we could split
    them into layers of 128 filters, and this would save almost half of the parameters,
    as the dense layer now needs half of the connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can try that. We learned in [*Chapter 4*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091),
    *Deep Learning with Neural Networks*, that to not lose resolution after a convolution,
    we can use padding in the *same* way on both the layers (dense layers omitted),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that the number of parameters now is lower:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the full results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Nice! It is faster, the accuracy went up slightly, and also, the validation
    improved!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do the same on the first layer, but this time without increasing the
    resolution so as not to increase the parameters, since, between two convolutional
    layers, the gain is lower:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When we try this, we get these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is similar to before, though the validation accuracy improved slightly.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will add more layers.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the depth
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous model is actually an excellent starting point.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we will add more layers, to increase the number of non-linear activations
    and to be able to learn more complex functions. This is the model (dense layers
    omitted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The network is now significantly slower and the accuracy went down (maybe because
    more epochs are required), but the validation accuracy improved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try now to reduce the dense layers, as follows (convolutional layers
    omitted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have fewer parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'But something very bad happened:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Both the validations dropped! In fact, they are now 10%, or if you prefer, the
    network is now producing a random result—it did not learn!
  prefs: []
  type: TYPE_NORMAL
- en: 'You might conclude that we broke it. In fact, that is not the case. It''s enough
    to run it again, using the randomness to our advantage, and our network learns
    as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: However, this is not a very good sign. This might be due to the increase in
    layers, as networks with more layers are more difficult to train, due to the fact
    that the original input can have problems in terms of being propagated to the
    upper layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Losses and accuracies graph](img/Figure_6.1_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Losses and accuracies graph
  prefs: []
  type: TYPE_NORMAL
- en: You can see that while the training loss (blue line) keeps decreasing, the validation
    loss (orange line) after some epochs starts to increase. As explained in [*Chapter
    5*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118)*,* *Deep Learning Workflow*,
    this means that the model is overfitting. This is not necessarily the best model,
    but we will continue developing it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will simplify this model.
  prefs: []
  type: TYPE_NORMAL
- en: A more efficient network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training the previous model requires 686 seconds on my laptop, and achieves
    a validation accuracy of 74.5%, and a training accuracy of 91.4%. Ideally, to
    improve the efficiency, we want to keep accuracy at the same level while reducing
    the training time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check some of the convolutional layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – First convolutional layer, 32 channels](img/Figure_6.2_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – First convolutional layer, 32 channels
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already seen these activation graphs in [*Chapter 5*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118),
    *Deep Learning Workflow*, and we know that channels that are black do not achieve
    a big activation, so they don''t contribute much to the result. In practice, it
    looks like half of the channels are not in use. Let''s try to halve the number
    of channels in every convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we get as the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the number of parameters is now much less, and training is much
    faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we see that we lost some accuracy also, but not too much:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – First convolutional layer, 16 channels](img/Figure_6.3_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – First convolutional layer, 16 channels
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s a bit better. Let''s check the second layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Second convolutional layer, 16 channels](img/Figure_6.4_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Second convolutional layer, 16 channels
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also better. Let''s check the fourth convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Fourth convolutional layer, 64 channels](img/Figure_6.5_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Fourth convolutional layer, 64 channels
  prefs: []
  type: TYPE_NORMAL
- en: 'It seems a bit empty. Let''s halve the third and fourth layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The training accuracy went down, but the validation accuracy is still fine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Fourth convolutional layer, 32 channels](img/Figure_6.6_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Fourth convolutional layer, 32 channels
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the sixth convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Sixth convolutional layer, 128 channels](img/Figure_6.7_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Sixth convolutional layer, 128 channels
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s a bit empty. Let''s also halve the last two convolutional layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It is much smaller, with these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It still looks good. Let''s check the activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Sixth convolutional layer, 64 channels](img/Figure_6.8_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Sixth convolutional layer, 64 channels
  prefs: []
  type: TYPE_NORMAL
- en: You can see that now, many channels are activated, which hopefully is an indication
    that the neural network is better at taking advantage of its resources.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing this model with the one built in the previous section, you can see
    that this model can be trained in a bit less than half the time, the validation
    accuracy is almost unchanged, and the training accuracy decreased a bit, but not
    much. So, it is indeed more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss batch normalization, a layer very common
    on modern neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Building a smarter network with batch normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We normalize the input that we provide to the network, constraining the range
    from 0 to 1, so it could be beneficial to do that also in the middle of the network.
    This is called **batch normalization**, and it does wonders!
  prefs: []
  type: TYPE_NORMAL
- en: In general, you should add the batch normalization after the output that you
    want to normalize, and before the activation, but adding it after the activation
    might provide faster performance, and this is what we will do.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the new code (dense layers omitted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of parameters increased just a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Not bad, even if unfortunately, now it is much slower. But we can add even
    more batch normalization, to see whether this improves the situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Yes, both the accuracies improved. We are actually very close to our initial
    goal of having 80% accuracy. But let's go further and see what we can do.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we only used ReLU activation, but even if it is used very much, it's
    not the only one. Keras supports a variety of activations, and sometimes it is
    worth experimenting with. We will stick to ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check some activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Second convolutional layer, 16 channels, batch normalization](img/Figure_6.9_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Second convolutional layer, 16 channels, batch normalization
  prefs: []
  type: TYPE_NORMAL
- en: Now, all the channels of the second layer are learning. Very good!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the result of the fourth layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Fourth convolutional layer, 32 channels, batch normalization](img/Figure_6.10_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Fourth convolutional layer, 32 channels, batch normalization
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the result of the sixth layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Sixth convolutional layer, 64 channels, batch normalization](img/Figure_6.11_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Sixth convolutional layer, 64 channels, batch normalization
  prefs: []
  type: TYPE_NORMAL
- en: It is starting to look good!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to visualize the effect of batch normalization on the activations
    of the first layer before and after batch normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – First convolutional layer, 16 channels, before and after batch
    normalization](img/Figure_6.12_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – First convolutional layer, 16 channels, before and after batch
    normalization
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the intensity of the channels is now more uniform; there are
    no longer channels without activity and channels with very strong activations.
    However, the channels that are inactive are still without real information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also check the second layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Second convolutional layer, 16 channels, before and after batch
    normalization](img/Figure_6.13_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Second convolutional layer, 16 channels, before and after batch
    normalization
  prefs: []
  type: TYPE_NORMAL
- en: Here, it is maybe less evident, but you can still see that the difference between
    the channels is reduced, as clearly, they have been normalized. Intuitively, this
    helps propagate weaker signals through layers, and it has some regularization
    effect, which results in better validation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have talked about batch normalization, it is time to discuss further
    what a batch is and what implications the size of a batch has.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right batch size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During training, we have a high number of samples, typically from thousands
    to millions. If you remember, the optimizer will compute the loss function and
    update the hyperparameters to try to reduce the loss. It could do this after every
    sample, but the result could be noisy, with continuous changes that can slow down
    the training. At the opposite end, the optimizer could update the hyperparameters
    only once per epoch – for example, using the average of the gradients – but this
    typically leads to bad generalization. Usually, there is a range on the batch
    size that performs better than these two extremes, but unfortunately, it depends
    on the specific neural network.
  prefs: []
  type: TYPE_NORMAL
- en: A larger batch size can marginally improve the training time on a GPU, but if
    your model is big, you might find that the memory of your GPU is a limit to how
    big your batch size can be.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization is also affected by the size of the batch, as small batches
    reduce its effectiveness (as there is not enough data do to a proper normalization).
  prefs: []
  type: TYPE_NORMAL
- en: Given these considerations, the best thing to do is to just try. Normally, you
    can try using 16, 32, 64, and 128, and eventually extend the range if you see
    that the best value is at the limit of the range.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen, the best batch size can improve accuracy and possibly increase
    the speed, but there is another technique that can help us to improve the validation
    accuracy while speeding up, or at least simplifying, the training: early stopping.'
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When should we stop training? That's a good question! Ideally, you want to stop
    at the minimum validation error. While you cannot know this in advance, you can
    check the losses and get an idea of how many epochs you need. However, when you
    train your network, sometimes you need more epochs depending on how you tune your
    model, and it is not simple to know in advance when to stop.
  prefs: []
  type: TYPE_NORMAL
- en: We already know that we can use `ModelCheckpoint`, a callback of Keras, to save
    the model with the best validation error seen during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there is also another very useful callback, `EarlyStopping`, which stops
    the training when a predefined set of conditions happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The most important parameters to configure early stopping are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`monitor`: This decides which parameter to monitor, by default: validation
    loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_delta`: If the difference in validation loss between epochs is below this
    value, the loss is considered to not have changed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patience`: This is the number of epochs with no validation improvement to
    allow before stopping the training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose`: This is instructing Keras to provide more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reason why we need early stopping is that with data augmentation and dropout,
    we will need many more epochs, and instead of guessing when it is time to stop,
    we will use early stopping to do that for us.
  prefs: []
  type: TYPE_NORMAL
- en: Let's talk now about data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the dataset with data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's time to use data augmentation, and basically, increase the size of our
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: From this moment, we will no longer care about the accuracy of the training
    dataset, as this technique will reduce it, but we will focus on the validation
    accuracy, which is expected to improve.
  prefs: []
  type: TYPE_NORMAL
- en: We also expect to need more epochs, as our dataset is now more difficult, so
    we will now set the epochs to `500` (though we don't plan to reach it) and use
    `EarlyStopping` with a patience of `7`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try with this augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: You should take care not to overdo things because the network might learn a
    dataset too different from validation, and in this case, you will see the validation
    accuracy stuck at 10%.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Early stopping interrupted the training after `31` epochs, and we reached a
    validation accuracy of above 84% –not bad. As expected, we need many more epochs
    now. This is a graph of the losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Losses with data augmentation and early stopping](img/Figure_6.14_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Losses with data augmentation and early stopping
  prefs: []
  type: TYPE_NORMAL
- en: You can see how the training accuracy kept increasing, while the validation
    accuracy at some point decreased. The network is still overfitting a bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the activations of the first convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – First convolutional layer, 16 channels, with data augmentation
    and early stopping](img/Figure_6.15_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – First convolutional layer, 16 channels, with data augmentation
    and early stopping
  prefs: []
  type: TYPE_NORMAL
- en: It's marginally better, but it can probably improve again.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can try to augment the data a little more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This model is slower and less accurate. Let''s see the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Losses with more data augmentation and early stopping](img/Figure_6.16_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Losses with more data augmentation and early stopping
  prefs: []
  type: TYPE_NORMAL
- en: Maybe it needs more patience. We will stick to the previous data augmentation
    then.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will analyze a simple but effective way to increase
    the validation accuracy using the dropout layer.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the validation accuracy with dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A source of overfitting is the fact that the neural network relies more on some
    neurons to draw its conclusions, and if those neurons are wrong, the network is
    wrong. One way to reduce this problem is simply to randomly shut down some neurons
    during training while keeping them working normally during inference. In this
    way, the neural network learns to be more resistant to errors and to generalize
    better. This mechanism is called **dropout**, and obviously, Keras supports it.
    Dropout increases the training time, as the network needs more epochs to converge.
    It might also require a bigger network, as some neurons are randomly deactivated
    during training. It is also more useful when the dataset is not very big for the
    network, as it is more likely to overfit. In practice, as dropout is meant to
    reduce overfitting, it brings little benefit if your network is not overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: A typical value of dropout for dense layers is 0.5, though we might use a bit
    less, as our model is not overfitting much. We will also increase the *patience*
    to `20`, as the model needs more epochs to be trained now and the validation loss
    can fluctuate for more time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to add some dropout to the dense layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: A bit disappointing. It took a lot of time to train, with small gains. We assume
    that our dense layers are a bit small.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s increase the size of the layers by 50%, and we will also increase the
    dropout of the first dense layer while reducing the dropout of the second one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'It is, of course, bigger, as we can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'It has a slightly better result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As we improve the validation accuracy, even small gains are difficult to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Losses with more data augmentation and dropout on dense layers](img/Figure_6.17_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Losses with more data augmentation and dropout on dense layers
  prefs: []
  type: TYPE_NORMAL
- en: There is a bit of overfitting, so let's try to fix it. We can also use `Dropout`
    in the convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the disappointing result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The network did not improve!
  prefs: []
  type: TYPE_NORMAL
- en: Here, we see an interesting case—the validation accuracy is significantly higher
    than the training accuracy. How is that possible?
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming that your split is correct (for example, you don''t have a validation
    set that is too easy or containing images too similar to the training dataset),
    two factors can create this situation:'
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation can potentially make the training dataset harder than the
    validation dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout is active during the training phase and deactivated during the prediction
    phase, so this means that the training dataset can be significantly harder than
    the validation dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our case, the culprit is the dropout. You don't necessarily need to avoid
    this situation, if it is justified, but in our case, the validation accuracy went
    down, so we need to fix our dropout, or maybe increase the size of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'I find `Dropout` more difficult to use with the convolutional layers, and I
    would personally not use a big dropout in that case. Here, there are some guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: No batch normalization right after `Dropout`, as normalization would suffer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dropout` is more effective after `MaxPooling` than before.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dropout` after a convolutional layer drops single pixels, but `SpatialDropout2D`
    drops channels, and it is recommended on the first few layers at the beginning
    of the neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I ran another few (long!) experiments and I decided to increase the size of
    the convolutional layers, reduce the dropout, and use `Spatial Dropout` in a couple
    of layers. I ended up with this neural network, which is what I consider my final
    version.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the code of the convolutional layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'There is an improvement in the validation accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Losses with more data augmentation and dropout on dense and
    convolutional layers](img/Figure_6.18_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Losses with more data augmentation and dropout on dense and convolutional
    layers
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! Now you have an idea of how to train a neural network, and
    feel free to experiment and go crazy! Every task is different, and the possibilities
    are really endless.
  prefs: []
  type: TYPE_NORMAL
- en: For fun, let's train it again and to see how the same logical model performs
    on MNIST.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the model to MNIST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our previous MNIST model achieved 98.3% validation accuracy and, as you might
    have noticed, the closer you get to 100%, the more difficult it is to improve
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our CIFAR-10 is trained on a different task than MNIST, but let''s see how
    it performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the graph for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – MNIST, losses](img/Figure_6.19_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – MNIST, losses
  prefs: []
  type: TYPE_NORMAL
- en: I wish every task was as easy as MNIST!
  prefs: []
  type: TYPE_NORMAL
- en: 'Out of curiosity, these are the activations of the first layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – MNIST, activations of the first convolutional level ](img/Figure_6.20_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – MNIST, activations of the first convolutional level
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, many channels are activated, and they easily detect the most
    important features of the numbers.
  prefs: []
  type: TYPE_NORMAL
- en: This could be an excellent time for you to try the code in GitHub and experiment
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: Now it's your turn!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have some time, you should really experiment with a public dataset, or
    even create your own dataset and train a neural network from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: If you are out of ideas, you could use CIFAR-100.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that training a neural network usually is not linear—you might have
    to guess what can help you, or you might try many different things. And remember
    to repeat, going back and forth, because while your model evolves, the importance
    of different techniques and different layers can change.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This has been a very practical chapter, showing one way to proceed when training
    a neural network. We started from a big model, achieving 69.7% validation accuracy,
    and then we reduced its size and added some layers to increase the number of non-linear
    activations. We used batch normalization to equalize the contribution of all the
    channels and then we learned about early stopping, which helped us to decide when
    to stop the training.
  prefs: []
  type: TYPE_NORMAL
- en: After learning how to automatically stop the training, we applied it immediately
    with data augmentation, which increases not only the size of the dataset but also
    the number of epochs required to properly train the network. We then introduced
    `Dropout` and `SpatialDropout2D`, a powerful way to reduce overfitting, though
    not always easy to use.
  prefs: []
  type: TYPE_NORMAL
- en: We ended up with a network achieving 87.8% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will train a neural network that will be able to drive
    a car on an empty track!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After this chapter, you will be able to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we want to use more layers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is a network with more layers automatically slower than a shallower one?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we know how to stop training the model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which Keras function can we use to stop the training before the model starts
    to overfit?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you normalize the channels?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you effectively make your dataset bigger and more difficult?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does dropout make your model more robust?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you use data augmentation, would you expect the training to become slower
    or faster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you use dropout, would you expect the training to become slower or faster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
