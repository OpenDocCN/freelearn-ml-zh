- en: '*Chapter 6*: Improving Your Neural Network'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：改进你的神经网络'
- en: In [*Chapter 4*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091), *Deep Learning
    with Neural Networks*, we designed a network that is able to achieve almost 93%
    accuracy in the training dataset, but that translated to less than 66% accuracy
    in the validation dataset.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091)“使用神经网络的深度学习”中，我们设计了一个能够在训练数据集中达到几乎93%准确率的网络，但它在验证数据集中的准确率却低于66%。
- en: In this chapter, we will continue working on that neural network, with the aim
    to improve the validation accuracy significantly. Our goal is to reach at least
    80% validation accuracy. We will apply some of the knowledge acquired in [*Chapter
    5*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118), *Deep Learning Workflow*,
    and we will also learn new techniques that will help us very much, such as batch
    normalization.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续改进那个神经网络，目标是显著提高验证准确率。我们的目标是达到至少80%的验证准确率。我们将应用在[*第5章*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118)“深度学习工作流程”中获得的一些知识，我们还将学习一些对我们非常有帮助的新技术，例如批量归一化。
- en: 'We will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Reducing the number of parameters
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过数据增强减少参数数量
- en: Increasing the size of the network and the number of layers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加网络大小和层数
- en: Understanding batch normalization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解批量归一化
- en: Improving validation with early stopping
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提前停止提高验证准确率
- en: Virtually increasing the dataset size with data augmentation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过数据增强几乎增加数据集大小
- en: Improving validation accuracy with dropout
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用dropout提高验证准确率
- en: Improving validation accuracy with spatial dropout
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用空间dropout提高验证准确率
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The full source code for the chapter can be found here: [https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter6](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter6)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的完整源代码可以在以下位置找到：[https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter6](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter6)
- en: 'This chapter requires the following software prerequisites, and a basic knowledge
    of the following would help to understand the chapter better:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要以下软件先决条件，以及以下基本知识将有助于更好地理解本章：
- en: Python 3.7
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.7
- en: The NumPy module
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy模块
- en: The Matplotlib module
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib模块
- en: The TensorFlow module
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow模块
- en: The Keras module
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras模块
- en: The OpenCV-Python module
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV-Python模块
- en: A recommended GPU
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐的GPU
- en: 'The Code in Action videos for this chapter can be found here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的“代码实战”视频可以在以下位置找到：
- en: '[https://bit.ly/3dGIdJA](https://bit.ly/3dGIdJA)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3dGIdJA](https://bit.ly/3dGIdJA)'
- en: A bigger model
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更大的模型
- en: Training your own neural network is an art; you need intuition, some luck, a
    lot of patience, and all the knowledge and help that you can find. You will also
    need money and time to either buy a faster GPU, use clusters to test more configurations,
    or pay to get a better dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 训练自己的神经网络是一种艺术；你需要直觉、一些运气、大量的耐心，以及你能找到的所有知识和帮助。你还需要资金和时间，要么购买更快的GPU，使用集群测试更多配置，或者付费获取更好的数据集。
- en: 'But there are no real recipes. That said, we will divide our journey into two
    phases, as explained in [*Chapter 5*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118),
    *Deep Learning Workflow*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但没有真正的食谱。话虽如此，我们将把我们的旅程分为两个阶段，如[*第5章*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118)“深度学习工作流程”中所述：
- en: Overfitting the training dataset
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合训练数据集
- en: Improving generalization
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高泛化能力
- en: We will start from where we left off in [*Chapter 4*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091),
    *Deep Learning with Neural Networks*, with our basic model reaching 66% validation
    accuracy on CIFAR-10, and then we will improve it significantly, first to make
    it faster, and then to make it more precise.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从[*第4章*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091)“使用神经网络的深度学习”中我们留下的地方开始，我们的基本模型在CIFAR-10上达到了66%的验证准确率，然后我们将显著改进它，首先是让它更快，然后是让它更精确。
- en: The starting point
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 起始点
- en: 'The following is the model that we developed in [*Chapter 4*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091),
    *Deep Learning with Neural Networks*, a model that overfits the dataset because
    it achieves a high training accuracy value at relatively low validation accuracy:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们[*第4章*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091)“使用神经网络的深度学习”中开发的模型，该模型由于在相对较低的验证准确率下实现了较高的训练准确率，因此过拟合了数据集：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It is a shallow but relatively big model as it has the following number of
    parameters:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个浅但相对较大的模型，因为它有以下数量的参数：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We previously trained it for 12 epochs, with these results:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前训练了12个epoch，结果如下：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The training accuracy is actually good enough for us (here, in this run, it
    is higher than in [*Chapter 5*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118),
    *Deep Learning Workflow*, mostly because of randomness), but the validation accuracy
    is low too. It is overfitting. So, we could even keep it as a starting point,
    but it would be nice to tune it a bit and see whether we can do better or make
    it faster.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 训练准确率实际上对我们来说已经足够好了（在这里，在这个运行中，它高于[*第5章*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118)，“深度学习工作流程”，主要是因为随机性），但验证准确率也很低。它是过拟合的。因此，我们甚至可以将其作为起点，但最好对其进行一点调整，看看我们是否可以做得更好或使其更快。
- en: 'We should also keep an eye on five epochs, as we might do some tests on fewer
    epochs, to speed up the whole process:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该关注五个epoch，因为我们可能会在较少的epoch上进行一些测试，以加快整个过程：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When you use fewer epochs, you are betting on being able to understand the evolution
    of the curves, so you are trading speed of development for accuracy of your choices.
    Sometimes, this is fine, but sometimes it is not.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用较少的epoch时，你是在赌自己能够理解曲线的演变，因此你是在用开发速度换取选择准确性。有时，这很好，但有时则不然。
- en: Our model is too big, so we will start reducing its size and speeding up the
    training a bit.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型太大，所以我们将开始减小其尺寸并稍微加快训练速度。
- en: Improving the speed
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高速度
- en: Our model is not just very big – in fact, it is too big. The second convolutional
    layer has 256 filters and, combined with the 512 neurons of the dense layer, they
    use a high number of parameters. We can do better. We know that we could split
    them into layers of 128 filters, and this would save almost half of the parameters,
    as the dense layer now needs half of the connections.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式不仅非常大——事实上，它太大了。第二个卷积层有256个过滤器，与密集层的512个神经元结合，它们使用了大量的参数。我们可以做得更好。我们知道我们可以将它们分成128个过滤器的层，这将节省几乎一半的参数，因为密集层现在只需要一半的连接。
- en: 'We can try that. We learned in [*Chapter 4*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091),
    *Deep Learning with Neural Networks*, that to not lose resolution after a convolution,
    we can use padding in the *same* way on both the layers (dense layers omitted),
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试一下。我们在[*第4章*](B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091)，“使用神经网络的深度学习”中了解到，为了在卷积后不丢失分辨率，我们可以在两层（密集层省略）上以相同的方式使用填充，如下所示：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, we can see that the number of parameters now is lower:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到现在的参数数量更低：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s check the full results:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查完整的结果：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Nice! It is faster, the accuracy went up slightly, and also, the validation
    improved!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！它更快了，准确率略有提高，而且，验证也提高了！
- en: 'Let''s do the same on the first layer, but this time without increasing the
    resolution so as not to increase the parameters, since, between two convolutional
    layers, the gain is lower:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在第一层做同样的操作，但这次不增加分辨率，以免增加参数，因为，在两个卷积层之间，增益较低：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When we try this, we get these results:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们尝试这样做时，我们得到以下结果：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is similar to before, though the validation accuracy improved slightly.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前类似，尽管验证准确率略有提高。
- en: Next, we will add more layers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加更多层。
- en: Increasing the depth
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加深度
- en: The previous model is actually an excellent starting point.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的模型实际上是一个很好的起点。
- en: 'But we will add more layers, to increase the number of non-linear activations
    and to be able to learn more complex functions. This is the model (dense layers
    omitted):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们将添加更多层，以增加非线性激活的数量，并能够学习更复杂的函数。这是模型（密集层省略）：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is the result:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The network is now significantly slower and the accuracy went down (maybe because
    more epochs are required), but the validation accuracy improved.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 网络现在明显变慢，准确率下降（可能因为需要更多的epoch），但验证准确率提高了。
- en: 'Let''s try now to reduce the dense layers, as follows (convolutional layers
    omitted):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在尝试减少密集层，如下所示（卷积层省略）：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we have fewer parameters:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有更少的参数：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'But something very bad happened:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但发生了一件非常糟糕的事情：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Both the validations dropped! In fact, they are now 10%, or if you prefer, the
    network is now producing a random result—it did not learn!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 两个验证都下降了！事实上，它们现在为10%，或者如果你愿意，网络现在正在产生随机结果——它没有学习！
- en: 'You might conclude that we broke it. In fact, that is not the case. It''s enough
    to run it again, using the randomness to our advantage, and our network learns
    as expected:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会得出结论说我们把它搞坏了。实际上并非如此。只需再次运行它，利用随机性来利用它，我们的网络就会按预期学习：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: However, this is not a very good sign. This might be due to the increase in
    layers, as networks with more layers are more difficult to train, due to the fact
    that the original input can have problems in terms of being propagated to the
    upper layers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是一个好兆头。这可能是由于层数的增加，因为具有更多层的网络更难训练，因为原始输入在向上层传播时可能存在问题。
- en: 'Let''s check the graph:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下图表：
- en: '![Figure 6.1 – Losses and accuracies graph](img/Figure_6.1_B16322.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 损失和准确率图](img/Figure_6.1_B16322.jpg)'
- en: Figure 6.1 – Losses and accuracies graph
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 损失和准确率图
- en: You can see that while the training loss (blue line) keeps decreasing, the validation
    loss (orange line) after some epochs starts to increase. As explained in [*Chapter
    5*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118)*,* *Deep Learning Workflow*,
    this means that the model is overfitting. This is not necessarily the best model,
    but we will continue developing it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，虽然训练损失（蓝色线）持续下降，但经过一些个epoch后，验证损失（橙色线）开始上升。正如在 [*第五章*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118)
    的 *深度学习工作流程* 中解释的那样，这意味着模型过拟合了。这不一定是最优模型，但我们将继续开发它。
- en: In the next section, we will simplify this model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将简化这个模型。
- en: A more efficient network
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个更高效的网络
- en: Training the previous model requires 686 seconds on my laptop, and achieves
    a validation accuracy of 74.5%, and a training accuracy of 91.4%. Ideally, to
    improve the efficiency, we want to keep accuracy at the same level while reducing
    the training time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的笔记本电脑上训练先前的模型需要 686 秒，并达到 74.5% 的验证准确率和 91.4% 的训练准确率。理想情况下，为了提高效率，我们希望在保持准确率不变的同时减少训练时间。
- en: 'Let''s check some of the convolutional layers:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一些卷积层：
- en: '![Figure 6.2 – First convolutional layer, 32 channels](img/Figure_6.2_B16322.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 第一卷积层，32 个通道](img/Figure_6.2_B16322.jpg)'
- en: Figure 6.2 – First convolutional layer, 32 channels
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 第一卷积层，32 个通道
- en: 'We have already seen these activation graphs in [*Chapter 5*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118),
    *Deep Learning Workflow*, and we know that channels that are black do not achieve
    a big activation, so they don''t contribute much to the result. In practice, it
    looks like half of the channels are not in use. Let''s try to halve the number
    of channels in every convolutional layer:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 [*第五章*](B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118) 的 *深度学习工作流程* 中看到了这些激活图，并且我们知道黑色通道没有达到大的激活，因此它们对结果贡献不大。在实践中，看起来一半的通道都没有使用。让我们尝试在每个卷积层中将通道数减半：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is what we get as the result:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的结果：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As expected, the number of parameters is now much less, and training is much
    faster:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，参数数量现在少得多，训练速度也快得多：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here we see that we lost some accuracy also, but not too much:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们失去了一些准确率，但不是太多：
- en: '![Figure 6.3 – First convolutional layer, 16 channels](img/Figure_6.3_B16322.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – 第一卷积层，16 个通道](img/Figure_6.3_B16322.jpg)'
- en: Figure 6.3 – First convolutional layer, 16 channels
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 第一卷积层，16 个通道
- en: 'Now it''s a bit better. Let''s check the second layer:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在好一些了。让我们检查第二层：
- en: '![Figure 6.4 – Second convolutional layer, 16 channels](img/Figure_6.4_B16322.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 第二卷积层，16 个通道](img/Figure_6.4_B16322.jpg)'
- en: Figure 6.4 – Second convolutional layer, 16 channels
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 第二卷积层，16 个通道
- en: 'This is also better. Let''s check the fourth convolutional layer:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这也好一些。让我们检查一下第四卷积层：
- en: '![Figure 6.5 – Fourth convolutional layer, 64 channels](img/Figure_6.5_B16322.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – 第四卷积层，64 个通道](img/Figure_6.5_B16322.jpg)'
- en: Figure 6.5 – Fourth convolutional layer, 64 channels
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 第四卷积层，64 个通道
- en: 'It seems a bit empty. Let''s halve the third and fourth layers:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来有点空。让我们将第三层和第四层减半：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We then get these results:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The training accuracy went down, but the validation accuracy is still fine:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 训练准确率下降了，但验证准确率仍然不错：
- en: '![Figure 6.6 – Fourth convolutional layer, 32 channels](img/Figure_6.6_B16322.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 第四卷积层，32 个通道](img/Figure_6.6_B16322.jpg)'
- en: Figure 6.6 – Fourth convolutional layer, 32 channels
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 第四卷积层，32 个通道
- en: 'Let''s check the sixth convolutional layer:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查第六卷积层：
- en: '![Figure 6.7 – Sixth convolutional layer, 128 channels](img/Figure_6.7_B16322.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 第六卷积层，128 个通道](img/Figure_6.7_B16322.jpg)'
- en: Figure 6.7 – Sixth convolutional layer, 128 channels
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 第六卷积层，128个通道
- en: 'It''s a bit empty. Let''s also halve the last two convolutional layers:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 它有点空。让我们也将最后两个卷积层减半：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It is much smaller, with these results:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 它要小得多，这些结果是：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It still looks good. Let''s check the activations:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来仍然不错。让我们检查一下激活情况：
- en: '![Figure 6.8 – Sixth convolutional layer, 64 channels](img/Figure_6.8_B16322.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 第六卷积层，64个通道](img/Figure_6.8_B16322.jpg)'
- en: Figure 6.8 – Sixth convolutional layer, 64 channels
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 第六卷积层，64个通道
- en: You can see that now, many channels are activated, which hopefully is an indication
    that the neural network is better at taking advantage of its resources.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到现在，许多通道被激活了，这希望是神经网络更好地利用其资源的指示。
- en: Comparing this model with the one built in the previous section, you can see
    that this model can be trained in a bit less than half the time, the validation
    accuracy is almost unchanged, and the training accuracy decreased a bit, but not
    much. So, it is indeed more efficient.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个模型与上一节中构建的模型进行比较，你可以看到这个模型可以在不到一半的时间内训练完成，验证准确率几乎未变，训练准确率略有下降，但不是很多。所以，它确实更有效率。
- en: In the next section, we will discuss batch normalization, a layer very common
    on modern neural networks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论批量归一化，这是一个在现代神经网络中非常常见的层。
- en: Building a smarter network with batch normalization
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用批量归一化构建更智能的网络
- en: We normalize the input that we provide to the network, constraining the range
    from 0 to 1, so it could be beneficial to do that also in the middle of the network.
    This is called **batch normalization**, and it does wonders!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提供给网络的输入进行归一化，将范围限制在0到1之间，因此在中部网络中也这样做可能是有益的。这被称为**批量归一化**，它确实很神奇！
- en: In general, you should add the batch normalization after the output that you
    want to normalize, and before the activation, but adding it after the activation
    might provide faster performance, and this is what we will do.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你应该在想要归一化的输出之后、激活之前添加批量归一化，但添加在激活之后可能会提供更快的性能，这正是我们将要做的。
- en: 'This is the new code (dense layers omitted):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是新的代码（省略了密集层）：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The number of parameters increased just a bit:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数数量仅略有增加：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This is the result:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Not bad, even if unfortunately, now it is much slower. But we can add even
    more batch normalization, to see whether this improves the situation:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 还不错，尽管不幸的是，现在它要慢得多。但我们可以添加更多的批量归一化，看看是否可以改善情况：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Yes, both the accuracies improved. We are actually very close to our initial
    goal of having 80% accuracy. But let's go further and see what we can do.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，准确率都有所提高。我们实际上非常接近我们最初的80%准确率的目标。但让我们更进一步，看看我们能做什么。
- en: Until now, we only used ReLU activation, but even if it is used very much, it's
    not the only one. Keras supports a variety of activations, and sometimes it is
    worth experimenting with. We will stick to ReLU.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了ReLU激活函数，但即使它被广泛使用，它也不是唯一的。Keras支持多种激活函数，有时进行实验是值得的。我们将坚持使用ReLU。
- en: 'Let''s check some activations:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一些激活情况：
- en: '![Figure 6.9 – Second convolutional layer, 16 channels, batch normalization](img/Figure_6.9_B16322.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – 第二卷积层，16个通道，批量归一化](img/Figure_6.9_B16322.jpg)'
- en: Figure 6.9 – Second convolutional layer, 16 channels, batch normalization
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 第二卷积层，16个通道，批量归一化
- en: Now, all the channels of the second layer are learning. Very good!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，第二层的所有通道都在学习。非常好！
- en: 'Here is the result of the fourth layer:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第四层的成果：
- en: '![Figure 6.10 – Fourth convolutional layer, 32 channels, batch normalization](img/Figure_6.10_B16322.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – 第四卷积层，32个通道，批量归一化](img/Figure_6.10_B16322.jpg)'
- en: Figure 6.10 – Fourth convolutional layer, 32 channels, batch normalization
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 第四卷积层，32个通道，批量归一化
- en: 'Here is the result of the sixth layer:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第六层的成果：
- en: '![Figure 6.11 – Sixth convolutional layer, 64 channels, batch normalization](img/Figure_6.11_B16322.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – 第六卷积层，64个通道，批量归一化](img/Figure_6.11_B16322.jpg)'
- en: Figure 6.11 – Sixth convolutional layer, 64 channels, batch normalization
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 第六卷积层，64个通道，批量归一化
- en: It is starting to look good!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 它开始看起来不错了！
- en: 'Let''s try to visualize the effect of batch normalization on the activations
    of the first layer before and after batch normalization:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试可视化批量归一化对第一层在批量归一化前后激活情况的影响：
- en: '![Figure 6.12 – First convolutional layer, 16 channels, before and after batch
    normalization](img/Figure_6.12_B16322.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – 第一卷积层，16个通道，批量归一化前后的激活情况](img/Figure_6.12_B16322.jpg)'
- en: Figure 6.12 – First convolutional layer, 16 channels, before and after batch
    normalization
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – 第一卷积层，16个通道，批标准化前后的对比
- en: You can see that the intensity of the channels is now more uniform; there are
    no longer channels without activity and channels with very strong activations.
    However, the channels that are inactive are still without real information.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，通道的强度现在更加均匀；不再有既无活动又没有非常强烈的激活的通道。然而，那些不活跃的通道仍然没有真实信息。
- en: 'Let''s also check the second layer:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也检查第二层：
- en: '![Figure 6.13 – Second convolutional layer, 16 channels, before and after batch
    normalization](img/Figure_6.13_B16322.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – 第二卷积层，16个通道，批标准化前后的对比](img/Figure_6.13_B16322.jpg)'
- en: Figure 6.13 – Second convolutional layer, 16 channels, before and after batch
    normalization
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 第二卷积层，16个通道，批标准化前后的对比
- en: Here, it is maybe less evident, but you can still see that the difference between
    the channels is reduced, as clearly, they have been normalized. Intuitively, this
    helps propagate weaker signals through layers, and it has some regularization
    effect, which results in better validation accuracy.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可能不太明显，但你仍然可以看到通道之间的差异减少了，因为很明显，它们已经被归一化。直观上看，这有助于通过层传播较弱的信号，并且它有一些正则化效果，这导致更好的验证准确率。
- en: Now that we have talked about batch normalization, it is time to discuss further
    what a batch is and what implications the size of a batch has.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了批标准化，现在是时候进一步讨论什么是批，以及批的大小有什么影响。
- en: Choosing the right batch size
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择合适的批大小
- en: During training, we have a high number of samples, typically from thousands
    to millions. If you remember, the optimizer will compute the loss function and
    update the hyperparameters to try to reduce the loss. It could do this after every
    sample, but the result could be noisy, with continuous changes that can slow down
    the training. At the opposite end, the optimizer could update the hyperparameters
    only once per epoch – for example, using the average of the gradients – but this
    typically leads to bad generalization. Usually, there is a range on the batch
    size that performs better than these two extremes, but unfortunately, it depends
    on the specific neural network.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们有大量的样本，通常从几千到几百万。如果你还记得，优化器将计算损失函数并更新超参数以尝试减少损失。它可以在每个样本之后这样做，但结果可能会很嘈杂，连续的变化可能会减慢训练。在另一端，优化器可能只在每个epoch更新一次超参数——例如，使用梯度的平均值——但这通常会导致泛化不良。通常，批大小有一个范围，比这两个极端表现更好，但不幸的是，它取决于特定的神经网络。
- en: A larger batch size can marginally improve the training time on a GPU, but if
    your model is big, you might find that the memory of your GPU is a limit to how
    big your batch size can be.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的批量大小的确可以略微提高GPU上的训练时间，但如果你的模型很大，你可能会发现你的GPU内存是批量大小的限制因素。
- en: Batch normalization is also affected by the size of the batch, as small batches
    reduce its effectiveness (as there is not enough data do to a proper normalization).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 批标准化也受到批量大小的影響，因为小批量会降低其有效性（因为没有足够的数据进行适当的归一化）。
- en: Given these considerations, the best thing to do is to just try. Normally, you
    can try using 16, 32, 64, and 128, and eventually extend the range if you see
    that the best value is at the limit of the range.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些因素，最好的做法是尝试。通常，你可以尝试使用16、32、64和128，如果看到最佳值在范围的极限，最终可以扩展范围。
- en: 'As we have seen, the best batch size can improve accuracy and possibly increase
    the speed, but there is another technique that can help us to improve the validation
    accuracy while speeding up, or at least simplifying, the training: early stopping.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，最佳批大小可以提高准确率并可能提高速度，但还有一种技术可以帮助我们在加快速度的同时提高验证准确率，或者至少简化训练：提前停止。
- en: Early stopping
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提前停止
- en: When should we stop training? That's a good question! Ideally, you want to stop
    at the minimum validation error. While you cannot know this in advance, you can
    check the losses and get an idea of how many epochs you need. However, when you
    train your network, sometimes you need more epochs depending on how you tune your
    model, and it is not simple to know in advance when to stop.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在什么时候停止训练？这是一个好问题！理想情况下，你希望在最小验证错误时停止。虽然你事先不知道这一点，但你可以通过检查损失来获得需要多少个epoch的概览。然而，当你训练你的网络时，有时你需要更多的epoch，这取决于你如何调整你的模型，而且事先知道何时停止并不简单。
- en: We already know that we can use `ModelCheckpoint`, a callback of Keras, to save
    the model with the best validation error seen during training.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道我们可以使用`ModelCheckpoint`，这是Keras的一个回调，在训练过程中保存具有最佳验证错误的模型。
- en: 'But there is also another very useful callback, `EarlyStopping`, which stops
    the training when a predefined set of conditions happen:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有一个非常有用的回调，`EarlyStopping`，当预定义的一组条件发生时，它会停止训练：
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The most important parameters to configure early stopping are the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 配置提前停止最重要的参数如下：
- en: '`monitor`: This decides which parameter to monitor, by default: validation
    loss.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`monitor`: 这决定了要监控哪个参数，默认为验证损失。'
- en: '`min_delta`: If the difference in validation loss between epochs is below this
    value, the loss is considered to not have changed.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_delta`: 如果epoch之间的验证损失差异低于此值，则认为损失没有变化。'
- en: '`patience`: This is the number of epochs with no validation improvement to
    allow before stopping the training.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`耐心`: 这是允许在停止训练之前没有验证改进的epoch数量。'
- en: '`verbose`: This is instructing Keras to provide more information.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`: 这是指示Keras提供更多信息。'
- en: The reason why we need early stopping is that with data augmentation and dropout,
    we will need many more epochs, and instead of guessing when it is time to stop,
    we will use early stopping to do that for us.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要提前停止的原因是，在数据增强和dropout的情况下，我们需要更多的epoch，而不是猜测何时停止，我们将使用提前停止来为我们完成这项工作。
- en: Let's talk now about data augmentation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来谈谈数据增强。
- en: Improving the dataset with data augmentation
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据增强改进数据集
- en: It's time to use data augmentation, and basically, increase the size of our
    dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候使用数据增强了，基本上，增加我们数据集的大小。
- en: From this moment, we will no longer care about the accuracy of the training
    dataset, as this technique will reduce it, but we will focus on the validation
    accuracy, which is expected to improve.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，我们将不再关心训练数据集的精度，因为这项技术会降低它，但我们将关注验证精度，预计它会提高。
- en: We also expect to need more epochs, as our dataset is now more difficult, so
    we will now set the epochs to `500` (though we don't plan to reach it) and use
    `EarlyStopping` with a patience of `7`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也预计需要更多的epoch，因为我们的数据集现在更难了，所以我们将epoch设置为`500`（尽管我们并不打算达到它）并使用具有`7`个patience的`EarlyStopping`。
- en: 'Let''s try with this augmentation:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试这个增强：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You should take care not to overdo things because the network might learn a
    dataset too different from validation, and in this case, you will see the validation
    accuracy stuck at 10%.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该注意不要过度操作，因为网络可能会学习到一个与验证集差异很大的数据集，在这种情况下，您将看到验证精度停滞在10%。
- en: 'This is the result:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '[PRE28]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Early stopping interrupted the training after `31` epochs, and we reached a
    validation accuracy of above 84% –not bad. As expected, we need many more epochs
    now. This is a graph of the losses:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止在`31`个epoch后中断了训练，我们达到了84%以上的验证精度——还不错。正如预期的那样，我们现在需要更多的epoch。这是损失图：
- en: '![Figure 6.14 – Losses with data augmentation and early stopping](img/Figure_6.14_B16322.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图6.14 – 使用数据增强和提前停止的损失](img/Figure_6.14_B16322.jpg)'
- en: Figure 6.14 – Losses with data augmentation and early stopping
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 使用数据增强和提前停止的损失
- en: You can see how the training accuracy kept increasing, while the validation
    accuracy at some point decreased. The network is still overfitting a bit.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到训练精度一直在增加，而验证精度在某些时候下降了。网络仍然有点过拟合。
- en: 'Let''s check the activations of the first convolutional layer:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查第一卷积层的激活：
- en: '![Figure 6.15 – First convolutional layer, 16 channels, with data augmentation
    and early stopping](img/Figure_6.15_B16322.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图6.15 – 使用数据增强和提前停止的第一卷积层，16个通道](img/Figure_6.15_B16322.jpg)'
- en: Figure 6.15 – First convolutional layer, 16 channels, with data augmentation
    and early stopping
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 – 使用数据增强和提前停止的第一卷积层，16个通道
- en: It's marginally better, but it can probably improve again.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 它略有改善，但可能还会再次提高。
- en: 'We can try to augment the data a little more:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试稍微增加数据增强：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This is the result:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This model is slower and less accurate. Let''s see the graph:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型速度较慢且精度较低。让我们看看图表：
- en: '![Figure 6.16 – Losses with more data augmentation and early stopping](img/Figure_6.16_B16322.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图6.16 – 增加数据增强和提前停止的损失](img/Figure_6.16_B16322.jpg)'
- en: Figure 6.16 – Losses with more data augmentation and early stopping
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – 增加数据增强和提前停止的损失
- en: Maybe it needs more patience. We will stick to the previous data augmentation
    then.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要更多的耐心。我们将坚持之前的数据增强。
- en: In the next section, we will analyze a simple but effective way to increase
    the validation accuracy using the dropout layer.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将分析一种简单但有效的方法，通过使用dropout层来提高验证准确率。
- en: Improving the validation accuracy with dropout
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用dropout提高验证准确率
- en: A source of overfitting is the fact that the neural network relies more on some
    neurons to draw its conclusions, and if those neurons are wrong, the network is
    wrong. One way to reduce this problem is simply to randomly shut down some neurons
    during training while keeping them working normally during inference. In this
    way, the neural network learns to be more resistant to errors and to generalize
    better. This mechanism is called **dropout**, and obviously, Keras supports it.
    Dropout increases the training time, as the network needs more epochs to converge.
    It might also require a bigger network, as some neurons are randomly deactivated
    during training. It is also more useful when the dataset is not very big for the
    network, as it is more likely to overfit. In practice, as dropout is meant to
    reduce overfitting, it brings little benefit if your network is not overfitting.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合的一个来源是神经网络更依赖于一些神经元来得出结论，如果这些神经元出错，网络也会出错。减少这种问题的一种方法是在训练期间随机关闭一些神经元，而在推理期间保持它们正常工作。这样，神经网络就能学会更加抵抗错误，更好地泛化。这种机制被称为**dropout**，显然，Keras支持它。Dropout会增加训练时间，因为网络需要更多的epoch来收敛。它可能还需要更大的网络，因为在训练期间一些神经元会被随机关闭。当数据集对于网络来说不是很大时，它更有用，因为它更有可能过拟合。实际上，由于dropout旨在减少过拟合，如果你的网络没有过拟合，它带来的好处很小。
- en: A typical value of dropout for dense layers is 0.5, though we might use a bit
    less, as our model is not overfitting much. We will also increase the *patience*
    to `20`, as the model needs more epochs to be trained now and the validation loss
    can fluctuate for more time.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于密集层，dropout的典型值是0.5，尽管我们可能使用略小的值，因为我们的模型过拟合不多。我们还将增加*耐心*到`20`，因为现在模型需要更多的epoch来训练，验证损失可能会波动更长的时间。
- en: 'Let''s try to add some dropout to the dense layers:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在密集层中添加一些dropout：
- en: '[PRE31]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This is the result:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '[PRE32]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: A bit disappointing. It took a lot of time to train, with small gains. We assume
    that our dense layers are a bit small.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点令人失望。训练花费了很长时间，但只有微小的进步。我们假设我们的密集层有点小。
- en: 'Let''s increase the size of the layers by 50%, and we will also increase the
    dropout of the first dense layer while reducing the dropout of the second one:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将层的尺寸增加50%，同时增加第一个密集层中的dropout，并减少第二个密集层中的dropout：
- en: '[PRE33]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It is, of course, bigger, as we can see here:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，它更大，正如我们在这里可以看到的：
- en: '[PRE34]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'It has a slightly better result:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 它的结果略好一些：
- en: '[PRE35]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As we improve the validation accuracy, even small gains are difficult to achieve.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们提高验证准确率，即使是小的进步也变得困难。
- en: 'Let''s check the graph:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下图表：
- en: '![Figure 6.17 – Losses with more data augmentation and dropout on dense layers](img/Figure_6.17_B16322.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – 在密集层上使用更多数据增强和dropout的损失](img/Figure_6.17_B16322.jpg)'
- en: Figure 6.17 – Losses with more data augmentation and dropout on dense layers
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – 在密集层上使用更多数据增强和dropout的损失
- en: There is a bit of overfitting, so let's try to fix it. We can also use `Dropout`
    in the convolutional layers.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一点过拟合，所以让我们尝试修复它。我们也可以在卷积层中使用`Dropout`。
- en: 'Let''s try this:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试这个：
- en: '[PRE36]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This is the disappointing result:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这是令人失望的结果：
- en: '[PRE37]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The network did not improve!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 网络没有改进！
- en: Here, we see an interesting case—the validation accuracy is significantly higher
    than the training accuracy. How is that possible?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们看到一个有趣的情况——验证准确率显著高于训练准确率。这是怎么可能的？
- en: 'Assuming that your split is correct (for example, you don''t have a validation
    set that is too easy or containing images too similar to the training dataset),
    two factors can create this situation:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的分割是正确的（例如，你没有包含与训练数据集太相似图像的验证集），两个因素可以造成这种情况：
- en: Data augmentation can potentially make the training dataset harder than the
    validation dataset.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强可能会使训练数据集比验证数据集更难。
- en: Dropout is active during the training phase and deactivated during the prediction
    phase, so this means that the training dataset can be significantly harder than
    the validation dataset.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout在训练阶段是激活的，而在预测阶段是关闭的，这意味着训练数据集可以比验证数据集显著更难。
- en: In our case, the culprit is the dropout. You don't necessarily need to avoid
    this situation, if it is justified, but in our case, the validation accuracy went
    down, so we need to fix our dropout, or maybe increase the size of the network.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这个例子中，罪魁祸首是`Dropout`。你不必一定避免这种情况，如果它是合理的，但在这个例子中，验证准确率下降了，所以我们需要修复我们的`Dropout`，或者也许增加网络的大小。
- en: 'I find `Dropout` more difficult to use with the convolutional layers, and I
    would personally not use a big dropout in that case. Here, there are some guidelines:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现`Dropout`在卷积层中使用起来更困难，我个人在那个情况下不会使用大的`Dropout`。这里有一些指导方针：
- en: No batch normalization right after `Dropout`, as normalization would suffer.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要在`Dropout`之后立即使用批量归一化，因为归一化会受到影响。
- en: '`Dropout` is more effective after `MaxPooling` than before.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dropout`在`MaxPooling`之后比之前更有效。'
- en: '`Dropout` after a convolutional layer drops single pixels, but `SpatialDropout2D`
    drops channels, and it is recommended on the first few layers at the beginning
    of the neural network.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在卷积层之后的`Dropout`会丢弃单个像素，但`SpatialDropout2D`会丢弃通道，并且建议在神经网络开始的第一几个层中使用。
- en: I ran another few (long!) experiments and I decided to increase the size of
    the convolutional layers, reduce the dropout, and use `Spatial Dropout` in a couple
    of layers. I ended up with this neural network, which is what I consider my final
    version.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我又进行了一些（长！）实验，并决定增加卷积层的大小，减少`Dropout`，并在几个层中使用`Spatial Dropout`。最终我得到了这个神经网络，这是我认为的最终版本。
- en: 'This is the code of the convolutional layers:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这是卷积层的代码：
- en: '[PRE38]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'These are the results:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是结果：
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'There is an improvement in the validation accuracy:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 验证准确率有所提高：
- en: '![Figure 6.18 – Losses with more data augmentation and dropout on dense and
    convolutional layers](img/Figure_6.18_B16322.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图6.18 – 在密集和卷积层上使用更多数据增强和`Dropout`的损失](img/Figure_6.18_B16322.jpg)'
- en: Figure 6.18 – Losses with more data augmentation and dropout on dense and convolutional
    layers
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 – 在密集和卷积层上使用更多数据增强和`Dropout`的损失
- en: Congratulations! Now you have an idea of how to train a neural network, and
    feel free to experiment and go crazy! Every task is different, and the possibilities
    are really endless.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在你有了如何训练神经网络的思路，并且可以自由地实验和发挥创意！每个任务都是不同的，可能性真的是无限的。
- en: For fun, let's train it again and to see how the same logical model performs
    on MNIST.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了好玩，让我们再次训练它，看看相同的逻辑模型在MNIST上的表现如何。
- en: Applying the model to MNIST
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型应用于MNIST
- en: Our previous MNIST model achieved 98.3% validation accuracy and, as you might
    have noticed, the closer you get to 100%, the more difficult it is to improve
    the model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前的MNIST模型达到了98.3%的验证准确率，正如你可能已经注意到的，你越接近100%，提高模型就越困难。
- en: 'Our CIFAR-10 is trained on a different task than MNIST, but let''s see how
    it performs:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的CIFAR-10是在与MNIST不同的任务上训练的，但让我们看看它的表现：
- en: '[PRE40]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Here is the graph for it:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的图表：
- en: '![Figure 6.19 – MNIST, losses](img/Figure_6.19_B16322.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图6.19 – MNIST，损失](img/Figure_6.19_B16322.jpg)'
- en: Figure 6.19 – MNIST, losses
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 – MNIST，损失
- en: I wish every task was as easy as MNIST!
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望每个任务都像MNIST一样简单！
- en: 'Out of curiosity, these are the activations of the first layer:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，这是第一层的激活：
- en: '![Figure 6.20 – MNIST, activations of the first convolutional level ](img/Figure_6.20_B16322.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图6.20 – MNIST，第一卷积层的激活](img/Figure_6.20_B16322.jpg)'
- en: Figure 6.20 – MNIST, activations of the first convolutional level
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 – MNIST，第一卷积层的激活
- en: As you can see, many channels are activated, and they easily detect the most
    important features of the numbers.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，许多通道被激活，并且它们很容易检测到数字的最重要特征。
- en: This could be an excellent time for you to try the code in GitHub and experiment
    with it.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个尝试GitHub上的代码并对其进行实验的绝佳时机。
- en: Now it's your turn!
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现在轮到你了！
- en: If you have some time, you should really experiment with a public dataset, or
    even create your own dataset and train a neural network from scratch.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有些时间，你真的应该尝试一个公共数据集，或者甚至创建你自己的数据集，并从头开始训练一个神经网络。
- en: If you are out of ideas, you could use CIFAR-100.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经没有想法了，可以使用CIFAR-100。
- en: Remember that training a neural network usually is not linear—you might have
    to guess what can help you, or you might try many different things. And remember
    to repeat, going back and forth, because while your model evolves, the importance
    of different techniques and different layers can change.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，训练神经网络通常不是线性的——你可能需要猜测什么可以帮助你，或者你可能尝试许多不同的事情。并且记住要反复试验，因为当你的模型发展时，不同技术和不同层的重要性可能会改变。
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This has been a very practical chapter, showing one way to proceed when training
    a neural network. We started from a big model, achieving 69.7% validation accuracy,
    and then we reduced its size and added some layers to increase the number of non-linear
    activations. We used batch normalization to equalize the contribution of all the
    channels and then we learned about early stopping, which helped us to decide when
    to stop the training.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常实用的章节，展示了在训练神经网络时的一种进行方式。我们从一个大模型开始，实现了69.7%的验证准确率，然后我们减小了其尺寸并添加了一些层来增加非线性激活的数量。我们使用了批量归一化来均衡所有通道的贡献，然后我们学习了提前停止，这有助于我们决定何时停止训练。
- en: After learning how to automatically stop the training, we applied it immediately
    with data augmentation, which increases not only the size of the dataset but also
    the number of epochs required to properly train the network. We then introduced
    `Dropout` and `SpatialDropout2D`, a powerful way to reduce overfitting, though
    not always easy to use.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习如何自动停止训练后，我们立即将其应用于数据增强，这不仅增加了数据集的大小，还增加了正确训练网络所需的epoch数量。然后我们介绍了`Dropout`和`SpatialDropout2D`，这是一种强大的减少过拟合的方法，尽管使用起来并不总是容易。
- en: We ended up with a network achieving 87.8% accuracy.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到了一个达到87.8%准确率的网络。
- en: In the next chapter, we will train a neural network that will be able to drive
    a car on an empty track!
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将训练一个能够在空旷的轨道上驾驶汽车的神经网络！
- en: Questions
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'After this chapter, you will be able to answer the following questions:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章之后，你将能够回答以下问题：
- en: Why do we want to use more layers?
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为什么想使用更多层？
- en: Is a network with more layers automatically slower than a shallower one?
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有更多层的网络是否自动比较浅的网络慢？
- en: How do we know how to stop training the model?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何知道何时停止训练模型？
- en: Which Keras function can we use to stop the training before the model starts
    to overfit?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用哪个Keras函数在模型开始过拟合之前停止训练？
- en: How can you normalize the channels?
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何归一化通道？
- en: How can you effectively make your dataset bigger and more difficult?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何有效地使你的数据集更大、更难？
- en: Does dropout make your model more robust?
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: dropout会使你的模型更鲁棒吗？
- en: If you use data augmentation, would you expect the training to become slower
    or faster?
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你使用数据增强，你会期望训练变慢还是变快？
- en: If you use dropout, would you expect the training to become slower or faster?
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你使用dropout，你会期望训练变慢还是变快？
