<html><head></head><body><div><h1 class="header-title">Human Pose Estimation with TensorFlow</h1>
                
            
            
                
<p>In this chapter, we're going to cover human pose estimation with TensorFlow using the DeeperCut algorithm. We will learn single-person and multi-person pose detection using the DeeperCut and ArtTrack models. Later, we will also learn how to use the model with videos and retrain it to use it for the customized images in our projects.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Pose estimation with DeeperCut and ArtTrack</li>
<li>Single-person pose detection</li>
<li>Multi-person pose detection</li>
<li>Videos and retraining</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Pose estimation using DeeperCut and ArtTrack</h1>
                
            
            
                
<p>Human pose estimation is the process of estimating the configuration of the body (pose) from an image or video. It includes landmarks (points), which are similar to joints such as the feet, ankles, chin, shoulder, elbows, hands, head, and so on. We will be doing this automatically using deep learning. If you consider a face, the landmarks are relatively rigid or, rather, relatively constant from face to face, such as the relative position of the eyes to the nose, the mouth to the chin, and so forth.</p>
<p>The following photo provides an example:</p>
<div><img src="img/a2c43966-d8da-481e-aff1-ff313b244730.png" style="width:24.75em;height:18.67em;" width="565" height="429"/></div>
<p>Although the body structure remains the same, our bodies aren't rigid. So, we need to detect the different parts of our body relative to the other parts. For example, detecting the feet relative to the knee is very challenging compared to facial detection. Also, we can move our hands and feet, which can lead to a wide variety of positions. The following picture gives an example:</p>
<div><img src="img/85561526-7b48-492b-b381-eb42bfddb855.png" style="width:17.00em;height:25.42em;" width="386" height="573"/></div>
<p class="mce-root"/>
<p>This was very difficult until we had some breakthroughs in computer vision from different groups around the world. Different code has been developed to carry out pose estimation, but we will cover an algorithm called <strong>DeeperCut</strong>.</p>
<p>You can refer to MPII Human Pose Models (<a href="http://pose.mpi-inf.mpg.de" target="_blank">pose.mpi-inf.mpg.de</a>) for detailed information.</p>
<p>DeeperCut was developed by a group in Germany at the Max Planck Society, in conjunction with Stanford University, who released their algorithm and published papers. It is recommended to checkout their paper <em>DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation</em>, which gives an overview of an earlier algorithm, before DeeperCut, where they talk about how they detected body parts and how they ran an optimization algorithm to achieve good results. You can also refer to their subsequent paper, <em>DeeperCuts</em>: <em>a deeper, stronger and faster multi person pose estimation model</em>, which was published by the same group of authors, as this will cover a lot of the technical details. We will definitely not get exact results, but you can determine things with a reasonable amount of probability.</p>
<p>On the GitHub page, <a href="https://github.com/eldar/pose-tensorflow" target="_blank">https://github.com/eldar/pose-tensorflow</a>, there is the public implementation of their code, which covers DeeperCut and a new version called ArtTrack. It is articulated multi-person tracking in the wild, and you can see the output result in the following photo:</p>
<div><img src="img/aaf57d4e-3c54-4ea2-8635-9385de2c2ab3.png" style="width:32.33em;height:16.50em;" width="897" height="458"/></div>
<p>We are going to run a modified version of the code, which is made to run in the Jupyter Notebook environment and is made for all learning purposes, so it should be a little easier than just getting it straight from GitHub. We will learn exactly how we can run the code and use it in our own projects. All of the pre-trained models are included here: <a href="https://github.com/eldar/pose-tensorflow" target="_blank">https://github.com/eldar/pose-tensorflow.</a></p>


            

            
        
    </div>



  
<div><h1 class="header-title">Single-person pose detection</h1>
                
            
            
                
<p>Now that we have an overview of human pose estimation and the new DeeperCut algorithm, we can run the code for single-person pose detection and check that out in the Jupyter Notebook.</p>
<p>We will start with single-person detection. Before starting, we need to make sure that we are using a clean kernel. You can restart your kernel, or you can use the hotkeys to do the same. You can then press the <em>0</em> key twice when you're in command mode, which is opposed to edit mode when you're actually editing the cells. </p>
<p>Let's start with our single-person detection code, as shown in the following example:</p>
<pre>!pip install pyyaml easydict munkres</pre>
<p>The exclamation mark means execute a shell command. This will install a couple of libraries that you might not have, and if you have Python 3 installed in your system, you might need to change the command to <kbd>pip 3</kbd>.</p>
<p>In the next cell, we will call the <kbd>%pylab notebook</kbd> function, which will allow us to look at images with some useful widgets in the notebook, as well as load some numerical libraries, such as <kbd>numpy</kbd> and so forth. We will do some general imports, such as <kbd>os</kbd>, <kbd>sys</kbd> and <kbd>cv2</kbd>. To do some annotations, we will use <kbd>imageio</kbd> for the <kbd>imread</kbd> function and get everything from <kbd>randint</kbd>. You don't need to import <kbd>numpy</kbd> because we have already used <kbd>%pylab notebook</kbd>, but in case you want to copy and paste this code outside of the notebook, you will need it. Then, we need to import <kbd>tensorflow</kbd>, which already has some glued utilities here that come from the <kbd>pose-tensorflow</kbd> repository. The code, for your reference, is shown in the following example:</p>
<pre>%pylab notebook<br/>import os<br/>import sys<br/>import cv2<br/>from imageio import imread<br/>from random import randint<br/>import numpy as np<br/>import tensorflow as tf<br/>from config import load_config<br/>from nnet.net factory import pose_net</pre>
<p>We then execute the preceding cell.</p>
<p>We will now set up pose prediction, as shown in the following code:</p>
<pre>def setup_pose_prediction(cfg):<br/>    inputs = tf.placeholder(tf.float32, shape=[cfg.batch_size, None, None, 3])<br/><br/>    outputs = pose_net(cfg).test(inputs)<br/><br/>    restorer = tf.train.Saver()<br/><br/>    sess = tf.Session()<br/><br/>    sess.run(tf.global_variables_initializer())<br/>    sess.run(tf.local_variables_initializer())<br/><br/>    # Restore variables from disk.<br/>    restorer.restore(sess, cfg.init_weights)<br/><br/>    return sess, inputs, outputs</pre>
<p>It will start our session and load our model. We will be using a pre-trained model, which you can quickly access from the GitHub repository. The <kbd>tf.Session()</kbd> will start the TensorFlow session and save it to the <kbd>sess</kbd> variable, which we're going to return. Note that when you run this function, it's going to leave the TensorFlow session open, so if you want to move on and do something else, such as load a new model, then you will have to close the session or restart. It's useful here because we're going to be looking at multiple images and it will be slower if you load the session every single time. We will then take the configuration, which loads the corresponding model and variables, and is going to return the necessary values in order to actually run the model.</p>
<p>Then, we extract CNN outputs using the <kbd>extract_cnn_outputs</kbd> function. In the output, we'll get joint locations to know where everything is, exactly relative to something else. We want a nice ordered 2D array where we know the X and Y locations of where the ankles, hands, or shoulders are present. This is demonstrated in the following example:</p>
<pre>def extract_cnn_output(outputs_np, cfg, pairwise_stats = None):<br/>    scmap = outputs_np['part_prob']<br/>    scmap = np.squeeze(scmap)<br/>    locref = None<br/>    pairwise_diff = None<br/>    if cfg.location_refinement:<br/>        locref = np.squeeze(outputs_np['locref'])<br/>        shape = locref.shape<br/>        locref = np.reshape(locref, (shape[0], shape[1], -1, 2))<br/>        locref *= cfg.locref_stdev<br/>    if cfg.pairwise_predict:<br/>        pairwise_diff = np.squeeze(outputs_np['pairwise_pred'])<br/>        shape = pairwise_diff.shape<br/>        pairwise_diff = np.reshape(pairwise_diff, (shape[0], shape[1], -1, 2))<br/>        num_joints = cfg.num_joints<br/>        for pair in pairwise_stats:<br/>            pair_id = (num_joints - 1) * pair[0] + pair[1] - int(pair[0] &lt; pair[1])<br/>            pairwise_diff[:, :, pair_id, 0] *= pairwise_stats[pair]["std"][0]<br/>            pairwise_diff[:, :, pair_id, 0] += pairwise_stats[pair]["mean"][0]<br/>            pairwise_diff[:, :, pair_id, 1] *= pairwise_stats[pair]["std"][1]<br/>            pairwise_diff[:, :, pair_id, 1] += pairwise_stats[pair]["mean"][1]<br/>    return scmap, locref, pairwise_diff</pre>
<p>This is going to take the output from the neural network (which is kind of unintelligible) and put it in a form we can actually use. Then, we will feed the output to something else, or just visualize it in this case. <kbd>argmax_pose_predict</kbd> is complementary to what we did before. It is another utility function that is going to help us understand the output, which is shown in the following example:</p>
<pre>def argmax_pose_predict(scmap, offmat, stride):<br/>    """Combine scoremat and offsets to the final pose."""<br/>    num_joints = scmap.shape[2]<br/>    pose = []<br/>    for joint_idx in range(num_joints):<br/>        maxloc = np.unravel_index(np.argmax(scmap[:, :, joint_idx]),<br/>                                  scmap[:, :, joint_idx].shape)<br/>        offset = np.array(offmat[maxloc][joint_idx])[::-1] if offmat is not None else 0<br/>        pos_f8 = (np.array(maxloc).astype('float') * stride + 0.5 * stride +<br/>                  offset)<br/>        pose.append(np.hstack((pos_f8[::-1],<br/>                               [scmap[maxloc][joint_idx]])))<br/>    return np.array(pose)</pre>
<p>Let's now execute that cell in which we have defined the functions. It will run instantly.</p>
<p>The following code will load the configuration file, which is <kbd>demo/pose_cfg.yaml</kbd>, and <kbd>setup_pose_prediction(cfg)</kbd> will return <kbd>sess</kbd>, <kbd>inputs</kbd>, and <kbd>outputs</kbd>. This is shown in the following example:</p>
<pre>cfg = load_config("demo/pose_cfg.yaml")<br/>sess, inputs, outputs = setup_pose_prediction(cfg)</pre>
<p>When we run the preceding code, it will leave the TensorFlow session open and it is recommended to run it only once to avoid errors, or you might have to restart the kernel. So, if the command gets executed, we understand that the model has been loaded, as you can see in the following output:</p>
<pre>INFO:tensorflow:restoring parameters from models/mpii/mpii-single-resnet-101</pre>
<p>Now, we'll see how to actually apply the model:</p>
<pre>file_name = "testcases/standing-lef-lift.jpg"<br/>image = np.array(imread(file_name))<br/>image_batch = np.expand_dims(image, axis=0).astype(float)<br/>outputs_np = sess.run(outputs, feed_dict={inputs: image_batch})<br/>scmap, locref, pairwise_diff = extract_cnn_output(outputs_np, cfg)<br/>pose = argmax_pose_predict(scmap, locref, cfg.stride)</pre>
<p>For our model, we have to give our file a name. So, we have a directory called <kbd>testcases</kbd> with a bunch of stock photos of people in various poses, which we will be using for our test. We then need to load the <kbd>standing-leg-lift.jpg</kbd> image in a suitable format. We will convert the image to something that TensorFlow actually needs. The input is like an <kbd>image_batch</kbd>, which is going to expand the dimensions along the <kbd>0</kbd> axis. So, just create an array that TensorFlow can actually use. Then, <kbd>outputs_np</kbd> will run the session, extract the CNN output in the next line, and then do the actual pose prediction. The <kbd>pose</kbd> variable is the best to use here. We should then execute the cell and hit <em>Esc</em> button to get into the command mode. Then, we need to create a new cell; type <kbd>pose</kbd> and hit C<em>trl </em>+ E<em>nter</em>. We will then get the following 2D array output:</p>
<div><img src="img/e66b51af-effe-45c2-8b51-20e4e815f1fc.png" style="width:27.92em;height:17.08em;" width="574" height="351"/></div>
<p>The output gives us the <kbd>x</kbd> and <kbd>y</kbd> coordinates corresponding to the joints such as wrists, ankles, knees, head, chin, shoulders, and so on. From this, we get the <kbd>x</kbd> coordinate, <kbd>y</kbd> coordinate, and matching score. We do not need the sub-pixel level, so we can round it to the nearest integer. In the following example, you can see that we have labeled our opposing joints with numbers and drawn lines between them:</p>
<pre>pose2D = pose[:, :2]<br/>image_annot = image.copy()<br/><br/>for index in range(5):<br/>    randcolor = tuple([randint(0, 255) for i in range(3)])<br/>    thickness = int(min(image_annot[:,:,0].shape)/250) + 1<br/>    start_pt = tuple(pose2D[index].astype('int'))<br/>    end_pt = tuple(pose2D[index+1].astype('int'))<br/>    image_annot = cv2.line(image_annot, start_pt, end_pt, randcolor, thickness)<br/>for index in range(6,11): #next bunch are arms/shoulders (from one hand to other)<br/>    randcolor = tuple([randint(0,255) for i in range(3)])<br/>    thickness = int(min(image_annot[:,:,0].shape)/250) + 1<br/>    start_pt = tuple(pose2D[index].astype('int'))<br/>    end_pt = tuple(pose2D[index+1].astype('int'))<br/>    image_annot = cv2.line(image_annot, start_pt, end_pt, randcolor, thickness)<br/>#connect Line from chin to top of head<br/>image_annot = cv2.line(image_annot,<br/>                       tuple(pose2D[12].astype('int')), tuple(pose2D[13].astype('int'))<br/>                       tuple([randint(0,255) for i in range(3)]), thickness)</pre>
<p>We need to create a <kbd>pose2D</kbd> label here, and then we are going to extract the x and y coordinates in the first two columns. We will make a copy using <kbd>image.copy()</kbd>, because we want our annotated image to be separate from our original image.</p>
<p>We will run the following code to show our original image:</p>
<pre>figure()<br/>imshow(image)</pre>
<p>We are now going to learn how to annotate the original image. We're going to create a copy of the image and then we're going to iterate it over the first six joints and draw lines between them. It starts on the ankle, <kbd>1</kbd>, goes up through the hips, and then goes down to the the other ankle. Numbers <kbd>6</kbd> through <kbd>11</kbd> will be the arms and shoulders, and the last two points are the chin and the top of the head. We're now going to connect all these points with lines from our <kbd>pose2D</kbd>. We actually don't have points for the <kbd>waist</kbd> and the <kbd>collar</kbd>, but we can easily estimate those from the midpoints of the hips and the shoulders, which is useful for completing the skeleton.</p>
<p>Let's look at the following code, which helps us estimate the midpoints:</p>
<pre># There no actual joints on waist or coLLar,<br/># but we can estimate them from hip/shoulder midpoints<br/>waist = tuple(((pose2D[2]+pose2D[3])/2).astype('int'))<br/>collar = tuple(((pose2D[8]+pose2D[9])/2).astype('int'))<br/># draw the "spine"<br/>image_annot = cv2.line(image_annot, waist, collar,<br/>                       tuple([randint(0,255) for i in range(3)]), thickness)<br/>image_annot = cv2.line(image_annot, tuple(pose2D[12].astype('int')), collar,<br/>                       tuple([randint(0,255) for i in range(3)]), thickness)<br/># now Label the joints with numbers<br/>font = cv2.FONT_HERSHEY_SIMPLEX<br/>fontsize = min(image_annot[:,:,0].shape)/750 #scale the font size to the image size<br/>for idx, pt in enumerate(pose2D):<br/>    randcolor = tuple([randint(0,255) for i in range(3)])<br/>image_annot = cv2.putText(image_annot, str(idx+1),<br/>                          tup1e(pt.astype('int')),font, fontsize,<br/>                          randcolor,2,cv2.LINE_AA)<br/>figure()<br/>imshow(image_annot)</pre>
<p>We can now draw a spine by drawing a point from the waist to the collar, and the collar to the chin. We can also label these joints to show exactly what we are joining, and this will help in your customized application. We are going to label the joints, create the figure, show the annotated image, and deal with random colors. The following screenshot shows what the output looks like:</p>
<div><img src="img/cbe433a8-4db9-4dcc-b044-b7a2d51bad78.png" style="width:24.75em;height:22.75em;" width="597" height="548"/></div>
<p>Here, 1 is the right ankle, but it could be the left ankle depending on which way the person's facing. So, all the links are joined except for 13, which is a bit occluded here, and 14, which is slightly out of the image. The nice thing about this is that it potentially works even if other joints are occluded (for instance, if they're off-screen or covered up by something). You will notice that the image is simple with a flat background, flat floor, and a simple pose and clothes. The code will also work with more complicated images, and if you have any trouble reading the details, you can use the widgets here and zoom in.</p>
<p>Let's try using different images and analyze our results, which are shown in the following example:</p>
<pre>file_name = "testcases/mountain_pose.jpg"<br/>image = np.array(imread(file_name))<br/>image_batch = np.expand_dims(image, axis=0).astype(float)<br/>outputs_np = sess.run(outputs, feed_dict={inputs: image_batch})<br/>scmap, locref, pairwise_diff = extract_cnn_output(outputs_np, cfg)<br/>pose = argmax_pose_predict(scmap, locref, cfg.stride)</pre>
<p>The following shows the photo we will be testing:</p>
<div><img src="img/a2eb4e31-b79b-46d9-b13a-301bab289c87.png" style="width:37.08em;height:20.33em;" width="785" height="433"/></div>
<p>When we run our model again, using a different image, we get the following output:</p>
<div><img src="img/19445ba0-e0f2-4b91-be29-1b3ab509e062.png" style="width:36.92em;height:20.92em;" width="804" height="456"/></div>
<p>If we take an image of a guy with crossed arms, We get the following screenshot:</p>
<div><img src="img/68002f06-88dc-43a5-8da1-c0a1019254c7.png" style="width:18.58em;height:24.00em;" width="429" height="555"/></div>
<p>The result is very good, even though the arms are crossed. </p>
<p>Now, let's take a look at a few difficult images. This might not give us the accurate results of a complete motion capture pose estimation solution, but is still very impressive.</p>
<p>Select <kbd>acrobatic.jpeg</kbd>, which is as follows:</p>
<div><img src="img/e6d30678-98a9-45f0-bbed-2b148f6e243e.png" style="width:27.33em;height:19.25em;" width="735" height="518"/></div>
<p class="mce-root"/>
<p>The output we get when we run this photo is shown in the following example:</p>
<div><img src="img/6a998f24-5ca7-4468-a184-b9e35e12a17c.png" style="width:28.58em;height:21.67em;" width="743" height="565"/></div>
<p>It looks as if it found the joints, more or less, but did not connect them properly. It shows that the guy's head is on his hand, which is touching the ground. We can see that the results are not that good. But we cannot expect accurate results for all images, even though this is state of the art.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Multi-person pose detection</h1>
                
            
            
                
<p>Now, let's move from single-person pose detection to multi-person pose detection. With single-person pose detection, we saw that the code will take an image of a single person and generate pose estimation with all the joints labeled. We will now learn a more advanced model called ArtTrack, which will allow us to count the number of people, find people, and estimate their poses. </p>
<p>Let's look at the code for multi-person pose detection, which is shown in the following example:</p>
<pre>import os<br/>import sys<br/>import numpy as np<br/>import cv2 I<br/>from imageio import imread, imsave<br/>from config import load_config<br/>from dataset.factory import create as create_dataset<br/>from nnet import predict<br/>from dataset.pose_dataset import data_to_input<br/>from multiperson.detections import extract_detections<br/>from multiperson.predict import SpatialModel, eval_graph, get_person_conf_mu1ticut<br/># from muLtiperson.visuaLize import PersonDraw, visuaLize_detections</pre>
<p>This is a little more complicated. We will first list our directories here using the <kbd>!ls</kbd> command in the current directory, where you will find a file called <kbd>compile.sh</kbd>.</p>
<p>We need to run this file because there are some binary dependencies in this module. But this is a shell script file, and you might face some issues on macOS or Linux. So, in order to generate those files/commands that are OS-specific, you will need to run that script. For Windows, those binary files have already been generated. So, if you are using the latest version of Python and TensorFlow, then the files will be compatible and the binary should work.</p>
<p>If it does not work, you will need to download and install Visual Studio Community. There are some instructions that you can follow at <a href="https://github.com/eldar/pose-tensorflow" target="_blank">https://github.com/eldar/pose-tensorflow</a> under the <kbd>demo</kbd> code section for multi-person pose.</p>
<p>Once you have everything up and running, we can start with our example. Also, as we have already discussed, we need to make sure that we restart the kernel. This is important because if you have your session already open for running a different project, TensorFlow might not be able to compute the code as the previous model is loaded. It is always a good practice to start from a fresh kernel.</p>
<p>We will run our <kbd>%pylab notebook</kbd> to make our visualizations and numerix. The code works similarly to what we have already covered, where we have the boilerplate and load a pre-trained model. The pre-trained model is already included, so we don't need to download it. The code will execute within a second because of TensorFlow, and we will get the modules imported and load the repositories as well. Also, we need to load the model and actually do the predictions separately. If we hit C<em>trl </em>+ S<em>hift </em>+ <em>-</em>,<em> </em>we can separate the predictions into different cells to make it look neat.</p>
<p>When we run the first cell, we get the following output:</p>
<div><img src="img/a2797da1-e46c-40d2-8f76-5cc565d2b363.png" style="width:39.75em;height:6.58em;" width="859" height="143"/></div>
<p>This is not a big error message, and is because <kbd>imread</kbd> was defined here; the Notebook clobbers that and just gives you a warning message. We can just rerun that code to ignore the warning and get a tidy output.</p>
<p>In this cell, we are going load the configuration file for multiple people provided by the authors of ArtTrack/DeeperCut. </p>
<p>The following line loads the dataset:</p>
<pre>cf = load_config("demo/pose_cfg_multi.yaml) </pre>
<p>Then, the following line creates the model and loads it:</p>
<pre>dataset = create_dataset(cfg)<br/>sm = SpatialModel(cfg)<br/>sm.load()<br/>sess, inputs, outputs = predict.setup_pose_prediction(cfg)</pre>
<p>When we execute that, we get the following output:</p>
<div><img src="img/75cf5251-4345-430b-8250-fac3e0b66805.png" style="width:35.75em;height:1.42em;" width="767" height="30"/></div>
<p>We will keep the session open here so that we can keep running different things and quickly run through different frames.</p>
<p>We will now run our code for some test cases that actually have multiple people, as follows:</p>
<pre>file_name = "testcases/bus_people.jpg"<br/>image = np.array(imread(file_name))<br/>image_batch = data_to_input(image)<br/># Compute prediction with the CNN<br/>outputs_np = sess.run(outputs, feed_dict={inputs: image_batch})<br/>scmap, locref, pairwise_diff = predict.extract_cnn_output(outputs_np, cfg, dataset<br/>detections = extract_detections(cfg, scmap, locref, pairwise_diff)<br/>unLab, pos_array, unary_array, pwidx_array, pw_array = eval_graph(sm, detections)<br/>person_conf_multi = get_person_conf_multicut(sm, unLab, unary_array, pos_array)<br/>image_annot = image.copy()<br/>for pose2D in person_conf_mu1ti:<br/>    font = cv2.FONT_HERSHEY_SIMPLEX<br/>    fontsize = min(image_annot[:,:,0].shape)/1000</pre>
<p>We need to go to the <kbd>np.array</kbd> and convert it to a flat array network to compute the predictions with <kbd>sess.run</kbd>, and then extract the CNN output and <kbd>detections</kbd> using the model utilities. We will not label the bones here, but we will instead label the joints with numbers.</p>
<p>When we run the code, we get the following output:</p>
<div><img src="img/19c5a46a-bb1f-4893-b003-02abe003965d.png" style="width:21.58em;height:28.08em;" width="425" height="554"/></div>
<p>This is a simple image of multiple people, in plain dress and with a flat background. This actually worked. However, the numbers aren't the same as before. Previously, number 1 corresponded to the right ankle and went up through 2, 3, 4, 5, and 6, and then 7 was the right wrist, and so on. So, the numbers are different, and there are more of them, which actually detects more joints because they have multiple numbers for the face, so there are multiple points here. Let's zoom in to check the details, as shown in the following picture:</p>
<div><img src="img/53647dd1-9068-4d16-b90c-91ae7364ca89.png" style="width:24.42em;height:19.25em;" width="673" height="531"/></div>
<p>Here, we have the facial landmarks as 1, 2, 3, 4, and 5, and hence this could be used in conjunction with the dlib detector, which is covered in <a href="c59fb392-c966-4da6-987a-625378474e71.xhtml" target="_blank"/><a href="c59fb392-c966-4da6-987a-625378474e71.xhtml" target="_blank">Chapter 6</a>, <em>Facial Feature Tracking and Classification with dlib</em>. If we want to know somebody's facial expression, in addition to the full-body landmark detectors and their pose, then this could be done here. We can also get a really thorough description of which way people are facing and exactly what they're doing within the image.</p>
<p>Let's try another <kbd>exercise_class.jpeg</kbd> image, which gives us the following output:</p>
<div><img src="img/5770e208-4320-4fd7-8d02-c2567cc740b4.png" style="width:34.08em;height:22.33em;" width="816" height="536"/></div>
<p class="mce-root"/>
<p>Here, we can see how multiple points are present on the knees for the lady on the extreme right. It is still a good result.</p>
<p>Let's try one more image, which we saw previously on the GitHub page, <kbd>gym.png</kbd>.</p>
<p>You can see the output as follows:</p>
<div><img src="img/8ab0b140-7c79-4dda-952f-4f795d32db80.png" style="width:36.75em;height:21.17em;" width="792" height="457"/></div>
<p>This does detect the body parts here. So, let's try using this model to detect the pose for a single person. Do you think it will work? The answer is <em>yes</em>, it does work. You must be wondering why we would use the previous model if this is available. This model is slightly more computationally efficient, so if you know you only have one person, you don't actually need it, because this algorithm provides the number of people.</p>
<p>You can select the photo of a single person from among the photos available. For example, we'll select <kbd>mountain_pose.jpg</kbd>, which gives the following output:</p>
<div><img src="img/a40eaebf-7ee6-440f-9b1a-491c13fd042f.png" style="width:34.75em;height:19.92em;" width="814" height="466"/></div>
<p>It will also show the number of people, as demonstrated by the following code:</p>
<div><img src="img/c96952fb-1a85-4c57-9c85-272547f8ff93.png" style="width:10.08em;height:1.92em;" width="176" height="33"/></div>
<p>But, if you use the multi-person detector for a single person, it might be prone to over-fitting and detecting more people than are actually in the image. So, if you already know there is only one person, then it may still be a good idea to just use that original model rather than the ArtTrack model. But if it does work, try both, or use whatever is best for your application. However, this might not work perfectly for complex images and a complicated variety of poses.</p>
<p>Let's try one last <kbd>island_dance.jpeg</kbd> image. The following screenshot shows the result:</p>
<div><img src="img/ca3d7ae4-b8d8-4b8f-a451-b77bd2361b8f.png" style="width:33.75em;height:20.50em;" width="782" height="477"/></div>


            

            
        
    </div>



  
<div><h1 class="header-title">Retraining the human pose estimation model</h1>
                
            
            
                
<p>We will now discuss how to handle videos and retrain our human pose estimation network. We have already covered face detection and how to apply a model to a video. Opening a video is pretty straightforward and OpenCV provides a mechanism for that. It's basically doing the same thing one frame at a time. The following example shows the code for this:</p>
<pre>predictor_path = "./shape_predictor_68_face_landmarks.dat"<br/>detector = dlib.get_fronta1_face_detector()<br/>predictor = dlib.shape_predictor(predictor_path)<br/><br/>#Uncomment Line below if you want to use your webcam<br/>#cap = cv2.VideoCapture(0) #0 is the first camera on your computer, change if you<br/>#more than one camera<br/><br/>#Comment out the Line below if using webcam<br/>cap = cv2.VideoCapture('./rollerc.mp4')<br/>figure(100)<br/>font = cv2.FONT_HERSHEY_SIMPLEX</pre>
<p>First, we need to create a <kbd>cv2</kbd> capture device, then open the file, and while reading the file, we should load the image and run the network on the image. Please refer to the following code:</p>
<pre>font = cv2.FONT_HERSHEY_SIMPLEX<br/>while(True):<br/>    #Capture frame-by-frame<br/>    ret, img = cap.read()<br/>    img.flags['WRITEABLE']=True #just in case<br/>  <br/>    try:<br/>        dets = detector(img, 1)<br/>        shape = predictor(img, dets[0])<br/>    except:<br/>        print('no face detected', end='\r')<br/>        cap.release()<br/>        break<br/>#similar to previous example, except frame-by-frame here<br/>    annotated=img.copy()<br/>    head_width = shape.part(16).x-shape.part(6).x<br/>    fontsize = head_width/650<br/>    for pt in range(68):<br/>        x,y = shape.part(pt).x, shape.part(pt).y<br/>        annotated=cv2.putText(annotated, str(pt), (x,y), font, fontsize, (255,255,255), 2, cv2.LINE_AA)<br/><br/>#Let's see our results<br/>    fig=imshow(cv2.cvtColor(annotated,cv2.COLOR_BGR2RGB)) #OpenCV uses BGR format<br/><br/>    display.c1ear_output(wait=True)<br/>    display.display(gcf())<br/><br/>#When everything is done, release the capture<br/>cap.release()</pre>
<p>Using a good GPU, we should be able to do the computation in few frames per second, if not 30 to 60 FPS, depending on your hardware. You should be able to do it almost in real time.</p>
<p>For training your model, you need to first make sure that you have good hardware and a lot of time. First, you need to download the ImageNet and ResNet models. Then, you need to go through the steps and instructions on the <a href="https://github.com/eldar/pose-tensorflow/blob/master/models/README.md" target="_blank">https://github.com/eldar/pose-tensorflow/blob/master/models/README.md</a> page. You will need a lot of data, so you can use the data they provide. Using your own data could be time consuming and difficult to obtain, but it is possible. You can refer to the previous link provided for complete instructions.</p>
<p>The instructions here use MATLAB at one point to convert the data, although there are ways to do that in Python and train the model with the MS COCO dataset. This is similar to what we did in <a href="d5bf725b-9e79-4865-96e9-338481599464.xhtml" target="_blank">Chapter 2</a>, <em>Image Captioning with TensorFlow</em> and it also provides instructions on how to train the model with your own data set. This involves a lot of work and a lot of computational power. You can try this or use what has already been provided in the pre-trained model, which can do a lot of things.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we learned the basics of human pose estimation and then used the DeeperCut and ArtTrack models in our project for human pose estimation. Using these models, we carried out single-person and multi-person pose detection. Towards the end of the chapter, we learned how to use the model with videos and retrained the model for customized images.</p>
<p>In the next chapter, <a href="c8202017-404a-42aa-9f24-93488e3abd0a.xhtml" target="_blank">Chapter 5</a>, <em>Handwritten Digit Recognition with scikit-learn and TensorFlow</em>, we will learn handwritten digit recognition with scikit-learn and TensorFlow.</p>


            

            
        
    </div>



  </body></html>