<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Implementing Deep Learning Algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">Deep learning is an area of machine learning that has gained significantly in terms of popularity in recent years. Deep learning, which is also referred to as deep structured learning or hierarchical learning refers to using multiple layers of artificial neural networks to train from data. Over the last few years, it has become possible to perform certain tasks, such as image recognition, better than human beings.</p>
<p class="mce-root">We will cover the following topics in this chapter:</p>
<ul>
<li>Understanding deep learning</li>
<li>Applications of deep learning</li>
<li>Understanding deep neural networks</li>
<li><span>Understanding </span>convolutional neural networks</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding deep learning</h1>
                </header>
            
            <article>
                
<p>Deep learning algorithms have gained in popularity over the last decade. Technologies such as self-driving cars, speech recognition, and robotics have improved significantly on account of deep learning algorithms. Deep learning has helped researchers to significantly reduce the number of errors when training models to perform such tasks and also surpassed humans in performing certain tasks. However, what is most interesting is that deep learning algorithms are inspired by how human brains work. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's take an example of image recognition. We see objects and are able to recognize them based on past experiences of when we saw these objects. However, let's break this process down into what exactly happens. First, light hits the object, enters our eye, and hits the retina. The retina is a sensory membrane that converts this light into nerve signals. This signal is then passed through various layers behind the retina to the brain. Our brain identifies the number of objects that exist in the scene before our eyes. Based on past references, our brain can identify the objects. </p>
<p>There is no one process of us looking at the object and recognizing it. There are various levels of abstractions between when the light enters our eyes and when our brain identifies the object. There is no specific process when our brain stops and decides what features in the signal it is trying to interpret. Such a feature extraction process occurs automatically. </p>
<p>Deep learning algorithms also follow a similar process. Deep learning breaks the tasks of getting the data into the various layers of abstractions, such that each layer interprets the input data, and provides a meaningful output for the next layer of abstraction. For example, in image recognition tasks, the input may be a set of pixels from the image. In the first layer, the pixels can be processed to find edges in the image. In the second layer, this information regarding edges can be processed to detect corners between these edges. In the next layer, these corners and edges can be used to detect objects in the image. And the next layer may predict what each object is. These layers of abstractions do not need to be defined by us, but can train automatically by themselves. </p>
<p>In algorithms such as Naive Bayes and linear regression, we always used hand-crafted features. We already had analysts look at the incoming dataset and define feature sets based on the data. We labeled each category as categorical or continuous. However, in deep learning methodologies, we only require datasets with simple features and use layers of abstractions to create additional abstract features. Hence, in tasks such as image recognition, where the datasets are sets of pixels, traditional algorithms would need help in identifying objects in the images before they can learn how to classify them. We would also have to extract features from the objects, such as color and size before we can feed these features to the classification models. However, for deep learning algorithms, we use pixels of the image as input to the algorithm with labeled objects such that the deep learning models can identify when errors are made and undertake self-correction. </p>
<p>Deep learning algorithms can perform both supervised and unsupervised learning algorithms. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applications of deep learning</h1>
                </header>
            
            <article>
                
<p>We will present examples of popular deep learning algorithm applications in this section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Self-driving cars</h1>
                </header>
            
            <article>
                
<p>Self-driving cars have become a mainstay in the auto industry, with every major company investing in building the next generation of self-driving cars. Most companies offer some level of autopilot capabilities in their latest cars. These algorithms are mostly powered by deep learning algorithms. Let's take a look at how self-driving algorithms are developed using deep learning. </p>
<p>The task of the self-driving algorithm is to analyze the conditions on the road and react to them correctly in order to drive the car from the origin to the destination address. The input for this algorithm is the video feed they receive from the cameras fitted on all sides of the car. The output of the algorithm is the signals to the steering wheel, accelerator, and brakes. </p>
<p>This task is extremely complicated since the driver needs to make split-second decisions when dealing with road conditions. The driver not only has to remember which turns to take in order to reach the destination or the speed limits on the road, but also has to monitor the movements of other cars on the road and pedestrians who may cross the roads. </p>
<p>Creating a rules-based algorithm for such a task is very difficult as there are a vast number of permutations that can occur on the road. Moreover, generating any labeled dataset with well-defined features is also difficult since the number of situations that could arise is very hard to label in a comprehensive dataset. </p>
<p>Deep learning algorithms are perfect in such situations because automatically extracting features from the video feed and training the models based on a reward function helps us to abstract the issues in self-driving cars. We can set the input of the deep learning algorithm as the pixels in the video feed and the reward function as our progress toward the destination while obeying all traffic rules. Simulators are used to train these models. Such simulations mimic the actual conditions on the road.</p>
<p>Deep learning algorithms can automatically determine how to generate the layers of abstractions to translate the pixels from the video feed to detecting edges and objects, similar to image recognition models. Once the objects are detected, based on the mistakes and corrections made by the car, we train the models to learn how to output the accelerator, brakes, and steering wheel instructions. Initially, when running the models, self-driving cars make mistakes and crash into objects. However, with sufficient iterations, deep learning models can learn how to avoid such mistakes and drive on a predetermined path. Thus, without extracting the features manually from the video feeds, and without generating any structured datasets, deep learning algorithms can automatically learn to achieve certain objectives. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning to play video games using a deep learning algorithm</h1>
                </header>
            
            <article>
                
<p>Another popular example of using deep learning is to train a machine to play computer games. Researchers across the world tested their deep learning algorithms by training models to play 2D platforming titles, such as Super Mario. The input to the model is the pixels on the screen, while the output generated by the model is a sequence of controller instructions that control the characters and finish the objectives in the game. </p>
<p>We do not need to teach deep learning models that this is a video game and that a character named Mario has to jump on platforms to finish the levels. We just have to define a reward function such that if the character moves to the next platform without dying, we reward the deep learning model, and if the character dies, we penalize the model. As mentioned before, the deep learning model automatically divides the problem into multiple levels of abstractions. </p>
<p>The model learns how to detect edges and platforms on the screen automatically. It starts by making random movements with the character and quickly learns how the pixels on the screen are manipulated when different controller buttons are pushed. Based on the movements of the character, the model learns how to move the main character forward. Similar to the self-driving car, it will also automatically learn that touching certain objects on the screen leads to a penalty, and jumping on certain edges on the screen leads to the player falling into pits. Hence, based on these reward functions giving feedback to the model, the model learns how to navigate the obstacles in the level to move the player in the right direction. With further training, it can also learn how to solve puzzles in the game. </p>
<p>Thus, just by providing the screen pixels to the deep learning model, we can train a machine to play video games. You see examples of these implementations everywhere around you. Soon, machines will learn how to solve complex puzzles by thinking rationally based on these machine learning models. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding deep learning algorithms</h1>
                </header>
            
            <article>
                
<p>In the next section, we study one of the most popular deep learning algorithm, called deep neural networks. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Before we look at what deep neural networks are, we will study what neural networks are. Then, we will learn what deep neural network algorithms are and why they are an improvement over neural networks. Finally, we will study convolutional neural networks—which is a variant of neural networks that is used in the field of image recognition <span>–</span> and show how we can automatically learn layers of abstractions from the pixels in the image. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural network algorithms</h1>
                </header>
            
            <article>
                
<p>Neural network algorithms are machine learning algorithms that are inspired by biological neural network algorithms. Neural networks mimic how our neurons in our brain work. They have input nodes where the information is fed into the network, and an output layer that transmits a specific action or prediction.  Neural networks define a structure in which the information of the machine learning model is stored.</p>
<p>The following <span><span>screenshot </span></span>shows a neural network structure:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-766 image-border" src="assets/35bde629-dac9-4460-94ed-060bcbd1ea8c.png" style="width:16.50em;height:20.42em;"/></p>
<p>The input features from a dataset are fed into neural network input nodes. For example, if we have a dataset that has features such as temperature, cloud conditions, and wind speed, and our task is to predict whether it will rain on a given day, then such features are fed to the neural networks as input. These features can either be in binary or continuous values. Note that each input feature corresponds to one input node. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The information regarding the model is stored on the edges and the nodes in the hidden layer. There are various algorithms that can be used to train neural networks. Most algorithms iteratively pass input parameters in the neural network and predict the values of output nodes based on the weights in the hidden nodes. Based on the error in prediction, these weights are adjusted to improve the model. </p>
<p>The output nodes correspond to the expected actions or predictions that the neural network algorithms need to make. Our aim is to train the weights in the hidden nodes such that the values of the output nodes are accurate. </p>
<p>Thus, the neural networks are loosely based on biological neurons that can process an input signal and produce an output based on the function of that neuron. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activation function</h1>
                </header>
            
            <article>
                
<p><span>Now, let's look at how a neural network algorithm is trained to calculate the weights of each hidden node. </span>Before we begin training a neural network model, we need to define how each hidden node will process the input signal and produce an output. The function that is used to calculate the output of a hidden node based on an input function is called the activation function. Activation functions define the range of output that can be generated by the hidden nodes. In its simplest form, an activation function can be a step function where the node output is either 0 or 1 based on the input. A simple example in our weather dataset is this: if the sky is cloudy, the output of a hidden node might be 1 as a prediction for rain, and if the sky is sunny, the output of the hidden node is 0.</p>
<p>Such a step activation function is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/acd1e5b4-e4c8-4df9-ae16-3f31e8894833.png" style="width:10.50em;height:3.00em;"/></p>
<p>Similarly, if we plan to use a logistic or sigmoid step function, the range of the output is from <img class="fm-editor-equation" src="assets/d7e0e539-96f2-4292-99c6-b3f9d32dfb17.png" style="width:2.42em;height:1.08em;"/> to <img class="fm-editor-equation" src="assets/b4885d2d-91c3-4e88-b502-486e8f1c328c.png" style="width:1.42em;height:0.83em;"/>.</p>
<p>A logistic step function is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e798e20d-a315-44ea-b02a-5622a5093094.png" style="width:7.17em;height:2.42em;"/></p>
<p>Based on the learning algorithm we are using, we can select activation functions. Most machine learning libraries that support neural network learning also support using various activation functions. </p>
<p>Each edge between nodes is assigned a weight, <img class="fm-editor-equation" src="assets/a429fa77-3a96-4f6c-b5a9-10c02fbfcf13.png" style="width:1.83em;height:1.25em;"/>, such that that link is between neuron <img class="fm-editor-equation" src="assets/a6be5976-da28-4d11-b71a-d2de022f2237.png" style="width:0.50em;height:1.17em;"/> and neuron <img class="fm-editor-equation" src="assets/4df7fbb5-6c92-4af8-bee5-cd62dd730db6.png" style="width:0.58em;height:1.42em;"/>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Backpropagation</h1>
                </header>
            
            <article>
                
<p>Once we have established the weights of the connections in the neural networks and the activation function, a neural network is able to effectively produce an output based on a given input. However, this is an untrained neural network, and an algorithm is needed to modify and adapt a neural network based on the errors it makes when predicting an output. </p>
<p>The weight updates for backpropagation using stochastic gradient descent can be executed using the following equation.</p>
<p>The backpropagation algorithm is one of the popular mechanisms that can achieve this outcome. Backpropagation algorithms define a methodology for how the errors in the output can be propagated through the connections by modifying the values of the connections. The intuition behind the algorithm is very simple. Consider a child touching a very hot pan and learning not to touch pans that are situated on top of a stove. The child makes a mistake, but learns from it and avoids making the same mistake again. Backpropagation algorithms also allow neural networks to make errors. The difference between the predicted output and the expected output can be calculated using formulas, such as mean squared errors. Once we quantify the error, we can use algorithms, such as gradient descent, to determine how to modify the weights of the connections. We also used the algorithm of gradient descent for the linear regression algorithm in <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, <em>Predicting House Value with Regression Algorithms</em>. The backpropagation process is similar to how we learn the coefficients for the linear regression algorithm. However, instead of learning the values of the regressors, we are estimating the value of the weights of the connections in neural networks. </p>
<p>The weight updates for backpropagation using stochastic gradient descent can be done using the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c5fced8c-865f-4de2-8e1f-d0654113d681.png" style="width:17.00em;height:2.00em;"/></p>
<p>In this equation, <img class="fm-editor-equation" src="assets/32b3cd5d-7509-4e2d-85e4-642e50c14c76.png" style="width:0.75em;height:1.17em;"/> is the learning rate of the neural network. This is a tunable parameter and defines how quickly the neural network can adapt to the training dataset. The weight, <img class="fm-editor-equation" src="assets/522994fb-177e-42c8-b77f-e17be8d6b0fc.png" style="width:5.00em;height:1.42em;"/>, is calculated based on the previous weight of the connection. The value of change in the weight is determined by the learning rate, which is the difference between the error, the previous weight, and a stochastic term. </p>
<p>We iterate over the training data by passing it through the neural network and modifying the weights of the connections during each iteration. The weights are modified such that the error rates reduce with each iteration. Although stochastic gradient descent does not achieve a global maxima, it is effective in training a neural network to reduce errors. We terminate the iteration when the error is below a certain acceptable value, or it converges such that the improvements in accuracy are minimal. </p>
<p>Neural networks can be used to train supervised learning as well as unsupervised learning. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to deep neural networks</h1>
                </header>
            
            <article>
                
<p>A <strong>deep neural network</strong> (<strong>DNN</strong>) is a variant of neural networks where we use more than one hidden layer. The data has to pass through more than one hidden layer for a network to qualify as a deep neural network. This adds complexity to the neural network model as it drastically increases the connections in the network, and thus the learning time.</p>
<p>A representation of a deep neural network is shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-837 image-border" src="assets/f037847a-0051-4af1-8315-8a3782c17290.png" style="width:33.33em;height:20.67em;"/></p>
<p>However, having additional hidden layers also allows the network to pass the input data through multiple layers of pattern recognition. Each hidden layer gets the input from the previous hidden layers. Hence, they can recognize more complex patterns than the previous layers. This happens as the previous layers aggregate, and recombines the features from the previous layers. This is called the feature hierarchy. The features that are deeper in the DNN can recognize more complex patterns. Hence, DNN is more capable of handling datasets with complex patterns. Moreover, since the hidden layers automatically generate these layers of abstractions, domain expertise is not required for feature extraction. For example, in image recognition, we do not need to label the edges of objects in the image since initial layers can learn to identify edges, while deeper layers learn to identify the objects that may be generated by those edges. </p>
<p>Deep learning and DNN are popular buzzwords that data scientists hear about in the industry. For most applications, such as self-driving cars or robotics, DNN is synonymous with  artificial intelligence. Due to the advances in GPU architectures, which suit the generation of these DNN structures, such algorithms are not able to process large datasets in order to train highly accurate machine learning algorithms. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding convolutional neural networks</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will take a look at a variant of DNNs, where the structure of the network is modified for image recognition tasks. </span></p>
<p>In the neural networks that we've discussed in this chapter so far, we've seen that all the input layers are one dimensional. However, images are two-dimensional. To capture how images are fed to a neural network for training, we have to modify the structure of the input layer. Traditional algorithms require humans to label the edges of the objects in the image. <strong>Convolutional neural networks</strong> (<strong>CNNs</strong>) can automatically detect the objects in the image with enough training and, based on the labels of the image, they can learn how to identify objects in the images without explicitly labeling the edges in the image.</p>
<p>CNNs require a preprocessing phase where the image has to be prepared into a specific data structure that is used as an input for feed-forward DNNs. The first task in the preprocessing phase is to break the picture down into smaller images, such that we do not lose any information from the image. The inspiration for a CNN comes from the organization of the visual cortex in humans. Our neurons respond to visuals that are seen in a specific field of vision. This is called the local receptive field. These local receptive fields overlap with each other. Similarly, in CNN, we take an image as an input and represent overlapping subsections of an image as local receptive fields.</p>
<p>The following diagram shows how a sliding window is used to generate feature maps from an image using the concept of local receptive fields:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-768 image-border" src="assets/c1946649-c3b8-4274-a761-557c7b803208.png" style="width:26.58em;height:17.50em;"/></p>
<p>The advantage of using this methodology is that it eliminates the issues of size and position of an object in an image. For example, imagine there is a cat in the image. Based on our training examples, we have labeled images with pictures of cats. Using local receptive fields, we detect that cat and label the feature map as having a picture of a cat in the image. In a new image, irrespective of the location of the cat in the image, we will find a feature map that has an image of a cat, since we create multiple sub-images using this sliding window approach. This layer of feature maps generated from the image is called the convolutional layer. </p>
<p>We can also generate multiple feature maps from the same set of pixels by applying various filters to the process. For example, we can apply color filters to the pixels and generate three feature maps from the same set of pixels. As a data scientist, you would have to design the CNNs based on the amount of information that you want to extract from the image, as well as the amount of processing power that we can use when generating these networks. </p>
<p>Once the convolution layers are generated, we create condensed feature maps from the image by using a process called pooling. This helps us to represent the feature maps in smaller feature maps. There are two popular pooling processes that can be applied when condensing a feature map. In a max-pooling approach, reduce the dimensionality of the feature map by only selecting the maximum value from each grid.</p>
<p>The following screenshot shows how max-pooling takes the maximum value from each feature map and reduces the dimensionality of the feature map from a matrix of 4x4 to 2x2:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-769 image-border" src="assets/41c66ced-bf18-4abb-8cce-5b139cfa8c26.png" style="width:20.50em;height:7.75em;"/></p>
<p>Another type of pooling is called average pooling, which is where we select the average of the values in a grid when pooling the data. The following diagram shows how average pooling works:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-770 image-border" src="assets/9b974661-46fb-4bca-892e-bb72c9ee45f8.png" style="width:19.75em;height:7.58em;"/></p>
<p>Max-pooling is generally preferred over average pooling as it acts as a noise suppressant and removes the non-dominant features when reducing the dimensionality of a feature map. Similar to a convolutional layer, a pooling layer can also use overlapping windows to create a smaller feature map. Note that these decisions can be made based on the level of detail you want to capture from an image. </p>
<p>Another component of a <span><span>CNN</span></span> is the convolution layer. When we design a CNN, a set of images might determine what feature maps we extract from the image. However, based on the application, we would want to extract different features from the images. For example, if our image recognition software is detecting charts generated by a seismometer (a device that detects earthquakes), our feature maps would have black and white graphs, where our algorithm needs to be sensitive to detecting the edges in time-series. In such cases, we can design a convolution kernel that can translate certain patterns in a feature graph into another feature graph that can annotate such patterns. Similarly, if you are processing colored images with objects, creating three feature maps for each color, detecting edges, and then merging the feature maps, is helpful. Thus, convolutional layers help scientists who design such neural networks to adapt them to specific applications. We are not going to explain the details of how convolution layers can be set up, as most libraries allow you to use predesigned CNNs to apply to your applications. </p>
<p class="mce-root"/>
<p>Thus, using local receptive fields, convolution layers, and pooling, we construct the following structure to flatten an image into input for a DNN:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-838 image-border" src="assets/8a319ddb-08cb-417b-9c4a-7656c72a8025.png" style="width:162.50em;height:62.58em;"/></p>
<p>An image is translated into feature maps using the first layer of convolution by employing the local perceptive fields methodology. Then, we reduce the dimensions of the feature maps by pooling the data, so as to reduce the dimensionality of feature maps from 20x20 to 10x10. In the next phase, we translate the pooled feature maps into more feature maps based on a kernel we might have selected. These kernels may translate the feature maps that detect straight lines or intersections. We then pool the output of the convolution layer into 4x4 feature maps. At this point, the original image is translated into information that is specific to the task of a DNN. These feature maps represent the spatial components of the images too. The DNN then trains based on this data and learns to predict output based on what the feature maps may represent. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we explained what deep learning means and how it is applied in real-world applications. We also studied applications, such as self-driving cars and a video game bot, and how they can automatically learn how to perform tasks using deep learning. We explained what neural networks are and how DNNs are an improved version of them. We also studied a variant of DNNs, called CNNs and presented the various components of a CNN. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Our aim in this chapter was to provide you with information about deep learning algorithms so that you could understand how they can be applied in the real world. Although we did not dive deep into the mathematics of deep learning, or provide all details on concepts such as activation function, we hope that you gained a working knowledge in the field of deep learning. For those curious minds out there, there is a vast amount of ongoing research in this field and we implore you to learn more about the algorithms that you are interested in. </p>
<p>In the next chapter, we will look at how deep learning can be implemented using popular technologies, such as TensorFlow and MXNet. This knowledge will help you to implement a large array of deep learning algorithms. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<ol>
<li>If you own a smartphone, you have a lot of apps on your phone that employ deep learning. Explore which apps on your phone use one of the algorithms listed in this chapter and examine how to design such an algorithm.</li>
<li>List the various components of CNN and design a CNN that would detect the features of a human face.</li>
</ol>


            </article>

            
        </section>
    </body></html>