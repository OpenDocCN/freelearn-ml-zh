<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Mathematics for Computer Vision</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Mathematics for Computer Vision</h1>
                </header>
            
            <article>
                
<p>In the book, we are using several advanced algorithms that require a good background in mathematics. This appendix begins by describing some prerequisites, with Python implementations wherever required.</p>
<p>In this <span>appendix</span>, we will cover the following topics:</p>
<ul>
<li>Linear algebraic operation and properties of vectors and matrices</li>
<li>Probability and common probabilistic functions</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Datasets and libraries</h1>
                </header>
            
            <article>
                
<p>In this <span>appendix</span>, we won't use a specific dataset; rather, we will use example values to show the workings. The libraries used are NumPy and scipy. In <a href="prac-cv_ch02.html" target="_blank">Chapter 2</a>,&#160;<em>Libraries, Development Platform, and Datasets</em> we saw the installation of the Anaconda tool, which has both NumPy and SciPy; therefore, there is no need of a new installation.</p>
<p>If Anaconda is not installed, then to install Numpy and SciPy, use the following command:</p>
<pre>pip install numpy scipy</pre>
<p>To plot a figure, we will use <kbd>matplotlib</kbd>. This also comes with Anaconda; however, if there is a need for installation, use the following command:</p>
<pre>pip install matplotlib</pre>
<p>To begin with the codes in the chapter, we will use this common import:</p>
<pre>import numpy as np<br/>import scipy<br/>import matplotlib.pyplot as plt</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear algebra</h1>
                </header>
            
            <article>
                
<p>Computer vision tools and techniques are highly dependent on linear algebraic operations. We will use see an explanation of basic to advanced operations that are required in developing CV applications.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Vectors</h1>
                </header>
            
            <article>
                
<p>In a 2D plane, vectors are denoted as a point&#160;<img height="16" width="56" class="fm-editor-equation" src="images/020ab844-5f5d-4de5-8339-2cc76ebd72d2.png"/>.</p>
<p>In this case, the magnitude of <kbd>p</kbd> is denoted as&#160;<img height="17" width="23" class="fm-editor-equation" src="images/9e346d48-09b8-4a5e-8835-3f1418d57213.png"/>&#160;and is given by the following:</p>
<p style="padding-left: 150px"><img height="33" width="120" class="fm-editor-equation" src="images/33607bcc-a031-4f4b-9b36-0ddc696fb7d0.png"/></p>
<p>In Python, a vector is denoted by a one-dimensional array, as follows:</p>
<pre>p = [1, 2, 3]</pre>
<p>Here, the common properties required are length of the vector and magnitude of the vector, which is given as follows:</p>
<pre>print(len(p))<br/><br/>&gt;&gt;&gt; 3</pre>
<p>Common vector operations are as follows:</p>
<ul>
<li>Addition</li>
<li>Subtraction</li>
<li>Vector multiplication</li>
<li>Vector norm</li>
<li>Orthogonality</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Addition</h1>
                </header>
            
            <article>
                
<p>Let's say there are two vectors denoted as follows:</p>
<pre class="CDPAlignLeft CDPAlign">v1 = np.array([2, 3, 4, 5])<br/>v2 = np.array([4, 5, 6, 7])</pre>
<p>The resulting vector is the element-wise sum, as follows:</p>
<pre>print(v1 + v2)<br/><br/>&gt;&gt;&gt; array([ 6, 8, 10, 12])</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Subtraction</h1>
                </header>
            
            <article>
                
<p>Subtraction is similar to addition; instead of the element-wise sum, we compute the element-wise difference:</p>
<pre>v1 = np.array([2, 3, 4, 5])<br/>v2 = np.array([4, 5, 6, 7])<br/>print(v1 - v2)<br/><br/>&gt;&gt;&gt; array([-2, -2, -2, -2])</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Vector multiplication</h1>
                </header>
            
            <article>
                
<p>There are two methods of computing vector multiplication:</p>
<ul>
<li>Inner product—this is also known as dot product and is the sum of element-wise products of two vectors:</li>
</ul>
<p style="padding-left: 180px"><img height="39" width="197" class="fm-editor-equation" src="images/4489a6aa-c2be-478a-b863-374244d28164.png"/></p>
<p style="padding-left: 60px">Where &#160;<img height="21" width="13" class="fm-editor-equation" src="images/fbc0fad0-c76a-4a6d-bacf-b94f56ef3ea7.png"/>&#160;and&#160;&#160;<img height="18" width="13" class="fm-editor-equation" src="images/c0bd8e6e-8050-41f0-881f-e127dbc178f9.png"/> are <img height="11" width="4" class="fm-editor-equation" src="images/85401d86-2e14-4b89-bab8-089a4b3c0542.png"/>th elements of vectors&#160;<img height="12" width="13" class="fm-editor-equation" src="images/8e7755a1-b3f2-4cf4-a4b0-930607923da1.png"/> and&#160;<img height="12" width="13" class="fm-editor-equation" src="images/359233f1-8fcd-4c78-ab56-e5ad2b25f24f.png"/> respectively.</p>
<p style="padding-left: 60px">In Python, we can compute this using NumPy:</p>
<pre style="padding-left: 60px">v1 = np.array([2, 3, 4, 5])<br/>v2 = np.array([4, 5, 6, 7])<br/>print(np.inner(v1, v2))<br/><br/>&gt;&gt;&gt; 82</pre>
<ul>
<li>Outer product—this takes in two vectors and computes a matrix</li>
</ul>
<p style="padding-left: 60px">&#160;<img height="12" width="67" class="fm-editor-equation" src="images/d3777a57-8a11-40c5-8166-cc0d0439b5f6.png"/>&#160;where each element <kbd>i</kbd>, <kbd>j</kbd> in <em>V<sub>3</sub></em> is given as:</p>
<p style="padding-left: 180px">&#160;<img height="24" width="123" class="fm-editor-equation" src="images/867ed238-0d5b-4156-9f5b-e704a88a2520.png"/></p>
<p style="padding-left: 60px">In Python we can compute this as follows:</p>
<pre style="padding-left: 60px">v1 = np.array([2, 3, 4, 5])<br/>v2 = np.array([4, 5, 6, 7])<br/>print(np.outer(v1, v2))<br/><br/>&gt;&gt;&gt; array([[ 8, 10, 12, 14],<br/>[12, 15, 18, 21],<br/>[16, 20, 24, 28],<br/>[20, 25, 30, 35]])</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Vector norm</h1>
                </header>
            
            <article>
                
<p>A <img height="18" width="12" class="fm-editor-equation" src="images/fa4e3c9c-b6b4-4451-ad66-a76c56a501d2.png"/>th order norm of a vector <em>V</em> is given as follows:</p>
<p style="padding-left: 150px"><img height="40" width="124" class="fm-editor-equation" src="images/657e7e9e-cf5e-4d23-9170-b38c904197ea.png"/></p>
<p>There are&#160;<span>two</span><span>&#160;</span><span>popular kinds of norms for vectors:</span></p>
<ul>
<li><img height="19" width="14" class="fm-editor-equation" src="images/650d12a4-fec9-4487-8293-2dd540d4f483.png"/> norm—this is given as <img height="20" width="57" class="fm-editor-equation" src="images/cb13a987-1b45-46ff-a011-fe8bae9f6e36.png"/>&#160;<span>and an example is as shown here:</span></li>
</ul>
<pre style="padding-left: 90px">v = np.array([2, 3, 4, 5])<br/>print(np.linalg.norm(v, ord=1))<br/><br/>&gt;&gt;&gt;14.0</pre>
<ul>
<li><img height="19" width="14" class="fm-editor-equation" src="images/156cdc07-de10-409f-ad83-f55b15c4be1e.png"/>&#160;norm—this is given as <img height="18" width="70" class="fm-editor-equation" src="images/e9c191c5-0c1a-463f-883f-b42d74cc72e6.png"/>&#160;<span>and an example is as follows:</span></li>
</ul>
<pre style="padding-left: 90px">v = np.array([2, 3, 4, 5])<br/>print(np.linalg.norm(v, ord=2))<br/><br/>&gt;&gt;&gt;7.34846922835</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Orthogonality</h1>
                </header>
            
            <article>
                
<p>Two vectors are said to be orthogonal if their inner product is zero. From the geometric point of view, if the two vectors are perpendicular, they are said to be orthogonal to each other:</p>
<pre>v1 = np.array([2, 3, 4, 5])<br/>v2 = np.array([1,-1,-1,1]) # orthogonal to v1<br/>np.inner(v1, v2)<br/><br/>&gt;&gt;&gt; 0</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Matrices</h1>
                </header>
            
            <article>
                
<p>Two-dimensional arrays are referred to as matrices and, in computer vision, these play a significant role. An image in the digital world is represented as a matrix; hence, the operations that we will study here are applicable to images as well.</p>
<p>Matrix <img height="15" width="12" class="fm-editor-equation" src="images/87cc0af2-5de1-4537-9109-a3b43d5c8df5.png"/>&#160;is denoted as follows:</p>
<p style="padding-left: 150px"><img height="98" width="214" class="fm-editor-equation" src="images/cf2d5797-d82c-4783-af6c-18c51b1d6156.png"/></p>
<p>Here, the shape of the matrix is m x n &#160;with <img height="12" width="16" class="fm-editor-equation" src="images/10eaa40b-1f57-4ade-866d-e0f4567ff54d.png"/>&#160;rows and&#160;<img height="12" width="11" class="fm-editor-equation" src="images/f44f6cfc-e944-4dc7-9f10-08b3a8c31806.png"/> columns. If <img height="9" width="39" class="fm-editor-equation" src="images/71eb906d-a20f-4811-82fa-3c1bac00b478.png"/>, the matrix is termed as a square matrix.</p>
<p>In Python, we can make a sample matrix, as follows:</p>
<pre>A = np.array([[1, 2, 3],[4, 5, 6], [7, 8, 9]])</pre>
<p>This is printed as follows:</p>
<pre>print(A)<br/><br/>&gt;&gt;&gt; array([[1, 2, 3],<br/>[4, 5, 6],<br/>[7, 8, 9]])</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Operations on matrices</h1>
                </header>
            
            <article>
                
<p>We will be performing similar operations on matrices as we did on vectors. The only difference will be in the way we perform these operations. To understand this in detail, go through the following sections.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Addition</h1>
                </header>
            
            <article>
                
<p>In order to perform the addition of two matrices <kbd>A</kbd> and <kbd>B</kbd>, both of them should be of the same shape. The addition operation is an element-wise addition done to create a matrix <kbd>C</kbd> of the same shape as <kbd>A</kbd> and <kbd>B</kbd>. Here is an example:</p>
<pre>A = np.array([[1, 2, 3],[4, 5, 6], [7, 8, 9]])<br/>B = np.array([[1,1,1], [1,1,1], [1,1,1]])<br/>C = A+B<br/>print(C)<br/><br/>&gt;&gt;&gt; array([[ 2, 3, 4],<br/>[ 5, 6, 7],<br/>[ 8, 9, 10]])</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Subtraction</h1>
                </header>
            
            <article>
                
<p>Similar to addition, subtracting matrix <kbd>B</kbd> from matrix&#160;<kbd>A</kbd> requires both of them to be of the same shape. The resulting matrix <kbd>C</kbd> will be of the same shape as <kbd>A</kbd> and <kbd>B</kbd>. The following is an example of subtracting <kbd>B</kbd> from <kbd>A</kbd>:</p>
<pre>A = np.array([[1, 2, 3],[4, 5, 6], [7, 8, 9]])<br/>B = np.array([[1,1,1], [1,1,1], [1,1,1]])<br/>C = A - B<br/>print(C)<br/><br/>&gt;&gt;&gt; array([[0, 1, 2],<br/>[3, 4, 5],<br/>[6, 7, 8]])</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Matrix multiplication</h1>
                </header>
            
            <article>
                
<p>Let there be two matrices: <kbd>A</kbd> with size <em>m x n</em> &#160;and <kbd>B</kbd> with size <em>q x p</em>. The assumption here is that&#160;<img height="11" width="43" class="fm-editor-equation" src="images/02925eb1-b19c-467d-aaf3-ffbd710d3db6.png"/>. Now, the two matrices of sizes <em>m x n</em>&#160;and <em>n x p</em> are compatible for matrix multiplication. The multiplication is given as follows:</p>
<p style="padding-left: 180px"><img height="14" width="57" class="fm-editor-equation" src="images/85f9afbc-9a90-46f3-8251-560263f08e64.png"/></p>
<p>Here, each element in <img height="12" width="10" class="fm-editor-equation" src="images/156acaf4-f860-4faf-bdbb-d454b7636383.png"/>&#160;is given as follows:</p>
<p style="padding-left: 180px"><img height="49" width="118" class="fm-editor-equation" src="images/d78baab5-f8e8-4770-aeeb-afd5510d4bd6.png"/></p>
<p>This is performed with Python, as follows:</p>
<pre># A matrix of size (2x3)<br/>A = np.array([[1, 2, 3],[4, 5, 6]])<br/># B matrix of size (3x2)<br/>B = np.array([[1, 0], [0, 1], [1, 0]])<br/>C = np.dot(A, B) # size (2x2)<br/>print(C)<br/><br/>&gt;&gt;&gt; array([[ 4, 2],<br/>[10, 5]])</pre>
<p>Since matrix multiplication depends on the order of multiplication, reversing the order may result in a different matrix or an invalid multiplication due to size mismatch.</p>
<p>We have seen the basic operations with matrices; now we will some properties of them.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Matrix properties</h1>
                </header>
            
            <article>
                
<p>There are a few properties that are used on matrices for executing mathematical operations. They are mentioned in detail in this section.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Transpose</h1>
                </header>
            
            <article>
                
<p>When we interchange columns and rows of a matrix with each other, the resulting matrix is termed as the transpose of the matrix and is denoted as <img height="15" width="19" class="fm-editor-equation" src="images/c1354917-508e-4b04-b062-0417ea7b9138.png"/>, for an original matrix <img height="15" width="12" class="fm-editor-equation" src="images/4ef088d6-9beb-4aa1-9061-c3e86d79d624.png"/>. An example of this is as follows:</p>
<pre>A = np.array([[1, 2, 3],[4, 5, 6]])<br/>np.transpose(A)<br/><br/>&gt;&gt;&gt; array([[1, 4],<br/>[2, 5],<br/>[3, 6]])</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Identity matrix</h1>
                </header>
            
            <article>
                
<p>This is a special kind of matrix with diagonal elements as <kbd>1</kbd>&#160;and all other elements as zero:</p>
<pre>I = np.identity(3) # size of identity matrix<br/>print(I)<br/><br/>&gt;&gt;&gt; [[ 1. 0. 0.]<br/>[ 0. 1. 0.]<br/>[ 0. 0. 1.]]</pre>
<p>An interesting property of the identity matrix is that it doesn't modify target matrix after multiplication, that is &#160;<img height="12" width="45" class="fm-editor-equation" src="images/ae534000-7fe4-42c5-87a9-22c0b85136bc.png"/>&#160;or&#160;<img height="12" width="45" class="fm-editor-equation" src="images/6ab373f5-b9ba-41c2-ad46-0f81c7bcf5fb.png"/>&#160;will result in&#160;<img height="10" width="29" class="fm-editor-equation" src="images/dbdcff17-60f6-4cb0-a474-59c2593751d3.png"/>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Diagonal matrix</h1>
                </header>
            
            <article>
                
<p>Extending the definition of an identity matrix, in a diagonal matrix, the entries of a matrix along the main diagonal are non-zero and the rest of the values are zero. An example is as follows:</p>
<pre>A = np.array([[12,0,0],[0,50,0],[0,0,43]])<br/><br/>&gt;&gt;&gt; array([[12, 0, 0],<br/>[ 0, 50, 0],<br/>[ 0, 0, 43]])</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Symmetric matrix</h1>
                </header>
            
            <article>
                
<p>&#160;In a symmetric matrix, the elements follow a property: <img height="15" width="70" class="fm-editor-equation" src="images/a81e61a1-073e-4fc9-b921-558404d57d1a.png"/>. This element wise property for a given symmetric matrix <img height="12" width="10" class="fm-editor-equation" src="images/dcba0d39-a0c9-48c1-9199-89644596afe5.png"/><span>,</span>&#160;can also be defined in terms of a transpose as&#160;<img height="15" width="48" class="fm-editor-equation" src="images/3b5d0f91-d7a5-40d7-b772-4d2b13ccfd9e.png"/>.&#160;</p>
<p>Let's consider an asymmetric square matrix (with size <em>n x n&#160;</em>) given as follows:</p>
<pre>A = np.array([[1, 2, 3],[4, 5, 6], [7, 8, 9]])</pre>
<p>Its transpose can be computed, as follows:</p>
<pre>A_T = np.transpose(A)<br/><br/>&gt;&gt;&gt; [[1 4 7]<br/>[2 5 8]<br/>[3 6 9]]</pre>
<p>We can show that <img height="15" width="44" class="fm-editor-equation" src="images/04422d51-ef98-4847-975d-ee83dab65d11.png"/>&#160;is a symmetric matrix:</p>
<pre>print(A + A_T)<br/><br/>&gt;&gt;&gt; [[ 2 6 10]<br/>[ 6 10 14]<br/>[10 14 18]]</pre>
<p>Here, we can see that the elements <img height="15" width="70" class="fm-editor-equation" src="images/f7fb4de1-e365-4884-b210-b4ae1a41576b.png"/>.</p>
<p>Also, we can compute anti-symmetric matrix as <img height="15" width="44" class="fm-editor-equation" src="images/9a658818-17ce-4ad9-9cfc-b9a4d861c36e.png"/>, where each element <img height="15" width="75" class="fm-editor-equation" src="images/2bed7459-9f42-4084-8224-eea4b2fd16af.png"/>:</p>
<pre>print(A - A_T)<br/><br/>&gt;&gt;&gt; [[ 0 -2 -4]<br/>[ 2 0 -2]<br/>[ 4 2 0]]</pre>
<p>An important property arises from this; we can break any square matrix into a summation of symmetric and anti-symmetric matrix, as follows:</p>
<p style="padding-left: 90px"><img height="17" width="227" class="fm-editor-equation" src="images/800c93cd-badb-4cd2-b800-aa8b046d1016.png"/></p>
<p>Continuing the Python Scripts as follows:&#160;</p>
<pre>symm = A + A_T<br/>anti_symm = A - A_T<br/>print(0.5*symm + 0.5*anti_symm)<br/><br/>&gt;&gt;&gt; [[ 1. 2. 3.]<br/>[ 4. 5. 6.]<br/><br/>[ 7. 8. 9.]]</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Trace of a matrix</h1>
                </header>
            
            <article>
                
<p>The trace of a matrix is the sum of all its diagonal elements:</p>
<pre>A = np.array([[1, 2, 3],[4, 5, 6], [7, 8, 9]])<br/><br/>np.trace(A)</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Determinant</h1>
                </header>
            
            <article>
                
<p>Geometrically, the absolute value of a determinant of a matrix is the volume enclosed by taking each row as a vector. This can be computed, as follows:</p>
<pre>A = np.array([[2, 3],[ 5, 6]])<br/>print(np.linalg.det(A))<br/><br/>&gt;&gt;&gt; -2.9999999999999982</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Norm of a matrix</h1>
                </header>
            
            <article>
                
<p>Continuing the norm formulation from the previous section on vectors, in a matrix, the most common type of norm is the Frobenius norm:</p>
<p style="padding-left: 120px"><img height="48" width="231" class="fm-editor-equation" src="images/7b0bbb7c-1e44-4b11-b6d6-385e3e057b2b.png"/></p>
<p>In Python we compute this as follows:&#160;</p>
<pre>A = np.array([[1, 2, 3],[4, 5, 6], [7, 8, 9]])<br/>np.linalg.norm(A)<br/><br/>&gt;&gt;&gt; 16.881943016134134</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting the inverse of a matrix&#160;</h1>
                </header>
            
            <article>
                
<p>An inverse of a matrix, denoted as <img height="15" width="26" class="fm-editor-equation" src="images/e2331019-1793-48d8-8d78-67303b16eb8c.png"/>, has an interesting property; <img height="17" width="136" class="fm-editor-equation" src="images/00c984ab-d764-4889-a631-96f2b7d0e825.png"/>. The inverse is unique for each matrix; however, not all matrices have inverse matrices. An example of inverse of matrix is as follows:</p>
<pre>A = np.array([[1, 2, 3],[5, 4, 6], [9, 8, 7]])<br/>A_inv = np.linalg.inv(A)<br/>print(A_inv)<br/><br/>&gt;&gt;&gt;[[ -6.66666667e-01 3.33333333e-01 4.93432455e-17]<br/>[ 6.33333333e-01 -6.66666667e-01 3.00000000e-01]<br/>[ 1.33333333e-01 3.33333333e-01 -2.00000000e-01]]</pre>
<p class="mce-root">Now, if we take a product of <img height="12" width="10" class="fm-editor-equation" src="images/d42cff23-486b-4408-9b4c-2b6184a48542.png"/>&#160;and <img height="14" width="25" class="fm-editor-equation" src="images/abe60ef9-ac86-49a6-9342-21a209fdcec3.png"/>, we get the following result:</p>
<pre>np.dot(A, A_inv)<br/><br/>&gt;&gt;&gt; [[ 1.00000000e+00 1.66533454e-16 -5.55111512e-17]<br/>[ 3.33066907e-16 1.00000000e+00 1.11022302e-16]<br/>[ 8.32667268e-16 -2.77555756e-16 1.00000000e+00]]</pre>
<p>We can see that the diagonal elements are <kbd>1</kbd> and all others are approximately <kbd>0</kbd>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Orthogonality</h1>
                </header>
            
            <article>
                
<p>Another property associated with a square matrix is orthogonality, where <img height="14" width="51" class="fm-editor-equation" src="images/cabcfb29-1bb2-4614-92bc-a0dca5ae5e01.png"/>&#160;or<img height="12" width="44" class="fm-editor-equation" src="images/84222b8f-b739-42a3-a941-2e921315fb59.png"/>. This is also leads to <img height="14" width="58" class="fm-editor-equation" src="images/6d9cc25f-6b55-4919-a0a4-18ef3d5c0f82.png"/>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Computing eigen values and eigen vectors</h1>
                </header>
            
            <article>
                
<p>The eigenvalue λ&#160;of a square matrix <kbd>A</kbd> has the property such that any transformation on it with eigen vector <img height="11" width="11" class="fm-editor-equation" src="images/d45a8fdd-3757-4214-9f27-8e87b8f0c9fc.png"/>&#160;is equal to the scalar multiplication of <span><img height="14" width="9" class="fm-editor-equation" src="images/0ba4f1bd-e748-408e-b5dc-c1c0ff8ad46e.png"/></span>&#160;with <img height="15" width="12" class="fm-editor-equation" src="images/220778cc-0dae-4349-a55a-aaec4ecacd56.png"/>:</p>
<p style="padding-left: 180px">&#160;<img height="13" width="55" class="fm-editor-equation" src="images/d7b43bee-2210-4bb8-89f1-05c194771b3e.png"/>&#160;where&#160;<img height="15" width="32" class="fm-editor-equation" src="images/a248ac34-737a-4e02-ba46-e1223b3da5a2.png"/></p>
<p>To compute eigenvalues and eigen vectors of <img height="16" width="13" class="fm-editor-equation" src="images/3a80067e-3bba-4517-b190-dc8cf1c61947.png"/>, we need to solve the characteristic equation, as follows:</p>
<p style="padding-left: 180px"><img height="20" width="91" class="fm-editor-equation" src="images/4b1aacfb-1ddc-482e-96f9-a72bcc321967.png"/></p>
<p>Here,&#160;<img height="12" width="7" class="fm-editor-equation" src="images/5b4a4312-b4c4-48b6-ace8-4394d7cbe8a6.png"/>&#160;is an identity matrix of the same size as <img height="13" width="11" class="fm-editor-equation" src="images/d5c33285-fda9-49db-bb3c-ca45a9126974.png"/>:</p>
<p>We can do this using NumPy as follows:</p>
<pre>A = np.array([[1, 2, 3],[5, 4, 6], [9, 8, 7]])<br/>eigvals, eigvectors = np.linalg.eig(A)<br/>print("Eigen Values: ", eigvals)<br/>print("Eigen Vectors:", eigvectors)<br/><br/>&gt;&gt;&gt; Eigen Values: [ 15.16397149 -2.30607508 -0.85789641]<br/>Eigen Vectors: [[-0.24668682 -0.50330679 0.54359359]<br/>[-0.5421775 -0.3518559 -0.8137192 ]<br/>[-0.80323668 0.78922728 0.20583261]]</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hessian matrix</h1>
                </header>
            
            <article>
                
<p>A first-order gradient matrix of <img height="12" width="10" class="fm-editor-equation" src="images/252b67d7-9a15-4d7f-b814-30b2d6bd1ff4.png"/>&#160;is formed by computing partial gradients on each element of <kbd>A</kbd>:</p>
<p style="padding-left: 120px"><img height="130" width="291" class="fm-editor-equation" src="images/e71b6667-5161-44cb-a7f1-5b66cf4b07c2.png"/></p>
<p>Similarly, the second-order gradient of&#160;<img height="12" width="10" class="fm-editor-equation" src="images/30829119-fa88-42fa-ba63-11d705719f15.png"/> for a function <img height="18" width="9" class="fm-editor-equation" src="images/3360cf59-6e94-4961-a422-100b4d09474a.png"/>&#160;is given as follows:</p>
<p style="padding-left: 120px"><img height="145" width="307" class="fm-editor-equation" src="images/e94cae52-f606-4645-9889-8d888fbe9e8f.png"/></p>
<p>The Hessian is denoted by&#160;<img height="16" width="70" class="fm-editor-equation" src="images/e40ac39a-fb49-40fa-b29e-ce6ed61b6b77.png"/>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Singular Value Decomposition</h1>
                </header>
            
            <article>
                
<p><strong>Singular Value Decomposition</strong> (<strong>SVD</strong>)&#160;is used to perform decomposition of a matrix <img height="11" width="9" class="fm-editor-equation" src="images/4d0709dd-1f76-44d0-9316-5348c2de806a.png"/>&#160;into <img height="13" width="41" class="fm-editor-equation" src="images/e8390785-e3c4-4ce9-b49f-c5825964fe5d.png"/>, where <img height="11" width="10" class="fm-editor-equation" src="images/848a59e4-cddc-4613-a63b-a62ba437d1cf.png"/>&#160;and <img height="12" width="21" class="fm-editor-equation" src="images/25546536-ba98-46fe-a70e-e0629ba170bf.png"/>&#160;are orthogonal matrices and <img height="14" width="12" class="fm-editor-equation" src="images/9a7c0bfc-41b6-4e73-9931-bbd72f06dcf0.png"/>&#160;is a diagonal matrix:</p>
<pre>A = np.array([[1, 2, 3],[5, 4, 6], [9, 8, 7]])<br/><br/>U, s, V = np.linalg.svd(A, full_matrices=True)</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to probability theory</h1>
                </header>
            
            <article>
                
<p>We have studied probability in several courses through university or elsewhere. In this section, the aim is to fill in the gaps, so that computer vision algorithms that require probability theory can be easily built upon. The motivation to use probability theory in computer vision is to model uncertainty.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">What are random variables?</h1>
                </header>
            
            <article>
                
<p>Random variables are used to define the possibilities of an event in terms of real numbers. The values it can represent are random and, by applying certain assumptions, we can restrict it to given range. To get started with random variables, we need to either compute a function that approximates its behavior or assume and prove our hypothesis function through experimentation. These functions are of two types:</p>
<ul>
<li>In the discrete domain, random variables' values are discrete. Then the function used to model probabilities is termed as <strong>Probability Mass Function</strong> (<strong>PMF</strong>). For example, let <img height="11" width="11" class="fm-editor-equation" src="images/9a01c3a0-eec6-4889-961d-c0da2b1470c6.png"/>&#160;be a discrete random variable; its PMF is given by <img height="17" width="57" class="fm-editor-equation" src="images/ff30e722-e6ec-4526-bd44-a0c6895111dc.png"/>, where <img height="12" width="7" class="fm-editor-equation" src="images/bad0294c-856d-4618-929e-b5dbe45a0dbd.png"/>&#160;is one of the <img height="14" width="15" class="fm-editor-equation" src="images/c79dbb2a-3a00-402f-b5ac-65c7dc7bd62d.png"/>&#160;different values of random variable <img height="11" width="11" class="fm-editor-equation" src="images/8b6ddfc2-2316-46f4-afe3-72955b56d19e.png"/>.</li>
<li>In the continuous domain, the function to model random variable's is termed as <strong>Probability Density Function</strong> (<strong>PDF</strong>), which takes in continuous domain values of a random variable <img height="11" width="11" class="fm-editor-equation" src="images/830f3962-3ffb-4fd5-8bba-4e8c9cd8eed6.png"/>&#160;to produce probabilities <img height="18" width="29" class="fm-editor-equation" src="images/3f393ffc-a85a-4d72-a746-9a8e540fa587.png"/>.</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Expectation</h1>
                </header>
            
            <article>
                
<p>For a discrete random variable <img height="11" width="11" class="fm-editor-equation" src="images/4a3fc903-dc58-4ab1-aeea-6296a683f2c8.png"/>, the expectation of a function <img height="16" width="8" class="fm-editor-equation" src="images/49c8958c-35d2-4ce9-a0ea-10950c4b42a4.png"/>&#160;is given as follows:</p>
<p style="padding-left: 150px"><img height="38" width="195" class="fm-editor-equation" src="images/3932fda6-f162-4fc6-9b5b-04db0358cee7.png"/></p>
<p>Here <img height="14" width="25" class="fm-editor-equation" src="images/0d6376c4-f74e-46f0-8bd2-4131840e9394.png"/>&#160;is the probability mass function.</p>
<p>For a continuous random variable <img height="11" width="11" class="fm-editor-equation" src="images/7e0bcb65-173d-4aec-aa38-3db40c04d902.png"/>, the expectation of a function&#160;<img height="18" width="9" class="fm-editor-equation" src="images/ea870582-c36b-4485-9876-53f1459d468d.png"/> is given as follows:</p>
<p style="padding-left: 150px"><img height="40" width="198" class="fm-editor-equation" src="images/617c943b-f4df-4909-9770-00853adc5ae2.png"/></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Variance</h1>
                </header>
            
            <article>
                
<p>To measure the quality of concentration of a random variable <img height="8" width="8" class="fm-editor-equation" src="images/885c3976-994e-493b-ba91-a1e53ba82a8f.png"/>, we use variance. Mathematically, it is defined as follows:</p>
<p style="padding-left: 150px"><img height="22" width="175" class="fm-editor-equation" src="images/a6e2059f-4674-43db-914d-4f76603c2ae7.png"/></p>
<p>This expression can also be converted into:</p>
<p style="padding-left: 150px"><img height="22" width="166" class="fm-editor-equation" src="images/7d9ed2c7-2694-46fa-8d1e-233f576ec14d.png"/></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Probability distributions</h1>
                </header>
            
            <article>
                
<p>The distributions are explained in detail in the following sections.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Bernoulli distribution</h1>
                </header>
            
            <article>
                
<p>In Bernoulli distribution the function is given as follows:</p>
<p style="padding-left: 150px"><img height="43" width="196" class="fm-editor-equation" src="images/e07a7462-9aa4-433c-995a-3143f17ab841.png"/></p>
<p>Here, the parameter is <img height="10" width="7" class="fm-editor-equation" src="images/62cf1039-437b-4928-a5d1-82e27f8067cb.png"/>&#160;and we can model this using SciPy as:</p>
<pre>from scipy.stats import bernoulli<br/>import matplotlib.pyplot as plt<br/><br/># parameters for bernoulli distribution<br/>p = 0.3<br/># create random variable<br/>random_variable = bernoulli( p)</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Binomial distribution</h1>
                </header>
            
            <article>
                
<p>In Binomial distribution the function is given as&#160;<img height="22" width="93" class="fm-editor-equation" src="images/cd0407f9-c2ea-45e5-a434-c1164b75fb6c.png"/>, with parameters <img height="9" width="8" class="fm-editor-equation" src="images/d6466402-c319-4dd8-8979-a002babd6b44.png"/>&#160;and <img height="14" width="9" class="fm-editor-equation" src="images/90b4a68f-cdd6-483a-bfe7-9bc978c1e941.png"/>. We can model this using SciPy:</p>
<pre>from scipy.stats import binom<br/>import matplotlib.pyplot as plt<br/><br/># parameters for binomial distribution<br/>n = 10<br/>p = 0.3<br/><br/># create random variable<br/>random_variable = binom(n, p)<br/><br/># compute probability mass function<br/>x = scipy.linspace(0,10,11)<br/><br/># plot<br/>plt.figure(figsize=(12, 8))<br/>plt.vlines(x, 0, random_variable.pmf(x))<br/>plt.show()</pre>
<p>The resulting plot can be seen as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="433" width="663" src="images/53b2d56d-4ffe-434a-bd24-dd3d157ecf41.png"/>&#160;</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Poisson distribution</h1>
                </header>
            
            <article>
                
<p>The function for Poisson distribution is as follows:</p>
<p style="padding-left: 150px"><img height="38" width="140" class="fm-editor-equation" src="images/eda0303f-9dfe-48b3-95a8-30e92e16b832.png"/></p>
<p>Here, the parameter is λ&#160;and an example script in SciPy is as follows:</p>
<pre>from scipy.stats import poisson<br/>import matplotlib.pyplot as plt<br/><br/># parameters for bernoulli distribution<br/>lambda_ = 0.1<br/><br/># create random variable<br/>random_variable = poisson(lambda_)<br/><br/># compute probability mass function<br/>x = scipy.linspace(0,5,11)<br/><br/># plot<br/>plt.figure(figsize=(12, 8))<br/>plt.vlines(x, 0, random_variable.pmf(x))<br/>plt.show()</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Uniform distribution</h1>
                </header>
            
            <article>
                
<p>A distribution between <img height="9" width="8" class="fm-editor-equation" src="images/f6ad6b2a-a777-43d9-bc93-18842871818e.png"/>&#160;and <img height="15" width="7" class="fm-editor-equation" src="images/24a9c917-ce4a-4eb0-a14e-0d11cf6f4c21.png"/>&#160;is said to be uniform if it follows this:</p>
<p style="padding-left: 120px"><img height="45" width="216" class="fm-editor-equation" src="images/25e11e52-31f6-4c46-8683-7c851df4bfed.png"/></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gaussian distribution</h1>
                </header>
            
            <article>
                
<p>One of the most common distributions used in computer vision, Gaussian distribution is defined as follows:</p>
<p style="padding-left: 120px"><img height="46" width="220" class="fm-editor-equation" src="images/877da509-4fc8-4635-865e-5ea55f1b3c48.png"/></p>
<p>Here, the parameters are <img height="14" width="11" class="fm-editor-equation" src="images/cb1138b5-e95d-4376-a4fc-1c3e0ac8d705.png"/>&#160;and <img height="10" width="10" class="fm-editor-equation" src="images/9695bc95-5359-477a-b276-7ce73f53a5ae.png"/>, which are also termed as <strong>mean</strong> and <strong>variance</strong>. A special case arises when <img height="14" width="11" class="fm-editor-equation" src="images/e50c4f52-d4d3-4a48-a446-991f017e264c.png"/>&#160;is 0 and&#160;<img height="10" width="10" class="fm-editor-equation" src="images/be5b9734-f354-4293-aa18-54186638d44b.png"/>&#160;is 1.0; it is termed as normal distribution. Using SciPy, we can model it as follows:</p>
<pre>from scipy.stats import norm<br/>import matplotlib.pyplot as plt<br/>import scipy<br/><br/># create random variable<br/>random_variable = norm()<br/><br/># compute probability mass function<br/>x = scipy.linspace(-5,5,20)<br/><br/># plot<br/>plt.figure(figsize=(12, 8))<br/>plt.vlines(x, 0, random_variable.pdf(x))<br/>plt.show()<br/><br/></pre>
<p>The resulting plot can be seen as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="385" width="612" src="images/615b50ed-da31-48d6-b2d0-5e3598733e79.png"/></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Joint distribution</h1>
                </header>
            
            <article>
                
<p>Joint distribution is used for two random variables, if we would like to find the effective probabilities if the two events associated with them happen together. Let <img height="11" width="11" class="fm-editor-equation" src="images/583cced1-d114-4365-b4c4-4ab479196da6.png"/>&#160;and <img height="16" width="9" class="fm-editor-equation" src="images/a895bbe7-4e82-444a-bb67-6bed9d650eee.png"/>&#160;be the two random variables; their joint distribution function is given by <img height="18" width="47" class="fm-editor-equation" src="images/a88bba39-7cfc-437e-b205-617fd6a5ccb0.png"/>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Marginal distribution</h1>
                </header>
            
            <article>
                
<p>In the case of joint distribution, we want to know the probability density function of one event assuming we can observe all the other events. We term this marginal distribution and it is given as follows:</p>
<p style="padding-left: 150px"><img height="40" width="150" class="fm-editor-equation" src="images/615b89e2-0bb5-43d9-b042-d083b710566f.png"/></p>
<p>For a discrete case, the marginal distribution is as follows:</p>
<p style="padding-left: 150px"><img height="38" width="140" class="fm-editor-equation" src="images/b08724e4-5e11-46d5-b394-54552f1662b0.png"/></p>
<p>Here, we are finding the marginal distribution of&#160;<img height="14" width="8" class="fm-editor-equation" src="images/e38f26ce-d07f-49fe-a830-86a084b9a140.png"/>&#160;with respect to <img height="11" width="11" class="fm-editor-equation" src="images/34322146-e6c1-4ca3-a8be-81b74f8cd781.png"/>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Conditional distribution</h1>
                </header>
            
            <article>
                
<p>We would like to compute the probabilities after having known the values of one of the random variables. This is denoted mathematically as <img height="16" width="47" class="fm-editor-equation" src="images/e149697a-3c81-4283-9827-5cc52c24584e.png"/>&#160;for the known variable <img height="12" width="7" class="fm-editor-equation" src="images/fd3f73fe-d7ac-48d1-a80e-7a31765112c5.png"/>, and a relation with joint probability distribution is given as follows:</p>
<p style="padding-left: 150px"><img height="44" width="143" class="fm-editor-equation" src="images/8a6535a4-9819-4bb0-90a1-b2910400a616.png"/></p>
<p>Here, <img height="17" width="44" class="fm-editor-equation" src="images/8746fc5e-4f15-4580-955a-7d0d61c65832.png"/> is the joint distribution and <img height="17" width="37" class="fm-editor-equation" src="images/f33a3678-5b00-4ed4-9673-8a66fac834b5.png"/>&#160;is the marginal distribution.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Bayes theorem</h1>
                </header>
            
            <article>
                
<p>An important theorem used implicitly in many computer vision applications is Bayes theorem, which extends conditional probabilities as follows in the case of a continuous random variable:</p>
<p style="padding-left: 150px"><img height="47" width="277" class="fm-editor-equation" src="images/888b97d4-6e14-4c89-89a8-e17560a14751.png"/></p>
<p>Here, we have:</p>
<p style="padding-left: 150px"><img height="42" width="235" class="fm-editor-equation" src="images/7834e1e0-6092-43b6-bbdc-2188b1b65e54.png"/></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this appendix, we explained some prerequisites for computer vision algorithms. Linear algebraic expressions explained here are used in geometric modifications of image, such as translation, rotation, and so on.</p>
<p>Probabilistic approaches are used in a range of applications including, but not limited to, object detection, segmentation, and tracking applications. As such, having a good understanding of these prerequisites will make our application implementation faster and more efficient.</p>


            </article>

            
        </section>
    </div>
</body>
</html>