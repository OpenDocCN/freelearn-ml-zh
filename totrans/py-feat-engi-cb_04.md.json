["```py\n pip install yellowbrick\n```", "```py\n     import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.datasets import fetch_california_housing\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n     X, y = fetch_california_housing(\n        return_X_y=True, as_frame=True)\n    ```", "```py\n     X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=0)\n    ```", "```py\n     min_value = int(X_train[\"HouseAge\"].min())\n    max_value = int(X_train[\"HouseAge\"].max())\n    ```", "```py\n     width = int((max_value - min_value) / 10)\n    ```", "```py\n     interval_limits = [i for i in range(\n        min_value, max_value, width)]\n    ```", "```py\n    <st c=\"5819\">[1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51]</st>\n    ```", "```py\n     interval_limits[0] = -np.inf\n    interval_limits[-1] = np.inf\n    ```", "```py\n     train_t = X_train.copy()\n    test_t = X_test.copy()\n    ```", "```py\n     train_t[\"HouseAge_disc\"] = pd.cut(\n        x=X_train[\"HouseAge\"],\n        bins=interval_limits,\n        include_lowest=True)\n    test_t[\"HouseAge_disc\"] = pd.cut(\n        x=X_test[\"HouseAge\"],\n        bins=interval_limits,\n        include_lowest=True)\n    ```", "```py\n     print(train_t[[\"HouseAge\", \"HouseAge_disc\"]].head(5))\n    ```", "```py\n     <st c=\"7033\">HouseAge HouseAge_disc</st>\n    <st c=\"7056\">1989         52.0   (46.0, inf]</st>\n    <st c=\"7078\">256           43.0  (41.0, 46.0]</st>\n    <st c=\"7100\">7887         17.0  (16.0, 21.0]</st>\n    <st c=\"7123\">4581</st> <st c=\"7128\">17.0  (16.0, 21.0]</st>\n    <st c=\"7146\">1993         50.0   (46.0, inf]</st>\n    ```", "```py\n     t1 = train_t[\"HouseAge_disc\"].value_counts(\n        normalize=True, sort=False)\n    t2 = test_t[\"HouseAge_disc\"].value_counts(\n        normalize=True, sort=False)\n    tmp = pd.concat([t1, t2], axis=1)\n    tmp.columns = [\"train\", \"test\"]\n    tmp.plot.bar(figsize=(8, 5))\n    plt.xticks(rotation=45)\n    plt.ylabel(\"Number of observations per bin\")\n    plt.xlabel('Discretized HouseAge')\n    plt.title(\"HouseAge\")\n    plt.show()\n    ```", "```py\n     from feature_engine.discretisation import EqualWidthDiscretiser\n    ```", "```py\n     variables = ['MedInc', 'HouseAge', 'AveRooms']\n    disc = EqualWidthDiscretiser(\n        bins=8, variables=variables)\n    ```", "```py\n     disc.fit(X_train)\n    ```", "```py\n     train_t = disc.transform(X_train)\n    test_t = disc.transform(X_test)\n    ```", "```py\n     plt.figure(figsize=(6, 12), constrained_layout=True)\n    for i in range(3):\n        # location of plot in figure\n        ax = plt.subplot(3, 1, i + 1)\n        # the variable to plot\n        var = variables[i]\n        # determine proportion of observations per bin\n        t1 = train_t[var].value_counts(normalize=True,\n            sort=False)\n        t2 = test_t[var].value_counts(normalize=True,\n            sort=False)\n        # concatenate proportions\n        tmp = pd.concat([t1, t2], axis=1)\n        tmp.columns = ['train', 'test']\n        # sort the intervals\n        tmp.sort_index(inplace=True)\n        # make plot\n        tmp.plot.bar(ax=ax)\n        plt.xticks(rotation=0)\n        plt.ylabel('Observations per bin')\n        ax.set_title(var)\n    plt.show()\n    ```", "```py\n     from sklearn.compose import ColumnTransformer\n    from sklearn.preprocessing import kBinsDiscretizer\n    ```", "```py\n     disc = KBinsDiscretizer(\n        n_bins=8, encode='ordinal', strategy='uniform')\n    ```", "```py\n    <st c=\"12378\">ct = ColumnTransformer(</st>\n     <st c=\"12401\">[(\"discretizer\", disc, variables)],</st>\n     <st c=\"12437\">remainder=\"passthrough\",</st>\n    <st c=\"12462\">).set_output(transform=\"pandas\")</st>\n    ```", "```py\n     ct.fit(X_train)\n    ```", "```py\n     train_t = ct.transform(X_train)\n    test_t = ct.transform(X_test)\n    ```", "```py\n     import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.datasets import fetch_california_housing\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n     X, y = fetch_california_housing(\n        return_X_y=True, as_frame=True)\n    ```", "```py\n     X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=0)\n    ```", "```py\n     train_t = X_train.copy()\n    test_t = X_test.copy()\n    ```", "```py\n     train_t[\"House_disc\"], interval_limits = pd.qcut(\n        x=X_train[\"HouseAge\"],\n        q=8,\n        labels=None,\n        retbins=True,\n    )\n    ```", "```py\n     print(train_t[[\"HouseAge\", \"House_disc\"]].head(5))\n    ```", "```py\n     <st c=\"20111\">HouseAge     House_disc</st>\n    <st c=\"20131\">1989       52.0   (44.0, 52.0]</st>\n    <st c=\"20154\">256        43.0   (37.0, 44.0]</st>\n    <st c=\"20176\">7887       17.0   (14.0, 18.0]</st>\n    <st c=\"20199\">4581       17.0   (14.0, 18.0]</st>\n    <st c=\"20268\">HouseAge</st> in the test set, using pandas <st c=\"20307\">cut()</st> with the interval limits determined in *<st c=\"20352\">step 5</st>*:\n\n    ```", "```py\n\n    ```", "```py\n     # determine proportion of observations per bin\n    t1 = train_t[\"House_disc\"].value_counts(\n        normalize=True)\n    t2 = test_t[\"House_disc\"].value_counts(normalize=True)\n    # concatenate proportions\n    tmp = pd.concat([t1, t2], axis=1)\n    tmp.columns = [\"train\", \"test\"]\n    tmp.sort_index(inplace=True)\n    # plot\n    tmp.plot.bar()\n    plt.xticks(rotation=45)\n    plt.ylabel(\"Number of observations per bin\")\n    plt.title(\"HouseAge\")\n    plt.show()\n    ```", "```py\n     from feature_engine.discretisation import EqualFrequencyDiscretiser\n    ```", "```py\n     variables = ['MedInc', 'HouseAge', 'AveRooms']\n    disc = EqualFrequencyDiscretiser(\n        q=8, variables=variables, return_boundaries=True)\n    ```", "```py\n    <st c=\"22048\">disc.binner_dict_</st> attribute.\n    ```", "```py\n     train_t = disc.transform(X_train)\n    test_t = disc.transform(X_test)\n    ```", "```py\n     plt.figure(figsize=(6, 12), constrained_layout=True)\n    for i in range(3):\n        # location of plot in figure\n        ax = plt.subplot(3, 1, i + 1)\n        # the variable to plot\n        var = variables[i]\n        # determine proportion of observations per bin\n        t1 = train_t[var].value_counts(normalize=True)\n        t2 = test_t[var].value_counts(normalize=True)\n        # concatenate proportions\n        tmp = pd.concat([t1, t2], axis=1)\n        tmp.columns = ['train', 'test']\n        # sort the intervals\n        tmp.sort_index(inplace=True)\n        # make plot\n        tmp.plot.bar(ax=ax)\n        plt.xticks(rotation=45)\n        plt.ylabel(\"Observations per bin\")\n        # add variable name as title\n        ax.set_title(var)\n     plt.show()\n    ```", "```py\n     from sklearn.preprocessing import KBinsDiscretizer\n    ```", "```py\n     disc = KBinsDiscretizer(\n        n_bins=8, encode='ordinal', strategy='quantile')\n    ```", "```py\n     disc.fit(X_train[variables])\n    ```", "```py\n     train_t = X_train.copy()\n    test_t = X_test.copy()\n    ```", "```py\n     train_t[variables] = disc.transform(\n        X_train[variables])\n    test_t[variables] = disc.transform(X_test[variables])\n    ```", "```py\n     import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.datasets import fetch_california_housing\n    ```", "```py\n     X, y = fetch_california_housing(\n        return_X_y=True, as_frame=True)\n    ```", "```py\n     X[\"Population\"].hist(bins=30)\n    plt.title(\"Population\")\n    plt.ylabel(\"Number of observations\")\n    plt.show()\n    ```", "```py\n     intervals = [0, 200, 500, 1000, 2000, np.inf]\n    ```", "```py\n     labels = [\"0-200\", \"200-500\", \"500-1000\", \"1000-2000\",\n        \">2000\"]\n    ```", "```py\n     X_t = X.copy()\n    X_t[«Population_limits»] = pd.cut(\n        X[\"Population\"],\n        bins=intervals,\n        labels=None,\n        include_lowest=True)\n    ```", "```py\n     X_t[«Population_range»] = pd.cut(\n        X[„Population\"],\n        bins=intervals,\n        labels=labels,\n        include_lowest=True)\n    ```", "```py\n     X_t[['Population', 'Population_range',\n        'Population_limits']].head()\n    ```", "```py\n     <st c=\"29979\">Population Population_range Population_limits</st>\n    <st c=\"30025\">0       322.0          200-500    (200.0, 500.0]</st>\n    <st c=\"30056\">1      2401.0            >2000     (2000.0, inf]</st>\n    <st c=\"30085\">2       496.0          200-500    (200.0, 500.0]</st>\n    <st c=\"30116\">3       558.0         500-1000   (500.0, 1000.0]</st>\n    <st c=\"30149\">4       565.0         500-1000   (500.0, 1000.0]</st>\n    ```", "```py\n     X_t['Population_range'\n        ].value_counts().sort_index().plot.bar()\n    plt.xticks(rotation=0)\n    plt.ylabel(\"Number of observations\")\n    plt.title(\"Population\")\n    plt.show()\n    ```", "```py\n     from feature_engine.discretisation import\n        ArbitraryDiscretiser\n    ```", "```py\n     intervals = {\n        \"Population\": [0, 200, 500, 1000, 2000, np.inf],\n        \"MedInc\": [0, 2, 4, 6, np.inf]}\n    ```", "```py\n     discretizer = ArbitraryDiscretiser(\n        binning_dict=intervals, return_boundaries=True)\n    ```", "```py\n     X_t = discretizer.fit_transform(X)\n    ```", "```py\n     import pandas as pd\n    from sklearn.cluster import KMeans\n    from sklearn.datasets import fetch_california_housing\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import KBinsDiscretizer\n    from yellowbrick.cluster import KElbowVisualizer\n    ```", "```py\n     X, y = fetch_california_housing(\n        return_X_y=True, as_frame=True)\n    ```", "```py\n     X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=0)\n    ```", "```py\n     variables = ['MedInc', 'HouseAge', 'AveRooms']\n    ```", "```py\n     k_means = KMeans(random_state=10)\n    ```", "```py\n     for variable in variables:\n        # set up a visualizer\n        visualizer = KElbowVisualizer(\n            k_means, k=(4,12),\n            metric='distortion',\n            timings=False)\n        visualizer.fit(X_train[variable].to_frame())\n        visualizer.show()\n    ```", "```py\n     disc = KBinsDiscretizer(\n        n_bins=6,\n        encode=\"onehot-dense\",\n        strategy=\"kmeans\",\n        subsample=None,\n    ).set_output(transform=\"pandas\")\n    ```", "```py\n     disc.fit(X_train[variables])\n    ```", "```py\n     disc.bin_edges_\n    ```", "```py\n    <st c=\"38587\">array([array([0.4999, 2.49587954, 3.66599029, 4.95730115, 6.67700141, 9.67326677, 15.0001]),</st>\n    <st c=\"38679\">array([1., 11.7038878, 19.88430419, 27.81472503, 35.39424098, 43.90930314, 52.]),</st>\n    <st c=\"38761\">array([0.84615385, 4.84568771, 6.62222005, 15.24138445, 37.60664483, 92.4473438, 132.53333333])], dtype=object)</st>\n    ```", "```py\n     train_features = disc.transform(X_train[variables])\n    test_features = disc.transform(X_test[variables])\n    ```", "```py\n     <st c=\"39300\">MedInc_0.0  MedInc_1.0  MedInc_2.0  MedInc_3.0  MedInc_4.0  MedInc_5.0  \\</st>\n    <st c=\"39368\">14740            0.0            0.0</st> <st c=\"39383\">1.0            0.0            0.0            0.0</st>\n    <st c=\"39398\">10101            0.0            0.0            0.0            1.0            0.0</st> <st c=\"39425\">0.0</st>\n    <st c=\"39428\">20566            0.0            0.0            1.0            0.0            0.0            0.0</st>\n    <st c=\"39458\">2670              1.0</st> <st c=\"39468\">0.0            0.0            0.0            0.0            0.0</st>\n    <st c=\"39487\">15709            0.0            0.0            0.0            1.0</st> <st c=\"39510\">0.0            0.0</st>\n     <st c=\"39517\">HouseAge_0.0  HouseAge_1.0  HouseAge_2.0  HouseAge_3.0  HouseAge_4.0  \\</st>\n    <st c=\"39584\">14740               0.0</st> <st c=\"39594\">0.0               1.0               0.0               0.0</st>\n    <st c=\"39610\">10101               0.0               0.0               0.0               1.0               0.0</st>\n    <st c=\"39636\">20566               0.0               0.0               0.0               1.0               0.0</st>\n    <st c=\"39662\">2670                 0.0               0.0               0.0</st> <st c=\"39679\">0.0               1.0</st>\n    <st c=\"39687\">15709               0.0               0.0               1.0               0.0               0.0</st>\n     <st c=\"39713\">HouseAge_5.0</st> <st c=\"39726\">AveRooms_0.0  AveRooms_1.0  AveRooms_2.0  AveRooms_3.0  \\</st>\n    <st c=\"39780\">14740               0.0               0.0               1.0               0.0</st> <st c=\"39803\">0.0</st>\n    <st c=\"39806\">10101               0.0               0.0               1.0               0.0               0.0</st>\n    <st c=\"39832\">20566               0.0               0.0</st> <st c=\"39847\">1.0               0.0               0.0</st>\n    <st c=\"39858\">2670                 0.0               0.0               1.0               0.0               0.0</st>\n    <st c=\"39883\">15709</st> <st c=\"39890\">0.0               1.0               0.0               0.0               0.0</st>\n     <st c=\"39909\">AveRooms_4.0  AveRooms_5.0</st>\n    <st c=\"39935\">14740               0.0</st> <st c=\"39946\">0.0</st>\n    <st c=\"39949\">10101               0.0               0.0</st>\n    <st c=\"39963\">20566               0.0               0.0</st>\n    <st c=\"39977\">2670                 0.0               0.0</st>\n    <st c=\"39990\">15709               0.0               0.0</st>\n    ```", "```py\n     import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import Binarizer\n    ```", "```py\n     data = pd.read_csv(\"bag_of_words.csv\")\n    ```", "```py\n     data.hist(bins=30, figsize=(20, 20), layout=(3,4))\n    plt.show()\n    ```", "```py\n     binarizer = Binarizer(threshold = 0) .set_output(transform=\"pandas\")\n    ```", "```py\n     data_t = binarizer.fit_transform(data)\n    ```", "```py\n     variables = data_t.columns.to_list()\n    plt.figure(figsize=(20, 20), constrained_layout=True)\n    for i in range(10):\n        ax = plt.subplot(3, 4, i + 1)\n        var = variables[i]\n        t = data_t[var].value_counts(normalize=True)\n        t.plot.bar(ax=ax)\n        plt.xticks(rotation=0)\n        plt.ylabel(\"Observations per bin\")\n        ax.set_title(var)\n    plt.show()\n    ```", "```py\n     import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.datasets import fetch_california_housing\n    from sklearn.model_selection import train_test_split\n    from sklearn.tree import plot_tree\n    from feature_engine.discretisation import DecisionTreeDiscretiser\n    ```", "```py\n     X, y = fetch_california_housing(return_X_y=True,\n        as_frame=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=0)\n    ```", "```py\n     variables = list(X.columns)[:-2]\n    ```", "```py\n     disc = DecisionTreeDiscretiser(\n        bin_output=\"boundaries\",\n        precision=3,\n        cv=3,\n        scoring=\"neg_mean_squared_error\",\n        variables=variables,\n        regression=True,\n        param_grid={\n            \"max_depth\": [1, 2, 3],\n            \"min_samples_leaf\": [10, 20, 50]},\n    )\n    ```", "```py\n     disc.fit(X_train, y_train)\n    ```", "```py\n     train_t = disc.transform(X_train)\n    test_t = disc.transform(X_test)\n    train_t[variables].head()\n    ```", "```py\n     disc = DecisionTreeDiscretiser(\n        bin_output=\"bin_number\",\n        cv=3,\n        scoring=\"neg_mean_squared_error\",\n        variables=variables,\n        regression=True,\n        param_grid={\n            \"max_depth\": [1, 2, 3],\n            \"min_samples_leaf\": [10, 20, 50]})\n    ```", "```py\n     train_t = disc.fit_transform(X_train, y_train)\n    test_t = disc.transform(X_test)\n    ```", "```py\n     disc = DecisionTreeDiscretiser(\n        bin_output=\"prediction\",\n        precision=1,\n        cv=3,\n        scoring=\"neg_mean_squared_error\",\n        variables=variables,\n        regression=True,\n        param_grid=\n            {\"max_depth\": [1, 2, 3],\n                \"min_samples_leaf\": [10, 20, 50]},\n    )\n    train_t = disc.fit_transform(X_train, y_train)\n    test_t = disc.transform(X_test)\n    ```", "```py\n     X_test[\"AveRooms\"].nunique(), test_t[\"AveRooms\"].nunique()\n    ```", "```py\n    (6034, 7)\n    ```", "```py\n     tree = disc.binner_dict_[\"AveRooms\"].best_estimator_\n    ```", "```py\n     fig = plt.figure(figsize=(20, 6))\n    plot_tree(tree, fontsize=10, proportion=True)\n    plt.show()\n    ```", "```py\n     plt.figure(figsize=(6, 12), constrained_layout=True)\n    for i in range(3):\n        ax = plt.subplot(3, 1, i + 1)\n        var = variables[i]\n        t1 = train_t[var].value_counts(normalize=True)\n        t2 = test_t[var].value_counts(normalize=True)\n        tmp = pd.concat([t1, t2], axis=1)\n        tmp.columns = [\"train\", \"test\"]\n        tmp.sort_index(inplace=True)\n        tmp.plot.bar(ax=ax)\n        plt.xticks(rotation=0)\n        plt.ylabel(\"Observations per bin\")\n        ax.set_title(var)\n    plt.show()\n    ```"]