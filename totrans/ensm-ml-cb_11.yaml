- en: Heterogeneous Ensemble for Text Classification Using NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Spam filtering using an ensemble of heterogeneous algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis of movie reviews using an ensemble model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text classification is a widely studied area of language processing and text
    mining. Using text classification mechanisms, we can classify documents into predefined
    categories based on their content.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll take a look at how to classify short text messages that
    get delivered to our mobile phones. While some messages we receive are important,
    others might represent a serious threat to our privacy. We want to be able to
    classify the text messages correctly in order to avoid spam and to avoid missing
    important messages.
  prefs: []
  type: TYPE_NORMAL
- en: Spam filtering using an ensemble of heterogeneous algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the SMS Spam Collection dataset from the UCI ML repository to create
    a spam classifier. Using the spam classifier, we can estimate the polarity of
    these messages. We can use various classifiers to classify the messages either
    as spam or ham.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we opt for algorithms such as Naive Bayes, random forest, and
    support vector machines to train our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We prepare our data using various data-cleaning and preparation mechanisms.
    To preprocess our data, we will perform the following sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert all text to lowercase
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove punctuation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove stop words
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform stemming
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We also process our data using **t****erm frequency-inverse data frequency**
    (**TF-IDF**), which tells us how often a word appears in a message or a document.
    TF is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TF = No. of times a word appears in a document / Total No. of words in the
    document`'
  prefs: []
  type: TYPE_NORMAL
- en: 'TF-IDF numerically scores the importance of a word based on how often the word
    appears in a document or a collection of documents. Simply put, the higher the
    TF-IDF score, the rarer the term. The lower the score, the more common it is.
    The mathematical representation of TD-IDF would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tfidf(w,d,D)= tf(t,d) × idf(t,D)`'
  prefs: []
  type: TYPE_NORMAL
- en: where w represents the word, d represents a document and D represents the collection
    of documents.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we'll use the SMS spam collection dataset, which has labelled
    messages that have been gathered for cellphone spam research. This dataset is
    available in the UCI ML repository and is also provided in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that for this example we import libraries such as `nltk` to prepare our
    data. We also import the `CountVectorizer` and `TfidVectorizer` modules from `sklearn.feature_extraction`.
    These modules are used for feature extraction in ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We reuse `plot_confusion_matrix` from the scikit-learn website to plot our
    confusion matrix. This is the same function that we''ve used in earlier chapters
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We set our working directory and read the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that we use `encoding='utf8'`. This is to instruct the `read_csv()` method
    to use UTF encoding to read the file. Python comes with a number of codecs. An
    exhaustive list is available at [https://docs.python.org/3/library/codecs.html#standard-encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).
  prefs: []
  type: TYPE_NORMAL
- en: 'After reading the data, we check whether it has been loaded properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We also check the number of observations and features in the dataset with `dataframe.shape`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We take a look at the counts of spam and ham messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also visualize the proportion of spam and ham messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding code, we see the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d71f67d-1ed3-4372-a211-55d2e492354f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also define a function to remove punctuation, convert the text to lowercase,
    and remove stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the defined `process_text()` function to our text variable in the
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We separate our feature and target variables, and split our data into `train`
    and `test` subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `CountVectorizer` module to convert the text into vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We also use the `TfidfVectorizer` module to convert the text into TF-IDF vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now move on to training our models. We use the following algorithms
    both on the count data and the TF-IDF data and see how the individual models perform:'
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also combine the model predictions to see the result from the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin with training our models, and see how they perform in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the model using the Naive Bayes algorithm. Apply this algorithm to both
    the count data and the TF-IDF data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the code to train the Naive Bayes on the count data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the `train` and `test` accuracy for the preceding model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ffd5dab-de2d-49a5-927c-ab6c7608e50c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Print the classification report using the `classification_report()` method.
    Pass `Y_test` and `nb_pred_test` to the `classification_report()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output, which shows the `precision`, `recall`,
    `f1-score`, and `support` for each class in the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0befb29a-c9bb-40fa-9f71-95dd23122c88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Pass `Y_test` and `nb_pred_test` to the `plot_confusion_matrix()` function
    to plot the confusion matrix, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot shows us the true negative, false positive, false negative,
    and true positive values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2c7b305-a462-4611-8e53-6bfe868ce3d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that in the *Getting ready* section earlier, we used the `TfidfVectorizer` module
    to convert text into TF-IDF vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the Naive Bayes model to the TF-IDF train data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the performance statistics of the TF-IDF test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we can see the output from the preceding code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b77d827c-6163-446d-ad09-b4efab591e6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fit the model with the support vector machine classifier with the count data.
    Use `GridSearchCV` to perform a search over the specified parameter values for
    the estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The grid-search provides us with the optimum model. We get to see the parameter
    values and the score of the optimum model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecdb5b29-7e5f-410e-94f2-3e40a8426650.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the `test` accuracy of the count data with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output from `classification_report()` and the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c07bc83-17ce-42e4-96df-7056ff7fe50d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Use SVM with the TF-IDF data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows the best score of the model trained with the SVM
    and RBF kernel on the TF-IDF data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ece4a414-3e4f-4767-8574-665c1e7a94fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Print the classification report and the confusion matrix for the preceding
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ba4c57e8-fb26-4d52-9d83-62b8d47b5bf9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fit the random forest model on the count data with grid search cross-validation,
    as we did for SVM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'A grid search of the random forest with the grid parameters returns the best
    parameters and the best score, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c25f7a64-fd07-4365-92f9-5213bfc47fe8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using a classification report and a confusion matrix, take a look at the performance
    metrics of the random forest model with the count data on our test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The report is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e81e861e-fe2f-4d73-80a0-abc8eae6df88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Build a model on a random forest with a grid-search on the TF-IDF data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Take the output of the `predict_proba()` methods to gather the predicted probabilities
    from each model to plot the ROC curves. The full code is provided in the code
    bundle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s a sample of the code to plot the ROC curve from the Naive Bayes model
    on the count data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'With the complete code provided in the code bundle, we can view the ROC plot
    from all the models and compare them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/385bf4df-cbb8-4f7d-a2f4-552a662169c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Average the probabilities from all the models and plot the ROC curves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the average result of the ROC and AUC scores in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c5a656e-c8ec-432c-b6c1-352c8a384032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Check the accuracy of the ensemble result. Create an array of the predicted
    results, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the mode of the predicted values for the respective observations
    to perform max-voting in order to get the final predicted result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the test accuracy for the models trained on the count data and TF-IDF
    data, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9c36b9ad-7304-436c-9e63-8f6ddd2f907f.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Getting ready* section, we imported all the required libraries and defined
    the function to plot the confusion matrix. We read our dataset, using UTF8 encoding.
    We checked the proportion of spam and ham messages in our dataset and used the `CountVectorizer`
    and `TfidfVectorizer` modules to convert the texts into vectors and TF-IDF vectors,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we built multiple models using various algorithms. We also applied
    each algorithm on both the count data and the TF-IDF data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The models need to be built in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes on count data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Naive Bayes on TF-IDF data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SVM with RBF kernel on count data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SVM with RBF kernel on TF-IDF data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random forest on count data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random forest on TF-IDF data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Naive Bayes classifier is widely used for text classification in machine
    learning. The Naive Bayes algorithm is based on the conditional probability of
    features belonging to a class. In *Step 1*, we built our first model with the
    Naive Bayes algorithm on the count data. In *Step 2*, we checked the performance
    metrics using `classification_report()` to see the `precision`, `recall`, `f1-score`,
    and `support`. In *Step 3*, we called `plot_confusion_matrix()` to plot the confusion
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in *Step 4*, we built the Naive Bayes model on the TF-IDF data and evaluated
    the performance in *Step 5*. In *Step 6* and *Step 7*, we trained our model using
    the support vector machine on the count data, evaluated its performance using
    the output from `classification_report`, and plotted the confusion matrix. We
    trained our SVM model using the RBF kernel. We also showcased an example of using
    `GridSearchCV` to find the best parameters. In *Step 8* and *Step 9*, we repeated
    what we did in *Step* 6 and *Step* 7, but this time, we trained the SVM on TF-IDF
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 10*, we trained a random forest model using grid search on the count
    data. We set **gini** and **entropy** for the `criterion` hyperparameter. We also
    set multiple values for the parameters, such as `min_samples_split`, `max_depth`, 
    and `min_samples_leaf`. In *Step 11*, we evaluated the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: We then trained another random forest model on the TF-IDF data in *Step 12*. Using
    the `predic_proba()` function, we got the class probabilities on our test data.
    We used the same in *Step 13* to plot the ROC curves with AUC scores annotated
    on the plots for each of the models. This helps us to compare the performance
    of the models.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 14*, we averaged the probabilities, which we got from the models for
    both the count and TF-IDF data. We then plotted the ROC curves for the ensemble
    results. From *Step 15* through to *Step 17*, we plotted the test accuracy for
    each of the models built on the count data as well as the TF-IDF data.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis of movie reviews using an ensemble model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment analysis is another widely studied research area in **natural language
    processing** (**NLP**). It's a popular task performed on reviews to determine
    the sentiments of comments provided by reviewers. In this example, we'll focus
    on analyzing movie review data from the **Internet Movie Database** (**IMDb**)
    and classifying it according to whether it is positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have movie reviews in `.txt` files that are separated into two folders:
    negative and positive. There are 1,000 positive reviews and 1,000 negative reviews.
    These files can be retrieved from GitHub.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have divided this case study into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The first part is to prepare the dataset. We'll read the review files that are
    provided in the `.txt` format, append them, label them as positive or negative
    based on which folder they have been put in, and create a `.csv` file that contains
    the label and text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second part, we'll build multiple base learners on both the count data
    and on the TF-IDF data. We'll evaluate the performance of the base learners and
    then evaluate the ensemble of the predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We set our working folder as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We set our path variable and iterate through the `.txt` files in the folders.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have a subfolder, `/txt_sentoken/pos`, which holds the TXT files
    for the positive reviews. Similarly, we have a subfolder, `/txt_sentoken/neg`,
    which holds the TXT files for the negative reviews.
  prefs: []
  type: TYPE_NORMAL
- en: The TXT files for the positive reviews are read and the reviews are appended
    in an array. We use the array to create a DataFrame, `df_pos`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: With the `head()` method, we take a look at the positive reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also iterate through the TXT files in the negative folder to read the negative
    reviews and append them in an array. We use the array to create a DataFrame, `df_neg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we merge the positive and negative DataFrames into a single DataFrame
    using the `concat()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can take a look at the prepared DataFrame with the `head()` and `tail()`
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d92dc9a6-754b-4c3f-ba5c-aec37b8ab46d.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding image, we notice that the positive and negative reviews have
    been added sequentially. The first half of the DataFrame holds the positive reviews,
    while the next half holds the negative reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s shuffle the data so that it doesn''t stay in sequential order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now see that the data in the DataFrame is shuffled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b4555d1-3c8c-4439-bff5-51dc0357469f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We validate the dimensions of the merged DataFrame to see whether it holds
    2,000 observations, which would be the result of combining the 1,000 negative
    and 1,000 positive reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we notice that we have 2,000 observations and 2 columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'We may also write the resulting DataFrame into another `.csv` file in order
    to avoid recreating the CSV file from the TXT files as we did in the preceding
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll define the `plot_confusion_matrix()` method that we have used earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now see the share of the positive and negative reviews in our data.
    In our case, the proportion is exactly 50:50:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9162d18-91d9-4185-a16e-967abbd75d46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now replace the "positive" label with "1" and the "negative" label"
    with "0":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We prepare our data using various data-cleaning and preparation mechanisms.
    We''ll follow the same sequence as we followed in the previous recipe to preprocess
    our data:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert all text to lowercase
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove punctuation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove stop words
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform stemming
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we''ll define a function to perform the preceding clean-up steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We call the preceding function to process our text data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We'll now build our base learners and evaluate the ensemble result.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by importing the remaining libraries we need:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Separate the target and predictor variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the train-test split of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `CountVectorizer()` to convert the text into vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `TfidfVectorizer()` to convert the text into TF-IDF vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We proceed by training the base learners on the count data and on the TF-IDF data.
    We train the base learners with random forest models, Naive Bayes models, and
    the support-vector classifier models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the random forest model using grid-search on the count data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate `precision`, `recall`, `f1-score`, `support`, and `accuracy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we can see the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/197e72d5-407a-4560-a910-694b77fdc3af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Train a random forest model on the TF-IDF data using grid-search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model''s performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the Naive Bayes model on the count data and check the accuracy of the
    test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the other model''s performance parameters with `classification_report()`
    and the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the Naive Bayes model on the TF-IDF data and evaluate its performance
    the same way we did for earlier models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a model with a support vector classifier algorithm with the linear kernel
    on the count data. We also grid-search the `C` parameter for the SVC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a model with the support vector classifier algorithm with the linear
    kernel on the TF-IDF data. We also grid-search the `C` parameter for the SVC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the ROC curve for each of the models. The code for one of the plots is
    shown here (the complete code is provided in this book''s code bundle):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we can compare the ROC curves of all the models
    we''ve trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9041a0b7-65de-4953-8d62-d9f33d01c47a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plot the ROC curves for the ensemble results on the count and TF-IDF data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fc14287a-bf2c-4069-b5a0-5547aa7d1817.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate the accuracy of the ensemble with max-voting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the test accuracy for each of the models trained on the count data and
    the TF-IDF data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot shows the accuracy comparison between the count data and
    the TF-IDF data across all models and the ensemble result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/145caddb-b65c-419c-a1d2-a5ab4350fe56.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started by importing the required libraries. In this chapter, we used a module
    called `glob`. The `glob` module is used to define the techniques to match a specified
    pattern to a path, a directory, and a filename. We used the glob module to look
    for all the files in a specified path. After that, we used the `open()` method
    to open each file in read mode. We read each file and appended it to form a dataset
    with all the review comments. We also created a label column to tag each review
    with a positive or negative tag.
  prefs: []
  type: TYPE_NORMAL
- en: However, after we appended all the positive and negative reviews, we noticed
    that they were added sequentially, which means the first half held all the positive
    reviews and the second half contained the negative reviews. We shuffled the data
    using the `shuffle()` method.
  prefs: []
  type: TYPE_NORMAL
- en: We cleaned our data by converting it to lowercase, removing the punctuation
    and stop words, performing stemming, and tokenizing the texts to create feature
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: In the *How to do it...* section, we started by importing the libraries in *Step
    1*. In *Step 2*, we separated our target and feature variables into *X* and *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: We split our data into train and test subsets in S*tep 3*. We used `test_size=.3`
    to split the data into train and test subsets.
  prefs: []
  type: TYPE_NORMAL
- en: In S*tep 4* and S*tep 5*, we used `CountVectorizer()` and `TfidfVectorizer()`
    to convert the text into vectors and the text into TF-IDF vectors, respectively.
    Note that with `CountVectorizer()`, we generated the `count_train` and `count_test`
    datasets. With `TfidfVectorizer()`, we generated the `tfidf_train` and `tfidf_test`
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 6*, we set our hyperparameters for grid-search to train a random forest
    model. We trained our random forest model on the count data and checked our train
    and test accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: We used the `predict()` and `predict_proba()` methods on our test data for all
    the models we built to predict the class as well as the class probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 7*, we generated the confusion matrix to evaluate the model's performance
    for the random forest model we built in the preceding step. In *Step 8* and *Step
    9*, we repeated the training for another random forest model on the TF-IDF data
    and evaluated the performance. We trained the Naive Bayes model on the count data
    and the TF-IDF data from *Step 10* through to *Step 12*.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 13* and *Step 14*, we trained the support vector classifier algorithm
    with the linear kernel on the count data and the TF-IDF data, respectively. In
    *Step 15*, we plotted the ROC curves with the AUC score for each of the base learners
    we built. We also plotted the RUC curves for the ensemble in *Step 16* to compare
    the performance with the base learners. Finally, in *Step 17*, we plotted the
    test accuracy of each of the models on the count and TF-IDF data.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today’s world, the availability and flow of textual information are limitless.
    This means we need various techniques to deal with these textual matters to extract
    meaningful information. For example, **parts-of-speech (POS) tagging** is one
    of the fundamental tasks in the NLP space. **POS tagging** is used to label words
    in a text with their respective parts of speech. These tags may then be used with more
    complex tasks, such as syntactic and semantic parsing, **machine translation**
    (**MT**), and question answering.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are eight main parts of speech:'
  prefs: []
  type: TYPE_NORMAL
- en: Nouns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pronouns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjectives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verbs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adverbs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepositions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conjunctions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interjections:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/6ab29f62-59f4-4509-99e2-976ee50b21b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The NLTK library has functions to get POS tags that can be applied to texts
    after tokenization. Let''s import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We take our previously created DataFrame `df_moviereviews`. We convert text
    into lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We preprocess the text by removing stop words, punctuation, lemmatization,
    and tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We take a look at the list of the first 10 tokens from the first movie review:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c975229-a885-4037-afc2-c6d12b525fbd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We perform POS tagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We print the first 10 POS tags for the first movie review:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We see the POS tagged words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/089f0a97-2f31-4b4b-8e9a-c4f8d5165df3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Chunking** is another process that can add more structure to POS tagging. Chunking
    is used for entity detection; it tags multiple tokens to recognize them as meaningful
    entities. There are various chunkers available; `NLTK` provides `ne_chunk`, which
    recognizes people (names), places, and organizations. Other frequently used chunkers
    include `OpenNLP`, `Yamcha`, and `Lingpipe`. It''s also possible to use a combination
    of chunkers and apply max-voting on the results to improve the classification''s
    performance.'
  prefs: []
  type: TYPE_NORMAL
