["```py\nif __name__ == '__main__':\n    train_data, train_labels = load_training_data(labels=None)\n    np.random.seed(75)\n    for _ in range(100):\n        indices = np.arange(len(train_data))\n        np.random.shuffle(indices)\n        for r in range(3):\n            for c in range(5):\n                i = 5 * r + c\n                ax = plt.subplot(3, 5, 1 + i)\n                sample = train_data[indices[i]]\n                ax.imshow(cv2.resize(sample, (32, 32)), cmap=cm.Greys_r)\n                ax.axis('off')\n        plt.tight_layout()\n        plt.show()\n        np.random.seed(np.random.randint(len(indices)))\n```", "```py\nARCHIVE_PATH = 'https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/'\n\ndef _download(filename, *, md5sum=None):\n    write_path = Path(__file__).parent / filename\n    if write_path.exists() and _md5sum_matches(write_path, md5sum):\n        return write_path\n    response = requests.get(f'{ARCHIVE_PATH}/{filename}')\n    response.raise_for_status()\n    with open(write_path, 'wb') as outfile:\n        outfile.write(response.content)\n    return write_path\n```", "```py\ndef _load_data(filepath, labels):\n    data, targets = [], []\n\n    with ZipFile(filepath) as data_zip:\n        for path in data_zip.namelist():\n            if not path.endswith('.csv'):\n                continue\n            # Only iterate over annotations files\n            ...\n```", "```py\n            ....\n            # Only iterate over annotations files\n            *dir_path, csv_filename = path.split('/')\n            label_str = dir_path[-1]\n            if labels is not None and int(label_str) not in labels:\n                continue\n            with data_zip.open(path, 'r') as csvfile:\n                reader = csv.DictReader(TextIOWrapper(csvfile), delimiter=';')\n                for img_info in reader:\n                    ... \n```", "```py\n                    img_path = '/'.join([*dir_path, img_info['Filename']])\n                    raw_data = data_zip.read(img_path)\n                    img = cv2.imdecode(np.frombuffer(raw_data, np.uint8), 1)\n\n                    x1, y1 = np.int(img_info['Roi.X1']), \n                    np.int(img_info['Roi.Y1'])\n                    x2, y2 = np.int(img_info['Roi.X2']), \n                    np.int(img_info['Roi.Y2'])\n\n                    data.append(img[y1: y2, x1: x2])\n                    targets.append(np.int(img_info['ClassId']))\n```", "```py\ndef load_training_data(labels):\n    filepath = _download('GTSRB-Training_fixed.zip',\n                         md5sum='513f3c79a4c5141765e10e952eaa2478')\n    return _load_data(filepath, labels)\n\ndef load_test_data(labels):\n    filepath = _download('GTSRB_Online-Test-Images-Sorted.zip',\n                         md5sum='b7bba7dad2a4dc4bc54d6ba2716d163b')\n    return _load_data(filepath, labels)\n```", "```py\ndef your_featurize(data: List[np.ndarry], **kwargs) -> np.ndarray: \n    ...\n```", "```py\nresized_images = (cv2.resize(x, scale_size) for x in data)\n```", "```py\ngray_data = (cv2.cvtColor(x, cv2.COLOR_BGR2GRAY) for x in resized_images)\n```", "```py\nscaled_data = (np.array(x).astype(np.float32).flatten() / 255 for x in gray_data)\n```", "```py\nreturn np.vstack([x - x.mean() for x in scaled_data])\n```", "```py\n    hsv_data = (cv2.cvtColor(x, cv2.COLOR_BGR2HSV) for x in resized_images)\n```", "```py\ndef surf_featurize(data, *, scale_size=(16, 16)):\n    all_kp = [cv2.KeyPoint(float(x), float(y), 1)\n              for x, y in itertools.product(range(scale_size[0]),\n                                            range(scale_size[1]))]\n```", "```py\n    surf = cv2.xfeatures2d_SURF.create(hessianThreshold=400)\n```", "```py\n    kp_des = (surf.compute(x, kp) for x in data)\n```", "```py\n    return np.array([d.flatten()[:num_surf_features]\n                     for _, d in kp_des]).astype(np.float32)\n```", "```py\ndef hog_featurize(data, *, scale_size=(32, 32)):\n    block_size = (scale_size[0] // 2, scale_size[1] // 2)\n    block_stride = (scale_size[0] // 4, scale_size[1] // 4)\n    cell_size = block_stride\n    hog = cv2.HOGDescriptor(scale_size, block_size, block_stride,\n                            cell_size, 9)\n    resized_images = (cv2.resize(x, scale_size) for x in data)\n    return np.array([hog.compute(x).flatten() for x in resized_images])\n```", "```py\ndef train(training_features: np.ndarray, training_labels: np.ndarray):\n```", "```py\ndef train_one_vs_all_SVM(X_train, y_train):\n    svm = cv2.ml.SVM_create()\n```", "```py\n    svm.setKernel(cv2.ml.SVM_LINEAR)\n    svm.setType(cv2.ml.SVM_C_SVC)\n    svm.setC(2.67)\n    svm.setGamma(5.383)\n```", "```py\n    svm.train(X_train, cv2.ml.ROW_SAMPLE, y_train)\n    return svm\n```", "```py\n        x_train = featurize(train_data)\n```", "```py\n        y_predict = model.predict(x_test)\n```", "```py\n        num_correct = sum(y_predict == y_test)\n```", "```py\ndef accuracy(y_predicted, y_true):\n    return sum(y_predicted == y_true) / len(y_true)\n```", "```py\ndef confusion_matrix(y_predicted, y_true):\n    num_classes = max(max(y_predicted), max(y_true)) + 1\n    ...\n```", "```py\n    conf_matrix = np.zeros((num_classes, num_classes))\n```", "```py\n    for r, c in zip(y_predicted, y_true):\n        conf_matrix[r, c] += 1\n```", "```py\n    return conf_matrix\n```", "```py\n    cm = confusion_matrix(y_predicted, y_true)\n    accuracy = cm.trace() / cm.sum()  # 0.95 in this case.\n```", "```py\ndef precision(y_predicted, y_true, positive_label):\n    ...\n```", "```py\n    cm = confusion_matrix(y_predicted, y_true)\n    true_positives = cm[positive_label, positive_label]\n```", "```py\n    total_positives = sum(cm[positive_label])\n```", "```py\n    return true_positives / total_positives\n```", "```py\ndef recall(y_predicted, y_true, positive_label):\n    cm = confusion_matrix(y_predicted, y_true)\n    true_positives = cm[positive_label, positive_label]\n```", "```py\n    class_members = sum(cm[:, positive_label])\n```", "```py\n    return true_positives / class_members\n```", "```py\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom data.gtsrb import load_training_data\nfrom data.gtsrb import load_test_data\nfrom data.process import grayscale_featurize, hog_featurize\n```", "```py\ndef main(labels):\n    train_data, train_labels = load_training_data(labels)\n    test_data, test_labels = load_test_data(labels)\n    y_train, y_test = np.array(train_labels), np.array(test_labels)\n    accuracies = {}\n    for featurize in [hog_featurize, grayscale_featurize, hsv_featurize, \n    surf_featurize]:\n       ...\n```", "```py\n        x_train = featurize(train_data)\n```", "```py\n        model = train_one_vs_all_SVM(x_train, y_train)\n```", "```py\n        x_test = featurize(test_data)\n        res = model.predict(x_test)\n        y_predict = res[1].flatten()\n```", "```py\n        accuracies[featurize.__name__] = accuracy(y_predict, y_test)\n```", "```py\n    plt.bar(accuracies.keys(), accuracies.values())\n    plt.ylim([0, 1])\n```", "```py\n    plt.axes().xaxis.set_tick_params(rotation=20)\n    plt.grid()\n    plt.title('Test accuracy for different featurize functions')\n    plt.show()\n```", "```py\ndef train_tf_model(X_train, y_train):\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(20, (8, 8),\n                               input_shape=list(UNIFORM_SIZE) + [3],\n                               activation='relu'),\n        tf.keras.layers.MaxPooling2D(pool_size=(4, 4), strides=4),\n        tf.keras.layers.Dropout(0.15),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dropout(0.15),\n        tf.keras.layers.Dense(43, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    model.fit(x_train, np.array(train_labels), epochs=10)\n    return model\n```"]