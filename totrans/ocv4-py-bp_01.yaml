- en: Fun with Filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this chapter is to develop a number of image processing filters
    and then apply them to the video stream of a webcam in real time. These filters
    will rely on various OpenCV functions to manipulate matrices through splitting,
    merging, arithmetic operations, and applying lookup tables for complex functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following three effects, which will help familiarize you
    with OpenCV, and we will build on these effects in future chapters of this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Warming and cooling filters**: We will implement our own **curve filters**
    using a lookup table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Black-and-white pencil sketch**: We will make use of two image-blending techniques,
    known as **dodging** and **burning**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cartoonizer**: We will combine a bilateral filter, a median filter, and adaptive
    thresholding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCV is an advanced toolchain. It often raises the question, that is, not
    how to implement something from scratch, but which precanned implementation to
    choose for your needs. Generating complex effects is not hard if you have a lot
    of computing resources to spare. The challenge usually lies in finding an approach
    that not only gets the job done but also gets it done in time.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of teaching the basic concepts of image manipulation through theoretical
    lessons, we will take a practical approach and develop a single end-to-end app
    that integrates a number of image filtering techniques. We will apply our theoretical
    knowledge to arrive at a solution that not only works but also speeds up seemingly
    complex effects so that a laptop can produce them in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to do the following using OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a black-and-white pencil sketch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying pencil sketch transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a warming and cooling filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cartoonizing an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning this will allow you to familiarize yourself with loading images into
    OpenCV and applying different transformations to those images using OpenCV. This
    chapter will help you learn the basics of how OpenCV operates, so we can focus
    on the internals of the algorithms in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at how to get everything up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the code in this book is targeted for **OpenCV 4.2** and has been tested
    on **Ubuntu 18.04**. Throughout this book, we will make extensive use of the `NumPy`
    package ([http://www.numpy.org](http://www.numpy.org)).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, this chapter requires the `UnivariateSpline` module of the `SciPy`
    package ([http://www.scipy.org](http://www.scipy.org)) and the **wxPython 4.0**
    **Graphical User Interface** (**GUI**) ([http://www.wxpython.org/download.php](http://www.wxpython.org/download.php))
    for cross-platform GUI applications. We will try to avoid further dependencies
    where possible.
  prefs: []
  type: TYPE_NORMAL
- en: For more book-level dependencies, see [Appendix A](a4f1f102-9f62-4644-bcde-f478cd28621a.xhtml),
    *Profiling and Accelerating Your Apps*, and [Appendix B](c86bca68-4b4a-4be6-8edd-67b1d43f0bfa.xhtml),
    *Setting Up a Docker Container*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code that we present in this chapter at our GitHub repository
    here: [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter1](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter1).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by planning the application we are going to create in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Planning the app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The final app must consist of the following modules and scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`wx_gui.py`: This module is our implementation of a basic GUI using `wxpython`.
    We will make extensive use of this file throughout the book. This module includes
    the following layouts:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wx_gui.BaseLayout`: This is a generic layout class from which more complicated
    layouts can be built.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chapter1.py`: This is the main script for this chapter. It contains the following
    functions and classes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chapter1.FilterLayout`: This is a custom layout based on `wx_gui.BaseLayout`,
    which displays the camera feed and a row of radio buttons that allows the user
    to select from the available image filters to be applied to each frame of the
    camera feed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chapter1.main`: This is the main routine function for starting the GUI application
    and accessing the webcam.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tools.py`: This is a Python module and has a lot of helper functions that
    we use in this chapter, which you can reuse for your projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next section demonstrates how to create a black-and-white pencil sketch.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a black-and-white pencil sketch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to obtain a pencil sketch (that is, a black-and-white drawing) of the
    camera frame, we will make use of two image-blending techniques, known as **dodging**
    and **burning**. These terms refer to techniques employed during the printing
    process in traditional photography; here, photographers would manipulate the exposure
    time of a certain area of a darkroom print in order to lighten or darken it. Dodging
    *lightens* an image, whereas burning *darkens* it. Areas that were not supposed
    to undergo changes were protected with a **mask**.
  prefs: []
  type: TYPE_NORMAL
- en: Today, modern image editing programs, such as **Photoshop** and **Gimp**, offer
    ways to mimic these effects in digital images. For example, masks are still used
    to mimic the effect of changing the exposure time of an image, wherein areas of
    a mask with relatively intense values will *expose* the image more, thus lightening
    the image. OpenCV does not offer a native function to implement these techniques;
    however, with a little insight and a few tricks, we will arrive at our own efficient
    implementation that can be used to produce a beautiful pencil sketch effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you search on the internet, you might stumble upon the following common
    procedure to achieve a pencil sketch from an **RGB** (**red**, **green**, and
    **blue**) color image:'
  prefs: []
  type: TYPE_NORMAL
- en: First, convert the color image to grayscale.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, invert the grayscale image to get a negative.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a **Gaussian blur** to the negative from *step 2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Blend the grayscale image (from *step 1*) with the blurred negative (from *step
    3*) by using **color dodge**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Whereas *steps 1* to *3* are straightforward, *step 4* can be a little tricky.
    Let's get that one out of the way first.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV 3 came with a pencil sketch effect right out of the box. The `cv2.pencilSketch`
    function uses a domain filter introduced in the 2011 paper, *Domain Transform
    for Edge-Aware Image and Video Processing*, by Eduardo Gastal and Manuel Oliveira.
    However, for the purposes of this book, we will develop our own filter.
  prefs: []
  type: TYPE_NORMAL
- en: The next section shows you how to implement dodging and burning in OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding approaches for using dodging and burning techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dodging decreases the exposure for areas of the image that we wish to make lighter
    (than before) in an image, `A`. In image processing, we usually select or specify
    areas of the image that need to be altered using masks. A mask, `B`, is an array
    of the same dimensions as the image on which it can be applied (think of it as
    a sheet of paper you use to cover the image that has holes in it). "Holes" in
    the sheet of paper are represented with `255` (or ones if we are working on the
    0-1 range) in an opaque region with zeros.
  prefs: []
  type: TYPE_NORMAL
- en: 'In modern image editing tools, such as Photoshop, the color dodging of the
    image `A` with the mask `B` is implemented by using the following ternary statement
    that acts on every pixel using the index `i`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The previous code essentially divides the value of the `A[i]` image pixel by
    the inverse of the `B[i]` mask pixel value (which are in the range of `0`-`255`),
    while making sure that the resulting pixel value will be in the range of (0, 255)
    and that we do not divide by 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could translate the previous complex-looking expression or code into the
    following naive Python function, which accepts two OpenCV matrices (`image` and
    `mask`) and returns the blended image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you might have guessed, although the previous code might be functionally
    correct, it will undoubtedly be horrendously slow. Firstly, the function uses
    the `for` loops, which are almost always a bad idea in Python. Secondly, the NumPy
    arrays (the underlying format of OpenCV images in Python) are optimized for the
    array calculations, so accessing and modifying each `image[c, r]` pixel separately
    will be really slow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we should realize that the `<<8` operation is the same as multiplying
    the pixel value with the number **2⁸** (**=256**), and that pixel-wise division
    can be achieved with the `cv2.divide` function. Thus, an improved version of our
    `dodge` function that takes advantage of matrix multiplication (which is faster)
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have reduced the `dodge` function to a single line! The new `dodge`
    function produces the same result as `dodge_naive`, but it is orders of magnitude
    faster than the naive version. In addition to this, `cv2.divide` automatically
    takes care of the division by zero, making the result zero, where `255 - mask`
    is zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a dodged version of `Lena.png` where we have dodges in the square with
    pixels in the range of (100:300, 100:300**)**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12387677-69ad-460a-9cc9-b339d74f9acd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image credit—"Lenna" by Conor Lawless is licensed under CC BY 2.0
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the lightened region is very obvious in the right photograph
    because the transition is very sharp. There are ways to correct this, one of which
    we will take a look at in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Let's learn how to obtain a Gaussian blur by using two-dimensional convolution
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Gaussian blur with two-dimensional convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Gaussian blur is implemented by convolving the image with a kernel of Gaussian
    values. Two- dimensional convolution is something that is used very widely in
    image processing. Usually, we have a big picture (let's look at a 5 x 5 subsection
    of that particular image), and we have a kernel (or filter) that is another matrix
    of a smaller size (in our example, 3 x 3).
  prefs: []
  type: TYPE_NORMAL
- en: In order to get the convolution values, let's suppose that we want to get the
    value at *location (2, 3)*. We place the kernel centered at the *location* (*2*,
    *3*), and we calculate the pointwise product of the overlay matrix (highlighted
    area, in the following image (red color)) with the kernel and take the overall
    sum. The resulting value (that is, 158.4) is the value we write on the other matrix
    at the *location (2, 3)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat this process for all elements, and the resulting matrix (the matrix
    on the right) is the convolution of the kernel with the image. In the following
    diagram, on the left, you can see the original image with the pixel values in
    the boxes (values higher than 100). We also see an orange filter with values in
    the bottom right of each cell (a collection of 0.1 or 0.2 that sum to 1). In the
    matrix on the right, you see the values when the filter is applied to the image
    on the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/658345ff-7977-4546-94d7-559959cbcdb2.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, for points on the boundaries, the kernel is not aligned with the
    matrix, so we have to figure out a strategy to give values for those points. There
    is no single good strategy that works for everything; some of the approaches are
    to either extend the border with zeros or extend with border values.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at how to transform a normal picture into a pencil sketch.
  prefs: []
  type: TYPE_NORMAL
- en: Applying pencil sketch transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the tricks that we learned from the previous sections in our bag, we are
    now ready to take a look at the entire procedure.
  prefs: []
  type: TYPE_NORMAL
- en: The final code can be found in the `convert_to_pencil_sketch` function within
    the `tools.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following procedure shows you how to convert a color image into grayscale.
    After that, we aim to blend the grayscale image with its blurred negative:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we convert an RGB image (`imgRGB`) into grayscale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have used `cv2.COLOR_RGB2GRAY` as a parameter to the `cv2.cvtColor`
    function, which changes the color spaces. Note that it does not matter whether
    the input image is RGB or BGR (which is the default for OpenCV); we will get a
    nice grayscale image in the end.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we invert the image and blur it with a large Gaussian kernel of size
    `(21,21)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `dodge` to blend the original grayscale image with the blurred inverse:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting image looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edd66dc7-744c-4e1e-80ef-facc5258db11.png)'
  prefs: []
  type: TYPE_IMG
- en: Image credit—"Lenna" by Conor Lawless is licensed under CC BY 2.0
  prefs: []
  type: TYPE_NORMAL
- en: Did you notice that our code can be optimized further? Let's take a look at
    how to optimize with OpenCV next.
  prefs: []
  type: TYPE_NORMAL
- en: Using an optimized version of a Gaussian blur
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Gaussian blur is basically a convolution with a Gaussian function. Well, one
    of the features of convolutions is their associative property. This means that
    it does not matter whether we first invert the image and then blur it, or first
    blur the image and then invert it.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we start with a blurred image and pass its inverse to the `dodge` function,
    then within that function the image will be inverted again (the `255-mask` part),
    essentially yielding the original image. If we get rid of these redundant operations,
    the optimized `convert_to_pencil_sketch` function will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For kicks and giggles, we want to lightly blend our transformed image (`img_sketch`)
    with a background image (`canvas`) that makes it look as though we drew the image
    on a canvas. So, before returning, we would like to blend with `canvas` if it
    exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We name our final function `pencil_sketch_on_canvas`, and it looks like this
    (together with optimizations):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is just our `convert_to_pencil_sketch` function, with the optional `canvas`
    argument that can add an artistic touch to the pencil sketch.
  prefs: []
  type: TYPE_NORMAL
- en: 'And we''re done! The final output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/719a8a97-3961-4214-b22e-17abd4c22c02.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's take a look at how to generate a warming and cooling filter in the next
    section, where you'll learn how to use **lookup tables** for image manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a warming and cooling filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we perceive images, our brain picks up on a number of subtle clues to infer
    important details about the scene. For example, in broad daylight, highlights
    may have a slightly yellowish tint because they are in direct sunlight, whereas
    shadows may appear slightly bluish because of the ambient light of the blue sky.
    When we view an image with such color properties, we might immediately think of
    a sunny day.
  prefs: []
  type: TYPE_NORMAL
- en: This effect is not a mystery to photographers, who sometimes purposely manipulate
    the white balance of an image to convey a certain mood. Warm colors are generally
    perceived as more pleasant, whereas cool colors are associated with night and
    drabness.
  prefs: []
  type: TYPE_NORMAL
- en: To manipulate the perceived color temperature of an image, we will implement
    a curve filter. These filters control how color transitions appear between different
    regions of an image, allowing us to subtly shift the color spectrum without adding
    an unnatural-looking overall tint to the image.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll look at how to manipulate color using curve shifting.
  prefs: []
  type: TYPE_NORMAL
- en: Using color manipulation via curve shifting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A curve filter is essentially a function, ***y = f (x)***, that maps an input
    pixel value, *x*, to an output pixel value, *y*. The curve is parameterized by
    a set of ***n + 1*** anchor points, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e3aa2ee-d206-42ee-bdeb-a0fc7aa27fb3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, each anchor point is a pair of numbers that represent the input and output
    pixel values. For example, the pair (30, 90) means that an input pixel value of
    30 is increased to an output value of 90\. Values between anchor points are interpolated
    along a smooth curve (hence, the name curve filter).
  prefs: []
  type: TYPE_NORMAL
- en: Such a filter can be applied to any image channel, be it a single grayscale
    channel or the **R** (**red**), **G** (**green**), and **B** (**blue**) channels
    of an RGB color image. Therefore, for our purposes, all values of *x* and *y*
    must stay between 0 and 255.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we wanted to make a grayscale image slightly brighter, we could
    use a curve filter with the following set of control points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b4f25f1-a4eb-491d-9e13-2fab7cf96333.png)'
  prefs: []
  type: TYPE_IMG
- en: This would mean that all input pixel values except **0** and **255** would be
    increased slightly, resulting in an overall brightening effect on the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want such filters to produce natural-looking images, it is important
    to respect the following two rules:'
  prefs: []
  type: TYPE_NORMAL
- en: Every set of anchor points should include **(0,0)** and **(255,255)**. This
    is important in order to prevent the image from appearing as if it has an overall
    tint, as black remains black and white remains white.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *f(x)* function should be monotonously increasing. In other words, by increasing
    *x*, *f(x)* either stays the same or increases (that is, it never decreases).
    This is important for making sure that shadows remain shadows and highlights remain
    highlights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next section demonstrates how to implement a curve filter using lookup tables.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a curve filter using lookup tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Curve filters are computationally expensive because the values of *f(x)* must
    be interpolated whenever *x* does not coincide with one of the prespecified anchor
    points. Performing this computation for every pixel of every image frame that
    we encounter would have dramatic effects on performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we make use of a lookup table. Since there are only 256 possible pixel
    values for our purposes, we need to calculate ***f(x)*** only for all the 256
    possible values of ***x***. Interpolation is handled by the `UnivariateSpline`
    function of the `scipy.interpolate` module, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `return` argument of the function is a list of 256 elements that contains
    the interpolated *f(x)* values for every possible value of *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'All we need to do now is to come up with a set of anchor points, (*x[i]*, *y[i]*),
    and we are ready to apply the filter to a grayscale input image (`img_gray`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The result looks like this (the original image is on the *left*, and the transformed
    image is on the *right*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e47feb6-4d50-4fac-b177-981ba3253ca7.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we'll design the warming and cooling effect. You will also
    learn how to apply lookup tables to colored images, and how warming and cooling
    effects work.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the warming and cooling effect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the mechanism to quickly apply a generic curve filter to any image channel
    in place, we can now turn to the question of how to manipulate the perceived color
    temperature of an image. Again, the final code will have its own function in the
    `tools` module.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a minute to spare, I advise you to play around with the different
    curve settings for a while. You can choose any number of anchor points and apply
    the curve filter to any image channel you can think of (red, green, blue, hue,
    saturation, brightness, lightness, and so on). You could even combine multiple
    channels, or decrease one and shift another to the desired region. *What will
    the result look like?*
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if the number of possibilities dazzles you, take a more conservative
    approach. First, by making use of our `spline_to_lookup_table` function developed
    in the preceding steps, let''s define two generic curve filters: one that (by
    trend) increases all the pixel values of a channel and one that generally decreases
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s examine how we could apply lookup tables to an RGB image. OpenCV
    has a nice function called `cv2.LUT` that takes a lookup table and applies it
    to a matrix. So, first, we have to decompose the image into different channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we apply a filter to each channel if desired:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Doing this for all the three channels in an RGB image, we get the following
    helper function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The easiest way to make an image appear as if it was taken on a hot, sunny
    day (maybe close to sunset) is to increase the reds in the image and make the
    colors appear vivid by increasing the color saturation. We will achieve this in
    two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Increase the pixel values in the **R channel** (from RGB image) and decrease
    the pixel values in the **B channel** of an RGB color image using `INCREASE_LOOKUP_TABLE`
    and `DECREASE_LOOKUP_TABLE`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Transform the image into the **HSV** color space (**H** means **hue**, **S**
    means **saturation**, and **V** means **value**), and increase the **S channel**
    using `INCREASE_LOOKUP_TABLE`. This can be achieved with the following function,
    which expects an RGB color image and a lookup table to apply (similar to the `apply_rgb_filters`
    function) as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c931305-7b18-4362-8a56-26647fa3c649.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Analogously, we can define a cooling filter that increases the pixel values
    in the B channel, decreases the pixel values in the R channel of an RGB image,
    converts the image into the HSV color space, and decreases color saturation via
    the S channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4683d58-a8b6-418d-b0eb-03c672dd4c87.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's explore how to cartoonize an image in the next section, where we'll learn
    what a bilateral filter is and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Cartoonizing an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the past few years, professional cartoonizer software has popped up all
    over the place. In order to achieve a basic cartoon effect, all we need is a **bilateral
    filter** and some **edge detection**.
  prefs: []
  type: TYPE_NORMAL
- en: The bilateral filter will reduce the color palette or the numbers of colors
    that are used in the image. This mimics a cartoon drawing, wherein a cartoonist
    typically has few colors to work with. Then, we can apply edge detection to the
    resulting image to generate bold silhouettes. The real challenge, however, lies
    in the computational cost of bilateral filters. We will, therefore, use some tricks
    to produce an acceptable cartoon effect in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will adhere to the following procedure to transform an RGB color image into
    a cartoon:'
  prefs: []
  type: TYPE_NORMAL
- en: First, apply a bilateral filter to reduce the color palette of the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, convert the original color image into grayscale.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, apply a **median blur** to reduce image noise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use **adaptive thresholding** to detect and emphasize the edges in an edge mask.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, combine the color image from *step 1* with the edge mask from *step
    4*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the upcoming sections, we will learn about the previously mentioned steps
    in detail. First, we'll learn how to use a bilateral filter for edge-aware smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: Using a bilateral filter for edge-aware smoothing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A strong bilateral filter is ideally suitable for converting an RGB image into
    a color painting or a cartoon, because it smoothens the flat regions while keeping
    the edges sharp. The only drawback of this filter is its computational cost—it
    is orders of magnitude slower than other smoothing operations, such as a Gaussian
    blur.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first measure to take when we need to reduce the computational cost is
    to perform an operation on an image of low resolution. In order to downscale an
    RGB image (`imgRGB`) to a quarter of its size (that is, reduce the width and height
    to half), we could use `cv2.resize`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: A pixel value in the resized image will correspond to the pixel average of a
    small neighborhood in the original image. However, this process may produce image
    artifacts, which is also known as **aliasing**. While image aliasing is a big
    problem on its own, the negative effect might be enhanced by subsequent processing,
    for example, edge detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better alternative might be to use the **Gaussian pyramid** for downscaling
    (again to a quarter of the original size). The Gaussian pyramid consists of a
    blur operation that is performed before the image is resampled, which reduces
    any aliasing effects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'However, even at this scale, the bilateral filter might still be too slow to
    run in real time. Another trick is to repeatedly (say, five times) apply a small
    bilateral filter to the image instead of applying a large bilateral filter once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The three parameters in `cv2.bilateralFilter` control the diameter of the pixel
    neighborhood (`d=9`) and the standard deviation of the filter in the color space
    (`sigmaColor=9`) and coordinate space (`sigmaSpace=7`).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the final code to run the bilateral filter that we use is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Downsample the image using multiple `pyrDown` calls:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, apply multiple bilateral filters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, upsample it to the original size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The result looks like a blurred color painting of a creepy programmer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96cbe5b4-6504-43aa-9795-00cd3cd0caaa.png)'
  prefs: []
  type: TYPE_IMG
- en: The next section shows you how to detect and emphasize prominent edges.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and emphasizing prominent edges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, when it comes to edge detection, the challenge often does not lie in
    how the underlying algorithm works, but instead lies in which particular algorithm
    to choose for the task at hand. You might already be familiar with a variety of
    edge detectors. For example, **Canny edge detection** (`cv2.Canny`) provides a
    relatively simple and effective method to detect edges in an image, but it is
    susceptible to noise.
  prefs: []
  type: TYPE_NORMAL
- en: The **Sobel** operator (`cv2.Sobel`) can reduce such artifacts, but it is not
    rotationally symmetric. The **Scharr** operator (`cv2.Scharr`) was targeted at
    correcting this but only looks at the first image derivative. If you are interested,
    there are even more operators for you, such as the **Laplacian ridge operator**
    (which includes the second derivative), but they are far more complex. And in
    the end, for our specific purposes, they might not look better, perhaps because
    they are as susceptible to lighting conditions as any other algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of this project, we will choose a function that might not even
    be associated with conventional edge detection—`cv2.adaptiveThreshold`. Like `cv2.threshold`,
    this function uses a threshold pixel value to convert a grayscale image into a
    binary image. That is, if a pixel value in the original image is above the threshold,
    then the pixel value in the final image will be 255\. Otherwise, it will be 0.
  prefs: []
  type: TYPE_NORMAL
- en: However, the beauty of adaptive thresholding is that it does not look at the
    overall properties of the image. Instead, it detects the most salient features
    in each small neighborhood independently, without regard to the global image characteristics.
    This makes the algorithm extremely robust to lighting conditions, which is exactly
    what we want when we seek to draw bold black outlines around objects, and people
    in a cartoon.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it also makes the algorithm susceptible to noise. To counteract this,
    we will preprocess the image with a median filter. A median filter does what its
    name suggests: it replaces each pixel value with the median value of all the pixels
    in a small pixel neighborhood. Therefore, to detect edges, we follow this short
    procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first convert the RGB image (`rgb_image`) into grayscale (`img_gray`) and
    then apply a median blur with a seven-pixel local neighborhood:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After reducing the noise, it is now safe to detect and enhance the edges using
    adaptive thresholding. Even if there is some image noise left, the `cv2.ADAPTIVE_THRESH_MEAN_C`
    algorithm with `blockSize=9` will ensure that the threshold is applied to the
    mean of a 9 x 9 neighborhood minus `C=2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the adaptive thresholding looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3f8ac30-e43d-4040-a25d-2880426348d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, let's look at how to combine colors and outlines to produce a cartoon
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Combining colors and outlines to produce a cartoon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last step is to combine the two previously achieved effects. Simply fuse
    the two effects together into a single image using `cv2.bitwise_and`. The complete
    function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/087be866-ebdc-44ac-a391-a39305957070.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we'll set up the main script and design a GUI application.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we implemented a couple of nice filters that show
    how we can get nice effects with OpenCV. In this section, we want to build an
    interactive application that will allow you to apply these filters in real time
    to your laptop camera.
  prefs: []
  type: TYPE_NORMAL
- en: So, we need to write a **user interface** (**UI**) that will allow us to capture
    the camera stream and have some buttons so that you can select which filter you
    want to apply. We will start by setting up the camera capture with OpenCV. Then,
    we will build a nice interface around it using `wxPython`.
  prefs: []
  type: TYPE_NORMAL
- en: Running the app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the application, we will turn to the `chapter1.py` script. Follow these
    steps to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first start by importing all the necessary modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also have to import a generic GUI layout (from `wx_gui`) and all the
    designed image effects (from `tools`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenCV provides a straightforward way to access a computer''s webcam or camera
    device. The following code snippet opens the default camera ID (`0`) of a computer
    using `cv2.VideoCapture`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to give our application a fair chance to run in real time, we will
    limit the size of the video stream to `640` x `480` pixels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the `capture` stream can be passed to our GUI application, which is an
    instance of the `FilterLayout` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: After we create `FilterLayout`, we center the layout, so it appears in the center
    of the screen. And we call `Show()` to actually show the layout. Finally, we call
    `app.MainLoop()`, so the application starts working, receiving, and processing
    events.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing left to do now is to design the said GUI.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping the GUI base class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `FilterLayout` GUI will be based on a generic, plain layout class called
    `BaseLayout`, which we will be able to use in subsequent chapters as well.
  prefs: []
  type: TYPE_NORMAL
- en: The `BaseLayout` class is designed as an **abstract base class**. You can think
    of this class as a blueprint or recipe that will apply to all the layouts that
    we are yet to design, that is, a skeleton class that will serve as the backbone
    for all of our future GUI code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start the file by importing the packages that we will use—the `wxPython`
    module, which we use to create the GUI; `numpy`, which we use to do matrix manipulations;
    and OpenCV (of course):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The class is designed to be derived from the blueprint or skeleton, that is,
    the `wx.Frame` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Later on, when we write our own custom layout (`FilterLayout`), we will use
    the same notation to specify that the class is based on the `BaseLayout` blueprint
    (or skeleton) class, for example, in `class FilterLayout(BaseLayout):`. But for
    now, let's focus on the `BaseLayout` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'An abstract class has at least one abstract method. We are going to make the
    method abstract by ensuring that if the method stays unimplemented, the application
    will not run and we throw an exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Then, any class that is derived from it, such as `FilterLayout`, must specify
    a full implementation of that method. This will allow us to create custom layouts,
    as you will see in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: But first, let's proceed to the GUI constructor.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the GUI constructor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `BaseLayout` constructor accepts an ID (`-1`), a title string (`''Fun with
    Filters''`), a video capture object, and an optional argument that specifies the
    number of frames per second. Then, the first thing to do in the constructor is
    to try to read a frame from the captured object in order to determine the image
    size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the image size to prepare a buffer that will store each video frame
    as a bitmap and to set the size of the GUI. Because we want to display a bunch
    of control buttons below the current video frame, we set the height of the GUI
    to `self.imgHeight + 20`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will build a basic layout for our application with a
    video stream and some buttons using `wxPython`.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about a basic GUI layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most basic layout consists of only a large black panel that provides enough
    room to display the video feed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In order for the layout to be extendable, we add it to a vertically arranged
    `wx.BoxSizer` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we specify an abstract method, `augment_layout`, for which we will not
    fill in any code. Instead, any user of our base class can make their own custom
    modifications to the basic layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we just need to set the minimum size of the resulting layout and center
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The next section shows you how to handle video streams.
  prefs: []
  type: TYPE_NORMAL
- en: Handling video streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The video stream of the webcam is handled by a series of steps that begin with
    the `__init__` method. These steps might appear overly complicated at first, but
    they are necessary in order to allow the video to run smoothly, even at higher
    frame rates (that is, to counteract flickering).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `wxPython` module works with events and callback methods. When a certain
    event is triggered, it can cause a certain class method to be executed (in other
    words, a method can *bind* to an event). We will use this mechanism to our advantage
    and display a new frame every so often using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a timer that will generate a `wx.EVT_TIMER` event whenever `1000./self.fps`
    milliseconds have passed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Whenever the timer is up, we want the `_on_next_frame` method to be called.
    It will try to acquire a new video frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `_on_next_frame` method will process the new video frame and store the
    processed frame in a bitmap. This will trigger another event, `wx.EVT_PAINT`.
    We want to bind this event to the `_on_paint` method, which will paint the display
    of the new frame. So, we create a placeholder for the video and bind `wx.EVT_PAINT`
    to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `_on_next_frame` method grabs a new frame and, once done, sends the frame
    to another method, `process_frame`, for further processing (which is an abstract
    method and should be implemented by the child class):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The processed frame (`frame`) is then stored in a bitmap buffer (`self.bmp`).
    Calling `Refresh` triggers the aforementioned `wx.EVT_PAINT` event, which binds
    to `_on_paint`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The `paint` method then grabs the frame from the buffer and displays it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The next section shows you how to create a custom filter layout.
  prefs: []
  type: TYPE_NORMAL
- en: Drafting a custom filter layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we are almost done! If we want to use the `BaseLayout` class, we need to
    provide code for the two methods that were previously left blank, which are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`augment_layout`: This is where we can make task-specific modifications to
    the GUI layout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`process_frame`: This is where we perform task-specific processing on each
    captured frame of the camera feed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also need to change the constructor to initialize any parameters we will
    need—in this case, the canvas background for the pencil sketch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To customize the layout, we arrange a number of radio buttons horizontally—one
    button per image effect mode. Here, the `style=wx.RB_GROUP` option makes sure
    that only one of `radio buttons` can be selected at a time. And to make these
    changes visible, `pnl` needs to be added to a list of existing panels—`self.panels_vertical`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The last method to be specified is `process_frame`. Recall that this method
    is triggered whenever a new camera frame is received. All that we need to do is
    pick the right image effect to be applied, which depends on the radio button configuration.
    We simply check which of the buttons is currently selected and call the corresponding
    `render` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'And we''re done! The following screenshot shows us the output pictures with
    different filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15078696-d7c1-48e3-a98f-b0a981674cbb.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows all of the four filters that we created applied
    to a single image.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored a number of interesting image processing effects.
    We used dodging and burning to create a black-and-white pencil sketch effect,
    explored lookup tables to arrive at an efficient implementation of curve filters,
    and got creative to produce a cartoon effect.
  prefs: []
  type: TYPE_NORMAL
- en: One of the techniques used was two-dimesional convolution, which takes a filter
    and an image and creates a new image. In this chapter, we provided the filters
    to get the results we wanted, but we don't always have the filters that are necessary
    to produce the results we want. Recently, deep learning has emerged, which tries
    to learn the values for different filters to help it get the results it wants.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will shift gears a bit and explore the use of depth
    sensors, such as **Microsoft Kinect 3D**, to recognize hand gestures in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Attributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Lenna.png`—the image of Lenna is available at [http://www.flickr.com/photos/15489034@N00/3388463896](http://www.flickr.com/photos/15489034@N00/3388463896)
    by Conor Lawless under the generic CC 2.0 attribution.'
  prefs: []
  type: TYPE_NORMAL
