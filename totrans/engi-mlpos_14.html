<html><head></head><body>
		<div id="_idContainer153">
			<h1 id="_idParaDest-177"><a id="_idTextAnchor206"/>Chapter 11: Key Principles for Monitoring Your ML System</h1>
			<p><a id="_idTextAnchor207"/>In this chapter, we will learn about the fundamental principles that are essential for monitoring your <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models in production. You will learn how to build trustworthy and Explainable AI solutions using the Explainable Monitoring Framework. The Explainable Monitoring Framework can be used to build functional monitoring pipelines so that you can monitor ML models in production, analyze application and model performance, and govern ML systems. The goal of monitoring ML systems is to enable trust, transparency, and explainability in order to increase business impact. We will learn about this by looking at some real-world examples. </p>
			<p>Understanding the principles mentioned in this chapter will equip you with the knowledge to build end-to-end monitoring systems for your use case or company. This will help you engage business, tech, and public (customers and legal) stakeholders so that you can efficiently achieve your business goals. This will also help you have the edge and have a systematic approach to governing your ML system. Using the frameworks in this chapter, you can enable trust, transparency, and explainability for your stakeholders and ML system.</p>
			<p>We are going to cover the following main topics in this chapter:<a id="_idTextAnchor208"/></p>
			<ul>
				<li>Understanding the key principles of monitoring an ML system</li>
				<li>Monitoring in the MLOps workflow</li>
				<li>Understanding the Explainable Monitoring Framework</li>
				<li>Enabling continuous monitoring for the service </li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor209"/>Understanding the key principles of monitoring an ML system</h1>
			<p>Building<a id="_idIndexMarker731"/> trust into AI systems is vital these days with the growing demands for products to be data-driven and to adjust to the changing environment and regulatory frameworks. One of the reasons ML projects are failing to bring value to businesses is due to the lack of trust and transparency in their decision making. Many black box models are good at reaching high accuracy, but they become obsolete when it comes to explaining the reasons behind the decisions that have been made. At the time of writing, news has been surfacing that raises these concerns of trust and explainability, as shown in the following figure:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/image001.jpg" alt="Figure 11.1 – Components of model trust and explainability&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – Components of model trust and explainability</p>
			<p>This image showcases concerns in important areas in real life. Let's look at how this translates into some key aspects of model explainability, such as model drift, model bias, model transparency, and model compliance, using some real-life examples.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor210"/>Model drift</h2>
			<p>We live in a dynamically changing world. Due to this, the environment<a id="_idIndexMarker732"/> and data in which an ML model is deployed<a id="_idIndexMarker733"/> to perform a task or make predictions is continually evolving, and it is essential to consider this change. For example, the COVID-19 pandemic has presented us with an unanticipated reality. Many business operations have turned virtual, and this pandemic has presented us with a unique situation that many perceive as the <strong class="bold">new normal</strong>. Many small businesses have gone bankrupt, and individuals are facing extreme financial scarcity due to the rise of unemployment. These people (small business owners and individuals) have been applying for loans and financial reliefs to banks and institutions like never before (on a large scale). Fraud detection algorithms that have already been deployed and used by banks and institutions have not seen this velocity and veracity of data, in terms of loan and financial relief applications. </p>
			<p>All these changes in features (such as the applicant's income, their credit history, the location of the applicant, the amount they've requested, and so on), due to an otherwise loan-worthy applicant who hasn't applied for any loan beforehand losing their job, may skew the model's weights/perceptive (or confuse the model). This presents an important challenge for the models. To deal with such dynamically changing environments, it is crucial to consider model drift and continually learn from it.</p>
			<p>Drift is related to changes in the environment and refers to the degradation of predictive ML models' performance and the relationship between the variables degrading. Following are the four types of model changes with regards to models and data:</p>
			<ul>
				<li><strong class="bold">Data drift</strong>: This<a id="_idIndexMarker734"/> is where properties of the independent variables change. For example, as in the previous example, data changes due to seasonality or new products or changes being added to meet the consumer's needs, as in the COVID-19 pandemic.</li>
				<li><strong class="bold">Feature drift</strong>: This is <a id="_idIndexMarker735"/>where the properties of the feature(s) change over time. For instance, temperature changes with change in seasons. In winter the temperature is cooler compared to the temperatures in summer or autumn.</li>
				<li><strong class="bold">Model drift</strong>: This is<a id="_idIndexMarker736"/> where properties of dependent variables change. For instance, in <a id="_idIndexMarker737"/>the preceding example, this is where the classification of fraud detection changes. </li>
				<li><strong class="bold">Upstream data changes</strong>: This is when the data pipeline undergoes operational data changes, such as <a id="_idIndexMarker738"/>when a feature is no longer being generated, resulting in missing values. An example of this is a change of salary value for the customer (from dollar to euros), where a dollar value is no longer being generated.</li>
			</ul>
			<p>For more clarity, we will learn more about drift and develop drift monitors in the next chapter (<a href="B16572_12_Final_JM_ePub.xhtml#_idTextAnchor222"><em class="italic">Chapter 12</em></a>, <em class="italic">Model Serving and Monitoring)</em>.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor211"/>Model bias </h2>
			<p>Whether <a id="_idIndexMarker739"/>you like it or not, ML is already impacting <a id="_idIndexMarker740"/>many decisions in your life, such as getting shortlisted for your next job or getting mortgage approvals from banks. Even Evan law enforcement agencies are using it to drill down potential crime suspects to prevent crimes. ProPublica, a journalism organization (uses ML to predict future criminals  - <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a>). In 2016, Propublica’s ML showed cases where the model was biased to predict black women as higher risk than white men, while all previous records showed otherwise. Such cases can be costly and have devastating societal impacts, so they need to be avoided. In another case, Amazon built an AI to hire people but had to shut it down as it was discriminating against women (as reported by the Washington Post). These kinds of biases can be costly and unethical. To avoid them, AI systems need to be monitored so that we can build our trust in them. </p>
			<p>Model bias is a type of error that happens due to certain features of the dataset (used for model training) being more heavily represented and/or weighted than others. A misrepresenting or biased dataset can result in skewed outcomes for the model's use case, low accuracy levels, and analytical errors. In other words, it is the error resulting from incorrect assumptions being made by the ML algorithm. High bias can result in predictions being inaccurate and can cause a model to miss relevant relationships between the features and the target variable being predicted. An example of this is the aforementioned AI that had been built by Amazon to hire people but had a bias against women. We will learn more about model bias in the <em class="italic">Explainable Monitoring Framework</em> section, where we will explore <em class="italic">Bias and threat detection</em>. </p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor212"/>Model transparency </h2>
			<p>AI is <a id="_idIndexMarker741"/>non-deterministic in <a id="_idIndexMarker742"/>nature. ML in particular continually evolves, updates, and retrains over its life cycle. AI is impacting almost all industries and sectors. With its increasing adoption and important decisions being made using ML, it has become vital to establish the same trust level that deterministic systems have. After all, digital systems are only useful when they can be trusted to do their jobs. There is a clear need for model transparency – many CEO and business leaders are encouraging us to understand AI's business decisions and their business impact. Recently, the CEO of TikTok made a statement stating the following:</p>
			<p class="author-quote">"We believe all companies should disclose their algorithms, moderation policies, and data flows to regulators" (Source: TikTok). </p>
			<p>Such a level of openness and transparency by companies can build our trust in AI as a society and enable smoother adoption and compliance. </p>
			<p>Model transparency is the pursuit of building trust in AI systems to ensure fairness, reduce or eliminate bias, provide accountability (auditing the end-to-end process of how the system derives results), and justify model outputs and system decisions.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor213"/>Model compliance </h2>
			<p>Model <a id="_idIndexMarker743"/>compliance has <a id="_idIndexMarker744"/>become important as the cost of non-compliance with governments and society can be huge. The following headline was reported by the Washington Post:</p>
			<p class="author-quote">"JPMorgan settles federal mortgage discrimination suit for $55 million" </p>
			<p>Non-compliance turned out to be a costly affair for JP Morgan. Operationalizing regulatory compliance is increasingly becoming important to avoid unnecessary fines and damage to society. Here are some drivers that enable model compliance within companies:</p>
			<ul>
				<li><strong class="bold">Culture of accountability</strong>: End-to-end auditing of ML systems is essential to monitoring compliance. MLOps can play a vital role in facilitating auditing and<a id="_idIndexMarker745"/> redacting the operations and business decisions that are made using AI. </li>
				<li><strong class="bold">Ethics at the forefront</strong>: Building responsible AI systems that bring value to society and gain our trust requires that AI predictions are inclusive, fair, and ethical. Having an ethics framework can help a company connect their customers to their values and principles, as well as ensuring AI decisions are made ethically. The European Commission has done a good job here by coming up with the <em class="italic">Ethics and guidelines for trustworthy AI</em>. You can find these guidelines here: <a href="https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai">https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai</a>.</li>
				<li><strong class="bold">Compliance pipeline</strong>: Having a compliance pipeline that satisfies both business<a id="_idIndexMarker746"/> and government regulations can be rewarding for organizations seeking to ensure real-time compliance, auditing, and redaction. MLOps can facilitate this by keeping track of all the ML models' inventory, thus giving visibility into how they work and explaining how they are working visually to stakeholders. This way of working enables humans to monitor, redact, and explain correlations to regulations, making it efficient for business stakeholders, data scientists, and regulators to work hand in hand to ensure they have transparent and explainable operations.</li>
			</ul>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor214"/>Explainable AI </h2>
			<p>In an ideal case, a business keeps <a id="_idIndexMarker747"/>model transparency and compliance at the forefront so that the business is dynamically adapting to the changing environment, such as model drift, and dealing with bias on the go. All this needs a framework that keeps all business stakeholders (IT and business leaders, regulators, business users, and so on) in touch with the AI model in order to understand the decisions the model is making, while focusing on increasing model transparency and compliance. Such a framework can be delivered using Explainable AI as part of MLOps. Explainable AI enables ML to be easily understood by humans. </p>
			<p>Model transparency and explainability are two approaches that enable Explainable AI. The <a id="_idIndexMarker748"/>ML models form patterns or rules based on the data they are trained on. Explainable AI can help humans or business stakeholders understand these rules or patterns the model has discovered, and also helps validate business decisions that have been made by the ML model. Ideally, Explainable AI should be able to serve multiple business stakeholders, as shown in the following diagram: </p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/image003.jpg" alt="Figure 11.2 – Business-driven Explainable AI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – Business-driven Explainable AI</p>
			<p>Black <a id="_idIndexMarker749"/>box models can get high accuracy on predictions but become obsolete when they are unable to explain why they've made these decisions. Most black box models offer no visibility into model performance, no monitoring to catch potential bias or drift, and no explainability of model behavior. To address this issue, a vast amount of research and development is going on in to Explainable AI methods to offer model transparency and model explainability. </p>
			<p>Explainable AI methods infused with MLOps can enable almost all business stakeholders to understand and validate business decisions made by the AI, and also helps explain them to the internal and external stakeholders. There is no one-stop solution for Explainable AI as every use case needs its own Explainable AI method. There are various methods that are gaining in popularity. We'll look at some examples in the following subsections.</p>
			<h3>Feature attribution methods</h3>
			<p>Feature attribution methods show how <a id="_idIndexMarker750"/>much each feature in your model has contributed to each instance's predictions. When you request explanations, you get the predictions, along with feature attribution information. Here are some feature attribution methods:</p>
			<ul>
				<li><strong class="bold">SHapley Additive exPlanations</strong> (<strong class="bold">SHAP</strong>): A method to explain the outputs of any ML<a id="_idIndexMarker751"/> model. It is <a id="_idIndexMarker752"/>based on the game theory approach, which explains the output of any ML model. In particular, it explains each feature's contribution to push the model's output.</li>
				<li><strong class="bold">Integrated Gradients</strong>: A<a id="_idIndexMarker753"/> technique that aims to <a id="_idIndexMarker754"/>explain the relationship between a model's predictions in terms of its features. It was introduced in the paper <em class="italic">Axiomatic Attribution for Deep Networks</em>. It can explain feature importance by identifying skewed data and help with debugging model performance. </li>
				<li><strong class="bold">Local Interpretable Model-Agnostic Explanation</strong> (<strong class="bold">LIME</strong>): This is a model-agnostic method<a id="_idIndexMarker755"/> that's used to explain predictions. It focuses <a id="_idIndexMarker756"/>on local explanations; that is, explanations to reflect the model's behavior regarding the instance of data being predicted. For instance, LIME can suggest which factors or features were important for a model to predict an outcome. This can be seen in the following diagram:</li>
			</ul>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/image005.jpg" alt="Figure 11.3 – Explaining individual predictions using LIME&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – Explaining individual predictions using LIME</p>
			<p>In the preceding diagram, a model is predicting that a patient has diabetes. The LIME explainer<a id="_idIndexMarker757"/> highlights and<a id="_idIndexMarker758"/> implies the symptoms of diabetes, such as dry skin, excessive urine, and blurry vision, that contribute to the <strong class="screen-inline">Diabetes</strong> prediction, while <strong class="screen-inline">No fatigue</strong> is evidence against it. Using the explainer, a doctor can decide and draw conclusions from the model's prediction and provide the patient with the appropriate treatment.</p>
			<p>You can learn more about AI explanations at <a href="https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview">https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview</a>. </p>
			<h3>Non-feature attribution methods </h3>
			<p>Non-feature attribution <a id="_idIndexMarker759"/>methods do not focus on how features contribute to your model's predictions. Instead, they focus on the relationship between input data and output data for your model inference. Here are some non-feature attribution methods:</p>
			<ul>
				<li><strong class="bold">Deeplift</strong>: This is <a id="_idIndexMarker760"/>used to evaluate neural networks by <a id="_idIndexMarker761"/>comparing each neuron's activation to its reference activation and assigning contribution scores according to the difference. Deeplift reveals correlations and contributions. For example, let's say we are using a cat and dog image classifier to classify images between cats and dogs. Suppose the classifier predicts that the input image is a dog by using the deeplift method. Here, we can backpropagate the neurons that were activated in the image classifier's neural network to<a id="_idIndexMarker762"/> their reference activations and then assign contribution scores to <a id="_idIndexMarker763"/>each feature based on the difference.   </li>
				<li><strong class="bold">Natural language explanations</strong> (<strong class="bold">NLE</strong>): NLE aims to capture input-output relationships for<a id="_idIndexMarker764"/> text explanations using a <a id="_idIndexMarker765"/>fusion of techniques such as partial dependence function, gradient analysis, contextual encoding, Individual Conditional Expectation, Accumulated Local Effects, and so on. These techniques are useful for interpreting language models that classify or generate text. NLE provide easily understandable and useful reasons for users to decide whether to trust the model's decision and take action. For example, you may wish to buy a product based on a model's recommendation and explanation. Tools such as Microsoft Power BI and Qlik Sense can be used to plug and play to study NLE. These tools need to be customized as per your need or use case.</li>
			</ul>
			<p>There are other methods apart from the ones mentioned in the previous list. This area is a hot topic for research in the field of AI. Many researchers and business leaders are pursuing solving Explainable AI problems to explain model decisions to internal and external stakeholders. Having an Explainable AI-driven interface provisioned for multiple business stakeholders can help them answer critical business questions. For instance, a business leader needs to be able to answer, "How do these model decisions impact business?" while for IT and Operations, it is vital to know the answer to "How do I monitor and debug?"</p>
			<p>Answering these questions for multiple business stakeholders enables employees and businesses to adapt to AI and maximize value from it, by ensuring model transparency and model compliance while adapting to changing environments by optimizing model bias and drift.</p>
			<h3>Explainable AI = model transparency and explainability</h3>
			<p>Since ML models are <a id="_idIndexMarker766"/>becoming first-class citizens, in order to monitor a model's performance with respect to these areas, we can use Explainable Monitoring, which allows us to analyze and govern ML systems in production by <a id="_idIndexMarker767"/>monitoring and explaining their decisions using Explainable AI methods. Explainable monitoring is a hybrid of Explainable AI; it uses Explainable AI methods infused with <strong class="bold">operations</strong> (<strong class="bold">Ops</strong>) in production. Explainable monitoring is becoming an integral part of the MLOps workflow. We'll look at how Explainable Monitoring brings value to the MLOps workflow in the next section. </p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor215"/>Monitoring in the MLOps workflow</h1>
			<p>We learned about the <a id="_idIndexMarker768"/>MLOps workflow in <a href="B16572_01_Final_JM_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Fundamentals of MLOps Workflow</em>. As shown in the following diagram, the monitoring block is an integral part of the MLOps workflow for evaluating the ML models' performance in production and measuring the ML system's business value. We can only do both (measure the performance and business value that's been generated by the ML model) if we understand the model's decisions in terms of transparency and explainability (to explain the decisions to stakeholders and customers).  </p>
			<p>Explainable Monitoring <a id="_idIndexMarker769"/>enables both transparency and explainability to govern ML systems in order to drive the best business value:</p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/image007.jpg" alt="Figure 11.4 – MLOps workflow – Monitor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – MLOps workflow – Monitor</p>
			<p>In practice, <strong class="bold">Explainable Monitoring</strong> enables us to monitor, analyze, and govern ML system, and it works in a continuous loop with other components in the MLOps <a id="_idIndexMarker770"/>workflow. It also empowers humans to engage in the loop to understand model decisions and teach the model (by labeling data and retraining the model) on the go. Explainable Monitoring enables continual learning and can be highly rewarding for a company in the long run. A continual learning pipeline with a human in the loop, enabled using Explainable Monitoring, can be seen in the following diagram:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/image009.jpg" alt="Figure 11.5 – Continual learning enabled by Explainable Monitoring&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – Continual learning enabled by Explainable Monitoring</p>
			<p>Continual learning<a id="_idIndexMarker771"/> is the system's ability to learn continuously in a changing environment while building on what had been learned previously. To facilitate continual learning, data and modeling must work hand in hand, and be assisted by humans in the loop (typically, a QA analyst or system admin, such as a data scientist or ML engineer). Explainable Monitoring plays a vital role in continual learning systems to increase revenue, stay compliant, and build ML systems responsibly. After all, only a model that's been deployed with continual learning capabilities can bring business value. </p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor216"/>Understanding the Explainable Monitoring Framework</h1>
			<p>In this section, we will explore the <a id="_idIndexMarker772"/>Explainable Monitoring Framework (as shown in the following diagram) in detail to understand and learn how Explainable Monitoring enhances the MLOps workflow and the ML system itself:</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/image011.jpg" alt="Figure 11.6 – Explainable Monitoring Framework&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – Explainable Monitoring Framework</p>
			<p>The Explainable Monitoring Framework is a modular framework that's used to monitor, analyze, and govern a ML system while enabling continual learning. All the modules work in sync to enable transparent and Explainable Monitoring. Let's look at how each module works to understand how they contribute and function in the framework. First, let's look at the monitor module (the first panel in the preceding diagram). </p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor217"/>Monitor </h2>
			<p>The <a id="_idIndexMarker773"/>monitor module is <a id="_idIndexMarker774"/>dedicated to monitoring the application in production (serving the ML model). Several factors are at play in an ML system, such as application performance (telemetry data, throughput, server request time, failed requests, error handling, and so on), data integrity and model drift, and changing environments. The monitor module should capture vital information from the system logs in production to track the ML system's robustness. Let's look at the importance and functionality of three of the monitor module's functionalities: data integrity, model drift, and application performance. </p>
			<h3>Data integrity </h3>
			<p>Ensuring the data integrity<a id="_idIndexMarker775"/> of an ML application includes checking incoming (input data to the ML model) and outgoing (ML model prediction) data to ensure ML systems' integrity and robustness. The monitor module ensures data integrity by inspecting the volume, variety, veracity, and velocity of the data in order to detect outliers or anomalies. Detecting outliers or anomalies prevents ML systems from having poor performance and being susceptible to security attacks (for example, adversarial attacks). Data integrity coupled with efficient auditing can facilitate the desired performance of ML systems to derive business value. </p>
			<h3>Model drift </h3>
			<p>If model drift is not <a id="_idIndexMarker776"/>measured, the model's performance can easily become sub-par and can hamper the business with poor decision making and customer service. For example, it is hard to foresee changes or trends in data during a black swan event such as COVID-19. Here is some news that made it to the headlines:</p>
			<ul>
				<li>The accuracy of the Instacart model forecasting item's accessibility in stores fell from 93% to 61% due to a dramatic change in shopping habits (<a href="https://fortune.com/2020/06/09/instacart-coronavirus-artificial-intelligence/">https://fortune.com/2020/06/09/instacart-coronavirus-artificial-intelligence/</a>).</li>
				<li>Bankers doubted if credit models that have been trained for good times will respond accurately to stress scenarios (<a href="https://www.americanbanker.com/opinion/ai-models-could-struggle-to-handle-the-market-downturn">https://www.americanbanker.com/opinion/ai-models-could-struggle-to-handle-the-market-downturn</a>).</li>
				<li>In response to market uncertainty, trading algorithms misfired. There was a 21% decline in some funds (<a href="https://www.wired.com/story/best-ai-models-no-match-coronavirus">https://www.wired.com/story/best-ai-models-no-match-coronavirus</a>).</li>
				<li>Image classification models struggle to adapt to the "new normal" in the wake of the COVID-19 pandemic: a family at home in front of laptops can now mean "work," not "leisure." (<a href="https://techcrunch.com/2020/08/02/ai-is-struggling-to-adjust-to-2020/">https://techcrunch.com/2020/08/02/ai-is-struggling-to-adjust-to-2020/</a>)</li>
			</ul>
			<p>Hence, it is <a id="_idIndexMarker777"/>important to monitor model drift in any form, such as data drift, concept drift, or any upstream data changes, in order to adapt to the changing environments and serve businesses and customers in the most relevant way and generate the maximum business value.</p>
			<h3>Application performance </h3>
			<p>It is critical to <a id="_idIndexMarker778"/>monitor application performance to foresee and prevent any potential failures, since this ensures the robustness of ML systems. Here, we can monitor the critical system logs and telemetry data of the production deployment target (for example, Kubernetes or an on-premises server). Monitoring application performance can give us key insights in real time, such as the server's throughput, latency, server request time, number of failure requests or control flow errors, and so on. There is no hard and set way of monitoring applications and, depending on your business use case, your application performance mechanism can be curated and monitored to keep the system up and running to generate business value. </p>
			<p>In terms of the monitor component, we monitored data integrity, model drift, and application performance. In the next section, we will analyze how to monitor the data of the model and application.  </p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor218"/>Analyze  </h2>
			<p>Analyzing your ML <a id="_idIndexMarker779"/>system in production in <a id="_idIndexMarker780"/>real time is key to understanding the performance of your ML system and ensuring its robustness. Humans play a key role in analyzing model performance and detecting subtle anomalies and threats. Hence, having a human in the loop can introduce great transparency and explainability to the ML system. We can analyze model performance to detect any biases or threats and to understand why the model makes decisions in a certain pattern. We can do this by applying advanced techniques such as data slicing, adversarial attack prevention techniques, or by understanding local and global explanations. Let's see how we can do this in practice.   </p>
			<h3>Data slicing </h3>
			<p>There are a great number <a id="_idIndexMarker781"/>of success stories surrounding ML in terms of improving businesses and life in general. However, there is still room to improve data tools for debugging and interpreting models. One key area of improvement is understanding why models perform poorly on certain parts or slices of data and how we can balance their overall performance. A slice is a part or a subset of a dataset. Data slicing can help us understand the model's performance on different types of sub-datasets. We can split the dataset into multiple slices or subsets and study the model's behavior on them. </p>
			<p>For example, let's consider a hypothetical case where we have trained a random forest model to classify whether a person's income is above or below $50,000. The model has been trained on the UCI census data (<a href="https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29">https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29</a>). The results of the model for slices (or subsets) of data can be seen in the following table. This table suggests that the overall metrics may be considered acceptable as the overall log loss is low for all the data (see the <em class="italic">All</em> row). This is a widely used loss metric for binary classification problems and represents how close the prediction's likelihood is to the actual/true value; it is 0 or 1 in the case of binary classification. The more the predicted probability diverges from the actual value, the higher the log loss value is. However, the individual slices tell a different story:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/011.jpg" alt="Table 11.1 – UCI census data slices&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 11.1 – UCI census data slices</p>
			<p>By <a id="_idIndexMarker782"/>looking at the previous table, we can conclude that the model's performance is decent. However, if we look at the performance of male versus female subjects, we can see that the model only performs well for female subjects where the log losses are less compared to the log loss for male subjects. On the other hand, if you look at the <em class="italic">Prof – specialty</em> occupation, you will see that the net performance is on par with the performance of male subjects with log losses of 0.45 and 0.41, respectively, whereas the effect size for <em class="italic">Prof – specialty</em> is considerably less. The model performs poorly for <em class="italic">Bachelors</em>, <em class="italic">Masters</em>, and <em class="italic">Doctorates</em> as the log losses are high with values of 0.44, 0.49, and 0.59, respectively. It is also important to note that if the log loss of a slice and its counterpart is below acceptable, this suggests that the model is bad overall and not just on a particular data slice.  </p>
			<p>Data slicing enables us to see subtle biases and unseen correlations to understand why a model might perform poorly on a subset of data. We can avoid these biases and improve the model's overall performance by training the model using balanced datasets that represent all the data slices (for example, using synthetic data or by undersampling, and so on) or by tuning hyperparameters of the models to reduce overall biases. Data slicing can provide an overview of model fairness and performance for an ML system, and can also help an organization optimize the data and ML models to reach optimal performance and decent fairness thresholds. Data <a id="_idIndexMarker783"/>slicing can help build trust in the AI system by offering transparency and explainability into data and model performance. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">To get a comprehensive overview of data slicing and automated data slicing methods, take a look at <em class="italic">Automated Data Slicing for Model Validation: A Big data - AI Integration Approach</em> at <a href="https://arxiv.org/pdf/1807.06068.pdf">https://arxiv.org/pdf/1807.06068.pdf</a>.</p>
			<h3>Bias and threat detection </h3>
			<p>To ensure that robust and ethical decisions are made using ML models, we need to make sure that the models are fair and secure. Any biases and threats need to be monitored and mitigated, to avoid unethical or partial decisions that benefit any particular party in order to comply with business values and the law. </p>
			<p>There are different <a id="_idIndexMarker784"/>types of bias, such as selection bias (data that's used for training models is not representative of the population, such as minorities), framing bias (questions or a survey that's used to collect data is framed in a point of view or slant), systematic bias (repetitive or consistent error), response bias (data in which participants respond incorrectly by following their conscious bias), or confirmation bias (collecting data to <a id="_idIndexMarker785"/>validate your own preconceptions). To avoid these biases and mitigate them, techniques such as data slicing, slice-based learning, or balancing the bias-variance tradeoff can be applied, depending on the use case.</p>
			<p>An ML system is exposed to security threats that need monitoring and mitigation. We have discussed some common threats and threat-prevention techniques involving adversarial attacks, poison attacks, privacy attacks or backdoor attacks, and so on, in <a href="B16572_09_Final_JM_ePub.xhtml#_idTextAnchor176"><em class="italic">Chapter 9</em></a>, <em class="italic">Testing and Securing Your ML Solution.</em></p>
			<h3>Local and global explanations</h3>
			<p>Local and global explanations offer different perspectives on model performance. Local explanations<a id="_idIndexMarker786"/> offer justification for model prediction for a specific or individual input, whereas<a id="_idIndexMarker787"/> global explanations provide insights into the model's predictive process, independent of any particular input. For example, let's take a look at a hypothetical case<a id="_idIndexMarker788"/> of a <strong class="bold">recurrent neural network</strong> (<strong class="bold">RNN</strong>) model being used to perform sentiment analysis for customer reviews. The following diagram shows the global explanation (the process as a whole) for the RNN model's sentiment analysis using the RNNVis tool:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/image013.jpg" alt="Figure 11.7 – Global explanation of the RNN model (using RNNVis) to understand the process as a whole (how hidden states, layers, and so on impact model outputs and predictive processes)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – Global explanation of the RNN model (using RNNVis) to understand the process as a whole (how hidden states, layers, and so on impact model outputs and predictive processes)</p>
			<p>Source: <a href="https://blog.acolyer.org/2019/02/25/understanding-hidden-memories-of-recurrent-neural-networks/">https://blog.acolyer.org/2019/02/25/understanding-hidden-memories-of-recurrent-neural-networks/</a></p>
			<p>Here, for example, the co-clustering visualization shows different word clouds for words with positive and negative sentiments. Using global explanations, we can simulate the model's predictive process and understand correlations with regards to parameters or the model's architecture (for example, hidden states and layers). Global explanations <a id="_idIndexMarker789"/>offer two perspectives of explainability: the high-level model process and the predictive explanations. On the other hand, local explanations give insights into single predictions. Both explanations are valuable if we wish to understand the model's performance and validate it comprehensively. </p>
			<p>In the analyze component, we can analyze the model's performance using the techniques we have explored, such as data slicing, Bias and threat detection, and local and global explanations. In the next section, we will learn how to govern and control ML systems to efficiently guide it to achieve operational or business objectives.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor219"/>Govern</h2>
			<p>The ML systems' efficacy is dependent on the way it is governed to achieve maximum business value. A great <a id="_idIndexMarker790"/>part of system governance involves quality assurance and control, as well as model auditing and reporting, to ensure it has end-to-end trackability and complies with regulations. Based on monitoring and analyzing the model's performance, we can control and govern ML systems. Governance is driven by smart alerts and actions to maximize business value. Let's look into how alerts and actions, model quality assurance and control, and model auditing and reports orchestrate the ML system's governance.</p>
			<h3>Alerts and actions</h3>
			<p>Governing a ML system involves monitoring and analyzing the ML application. Here, system developers can be alerted about when the system is showing anomalous behavior such as failed requests, slow server response times, server exceptions, errors, or high latency. Alerting the system developers or admin can ensure quality assurance and prevent system failures. There are two different types of alerts: alerts for system performance and model performance-based alerts. Here are some examples of alerts<a id="_idIndexMarker791"/> for system performance:</p>
			<ul>
				<li>Rule-based alerts for failed requests based on a threshold</li>
				<li>Rule-based alerts for server response time based on a threshold</li>
				<li>Rule-based alerts for server exceptions based on a threshold</li>
				<li>Rule-based alerts for availability based on a threshold</li>
			</ul>
			<p>Model performance <a id="_idIndexMarker792"/>alerts are generated when the model experiences drift or anomalous feature distribution or bias. When such events are recorded, the system administrator or developers are alerted via email, SMS, push notifications, and voice alerting. These alert actions (automated or semi-automated) can be used to mitigate system performance deterioration. Depending on the situation and need, some possible actions can be evoked, such as the following:</p>
			<ul>
				<li>Deploying an alternative model upon experiencing high model drift  </li>
				<li>Retraining a model</li>
				<li>Training a new model</li>
				<li>Restarting the ML system</li>
				<li>Redeploying the ML system</li>
			</ul>
			<h3>Model quality assurance and control</h3>
			<p>A model's quality assurance and control mechanism<a id="_idIndexMarker793"/> can be quite rewarding for those using an ML system, if we wish to prevent many possible mishaps and to ensure regular and healthy monitoring and functionality of the ML system. It is recommended to have a framework or mechanism for model quality assurance and control. For this, a quality assurance framework for ML systems, as shown in the following diagram, can enable the mechanism for your organization. It is a modular framework that's used to monitor three important aspects of ML systems:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/image015.jpg" alt="Figure 11.8 – Model Quality Assurance Framework&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – Model Quality Assurance Framework</p>
			<p>Quality <a id="_idIndexMarker794"/>assurance experts can help your company or organization put a test mechanism into place to validate whether the data being used for training has been sanitized, ensure the data being used for model inference does not contain threats for that ML system, and to monitor data drift to understand and validate the changing environment. Monitoring and testing data can be achieved by quality assurance or test engineers, together with product managers, by doing the following:</p>
			<ul>
				<li>Understand and validate the data's statistical relations (for example, mean, median, mode, and so on) for training, testing, and inferring data. </li>
				<li>Develop tests to verify the aforementioned statistics and relationships (using scripts).</li>
				<li>Evaluate the distribution of characteristics using feature engineering techniques such as feature selection, dimensionality reduction, and so on.</li>
				<li>Retrain and review the performance of all models.</li>
				<li>Monitor the performance of all the models at regular intervals with new datasets.</li>
				<li>Raise an alert if another model (from the model inventory) performs with better accuracy than the existing model.</li>
				<li>Perform tests at regular intervals.</li>
			</ul>
			<h3>Model auditing and reports</h3>
			<p>Model auditing and reporting<a id="_idIndexMarker795"/> is essential if you want to have enough information for the regulators and for compliance with the law. Having end-to-end traceability for the model ensures great transparency and explainability, which can result in transparent governance mechanisms for an organization or company. The goal of model auditing and reporting is to assess the model's performance and based on that, enable ML system governance. In the following diagram, we can see a big-picture overview of the model transparency chart that's generated from auditing and reporting:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/image017.jpg" alt="Figure 11.9 – Model QA framework&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.9 – Model  Transparency chart</p>
			<p>Model assessments based on auditing and reporting will ensure healthy, transparent, and robust governance mechanisms for organizations and enable them to have end-to-end traceability in order to comply with the regulators. Having such mechanisms will help save organizations a great amount of time and resources and enable efficiency in interactions with the regulators.</p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor220"/>Enabling continuous monitoring for the service</h1>
			<p>The <a id="_idIndexMarker796"/>Explainable Monitoring Framework can be <a id="_idIndexMarker797"/>resourceful if we wish to monitor ML systems in production. In the next chapter, we will enable the Explainable Monitoring Framework for the business use case we worked on in the previous chapters. We will enable continuous monitoring for the system we have deployed. We will then monitor the ML application that's been deployed to production and analyze the incoming data and the model's performance to govern the ML system to produce maximum business value for the use case.  </p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor221"/>Summary</h1>
			<p>In this chapter, we learned about the key principles for monitoring an ML system. We explored some common monitoring methods and the Explainable Monitoring Framework (including the monitor, analyze, and govern stages). We then explored the concepts of Explainable Monitoring thoroughly. </p>
			<p>In the next chapter, we will delve into a hands-on implementation of the Explainable Monitoring Framework. Using this, we will build a monitoring pipeline in order to continuously monitor the ML system in production for the business use case (predicting weather at the port of Turku). </p>
			<p>The next chapter is quite hands-on, so buckle up and get ready!</p>
		</div>
	</body></html>