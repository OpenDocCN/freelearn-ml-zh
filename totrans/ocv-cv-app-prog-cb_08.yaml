- en: Chapter 8. Detecting Interest Points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting corners in an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting features quickly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting scale-invariant features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting FAST features at multiple scales
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In computer vision, the concept of interest points—also called **keypoints**
    or **feature points**—has been largely used to solve many problems in object recognition,
    image registration, visual tracking, 3D reconstruction, and more. This concept
    relies on the idea that instead of looking at the image as a whole, it could be
    advantageous to select some special points in the image and perform a local analysis
    on them. This approach works well as long as a sufficient number of such points
    are detected in the images of interest and these points are distinguishing and
    stable features that can be accurately localized.
  prefs: []
  type: TYPE_NORMAL
- en: Because they are used for analyzing image content, feature points should ideally
    be detected at the same scene or object location no matter from which viewpoint,
    scale, or orientation the image was taken. View invariance is a very desirable
    property in image analysis and has been the object of numerous studies. As we
    will see, different detectors have different invariance properties. This chapter
    focuses on the keypoint extraction process itself. The next two chapters will
    then show you how interest points can be put to work in different contexts such
    as image matching or image geometry estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting corners in an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When searching for interesting feature points in images, corners come out as
    an interesting solution. They are indeed local features that can be easily localized
    in an image, and in addition, they should abound in scenes of man-made objects
    (where they are produced by walls, doors, windows, tables, and so on). Corners
    are also interesting because they are two-dimensional features that can be accurately
    localized (even at sub-pixel accuracy), as they are at the junction of two edges.
    This is in contrast to points located on a uniform area or on the contour of an
    object and points that would be difficult to repeatedly localize precisely on
    other images of the same object. The Harris feature detector is a classical approach
    to detecting corners in an image. We will explore this operator in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The basic OpenCV function that is used to detect Harris corners is called `cv::cornerHarris`
    and is straightforward to use. You call it on an input image, and the result is
    an image of floats that gives you the corner strength at each pixel location.
    A threshold is then applied on this output image in order to obtain a set of detected
    corners. This is accomplished with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00125.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The result is a binary map image shown in the following screenshot, which is
    inverted for better viewing (that is, we used `cv::THRESH_BINARY_INV` instead
    of `cv::THRESH_BINARY` to get the detected corners in black):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00126.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding function call, we observe that this interest point detector
    requires several parameters (these will be explained in the next section) that
    might make it difficult to tune. In addition, the corner map that is obtained
    contains many clusters of corner pixels that contradict the fact that we would
    like to detect well-localized points. Therefore, we will try to improve the corner-detection
    method by defining our own class to detect Harris corners.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class encapsulates the Harris parameters with their default values and
    corresponding getter and setter methods (which are not shown here):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To detect the Harris corners on an image, we proceed with two steps. First,
    the Harris values at each pixel are computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the feature points are obtained based on a specified threshold value.
    Since the range of possible values for Harris depends on the particular choices
    of its parameters, the threshold is specified as a quality level that is defined
    as a fraction of the maximal Harris value computed in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This method returns a binary corner map of the detected features. The fact
    that the detection of the Harris features has been split into two methods allows
    us to test the detection with a different threshold (until an appropriate number
    of feature points are obtained) without the need to repeat costly computations.
    It is also possible to obtain the Harris features in the form of a `std::vector`
    of `cv::Point`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This class also improves the detection of the Harris corners by adding a non-maxima
    suppression step, which will be explained in the next section. The detected points
    can now be drawn on an image using the `cv::circle` function, as demonstrated
    by the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this class, the detection of the Harris points is accomplished as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00127.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To define the notion of corners in images, the Harris feature detector looks
    at the average change in directional intensity in a small window around a putative
    interest point. If we consider a displacement vector, `(u,v)`, the average intensity
    change is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00128.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The summation is over a defined neighborhood around the considered pixel (the
    size of this neighborhood corresponds to the third parameter in the `cv::cornerHarris`
    function). This average intensity change can then be computed in all possible
    directions, which leads to the definition of a corner as a point for which the
    average change is high in more than one direction. From this definition, the Harris
    test is performed as follows. We first obtain the direction of the maximal average
    intensity change. Next, we check whether the average intensity change in the orthogonal
    direction is high as well. If this is the case, then we have a corner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, this condition can be tested by using an approximation of the
    preceding formula using the Taylor expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is then rewritten in the matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This matrix is a covariance matrix that characterizes the rate of intensity
    change in all directions. This definition involves the image's first derivatives
    that are often computed using the Sobel operator. This is the case with the OpenCV
    implementation, which is the fourth parameter of the function that corresponds
    to the aperture used for the computation of the Sobel filters. It can be shown
    that the two eigenvalues of the covariance matrix give you the maximal average
    intensity change and the average intensity change for the orthogonal direction.
    Then, if these two eigenvalues are low, we are in a relatively homogenous region.
    If one eigenvalue is high and the other is low, we must be on an edge. Finally,
    if both eigenvalues are high, then we are at a corner location. Therefore, the
    condition for a point to be accepted as a corner is that it must have the smallest
    eigenvalue of the covariance matrix at a higher point than a given threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original definition of the Harris corner algorithm uses some properties
    of the eigen decomposition theory in order to avoid the cost of explicitly computing
    the eigenvalues. These properties are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The product of the eigenvalues of a matrix is equal to its determinant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of the eigenvalues of a matrix is equal to the sum of the diagonal of
    the matrix (also known as the **trace** of the matrix)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It then follows that we can verify whether the eigenvalues of a matrix are
    high by computing the following score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: One can easily verify that this score will indeed be high only if both eigenvalues
    are high too. This is the score that is computed by the `cv::cornerHarris` function
    at each pixel location. The value of `k` is specified as the fifth parameter of
    the function. It could be difficult to determine what would be the best value
    for this parameter. However, in practice, it has been seen that a value in the
    range of `0.05` and `0.5` generally gives good results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve the result of the detection, the class described in the previous
    section adds an additional non-maxima suppression step. The goal here is to exclude
    Harris corners that are adjacent to others. Therefore, to be accepted, the Harris
    corner must not only have a score higher than the specified threshold, but it
    must also be a local maximum. This condition is tested by using a simple trick
    that consists of dilating the image of the Harris score in our `detect` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the dilation replaces each pixel value with the maximum in the defined
    neighborhood, the only points that will not be modified are the local maxima.
    This is what is verified by the following equality test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `localMax` matrix will therefore be true (that is, non-zero) only at local
    maxima locations. We then use it in our `getCornerMap` method to suppress all
    non-maximal features (using the `cv::bitwise_and` function).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Additional improvements can be made to the original Harris corner algorithm.
    This section describes another corner detector found in OpenCV, which expands
    the Harris detector to make its corners more uniformly distributed across the
    image. As we will see, this operator has an implementation for the feature detector
    in the OpenCV 2 common interface.
  prefs: []
  type: TYPE_NORMAL
- en: Good features to track
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the advent of floating-point processors, the mathematical simplification
    introduced to avoid eigenvalue decomposition has become negligible, and consequently,
    the detection of Harris corners can be made based on the explicitly computed eigenvalues.
    In principle, this modification should not significantly affect the result of
    the detection, but it avoids the use of the arbitrary `k` parameter. Note that
    two functions exist that allow you to explicitly get the eigenvalues (and eignevectors)
    of the Harris covariance matrix; these are `cv::cornerEigenValsAndVecs` and `cv::cornerMinEigenVal`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A second modification addresses the problem of feature point clustering. Indeed,
    in spite of the introduction of the local maxima condition, interest points tend
    to be unevenly distributed across an image, showing concentrations at highly textured
    locations. A solution to this problem is to impose a minimum distance between
    two interest points. This can be achieved using the following algorithm. Starting
    from the point with the strongest Harris score (that is, with the largest minimum
    eigenvalue), only accept interest points if they are located at, at least, a given
    distance from the already accepted points. This solution is implemented in OpenCV
    in the `cv::goodFeaturesToTrack` function, which is thus named because the features
    it detects can be used as a good starting set in visual tracking applications.
    This is called as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to the quality-level threshold value and the minimum tolerated
    distance between interest points, the function also uses a maximum number of points
    that can be returned (this is possible since points are accepted in the order
    of strength). The preceding function call produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Good features to track](img/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This approach increases the complexity of the detection, since it requires the
    interest points to be sorted by their Harris score, but it also clearly improves
    the distribution of the points across the image. Note that this function also
    includes an optional flag that requests Harris corners to be detected using the
    classical corner score definition (using the covariance matrix determinant and
    trace).
  prefs: []
  type: TYPE_NORMAL
- en: The feature detector's common interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenCV 2 has introduced a common interface for its different interest point
    detectors. This interface allows easy testing of different interest point detectors
    within the same application.
  prefs: []
  type: TYPE_NORMAL
- en: The interface defines a `cv::Keypoint` class that encapsulates the properties
    of each detected feature point. For the Harris corners, only the position of the
    keypoints and its response strength is relevant. The *Detecting scale-invariant
    features* recipe will discuss the other properties that can be associated with
    a keypoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cv::FeatureDetector` abstract class basically imposes the existence of
    a `detect` operation with the following signatures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The second method allows interest points to be detected in a vector of images.
    The class also includes other methods that can read and write the detected points
    in a file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cv::goodFeaturesToTrack` function has a wrapper class called `cv::GoodFeaturesToTrackDetector`,
    which inherits from the `cv::FeatureDetector` class. It can be used in a way that
    is similar to what we did with our Harris corners class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The result is the same as the one obtained previously, since the same function
    is ultimately called by the wrapper. Note how we used the OpenCV 2 smart pointer
    class `(cv::Ptr)` that, as explained in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Playing with Images"), *Playing with Images*, automatically releases
    the pointed object when the reference count drops to zero.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The classic article that describes the Harris operator by C. Harris and M.J.
    Stephens*, A combined corner and edge detector, Alvey Vision Conference, pp. 147–152,
    1988*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article by J. Shi and C. Tomasi*, Good features to track, Int. Conference
    on Computer Vision and Pattern Recognition, pp. 593-600, 1994*, introduces these
    features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article by K. Mikolajczyk and C. Schmid, *Scale and Affine invariant interest
    point detectors, International Journal of Computer Vision, vol 60, no 1, pp. 63-86,
    2004*, proposes a multi-scale and affine-invariant Harris operator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting features quickly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Harris operator proposed a formal mathematical definition for corners (or
    more generally, interest points) based on the rate of intensity changes in two
    perpendicular directions. Although this constitutes a sound definition, it requires
    the computation of the image derivatives, which is a costly operation, especially
    considering the fact that interest point detection is often just the first step
    in a more complex algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we present another feature point operator, called **FAST** (**Features
    from Accelerated Segment Test**). This one has been specifically designed to allow
    quick detection of interest points in an image; the decision to accept or not
    to accept a keypoint is based on only a few pixel comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the OpenCV 2 common interface for feature point detection makes the deployment
    of any feature point detectors easy. The detector presented in this recipe is
    the FAST detector. As the name suggests, it has been designed to be quick in order
    to compute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that OpenCV also proposes a generic function to draw keypoints on an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying the chosen drawing flag, the keypoints are drawn over the input
    image, thus producing the following output result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An interesting option is to specify a negative value for the keypoint color.
    In this case, a different random color will be selected for each drawn circle.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in the case with the Harris point detector, the FAST feature algorithm derives
    from the definition of what constitutes a *corner*. This time, this definition
    is based on the image intensity around a putative feature point. The decision
    to accept a keypoint is taken by examining a circle of pixels centered at a candidate
    point. If an arc of contiguous points of a length greater than 3/4 of the circle
    perimeter in which all pixels significantly differ from the intensity of the center
    point (being all darker or all brighter) is found, then a keypoint is declared.
  prefs: []
  type: TYPE_NORMAL
- en: This is a simple test that can be computed quickly. Moreover, in its original
    formulation, the algorithm uses an additional trick to further speed up the process.
    Indeed, if we first test four points separated by 90 degrees on the circle (for
    example, top, bottom, right, and left points), it can be easily shown that in
    order to satisfy the condition expressed previously, at least three of these points
    must all be brighter or darker than the central pixel.
  prefs: []
  type: TYPE_NORMAL
- en: If this is not the case, the point can be rejected immediately, without inspecting
    additional points on the circumference. This is a very effective test, since in
    practice, most of the image points will be rejected by this simple 4-comparison
    test.
  prefs: []
  type: TYPE_NORMAL
- en: 'In principle, the radius of the circle of examined pixels could have been a
    parameter of the method. However, it has been found that in practice, a radius
    of `3` gives you both good results and high efficiency. There are, then, `16`
    pixels that need to be considered on the circumference of the circle, shown as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00134.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The four points used for the pretest are the **1**, **5**, **9**, and **13**
    pixels, and the required number of contiguous darker or brighter points is **12**.
    However, it has been observed that by reducing the length of the contiguous segment
    to **9**, better repeatability of the detected corners across images is obtained.
    This variant is often designated as the **FAST-9** corner detector, and this is
    the one that is used by OpenCV. Note that there exists a `cv::FASTX` function
    that proposes another variant of the FAST detector.
  prefs: []
  type: TYPE_NORMAL
- en: To be considered as being significantly darker or brighter, the intensity of
    a point must differ from the intensity of the central pixel by at least a given
    amount; this value corresponds to the threshold parameter specified in the function
    call. The larger this threshold is, the fewer corner points will be detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for Harris features, it is often better to perform non-maxima suppression
    on the corners that have been found. Therefore, a corner strength measure needs
    to be defined. Several alternatives measures to this can considered, and the one
    that has been retained is the following. The strength of a corner is given by
    the sum of the absolute difference between the central pixel and the pixels on
    the identified contiguous arc. Note that the algorithm is also available through
    a direct function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: However, because of its flexibility, the use of the `cv::FeatureDetector` interface
    is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm results in very fast interest point detection and is therefore
    the feature of choice when speed is a concern. This is the case, for example,
    in real-time visual tracking or object-recognition applications where several
    points must be tracked or matched in a live video stream.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To improve the detection of feature points, additional tools are offered by
    OpenCV. Indeed, a number of class adapters are available in order to better control
    the way the keypoints are extracted.
  prefs: []
  type: TYPE_NORMAL
- en: Adapted feature detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you wish to better control the number of detected points, a special subclass
    of the `cv::FeatureDetector` class, called `cv::DynamicAdaptedFeatureDetector`,
    is available. This allows you to specify the number of interest points that can
    be detected as an interval. In the case of the FAST feature detector, this is
    used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The interest points will then be iteratively detected. After each iteration,
    the number of detected points is checked and the detector threshold is adjusted
    accordingly in order to produce more or less points; this process is repeated
    until the number of detected points fit into the specified interval. A maximum
    number of iterations is specified in order to avoid that the method spends too
    much time on multiple detections. For this method to be implemented in a generic
    way, the used `cv::FeatureDetector` class must implement the `cv::AdjusterAdapter`
    interface. This class includes a `tooFew` method and a `tooMany` method, both
    of which modify the internal threshold of the detector in order to produce more
    or less keypoints. There is also a `good` predicate method that returns `true`
    if the detector threshold can still be adjusted. Using a `cv::DynamicAdaptedFeatureDetector`
    class can be a good strategy to obtain an appropriate number of feature points;
    however, you must understand that there is a performance price that you will have
    to to pay for this benefit. Moreover, there is no guarantee that you will indeed
    obtain the requested number of features within the specified number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: You probably noticed that we passed an argument, which is the address of a dynamically
    allocated object, to specify the feature detector that will be used by the adapter
    class. You might wonder whether you have to release the allocated memory at some
    point in order to avoid memory leaks. The answer is no, and this is because the
    pointer is transferred to a `cv::Ptr<FeatureDetector>` parameter that automatically
    releases the pointed object.
  prefs: []
  type: TYPE_NORMAL
- en: Grid adapted feature detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A second useful class adapter is the `cv::GridAdaptedFeatureDetector` class.
    As the name suggests, it allows you to define a grid over the image. Each cell
    of this grid is then constrained to contain a maximum number of elements. The
    idea here is to spread the set of detected keypoints over the image in a better
    manner. When detecting keypoints in an image, it is indeed common to see a concentration
    of interest points in a specific textured area. This is the case, for example,
    of the two towers of the church image on which a very dense set of FAST points
    have been detected. This class adapter is used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The class adapter simply proceeds by detecting feature points on each individual
    cell using the provided `cv::FeatureDetector` object. A maximum total number of
    points is also specified. Only the strongest points in each cell are kept in order
    to not exceed the specified maximum.
  prefs: []
  type: TYPE_NORMAL
- en: Pyramid adapted feature detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `cv::PyramidAdaptedFeatureDetector` adapter proceeds by applying the feature
    detector on an image pyramid. The results are combined in the output vector of
    keypoints. This is called as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The coordinates of each point are specified in the original image coordinates.
    In addition, the special `size` attribute of the `cv::Keypoint` class is set such
    that points detected at half the original resolution are attributed a size that
    is twice the size of the detected points in the original image. There is a special
    flag in the `cv::drawKeypoints` function that will draw the keypoints with a radius
    that is equal to the keypoint's `size` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: '![Pyramid adapted feature detection](img/00135.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The article by E. Rosten and T. Drummond*, Machine learning for high-speed corner
    detection, In European Conference on Computer Vision, pp. 430-443, 2006*, describes
    the FAST feature algorithm and its variants in detail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting scale-invariant features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The view invariance of feature detection was presented as an important concept
    in the introduction of this chapter. While orientation invariance, which is the
    ability to detect the same points even if an image is rotated, has been relatively
    well handled by the simple feature point detectors that have been presented so
    far, the invariance to scale changes is more difficult to achieve. To address
    this problem, the concept of scale-invariant features has been introduced in computer
    vision. The idea here is to not only have a consistent detection of keypoints
    no matter at which scale an object is pictured, but to also have a scale factor
    associated with each of the detected feature points. Ideally, for the same object
    point featured at two different scales on two different images, the ratio of the
    two computed scale factors should correspond to the ratio of their respective
    scales. In recent years, several scale-invariant features have been proposed,
    and this recipe presents one of them, the **SURF** features. SURF stands for Speeded
    Up Robust Features, and as we will see, they are not only scale-invariant features,
    but they also offer the advantage of being computed very efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SURF feature detector is implemented in OpenCV in the `cv::SURF` function.
    It is also possible to use this through `cv::FeatureDetector` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To draw these features, we again use the `cv::drawKeypoints` OpenCV function
    with the `DRAW_RICH_KEYPOINTS` flag such that we can visualize the associated
    scale factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting image with the detected features is then as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00136.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As explained in the previous recipe, the size of the keypoint circles resulting
    from the use of the `DRAW_RICH_KEYPOINTS` flag is proportional to the computed
    scale of each feature. The SURF algorithm also associates an orientation with
    each feature to make them invariant to rotations. This orientation is illustrated
    by a radial line inside each drawn circle.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take another picture of the same object but at a different scale, the
    feature-detection result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00137.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: By carefully observing the detected keypoints on the two images, it can be seen
    that the change in the size of corresponding circles is often proportional to
    the change in scale. As an example, consider the bottom part of the upper-right
    window of the church. In both images, a SURF feature has been detected at that
    location, and the two corresponding circles (of different sizes) contain the same
    visual elements. Of course, this is not the case for all features, but as we will
    discover in the next chapter, the repeatability rate is sufficiently high to allow
    good matching between the two images.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 6](part0047_split_000.html#page "Chapter 6. Filtering the Images"),
    *Filtering the Images*, we learned that the derivatives of an image can be estimated
    using Gaussian filters. These filters make use of a `σ` parameter, which defines
    the aperture (size) of the kernel. As we saw, this `σ` parameter corresponds to
    the variance of the Gaussian function used to construct the filter, and it then
    implicitly defines a scale at which the derivative is evaluated. Indeed, a filter
    that has a larger `σ` value smoothes out the finer details of the image. This
    is why we can say that it operates at a coarser scale.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we compute, for instance, the Laplacian of a given image point using
    Gaussian filters at different scales, then different values are obtained. Looking
    at the evolution of the filter response for different scale factors, we obtain
    a curve that eventually reaches a maximum value at a `σ` value. If we extract
    this maximum value for two images of the same object taken at two different scales,
    the ratio of these two `σ` maxima will correspond to the ratio of the scales at
    which the images were taken. This important observation is at the core of the
    scale-invariant feature extraction process. That is, scale-invariant features
    should be detected as the local maxima in both the spatial space (in the image)
    and the scale space (as obtained from the derivative filters applied at different
    scales).
  prefs: []
  type: TYPE_NORMAL
- en: 'SURF implements this idea by proceeding as follows. First, to detect the features,
    the Hessian matrix is computed at each pixel. This matrix measures the local curvature
    of a function and is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00138.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The determinant of this matrix gives you the strength of this curvature. The
    idea, therefore, is to define corners as image points with high local curvature
    (that is, high variation in more than one direction). Since it is composed of
    second-order derivatives, this matrix can be computed using Laplacian of Gaussian
    kernels of a different scale, such as `σ`. This Hessian then becomes a function
    of three variables, which are `H(x,y,σ)`. Therefore, a scale-invariant feature
    is declared when the determinant of this Hessian reaches a local maximum in both
    spatial and scale space (that is, `3x3x3` non-maxima suppression needs to be performed).
    Note that in order to be considered as a valid point, this determinant must have
    a minimum value as specified by the first parameter in the constructor of the
    `cv::SURF` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the calculation of all of these derivatives at different scales is
    computationally costly. The objective of the SURF algorithm is to make this process
    as efficient as possible. This is achieved by using approximated Gaussian kernels
    that involve only few integer additions. These have the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00139.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The kernel on the left-hand side is used to estimate the mixed second derivatives,
    while the one on the right-hand side estimates the second derivative in the vertical
    direction. A rotated version of this second kernel estimates the second derivative
    in the horizontal direction. The smallest kernels have a size of `9x9` pixels,
    corresponding to `σ≈1.2`. To obtain a scale-space representation, kernels of increasing
    size are successively applied. The exact number of filters that are applied can
    be specified by additional parameters of the SURF class. By default, 12 different
    sizes of kernels are used (going up to size `99x99`). Note that the fact that
    integral images are used guarantees that the sum inside each lobe of each filter
    can be computed by using only three additions independent of the size of the filter.
  prefs: []
  type: TYPE_NORMAL
- en: Once the local maxima are identified, the precise position of each detected
    interest point is obtained through interpolation in both scale and image space.
    The result is then a set of feature points that are localized at sub-pixel accuracy
    and to which a scale value is associated.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SURF algorithm has been developed as an efficient variant of another well-known
    scale-invariant feature detector called **SIFT** (**Scale-Invariant Feature Transform**).
  prefs: []
  type: TYPE_NORMAL
- en: The SIFT feature-detection algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SIFT also detects features as local maxima in the image and scale space but
    uses the Laplacian filter response instead of the Hessian determinant. This Laplacian
    is computed at different scales (that is, increasing values of `σ`) using the
    difference of Gaussian filters, as explained in [Chapter 6](part0047_split_000.html#page
    "Chapter 6. Filtering the Images"), *Filtering the Images*. To improve efficiency,
    each time the value of `σ` is doubled, the size of the image is reduced by two.
    Each pyramid level corresponds to an **octave**, and each scale is a *layer*.
    There are typically three layers per octave.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates a pyramid of two octaves in which the four
    Gaussian-filtered images of the first octave produce three DoG layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The SIFT feature-detection algorithm](img/00140.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'OpenCV has a class that detects these features, and it is called in a way that
    is similar to the SURF one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we use all the default arguments to construct the detector, but you can
    specify the number of desired SIFT points (the strongest ones are kept), the number
    of layers per octave, and the initial value for `σ`. The result is similar to
    the one obtained with SURF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The SIFT feature-detection algorithm](img/00141.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: However, since the computation of the feature point is based on floating-point
    kernels, SIFT is generally considered to be more accurate in terms of feature
    localization in regards to space and scale. For the same reason, it is also more
    computationally expensive, although this relative efficiency depends on each particular
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: As a final remark, you might have noticed that the SURF and SIFT classes have
    been placed in a nonfree package of the OpenCV distribution. This is because these
    algorithms have been patented, and as such, their use in commercial applications
    might be subject to licensing agreements.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Computing the Laplacian of an image* recipe in [Chapter 6](part0047_split_000.html#page
    "Chapter 6. Filtering the Images"), *Filtering the Images*, gives you more details
    on the Laplacian-of-Gaussian operator and the use of the difference of Gaussians
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Describing local intensity patterns* recipe in [Chapter 9](part0063_split_000.html#page
    "Chapter 9. Describing and Matching Interest Points"), *Describing and Matching
    Interest Points*, explains how these scale-invariant features can be described
    for robust image matching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The article *SURF: Speeded Up Robust Features* by H. Bay, A. Ess, T. Tuytelaars
    and L. Van Gool in *Computer Vision and Image Understanding, vol. 110, No. 3,
    pp. 346-359, 2008*, describes the SURF feature algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pioneering work by D. Lowe*, Distinctive Image Features from Scale Invariant
    Features* in *International Journal of Computer Vision, Vol. 60, No. 2, 2004,
    pp. 91-110*, describes the SIFT algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting FAST features at multiple scales
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FAST has been introduced as a quick way to detect keypoints in an image. With
    SURF and SIFT, the emphasis was on designing scale-invariant features. More recently,
    new interest point detectors have been introduced with the objective of achieving
    both fast detection and invariance to scale changes. This recipe presents the
    **Binary Robust Invariant Scalable Keypoints** (**BRISK**) detector. It is based
    on the FAST feature detector that we described in a previous recipe of this chapter.
    Another detector, called **ORB** (**Oriented FAST and Rotated BRIEF**), will also
    be discussed at the end of this recipe. These two feature point detectors constitute
    an excellent solution when fast and reliable image matching is required. They
    are especially efficient when they are used in conjunction with their associated
    binary descriptors, as will be discussed in [Chapter 9](part0063_split_000.html#page
    "Chapter 9. Describing and Matching Interest Points"), *Describing and Matching
    Interest Points*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following what we did in the previous recipes, the detection of keypoints with
    BRISK uses the `cv::FeatureDetector` abstract class. We first create an instance
    of the detector, and then the `detect` method is called on an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The image result shows you the keypoints that are detected at multiple scales:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00142.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BRISK is not only a feature point detector; the method also includes a procedure
    that describes the neighborhood of each detected keypoint. This second aspect
    will be the subject of the next chapter. We describe here how the quick detection
    of keypoints at multiple scales is performed using BRISK.
  prefs: []
  type: TYPE_NORMAL
- en: In order to detect interest points at different scales, the method first builds
    an image pyramid through two down-sampling processes. The first process starts
    from the original image size and downscales it by half at each layer (or octave).
    Secondly, in-between layers are created by down-sampling the original image by
    a factor of 1.5, and from this reduced image, additional layers are generated
    through successive half-sampling.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00143.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The FAST feature detector is then applied on all the images of this pyramid.
    Keypoint extraction is based on a criterion that is similar to the one used by
    SIFT. First, an acceptable interest point must be a local maximum when comparing
    its strength with one of its eight spatial neighbors. If this is the case, the
    point is then compared with the scores of the neighboring points in the layers
    above and below; if its score is higher in scale as well, then it is accepted
    as an interest point. A key aspect of BRISK resides in the fact that the different
    layers of the pyramid have different resolutions. The method requires interpolation
    in both scale and space in order to locate each keypoint precisely. This interpolation
    is based on the FAST keypoint scores. In space, the interpolation is performed
    on a 3 x 3 neighborhood. In scale, it is computed by fitting a 1D parabola along
    the scale axis through the current point and its two neighboring local keypoints
    in the layers above and below; this keypoint localization in scale is illustrated
    in the preceding figure. As a result, even if the FAST keypoint detection is performed
    at discrete image scales, the resulting detected scales associated with each keypoint
    are continuous values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cv::BRISK` class proposes two optional parameters to control the detection
    of the keypoints. The first parameter is a threshold value that accepts FAST keypoints,
    and the second parameter is the number of octaves that will be generated in the
    image pyramid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BRISK is not the only multiscale, fast detector that is proposed in OpenCV.
    The ORB feature detector can also perform efficient keypoint detection.
  prefs: []
  type: TYPE_NORMAL
- en: The ORB feature-detection algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ORB stands for **Oriented FAST and Rotated BRIEF**. The first part of this acronym
    refers to the keypoint detection part, while the second part refers to the descriptor
    that is proposed by ORB. Here, we focus here on the detection method; the descriptor
    will be presented in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: As with BRISK, ORB first creates an image pyramid. This one is made of a number
    of layers in which each layer is a down-sampled version of the previous one by
    a certain scale factor (typically, `8` scales and `1.2` scale factor reduction;
    these are parameters in the `cv::ORB` function). The strongest `N` keypoints are
    then accepted where the keypoint score is defined by the Harris *cornerness* measure
    that was defined in the first recipe of this chapter (authors of this method found
    the Harris score to be a more reliable measure).
  prefs: []
  type: TYPE_NORMAL
- en: An original aspect of the ORB detector resides in the fact that an orientation
    is associated with each detected interest point. As we will see in the next chapter,
    this information will be useful to align the descriptors of keypoints detected
    in different images. In the *Computing components' shape descriptors* recipe of
    [Chapter 7](part0052_split_000.html#page "Chapter 7. Extracting Lines, Contours,
    and Components"), *Extracting Lines, Contours, and Components*, we introduced
    the concept of image moments and in particular, we showed you how the centroid
    of a component can be computed from its first three moments. ORB proposes that
    we use the orientation of the centroid of a circular neighborhood around the keypoint.
    Since, FAST keypoints, by definition, always have a decentered centroid, the angle
    of the line that joins the central point and the centroid will always be well
    defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ORB features are detected as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This call produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ORB feature-detection algorithm](img/00144.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen, since the keypoints are independently detected on each pyramid
    layer, the detector tends to repeatedly detect the same feature point at different
    scales.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Describing keypoints with binary features* recipe in [Chapter 9](part0063_split_000.html#page
    "Chapter 9. Describing and Matching Interest Points"), *Describing and Matching
    Interest Points*, explains how simple binary descriptors can be used for efficient
    robust matching of these features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The article *BRISK: Binary Robust Invariant Scalable Keypoint by* S. Leutenegger,
    M. Chli and R. Y. Siegwart in *IEEE International Conference on Computer Vision,
    pp. 2448--2555, 2011*, describes the BRISK feature algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The article *ORB: an efficient alternative to SIFT or SURF* by E. Rublee, V.
    Rabaud, K. Konolige and G. Bradski in *IEEE International Conference on Computer
    Vision, pp.2564-2571, 2011*, describes the ORB feature algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
