["```py\nselect * from openaq where aggdate like '2019-01%'\n```", "```py\n{“date”:{“utc”:”2021-03-20T19:00:00.000Z”,”local”:”2021-03-20T23:00:00+04:00”},”parameter”:”pm25”,”value”:32,”unit”:”µg/m³”,”averagingPeriod”:{“val\nue”:1,”unit”:”hours”},”location”:”US Diplomatic Post:Dubai”,”city”:”Dubai”,”country”:”AE”,”coordinates”:{“latitude”:25.25848,”longitude”:55.309166\n},”attribution”:[{“name”:”EPA AirNow DOS”,”url”:”http://airnow.gov/index.cfm?action=airnow.global_summary”}],”sourceName”:”StateAir_Dubai”,”sourceT\nype”:”government”,”mobile”:false}\n```", "```py\n    spark_processor = PySparkProcessor(\n        base_job_name=”spark-preprocessor”,\n        framework_version=”3.0”,\n        role=role,\n        instance_count=15,\n        instance_type=”ml.m5.4xlarge”,\n        max_runtime_in_seconds=7200,\n    )\n    ```", "```py\n    configuration = [\n        {\n        “Classification”: “spark-defaults”,\n        “Properties”: {“spark.executor.memory”: “18g”, \n            “spark.yarn.executor.memoryOverhead”: “3g”,\n                       “spark.driver.memory”: “18g”,\n              “spark.yarn.driver.memoryOverhead”: “3g”,\n                       “spark.executor.cores”: “5”, \n                       “spark.driver.cores”: “5”,\n                       “spark.executor.instances”: “44”,\n                       “spark.default.parallelism”: “440”,\n                “spark.dynamicAllocation.enabled”: “false”\n                      },\n        },\n        {\n        “Classification”: “yarn-site”,\n        “Properties”: {“yarn.nodemanager.vmem-check-enabled”: “false”, \n          “yarn.nodemanager.mmem-check-enabled”: “false”},\n        }\n    ]\n    ```", "```py\n    spark_processor.run(\n        submit_app=”scripts/preprocess.py”,\n        submit_jars=[“s3://crawler-public/json/serde/json-serde.jar”],\n        arguments=['--s3_input_bucket', s3_bucket,\n                  '--s3_input_key_prefix', s3_prefix_parquet,\n                   '--s3_output_bucket', s3_bucket,\n                 '--s3_output_key_prefix', s3_output_prefix],\n        spark_event_logs_s3_uri=”s3://{}/{}/spark_event_logs”.format(s3_bucket, 'sparklogs'),\n        logs=True,\n        configuration=configuration\n    )\n    ```", "```py\n# the helper function `get_tables` lists the tables we want to include\ntables = get_tables()\ndf = spark.read.parquet( \n    f”s3://{args.s3_input_bucket}/” +\n    f”{args.s3_input_key_prefix}/{tables[0]}/”)\nfor t in tables[1:]:\n    df_new = spark.read.parquet( \n        f”s3://{args.s3_input_bucket}/” +\n        f”{args.s3_input_key_prefix}/{t}/”)\n    df = df.union(df_new)\n```", "```py\ndf = df.drop('date_local') \\     \n.drop('unit') \\\n.drop('attribution') \\\n.drop('averagingperiod') \\\n.drop('coordinates')\n```", "```py\ndf = df.withColumn(“ismobile”,col(“mobile”).cast(IntegerType())) \\\n.drop('mobile')\n```", "```py\nvalue_assembler = VectorAssembler(inputCols=[“value”], outputCol=”value_vec”)\nvalue_scaler = StandardScaler(inputCol=”value_vec”, outputCol=”value_scaled”)\nvalue_pipeline = Pipeline(stages=[value_assembler, value_scaler])\nvalue_model = value_pipeline.fit(df)\nxform_df = value_model.transform(df)\n```", "```py\nxform_df = xform_df.withColumn('aggdt', \n               to_date(unix_timestamp(col('date_utc'), \n“yyyy-MM-dd'T'HH:mm:ss.SSSX”).cast(“timestamp”)))\nxform_df = xform_df.withColumn('year',year(xform_df.aggdt)) \\\n        .withColumn('month',month(xform_df.aggdt)) \\\n        .withColumn('quarter',quarter(xform_df.aggdt))\nxform_df = xform_df.withColumn(“day”, date_format(col(“aggdt”), “d”))\n```", "```py\nisBadAirUdf = udf(isBadAir, IntegerType())\nxform_df = xform_df.withColumn('isBadAir', isBadAirUdf('value', 'parameter'))\n```", "```py\nparameter_indexer = StringIndexer(inputCol=”parameter”, \\\noutputCol=”indexed_parameter”, handleInvalid='keep')\nlocation_indexer = StringIndexer(inputCol=”location”, \\\noutputCol=”indexed_location”, handleInvalid='keep')\ncity_indexer = StringIndexer(inputCol=”city”, \\ \noutputCol=”indexed_city”, handleInvalid='keep')\ncountry_indexer = StringIndexer(inputCol=”country”, \\\noutputCol=”indexed_country”, handleInvalid='keep')\nsourcename_indexer = StringIndexer(inputCol=”sourcename”, \\\noutputCol=”indexed_sourcename”, handleInvalid='keep')\nsourcetype_indexer = StringIndexer(inputCol=”sourcetype”, \\\noutputCol=”indexed_sourcetype”, handleInvalid='keep')\nenc_est = OneHotEncoder(inputCols=[“indexed_parameter”], \\\noutputCols=[“vec_parameter”])\nenc_pipeline = Pipeline(stages=[parameter_indexer, location_indexer, \n        city_indexer, country_indexer, sourcename_indexer, \n        sourcetype_indexer, enc_est])\nenc_model = enc_pipeline.fit(xform_df)\nenc_df = enc_model.transform(xform_df)\nparam_cols = enc_df.schema.fields[17].metadata['ml_attr']['vals']\n```", "```py\n(train_df, validation_df, test_df) = final_df.randomSplit([0.7, 0.2, 0.1])\ntrain_df.write.option(“header”,True).csv('s3://' + \\\nos.path.join(args.s3_output_bucket, \n      args.s3_output_key_prefix, 'train/'))\nvalidation_df.write.option(“header”,True).csv('s3://' + \\\nos.path.join(args.s3_output_bucket, \n      args.s3_output_key_prefix, 'validation/'))\ntest_df.write.option(“header”,True).csv('s3://' + \\\nos.path.join(args.s3_output_bucket, \n      args.s3_output_key_prefix, 'test/'))\n```"]