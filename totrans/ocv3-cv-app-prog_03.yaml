- en: Chapter 3. Processing the Colors of an Image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing colors using the Strategy design pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmenting an image with the GrabCut algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting color representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing colors with hue, saturation, and brightness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability to see the world in colors is one of the important characteristics
    of the human visual system. The retina of the human eye includes specialized photoreceptors,
    called cones, which are responsible for the perception of colors. There are three
    types of cones that differ in the wavelength range of light they absorb; using
    the stimuli from these different cells, the human brain is able to create color
    perception. Most other animals only have rod cells, which are photoreceptors with
    better light sensitivity but that cover the full spectrum of visible light without
    color discrimination. In the human eye, rods are mainly located at the periphery
    of the retina, while the cones are concentrated in the central part.
  prefs: []
  type: TYPE_NORMAL
- en: In digital imaging, colors are generally reproduced by using the red, green,
    and blue additive primary colors. These have been selected because when they are
    combined together, they can produce a wide gamut of different colors. In fact,
    this choice of primaries mimics well the trichromatic color perception of the
    human visual system as the different cone cells have sensitivity located around
    the red, green, and blue spectrum. In this chapter, you will play with the pixel
    color and see how an image can be segmented based on the color information. In
    addition, you will learn that it can sometimes be useful to use a different color
    representation when performing color image processing.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing colors using the Strategy design pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say we want to build a simple algorithm that will identify all of the
    pixels in an image that have a given color. For this, the algorithm has to accept
    an image and a color as input and will return a binary image showing the pixels
    that have the specified color. The tolerance with which we want to accept a color
    will be another parameter to be specified before running the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In order to accomplish this objective, this recipe will use the **Strategy design
    pattern**. This object-oriented design pattern constitutes an excellent way of
    encapsulating an algorithm in a class. It becomes then easier to replace a given
    algorithm with another one, or to chain several algorithms together in order to
    build a more complex process. In addition, this pattern facilitates the deployment
    of an algorithm by hiding as much of its complexity as possible behind an intuitive
    programming interface.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once an algorithm has been encapsulated in a class using the Strategy design
    pattern, it can be deployed by creating an instance of this class. Typically,
    the instance will be created when the program is initialized. At the time of construction,
    the class instance will initialize the different parameters of the algorithm with
    their default values so that it will immediately be ready to be used. The algorithm's
    parameter values can also be read and set using appropriate methods. In the case
    of an application with a GUI, these parameters can be displayed and modified using
    different widgets (text fields, sliders, and so on) so that a user can easily
    play with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will show you the structure of a `Strategy` class in the next section; let''s
    start with an example of how it can be deployed and used. Let''s write a simple
    `main` function that will run our proposed color detection algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this program to detect a blue sky in the colored version of the *Castle*
    image presented in the previous chapter produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/image_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, a white pixel indicates a positive detection of the sought color, and
    black indicates negative.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, the algorithm we encapsulated in this class is relatively simple
    (as we will see next, it is composed of just one scanning loop and one tolerance
    parameter). The Strategy design pattern becomes really powerful when the algorithm
    to be implemented is more complex, has many steps, and includes several parameters.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The core process of this algorithm is easy to build. It is a simple scanning
    loop that goes over each pixel, comparing its color with the target color. Using
    what we learned in the *Scanning an image with iterators* recipe of the previous
    chapter, this loop can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `cv::Mat` variable image refers to the input image, while `result` refers
    to the binary output image. Therefore, the first step consists of setting up the
    required iterators. The scanning loop then becomes easy to implement. Note that
    the input image iterators are declared `const` as the values of their elements
    are not modified. The distance between the current pixel color and the target
    color is evaluated for each pixel in order to check whether it is within the tolerance
    parameter defined by `maxDist`. If that is the case, the value `255` (white) is
    then assigned to the output image; if not, `0` (black) is assigned. To compute
    the distance to the target color, the `getDistanceToTargetColor` method is used.
    There are different ways to compute this distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'One could, for example, calculate the Euclidean distance between the three
    vectors that contain the RGB color values. To keep this computation simple, we
    sum the absolute differences of the RGB values (this is also known as the **city-block
    distance**). Note that in modern architecture, a floating-point Euclidean distance
    can be faster to compute than a simple city-block distance (in addition, you can
    also use squared Euclidean distances to avoid the costly square-root); this is
    also something to take into consideration in your design. Also, for more flexibility,
    we write the `getDistanceToTargetColor` method in terms of a `getColorDistance`
    method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how we used `cv::Vec3d` to hold the three unsigned chars that represent
    the RGB values of a color. The `target` variable obviously refers to the specified
    target color, and as we will see, it is defined as a member variable in the class
    algorithm that we will define. Now, let''s complete the definition of the processing
    method. Users will provide an input image, and the result will be returned once
    the image scanning is completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Each time this method is called, it is important to check if the output image
    that contains the resulting binary map needs to be reallocated to fit the size
    of the input image. This is why we use the `create` method of `cv::Mat`. Remember
    that this method will only proceed to reallocation if the specified size and/or
    depth do not correspond to the current image structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the core processing method defined, let''s see what additional
    methods should be added in order to deploy this algorithm. We have previously
    determined what input and output data our algorithm requires. Therefore, we define
    the class attributes that will hold this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to create an instance of the class that encapsulates our algorithm
    (which we have named `ColorDetector`), we need to define a constructor. Remember
    that one of the objectives of the Strategy design pattern is to make algorithm
    deployment as easy as possible. The simplest constructor that can be defined is
    an empty one. It will create an instance of the class algorithm in a valid state.
    We then want the constructor to initialize all the input parameters to their default
    values (or the values that are known to generally give a good result). In our
    case, we decided that a distance of `100` is generally an acceptable tolerance
    parameter. We also set the default target color. We chose black for no particular
    reason. The idea is to make sure we always start with predictable and valid input
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Another option would have been not create an empty constructor and rather force
    the user to input a target color and a color distance in a more elaborated constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, a user who creates an instance of our class algorithm can immediately
    call the process method with a valid image and obtain a valid output. This is
    another objective of the Strategy pattern, that is, to make sure that the algorithm
    always runs with valid parameters. Obviously, the users of this class will want
    to use their own settings. This is done by providing the user with the appropriate
    getters and setters. Let''s start with the `color` tolerance parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how we first check the validity of the input. Again, this is to make sure
    that our algorithm will never be run in an invalid state. The target color can
    be set in a similar manner, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This time, it is interesting to note that we have provided the user with two
    definitions of the `setTargetColor` method. In the first version of the definition,
    the three color components are specified as three arguments, while in the second
    version, `cv::Vec3b` is used to hold the color values. Again, the objective is
    to facilitate the use of our class algorithm. The users can simply select the
    setter that best fits their needs.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The example algorithm used in this recipe consisted of identifying the pixels
    of an image that has a color sufficiently close to a specified target color. This
    computation could have been done otherwise. Interestingly, an OpenCV function
    performs a similar task in order to extract a connected component of a given color.
    Also, the implementation of a Strategy design pattern could be complemented using
    function objects. Finally, OpenCV has defined a base class, `cv::Algorithm`, that
    implements the Strategy design pattern concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the distance between two color vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To compute the distance between two color vectors, we used the following simple
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'However, OpenCV includes a function to compute the Euclidean norm of a vector.
    Consequently, we could have computed our distance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: A very similar result would then be obtained using this definition of the `getDistance`
    method. Here, we use `cv::Vec3i` (a 3-vector array of integers) because the result
    of the subtraction is an integer value.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also interesting to recall from [Chapter 2](ch02.html "Chapter 2. Manipulating
    Pixels") , *Manipulating Pixels*, that the OpenCV matrix and vector data structures
    include a definition of the basic arithmetic operators. Consequently, one could
    have proposed the following definition for the distance computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This definition may look right at the first glance; however, it is wrong. This
    is because all these operators always include a call to `saturate_cast` (see the
    *Scanning an image with neighbor access* recipe in the previous chapter) in order
    to ensure that the results stay within the domain of the input type (here, it
    is `uchar`). Therefore, in the cases where the target value is greater than the
    corresponding color value, the value `0` will be assigned instead of the negative
    value that one would have expected. A correct formulation would then be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: However, using two function calls to compute the distance between two 3-vector
    arrays is inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Using OpenCV functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this recipe, we used a loop with iterators in order to perform our computation.
    Alternatively, we could have achieved the same result by calling a sequence of
    OpenCV functions. The color detection method will then be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This method uses the `absdiff` function, which computes the absolute difference
    between the pixels of an image and, in this case, a scalar value. Instead of a
    scalar value, another image can be provided as the second argument to this function.
    In the latter case, a pixel-by-pixel difference will be applied; consequently,
    the two images must be of the same size. The individual channels of the difference
    image are then extracted using the `split` function (discussed in the *There's
    more...* section of the *Performing simple image arithmetic* recipe of [Chapter
    2](ch02.html "Chapter 2. Manipulating Pixels") , *Manipulating Pixels*) in order
    to be able to add them together. It is important to note that the result of this
    sum may sometimes be greater than `255`, but because saturation is always applied,
    the result will be stopped at `255`. The consequence is that with this version,
    the `maxDist` parameter must also be less than `256`; this should be corrected
    if you consider this behavior unacceptable.
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to create a binary image by using the `cv::threshold` function.
    This function is commonly used to compare all the pixels with a threshold value
    (the third parameter), and in the regular thresholding mode (`cv::THRESH_BINARY`),
    it assigns the defined maximum value (the fourth parameter) to all the pixels
    greater than the specified threshold and `0` to the other pixels. Here, we used
    the inverse mode (`cv::THRESH_BINARY_INV`) in which the defined maximum value
    is assigned to the pixels that have a value lower than or equal to the threshold.
    Of interest are also the `cv::THRESH_TOZERO` and `cv::THRESH_TOZERO_INV` modes,
    which leave the pixels greater than or lower than the threshold unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: Using the OpenCV functions is generally a good idea. You can then quickly build
    complex applications and potentially reduce the number of bugs. The result is
    often more efficient (thanks to the optimization efforts invested by the OpenCV
    contributors). However, when many intermediate steps are performed, you may find
    that the resulting method consumes more memory.
  prefs: []
  type: TYPE_NORMAL
- en: The floodFill function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our `ColorDetector` class identifies the pixels in an image that have a color
    similar to a given target color. The decision to accept or not a pixel is simply
    made on a per-pixel basis. The `cv::floodFill` function proceeds in a very similar
    way with one important difference: in this case, the decision to accept a pixel
    also depends on the state of its neighbors. The idea is to identify a connected
    area of a certain color. The user specifies a starting pixel location and tolerance
    parameters that determine color similarity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The seed pixel defines the color that is seek and from this seed location,
    the neighbors are considered in order to identify pixels of similar color; then
    the neighbors of the accepted neighbors are also considered and so on. This way,
    one area of constant color will be extracted from the image. For example, to detect
    the blue sky area in our example image, you could proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The seed pixel (`100`, `50`) is located in the sky. All connected pixels will
    be tested and the ones having a similar color will be repainted in a new color
    specified by the third parameter. To determine if a color is similar or not, different
    thresholds are defined independently for values that are higher or lower than
    the reference color. Here, we used fixed range mode, which implies that the tested
    pixels will all be compared to the seed pixel''s color. The default mode is the
    one where each tested pixel is compared to the color of its neighbors. The result
    obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The floodFill function](img/image_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A single connected area is repainted by the algorithm (here, we painted the
    sky in white). Therefore, even if there are some pixels somewhere else with a
    similar color (in the water, for instance), these ones would not be identified
    unless they were connected to the sky area.
  prefs: []
  type: TYPE_NORMAL
- en: Functor or function object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the C++ operator overloading, it is possible to create a class for which
    its instances behave as functions. The idea is to overload the `operator()` method
    so that a call to the processing method of a class looks exactly like a simple
    function call. The resulting class instance is called a function object, or a
    **functor**. Often, a functor includes a full constructor such that it can be
    used immediately after being created. For example, you can define the full constructor
    of your `ColorDetector` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Obviously, you can still use the setters and getters that have been defined
    previously. The functor method can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To detect a given color with this functor method, simply write the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the call to the color detection method now looks like a function
    call.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenCV base class for algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenCV offers many algorithms that perform various computer vision tasks. To
    facilitate their use, most of these algorithms have been made subclass of a generic
    base class called `cv::Algorithm`. This one implements some of the concepts dictated
    by the Strategy design pattern. First, all these algorithms are created dynamically
    using a specialized static method that makes sure that the algorithm is always
    created in a valid state (that is, with valid default values for the unspecified
    parameters). Let's consider, for example, one of these subclasses, `cv::ORB`;
    this one is an interest point operator that will be discussed in the *Detecting
    FAST features at Multiple Scales* recipe in  [Chapter 8](ch08.html "Chapter 8. Detecting
    Interest Points") , *Detecting Interest Points*. Here, we simply use it as an
    illustrative example of an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'An instance of this algorithm is therefore created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Once created, the algorithm can then be used. For example, the generic methods
    `read` and `write` can be used to load or store the state of the algorithm. The
    algorithms also have specialized methods (in the case of ORB, for example, the
    methods `detect` and `compute` can be used to trigger its main computational units).
    Algorithms also have specialized setter methods that allows specifying their internal
    parameters. Note that we could have declared the pointer as `cv::Ptr<cv::Algorithm>`
    but, in this case, we would not be able to use its specialized methods.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The policy-based class design, introduced by A. Alexandrescu, is an interesting
    variant of the Strategy design pattern in which algorithms are selected at compile
    time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Converting color representation* recipe introduces the concept of perceptually
    uniform color spaces to achieve more intuitive color comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmenting an image with the GrabCut algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous recipe showed how color information can be useful to segment an
    image into area corresponding to specific elements of a scene. Objects often have
    distinctive colors, and these ones can often be extracted by identifying areas
    of similar colors. OpenCV proposes an implementation of a popular algorithm for
    image segmentation: the **GrabCut** algorithm. GrabCut is a complex and computationally
    expensive algorithm, but it generally produces very accurate results. It is the
    best algorithm to use when you want to extract a foreground object in a still
    image (for example, to cut and paste an object from one picture to another).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `cv::grabCut` function is easy to use. You just need to input an image and
    label some of its pixels as belonging to the background or to the foreground.
    Based on this partial labeling, the algorithm will then determine a foreground/
    background segmentation for the complete image.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to specify a partial foreground/background labeling for an input image
    is by defining a rectangle inside which the foreground object is included:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This defines the following area in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/image_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'All the pixels outside this rectangle will then be marked as background. In
    addition to the input image and its segmentation image, calling the `cv::grabCut`
    function requires the definition of two matrices, which will contain the models
    built by the algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how we specified that we are using the bounding rectangle mode with the
    `cv::GC_INIT_WITH_RECT` flag as the last argument of the function (the next section,
    *How it works...*, will discuss the other available mode). The input/output segmentation
    image can have one of the following four values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cv::GC_BGD`: This is the value of the pixels that certainly belong to the
    background (for example, pixels outside the rectangle in our example)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cv::GC_FGD`: This is the value of the pixels that certainly belong to the
    foreground (there are none in our example)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cv::GC_PR_BGD`: This is the value of the pixels that probably belong to the
    background'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cv::GC_PR_FGD`: This is the value of the pixels that probably belong to the
    foreground (that is, the initial value of the pixels inside the rectangle in our
    example)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We get a binary image of the segmentation by extracting the pixels that have
    a value equal to `cv::GC_PR_FGD`. This is accomplished with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To extract all the foreground pixels, that is, with values equal to `cv::GC_PR_FGD`
    or `cv::GC_FGD`, it is possible to check the value of the first bit, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This is possible because these constants are defined as values `1` and `3`,
    while the other two (`cv::GC_BGD` and `cv::GC_PR_BGD`) are defined as `0` and
    `2`. In our example, the same result is obtained because the segmentation image
    does not contain the `cv::GC_FGD` pixels (only the `cv::GC_BGD` pixels have been
    inputted).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image is then obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/image_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding example, the GrabCut algorithm was able to extract the foreground
    object by simply specifying a rectangle inside which this objects (the castle)
    was contained. Alternatively, one could also assign the values `cv::GC_BGD` and
    `cv::GC_FGD` to some specific pixels of the input image, which are provided by
    using a mask image as the second argument of the `cv::grabCut` function. You would
    then specify `GC_INIT_WITH_MASK` as the input mode flag. These input labels could
    be obtained, for example, by asking a user to interactively mark a few elements
    of the image. It is also possible to combine these two input modes.
  prefs: []
  type: TYPE_NORMAL
- en: Using this input information, the GrabCut algorithm creates the background/foreground
    segmentation by proceeding as follows. Initially, a foreground label (`cv::GC_PR_FGD`)
    is tentatively assigned to all the unmarked pixels. Based on the current classification,
    the algorithm groups the pixels into clusters of similar colors (that is, `K`
    clusters for the background and `K` clusters for the foreground). The next step
    is to determine a background/ foreground segmentation by introducing boundaries
    between the foreground and background pixels.
  prefs: []
  type: TYPE_NORMAL
- en: This is done through an optimization process that tries to connect pixels with
    similar labels, and that imposes a penalty for placing a boundary in the regions
    of relatively uniform intensity. This optimization problem can be efficiently
    solved using the Graph Cuts algorithm, a method that can find the optimal solution
    of a problem by representing it as a connected graph on which cuts are applied
    in order to compose an optimal configuration. The obtained segmentation produces
    new labels for the pixels.
  prefs: []
  type: TYPE_NORMAL
- en: The clustering process can then be repeated, and a new optimal segmentation
    is found again, and so on. Therefore, the GrabCut algorithm is an iterative procedure
    that gradually improves the segmentation result. Depending on the complexity of
    the scene, a good solution can be found in more or less number of iterations (in
    easy cases, one iteration would be enough).
  prefs: []
  type: TYPE_NORMAL
- en: This explains the argument of the function where the user can specify the number
    of iterations to be applied. The two internal models maintained by the algorithm
    are passed as an argument of the function (and returned). Therefore, it is possible
    to call the function with the models of the last run again if one wishes to improve
    the segmentation result by performing additional iterations.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The article *GrabCut: Interactive Foreground Extraction using Iterated Graph
    Cuts* in *ACM Transactions on Graphics (SIGGRAPH) volume 23, issue 3, August 2004,
    C. Rother, V. Kolmogorov, and A. Blake* describes the GrabCut algorithm in detail'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Segmenting images using watersheds* recipe in [Chapter 5](ch05.html "Chapter 5. Transforming
    Images with Morphological Operations"), *Transforming Images with Morphological
    Operations*, presents another image segmentation algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting color representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RGB color space is based on the use of the red, green, and blue additive
    primary colors. We saw in the first recipe of this chapter that these primaries
    have been chosen because they can produce a good range of colors well aligned
    with the human visual system. It is often the default color space in digital imagery
    because this is the way color images are acquired, that is, through the use of
    red, green, and blue filters. Additionally, the red, green, and blue channels
    are normalized such that when combined in equal amounts, a gray-level intensity
    is obtained, that is, from black `(0,0,0)` to white `(255,255,255)`.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, computing the distance between the colors using the RGB color
    space is not the best way to measure the similarity between two given colors.
    Indeed, RGB is not a **perceptually uniform color space**. This means that two
    colors at a given distance might look very similar, while two other colors separated
    by the same distance might look very different.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, other color representations that have the property of
    being perceptually uniform have been introduced. In particular, the **CIE L*a*b***
    is one such color model. By converting our images to this representation, the
    Euclidean distance between an image pixel and the target color will then be a
    meaningful measure of the visual similarity between the two colors. In this recipe,
    we will show you how to convert colors from one representation to another in order
    to work with other color spaces.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Conversion of images between different color spaces is easily done through
    the use of the `cv::cvtColor` OpenCV function. Let''s revisit the `ColorDetector`
    class of the first recipe of this chapter, *Comparing colors using the Strategy
    design pattern*. We now convert the input image to the CIE L*a*b* color space
    at the beginning of the process method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `converted` variable contains the image after color conversion. In the
    `ColorDetector` class, it is defined as a class attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You also need to convert the input target color. You can do this by creating
    a temporary image that contains only one pixel. Note that you need to keep the
    same signature as in the earlier recipes, that is, the user continues to supply
    the target color in RGB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If the application of the preceding recipe is compiled with this modified class,
    it will now detect the pixels of the target color using the CIE L*a*b* color model.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When an image is converted from one color space to another, a linear or nonlinear
    transformation is applied on each input pixel to produce the output pixels. The
    pixel type of the output image will match the one of the input image. Even if
    you work with 8-bit pixels most of the time, you can also use a color conversion
    with floating-point images (in which case, the pixel values are generally assumed
    to vary between `0` and `1.0`) or with integer images (with pixels generally varying
    between `0` and `65535`). However, the exact domain of the pixel values depends
    on the specific color space and destination image type. For example, with the
    `CIE L*a*b*` color space, the `L` channel, which represents the brightness of
    each pixel, varies between `0` and `100`, and it is rescaled between `0` and `255`
    in the case of the 8-bit images.
  prefs: []
  type: TYPE_NORMAL
- en: The `a` and `b` channels correspond to the chromaticity components. These channels
    contain information about the color of a pixel, independent of its brightness.
    Their values vary between `-127` and `127`; for 8-bit images, `128` is added to
    each value in order to make it fit within the `0` to `255` interval. However,
    note that the 8-bit color conversion will introduce rounding errors that will
    make the transformation imperfectly reversible.
  prefs: []
  type: TYPE_NORMAL
- en: Most commonly used color spaces are available. It is just a question of providing
    the right color space conversion code to the OpenCV function (for CIE L*a*b*,
    this code is `CV_BGR2Lab`). Among these is YCrCb, which is the color space used
    in JPEG compression. To convert a color space from BGR to YCrCb, the code will
    be `CV_BGR2YCrCb`. Note that all the conversions that involve the three regular
    primary colors, red, green, and blue, are available in the RGB and BGR order.
  prefs: []
  type: TYPE_NORMAL
- en: The **CIE L*u*v*** color space is another perceptually uniform color space.
    You can convert from BGR to CIE L*u*v by using the `CV_BGR2Luv` code. Both L*a*b*
    and L*u*v* use the same conversion formula for the brightness channel but use
    a different representation for the chromaticity channels. Also, note that since
    these two color spaces distort the RGB color domain in order to make it perceptually
    uniform, these transformations are nonlinear (therefore, they are costly to compute).
  prefs: []
  type: TYPE_NORMAL
- en: There is also the CIE XYZ color space (with the `CV_BGR2XYZ` code). It is a
    standard color space used to represent any perceptible color in a device-independent
    way. In the computation of the L*u*v and L*a*b color spaces, the XYZ color space
    is used as an intermediate representation. The transformation between RGB and
    XYZ is linear. It is also interesting to note that the `Y` channel corresponds
    to a gray-level version of the image.
  prefs: []
  type: TYPE_NORMAL
- en: HSV and HLS are interesting color spaces because they decompose the colors into
    their hue and saturation components plus the value or luminance component, which
    is a more natural way for humans to describe colors. The next recipe will present
    this color space.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also convert color images to gray-level intensities. The output will
    be a one-channel image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: It is also possible to do the conversion in the other direction, but the three
    channels of the resulting color image will then be identically filled with the
    corresponding values in the gray-level image.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Using the mean shift algorithm to find an object* recipe in [Chapter 4](ch04.html
    "Chapter 4. Counting the Pixels with Histograms"), *Counting the Pixels with Histograms*,
    uses the HSV color space in order to find an object in an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many good references are available on the color space theory. Among them, the
    following is a complete reference: *The Structure and Properties of Color Spaces
    and the Representation of Color Images, E. Dubois, Morgan and Claypool Publishers,
    2009*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing colors with hue, saturation, and brightness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we played with image colors. We used different color spaces
    and tried to identify image areas of uniform color. The RGB color space was initially
    considered, and although it is an effective representation for the capture and
    display of colors in electronic imaging systems, this representation is not very
    intuitive. Indeed, this is not the way humans think about colors; they most often
    describe colors in terms of their tint, brightness, or colorfulness (that is,
    whether it is a vivid or pastel color). A color space based on the concept of
    hue, saturation, and brightness has then been introduced to help users to specify
    the colors using properties that are more intuitive to them. In this recipe, we
    will explore the concepts of hue, saturation, and brightness as a means to describe
    colors.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The conversion of a BGR image into another color space is done using the `cv::cvtColor`
    function that was explored in the previous recipe. Here, we will use the `CV_BGR2HSV`
    conversion code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can go back to the BGR space using the `CV_HSV2BGR` code. We can visualize
    each of the HSV components by splitting the converted image channels into three
    independent images, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note that the third channel is the value of the color, that is, an approximate
    measure of the brightness of the color. Since we are working on 8-bit images,
    OpenCV rescales the channel values to cover the `0` to `255` range (except for
    the hue, which is rescaled between `0` and `0180` as it will be explained in the
    next section). This is very convenient as we are able to display these channels
    as gray-level images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value channel of the castle image will then look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The same image in the saturation channel will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_03_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the image with the hue channel is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/B05388_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These images are interpreted in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The hue/saturation/value color space has been introduced because this representation
    corresponds to the way humans tend to naturally organize colors. Indeed, humans
    prefer to describe colors with intuitive attributes such as tint, colorfulness,
    and brightness. These three attributes are the basis of most phenomenal color
    spaces. **Hue** designates the dominant color; the names that we give to colors
    (such as green, yellow, blue, and red) correspond to the different hue values.
    **Saturation** tells us how vivid the color is; pastel colors have low saturation,
    while the colors of the rainbow are highly saturated. Finally, brightness is a
    subjective attribute that refers to the luminosity of a color. Other phenomenal
    color spaces use the concept of color **value** or color **lightness** as a way
    to characterize the relative color intensity.
  prefs: []
  type: TYPE_NORMAL
- en: 'These color components try to mimic the intuitive human perception of colors.
    In consequence, there is no standard definition for them. In the literature, you
    will find several different definitions and formulae of the hue, saturation, and
    brightness. OpenCV proposes two implementations of phenomenal color spaces: the
    HSV and the HLS color spaces. The conversion formulas are slightly different,
    but they give very similar results.'
  prefs: []
  type: TYPE_NORMAL
- en: The value component is probably the easiest to interpret. In the OpenCV implementation
    of the HSV space, it is defined as the maximum value of the three BGR components.
    It is a very simplistic implementation of the brightness concept. For a definition
    of brightness that matches the human visual system better, you should use the
    L channel of the perceptually uniform L*a*b* and L*u*v* color spaces. For example,
    the L channel takes into account the fact that a green color appears to human
    brighter than, for instance, a blue color of same intensity.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the saturation, OpenCV uses a formula based on the minimum and maximum
    values of the BGR components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B05388_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The idea is that a grayscale color in which the three R, G, and B components
    are all equal will correspond to a perfectly desaturated color; therefore, it
    will have a saturation value of `0`. Saturation is a value between `0` and `1.0`.
    For 8-bit images, saturation is rescaled to a value between `0` and `255`, and
    when displayed as a gray-level image, brighter areas correspond to the colors
    that have a higher saturation color.
  prefs: []
  type: TYPE_NORMAL
- en: For example, from the saturation image in the previous section, it can be seen
    that the blue of the water is more saturated than the light blue pastel color
    of the sky, as expected. The different shades of gray have, by definition, a saturation
    value equal to zero (because, in this case, all three BGR components are equal).
    This can be observed on the different roofs of the castle, which are made of a
    dark gray stone. Finally, in the saturation image, you may have noticed some white
    spots located in areas that correspond to very dark regions of the original image.
    These are a consequence of the used definition for saturation. Indeed, because
    saturation measures only the relative difference between the maximum and minimum
    BGR values, a triplet such as `(1,0,0)` gives a perfect saturation of `1.0`, even
    if this color would be seen as black. Consequently, the saturation values measured
    in dark regions are unreliable and should not be considered.
  prefs: []
  type: TYPE_NORMAL
- en: The hue of a color is generally represented by an angle value between `0` and
    `360`, with the red color at `0` degrees. In the case of an 8-bit image, OpenCV
    divides this angle by two to fit within the 1-byte range. Therefore, each hue
    value corresponds to a given color tint independent of its brightness and saturation.
    For example, both the sky and the water have the same hue value, approximately
    `200` degrees (intensity, `100`), which corresponds to the blue shade; the green
    color of the trees in the background has a hue of around `90` degrees. It is important
    to note that hue is less reliable when evaluated for colors that have a very low
    saturation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The HSB color space is often represented by a cone, where each point inside
    corresponds to a particular color. The angular position corresponds to the hue
    of the color, the saturation is the distance from the central axis, and the brightness
    is given by the height. The tip of the cone corresponds to the black color for
    which the hue and saturation are undefined:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/image_03_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can also generate an artificial image that will illustrate the different
    hue/saturation combinations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The columns of the following screenshot show the different possible hues (from
    0 to 180), while the different lines illustrate the effect of saturation; the
    top part of the image shows fully saturated colors while the bottom part corresponds
    to unsaturated colors. A brightness value of `255` has been attributed to all
    the displayed colors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/image_03_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Interesting effects can be created by playing with the HSV values. Several
    color effects that can be created using photo editing software are accomplished
    from this color space. For example, you may decide to modify an image by assigning
    a constant brightness to all the pixels of an image without changing the hue and
    saturation. This can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This gives the following image, which now looks like a drawing.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/image_03_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The HSV color space can also be very convenient to use when you want to look
    for objects of specific colors.
  prefs: []
  type: TYPE_NORMAL
- en: Using colors for detection - skin tone detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Color information can be very useful for the initial detection of specific objects.
    For example, the detection of road signs in a driver-assistance application could
    rely on the colors of standard signs in order to quickly identify potential road
    sign candidates. The detection of skin color is another example in which the detected
    skin regions could be used as an indicator of the presence of a human in an image;
    this approach is very often used in gesture recognition where skin tone detection
    is used to detect hand positions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, to detect an object using color, you first need to collect a large
    database of image samples that contain the object captured from different viewing
    conditions. These will be used to define the parameters of your classifier. You
    also need to select the color representation that you will use for classification.
    For skin tone detection, many studies have shown that skin color from the diverse
    ethnical groups clusters well in the hue/saturation space. For this reason, we
    will simply use the hue and saturation values to identify the skin tones in the
    following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using colors for detection - skin tone detection](img/image_03_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have defined a function that classifies the pixels of an image as skin or
    non-skin simply based on an interval of values (the minimum and maximum hue, and
    the minimum and maximum saturation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Having a large set of skin (and non-skin) samples at our disposal, we could
    have used a probabilistic approach in which the likelihood of observing a given
    color in the skin class versus that of observing the same color in the non-skin
    class would have been estimated. Here, we empirically define an acceptable hue/saturation
    interval for our test image (remember that the 8-bit version of the hue goes from
    `0` to `180` and saturation goes from `0` to `255`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following detection image is obtained as the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using colors for detection - skin tone detection](img/image_03_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that, for simplicity, we have not considered color brightness in the detection.
    In practice, excluding brighter colors would have reduced the possibility of wrongly
    detecting a bright reddish colors as skin. Obviously, a reliable and accurate
    detection of skin color would require a much more elaborate analysis. It is also
    very difficult to guarantee good detection across different images because many
    factors influence color rendering in photography, such as white balancing and
    lighting conditions. Nevertheless, as shown here, using hue/saturation information
    as an initial detector gives us acceptable results.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Chapter 5](ch05.html "Chapter 5. Transforming Images with Morphological Operations"),
    *Transforming Images with Morphological Operations*, shows you how to post-process
    binary images obtained from detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article, *A survey of skin-color modeling and detection methods, Pattern
    Recognition, vol. 40, 2007, P. Kakumanu, S. Makrogiannis, N. Bourbakis*, reviews
    different methods of skin detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
