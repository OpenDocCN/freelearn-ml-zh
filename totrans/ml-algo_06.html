<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes</h1>
                </header>
            
            <article>
                
<p>Naive Bayes are a family of powerful and easy-to-train classifiers that determine the probability of an outcome given a set of conditions using Bayes' theorem. In other words, the conditional probabilities are inverted, so that the query can be expressed as a function of measurable quantities. The approach is simple, and the adjective "naive" has been attributed not because these algorithms are limited or less efficient, but because of a fundamental assumption about the causal factors that we're going to discuss. Naive Bayes are multi-purpose classifiers and it's easy to find their application in many different contexts; however, their performance is particularly good in all those situations where the probability of a class is determined by the probabilities of some causal factors. A good example is natural language processing, where a piece of text can be considered as a particular instance of a dictionary and the relative frequencies of all terms provide enough information to infer a belonging class. We're going to discuss these concepts in later chapters. In this one, our examples will be always generic to let the reader understand how to apply naive Bayes in various contexts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayes' theorem</h1>
                </header>
            
            <article>
                
<p>Let's consider two probabilistic events A and B. We can correlate the marginal probabilities <em>P(A)</em> and <em>P(B)</em> with the conditional probabilities <em>P(A|B)</em> and <em>P(B|A)</em> using the product rule:</p>
<div class="CDPAlignCenter CDPAlign"><img height="44" width="165" src="assets/5dc0d26e-1f50-4e36-810e-86d776378850.png"/></div>
<p class="CDPAlignLeft CDPAlign">Considering that the intersection is commutative, the first members are equal; so we can derive <strong>Bayes' theorem</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="41" width="132" src="assets/34364511-0ab8-4a58-b0ce-437b5d6a0c05.png"/></div>
<p class="CDPAlignLeft CDPAlign">This formula has very deep philosophical implications and it's a fundamental element of statistical learning. First of all, let's consider the marginal probability <em>P(A)</em>; this is normally a value that determines how probable a target event is, such as <em>P(Spam)</em> or <em>P(Rain)</em>. As there are no other elements, this kind of probability is called <strong>Apriori</strong>, because it's often determined by mathematical considerations or simply by a frequency count. For example, imagine we want to implement a very simple spam filter and we've collected 100 emails. We know that 30 are spam and 70 are regular. So we can say that <em>P(Spam)</em> = 0.3.</p>
<p class="CDPAlignLeft CDPAlign">However, we'd like to evaluate using some criteria (for simplicity, let's consider a single one), for example, email text is shorter than 50 characters<em>. </em>Therefore, our query becomes:</p>
<div class="CDPAlignCenter CDPAlign"><img height="34" width="333" src="assets/92428044-51de-4f08-be62-88a4c4c67b25.png"/></div>
<p class="CDPAlignLeft CDPAlign">The first term is similar to <em>P(Spam)</em> because it's the probability of spam given a certain condition. For this reason, it's called <strong>a posteriori</strong> (in other words, it's a probability that we can estimate after knowing some additional elements). On the right-hand side, we need to calculate the missing values, but it's simple. Let's suppose that 35 emails have text shorter than 50 characters, so <em>P(Text &lt; 50 chars)</em> <em>= 0.35</em>. Looking only into our spam folder, we discover that only 25 spam emails have short text, so that <em>P(Text &lt; 50 chars|Spam) = 25/30 = 0.83</em>. The result is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="45" width="254" src="assets/d2e0edfe-4670-4c3d-a1e8-d583a1127ae8.png"/></div>
<p class="CDPAlignLeft CDPAlign">So, after receiving a very short email, there is a 71% probability that it's spam. Now, we can understand the role of <span><em>P(Text &lt; 50 chars|Spam)</em>; as we have actual data, we can measure how probable is our hypothesis given the query. In other words, we have defined a likelihood (compare this with logistic regression), which is a weight between the Apriori probability and the a posteriori one (the term in the denominator is less important because it works as a normalizing factor):</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="35" width="229" src="assets/84a1e8b9-ae4e-4091-8ba2-a8de4241f2f6.png"/></div>
<p class="CDPAlignLeft CDPAlign">The normalization factor is often represented by the Greek letter alpha, so the formula becomes:</p>
<div class="CDPAlignCenter CDPAlign"><img height="35" width="150" src="assets/af320552-6e08-4cf4-b2ff-230fd4d455b5.png"/></div>
<p class="CDPAlignLeft CDPAlign">The last step is considering the case when there are more concurrent conditions (this is more realistic in real-life problems):</p>
<div class="CDPAlignCenter CDPAlign"><img height="31" width="148" src="assets/1cd46b0a-20c6-4eb7-952c-de350e0b0d30.png"/></div>
<p class="CDPAlignLeft CDPAlign">A common assumption is called <strong>conditional independence</strong> (in other words, the effects produced by every cause are independent of each other) and this allows us to write a simplified expression:</p>
<div class="CDPAlignCenter CDPAlign"><img height="32" width="356" src="assets/422c5d68-3418-419b-a452-b67d76c1d3be.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes classifiers</h1>
                </header>
            
            <article>
                
<p>A naive Bayes classifier is called so because it's based on a naive condition, which implies the conditional independence of causes. This can seem very difficult to accept in many contexts where the probability of a particular feature is strictly correlated to another one. For example, in spam filtering, a text shorter than 50 characters can increase the probability of the presence of an image, or if the domain has been already blacklisted for sending the same spam emails to million users, it's likely to find particular keywords. In other words, the presence of a cause isn't normally independent from the presence of other ones. However, in Zhang H., <em>The Optimality of Naive Bayes</em>, AAAI 1, no. 2 (2004): 3, the author showed that under particular conditions (not so rare to happen), different dependencies clears one another, and a naive Bayes classifier succeeds in achieving very high performances even if its naiveness is violated.</p>
<p class="CDPAlignLeft CDPAlign">Let's consider a dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img height="46" width="257" src="assets/6f771e2c-796e-46bc-8850-19489ca0cbfb.png"/></div>
<p class="CDPAlignLeft CDPAlign">Every feature vector, for simplicity, will be represented as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="32" width="133" src="assets/e2d6f8c8-9514-4d42-9cab-37316828c236.png"/></div>
<p class="CDPAlignLeft CDPAlign">We need also a target dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img height="29" width="244" src="assets/0c5cb9f4-8da1-4b55-8e4a-a48ec24e38ee.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here, each <em>y</em> can belong to one of <em>P</em> different classes. Considering Bayes' theorem under conditional independence, we can write:</p>
<div class="CDPAlignCenter CDPAlign"><img height="41" width="226" src="assets/4a9e8a86-54c2-4daa-a963-14fdbf7869cf.png"/></div>
<p class="CDPAlignLeft CDPAlign">The values of the marginal Apriori probability <em>P(y)</em> and of the conditional probabilities <em>P(x<sub>i</sub>|y)</em> is obtained through a frequency count; therefore, given an input vector <em>x</em>, the predicted class is the one for which the a posteriori probability is maximum.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes in scikit-learn</h1>
                </header>
            
            <article>
                
<p>scikit-learn implements three naive Bayes variants based on the same number of different probabilistic distributions: Bernoulli, multinomial, and Gaussian. The first one is a binary distribution, useful when a feature can be present or absent. The second one is a discrete distribution and is used whenever a feature must be represented by a whole number (for example, in natural language processing, it can be the frequency of a term), while the third is a continuous distribution characterized by its mean and variance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bernoulli naive Bayes</h1>
                </header>
            
            <article>
                
<p>If <kbd>X</kbd> is random variable and is Bernoulli-distributed, it can assume only two values (for simplicity, let's call them 0 and 1) and their probability is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="66" width="185" src="assets/846f4f5d-fcc5-4a35-9343-9f31856c4cfb.png"/></div>
<p class="CDPAlignLeft CDPAlign">To try this algorithm with scikit-learn, we're going to generate a dummy dataset. Bernoulli naive Bayes expects binary feature vectors; however, the class <kbd>BernoulliNB</kbd> has a <kbd>binarize</kbd> parameter, which allows us to specify a threshold that will be used internally to transform the features:</p>
<pre><strong>from sklearn.datasets import make_classification<br/><br/>&gt;&gt;&gt; nb_samples = 300<br/>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_features=2, n_informative=2, n_redundant=0)</strong></pre>
<p class="CDPAlignLeft CDPAlign">We have generated the bidimensional dataset shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="409" width="507" class="image-border" src="assets/16dea9d7-5994-4309-9a14-e8b63c13ea42.png"/></div>
<p class="mce-root">We have decided to use 0.0 as a binary threshold, so each point can be characterized by the quadrant where it's located. Of course, this is a rational choice for our dataset, but Bernoulli naive Bayes is envisaged for binary feature vectors or continuous values, which can be precisely split with a predefined threshold:</p>
<pre><strong>from sklearn.naive_bayes import BernoulliNB<br/>from sklearn.model_selection import train_test_split<br/><br/>&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)<br/><br/>&gt;&gt;&gt; bnb = BernoulliNB(binarize=0.0)<br/>&gt;&gt;&gt; bnb.fit(X_train, Y_train)<br/>&gt;&gt;&gt; bnb.score(X_test, Y_test)<br/>0.85333333333333339</strong></pre>
<p class="CDPAlignLeft CDPAlign">The score is rather good, but if we want to understand how the binary classifier worked, it's useful to see how the data has been internally binarized:</p>
<div class="CDPAlignCenter CDPAlign"><img height="327" width="427" class="image-border" src="assets/012d8f2b-dcd6-47c6-ae6d-b28da747bb40.png"/></div>
<p class="mce-root">Now, checking the naive Bayes predictions, we obtain:</p>
<pre><strong>&gt;&gt;&gt; data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])<br/>&gt;&gt;&gt; bnb.predict(data)<br/>array([0, 0, 1, 1])</strong></pre>
<p class="CDPAlignLeft CDPAlign">This is exactly what we expected.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multinomial naive Bayes</h1>
                </header>
            
            <article>
                
<p>A multinomial distribution is useful to model feature vectors where each value represents, for example, the number of occurrences of a term or its relative frequency. If the feature vectors have <em>n</em> elements and each of them can assume <em>k</em> different values with probability <em>p<sub>k</sub></em>, then:</p>
<div class="CDPAlignCenter CDPAlign"><img height="50" width="310" src="assets/a56c6bb0-c27d-48fb-877d-5c172b42624c.png"/></div>
<p class="CDPAlignLeft CDPAlign">The conditional probabilities <em>P(x<sub>i</sub>|y)</em> are computed with a frequency count (which corresponds to applying a maximum likelihood approach), but in this case, it's important to consider the <strong>alpha parameter</strong> (called <strong>Laplace smoothing factor</strong>). Its default value is 1.0 and it prevents the model from setting null probabilities when the frequency is zero. It's possible to assign all non-negative values; however, larger values will assign higher probabilities to the missing features and this choice could alter the stability of the model. In our example, we're going to consider the default value of 1.0.</p>
<p class="CDPAlignLeft CDPAlign">For our purposes, we're going to use <kbd>DictVectorizer</kbd>, already analyzed in <a href="c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml">Chapter 2 - Important Elements in Machine Learning</a>. There are automatic instruments to compute the frequencies of terms, but we're going to discuss them later. Let's consider only two records: the first one representing a city, and the second one, the countryside. Our dictionary contains hypothetical frequencies, as if the terms were extracted from a text description:</p>
<pre><strong>from sklearn.feature_extraction import DictVectorizer<br/><br/>&gt;&gt;&gt; data = [<br/>   {'house': 100, 'street': 50, 'shop': 25, 'car': 100, 'tree': 20},<br/>   {'house': 5, 'street': 5, 'shop': 0, 'car': 10, 'tree': 500, 'river': 1}<br/>]<br/><br/>&gt;&gt;&gt; dv = DictVectorizer(sparse=False)<br/>&gt;&gt;&gt; X = dv.fit_transform(data)<br/>&gt;&gt;&gt; Y = np.array([1, 0])<br/><br/>&gt;&gt;&gt; X<br/>array([[ 100.,  100.,    0.,   25.,   50.,   20.],<br/>       [  10.,    5.,    1.,    0.,    5.,  500.]])</strong></pre>
<p class="CDPAlignLeft CDPAlign">Note that the term <kbd>'river'</kbd> is missing from the first set, so it's useful to keep alpha equal to 1.0 to give it a small probability. The output classes are 1 for city and 0 for the countryside. Now we can train a <kbd>MultinomialNB</kbd> instance:</p>
<pre><strong>from sklearn.naive_bayes import MultinomialNB<br/><br/>&gt;&gt;&gt; mnb = MultinomialNB()<br/>&gt;&gt;&gt; mnb.fit(X, Y)<br/>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</strong></pre>
<p class="CDPAlignLeft CDPAlign">To test the model, we create a dummy city with a river and a dummy countryside place without any river:</p>
<pre><strong>&gt;&gt;&gt; test_data = data = [<br/>   {'house': 80, 'street': 20, 'shop': 15, 'car': 70, 'tree': 10, 'river': 1},<br/>   {'house': 10, 'street': 5, 'shop': 1, 'car': 8, 'tree': 300, 'river': 0}<br/>]<br/><br/>&gt;&gt;&gt; mnb.predict(dv.fit_transform(test_data))<br/>array([1, 0])</strong></pre>
<p class="CDPAlignLeft CDPAlign">As expected, the prediction is correct. Later on, when discussing some elements of natural language processing, we're going to use multinomial naive Bayes for text classification with larger corpora. Even if a multinomial distribution is based on the number of occurrences, it can be used <span>successfully</span><span> </span><span>with frequencies or more complex functions.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gaussian naive Bayes</h1>
                </header>
            
            <article>
                
<p>Gaussian naive Bayes is useful when working with continuous values whose probabilities can be modeled using a Gaussian distribution:</p>
<div class="CDPAlignCenter CDPAlign"><img height="46" width="140" src="assets/41e8549a-4c7b-4a3d-940b-db47c3a446a1.png"/></div>
<p class="CDPAlignLeft CDPAlign">The conditional probabilities <em>P(x<sub>i</sub>|y)</em> are also Gaussian distributed; therefore, it's necessary to estimate the mean and variance of each of them using the maximum likelihood <span>approach. This quite easy; in fact, considering the property of a Gaussian, we get:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="47" width="318" src="assets/b9a2a040-43aa-4710-98ee-3954fa808d69.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here, the <em>k</em> index refers to the samples in our dataset and <em>P(x<sub>i</sub>|y)</em> <span> is a Gaussian itself</span>. By minimizing the inverse of this expression (in Russel S., Norvig P., <em>Artificial Intelligence: A Modern Approach</em>, Pearson, there's a complete analytical explanation), we get the mean and variance for each Gaussian associated with <em>P(x<sub>i</sub>|y)</em><span>, and the model is hence trained.</span></p>
<p class="CDPAlignLeft CDPAlign">As an example, we compare Gaussian naive Bayes with logistic regression using the ROC curves. The dataset has 300 samples with two features. Each sample belongs to a single class:</p>
<pre><strong>from sklearn.datasets import make_classification<br/><br/>&gt;&gt;&gt; nb_samples = 300<br/>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_features=2, n_informative=2, n_redundant=0)</strong></pre>
<p class="CDPAlignLeft CDPAlign">A plot of the dataset is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="402" width="499" class="image-border" src="assets/71c833f1-52e5-4534-99b2-fd01f9f2fdbb.png"/></div>
<p class="mce-root">Now we can train both models and generate the ROC curves (the <kbd>Y</kbd> scores for naive Bayes are obtained through the <kbd>predict_proba</kbd> method):</p>
<pre><strong>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import roc_curve, auc<br/>from sklearn.model_selection import train_test_split<br/><br/>&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)<br/><br/>&gt;&gt;&gt; gnb = GaussianNB()<br/>&gt;&gt;&gt; gnb.fit(X_train, Y_train)<br/>&gt;&gt;&gt; Y_gnb_score = gnb.predict_proba(X_test)<br/><br/>&gt;&gt;&gt; lr = LogisticRegression()<br/>&gt;&gt;&gt; lr.fit(X_train, Y_train)<br/>&gt;&gt;&gt; Y_lr_score = lr.decision_function(X_test)<br/><br/>&gt;&gt;&gt; fpr_gnb, tpr_gnb, thresholds_gnb = roc_curve(Y_test, Y_gnb_score[:, 1])<br/>&gt;&gt;&gt; fpr_lr, tpr_lr, thresholds_lr = roc_curve(Y_test, Y_lr_score)</strong></pre>
<p class="CDPAlignLeft CDPAlign">The resulting ROC curves (generated in the same way shown in the previous chapter) are shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="465" width="456" class="image-border" src="assets/156d6f86-7911-4da7-8591-0909c279f5f1.png"/></div>
<p class="CDPAlignLeft CDPAlign">Naive Bayes' performance is slightly better than logistic regression; however, the two classifiers have similar accuracy and <strong>Area Under the Curve</strong> (<strong>AUC</strong>). It's interesting to compare the performances of Gaussian and multinomial naive Bayes with the MNIST digit dataset. Each sample (belonging to 10 classes) is an 8 x 8 image encoded as an unsigned integer (0-255); therefore, even if each feature doesn't represent an actual count, it can be considered as a sort of magnitude or frequency:</p>
<pre><strong>from sklearn.datasets import load_digits<br/>from sklearn.model_selection import cross_val_score<br/><br/>&gt;&gt;&gt; digits = load_digits()<br/><br/>&gt;&gt;&gt; gnb = GaussianNB()<br/>&gt;&gt;&gt; mnb = MultinomialNB()<br/><br/>&gt;&gt;&gt; cross_val_score(gnb, digits.data, digits.target, scoring='accuracy', cv=10).mean()<br/>0.81035375835678214<br/><br/>&gt;&gt;&gt; cross_val_score(mnb, digits.data, digits.target, scoring='accuracy', cv=10).mean()<br/>0.88193962163008377<br/><br/></strong></pre>
<p class="CDPAlignLeft CDPAlign">Multinomial naive Bayes performs better than the Gaussian variant and the result is not really surprising. In fact, each sample can be thought of as a feature vector derived from a dictionary of 64 symbols. The value can be the count of each occurrence, so a multinomial distribution can better fit the data, while a Gaussian is slightly more limited by its mean and variance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Russel S., Norvig P., <em>Artificial Intelligence: A Modern Approach</em>, Pearson</li>
<li>Zhang H., <em>The Optimality of Naive Bayes, AAAI 1</em>, no. 2 (2004): 3</li>
<li>Papoulis A., <em>Probability, Random Variables and Stochastic Processes</em>, McGraw-Hill</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we exposed the generic naive Bayes approach, starting from the Bayes' theorem and its intrinsic philosophy. The naiveness of such algorithms is due to the choice to assume all the causes to be conditional independent. This means that each contribution is the same in every combination and the presence of a specific cause cannot alter the probability of the other ones. This is not so often realistic; however, under some assumptions, it's possible to show that internal dependencies clear each other so that the resulting probability appears unaffected by their relations.</p>
<p>scikit-learn provides three naive Bayes implementations: Bernoulli, multinomial and Gaussian. The only difference between them is in the probability distribution adopted. The first one is a binary algorithm, particularly useful when a feature can be present or not. Multinomial assumes having feature vectors, where each element represents the number of times it appears (or, very often, its frequency). This technique is very efficient in natural language processing or whenever the samples are composed starting from a common dictionary. Gaussian, instead, is based on a continuous distribution and it's suitable for more generic classification tasks.</p>
<p>In the next chapter, we're going to introduce a new classification technique called <strong>support vector machines</strong>. These algorithms are very powerful for solving both linear and non-linear problems. They're often the first choice for more complex scenarios because, despite their efficiency, the internal dynamics are very simple and they can be trained in a very short time.</p>


            </article>

            
        </section>
    </body></html>