<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Color Blending – Self-Organizing Maps and Elastic Neural Networks</h1>
                </header>
            
            <article>
                
<p><strong>Self-Organizing Maps</strong> (<strong>SOM</strong>), or <strong>Kohonen maps</strong> as you may have heard, are one of the basic types of self-organizing neural networks. The ability to self-organize provides adaptation to formerly unseen input data. It has been theorized as one of the most natural ways of learning, like that which is used by our brains, where no predefined patterns are thought to exist. Those patterns take shape during the learning process and are incredibly gifted at representing multidimensional data at a much lower level of dimensionality, such as 2D or 1D. Additionally, this network stores information in such a way that any topological relationships within the training set remain preserved.</p>
<p>More formally, an SOM is a clustering technique that will help us uncover interesting data categories in large datasets. It's a type of unsupervised neural network where neurons are arranged in a single, two-dimensional grid. The grid must be rectangular, as in, a pure rectangle or a hexagon. Throughout the iterations (which we will specify), the neurons in our grid will gradually coalesce around areas with a higher density of data points (the left-hand side of our display called <span class="packt_screen">Points</span>). As the neurons move, they bend and twist the grid until they move more closely to the points of interest and reflect the shape of that data.</p>
<p>In this chapter we will cover the following topics:</p>
<ul>
<li>Kohonen SOM</li>
<li>Working with AForge.NET</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Under the hood of an SOM</h1>
                </header>
            
            <article>
                
<p>So, the inevitable question now arises: how do these things work?</p>
<p class="mce-root">In a nutshell, we have neurons on the grid; gradually, via iterations, they adapt themselves to the shape of our data (in our example, shown in the following image on the left-hand side in the <span class="packt_screen">Points</span> panel). Let's talk a bit more about the iterative process itself.</p>
<ol>
<li>The first step is to randomly position data on the grid. We will randomly be placing our grid's neurons in our data space, as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-695 image-border" src="assets/d34f3e4a-3095-4a5e-8b75-ddd12ac87458.png" style=""/></div>
<ol start="2">
<li>The second step is where our algorithm will select a single data point.</li>
<li>In the third step, we need to find the neuron (data point) that is closest to the chosen data point. This then becomes our best matching unit.</li>
</ol>
<p> </p>
<ol start="4">
<li>The fourth step is to move our best-matching unit towards that data point. The distance that we move is determined by our learning rate, which <span>will</span><span> </span><span>ultimately decrease after each iteration.</span></li>
<li>Fifth, we will move the neighbors of our best-matching unit closer to it, with the farther-away neurons moving less than those that are closer. The <span class="packt_screen">Initial radius</span> variable you see on the screen is what we use to identify neighbors. This value, just like the <span class="packt_screen">Initial learning rate</span>, will decrease over time. If you have <strong>ReflectInsight</strong> (<strong>RI</strong>) up and running, you can watch the <span class="packt_screen">Initial learning rate</span> decrease over time, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-694 image-border" src="assets/bf11010d-c854-488e-8b8b-13aaf8fc1057.png" style=""/></div>
<ol start="6">
<li>Our sixth and final step will be to update the <span class="packt_screen">Initial learning rate</span> and <span class="packt_screen">Initial radius</span>, as we have described so far, and then repeat. We will continue this process until our data points have stabilized and are in the correct position.</li>
</ol>
<p>Now that we've introduced you to a little bit of intuition on SOMs, let's talk a little more about what we're going to do in this chapter. We have chosen a very common mechanism for teaching our principals, which is the mapping of colors. The colors themselves are 3D objects represented by red, green, and blue, but we will be organizing them into two dimensions. There are two key points you will see here with the organization of colors. First, the colors are clustered into distinct regions, and second, regions of similar properties are usually found adjacent to each other.</p>
<p>Our second example, which is a little bit more advanced, will use an <strong>artificial neural network</strong> (<strong>ANN</strong>) as we described before; this is an advanced form of machine learning, used to create an organizational mapping that matches the one presented to it. Let's look at our first example.</p>
<p>Here's a screenshot of our example. As you can see, we have a random pattern of colors, which, when finished, will be organized by clusters of similar colors:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dcfdf5aa-87f9-4adf-bdff-75186da6c1b7.png" style=""/></div>
<p>If we are successful<span>—</span>and we will be—here's what our result should look like:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f8663f07-1bd1-41f1-97be-10f5583fc31f.png" style=""/></div>
<p>Let us begin by following the steps of the process as follows:</p>
<ol>
<li>We will start out by using <span class="packt_screen">500</span> iterations to achieve our goal. Using a <span>smaller</span><span> </span><span>number may not produce the blend that we are ultimately after. As an example, if we have used <span class="packt_screen">500</span> iterations, here is what our result would look like:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4d88fb40-b1fd-4d5d-b283-0d845b742520.png" style=""/></div>
<ol start="2">
<li>As you can see, we are far from being where we need to be. Being able to change the iterations allows you to experiment with exactly the right setting. I can tell you that <span class="packt_screen">500</span> is higher than we need, so I will leave it as an exercise for you to figure out the number where the progression stops and you are satisfied with the organization.</li>
<li>After setting the number of iterations, all we must do is make sure we have the random color pattern we want, which can be achieved by clicking on the <span class="packt_screen">Randomize</span> button.</li>
<li>Once you have the pattern that you want, you simply click on the <span class="packt_screen">Start</span> button and watch the results.</li>
<li>Once you click on <span class="packt_screen">Start</span>, the <span class="packt_screen">Stop</span> button will be activated, and you can stop the progression anytime you like. The organization will automatically stop once you reach the number of iterations that you specified.</li>
</ol>
<p>Before we get into the actual code, let me show you some screenshots of what some organizational patterns look like. You can accomplish wonderful results by simply changing different parameters, which we will describe in detail further on. In the following screenshot, we have set the number of <span class="packt_screen">Iterations</span> to <span class="packt_screen">3000</span> and the <span class="packt_screen">Initial radius</span> as <span class="packt_screen">10</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-697 image-border" src="assets/a02cae80-2113-447b-810b-a16822a87476.png" style=""/></div>
<p><span>In the following screenshot we are using <span class="packt_screen">4000</span> iterations and an <span class="packt_screen">Initial radius</span> of <span class="packt_screen">18</span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-699 image-border" src="assets/b84882b8-9de6-4959-b247-3f80a2eebe18.png" style=""/></div>
<p><span>In the following screenshot we have set the number of <span class="packt_screen">Iterations</span> to <span class="packt_screen">4000</span> and the <span class="packt_screen">Initial radius</span> as <span class="packt_screen">5</span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-700 image-border" src="assets/997956c1-84ed-4704-8435-ee2c0682588e.png" style=""/></div>
<p>Here, we have set the number of <span class="packt_screen">Iterations</span> to <span class="packt_screen">5000</span>, the <span class="packt_screen">Initial learning rate</span> as <span class="packt_screen">0.3</span>, and the <span class="packt_screen">Initial radius</span> as <span class="packt_screen">25</span>, as shown in the following screenshot, to obtain the desired result:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/207b50b9-5ac8-4967-958a-3f3c3c91a21f.png" style=""/></div>
<p>As promised, let's now dive into the code.</p>
<p>In this example, we are going to work with <kbd>AForge</kbd> and use the <kbd>DistanceNetwork</kbd> object. A distance network is a neural network of only a single distance. As well as being used for an SOM, it is used for an ElasticNet operation, which is what we will be using to show the elastic connections between objects during progression.</p>
<p>We will create our distance network using three input neurons and <kbd>1000</kbd> neurons that will be doing the work under the hood:</p>
<pre>network = new DistanceNetwork(3, 100 * 100);</pre>
<p>When you click on the <span class="packt_screen">Randomize</span> button to randomize the colors, here's what happens under the hood:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">private void RandomizeNetwork()<br/>         {<br/>             if (network != null)<br/>             {<br/>                 foreach (var neuron in (network?.Layers.SelectMany(layer<br/>                   =&gt; layer?.Neurons)).Where(neuron =&gt; neuron != null))<br/>                     neuron.RandGenerator = <br/>                       new UniformContinuousDistribution<br/>                       (new Range(0, 255));<br/> <br/>                 network?.Randomize();<br/>             }<br/> <br/>             UpdateMap();<br/>         }</pre>
<p>You will notice that the randomization range we are dealing with stays within the range of any color's red, green, or blue characteristic, which is <kbd>255</kbd>.</p>
<p>Next, we will look at our learning loop, which looks like this. We'll do a deep dive into it in a moment:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">SOMLearning trainer = new SOMLearning(network);<br/>             double[] input = new double[3];<br/>             double fixedLearningRate = learningRate / 10;<br/>             double driftingLearningRate = fixedLearningRate * 9;<br/>             int i = 0;<br/> <br/>             while (!needToStop)<br/>             {<br/>                 trainer.LearningRate = driftingLearningRate <br/>                   * (iterations - i)<br/>                   / iterations + fixedLearningRate;<br/>                 trainer.LearningRadius = radius * (iterations - i) <br/>                   / iterations;<br/> <br/>                 if (rand != null)<br/>                 {<br/>                     input[0] = rand.Next(256);<br/>                     input[1] = rand.Next(256);<br/>                     input[2] = rand.Next(256);<br/>                 }<br/> <br/>                 trainer.Run(input);<br/> <br/>                 // update map once per 50 iterations<br/>                 if ((i % 10) == 9)<br/>                 {<br/>                     UpdateMap();<br/>                 }<br/> <br/>                 i++;<br/> <br/>                 SetText(currentIterationBox, i.ToString());<br/> <br/>                 if (i &gt;= iterations)<br/>                     break;<br/>             }</pre>
<p>If we look closer, the first object we create is an <kbd>SOMLearning</kbd> object. This object is optimized for square space learning, meaning it expects that the network it is working on has the same height as its width. This makes it easier to find the square root of the network's neuron counts:</p>
<pre>SOMLearning trainer = new SOMLearning(network);</pre>
<p>Next, we need to create variables to hold our red, green, and blue input colors, from which we will continually randomize the input colors in order to achieve our goals:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">                if (rand != null)<br/>                 {<br/>                     input[0] = rand.Next(256);<br/>                     input[1] = rand.Next(256);<br/>                     input[2] = rand.Next(256);<br/>                 }</pre>
<p>Once we enter our <kbd>while</kbd> loop, we will continually update our variables until we reach the total number of iterations we selected. In this update loop, there are several things happening. First, we will update the learning rate and learning radius, and store it in our <kbd>SOMLearning</kbd> object:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">trainer.LearningRate = driftingLearningRate * (iterations - i) /<br/>  iterations + fixedLearningRate;<br/>trainer.LearningRadius = radius * (iterations - i) / iterations;</pre>
<p>The learning rate determines our speed of learning. The learning radius, which can have a pretty dramatic affect on the visual output, determines the number of neurons to be updated relative to the distance from the winning neuron. The circle of the specified radius consists of neurons, and they are updated continually during the learning process. The closer a neuron is to the winning neuron, the more updating it will receive. Please note that if, during your experiments, you set this value to zero, then only the winning neurons' weights are updated and no others.</p>
<p>Although we will have a very pretty visual effect to watch, we'll still need to know what's going on within our application, and that's where RI comes in:</p>
<pre>RILogManager.Default.ViewerSendWatch("Learning Rate", $"{trainer.LearningRate}");<br/>RILogManager.Default.ViewerSendWatch("Learning Radius", $"{trainer.LearningRadius}");<br/>RILogManager.Default.ViewerSendWatch("Red", $"{RGBInput[0]}");<br/>RILogManager.Default.ViewerSendWatch("Green", $"{RGBInput[1]}");<br/>RILogManager.Default.ViewerSendWatch("Blue", $"{RGBInput[2]}");</pre>
<p>RI, as we mentioned earlier, has a watch panel that lets you continually track whatever variables you are interested in. In our case, we are interested in watching the learning rate, learning radius, and each RGB color that is randomized. All we need to do is supply the label and the value, and RI will do the rest, as we will see in a moment.</p>
<p>Finally, as they relate to RI, we want to see the RGB values in our message window as well, so we will add a debug message for that:</p>
<pre>RILogManager.Default.SendDebug($"Red {RGBInput[0]}, Green {RGBInput[1]}, Blue <br/>  {RGBInput[2]}");</pre>
<p>We now make a training <kbd>Run</kbd> for this iteration and pass to it the <kbd>RGBInput</kbd> array:</p>
<pre>trainer.Run(RGBInput);</pre>
<p>Let's talk about learning for a moment. As we mentioned, each iteration will try and learn more and more information. This learning iteration returns a learning error, which is the difference in the neurons' weights and the input vector <kbd>RGBInput</kbd>. As mentioned previously, the distance is measured according to the distance from the winning neuron. The process is as follows.</p>
<p>The trainer runs one learning iteration, finds the winning neuron (the neuron that has weights with values closest to those provided in the <kbd>RGBInput</kbd>), and updates its weights. It also updates the weights of the neighboring neurons. As each learning iteration occurs, the network gets closer and closer to the optimal solution.</p>
<p>Up next is a screenshot of our application running. In the background is RI, so that you can see how we are logging each iteration, what color values we are using when we update the map, as well as the learning rate and learning radius. As your machine learning programs and algorithms get more and more complex, you will realize that this kind of insight into your applications becomes incredibly invaluable. It is also an indispensable real-time debugging and diagnostic tool!</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c51dd71d-0559-4a4c-b362-f46020cdd2d3.png" style=""/></div>
<p>Since SOM are, well, self-organizing, our second example is going to be more graphical. Our hope is that it will help you to better understand what's happening behind the scenes.</p>
<p>In this example, we'll again use AForge.NET and build a 2D plane of objects, organized into a few groups. We will, starting from a single location, visually arrive at the location of those shapes. This is conceptually the same as our color example, which used points in a 3D space, except that this time, our points are in 2D. The visualization occurs in the <span class="packt_screen">Map</span> panel and is a top-down view of what is happening in 2D space, so as to get to a 1D graphical view.</p>
<p>Initially, neurons in the SOM grid start out at random positions, but they are gradually massaged into a molded outlining that is in the shape of our data. This is an iterative process, and although putting an animated <kbd>.gif</kbd> into the book is a feat that we have not yet achieved, I have taken screenshots at various points in the iteration to show you what happens. You can run the example for yourself to see it in real time.</p>
<p>We start out with all our objects in the positions on the left. We will run through <span class="packt_screen">500</span> iterations to show the evolution. We will go from a blank white panel to one that, hopefully, resembles the <span class="packt_screen">Points</span> panel:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-696 image-border" src="assets/06f51405-b689-445d-bf65-9daa6ffb480d.png" style=""/></div>
<p>Now we click on the <span class="packt_screen">Start</span> button and away it goes! You will see the points beginning to organize themselves by moving to their correct locations, which (hopefully) will mirror that of the <span class="packt_screen">Points</span> we have specified:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-701 image-border" src="assets/a4b44d9d-0dd3-4959-b32e-2f26a7efa8a5.png" style=""/></div>
<p>After <span class="packt_screen">199</span> iterations:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-702 image-border" src="assets/fd6dff9a-0d43-463b-9205-b174046a623d.png" style=""/></div>
<p>After <span class="packt_screen">343</span> iterations:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-703 image-border" src="assets/519621e9-f3fb-47ff-ada9-bdd13e615b6a.png" style=""/></div>
<p>And, after completion, you can see that the objects have organized themselves like the pattern that we initially created. If you imagine that you are looking down at the map, even though you are on a flat piece of paper, you can see the 3D experience if you look hard enough. The blue dots are the active neurons, the light gray dots are the inactive neurons, and the lines drawn are the elastic connections between neurons.</p>
<p>The checkboxes below the map allow you to easily choose whether to display either or both of these:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-704 image-border" src="assets/1591c8f9-5cdd-4465-9d96-6f41c3b7d6ed.png" style=""/></div>
<p>If you take a screenshot with the connections and inactive neurons not shown, you will see that the organizational patters in the map arrive at the same clustering as our objective, which for us means success:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-705 image-border" src="assets/25a9b838-fc2f-437a-b441-fd510fa7b254.png" style=""/></div>
<p>How exactly all this works is the next topic we will investigate. As always, let's take a look at our main execution loop. As you can see, we'll be using the same <kbd>DistanceNetwork</kbd> and <kbd>SOMLearning</kbd> objects that we previously discussed:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">DistanceNetwork network = new DistanceNetwork(2, networkSize <br/>  * networkSize);<br/> <br/>             // set random generators range<br/>             foreach (var neuron in network.Layers.SelectMany(layer =&gt;<br/>               layer.Neurons))<br/>                 neuron.RandGenerator = new UniformContinuousDistribution(<br/>                     new Range(0, Math.Max<br/>                       (pointsPanel.ClientRectangle.Width,<br/>                       pointsPanel.ClientRectangle.Height)));<br/> <br/>             // create learning algorithm<br/>             SOMLearning trainer = new SOMLearning(network, networkSize, <br/>               networkSize);<br/> <br/>             // create map<br/>             map = new int[networkSize, networkSize, 3];<br/> <br/>             double fixedLearningRate = learningRate / 10;<br/>             double driftingLearningRate = fixedLearningRate * 9;<br/> <br/>             // iterations<br/>             int i = 0;<br/> <br/>             // loop<br/>             while (!needToStop)<br/>             {<br/>                 trainer.LearningRate = driftingLearningRate <br/>                   * (iterations - i) / iterations + fixedLearningRate;<br/>                 trainer.LearningRadius = (double)learningRadius * <br/>                   (iterations - i) / iterations;<br/> <br/>                 // run training epoch<br/>                 trainer.RunEpoch(trainingSet);<br/> <br/>                 // update map<br/>                 UpdateMap(network);<br/> <br/>                 // increase current iteration<br/>                 i++;<br/> <br/>                 // set current iteration's info<br/>                 SetText(currentIterationBox, i.ToString());<br/> <br/>                 // stop ?<br/>                 if (i &gt;= iterations)<br/>                     break;<br/>             }</pre>
<p>As we mentioned earlier, the <kbd>LearningRate</kbd> and <kbd>LearningRadius</kbd> continue to evolve through every iteration. This time, let's talk a bit about the <kbd>RunEpoch</kbd> method of the trainer. This method, although very simplistic, is designed to take a vector of input values and then return a learning error for that iteration (as you can now see, also sometimes called an <strong>epoch</strong>). It does this by calculating against each one of the input samples in the vector. The learning error is the absolute difference between the neurons' weights and inputs. The difference is measured according to the distance from the winning neuron. As mentioned earlier, we run this calculation against one learning iteration/epoch, find the winner, and update its weights (as well as neighbor weights). I should point out that when I say <em>winner</em>, I mean the neuron that has weights with values closest to the specified input vector, that is, the minimum distance from the network's input.</p>
<p>Next, we will highlight how we update the <kbd>map</kbd> itself; our calculated projects should match the initial input vector (points):</p>
<pre class="mce-root CDPAlignLeft CDPAlign">            // get first layer<br/>             Layer layer = network.Layers[0];<br/> <br/>             // lock<br/>             Monitor.Enter(this);<br/> <br/>             // run through all neurons<br/>             for (int i = 0; i &lt; layer.Neurons.Length; i++)<br/>             {<br/>                 Neuron neuron = layer.Neurons[i];<br/> <br/>                 int x = i % networkSize;<br/>                 int y = i / networkSize;<br/> <br/>                 map[y, x, 0] = (int)neuron.Weights[0];<br/>                 map[y, x, 1] = (int)neuron.Weights[1];<br/>                 map[y, x, 2] = 0;<br/>             }<br/> <br/>             // collect active neurons<br/>             for (int i = 0; i &lt; pointsCount; i++)<br/>             {<br/>                 network.Compute(trainingSet[i]);<br/>                 int w = network.GetWinner();<br/> <br/>                 map[w / networkSize, w % networkSize, 2] = 1;<br/>             }<br/> <br/>             // unlock<br/>             Monitor.Exit(this);<br/> <br/>             //<br/>             mapPanel.Invalidate();</pre>
<p>As you can see from this code, we get the first layer, calculate <kbd>map</kbd> for all the neurons, collect the active neurons so that we can determine the winner, and then update <kbd>map</kbd>.</p>
<p>Since we have talked about the winner so much, let me show you just how much code is involved in calculating the winner:</p>
<pre>public int GetWinner()<br/>{<br/>// find the MIN value<br/>double <strong>min</strong> = output[0];<br/>int <strong>minIndex</strong> = 0;<br/>for (int <strong>i</strong> = 1; <strong>i</strong> &lt; output.Length; <strong>i</strong>++)<br/>{<br/>if (output[<strong>i</strong>] &lt; <strong>min</strong>)<br/>{<br/>// found new MIN value<br/><strong>min</strong> = output[<strong>i</strong>];<br/><strong>minIndex</strong> = <strong>i</strong>;<br/>}<br/>}<br/>return <strong>minIndex</strong>;<br/>}</pre>
<p>That's it! All we are doing is looking for the index of the neuron whose weights have the minimum distance from the network's input.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to harness the power of SOMs and elastic neural networks. You've now officially crossed up the ladder from machine learning into neural networks; congratulations!</p>
<p>In our next chapter, we will use some of our knowledge to start facial and motion detection programs and have some real fun! You are going to get to work with my associate for the chapter, Frenchie!</p>


            </article>

            
        </section>
    </body></html>