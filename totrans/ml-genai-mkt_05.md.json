["```py\nimport pandas as pd\ndf = pd.read_csv('Tweets.csv')\ndf.head(5) \n```", "```py\npd.set_option(\"max_colwidth\", None)\nexamples_idx = df.sample(5).index # [1106, 4860, 6977, 8884, 9108]\ndf_sample = df.loc[examples_idx] \n```", "```py\n!python -m space download en_core_web_sm\nimport re\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndef clean_text(text):\n    text = re.sub(r'@\\w+|#\\w+|https?://\\S+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text.lower()\ndf_sample['cleaned_text'] = df_sample['text'].apply(clean_text)\ndf_sample[[\"text\", \"cleaned_text\"]] \n```", "```py\ndef tokenize_and_remove_stopwords(row):\n    doc = nlp(row['cleaned_text'])\n    all_tokens = [token.text for token in doc]\n    tokens_without_stop = [token.text for token in doc if not token.is_stop]\n    processed_text = ' '.join(tokens_without_stop)\n    row['all_text_tokens'] = all_tokens\n    row['without_stop_words_tokens'] = tokens_without_stop\n    row['processed_text'] = processed_text\n    return row\ndf_sample = df_sample.apply(tokenize_and_remove_stopwords, axis=1)\ndf_sample[['cleaned_text', 'all_text_tokens', 'without_stop_words_tokens', 'processed_text']] \n```", "```py\ndef lemmatize_text(text):\n    doc = nlp(text)\n    lemmatized = [token.lemma_ for token in doc]\n    return ' '.join(lemmatized)\ndf_sample['final_text'] = df_sample['processed_text'].apply(lemmatize_text)\ndf_sample[['processed_text', 'final_text']] \n```", "```py\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nsentiment_by_airline = df.groupby(['airline', 'airline_sentiment']).size().unstack().fillna(0)\nplt.figure(figsize=(14, 6))\nsentiment_by_airline.plot(kind='bar', stacked=True, color=['red', 'yellow', 'green'])\nplt.title('Sentiment Distribution by Airline')\nplt.xlabel('Airline')\nplt.ylabel('Number of Tweets')\nplt.xticks(rotation=45)\nplt.legend(title='Sentiment')\nplt.tight_layout()\nplt.show() \n```", "```py\ndf['airline_sentiment'].value_counts() \n```", "```py\nfrom sklearn.utils import resample\nnegative = df[df.airline_sentiment == 'negative']\nneutral = df[df.airline_sentiment == 'neutral']\npositive = df[df.airline_sentiment == 'positive']\nnegative_downsampled = resample(negative, n_samples=len(positive))\ndf_downsampled = pd.concat([negative_downsampled, neutral, positive]) \n```", "```py\nfrom transformers import pipeline\ngenerator = pipeline('text-generation', model='distilgpt2')\ndef augment_text(text, augment_times=2):\n    augmented_texts = []\n    for _ in range(augment_times):\n        generated = generator(text, max_length=60, num_return_sequences=1)\n        new_text = generated[0]['generated_text'].strip()\n        augmented_texts.append(new_text)\n    return augmented_texts\nseed_text = \"Fantastic airline service on this flight. My favorite part of the flight was\"\naugmented_examples = augment_text(seed_text)\ndef remove_extra_spaces(text):\n    words = text.split()\n    return ' '.join(words)\nfor example in augmented_examples:\n    print(\"------\\n\", remove_extra_spaces(example)) \n```", "```py\naugmented_data = pd.DataFrame({\n    'text': augmented_examples,\n    'airline_sentiment': ['positive'] * len(augmented_examples)\n})\ndf_augmented = pd.concat([df, augmented_data], ignore_index=True) \n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndf['cleaned_text'] = df['text'].apply(clean_text)\ndf = df.apply(tokenize_and_remove_stopwords, axis=1)\ndf['processed_text'] = df['cleaned_text'].apply(tokenize_and_remove_stopwords)\ndf['final_text'] = df['processed_text'].apply(lemmatize_text)\ntfidf_vectorizer = TfidfVectorizer(max_features=1000)\nX = tfidf_vectorizer.fit_transform(df['final_text'])\ny = df['airline_sentiment'] \n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train) \n```", "```py\nfeature_names = tfidf_vectorizer.get_feature_names_out()\nclass_labels = model.classes_\nfor index, class_label in enumerate(class_labels):\n    coefficients = model.coef_[index]\n    coefficients_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Coefficient': coefficients\n    })\n    coefficients_df['Absolute_Coefficient'] = coefficients_df['Coefficient'].abs()\n    coefficients_df = coefficients_df.sort_values(by='Absolute_Coefficient', ascending=False)\n    print(f\"Class: {class_label}\")\n    print(coefficients_df[['Feature', 'Coefficient']].head(10)) \n```", "```py\nfrom sklearn.metrics import accuracy_score, classification_report\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred)) \n```", "```py\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\ncm = confusion_matrix(y_test, y_pred, labels=['negative', 'neutral', 'positive'])\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show() \n```", "```py\ngold_df = df[df['airline_sentiment_gold'].notnull()]\nX_gold = tfidf_vectorizer.transform(gold_df['final_text'])\ny_gold = gold_df['airline_sentiment_gold']\ny_gold_pred = model.predict(X_gold)\ngold_df['predicted_sentiment'] = y_gold_pred\nmisclassified = gold_df[gold_df['airline_sentiment_gold'] != gold_df['predicted_sentiment']]\nmisclassified[['airline_sentiment_gold', 'predicted_sentiment', 'text', 'final_text', 'negativereason_gold']] \n```", "```py\nfrom tqdm.auto import tqdm\nimport time\nfiltered_df = df[df['airline_sentiment'] != 'neutral']\nX = filtered_df['text']\ny = filtered_df['airline_sentiment']\nX_train_texts, X_test_texts, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nsentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\nstart_time = time.time()\nresults = []\nfor text in tqdm(X_test_texts, desc=\"Analyzing sentiments\"):\n    result = sentiment_pipeline(text)\n    results.append(result[0]['label'].lower())\nend_time = time.time()\ntotal_time = end_time - start_time\nprint(f\"Total time for analyzing {len(X_test_texts)} tweets: {total_time:.2f} seconds\") \n```", "```py\nprint(classification_report(y_test, results)) \n```", "```py\ncm = confusion_matrix(y_test, results, labels=['negative', 'positive'])\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'positive'], yticklabels=['negative', 'positive'])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show() \n```", "```py\n    !pip install tweepy\n    import tweepy\n    # Replace these with your API keys and tokens\n    consumer_key = 'YOUR_CONSUMER_KEY'\n    consumer_secret = 'YOUR_CONSUMER_SECRET'\n    access_token = 'YOUR_ACCESS_TOKEN'\n    access_token_secret = 'YOUR_ACCESS_TOKEN_SECRET'\n    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n    auth.set_access_token(access_token, access_token_secret)\n    api = tweepy.API(auth) \n    ```", "```py\n    query = \"@YourBrandHandle -filter:retweets\"\n    tweets = api.search_tweets(q=query, lang=\"en\", count=100) \n    ```", "```py\n    data = [{\n        'tweet_id': tweet.id,\n        'text': tweet.text,\n        'tweet_created': tweet.created_at,\n        'tweet_location': tweet.user.location,\n        } for tweet in tweets]\n    your_brand_df = pd.DataFrame(data) \n    ```", "```py\nnlp = spacy.load(\"en_core_web_sm\")\nreviews = [\n    \"I recently purchased a sleeping bag from Optimal Hiking Gear and it exceeded my expectations.\",\n    \"The tent I bought from Optimal Hiking was damaged on arrival. Very disappointed.\",\n    \"The Optimal Hiking company makes a backpack that's the best. I've been using mine for years without any issues.\"\n]\nfor review in reviews:\n    doc = nlp(review)\n    for ent in doc.ents:\n        print(f\"Entity: {ent.text}, Label: {ent.label_}\") \n```", "```py\nEntity: Optimal Hiking Gear, Label: ORG\nEntity: Optimal Hiking, Label: ORG\nEntity: Optimal Hiking, Label: ORG\nEntity: years, Label: DATE \n```", "```py\ndf.negativereason.value_counts() \n```", "```py\nfrom wordcloud import WordCloud\ntfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\ntfidf_matrix = tfidf_vectorizer.fit_transform(df['final_text'])\ntfidf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_matrix.sum(axis=0).tolist()[0]))\nwordcloud_tfidf = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(tfidf_scores)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud_tfidf, interpolation='bilinear')\nplt.axis('off')\nplt.show() \n```", "```py\nimport nltk\ndef extract_hashtags(text):\n    return re.findall(r\"#(\\w+)\", text)\nhashtags = sum(df['text'].apply(extract_hashtags).tolist(), [])\nhashtag_freq_dist = nltk.FreqDist(hashtags)\nwordcloud_hashtags = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(hashtag_freq_dist)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud_hashtags, interpolation='bilinear')\nplt.axis('off')\nplt.show() \n```", "```py\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\ndoc_term_matrix = count_vect.fit_transform(df['final_text'])\nLDA = LatentDirichletAllocation(n_components=5, random_state=42)\nLDA.fit(doc_term_matrix)\nfor i, topic in enumerate(LDA.components_):\n    print(f\"Top words for topic #{i}:\")\n    print([count_vect.get_feature_names_out()[index] for index in topic.argsort()[-10:]])\n    print(\"\\n\") \n```", "```py\nTop words for topic #0:\n['fly', 'follow', 'send', 'dm', 'flight', 'hour', 'sit', 'gate', 'seat', 'plane']\nTop words for topic #1:\n['tomorrow', 'change', 'amp', 'time', 'late', 'delay', 'fly', 'flightle', 'cancel', 'flight']\nTop words for topic #2:\n['response', 'bad', 'good', 'time', 'flight', 'bag', 'great', 'customer', 'service', 'thank']\nTop words for topic #3:\n['wait', 'need', 'help', 'problem', 'delay', 'luggage', 'hour', 'miss', 'bag', 'flight']\nTop words for topic #4:\n['service', 'number', 'customer', 'minute', 'email', 'hour', 'help', 'try', 'phone', 'hold'] \n```", "```py\ndf['tweet_created'] = pd.to_datetime(df['tweet_created']).dt.tz_convert(None)\ndf['date'] = df['tweet_created'].dt.date\nairline_handle = \"@JetBlue\"\nairline_tweets = df[df.text.str.contains(airline_handle)]\ngrouped = airline_tweets.groupby(['airline_sentiment', 'date']).agg({'tweet_id':'count', 'retweet_count':'sum'}).reset_index()\npositive_tweets = grouped[grouped['airline_sentiment'] == 'positive']\nneutral_tweets = grouped[grouped['airline_sentiment'] == 'neutral']\nnegative_tweets = grouped[grouped['airline_sentiment'] == 'negative']\nplt.figure(figsize=(14, 7))\nscale_factor = 3\nfor tweets, sentiment, color, linestyle in zip(\n    [positive_tweets, neutral_tweets, negative_tweets],\n    ['Positive', 'Neutral', 'Negative'],\n    ['green', 'orange', 'red'],\n    ['-', '--', '-.']\n):\n    scaled_retweet_count = tweets['retweet_count'] * scale_factor\n    plt.plot(tweets['date'], tweets['tweet_id'], linestyle=linestyle, label=sentiment, color=color)\n    plt.scatter(tweets['date'], tweets['tweet_id'], scaled_retweet_count, color=color)\nplt.title(f'Daily Sentiment Trend for {airline_handle} with Bubble Size Indicating Retweets')\nplt.xlabel('Date')\nplt.ylabel('Number of Tweets')\nplt.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show() \n```", "```py\ndates_of_interest = [pd.to_datetime('2015-02-22').date(), pd.to_datetime('2015-02-23').date(), pd.to_datetime('2015-02-24').date()]\nfiltered_df = airline_tweets[(airline_tweets['date'].isin(dates_of_interest)) & (airline_tweets['airline_sentiment'] == 'negative')]\ntop_tweets_per_date = filtered_df.groupby('date').apply(lambda x: x.nlargest(3, 'retweet_count'))\ntop_tweets_per_date[['text', 'retweet_count', 'negativereason']] \n```", "```py\n!pip install folium\nimport folium\nfrom folium.plugins import HeatMap\nfiltered_df = df[(df['text'].str.contains('@JetBlue') & (df['airline_sentiment'] == 'negative'))]\nfiltered_df = filtered_df.dropna(subset=['tweet_coord'])\nvalid_coords = []\nfor coord in filtered_df['tweet_coord']:\n    try:\n        lat, long = eval(coord)\n        valid_coords.append((lat, long))\n    except (TypeError, SyntaxError, NameError):\n        continue\nif valid_coords:\n    map_center = [sum(x)/len(valid_coords) for x in zip(*valid_coords)]\nelse:\n    map_center = [0, 0]\ntweet_map = folium.Map(location=map_center, zoom_start=4)\nHeatMap(valid_coords).add_to(tweet_map)\ntweet_map \n```"]