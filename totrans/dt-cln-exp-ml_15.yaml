- en: '*Chapter 11*: Decision Trees and Random Forest Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：决策树和随机森林分类'
- en: Decision trees and random forests are very popular classification models. This
    is partly because they are easy to train and interpret. They are also quite flexible.
    We can model complexity without necessarily having to increase the feature space
    or transform features. We do not even need to do anything special to apply the
    algorithm to multiclass problems, something we had to do with logistic regression.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树和随机森林是非常流行的分类模型。这部分的理由是它们易于训练和解释。它们也非常灵活。我们可以建模复杂性，而无需必然增加特征空间或转换特征。我们甚至不需要对多类问题应用算法做任何特殊处理，这是我们在逻辑回归中必须做的。
- en: On the other hand, decision trees can be less stable than other classification
    models, being fairly sensitive to small changes in the training data. Decision
    trees can also be biased when there is a significant class imbalance (when there
    are many more observations in one class than another). Fortunately, these issues
    can be addressed with techniques such as bagging to reduce variance and oversampling
    to deal with imbalance. We will examine these techniques in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，决策树可能不如其他分类模型稳定，对训练数据中的微小变化相当敏感。当存在显著的类别不平衡（一个类别的观测值比另一个类别多得多）时，决策树也可能存在偏差。幸运的是，这些问题可以通过诸如袋装法来减少方差和过采样来处理不平衡的技术来解决。我们将在本章中探讨这些技术。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Key concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键概念
- en: Decision tree models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树模型
- en: Implementing random forest
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现随机森林
- en: Implementing gradient boosting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现梯度提升
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In addition to the scikit-learn modules we have been using so far, we will use
    SMOTENC from Imbalanced-learn. We will use SMOTENC to address class imbalance.
    The Imbalanced-learn library can be installed with `pip install -U imbalanced-learn`.
    All the code in this chapter was tested with scikit-learn versions 0.24.2 and
    1.0.2.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们迄今为止一直在使用的scikit-learn模块之外，我们还将使用来自Imbalanced-learn的SMOTENC。我们将使用SMOTENC来解决类别不平衡问题。可以使用`pip
    install -U imbalanced-learn`命令安装Imbalanced-learn库。本章中的所有代码都使用scikit-learn版本0.24.2和1.0.2进行了测试。
- en: Key concepts
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键概念
- en: Decision trees are an exceptionally useful machine learning tool. They are non-parametric,
    easy to interpret, and can work with a wide range of data. No assumptions regarding
    linearity of relationships between features and targets, and normality of error
    terms, are made. It isn’t even necessary to scale the data. Decision trees also
    often do a good job of capturing complex relationships between predictors and
    targets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一个非常有用的机器学习工具。它们是非参数的，易于解释，并且可以处理各种类型的数据。不假设特征与目标之间关系的线性，也不假设误差项的正态性。甚至不需要对数据进行缩放。决策树还经常能够很好地捕捉预测变量和目标之间的复杂关系。
- en: The flexibility of the decision tree algorithm, and its ability to model complicated
    and unanticipated relationships in the data, is due to the **recursive partitioning**
    procedure that’s used to segment the data. Decision trees group observations based
    on the values of their features. This is done with a series of binary decisions,
    starting from an initial split at the root node, and ending with a leaf for each
    grouping. Each split is based on the feature, and feature values, that provide
    the most information about the target. More precisely, a split is chosen based
    on whether it produces the lowest Gini Impurity score. We will discuss Gini Impurity
    in more detail later.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法的灵活性和其建模数据中复杂和未预见的关联的能力，归功于用于分割数据的**递归分割**过程。决策树根据其特征值对观测值进行分组。这是通过一系列二进制决策来完成的，从根节点处的初始分割开始，以每个分组的叶子节点结束。每个分割都是基于提供关于目标最多信息的特征和特征值。更精确地说，分割的选择基于它是否产生最低的Gini不纯度得分。我们将在稍后更详细地讨论Gini不纯度。
- en: All new observations with the same values, or the same range of values, along
    the branches from the root node to the leaf, get the same predicted value for
    the target. When the target is categorical, that is the most frequent value for
    the target for the training observations at that leaf.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着从根节点到叶子的分支，具有相同值或相同值范围的全部新观测值，都会得到相同的预测目标值。当目标是分类时，这就是该叶子节点训练观测值中目标的最频繁值。
- en: 'The following diagram provides a fairly straightforward example of a decision
    tree, with made-up data and results for a model of college completion among those
    who attended college. For this decision tree, an initial split of high school
    GPA by those with a 3.0 or less and those with a GPA of greater than 3.0 was found
    to lead to the lowest impurity compared to other available features, as well as
    other thresholds. Therefore, high school GPA is our root node, also known as depth
    0:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表提供了一个相当直接的决策树示例，其中包含虚构的数据和大学完成情况模型的结果。对于这个决策树，通过高中学业成绩将那些成绩在3.0或以下和成绩高于3.0的人分开，发现与其他可用特征以及其他阈值相比，这种初始分割会导致最低的不纯度。因此，高中学业成绩是我们的根节点，也称为深度0：
- en: '![ Figure 11.1 – A decision tree for college completion ](img/B17978_11_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – 完成大学教育的决策树](img/B17978_11_001.jpg)'
- en: Figure 11.1 – A decision tree for college completion
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 完成大学教育的决策树
- en: The binary split at the root node results in 45% of the observations on the
    left-hand side of the tree and 55% on the right. At depth 1, there are binary
    splits on both sides based on parental income, though the threshold is different;
    $80k and $45k for the left and right-hand sides, respectively. For parental incomes
    above $80k, where the high school GPA is greater than 3, there is no more splitting.
    Here, we get to a prediction of **Graduated**. This is a leaf node.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点处的二分分割导致树左侧的观测值占45%，右侧的占55%。在深度1，两侧都有基于父母收入的二分分割，尽管阈值不同；左侧为$80k，右侧为$45k。对于高中学业成绩大于3且父母收入高于$80k的情况，没有更多的分割。在这里，我们得到了**毕业**的预测。这是一个叶子节点。
- en: We can navigate up the tree from each leaf to describe how the tree has segmented
    the data, just as we did for individuals with parental income above $80k and a
    high school GPA above 3\. For example, the decision tree predicts not graduating
    for individuals who receive low levels of support from their schools, have parental
    incomes greater than $45k, and have a high school GPA less than or equal to 3.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从每个叶子节点向上导航树，描述树是如何分割数据的，就像我们对父母收入高于$80k和高中学业成绩高于3.0的个人所做的那样。例如，决策树预测那些从学校获得低水平支持、父母收入超过$45k和高中学业成绩低于或等于3的个人不会毕业。
- en: So, how does the decision tree algorithm perform this magic? How does it select
    the features and the threshold amounts or class values? Why greater than $80k
    or $45k for parental income? Why received grants at depth 2 (split 3) for parental
    income less than or equal to $45k, but student support level for depth 2 for other
    leaves? One leaf does not even have a further split at depth 2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，决策树算法是如何施展这种魔法的？它是如何选择特征和阈值或类值的？为什么是父母收入大于$80k或$45k？为什么对于父母收入低于或等于$45k的情况，在深度2（分割3）处收到补助，而对于其他叶子节点的深度2则是学生支持水平？甚至有一个叶子节点在深度2都没有进一步的分割。
- en: 'One way of measuring the information about a class that a binary split provides
    is by how much it helps us distinguish between in-class and out-of-class membership.
    We frequently use Gini Impurity calculations for that evaluation, though entropy
    is sometimes used. The Gini Impurity statistic tells us how well class membership
    has been segmented at each node. This can be seen in the following formula:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一种衡量二分分割对类信息提供的信息的方法是它帮助我们区分类内和类外成员资格的程度。我们经常使用基尼不纯度计算来进行这种评估，尽管有时也使用熵。基尼不纯度统计量告诉我们每个节点上类成员资格分割得有多好。这可以在以下公式中看到：
- en: '![](img/B17978_11_0011.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_11_0011.jpg)'
- en: Here, ![](img/B17978_11_002.png) is the probability of membership in the *k*
    class and *m* is the number of classes. If class membership is equal at a node,
    then Gini Impurity is 0.5\. When completely pure, it is 0.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17978_11_002.png)是属于*k*类和*m*个类的概率，而*m*是类的数量。如果一个节点上的类成员资格相等，那么基尼不纯度为0.5。当完全纯净时，它为0。
- en: 'It might be helpful to try calculating Gini Impurity by hand to get a better
    sense of how it works. We can do that for the very simple decision tree shown
    in the following diagram. There are just two leaf nodes – one for individuals
    with a high school GPA greater than 3 and one for a high school GPA less than
    or equal to 3\. (Again, these counts are made up for expository purposes. We assume
    the percentages used in *Figure 11.1* are counts out of 100 people.) Take a look
    at the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试手动计算Gini不纯度可能会有所帮助，以更好地理解它是如何工作的。我们可以为以下图中显示的非常简单的决策树做这件事。这里只有两个叶节点——一个是为高中GPA大于3的个人，另一个是为高中GPA小于或等于3的个人。
    （再次强调，这些计数是为了说明目的而编造的。我们假设*图11.1*中使用的百分比是基于100人中的计数。）请看以下图表：
- en: '![Figure 11.2 - A decision tree with one split and Gini Impurity calculations
    ](img/B17978_11_002.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图11.2 - 具有一个分割和Gini不纯度计算的决策树](img/B17978_11_002.jpg)'
- en: Figure 11.2 - A decision tree with one split and Gini Impurity calculations
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 - 具有一个分割和Gini不纯度计算的决策树
- en: 'Based on this model, **Graduated** would be predicted for folks with a high
    GPA since most of those individuals, 40 out of 45, did graduate. Gini Impurity
    is fairly low, which is good. We can calculate Gini Impurity for that node using
    the preceding formula:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个模型，对于GPA高的个人，**Graduated**会被预测，因为其中大多数，45人中的40人，都毕业了。Gini不纯度相对较低，这是好的。我们可以使用前面的公式计算该节点的Gini不纯度：
- en: '![](img/B17978_11_003.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_11_003.jpg)'
- en: 'Our model would predict **Did not graduate** for individuals with a high school
    GPA less than or equal to 3, since the majority of those individuals did not graduate
    from college. However, there is much less purity here. The Gini Impurity value
    for that node is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型会预测高中GPA小于或等于3的个人**未毕业**，因为这些人中的大多数都没有从大学毕业。然而，这里的纯度要低得多。该节点的Gini不纯度值如下：
- en: '![](img/B17978_11_004.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_11_004.jpg)'
- en: The decision tree algorithm calculates the weighted sum of the Gini Impurity
    values for all possible splits from a given point and chooses the split with the
    lowest score. The algorithm would follow a similar process if entropy was used
    rather than Gini Impurity. We will use scikit-learn’s **classification and regression
    tree** (**CART**) algorithm to build decision trees in this chapter. That tool
    defaults to using Gini Impurity, though we could get it to use entropy instead.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法计算从给定点开始的所有可能分割的Gini不纯度值的加权总和，并选择得分最低的分割。如果使用熵而不是Gini不纯度，算法将遵循类似的过程。在本章中，我们将使用scikit-learn的**分类和回归树**（**CART**）算法来构建决策树。该工具默认使用Gini不纯度，尽管我们可以让它使用熵。
- en: A decision tree is what we call a greedy learner. The algorithm chooses a split
    that gives us the best Gini Impurity or entropy score at the current level. It
    does not examine how that choice affects the splits available subsequently, reconsidering
    the choice at the current level based on that information. This makes the algorithm
    much more efficient than it otherwise would be, but it may not provide the globally
    optimal solution.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是我们所说的贪婪学习器。算法选择在当前级别给出最佳Gini不纯度或熵得分的分割。它不会检查该选择如何影响随后可用的分割，也不会基于该信息重新考虑当前级别的选择。这使得算法比其他情况下更有效率，但它可能不会提供全局最优解。
- en: The main disadvantage of decision trees is their high variance. They can overfit
    anomalous observations in the training data and not do well with new data as a
    result. Depending on the characteristics of our data, we can get a very different
    model each time we fit a decision tree. We can use ensemble methods, such as bagging
    or random forest, to address this issue.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的主要缺点是它们的方差高。它们可能会过度拟合训练数据中的异常观测值，因此在新数据上表现不佳。根据我们数据的特点，我们每次拟合决策树时都可能得到一个非常不同的模型。我们可以使用集成方法，如bagging或随机森林，来解决这个问题。
- en: Using random forest for classification
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机森林进行分类
- en: Random forests, perhaps not surprisingly, are collections of decision trees.
    But this would not distinguish a random forest from bootstrap aggregating, commonly
    referred to as bagging. Bagging is often used to reduce the variance of machine
    learning algorithms, such as decision trees, that have high variances. With bagging,
    we generate random samples from our dataset, say 100\. Then, we run our model,
    such as a decision tree classifier, on each of those samples, averaging the predictions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林，可能不会令人惊讶，是一系列决策树的集合。但这并不能区分随机森林和自助聚合（通常称为bagging）。Bagging通常用于减少具有高方差的机器学习算法（如决策树）的方差。使用bagging，我们从数据集中生成随机样本，比如说100个。然后，我们在每个样本上运行我们的模型，例如决策树分类器，并对预测进行平均。
- en: However, the samples that are generated with bagging can be correlated, and
    the resulting decision trees may have many similarities. This is more likely to
    be the case when there are just a few features that explain much of the variation.
    Random forests address this issue by limiting the number of features that can
    be selected for each split. A good rule of thumb for a decision tree classification
    model is to take the square root of the number of features available to determine
    the number of features to use. For example, if there are 25 features, we would
    use five for each split.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用bagging生成的样本可能存在相关性，并且产生的决策树可能有很多相似之处。这种情况在只有少数特征可以解释大部分变化时更为可能。随机森林通过限制每个分割可以选定的特征数量来解决这一问题。对于决策树分类模型的一个好的经验法则是取可用特征数的平方根来确定要使用的特征数。例如，如果有25个特征，我们会在每个分割中使用5个。
- en: 'Let’s be a little more precise about the steps involved in constructing a random
    forest:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更精确地描述构建随机森林所涉及的步骤：
- en: Randomly sample, with replacement, instances from the training data. The sample
    has the same number of observations as the original dataset.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据中随机采样实例（样本具有与原始数据集相同的观测数。）
- en: Randomly select, with replacement, features from the sample. (A good number
    of features to select each time is the square root of the total number of features
    available.)
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择样本中的特征（每次选择的好数量是可用特征总数的平方根。）
- en: Identify a feature, from those randomly selected in *Step 2*, where a split
    would result in nodes with the greatest purity.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*步骤 2*中随机选择的特征中确定一个特征，分割会导致具有最大纯度的节点。
- en: '**Inner Loop**: Repeat *Steps 2* and *3* until a decision tree is constructed.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**内部循环**：重复执行*步骤 2*和*步骤 3*，直到构建出一个决策树。'
- en: '**Outer Loop**: Repeat all steps, including the inner loop, until the desired
    number of trees is created. The result across all trees is determined by voting;
    that is, the class is predicted based on the most frequent class label across
    all trees for the given feature values.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**外部循环**：重复所有步骤，包括内部循环，直到创建出所需数量的树。所有树的结果由投票决定；也就是说，基于所有树对于给定特征值的最高频率类别标签来预测类别。'
- en: One cool secondary effect of this process is that it generates test data for
    us, so to speak. The bootstrapping process – that is, sampling with replacement
    – will lead to many instances being left out of the sample for one or more trees,
    often as much as a third. These instances, known as **out-of-bag** samples, can
    be used to evaluate the model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的另一个有趣副作用是它为我们生成了测试数据。所谓的自助抽样过程——即带替换的抽样——会导致许多实例被排除在一个或多个树之外，通常多达三分之一。这些实例，被称为**袋外**样本，可以用来评估模型。
- en: Basing our class predictions on many uncorrelated decision trees has the positive
    effect on variance (lowering it!) that you would expect. Random forest models
    are often more generalizable than decision tree models. They are less vulnerable
    to overfitting and less likely to be yanked around by anomalous data. But this
    comes with a price. Building a hundred or more decision trees draws more on system
    resources than just building one. We also lose the ease of interpretation of decision
    trees; it is harder to explain the importance of each feature.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于许多不相关的决策树进行分类预测，对方差（降低它！）有积极的影响，这是你可以预期的。随机森林模型通常比决策树模型更具泛化能力。它们不太容易过拟合，也不太可能被异常数据所影响。但这也带来了一定的代价。构建一百个或更多的决策树比只构建一个需要更多的系统资源。我们也失去了决策树易于解释的优点；解释每个特征的重要性变得更加困难。
- en: Using gradient-boosted decision trees
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度提升决策树
- en: Conceptually, gradient-boosted decision trees are similar to a random forest.
    They rely on multiple decision trees to improve model performance. But they are
    performed sequentially, with each tree learning from the previous ones. Each new
    tree works from the residuals of the previous iteration.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，梯度提升决策树与随机森林相似。它们依赖于多个决策树来提高模型性能。但它们是顺序执行的，每个树都从前一个树学习。每个新的树都从前一个迭代的残差开始工作。
- en: The rate at which a gradient-boosted decision tree learns is determined by the
    *ɑ* hyperparameter. You might be wondering why would we not want our model to
    learn as fast as possible. A faster learning rate is more efficient and taxes
    system resources less. However, we can build a model that is more generalizable
    with a lower learning rate. There is less risk of overfitting. The optimal learning
    rate is ultimately an empirical question. We need to do some hyperparameter tuning
    to find that. We will do this in the last section of this chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升决策树的学习速率由*ɑ*超参数决定。你可能想知道为什么我们不希望我们的模型尽可能快地学习。更快的学习速率更有效率，对系统资源的消耗也更少。然而，我们可以通过降低学习速率构建一个更具泛化能力的模型。过度拟合的风险更小。最佳学习速率最终是一个经验问题。我们需要进行一些超参数调整来找到它。我们将在本章的最后部分进行这项工作。
- en: 'Just as was the case with random forests, we can improve our intuition about
    gradient boosting by going through the steps for a binary classification problem:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如随机森林的情况一样，我们可以通过进行二元分类问题的步骤来提高我们对梯度提升的直觉：
- en: Make an initial prediction for the target based on the mean value of the target
    across the sample. For a binary target, this is a proportion. Assign this prediction
    to all observations. (We use the log of the odds of class membership here.)
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据样本中目标变量的平均值对目标进行初步预测。对于二元目标，这是一个比例。将此预测分配给所有观测值。（我们在这里使用类别成员概率的对数。）
- en: Calculate the residual for each instance, which will be 1 minus the initial
    prediction for in-class instances, and 0 minus the prediction, or -prediction,
    for out-of-class instances.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个实例的残差，对于类内实例，将是1减去初始预测，对于类外实例，是0减去预测，或-预测。
- en: Construct a decision tree to predict the residuals.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个决策树来预测残差。
- en: Generate new predictions based on the decision tree model.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于决策树模型生成新的预测。
- en: Adjust the previous prediction for each instance based on the new predictions
    scaled by the learning rate. As mentioned previously, we use a learning rate because
    we do not want predictions to move in the right direction too quickly.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据新的预测（按学习率缩放）调整每个实例的先前预测。如前所述，我们使用学习率是因为我们不希望预测移动得太快。
- en: Loop back to *step 3* unless the maximum number of trees has been reached or
    the residuals are very small.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果未达到最大树的数量或残差非常小，则回退到*步骤3*。
- en: While this is a simplified explanation of how gradient boosting works, it does
    provide a good feel of what the algorithm is doing for us. Hopefully, it also
    helps you understand why gradient boosting has become so popular. The algorithm
    works repeatedly to adjust to previous errors but does so relatively efficiently
    and with less risk of overfitting than decision trees alone.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是梯度提升工作原理的简化解释，但它确实为我们提供了算法所做工作的良好感觉。希望这也有助于你理解为什么梯度提升变得如此受欢迎。该算法反复调整以适应先前错误，但这样做相对高效，并且比单独的决策树有更小的过度拟合风险。
- en: We will work through examples of decision trees, random forests, and gradient
    boosting in the rest of this chapter. We will discuss how to tune hyperparameters
    and how to evaluate these models. We will also go over the advantages and disadvantages
    of each approach.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将讨论决策树、随机森林和梯度提升的示例。我们将讨论如何调整超参数以及如何评估这些模型。我们还将讨论每种方法的优缺点。
- en: Decision tree models
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树模型
- en: 'We will work with the heart disease data again in this chapter. This will be
    a great way to compare our results from the logistic regression model to those
    of a non-parametric model such as a decision tree. Follow these steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将再次使用心脏病数据。这将是一个很好的方法来比较我们的逻辑回归模型的结果与决策树等非参数模型的结果。按照以下步骤进行：
- en: 'First, we load the same libraries that we have been using so far. The new modules
    are `DecisionTreeClassifier` from scikit-learn and `SMOTENC` from Imbalance Learn,
    which will help us deal with imbalanced data:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们加载到目前为止一直在使用的相同库。新的模块是来自scikit-learn的`DecisionTreeClassifier`和来自Imbalance
    Learn的`SMOTENC`，这将帮助我们处理不平衡数据：
- en: '[PRE0]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s load the heart disease data. We will convert our target, `heartdisease`,
    into a `0` and `1` integer. We will not repeat the frequencies and descriptive
    statistics that we generated from the data in the previous chapter. It is probably
    helpful to take a quick look at them if you did not work through [*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126),
    *Logistic Regression*:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载心脏病数据。我们将把我们的目标`heartdisease`转换为`0`和`1`的整数。我们不会重复之前章节中从数据中生成的频率和描述性统计信息。如果你没有完成[*第10章*](B17978_10_ePub.xhtml#_idTextAnchor126)，*逻辑回归*，快速查看它们可能是有帮助的：
- en: '[PRE1]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice the class imbalance. Less than 10% of observations have heart disease.
    We will need to deal with that in our model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意类别不平衡。不到10%的观察结果患有心脏病。我们将在模型中处理这个问题。
- en: 'Let’s also look at values for the age category feature since it is a tad unusual.
    It contains character data for the age range. We will convert it into an ordinal
    feature using the `MakeOrdinal` class we loaded. For example, values of `18-24`
    will give us a value of `0`, while values of `50-54` will be `6` after we do the
    transformation later:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再看看年龄类别特征的值，因为它有点不寻常。它包含年龄范围的字符数据。我们将使用我们加载的`MakeOrdinal`类将其转换为有序特征。例如，`18-24`的值将给我们一个`0`的值，而`50-54`的值在转换后将变为`6`：
- en: '[PRE2]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We should organize our features by data type as this will make some tasks easier
    later. We will also set up a dictionary to recode the `genhealth` and `diabetic`
    features:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该按数据类型组织我们的特征，这样将使一些任务更容易。我们还将设置一个字典来重新编码`genhealth`和`diabetic`特征：
- en: '[PRE3]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, let’s create training and testing DataFrames:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建训练和测试数据框：
- en: '[PRE4]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we will set up the column transformations. We will use our custom classes
    to encode the `agecategory` feature as ordinal and replace character values with
    numeric ones for `genhealth` and `diabetic`.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将设置列转换。我们将使用我们的自定义类将`agecategory`特征编码为有序，并将`genhealth`和`diabetic`的特征的字符值替换为数值。
- en: 'We will not transform the numerical columns since it is not typically necessary
    to scale those features when using a decision tree. We will not worry about outliers
    either as decision trees are less sensitive to them than in logistic regression.
    We will set `remainder` to `passthrough` to get the transformer to pass the remaining
    columns (the numerical columns) through as-is:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会转换数值列，因为在使用决策树时通常不需要缩放这些特征。我们也不会担心异常值，因为决策树对它们的敏感性低于逻辑回归。我们将设置`remainder`为`passthrough`，以便转换器将剩余的列（数值列）原样通过：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We need to do a little work before we run our model. As you will see in the
    next step, we need to know how many features will be returned from the one-hot
    encoder. We should also grab the new feature names while we are at it. We will
    need them later as well. (We only need to fit the column transformer on a small
    random sample of the data for this purpose.) Take a look at the following code:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行模型之前，我们需要做一些工作。正如你将在下一步看到的，我们需要知道单变量编码器将返回多少个特征。同时，我们也应该获取新的特征名称。我们稍后还需要它们。（我们只需要对数据的一个小随机样本进行列转换器拟合即可。）看看下面的代码：
- en: '[PRE6]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s view the feature names:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看特征名称：
- en: '[PRE7]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We need to deal with our imbalanced dataset before we fit our decision tree.
    We can use the `SMOTENC` module from Imbalanced-learn to oversample the heart
    disease class. This will generate enough representative instances of the heart
    disease class to balance the class memberships.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们拟合决策树之前，我们需要处理我们的不平衡数据集。我们可以使用来自Imbalanced-learn的`SMOTENC`模块来对心脏病类别进行过采样。这将生成足够的心脏病类别的代表性实例，以平衡类别成员资格。
- en: Next, we must instantiate a decision tree classifier and indicate that the leaf
    nodes need to have at least five observations and that the tree depth cannot exceed
    two. Then, we will use the column transformer to transform the training data and
    fit the model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须实例化一个决策树分类器，并指出叶子节点需要至少有五个观察结果，并且树的深度不能超过两个。然后，我们将使用列转换器转换训练数据并拟合模型。
- en: We will do some hyperparameter tuning later in this section. For now, we just
    want to produce a decision tree that is easy to interpret and visualize.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节稍后进行一些超参数调整。现在，我们只想生成一个易于解释和可视化的决策树。
- en: '`SMOTENC` needs to know which columns are categorical. When we set up the column
    transformer, we encoded the binary columns first, and then the categorical columns.
    So, the number of binary columns plus the number of categorical columns gives
    us the endpoint of those column indexes. Then, we must pass a range, starting
    from 0 and ending at the number of categorical columns, to the `categorical_features`
    parameter of `SMOTENC`.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`SMOTENC`需要知道哪些列是分类的。当我们设置列转换器时，我们首先编码二进制列，然后是分类列。因此，二进制列数加上分类列数给出了那些列索引的终点。然后，我们必须传递一个范围，从0开始，到分类列数结束，到`SMOTENC`的`categorical_features`参数。'
- en: 'Now, we can create a pipeline containing the column transformations, the oversampling,
    and the decision tree classifier, and fit it:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个包含列转换、过采样和决策树分类器的管道，并对其进行拟合：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Oversampling can be a good option when we are concerned that our model is doing
    a poor job of capturing variation in a class because we have too few instances
    of that class, relative to one or more other classes. Oversampling duplicates
    instances of that class.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们担心我们的模型在捕捉一个类别的变化方面做得不好，因为我们有太多少的该类实例，相对于一个或多个其他类别时，过采样可以是一个好的选择。过采样会复制该类别的实例。
- en: '**Synthetic Minority Oversampling Technique** (**SMOTE**) is an algorithm that
    uses KNN to duplicate instances. The implementation of SMOTE in this chapter is
    from Imbalanced-learn, specifically SMOTENC, which can handle categorical data.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**合成少数过采样技术**（**SMOTE**）是一个使用KNN来复制实例的算法。本章中SMOTE的实现来自Imbalanced-learn，特别是SMOTENC，它可以处理分类数据。'
- en: Oversampling is often done when the class imbalance is even worse than with
    this dataset, say 100 to 1\. Nonetheless, I thought it would be helpful to demonstrate
    how to use SMOTE and similar tools in this chapter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当类不平衡比这个数据集更严重时，通常会进行过采样，比如100到1。尽管如此，我认为在本章中演示如何使用SMOTE和类似工具是有帮助的。
- en: 'After running the fit, we can look at which features were identified as important.
    `agecategory`, `genhealth`, and `diabetic` are important features in this simple
    model:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行拟合后，我们可以查看哪些特征被识别为重要。`agecategory`、`genhealth`和`diabetic`是此简单模型中的重要特征：
- en: '[PRE9]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we can generate a graph of the decision tree:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以生成决策树的图形：
- en: '[PRE10]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This produces the following plot:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图形：
- en: '![Figure 11.3 – Decision tree example with a heart disease target ](img/B17978_11_0031.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3 – 以心脏病为目标的心脏病决策树示例](img/B17978_11_0031.jpg)'
- en: Figure 11.3 – Decision tree example with a heart disease target
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – 以心脏病为目标的心脏病决策树示例
- en: The initial binary split, at the root node (also known as depth 0), is based
    on whether `agecategory` is less than or equal to `6`. (Recall that `agecategory`
    was originally a character feature. Initial values of `50-54` get a value of `6`
    after our encoding.) If the root node statement is true, it leads to the node
    down one level and to the left. If the statement is false, it leads to the node
    down one level and to the right. The number of samples is the number of observations
    that made it to that node. So, the sample value of `12576` at the `diabetic<=0.001`
    (that is, not diabetic) node reflects the number of instances for which the statement
    from the parent node was true; that is, `12576` instances had values for the age
    category that were less than or equal to `6`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 初始的二分分割，在根节点（也称为深度0），是基于`agecategory`是否小于或等于`6`。 (回想一下，`agecategory`最初是一个字符特征。编码后的初始值`50-54`得到`6`的值。)
    如果根节点语句为真，它将导致下一级节点向左。如果语句为假，它将导致下一级节点向右。样本数是该节点到达的观察数。因此，`diabetic<=0.001`（即非糖尿病患者）节点上的样本值`12576`反映了从父节点中得到的语句为真的实例数；也就是说，有`12576`个实例的年龄类别值小于或等于`6`。
- en: The `value` list within each node gives us the instances in each class in the
    training data. In this case, the first value is for the count of no disease observations.
    The second value is the count of the observations with heart disease. For example,
    there are `10781` no disease observations and `1795` disease observations at the
    `diabetic<=0.001` node.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点内的`value`列表给我们提供了训练数据中每个类别的实例。在这种情况下，第一个值是无疾病观察值的计数。第二个值是有心脏病观察值的计数。例如，在`diabetic<=0.001`节点上，有`10781`个无疾病观察值和`1795`个疾病观察值。
- en: Decision trees predict the most frequent class at the leaf mode. So, this model
    would predict no disease for individuals less than or equal to 54 (`agecategory<=6`)
    and who are not diabetic (`diabetic<=0.001`). There were `10142` no disease observations
    for that group in the training data, and `990` disease observations. This gives
    us a leaf node with a very good Gini Impurity of `0.162`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在叶子模式下预测最频繁的类别。因此，这个模型会预测54岁或以下且不是糖尿病患者的个体没有疾病（`agecategory<=6`和`diabetic<=0.001`）。在训练数据中，该组有`10142`个无疾病观察结果，`990`个疾病观察结果。这给我们一个具有非常好的基尼不纯度`0.162`的叶子节点。
- en: If the person has diabetes, even if they are 54 or younger, our model predicts
    heart disease. It predicts this less definitively, however. Gini Impurity is `0.493`.
    By comparison, the prediction of disease for individuals over 54 (`agecategory<=6.001`
    is false) that have less than excellent health (`genhealth<=3.0`) has a significantly
    lower Gini Impurity.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个人有糖尿病，即使他们54岁或以下，我们的模型也会预测心脏病。然而，这种预测并不那么确定。基尼不纯度为`0.493`。相比之下，预测54岁以上（`agecategory<=6.001`为假）且健康状况不佳（`genhealth<=3.0`）的个体的疾病，其基尼不纯度显著较低。
- en: Our model predicts no disease for individuals over 54 and with general health
    equal to `4`. (We coded general health values of *Excellent* as `4` when we did
    the column transformations.) However, the poor Gini Impurity score indicates that
    our model does not exactly make that prediction with confidence.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型预测54岁以上且一般健康状况等于`4`的人没有疾病。（当我们进行列转换时，我们将一般健康状况值`Excellent`编码为`4`。）然而，较差的基尼不纯度分数表明，我们的模型并不完全有信心做出那个预测。
- en: 'Let’s look at some metrics for this model. The model has okay, though not great,
    sensitivity, predicting about 70% of the time that there is heart disease. Precision,
    the rate at which we are correct when we make a positive prediction, is quite
    low. Only 19% of the time that we make a positive prediction are we correct:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看这个模型的某些指标。该模型有不错的敏感性，但不是很好，大约70%的时间预测有心脏病。当我们做出积极预测时，精确度相当低。只有19%的时间我们做出的积极预测是正确的：
- en: '[PRE11]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This model makes some fairly arbitrary decisions regarding hyperparameters,
    setting maximum depth at `2` and the minimum number of samples for a leaf at `5`.
    We should explore alternative values for these hyperparameters that will yield
    a better-performing model. Let’s do a randomized grid search to find those values.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型在超参数方面做出了一些相当随意的决定，将最大深度设置为`2`，并将叶子的最小样本数设置为`5`。我们应该探索这些超参数的替代值，以获得性能更好的模型。让我们进行随机网格搜索以找到这些值。
- en: 'Let’s set up a pipeline with the column transformations, the oversampling,
    and a decision tree classifier. We will also create a dictionary with ranges for
    the minimum leaf size and maximum tree depth hyperparameters. Note that there
    are two underscores after `decisiontreeclassifier` for each dictionary key:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设置一个包含列转换、过采样和决策树分类器的管道。我们还将创建一个包含最小叶子大小和最大树深度超参数范围的字典。请注意，对于每个字典键，`decisiontreeclassifier`后面有两个下划线：
- en: '[PRE12]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we can run the randomized grid search. We will run 20 iterations to test
    a good number of values for our hyperparameters:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以运行随机网格搜索。我们将运行20次迭代以测试我们超参数的多个值：
- en: '[PRE13]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s look at the score from each iteration:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看每次迭代的得分：
- en: '[PRE14]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This produces the following output. The best-performing models have substantially
    greater `max_depth` than the model we constructed earlier. Our model also does
    best with much higher minimum instances for each leaf:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出。表现最好的模型与我们之前构建的模型相比，`max_depth`有显著增加。我们的模型在叶子中每个叶子的最小实例数也更高：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s generate a confusion matrix:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们生成一个混淆矩阵：
- en: '[PRE16]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This produces the following plot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 11.4 – Heart disease confusion matrix from the decision tree model
    ](img/B17978_11_0041.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4 – 决策树模型的冠心病混淆矩阵](img/B17978_11_0041.jpg)'
- en: Figure 11.4 – Heart disease confusion matrix from the decision tree model
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 – 决策树模型的冠心病混淆矩阵
- en: One thing about this plot that you may have noticed right away is how low our
    precision is. The vast majority of the time we predict positive, we are wrong.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图你可能立刻注意到的一个问题是我们的精确度有多低。绝大多数时间我们预测为阳性，我们都是错误的。
- en: 'Let’s look at the accuracy, sensitivity, specificity, and precision scores
    to see if there is much improvement in the metrics from the model without hyperparameter
    tuning. We do noticeably worse with sensitivity, which is 63% now compared to
    70% earlier. However, we do a little better with specificity. We correctly predict
    negative on the testing data 79% of the time now, compared to 73% with the earlier
    model:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看准确率、灵敏度、特异性和精确度分数，看看模型在没有超参数调整的情况下，这些指标是否有很大改进。我们在灵敏度上表现明显更差，现在为63%，而之前为70%。然而，我们在特异性上做得稍微好一些。我们现在在测试数据上正确预测负面的概率为79%，而之前的模型为73%：
- en: '[PRE17]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Decision trees are a good starting point for classification models. They make
    very few assumptions about the underlying data and do not require much preprocessing.
    Notice that we did not do any scaling or outlier detection in this example, as
    that is often not essential with decision trees. We also get a model that is fairly
    easy to understand or explain, so long as we limit the number of depths.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是分类模型的良好起点。它们对底层数据几乎没有假设，并且不需要太多的预处理。注意，在这个例子中我们没有进行任何缩放或异常值检测，因为在决策树中这通常不是必要的。我们还得到一个相当容易理解或解释的模型，只要我们限制深度数量。
- en: We can often improve the performance of our decision tree models with random
    forests, for reasons we discussed at the beginning of this chapter. A key reason
    for this is that variance is reduced when using random forests instead of decision
    trees.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常可以通过随机森林来提高我们的决策树模型的性能，原因我们在本章开头已经讨论过。一个关键的原因是，当使用随机森林而不是决策树时，方差会降低。
- en: In the next section, we’ll take a look at random forests.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨随机森林。
- en: Implementing random forest
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现随机森林
- en: 'Let’s try to improve our heart disease model with a random forest:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用随机森林来提高我们的心脏病模型：
- en: 'First, let’s load the same libraries that we used in the previous section,
    except we will import the random forest classifier this time:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们加载与上一节相同的库，但这次我们将导入随机森林分类器：
- en: '[PRE18]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We also load the `healthinfo` module; it loads the health information data and
    does our preprocessing. There is nothing fancy here. The preprocessing code we
    stepped through earlier was just copied to the `helperfunctions` subfolder of
    the current working directory.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还加载了`healthinfo`模块；它加载健康信息数据并执行我们的预处理。这里没有太多花哨的东西。我们之前步骤中使用的预处理代码只是复制到了当前工作目录的`helperfunctions`子文件夹中。
- en: 'Now, let’s grab the data that’s been processed by the `healthinfo` module so
    that we can use it for our random forest classifier:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们获取由`healthinfo`模块处理过的数据，以便我们可以使用它来为我们的随机森林分类器：
- en: '[PRE19]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s instantiate a random forest classifier and create a pipeline for the
    grid search. We will also create a dictionary for the hyperparameters to search.
    In addition to the `max_depth` and `min_samples_leaf` hyperparameters that we
    used for the decision tree, an important hyperparameter for a random forest is
    `n_estimators`. This indicates the number of trees to use for that iteration of
    the search. We will also add `entropy` as a criterion, in addition to `gini`,
    which we have been using so far:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实例化一个随机森林分类器并为网格搜索创建一个管道。我们还将创建一个用于搜索的超参数字典。除了我们用于决策树的`max_depth`和`min_samples_leaf`超参数外，随机森林的一个重要超参数是`n_estimators`。这表示用于该搜索迭代的树的数量。我们还将添加`entropy`作为标准，除了我们迄今为止使用的`gini`：
- en: '[PRE20]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can use the `best_params_` and `best_score_` attributes of the randomized
    grid search object to find the best parameters and the associated score, respectively.
    The best model has `1023` trees and a maximum depth of `9`.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用随机网格搜索对象的`best_params_`和`best_score_`属性来找到最佳参数和相应的分数。最佳模型有`1023`棵树和最大深度为`9`。
- en: 'Here, we can see some improvements in the `roc_auc` score over the decision
    tree model from the previous section:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到`roc_auc`分数相对于上一节中的决策树模型有所提高：
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The random forest’s results are more difficult to interpret than those for
    a single decision tree, but a good place to start is with the feature importance.
    The top three features are the same as the ones we saw with the decision tree
    – that is, `agecategory`, `genhealth`, and `diabetic`:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机森林的结果比单个决策树的结果更难以解释，但一个好的起点是查看特征重要性。前三个特征与我们在决策树中看到的是相同的——即`agecategory`、`genhealth`和`diabetic`：
- en: '[PRE22]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s look at some metrics. There is some improvement in sensitivity over the
    previous model, though not much in any other measure. We keep the same relatively
    decent specificity score. Overall, this is our best model so far:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看一些指标。与先前的模型相比，敏感性有所提高，但在其他任何指标上都没有太大变化。我们保持了相同的相对良好的特异性分数。总体而言，这是我们迄今为止最好的模型：
- en: '[PRE23]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We may still be able to improve on our model’s performance metrics. We should
    at least attempt some gradient boosting. As we discussed in the *Using gradient-boositng
    decision trees* section of this chapter, gradient-boosted decision trees can sometimes
    result in a better model than a random forest. This is because each tree learns
    from the errors of previous ones.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能仍然能够提高我们模型性能指标。我们至少应该尝试一些梯度提升。正如我们在本章的*使用梯度提升决策树*部分所讨论的，梯度提升决策树有时可能比随机森林产生更好的模型。这是因为每个树都是从先前树的错误中学习的。
- en: Implementing gradient boosting
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现梯度提升
- en: 'In this section, we will try to improve our random forest model using gradient
    boosting. One thing we will have to watch out for is overfitting, which can be
    more of an issue with gradient boosting decision trees than with random forests.
    This is because the trees for random forests do not learn from other trees, whereas
    with gradient boosting, each tree builds on the learning of previous trees. Our
    choice of hyperparameters here is key. Let’s get started:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试使用梯度提升来改进我们的随机森林模型。我们必须注意的一个问题是过拟合，这在梯度提升决策树中可能比在随机森林中更成问题。这是因为随机森林的树不会从其他树中学习，而梯度提升中，每个树都是基于先前树的学习的。我们在这里选择的超参数至关重要。让我们开始吧：
- en: 'We will start by importing the necessary libraries. We will use the same modules
    we used for random forests, except we will import `GradientBoostingClassifier`
    from `ensemble` rather than `RandomForestClassifier`:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入必要的库。我们将使用与随机森林相同的模块，但我们将从`ensemble`导入`GradientBoostingClassifier`而不是`RandomForestClassifier`：
- en: '[PRE24]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, let’s grab the data that’s been processed by the `healthinfo` module for
    our random forest classifier:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们获取由`healthinfo`模块处理过的数据，用于我们的随机森林分类器：
- en: '[PRE25]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Next, we will instantiate a gradient-boosting classifier instance and add it
    to a pipeline, along with our steps for preprocessing the health data (which are
    in the module we imported).
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实例化一个梯度提升分类器实例，并将其添加到一个管道中，包括我们用于预处理健康数据的步骤（这些步骤在我们导入的模块中）。
- en: 'We will create a dictionary that contains the hyperparameters for the gradient-boosting
    classifier. These include the familiar minimum samples per leaf, maximum depth,
    and the number of estimators hyperparameters. We will also add values to check
    for the learning rate:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个包含梯度提升分类器超参数的字典。这些包括熟悉的每个叶子的最小样本数、最大深度以及估计器的数量超参数。我们还将添加用于检查学习率的值：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we are ready to do our grid search. (Note that this may take some time
    to run on your machine.) The best model out of the seven iterations we ran had
    a learning rate of `0.25` and `308` estimators, or trees. It had a decent `roc_auc`
    score of `0.82`:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备进行网格搜索。（注意，这可能在您的机器上运行需要一些时间。）在我们运行的七次迭代中，最佳模型的学习率为`0.25`，估计器或树的数量为`308`。它有一个相当不错的`roc_auc`分数为`0.82`：
- en: '[PRE27]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s look at the feature importance:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看特征重要性：
- en: '[PRE28]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This produces the following plot:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 11.5 – Gradient boosting feature importance ](img/B17978_11_005.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5 – 梯度提升特征重要性](img/B17978_11_005.jpg)'
- en: Figure 11.5 – Gradient boosting feature importance
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 – 梯度提升特征重要性
- en: 'Let’s look at some metrics. Interestingly, we get outstanding accuracy and
    specificity but abysmal sensitivity. This may be due to overfitting:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看一些指标。有趣的是，我们得到了出色的准确性和特异性，但敏感性却非常糟糕。这可能是因为过拟合：
- en: '[PRE29]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Although our results were mixed, gradient-boosted decision trees are often a
    great choice for a classification model. This is particularly true if we are modeling
    complicated relationships between features and the target. When a decision tree
    is an appropriate choice, but we are concerned about high variance with our decision
    tree model, gradient-boosted decision trees are at least as good an option as
    random forests.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的结果参差不齐，但梯度提升决策树通常是一个很好的分类模型选择。这尤其适用于我们正在建模特征与目标之间的复杂关系时。当决策树是一个合适的选择，但我们担心决策树模型的高方差时，梯度提升决策树至少与随机森林一样是一个好的选择。
- en: Now, let’s summarize what we’ve learned in this chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结一下本章学到的内容。
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter explored how to use decision trees for classification problems.
    Although the examples in this chapter all involved a binary target, the algorithms
    we worked with can also handle multiclass problems. Unlike the switch from logistic
    to multinomial logistic regression, few changes need to be made to use the algorithms
    well when our target has more than two values.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了如何使用决策树来解决分类问题。尽管本章中的所有示例都涉及二元目标，但我们使用的算法也可以处理多类问题。与从逻辑回归到多项式逻辑回归的转变不同，当我们的目标值超过两个时，为了有效地使用这些算法，我们通常不需要做出太多改变。
- en: We looked at two approaches to dealing with the high variance of decision trees.
    One approach is to use a random forest, which is a form of bagging. This will
    reduce the variance in our predictions. Another approach is to use gradient-boosted
    decision trees. Boosting can help us capture very complicated relationships in
    the data, but there is a non-trivial risk of overfitting. It is particularly important
    to tune our hyperparameters with that in mind.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了两种处理决策树高方差的方法。一种方法是使用随机森林，这是一种袋装方法。这将减少我们预测中的方差。另一种方法是使用梯度提升决策树。提升可以帮助我们捕捉数据中的非常复杂的关系，但存在非平凡的过拟合风险。考虑到这一点，调整我们的超参数尤为重要。
- en: 'In the next chapter, we explore another well-known algorithm for classification:
    K-nearest neighbors.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨另一个著名的分类算法：K最近邻算法。
