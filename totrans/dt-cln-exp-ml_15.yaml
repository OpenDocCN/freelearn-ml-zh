- en: '*Chapter 11*: Decision Trees and Random Forest Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees and random forests are very popular classification models. This
    is partly because they are easy to train and interpret. They are also quite flexible.
    We can model complexity without necessarily having to increase the feature space
    or transform features. We do not even need to do anything special to apply the
    algorithm to multiclass problems, something we had to do with logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, decision trees can be less stable than other classification
    models, being fairly sensitive to small changes in the training data. Decision
    trees can also be biased when there is a significant class imbalance (when there
    are many more observations in one class than another). Fortunately, these issues
    can be addressed with techniques such as bagging to reduce variance and oversampling
    to deal with imbalance. We will examine these techniques in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing gradient boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the scikit-learn modules we have been using so far, we will use
    SMOTENC from Imbalanced-learn. We will use SMOTENC to address class imbalance.
    The Imbalanced-learn library can be installed with `pip install -U imbalanced-learn`.
    All the code in this chapter was tested with scikit-learn versions 0.24.2 and
    1.0.2.
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are an exceptionally useful machine learning tool. They are non-parametric,
    easy to interpret, and can work with a wide range of data. No assumptions regarding
    linearity of relationships between features and targets, and normality of error
    terms, are made. It isn’t even necessary to scale the data. Decision trees also
    often do a good job of capturing complex relationships between predictors and
    targets.
  prefs: []
  type: TYPE_NORMAL
- en: The flexibility of the decision tree algorithm, and its ability to model complicated
    and unanticipated relationships in the data, is due to the **recursive partitioning**
    procedure that’s used to segment the data. Decision trees group observations based
    on the values of their features. This is done with a series of binary decisions,
    starting from an initial split at the root node, and ending with a leaf for each
    grouping. Each split is based on the feature, and feature values, that provide
    the most information about the target. More precisely, a split is chosen based
    on whether it produces the lowest Gini Impurity score. We will discuss Gini Impurity
    in more detail later.
  prefs: []
  type: TYPE_NORMAL
- en: All new observations with the same values, or the same range of values, along
    the branches from the root node to the leaf, get the same predicted value for
    the target. When the target is categorical, that is the most frequent value for
    the target for the training observations at that leaf.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram provides a fairly straightforward example of a decision
    tree, with made-up data and results for a model of college completion among those
    who attended college. For this decision tree, an initial split of high school
    GPA by those with a 3.0 or less and those with a GPA of greater than 3.0 was found
    to lead to the lowest impurity compared to other available features, as well as
    other thresholds. Therefore, high school GPA is our root node, also known as depth
    0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 11.1 – A decision tree for college completion ](img/B17978_11_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – A decision tree for college completion
  prefs: []
  type: TYPE_NORMAL
- en: The binary split at the root node results in 45% of the observations on the
    left-hand side of the tree and 55% on the right. At depth 1, there are binary
    splits on both sides based on parental income, though the threshold is different;
    $80k and $45k for the left and right-hand sides, respectively. For parental incomes
    above $80k, where the high school GPA is greater than 3, there is no more splitting.
    Here, we get to a prediction of **Graduated**. This is a leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: We can navigate up the tree from each leaf to describe how the tree has segmented
    the data, just as we did for individuals with parental income above $80k and a
    high school GPA above 3\. For example, the decision tree predicts not graduating
    for individuals who receive low levels of support from their schools, have parental
    incomes greater than $45k, and have a high school GPA less than or equal to 3.
  prefs: []
  type: TYPE_NORMAL
- en: So, how does the decision tree algorithm perform this magic? How does it select
    the features and the threshold amounts or class values? Why greater than $80k
    or $45k for parental income? Why received grants at depth 2 (split 3) for parental
    income less than or equal to $45k, but student support level for depth 2 for other
    leaves? One leaf does not even have a further split at depth 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way of measuring the information about a class that a binary split provides
    is by how much it helps us distinguish between in-class and out-of-class membership.
    We frequently use Gini Impurity calculations for that evaluation, though entropy
    is sometimes used. The Gini Impurity statistic tells us how well class membership
    has been segmented at each node. This can be seen in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_11_0011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17978_11_002.png) is the probability of membership in the *k*
    class and *m* is the number of classes. If class membership is equal at a node,
    then Gini Impurity is 0.5\. When completely pure, it is 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'It might be helpful to try calculating Gini Impurity by hand to get a better
    sense of how it works. We can do that for the very simple decision tree shown
    in the following diagram. There are just two leaf nodes – one for individuals
    with a high school GPA greater than 3 and one for a high school GPA less than
    or equal to 3\. (Again, these counts are made up for expository purposes. We assume
    the percentages used in *Figure 11.1* are counts out of 100 people.) Take a look
    at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 - A decision tree with one split and Gini Impurity calculations
    ](img/B17978_11_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 - A decision tree with one split and Gini Impurity calculations
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this model, **Graduated** would be predicted for folks with a high
    GPA since most of those individuals, 40 out of 45, did graduate. Gini Impurity
    is fairly low, which is good. We can calculate Gini Impurity for that node using
    the preceding formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_11_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Our model would predict **Did not graduate** for individuals with a high school
    GPA less than or equal to 3, since the majority of those individuals did not graduate
    from college. However, there is much less purity here. The Gini Impurity value
    for that node is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_11_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The decision tree algorithm calculates the weighted sum of the Gini Impurity
    values for all possible splits from a given point and chooses the split with the
    lowest score. The algorithm would follow a similar process if entropy was used
    rather than Gini Impurity. We will use scikit-learn’s **classification and regression
    tree** (**CART**) algorithm to build decision trees in this chapter. That tool
    defaults to using Gini Impurity, though we could get it to use entropy instead.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree is what we call a greedy learner. The algorithm chooses a split
    that gives us the best Gini Impurity or entropy score at the current level. It
    does not examine how that choice affects the splits available subsequently, reconsidering
    the choice at the current level based on that information. This makes the algorithm
    much more efficient than it otherwise would be, but it may not provide the globally
    optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: The main disadvantage of decision trees is their high variance. They can overfit
    anomalous observations in the training data and not do well with new data as a
    result. Depending on the characteristics of our data, we can get a very different
    model each time we fit a decision tree. We can use ensemble methods, such as bagging
    or random forest, to address this issue.
  prefs: []
  type: TYPE_NORMAL
- en: Using random forest for classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random forests, perhaps not surprisingly, are collections of decision trees.
    But this would not distinguish a random forest from bootstrap aggregating, commonly
    referred to as bagging. Bagging is often used to reduce the variance of machine
    learning algorithms, such as decision trees, that have high variances. With bagging,
    we generate random samples from our dataset, say 100\. Then, we run our model,
    such as a decision tree classifier, on each of those samples, averaging the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: However, the samples that are generated with bagging can be correlated, and
    the resulting decision trees may have many similarities. This is more likely to
    be the case when there are just a few features that explain much of the variation.
    Random forests address this issue by limiting the number of features that can
    be selected for each split. A good rule of thumb for a decision tree classification
    model is to take the square root of the number of features available to determine
    the number of features to use. For example, if there are 25 features, we would
    use five for each split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s be a little more precise about the steps involved in constructing a random
    forest:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly sample, with replacement, instances from the training data. The sample
    has the same number of observations as the original dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly select, with replacement, features from the sample. (A good number
    of features to select each time is the square root of the total number of features
    available.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify a feature, from those randomly selected in *Step 2*, where a split
    would result in nodes with the greatest purity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Inner Loop**: Repeat *Steps 2* and *3* until a decision tree is constructed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Outer Loop**: Repeat all steps, including the inner loop, until the desired
    number of trees is created. The result across all trees is determined by voting;
    that is, the class is predicted based on the most frequent class label across
    all trees for the given feature values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One cool secondary effect of this process is that it generates test data for
    us, so to speak. The bootstrapping process – that is, sampling with replacement
    – will lead to many instances being left out of the sample for one or more trees,
    often as much as a third. These instances, known as **out-of-bag** samples, can
    be used to evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: Basing our class predictions on many uncorrelated decision trees has the positive
    effect on variance (lowering it!) that you would expect. Random forest models
    are often more generalizable than decision tree models. They are less vulnerable
    to overfitting and less likely to be yanked around by anomalous data. But this
    comes with a price. Building a hundred or more decision trees draws more on system
    resources than just building one. We also lose the ease of interpretation of decision
    trees; it is harder to explain the importance of each feature.
  prefs: []
  type: TYPE_NORMAL
- en: Using gradient-boosted decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Conceptually, gradient-boosted decision trees are similar to a random forest.
    They rely on multiple decision trees to improve model performance. But they are
    performed sequentially, with each tree learning from the previous ones. Each new
    tree works from the residuals of the previous iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The rate at which a gradient-boosted decision tree learns is determined by the
    *ɑ* hyperparameter. You might be wondering why would we not want our model to
    learn as fast as possible. A faster learning rate is more efficient and taxes
    system resources less. However, we can build a model that is more generalizable
    with a lower learning rate. There is less risk of overfitting. The optimal learning
    rate is ultimately an empirical question. We need to do some hyperparameter tuning
    to find that. We will do this in the last section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as was the case with random forests, we can improve our intuition about
    gradient boosting by going through the steps for a binary classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Make an initial prediction for the target based on the mean value of the target
    across the sample. For a binary target, this is a proportion. Assign this prediction
    to all observations. (We use the log of the odds of class membership here.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the residual for each instance, which will be 1 minus the initial
    prediction for in-class instances, and 0 minus the prediction, or -prediction,
    for out-of-class instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a decision tree to predict the residuals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate new predictions based on the decision tree model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the previous prediction for each instance based on the new predictions
    scaled by the learning rate. As mentioned previously, we use a learning rate because
    we do not want predictions to move in the right direction too quickly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loop back to *step 3* unless the maximum number of trees has been reached or
    the residuals are very small.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While this is a simplified explanation of how gradient boosting works, it does
    provide a good feel of what the algorithm is doing for us. Hopefully, it also
    helps you understand why gradient boosting has become so popular. The algorithm
    works repeatedly to adjust to previous errors but does so relatively efficiently
    and with less risk of overfitting than decision trees alone.
  prefs: []
  type: TYPE_NORMAL
- en: We will work through examples of decision trees, random forests, and gradient
    boosting in the rest of this chapter. We will discuss how to tune hyperparameters
    and how to evaluate these models. We will also go over the advantages and disadvantages
    of each approach.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will work with the heart disease data again in this chapter. This will be
    a great way to compare our results from the logistic regression model to those
    of a non-parametric model such as a decision tree. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the same libraries that we have been using so far. The new modules
    are `DecisionTreeClassifier` from scikit-learn and `SMOTENC` from Imbalance Learn,
    which will help us deal with imbalanced data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the heart disease data. We will convert our target, `heartdisease`,
    into a `0` and `1` integer. We will not repeat the frequencies and descriptive
    statistics that we generated from the data in the previous chapter. It is probably
    helpful to take a quick look at them if you did not work through [*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126),
    *Logistic Regression*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice the class imbalance. Less than 10% of observations have heart disease.
    We will need to deal with that in our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also look at values for the age category feature since it is a tad unusual.
    It contains character data for the age range. We will convert it into an ordinal
    feature using the `MakeOrdinal` class we loaded. For example, values of `18-24`
    will give us a value of `0`, while values of `50-54` will be `6` after we do the
    transformation later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should organize our features by data type as this will make some tasks easier
    later. We will also set up a dictionary to recode the `genhealth` and `diabetic`
    features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create training and testing DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we will set up the column transformations. We will use our custom classes
    to encode the `agecategory` feature as ordinal and replace character values with
    numeric ones for `genhealth` and `diabetic`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will not transform the numerical columns since it is not typically necessary
    to scale those features when using a decision tree. We will not worry about outliers
    either as decision trees are less sensitive to them than in logistic regression.
    We will set `remainder` to `passthrough` to get the transformer to pass the remaining
    columns (the numerical columns) through as-is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to do a little work before we run our model. As you will see in the
    next step, we need to know how many features will be returned from the one-hot
    encoder. We should also grab the new feature names while we are at it. We will
    need them later as well. (We only need to fit the column transformer on a small
    random sample of the data for this purpose.) Take a look at the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s view the feature names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We need to deal with our imbalanced dataset before we fit our decision tree.
    We can use the `SMOTENC` module from Imbalanced-learn to oversample the heart
    disease class. This will generate enough representative instances of the heart
    disease class to balance the class memberships.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we must instantiate a decision tree classifier and indicate that the leaf
    nodes need to have at least five observations and that the tree depth cannot exceed
    two. Then, we will use the column transformer to transform the training data and
    fit the model.
  prefs: []
  type: TYPE_NORMAL
- en: We will do some hyperparameter tuning later in this section. For now, we just
    want to produce a decision tree that is easy to interpret and visualize.
  prefs: []
  type: TYPE_NORMAL
- en: '`SMOTENC` needs to know which columns are categorical. When we set up the column
    transformer, we encoded the binary columns first, and then the categorical columns.
    So, the number of binary columns plus the number of categorical columns gives
    us the endpoint of those column indexes. Then, we must pass a range, starting
    from 0 and ending at the number of categorical columns, to the `categorical_features`
    parameter of `SMOTENC`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create a pipeline containing the column transformations, the oversampling,
    and the decision tree classifier, and fit it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling can be a good option when we are concerned that our model is doing
    a poor job of capturing variation in a class because we have too few instances
    of that class, relative to one or more other classes. Oversampling duplicates
    instances of that class.
  prefs: []
  type: TYPE_NORMAL
- en: '**Synthetic Minority Oversampling Technique** (**SMOTE**) is an algorithm that
    uses KNN to duplicate instances. The implementation of SMOTE in this chapter is
    from Imbalanced-learn, specifically SMOTENC, which can handle categorical data.'
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling is often done when the class imbalance is even worse than with
    this dataset, say 100 to 1\. Nonetheless, I thought it would be helpful to demonstrate
    how to use SMOTE and similar tools in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the fit, we can look at which features were identified as important.
    `agecategory`, `genhealth`, and `diabetic` are important features in this simple
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we can generate a graph of the decision tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Decision tree example with a heart disease target ](img/B17978_11_0031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Decision tree example with a heart disease target
  prefs: []
  type: TYPE_NORMAL
- en: The initial binary split, at the root node (also known as depth 0), is based
    on whether `agecategory` is less than or equal to `6`. (Recall that `agecategory`
    was originally a character feature. Initial values of `50-54` get a value of `6`
    after our encoding.) If the root node statement is true, it leads to the node
    down one level and to the left. If the statement is false, it leads to the node
    down one level and to the right. The number of samples is the number of observations
    that made it to that node. So, the sample value of `12576` at the `diabetic<=0.001`
    (that is, not diabetic) node reflects the number of instances for which the statement
    from the parent node was true; that is, `12576` instances had values for the age
    category that were less than or equal to `6`.
  prefs: []
  type: TYPE_NORMAL
- en: The `value` list within each node gives us the instances in each class in the
    training data. In this case, the first value is for the count of no disease observations.
    The second value is the count of the observations with heart disease. For example,
    there are `10781` no disease observations and `1795` disease observations at the
    `diabetic<=0.001` node.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees predict the most frequent class at the leaf mode. So, this model
    would predict no disease for individuals less than or equal to 54 (`agecategory<=6`)
    and who are not diabetic (`diabetic<=0.001`). There were `10142` no disease observations
    for that group in the training data, and `990` disease observations. This gives
    us a leaf node with a very good Gini Impurity of `0.162`.
  prefs: []
  type: TYPE_NORMAL
- en: If the person has diabetes, even if they are 54 or younger, our model predicts
    heart disease. It predicts this less definitively, however. Gini Impurity is `0.493`.
    By comparison, the prediction of disease for individuals over 54 (`agecategory<=6.001`
    is false) that have less than excellent health (`genhealth<=3.0`) has a significantly
    lower Gini Impurity.
  prefs: []
  type: TYPE_NORMAL
- en: Our model predicts no disease for individuals over 54 and with general health
    equal to `4`. (We coded general health values of *Excellent* as `4` when we did
    the column transformations.) However, the poor Gini Impurity score indicates that
    our model does not exactly make that prediction with confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some metrics for this model. The model has okay, though not great,
    sensitivity, predicting about 70% of the time that there is heart disease. Precision,
    the rate at which we are correct when we make a positive prediction, is quite
    low. Only 19% of the time that we make a positive prediction are we correct:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This model makes some fairly arbitrary decisions regarding hyperparameters,
    setting maximum depth at `2` and the minimum number of samples for a leaf at `5`.
    We should explore alternative values for these hyperparameters that will yield
    a better-performing model. Let’s do a randomized grid search to find those values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up a pipeline with the column transformations, the oversampling,
    and a decision tree classifier. We will also create a dictionary with ranges for
    the minimum leaf size and maximum tree depth hyperparameters. Note that there
    are two underscores after `decisiontreeclassifier` for each dictionary key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can run the randomized grid search. We will run 20 iterations to test
    a good number of values for our hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at the score from each iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following output. The best-performing models have substantially
    greater `max_depth` than the model we constructed earlier. Our model also does
    best with much higher minimum instances for each leaf:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s generate a confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Heart disease confusion matrix from the decision tree model
    ](img/B17978_11_0041.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Heart disease confusion matrix from the decision tree model
  prefs: []
  type: TYPE_NORMAL
- en: One thing about this plot that you may have noticed right away is how low our
    precision is. The vast majority of the time we predict positive, we are wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the accuracy, sensitivity, specificity, and precision scores
    to see if there is much improvement in the metrics from the model without hyperparameter
    tuning. We do noticeably worse with sensitivity, which is 63% now compared to
    70% earlier. However, we do a little better with specificity. We correctly predict
    negative on the testing data 79% of the time now, compared to 73% with the earlier
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Decision trees are a good starting point for classification models. They make
    very few assumptions about the underlying data and do not require much preprocessing.
    Notice that we did not do any scaling or outlier detection in this example, as
    that is often not essential with decision trees. We also get a model that is fairly
    easy to understand or explain, so long as we limit the number of depths.
  prefs: []
  type: TYPE_NORMAL
- en: We can often improve the performance of our decision tree models with random
    forests, for reasons we discussed at the beginning of this chapter. A key reason
    for this is that variance is reduced when using random forests instead of decision
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll take a look at random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s try to improve our heart disease model with a random forest:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s load the same libraries that we used in the previous section,
    except we will import the random forest classifier this time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We also load the `healthinfo` module; it loads the health information data and
    does our preprocessing. There is nothing fancy here. The preprocessing code we
    stepped through earlier was just copied to the `helperfunctions` subfolder of
    the current working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s grab the data that’s been processed by the `healthinfo` module so
    that we can use it for our random forest classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s instantiate a random forest classifier and create a pipeline for the
    grid search. We will also create a dictionary for the hyperparameters to search.
    In addition to the `max_depth` and `min_samples_leaf` hyperparameters that we
    used for the decision tree, an important hyperparameter for a random forest is
    `n_estimators`. This indicates the number of trees to use for that iteration of
    the search. We will also add `entropy` as a criterion, in addition to `gini`,
    which we have been using so far:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can use the `best_params_` and `best_score_` attributes of the randomized
    grid search object to find the best parameters and the associated score, respectively.
    The best model has `1023` trees and a maximum depth of `9`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, we can see some improvements in the `roc_auc` score over the decision
    tree model from the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The random forest’s results are more difficult to interpret than those for
    a single decision tree, but a good place to start is with the feature importance.
    The top three features are the same as the ones we saw with the decision tree
    – that is, `agecategory`, `genhealth`, and `diabetic`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at some metrics. There is some improvement in sensitivity over the
    previous model, though not much in any other measure. We keep the same relatively
    decent specificity score. Overall, this is our best model so far:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We may still be able to improve on our model’s performance metrics. We should
    at least attempt some gradient boosting. As we discussed in the *Using gradient-boositng
    decision trees* section of this chapter, gradient-boosted decision trees can sometimes
    result in a better model than a random forest. This is because each tree learns
    from the errors of previous ones.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will try to improve our random forest model using gradient
    boosting. One thing we will have to watch out for is overfitting, which can be
    more of an issue with gradient boosting decision trees than with random forests.
    This is because the trees for random forests do not learn from other trees, whereas
    with gradient boosting, each tree builds on the learning of previous trees. Our
    choice of hyperparameters here is key. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the necessary libraries. We will use the same modules
    we used for random forests, except we will import `GradientBoostingClassifier`
    from `ensemble` rather than `RandomForestClassifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s grab the data that’s been processed by the `healthinfo` module for
    our random forest classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we will instantiate a gradient-boosting classifier instance and add it
    to a pipeline, along with our steps for preprocessing the health data (which are
    in the module we imported).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will create a dictionary that contains the hyperparameters for the gradient-boosting
    classifier. These include the familiar minimum samples per leaf, maximum depth,
    and the number of estimators hyperparameters. We will also add values to check
    for the learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to do our grid search. (Note that this may take some time
    to run on your machine.) The best model out of the seven iterations we ran had
    a learning rate of `0.25` and `308` estimators, or trees. It had a decent `roc_auc`
    score of `0.82`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at the feature importance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Gradient boosting feature importance ](img/B17978_11_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Gradient boosting feature importance
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some metrics. Interestingly, we get outstanding accuracy and
    specificity but abysmal sensitivity. This may be due to overfitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Although our results were mixed, gradient-boosted decision trees are often a
    great choice for a classification model. This is particularly true if we are modeling
    complicated relationships between features and the target. When a decision tree
    is an appropriate choice, but we are concerned about high variance with our decision
    tree model, gradient-boosted decision trees are at least as good an option as
    random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s summarize what we’ve learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored how to use decision trees for classification problems.
    Although the examples in this chapter all involved a binary target, the algorithms
    we worked with can also handle multiclass problems. Unlike the switch from logistic
    to multinomial logistic regression, few changes need to be made to use the algorithms
    well when our target has more than two values.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at two approaches to dealing with the high variance of decision trees.
    One approach is to use a random forest, which is a form of bagging. This will
    reduce the variance in our predictions. Another approach is to use gradient-boosted
    decision trees. Boosting can help us capture very complicated relationships in
    the data, but there is a non-trivial risk of overfitting. It is particularly important
    to tune our hyperparameters with that in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we explore another well-known algorithm for classification:
    K-nearest neighbors.'
  prefs: []
  type: TYPE_NORMAL
