- en: Image Recognition with Deeplearning4j
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Images have become ubiquitous in web services, social networks, and web stores.
    In contrast to humans, computers have great difficulty in understanding what is
    in the image and what it represents. In this chapter, we'll first look at the
    challenges behind teaching computers how to understand images, and then focus
    on an approach based on deep learning. We'll look at a high-level theory thats
    required to configure a deep learning model and discuss how to implement a model
    that is able to classify images using a Java library, Deeplearning4j.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing image recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing deep learning fundamentals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an image recognition model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing image recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A typical goal of image recognition is to detect and identify an object in a
    digital image. Image recognition is applied in factory automation to monitor product
    quality; surveillance systems to identify potentially risky activities, such as
    moving persons or vehicles; security applications to provide biometric identification
    through fingerprints, iris, or facial features; autonomous vehicles to reconstruct
    conditions on the road and environment; and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Digital images are not presented in a structured way with attribute-based descriptions;
    instead, they are encoded as the amount of color in different channels, for instance,
    black-white and red-green-blue channels. The learning goal is to identify patterns
    that are associated with a particular object. The traditional approach for image
    recognition consists of transforming an image into different forms, for instance,
    to identify object corners, edges, same-color blobs, and basic shapes. Such patterns
    are then used to train a learner to distinguish between objects. Some notable
    examples of traditional algorithms are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Edge detection finds boundaries of objects within an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Corner detection identifies intersections of two edges or other interesting
    points, such as line endings, curvature maxima or minima, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blob detection identifies regions that differ in a property, such as brightness
    or color, compared to its surrounding regions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridge detection identifies additional interesting points in the image using
    smooth functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale invariant feature transform** (**SIFT**) is a robust algorithm that
    can match objects, even if their scale or orientation differs from the representative
    samples in the database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hough transform identifies particular patterns in the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A more recent approach is based on deep learning. Deep learning is a form of
    neural network, which mimics how the brain processes information. The main advantage
    of deep learning is that it's possible to design neural networks that can automatically
    extract relevant patterns, which in turn can be used to train a learner. With
    recent advances in neural networks, image recognition accuracy has significantly
    boosted. For instance, the **ImageNet** challenge, where competitors are provided
    more than 1.2 million images from 1,000 different object categories, reports that
    the error rate of the best algorithm was reduced from 28% in 2010, using **support
    vector machines** (**SVM**), to only 7% in 2014, using a deep neural network.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll take a quick look at neural networks, starting from the
    basic building block, the perceptron,and gradually introducing more complex structures.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first neural networks, which were introduced in the sixties, were inspired
    by biological neural networks. The idea of a neural network is to map the biological
    nervous system, that is, how the brain processes information. It consists of layers
    with interconnected neurons working together. In computer terms, they are also
    known as an **artificial neural network** (**ANN**). With computers, it requires
    training to make this model learn, the same as a human brain. A neuron in the
    brain gets activated on receiving a signal from nearby interconnected neurons,
    and the same applies to an ANN. Recent advances in neural networks has proved
    that deep neural networks fit very well in pattern recognition tasks, as they
    are able to automatically extract interesting features and learn the underlying
    presentation. In this section, we'll refresh ourselves on the fundamental structures
    and components, from a single perceptron to deep networks.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A perceptron is a basic neural network building block and one of the earliest
    supervised algorithms. It is defined as a sum of features, which is multiplied
    by the corresponding weights and a bias. When the input signals is received, it
    multiplies with the assigned weights. These weights are defined for each incoming
    signal or input, and the weight gets adjusted continuously during the learning
    phase. The adjustment of weight depends on the error of the last result. After
    multiplying with the respective weights, all of the inputs are summed up with
    some offset value called **bias**. The value of the bias is also adjusted by the
    weights. So, it starts with random weights and bias, and with each iteration,
    the weights and bias are adjusted so that the next result moves toward the desired
    output. At the end, the final result is turned into an output signal. The function
    that sums all of this together is called the **sum transfer function,** and it
    is fed into an activation function. If the binary step activation function reaches
    a threshold, the output is 1, otherwise it is 0, which gives us a binary classifier.
    A schematic illustration is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d234876-bb61-48f0-98b4-33d0da465223.png)'
  prefs: []
  type: TYPE_IMG
- en: Training perceptrons involves a fairly simple learning algorithm that calculates
    the errors between the calculated output values and correct training output values,
    and uses this to create an adjustment to the weights, thus implementing a form
    of gradient descent. This algorithm is usually called the **delta rule**.
  prefs: []
  type: TYPE_NORMAL
- en: A single-layer perceptron is not very advanced, and nonlinearly separable functions,
    such as XOR, cannot be modeled using it. To address this issue, a structure with
    multiple perceptrons was introduced, called the **multilayer perceptron**, also
    known as the **feedforward neural network**.
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A feedforward neural network is an ANN that consists of several perceptrons,
    which are organized into layers, as shown in the following diagram: input layer,
    output layer, and one or more hidden layers. The hidden layers have nothing to
    do with the outside world, hence the name. Each layer perceptron, also known as
    a neuron, has direct connections to the perceptrons in the next layer, whereas
    connections between two neurons carry a weight thats similar to the perceptron
    weights. So, all the perceptrons in one layer are connected with the perceptrons
    in the next layer, and the information is fed forward to the next layer. This
    diagram shows a network with a four-unit **Input layer**, corresponding to the
    size of the  feature vector of length `4`, a four-unit **Hidden layer**, and a
    two-unit **Output layer**, where each unit corresponds to one class value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ab8b4ab-f8aa-4184-8620-bd78071abb2c.png)'
  prefs: []
  type: TYPE_IMG
- en: A feedforward neural network learns by finding the relationship between input
    and output values, which are fed into the network multiple times. The most popular
    approach to training multilayer networks is backpropagation. In backpropagation,
    the calculated output values are compared with the correct values in the same
    way as in the delta rule. The error is then fed back through the network by various
    techniques, adjusting the weights of each connection in order to reduce the value
    of the error. The error is calculated using the squared difference between the
    output value of the network and the original output value. The error indicates
    how far we are from the original output values. This process is repeated for a
    sufficiently large number of training cycles, until the error is under a certain
    threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'A feedforward neural network can have more than one hidden layer, where each
    additional hidden layer builds a new abstraction atop the preceding layers. This
    often leads to more accurate models; however, increasing the number of hidden
    layers leads to two known issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vanishing gradients problem**: With more hidden layers, the training with
    backpropagation becomes less and less useful for passing information to the front
    layers, causing these layers to train very slowly'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: The model fits the training data too well and performs poorly
    on real examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at some other networks structures that address these issues.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**An autoencoder** is a feedforward neural network that aims to learn how to
    compress the original dataset. Its aim is to copy input to its output. Therefore,
    instead of mapping features to the input layer and labels to the output layer,
    we will map the features to both the input and output layers. The number of units
    in the hidden layers is usually different from the number of units in the input
    layers, which forces the network to either expand or reduce the number of original
    features. This way, the network will learn the important features, while effectively
    applying dimensionality reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example network is shown in the following diagram. The three-unit input
    layer is first expanded into a four-unit layer and then compressed into a single-unit
    layer. The other side of the network restores the single layer unit back in to
    the four-unit layer, and then to the original three-input layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f229c5d1-e298-4e0f-b8e0-eaf36699f2be.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the network is trained, we can take the left-hand side to extract the image
    features like we would with traditional image processing. It consists of encoders
    and decoders, where the encoder's work is to create or make hidden a layer or
    layers that captures the essence of the input, and the decoders reconstruct the input
    from the layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The autoencoders can be also combined into **stacked autoencoders**, as shown
    in the following diagram. First, we will discuss the hidden layer in a basic autoencoder,
    as described previously. Then, we will take the learned hidden layer (green circles)
    and repeat the procedure, which in effect learns a more abstract presentation.
    We can repeat this procedure multiple times, transforming the original features
    into increasingly reduced dimensions. At the end, we will take all of the hidden
    layers and stack them into a regular feedforward network, as shown at the top-right
    part of the diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e053137e-b22e-4ff8-ad2c-65eea70d5c4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Restricted Boltzmann machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**A restricted Boltzman machine** (**RBM**) is an undirected neural network,
    also denoted as **generative stochastic networks** (**GSNs**), and can learn probability
    distribution over its set of inputs. As the name suggests, they originate from
    the Boltzman machine, a recurrent neural network that was introduced in the eighties.
    In a Boltzmann machine, every node or neuron is connected with all other nodes,
    which makes it difficult to process when the node count increases. Restricted
    means that the neurons must form two fully connected layers, an input layer and
    a hidden layer, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4595eeb9-cbda-4f68-b845-a9f3b9bf91d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Unlike feedforward networks, the connections between the visible and hidden
    layers are undirected, hence the values can be propagated in both visible-to-hidden
    and hidden-to-visible directions.
  prefs: []
  type: TYPE_NORMAL
- en: Training RBMs is based on the contrastive divergence algorithm, which uses a
    gradient descent procedure, similar to backpropagation, to update weights, and
    Gibbs sampling is applied on the Markov chain to estimate the gradient, that is
    the direction on how to change the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'RBMs can also be stacked to create a class known as **deep belief networks**
    (**DBNs**). In this case, the hidden layer of an RBM acts as a visible layer for
    the RBM layer, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3710f522-4873-439c-a0b0-89625751661a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The training, in this case, is incremental: training layer by layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep convolutional networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A network structure that recently achieved very good results at image recognition
    benchmarks is the **convolutional neural network** (**CNN**) or ConvNet. CNNs
    are a type of feedforward neural network that are structured in such a way that
    it emulates the behavior of the visual cortex, exploiting 2D structures of an
    input image, that is, patterns that exhibit spatially local correlation. It works
    on the basic principles of how the brain recalls or remembers images. We, as humans,
    remember images on the basis of features only. Given the features, our brain will
    start forming the image itself. In computers, consider the following diagram,
    which shows how a feature is detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28592e50-146d-45af-9de0-426d399d967f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the same way, many features are detected from the image, as shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26759343-9494-4e76-a340-39649c60028c.png)'
  prefs: []
  type: TYPE_IMG
- en: A CNN consists of a number of convolutional and subsampling layers, optionally
    followed by fully connected layers. An example of this shown in the following
    diagram. The input layer reads all of the pixels in an image and then we apply
    multiple filters. In the following diagram, four different filters are applied.
    Each filter is applied to the original image; for example, one pixel of a 6 x
    6 filter is calculated as the weighted sum of a 6 x 6 square of input pixels and
    corresponding 6 x 6 weights. This effectively introduces filters that are similar
    to standard image processing, such as smoothing, correlation, edge detection,
    and so on. The resulting image is called a **feature map**. In the example in
    the following diagram, we have four feature maps, one for each filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next layer is the subsampling layer, which reduces the size of the input.
    Each feature map is subsampled typically with mean or max pooling over a contiguous
    region of 2 x 2 (up to 5 x 5 for large images). For example, if the feature map
    size is 16 x 16 and the subsampling region is 2 x 2, the reduced feature map size
    is 8 x 8, where 4 pixels (a 2 x 2 square) are combined into a single pixel by
    calculating the max, min, mean, or some other functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb58d84f-21ad-423b-b211-fd1e634e2a48.png)'
  prefs: []
  type: TYPE_IMG
- en: The network may contain several consecutive convolution and subsampling layers,
    as shown in the preceding diagram. A particular feature map is connected to the
    next reduced/convoluted feature map, while feature maps at the same layer are
    not connected to each other.
  prefs: []
  type: TYPE_NORMAL
- en: After the last subsampling or convolutional layer, there is usually a fully
    connected layer, identical to the layers in a standard multilayer neural network,
    which represents the target data.
  prefs: []
  type: TYPE_NORMAL
- en: A CNN is trained using a modified backpropagation algorithm that takes the subsampling
    layers into account and updates the convolutional filter weights based on all
    the values where this filter is applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some good CNN designs can be found at the ImageNet competition results page:
    [http://www.image-net.org/](http://www.image-net.org/). An example is *AlexNet*,
    which is described in the *ImageNet Classification with Deep Covolutional Neural
    Networks* paper by *A. Krizhevsky et al*.'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our review of the main neural network structures. In the following
    section, we'll move on to the actual implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss how to implement some of the neural network
    structures with the Deeplearning4j library. Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Deeplearning4j
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 2](6fd557d7-2807-4a6d-8f93-d7c4ca094b7e.xhtml),
    *Java Libraries and Platforms for Machine Learning*, Deeplearning4j is an open
    source, distributed deep learning project in Java and Scala. Deeplearning4j relies
    on Spark and Hadoop for MapReduce, trains models in parallel, and iteratively
    averages the parameters they produce in a central model. A detailed library summary
    is presented in [Chapter 2](6fd557d7-2807-4a6d-8f93-d7c4ca094b7e.xhtml), *Java
    Libraries and Platforms for Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting DL4J
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most convenient way to get Deeplearning4j is through the Maven repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a new Eclipse project and pick Maven Project, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8aed3b3c-3950-4926-b37c-4bbfc82bc7df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Open the `pom.xml` file and add the following dependencies under the `<dependencies>`
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Finally, right-click on Project, select Maven, and pick Update project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most famous datasets is the MNIST dataset, which consists of handwritten
    digits, as shown in the following image. The dataset consists of 60,000 training
    and 10,000 test images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3340f60-5ca7-425a-bb00-97edef834272.png)'
  prefs: []
  type: TYPE_IMG
- en: The dataset is commonly used in image recognition problems to benchmark algorithms.
    The worst recorded error rate is 12%, with no preprocessing and using an SVM in
    a one-layer neural network. Currently, as of 2016, the lowest error rate is only
    0.21%, using the **DropConnect** neural network, followed by a deep convolutional
    network at 0.23%, and a deep feedforward network at 0.35%.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at how to load the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deeplearning4j provides the MNIST dataset loader out of the box. The loader
    is initialized as `DataSetIterator`. First let''s import the `DataSetIterator`
    class and all of the supported datasets that are part of the `impl` package, for
    example, iris, MNIST, and others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll define some constants, for instance, the images consist of 28
    x 28 pixels and there are 10 target classes and 60,000 samples. We''ll initialize
    a new `MnistDataSetIterator` class that will download the dataset and its labels.
    The parameters are the iteration batch size, total number of examples, and whether
    the datasets should be binarized or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Having an already-implemented data importer is really convenient, but it won't
    work on your data. Let's take a quick look at how it is implemented and what needs
    to be modified to support your dataset. If you're eager to start implementing
    neural networks, you can safely skip the rest of this section and return to it
    when you need to import your own data.
  prefs: []
  type: TYPE_NORMAL
- en: To load the custom data, you'll need to implement two classes: `DataSetIterator`,
    which holds all of the information about the dataset, and `BaseDataFetcher`, which
    actually pulls the data either from a file, database, or the web. Sample implementations
    are available on GitHub at [https://github.com/deeplearning4j/deeplearning4j/tree/master/deeplearning4j-core/src/main/java/org/deeplearning4j/datasets/iterator/impl](https://github.com/deeplearning4j/deeplearning4j/tree/master/deeplearning4j-core/src/main/java/org/deeplearning4j/datasets/iterator/impl).
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to use the **Canova** library, which was developed by the
    same authors, at [http://deeplearning4j.org/canovadoc/](http://deeplearning4j.org/canovadoc/).
  prefs: []
  type: TYPE_NORMAL
- en: Building models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll discuss how to build an actual neural network model.
    We'll start with a basic single-layer neural network to establish a benchmark
    and discuss the basic operations. Later, we'll improve this initial result with
    DBN and a multilayer convolutional network.
  prefs: []
  type: TYPE_NORMAL
- en: Building a single-layer regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by building a single-layer regression model based on the softmax
    activation function, as shown in the following diagram. As we have a single layer,
    **Input** to the neural network will be all the figure pixels, that is, 28 x 28
    = **748** neurons. The number of **Output** neurons is **10**, one for each digit.
    The network layers are fully connected, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a5ce7ad-1fe2-46db-95f4-8e456cefd72d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A neural network is defined through a `NeuralNetConfiguration.Builder()` object
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define the parameters for gradient search in order to perform iterations
    with the conjugate gradient optimization algorithm. The `momentum` parameter determines
    how fast the optimization algorithm converges to an local optimum. The higher
    the `momentum`, the faster the training; but higher speed can lower the model''s
    accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will specify that the network has one layer and also define the error
    function, `NEGATIVELOGLIKELIHOOD`, internal perceptron activation function, `softmax`,
    and the number of input and output layers that correspond to the total
  prefs: []
  type: TYPE_NORMAL
- en: 'image pixels and the number of target variables, as shown in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will set the network to `pretrain`, disable backpropagation, and
    actually build the untrained network structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the network structure is defined, we can use it to initialize a new `MultiLayerNetwork`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will point the model to the training data by calling the `setListeners`
    method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also call the `fit(int)` method to trigger end-to-end network training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To evaluate the model, we will initialize a new `Evaluation` object that will
    store batch results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then iterate over the dataset in batches in order to keep the memory
    consumption at a reasonable rate and store the results in an `eval` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can get the results by calling the `stats()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'A basic one-layer model achieves the following accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Getting 89.22% accuracy, that is, a 10.88% error rate, on the MNIST dataset
    is quite bad. We'll improve this by going from a simple one-layer network to the
    moderately sophisticated deep belief network using Restricted Boltzmann machines
    and a Multilayer Convolutional Network.
  prefs: []
  type: TYPE_NORMAL
- en: Building a deep belief network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll build a deep belief network (DBN) based on the RBM,
    as shown in the following diagram. The network consists of four layers. The first
    layer recedes the **748** inputs to **500** neurons, then to **250**, followed
    by **200**, and finally to the last **10** target values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7859d3a0-9129-4f63-9274-6e95a9819d6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the code is the same as in the previous example, let''s take a look at how
    to configure such a network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define the gradient optimization algorithm, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also specify that our network will have four layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The input to the first layer will be `748` neurons and the output will be `500`
    neurons. We''ll use the root mean squared error cross entropy, and the Xavier
    algorithm to initialize weights by automatically determining the scale of initialization
    based on the number of input and output neurons, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The next two layers will have the same parameters, except for the number of
    input and output neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the last layer will map the neurons to outputs, where we''ll use the `softmax`
    activation function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the training and evaluation is the same as in the single-layer network
    example. Note that training a deep network might take significantly more time
    compared to a single-layer network. The accuracy should be around 93%.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at another deep network.
  prefs: []
  type: TYPE_NORMAL
- en: Building a multilayer convolutional network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this final example, we''ll discuss how to build a convolutional network,
    as shown in the following diagram. The network will consist of seven layers. First,
    we''ll repeat two pairs of convolutional and subsampling layers with max pooling.
    The last subsampling layer is then connected to a densely connected feedforward
    neuronal network, consisting of 120 neurons, 84 neurons, and 10 neurons in the
    last three layers, respectively. Such a network effectively forms the complete
    image recognition pipeline, where the first four layers correspond to feature
    extraction and the last three layers correspond to the learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56a26eae-f5f1-4d22-b174-2c2c43484a8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Network configuration is initialized as we did earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We will specify the gradient descent algorithm and its parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also specify the seven network layers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The input to the first convolutional layer is the complete image, while the
    output is six feature maps. The convolutional layer will apply a 5 x 5 filter,
    and the result will be stored in a 1 x 1 cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The second layer is a subsampling layer that will take a 2 x 2 region and store
    the max result in a 2 x 2 element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The next two layers will repeat the previous two layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will wire the output of the subsampling layer into a dense feedforward
    network, consisting of `120` neurons, and then through another layer, into `84` neurons,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The final layer connects `84` neurons with `10` output neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: To train this structure, we can reuse the code that we developed in the previous
    two examples. Again, the training might take some time. The network accuracy should
    be around 98%.
  prefs: []
  type: TYPE_NORMAL
- en: Since model training significantly relies on linear algebra, training can be
    significantly sped up by using a **graphics processing unit** (**GPU**) for an
    order of magnitude. As the GPU backend, at the time of writing this book, is undergoing
    a rewrite, please check the latest documentation at [http://deeplearning4j.org/documentation](http://deeplearning4j.org/documentation).
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in different examples, increasingly complex neural networks allow
    us to extract relevant features automatically, thus completely avoiding traditional
    image processing. However, the price we pay for this is an increased processing
    time and a lot of learning examples to make this approach efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to recognize patterns in images in order to
    distinguish between different classes by covering fundamental principles of deep
    learning and discussing how to implement them with the Deeplearning4j library.
    We started by refreshing the basic neural network structure and discussed how
    to implement them to solve handwritten digit recognition problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look into patterns further; however, instead of patterns
    in images, we'll tackle patterns with temporal dependencies, which can be found
    in sensor data.
  prefs: []
  type: TYPE_NORMAL
