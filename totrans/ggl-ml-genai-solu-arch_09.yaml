- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature Engineering and Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will dive progressively deeper into the kinds of data processing
    steps that are common in many data science projects, and how to perform those
    steps using Vertex AI in Google Cloud. We’ll begin this chapter by taking a more
    detailed look at how features are used in machine learning workloads, and what
    kinds of challenges often arise concerning how features are used.
  prefs: []
  type: TYPE_NORMAL
- en: We will then transition our discussion to focus on how to address those challenges,
    and how to use our machine learning features effectively in Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental concepts related to dimensions or features in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to the curse of dimensionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertex AI Feature Store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fundamental concepts in this chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll briefly cover concepts that provide additional context
    for this chapter’s learning activities.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensions and features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduced the concept of features in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015)
    and described examples of features using the King County housing sales dataset
    for illustration. To briefly recap, features are individual, measurable properties
    or characteristics of the observations in our dataset. They are the aspects of
    our dataset from which a machine learning algorithm learns to create a model.
    In other words, a model can be seen as a representation of patterns learned by
    the algorithm from the features in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The features of a house, for example, include information such as how many
    rooms it contains, the year it was constructed, where it is located, and other
    factors that describe the house, as depicted in *Table 7.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 7.1: King County house sales features ](img/B18143_01_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 7.1: King County house sales features'
  prefs: []
  type: TYPE_NORMAL
- en: When we’re dealing with tabular data, features are generally represented as
    columns in our dataset, and each row represents an individual data point or observation,
    sometimes referred to as an **instance**.
  prefs: []
  type: TYPE_NORMAL
- en: Features are also referred to as variables, attributes, or dimensions. So, when
    we talk about the dimensionality of a dataset, it relates to how many features
    or dimensions our dataset has, and how that affects our machine learning workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting, underfitting, and regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We briefly discussed the concepts of overfitting and underfitting in [*Chapter
    2*](B18143_02.xhtml#_idTextAnchor035), and we will continue to revisit these topics
    in more detail throughout this book since they are so fundamentally important
    to the process of machine learning. In this section, we’ll discuss how the number
    of features in our dataset can affect how our algorithms learn from our data.
    A key concept to keep in mind is that overfitting and underfitting can be strongly
    influenced by how many observations we have in our dataset, and how many features
    we have for each observation.
  prefs: []
  type: TYPE_NORMAL
- en: We usually need to find the right balance between these two aspects of our dataset.
    For example, if we have very few observations and a lot of features for each observation,
    our model is likely to overfit the dataset because it learns very specific patterns
    for those observations and their features, but it cannot generalize well to new
    observations. Conversely, if we have many observations, but very few pieces of
    information (that is, features) for each observation, then our model may not be
    able to learn any valuable patterns, meaning it will underfit our dataset. Because
    of this, reducing the number of features can help reduce overfitting, but only
    to an extent – removing too many features may result in underfitting. Also, we
    don’t want to remove features that contain useful information for our models to
    learn, so another way that we can address overfitting while keeping many of our
    features is to use a mechanism called regularization. We also briefly mentioned
    this in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035), and it’s something we
    will discuss in more detail here.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To begin our discussion on regularization, we need to bring up the concept of
    the loss function in machine learning once more, something we introduced in [*Chapter
    1*](B18143_01.xhtml#_idTextAnchor015). Remember that many machine learning algorithms
    work by trying to find the best coefficients (or weights) for each feature that
    result in the closest approximation of the target feature. So, overfitting is
    influenced by the mathematical relationship between the features and their coefficients.
    If we find that a model is overfitting to specific features, we can use regularization
    to reduce the influence of those features and their coefficients on the model.
  prefs: []
  type: TYPE_NORMAL
- en: Because overfitting usually happens when a model is too complex, such as having
    too many features relative to the number of observations, regularization addresses
    this issue by adding a penalty to the loss function, which discourages the model
    from assigning too much importance to any feature. This helps improve the generalizability
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to implement regularization in machine learning, but
    I’ll explain two of the most common types here – that is, **L1** and **L2** regularization
    – as well as a combination of both approaches, referred to as **elastic net**.
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This type of regularization is also known as **Lasso** regularization, and
    it works by adding a penalty equivalent to the L1 norm (or the absolute value)
    of the coefficients or weights in the cost function by using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Cost Function + λ * |weights|
  prefs: []
  type: TYPE_NORMAL
- en: Here, λ is the regularization parameter, which controls the strength of the
    penalty and can be considered a hyperparameter whose optimal value can vary depending
    on the problem. Note that if the penalty is too strong, it can result in underfitting,
    so it’s important to find the right balance in this regard.
  prefs: []
  type: TYPE_NORMAL
- en: The effect of L1 regularization is to shrink some of the model’s coefficients
    to exactly zero, effectively excluding the corresponding feature from the model,
    which makes L1 regularization useful for feature selection (something we’ll cover
    in more detail shortly) when dealing with high-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This type of regularization is also known as **Ridge** regularization. This
    method adds a penalty equivalent to the L2 norm (or the square) of the coefficients
    in the cost function by using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Cost Function + λ * (weights^2)
  prefs: []
  type: TYPE_NORMAL
- en: Unlike L1 regularization, L2 regularization doesn’t result in the exclusion
    of features but rather pushes the coefficients close to zero, distributing the
    weights evenly among the features. This can be beneficial when we’re dealing with
    correlated features as it allows the model to keep all of them under consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic net
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Elastic net, as a combination of L1 and L2 regularization, was designed to
    provide a compromise between these two methods, incorporating the strengths of
    both. Like L1 and L2 regularization, elastic net adds a penalty to the loss function,
    but instead of adding an L1 penalty or an L2 penalty, it adds a weighted sum of
    both by using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Cost Function + λ1 * |weights| + λ2 * (weights^2)
  prefs: []
  type: TYPE_NORMAL
- en: Here, λ1 and λ2 are hyperparameters that control the strength of the L1 and
    L2 penalties, respectively. If λ1 is zero, elastic net reduces to Ridge regression,
    and if λ2 is zero, it reduces to Lasso regression.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic net has the feature selection capability of L1 regularization (since
    it can shrink coefficients to zero) and the regularization strength of L2 regularization
    (since it can distribute weights evenly among correlated features). The trade-off
    with elastic net is that it has two hyperparameters to tune, rather than just
    one in the case of Lasso or Ridge, which can make the model more complex and computationally
    intensive to train.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the important topic of regularization in more detail,
    let’s dive into feature selection and feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection and feature engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We briefly discussed feature engineering in previous chapters of this book,
    but we will explore these concepts in more detail here. In [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035),
    we used the example of creating a new feature named `price-per-square-foot` in
    our housing data by dividing the total cost of each house by the total area of
    that house in square feet. We will explore many additional examples of feature
    engineering in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: However, creating new features from existing ones is not the only type of activity
    we need to perform on our features when preparing our dataset for training a machine
    learning model. We also need to select which features we think could be most important
    for achieving the task that we want our model to achieve, such as predicting the
    price of a house. As we saw in [*Chapter 6*](B18143_06.xhtml#_idTextAnchor187),
    we may also need to perform transformations on our features, such as ensuring
    that they are all represented on a common, standardized scale.
  prefs: []
  type: TYPE_NORMAL
- en: The goal in selecting and engineering features is to provide our model with
    the most relevant information, in the most digestible format, so that it can most
    effectively learn from the data.
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: High-dimensional datasets contain many dimensions or features for each observation
    in the dataset. It would be possible to presume that, the more features we include
    for each data point, the more information our model will learn, and therefore,
    the more accurate our model will be. However, note that not all features are equally
    useful. Some may contain little to no useful information for our model, others
    may contain redundant information, and some may even be harmful to the model’s
    ability to learn. Part of the art and science of machine learning is figuring
    out which features to use and how to prepare them in a way that allows the model
    to perform its best.
  prefs: []
  type: TYPE_NORMAL
- en: Also, bear in mind that, the more information we have in our dataset, the more
    information our model has to process. This directly translates to additional computing
    resources being required for our machine learning algorithm to process our dataset,
    which, in turn, directly translates to longer model training times, and increased
    monetary cost. Too much irrelevant data or **noise** in the dataset can also make
    it harder for the algorithm to identify (that is, to learn) patterns in the data.
    The ideal scenario, then, is to find the minimum number of features that provide
    the maximum amount of useful information to our model. If we can achieve the same
    results with three features or ten features, for example, it’s generally better
    to go with the option of using three features. The “maximum amount of useful information”
    can be measured in terms of **variance**, whereby features with high relative
    variance are those that influence our model’s outcomes most prominently, and features
    with little relative variance are often not as useful in training our model to
    identify patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The “curse of dimensionality” is a term that’s used in the data science industry
    to describe the challenges that arise when dealing with datasets that contain
    higher numbers of dimensions. Let’s take a look at what some of these challenges
    are. In subsequent sections, we will discuss mechanisms to address these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is an important point at which we can introduce the link between “dimensions”
    in our dataset, and the dimensions of physical space. As we all know, and as mentioned
    earlier in this book, humans can only perceive our physical world in up to a maximum
    of three dimensions (width, height, and depth), with “time” considered as the
    fourth dimension of our physical reality. For datasets that have two or three
    dimensions, we can easily create visualizations representing various aspects of
    those datasets, but we can’t create graphs or other visual representations of
    higher-dimensional datasets. In such cases, it helps if we can try to find other
    ways of visually interpreting those datasets, such as by projecting them down
    into lower-dimensional representations, something we will explore in more detail
    shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Feature sparsity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In high-dimensional space, points (that is, instances or samples) in our dataset
    tend to be far away from each other, leading to sparsity. Generally, as the number
    of features increases relative to the number of observations in our dataset, the
    feature space becomes increasingly sparse, and this sparsity makes it more difficult
    for algorithms to learn from the data since they have fewer examples to learn
    from in the vicinity of any given point.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s imagine that our dataset consists of information regarding
    a company’s customers, in which case each instance in the dataset represents a
    person, and each feature represents some characteristic of a person. If our dataset
    stores hundreds or even thousands of characteristics, then it’s unlikely that
    all characteristics for every person will be populated. As such, the overall feature
    space for our dataset will be sparsely populated. On the other hand, if we have
    fewer features, this is less likely to occur.
  prefs: []
  type: TYPE_NORMAL
- en: Distance measurements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In previous chapters, we talked about how Euclidean distance is used in many
    machine learning algorithms to find potential relationships or differences between
    data points in our dataset. One of the most direct examples of this concept that
    we’ve explored already is the K-means clustering algorithm. In high-dimensional
    spaces, distances can sometimes become less meaningful, because the difference
    between the maximum and minimum possible distances becomes increasingly smaller.
    This means that traditional distance metrics, such as Euclidean distance, also
    become less meaningful, which can particularly affect algorithms that rely on
    distance, such as **k-nearest neighbors** (**kNN**) or clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting and increased data requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Referring back to our discussion of how overfitting or underfitting are influenced
    by the ratio of observations to features in our dataset, high-dimensional datasets
    are more prone to overfitting unless we have enormous amounts of observations
    to help our model generalize. Bear in mind what we said earlier in this chapter,
    regarding the relationship between the amount of data our algorithms need to process
    and the cost of training our models. Datasets with lots of observations and lots
    of features will be more expensive to train and manage.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability and explainability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interpretability and explainability refer to our ability to understand and explain
    how our machine learning models work. This is important for numerous reasons,
    all of which we will discuss briefly here. Firstly, a lack of understanding of
    how our models work inhibits our ability to improve those models. However, more
    importantly, we need to ensure that our models are as fair and unbiased as possible,
    and this is where explainability plays a crucial role. If we cannot explain why
    our models are producing specific results, then we cannot adequately assess their
    fairness. Generally, the higher the dimensionality of our dataset, the more complex
    our models tend to be, which can directly affect (that is, reduce) interpretability
    and explainability.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve reviewed some of the common challenges associated with high-dimensional
    datasets, let’s take a look at some mechanisms to address those challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you might imagine, one of the first ways to address the challenge of having
    too many dimensions is to reduce the number of dimensions, and there are two main
    types of techniques we can use for this purpose: feature selection and feature
    projection.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This involves selecting a subset of the original features. There are several
    strategies for feature selection, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Filter methods, which rank features based on statistical measures and select
    a subset of features with the highest ranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapper methods, which evaluate multiple models using different subsets of input
    features and select the subset that results in the highest model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedded methods, which use machine learning algorithms that have built-in feature
    selection methods (such as Lasso regularization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s also important to understand that the feature projection methods we will
    discuss can be used to help select the most important subset of features from
    our dataset, so there is some overlap between these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Feature projection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With feature projection, we use mathematical transformations to project our
    features down into a lower-dimensional space. In this section, we’ll introduce
    three popular feature projection techniques: **Principal Component Analysis**
    (**PCA**), **Linear Discriminant Analysis** (**LDA**), and **t-distributed Stochastic
    Neighbor** **Embedding** (**t-SNE**).'
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'PCA is an **unsupervised** algorithm that aims to reduce the dimensionality
    of our feature space while maintaining as much of the original data’s variance
    as possible. In the high-dimensional data space, PCA identifies the axes (principal
    components) along which the variation in the data is maximized. These principal
    components are orthogonal, meaning they’re at right angles to each other in this
    multi-dimensional space. The **first principal component** (**PC1**) captures
    the direction of the greatest variance in the data. The **second principal component**
    (**PC2**) captures the maximum amount of remaining variance while being orthogonal
    to PC1, and so on. The process of PCA generally involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Standardize the data if the features have different scales. This is important
    because PCA is sensitive to feature scales, whereby features with larger scales
    could mistakenly be perceived as more dominant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate a covariance matrix to understand how different features vary together.
    The covariance matrix is a square matrix that contains the covariances between
    each pair of features. The covariance between two features measures how those
    features vary together: a positive covariance indicates that the features increase
    or decrease together, while a negative covariance indicates that one feature increases
    while the other decreases.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the **eigenvalues** and **eigenvectors** of the covariance matrix.
    The eigenvectors represent the directions or components of the new space, and
    the eigenvalues represent the magnitude or explained variance for each component.
    The eigenvectors are often called the principal components of the data, and they
    form a basis for the new feature space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the eigenvalues and their corresponding eigenvectors. After computing the
    eigenvalues and their associated eigenvectors, the next step is to sort the eigenvalues
    in descending order. The eigenvector with the highest corresponding eigenvalue
    is PC1\. The eigenvector with the second highest corresponding eigenvalue is PC2,
    and so on. The reason for this ordering is that the significance of each eigenvector
    is given by the magnitude of its eigenvalue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a subset of the principal components. PCA creates as many principal components
    as there are variables in the original dataset. However, since the goal of PCA
    is dimensionality reduction, we usually select a subset of the principal components,
    referred to as the **top k** principal components, which capture the most variance
    in the data. This step is what reduces dimensionality because the smaller eigenvalues
    and their vectors are dropped.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the original data. The final step in PCA is to transform the original
    data into the reduced subspace defined by the selected principal components, which
    is done by multiplying the original data matrix by the matrix of the top *k* eigenvectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The transformed data is now ready to be used for further analysis and visualization
    (as depicted in *Figure 7**.1*), or used as input to a machine learning algorithm.
    Importantly, the reduced dataset retains as much of the variance in the original
    data as possible (given the reduced number of dimensions):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: PCA visualization of European genetic structure (source: https://commons.wikimedia.org/wiki/File:PCA_plot_of_European_individuals.png)](img/B18143_07_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: PCA visualization of European genetic structure (source: https://commons.wikimedia.org/wiki/File:PCA_plot_of_European_individuals.png)'
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a powerful technique with many uses, but it also has limitations. For
    example, it assumes that the principal components are a linear combination of
    the original features. If this is not the case (that is, if the underlying structure
    in the data is non-linear), then PCA may not be the best dimensionality reduction
    technique to use. It’s also worth noting that the principal components are less
    interpretable than the original features – they don’t have an intuitive meaning
    in terms of the original features.
  prefs: []
  type: TYPE_NORMAL
- en: LDA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LDA is a **supervised** algorithm that aims to find a linear combination of
    features that best separates classes of objects. The resulting combination can
    then be used for dimensionality reduction. It involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the class means. For each class in the dataset, calculate the mean vector,
    which is simply the average of all vectors in that class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the within-class covariance matrix, which measures how the individual
    classes are dispersed around their respective means.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the between-class covariance matrix, which measures how the class means
    are dispersed around the overall mean of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the linear discriminants, which are the directions in the feature space
    along which the classes are best separated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the linear discriminants. Just like in PCA, the eigenvectors are sorted
    by their corresponding eigenvalues in descending order. The eigenvalues represent
    the amount of the data’s variance that is accounted for by each discriminant.
    The first few linear discriminants, corresponding to the largest eigenvalues,
    are the ones that account for the most variance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the data is projected onto the space spanned by the first few linear
    discriminants.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This results in a lower-dimensional representation of the data where the classes
    are maximally separated. We can visualize this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2: LDA plots of wine varieties](img/B18143_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: LDA plots of wine varieties'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the main assumption of LDA is that the classes have
    identical covariance matrices. If this assumption is not met, LDA might not perform
    well.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This method has perhaps the coolest name of them all. It’s an **unsupervised**,
    non-linear dimensionality reduction algorithm that is particularly well-suited
    for embedding high-dimensional data into a space of two or three dimensions while
    aiming to keep similar instances close and dissimilar instances apart. It does
    this by mapping the high-dimensional data to a lower-dimensional space in a way
    that retains much of the relative distances between points. It involves the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute similarities in the high-dimensional space**: t-SNE begins by calculating
    the probability that pairs of data points in the high-dimensional space are similar.
    Points that are close to each other have a higher probability of being picked,
    while points that are far away have a lower probability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Computing similarities in low-dimensional space**: t-SNE then calculates
    the probabilities of similarity for pairs of points in the low-dimensional representation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Optimization**: Finally, t-SNE uses gradient descent to minimize the difference
    between the probabilities in the high-dimensional and low-dimensional spaces.
    The goal is to have similar objects modeled by nearby points and dissimilar objects
    modeled by distant points in the low-dimensional space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result of t-SNE is a map that reveals the structure of the high-dimensional
    data in a way that it’s easier for humans to comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted that while t-SNE is excellent for visualization and can
    reveal clusters and structure in your data (as depicted in *Figure 7**.3*), it
    doesn’t provide explicit information about the importance or meaning of the features
    in your data like PCA does. It’s more of an exploratory tool that can help in
    dimensionality reduction, rather than a formal dimensionality reduction technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3: t-SNE visualization of digits dataset](img/B18143_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: t-SNE visualization of digits dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered some of the most popular feature projection techniques,
    let’s discuss one more set of important concepts regarding how the features in
    our datasets influence model training.
  prefs: []
  type: TYPE_NORMAL
- en: Using PCA and LDA for dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll start our hands-on activities in this chapter with dimensionality reduction
    using PCA and LDA. We can use the wine dataset within scikit-learn as an example.
    I always wish I could impress my friends by being a wine expert, but I can barely
    tell a $10 bottle from a $500 bottle, so instead, I’ll use data science to develop
    impressive knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: The wine dataset is an example of a multivariate dataset that contains the results
    of a chemical analysis of wines grown in the same region in Italy but derived
    from three different types of grapes (referred to as `cultivars`). The analysis
    focused on quantifying 13 constituents found in each of the three types of wines.
  prefs: []
  type: TYPE_NORMAL
- en: Using PCA on this dataset will help us to understand the important features.
    By looking at the weights of the original features in the principal components,
    we can see which features contribute most to the variability in the wine dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we can use the same Vertex AI Workbench Notebook Instance that we created
    in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168) for this purpose. Please open
    JupyterLab on that notebook instance and perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the directory explorer on the left-hand side of the screen, navigate to the
    `Chapter-07` directory and open the `dimensionality-reduction.ipynb` notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Python (Local**) as the kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run each cell in the notebook by selecting the cell and pressing *Shift* + *Enter*
    on your keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code in the notebook performs the following activities:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it imports the necessary libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it loads the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, it standardizes the features of the wine dataset. It applies PCA to reduce
    the dimensionality to two dimensions (that is, the first two principal components).
    This is done using the `fit_transform()` method, which fits the PCA model to the
    data and then transforms the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, it visualizes the data in the space of those two principal components,
    coloring the points according to the type of wine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting visualization should look similar to what’s shown in *Figure
    7**.4*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.4: PCA scatter plot for the wine dataset](img/B18143_07_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: PCA scatter plot for the wine dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The scatter plot should show a clear separation between the different types
    of wine, which suggests that the type of wine is closely related to its chemical
    constituents. Moreover, the PCA object (named `pca` in our code) stores the `components_`
    attribute, which contains the mappings of each feature concerning the principal
    components. By examining these, we can find out which features are the most important
    in distinguishing between the types of wine.
  prefs: []
  type: TYPE_NORMAL
- en: Each data point in the visualization represents a single sample from our dataset,
    but instead of being plotted in the original, high-dimensional feature space,
    it’s plotted in the lower-dimensional space defined by the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of the wine dataset, each data point in the PCA visualization
    represents a single wine sample. Because we have mapped our original 13 features
    to two PCA dimensions, the position of each point on the *X* and *Y* axes corresponds
    to the values of the first and second principal components for that wine sample.
  prefs: []
  type: TYPE_NORMAL
- en: The color of each point represents the true class of the wine sample (derived
    from three different cultivars). By coloring the points according to their true
    class, you can see how well the PCA transformation separates the different classes
    in the reduced dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that each principal component is a linear combination of the original
    features, so the position of each point is still determined by the values of its
    original features. In this way, PCA enables us to visualize the high-dimensional
    data, and it highlights the dimensions of greatest variance, which are often the
    most informative.
  prefs: []
  type: TYPE_NORMAL
- en: We can then access the `components_` attribute of the fitted PCA object to view
    the individual components. This attribute returns a matrix where each row corresponds
    to a principal component and each column corresponds to an original feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the following code will print a table, where the values in the table represent
    the weights of each feature in each component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The results should look similar to what’s shown in *Table 7.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 7.2: PCA components and features for the wine dataset](img/B18143_07_Table2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 7.2: PCA components and features for the wine dataset'
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the absolute values of these weights, we can determine which features
    are most important for each principal component. Large absolute values correspond
    to features that play a significant role in the variation captured by that principal
    component, and the sign (positive or negative) of the weight can tell us about
    the direction of the relationship between the feature and the principal component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s see how we could use LDA to identify the constituents that account
    for the most variance between the types of wine. Again, we’ll first import the
    necessary libraries and standardize the data. We’ll then apply LDA to the standardized
    features, specifying `n_components=2` to get a two-dimensional projection, and
    then fit the LDA model to the data and transform the data for the first two LDA
    components. Finally, we’ll visualize the transformed data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting visualization should look similar to what’s shown in *Figure
    7**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5: LDA scatter plot for the wine dataset](img/B18143_07_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: LDA scatter plot for the wine dataset'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the scatter plot shows the data points in the space of the first
    two LDA components, and the points are again colored according to the type of
    wine. We should also see a clear separation between the different types of wine,
    indicating that they have different distributions of their various chemical constituents.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did with the `components_` attribute of the fitted PCA object, we can
    inspect the `coef_` attribute of the fitted LDA object to view the most discriminative
    features, as shown in the following code and its respective output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The results should look similar to what’s shown in *Table 7.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 7.3: LDA classes and features for the wine dataset](img/B18143_07_Table3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 7.3: LDA classes and features for the wine dataset'
  prefs: []
  type: TYPE_NORMAL
- en: In the resulting table, each row corresponds to a class (compared to the rest),
    and each column corresponds to an original feature. So, the values in the table
    represent the coefficients of each feature in the context of the linear discriminants.
    Similar to our PCA assessment, large absolute values indicate features that contribute
    significantly to separating the classes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve looked at how to reduce the dimensionality of our datasets, let’s
    assume that we’ve identified our required features and begin to explore how we
    could further engineer features to ensure we have the best possible set of features
    to train our models.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering can constitute a large portion of a data scientist’s activities,
    and it can be just as important to their success, or sometimes even more important,
    than choosing the right machine learning algorithm. In this section, we will dive
    deeper into feature engineering, which can be considered both an art and a science.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the *Titanic* dataset available on OpenML ([https://www.openml.org/search?type=data&sort=runs&id=40945](https://www.openml.org/search?type=data&sort=runs&id=40945))
    for our examples in this section. This dataset contains information about passengers
    aboard the Titanic, including demographic data, ticket class, fare, and whether
    they survived the sinking of the ship.
  prefs: []
  type: TYPE_NORMAL
- en: In the `Chapter-07` directory in JupyterLab on your Vertex AI Workbench Notebook
    Instance, open the `feature-eng-titanic.ipynb` notebook and choose **Python (Local)**
    as the kernel. Again, run each cell in the notebook by selecting the cell and
    pressing *Shift* + *Enter* on your keyboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this notebook, the code performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it imports the necessary libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it loads the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, it performs some initial exploration to see what our dataset looks like.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it engineers new features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s take a look at each step in more detail, starting with importing the
    required libraries, and loading and exploring the dataset. We use the following
    code to perform those tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output from the `head()` method should look similar to what’s
    shown in *Table 7.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 7.4: Titanic dataset head() output](img/B18143_07_Table4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 7.4: Titanic dataset head() output'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fields in the dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`survived`: Indicates if a passenger survived or not. It’s a binary feature
    where 1 stands for survived and 0 stands for not survived.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pclass` (passenger class): Indicates the class of the passenger’s ticket.
    It has three categories: 1 for first class, 2 for second class, and 3 for third
    class. This could also indicate the socio-economic status of the passengers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: The name of the passenger.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sex`: The gender of the passenger; male or female.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`age`: The age of the passenger. Some ages are fractional, and for passengers
    less than 1 year old, the age is estimated as a fraction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sibsp`: The total number of the passengers’ siblings and spouses aboard the
    Titanic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parch`: The total number of the passengers’ parents and children aboard the
    Titanic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ticket`: The ticket number of the passenger.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fare`: The passenger fare – that is, how much the ticket cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cabin`: The cabin number where the passenger was staying. Some entries are
    NaN, indicating that the cabin number is missing from the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embarked`: The port where the passenger boarded the Titanic. C is for Cherbourg;
    Q is for Queenstown; S is for Southampton.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boat`: Which lifeboat the passenger was assigned to (if the passenger survived).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`body`: Body number (if the passenger did not survive and their body was recovered).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`home.dest`: The passenger’s home and destination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming that we want to use this dataset to build a model that could predict
    the likelihood of a passenger surviving based on the information recorded about
    them, let’s see if we can use some domain knowledge to assess which features are
    likely to be the biggest contributors to the outcome of surviving or not surviving,
    and whether we could use any data manipulation techniques to engineer more useful
    features.
  prefs: []
  type: TYPE_NORMAL
- en: '`Passenger Class` stands out as a potentially important feature to begin with
    because the first-class and second-class passengers had cabins on higher levels
    of the ship, which were closer to the lifeboats.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s unlikely that the passenger’s name would affect the outcome, nor their
    ticket number or port of embarkation. However, we could engineer a new feature
    named `Title` that’s extracted from the passengers’ names and could provide valuable
    information related to social status, occupation, marital status, and age, which
    might not be immediately apparent from the other features. We could also clean
    up this new feature by merging similar titles such as `Miss` and `Ms` and identifying
    elevated titles as `Distinguished`. The code to do that would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s consider the `Fare` and `Cabin` features. These could be somewhat
    correlated with class, but we will dive into these features in more detail. For
    the `Cabin` feature, we could extract another feature named `CabinClass`, which
    more clearly represents the class associated with each entry. We could do this,
    for example, by extracting the first letter from the cabin number, using it to
    represent the cabin class (for example, A, B, C, and so on), and storing it in
    the new `CabinClass` feature. The code to do that would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also ensure that we represent the fare as accurately as possible by considering
    that people may have purchased fares as families traveling together. To do this,
    we can create a new feature named `FamilySize` as a combination of the `SibSp`
    and `Parch` features (adding an additional “1” to account for the current passenger),
    and then compute `FarePerPerson` by dividing the `Fare` feature by the `FamilySize`
    feature by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Whether somebody is traveling alone or with their family could also affect
    their chances of survival. For example, family members could help each other when
    trying to get to the lifeboats. So, let’s create a feature from the `FamilySize`
    feature that identifies whether a passenger was traveling alone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s consider how age affects the likelihood of survival. People who
    are very young, or elderly, may, unfortunately, have less likelihood of surviving
    unless they have people to help them. However, we may not need yearly and fractional-yearly
    granularity when considering age in this context, and perhaps grouping people
    into age groups may be more effective. In that case, we can use the following
    code to create a new feature named `AgeGroup` that will group passengers by decades
    such as 0-9, 10-19, 20-29, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We also want to convert the categorical features into numerical values using
    one-hot encoding since machine learning models typically require numeric values.
    We could do this as follows (we need to do this for all of our categorical features):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s drop the features that we know will not be valuable for predicting
    the likelihood of survival (as well as the original features that we encoded,
    because only their encoded versions are needed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can take a quick peek at what our updated dataset looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output from the `head()` method should look similar to what’s
    shown in *Table 7.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 7.5: Output from the head() method for the updated dataset](img/B18143_07_Table5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 7.5: Output from the head() method for the updated dataset'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have an augmented dataset with engineered features that can
    be used to train a model. It’s important to bear in mind that any feature engineering
    steps we perform on our source dataset also need to be taken into account when
    we use our model to make predictions. This common need in machine learning is
    what gave rise to the requirement for Google Cloud to develop a service named
    Feature Store. We’ll explore this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Feature Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve done a lot of feature engineering work in this chapter. Bear in mind that
    we performed data transformations and engineered new features because we had reason
    to believe that the raw data was insufficient to train a machine learning model
    to suit our business case. This means that the raw data our model will see in
    the real world would usually not contain the enhancements we performed on the
    data during training. After all of that work, we would generally want to save
    the updated features we’ve engineered so that our model can reference them when
    it needs to make predictions. Vertex AI Feature Store was created for this purpose.
    We briefly mentioned Vertex AI Feature Store in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059),
    and in this section, we will dive into more detail regarding what it is and how
    we can use it to store and serve features for both training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Vertex AI Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s the official definition from the Google Cloud documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Feature Store is a managed, cloud-native feature store service that’s
    integral to Vertex AI. It streamlines your machine learning feature management
    and online serving processes by letting you manage your feature data in a BigQuery
    table or view. You can then serve features online directly from the BigQuery data
    source. Vertex AI Feature Store provisions resources that let you set up online
    serving by specifying your feature data sources. It then acts as a metadata layer
    interfacing with the BigQuery data sources and serves the latest feature values
    directly from BigQuery for online predictions at low latencies.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to storing and serving our features, Vertex AI Feature Store also
    integrates with Google Cloud Dataplex to provide feature governance capabilities,
    including the ability to track feature metadata such as feature labels and versions.
    In [*Chapter 13*](B18143_13.xhtml#_idTextAnchor328), we will dive into the importance
    of data governance and discuss how Dataplex can be used as an important component
    in building a robust governance framework.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it’s important to highlight that an entirely new version of Vertex
    AI Feature Store was launched in 2023\. As a result, there are now two different
    variants of the Feature Store service we can choose in Google Cloud, whereby the
    prior version is referred to as Vertex AI Feature Store (Legacy), and the new
    version is simply referred to as Vertex AI Feature Store. We will discuss both
    variants in this chapter, as well as some of the main distinctions between them.
    To provide context for the content in subsequent sections, I will briefly describe
    the topic of online versus offline feature serving.
  prefs: []
  type: TYPE_NORMAL
- en: Online versus offline feature serving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simply put, **online serving** refers to a scenario in which the interaction
    is happening in real time – that is, the requesting entity or client sends a request
    and synchronously waits for a response. In this scenario, latency needs to be
    reduced as much as possible. On the other side of the coin is **offline serving**,
    which refers to a scenario in which the requesting entity or client does not synchronously
    wait for a response, and the operation is allowed to happen over a longer period.
    In this case, latency is generally not a primary concern. This concept relates
    closely to the topic of online and offline inference, which we will cover in detail
    in [*Chapter 10*](B18143_10.xhtml#_idTextAnchor259).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of offline feature serving, Vertex AI Feature Store allows us to
    store and serve features directly in a Google Cloud BigQuery dataset. This is
    quite a convenient option as many Google Cloud customers already use BigQuery
    to store and analyze large amounts of their data.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of online feature serving, there are now two ways in which we can
    serve our features in Vertex AI Feature Store. The first option uses Google Cloud
    Bigtable to serve our features. Google Cloud Bigtable is a powerful service that
    is designed for serving large data volumes (terabytes of data).
  prefs: []
  type: TYPE_NORMAL
- en: The second option for online feature serving, which is referred to as **optimized
    online serving**, and was added as part of the new version of Vertex AI Feature
    Store, allows us to create an online store that is optimized specifically for
    serving feature data at ultra-low latencies.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing one option or the other depends on the needs of your use case, specifically
    whether you need to handle very large volumes of data or whether you need to serve
    your features with ultra-low latency. Cost is also a consideration in this decision,
    bearing in mind that the Bigtable solution generally costs less than the optimized
    online serving solution.
  prefs: []
  type: TYPE_NORMAL
- en: We will focus primarily on the optimized online serving approach in this chapter
    and the accompanying Jupyter Notebook. The following section dives deeper into
    the process of setting up online feature serving in Vertex AI Feature Store.
  prefs: []
  type: TYPE_NORMAL
- en: Online feature serving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At a high level, the following steps are required to set up online serving
    using Vertex AI Feature Store. We will elaborate on these steps in subsequent
    sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare data sources in BigQuery.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optional: Register data sources in the feature registry by creating feature
    groups and features.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Set up the online store and feature view resources to present the feature data
    sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Serve the latest online feature values from a feature view.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s take a look at these concepts in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Feature registry
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When using the optimized online serving approach, we can perform an optional
    step to register our features in the Vertex AI feature registry, which has also
    been added as a component of the new version of Vertex AI Feature Store.
  prefs: []
  type: TYPE_NORMAL
- en: This involves the process of creating resources referred to as **feature groups**,
    which represent logical groupings of feature columns and are associated with specific
    BigQuery source tables or views. In turn, feature groups contain resources referred
    to as **features**, which represent specific columns containing feature values
    within the data source represented by the feature group.
  prefs: []
  type: TYPE_NORMAL
- en: We can still serve features online even if we don’t add our BigQuery data sources
    to the feature registry, but note that the feature registry provides additional
    functionality, such as storing historical time series data associated with your
    features. As a result, we will use the feature registry in the practical exercises
    accompanying this chapter. Now, let’s take a look at the process of setting up
    online feature serving in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Online feature store and feature views
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After we have set up our feature data in BigQuery, and optionally registered
    feature groups and features in the feature registry, there are two main types
    of resources that we need to set up to enable online feature serving:'
  prefs: []
  type: TYPE_NORMAL
- en: An **online serving cluster instance**, which is referred to as the **online
    store**. Remember that we can either use Bigtable for online feature serving or
    the newly released optimized online feature serving option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One or more **feature view instances**, where each feature view is associated
    with a feature data source, such as a feature group in our feature registry (if
    we have chosen the option of registering our features in the feature registry),
    or a BigQuery source table or view.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After we create a feature view, we can configure synchronization settings to
    ensure that our feature data in BigQuery is synchronized with our feature view.
    We can trigger a synchronization manually, but if our source data is expected
    to be updated over time, then we can also configure a schedule to periodically
    refresh the contents of our feature view from the data source.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered many of the important concepts related to Vertex AI Feature
    Store, it’s time to dive in and build our very own feature store!
  prefs: []
  type: TYPE_NORMAL
- en: Building our feature store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will perform practical exercises that implement the concepts
    we learned about in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: Use our Vertex AI notebook to build the feature store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the Vertex AI Notebook Instance we created in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168),
    we can perform the following steps to build the feature store:'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Open JupyterLab** for the Vertex AI Notebook Instance we created in
    [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the JupyterLab opens, you should see a folder in your notebook called `Google-Machine-Learning-for-Solutions-Architects`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double-click on that folder, then double-click on the `Chapter-07` folder within
    it, and then double-click on the `feature-store.ipynb` file to open it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Select Kernel** screen that appears, select **Python (Local).**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Shift* + *Enter* to run each of the cells in the notebook and read the
    explanations in the Markdown and comments to understand what we’re doing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you’ve followed the steps in the notebook to perform feature selection
    and engineering, have built a feature store, and used some of the features to
    train a model, let’s take a look at how those features could be used during inference
    time. In later chapters, you will learn how to deploy models for online inference
    and send inference requests to those models, but for now, I’ll explain the process
    at a conceptual level.
  prefs: []
  type: TYPE_NORMAL
- en: How features are used during online inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, I’ll use the taxi fare prediction model use case that we built
    in the accompanying Jupyter Notebook as an example to explain how we can use features
    from our feature store during inference. We’ll take a look at each step in the
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Combining real-time and precomputed features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Features such as the current pickup time (`pickup_datetime`), `pickup_location`,
    and `passenger_count` can be obtained in real time as each taxi journey begins.
  prefs: []
  type: TYPE_NORMAL
- en: Our feature store also contains precomputed features, such as historical trip
    distances, fares per mile, pickup times, and locations. These features can be
    selected based on the current journey’s context from the available real-time features.
  prefs: []
  type: TYPE_NORMAL
- en: To get the precomputed features, the application handling the taxi journey can
    send a request to our feature store, passing identifiers such as the current time
    and location, after which the feature store can return the relevant feature values
    for these identifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Data assembly for prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, we can assemble the real-time data and fetched features into
    a feature vector matching the format expected by the model, and then pass the
    assembled feature vector to the model. The model then processes this vector and
    outputs a fare prediction, which can then be displayed in the app.
  prefs: []
  type: TYPE_NORMAL
- en: Well done! You have successfully built a feature store on Google Cloud. Let’s
    summarize everything we’ve discussed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how the quality of our features, and the ratio
    of features to observations in our dataset, influences how our algorithms learn
    from our data. We discussed challenges that can occur when our dataset contains
    many features and how to address those challenges by using mechanisms such as
    dimensionality reduction. We dived into details on dimensionality reduction techniques
    such as feature selection and feature projection, including algorithms such as
    PCA, LDA, and t-SNE, and we looked at examples of how to use some of these algorithms
    using hands-on activities.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we dived into feature engineering techniques in which we augmented a source
    dataset to create new features that contained information that was not readily
    available in the original dataset. Finally, we dived into Vertex AI Feature Store
    to learn about how we can use that service to store and serve our engineered feature
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will shift our focus away from the datasets and parameters
    that our models learn from and discuss different types of parameters that influence
    how our models learn. There, we’ll explore the concepts of hyperparameters and
    hyperparameter optimization.
  prefs: []
  type: TYPE_NORMAL
