<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Linear and Logistic Regression</h1>
                </header>
            
            <article>
                
<p class="mce-root">After the insights we gained by grouping similar information using common features, it's time to get a bit more mathematical and start to search for a way to describe the data by using a distinct function that will condense a large amount of information, and will allow us to predict future outcomes, assuming that the data samples maintain <span>their previous properties</span>.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Linear regression with a step-by-step implementation</li>
<li>Polynomial regression</li>
<li>Logistic regression and its implementation</li>
<li>Softmax regression</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression analysis</h1>
                </header>
            
            <article>
                
<p>This chapter will begin with an explanation of the general principles. So, let's ask the fundamental question: w<em>hat's regression?</em></p>
<p>Before all considerations, regression is basically a <strong>statistical process</strong><em><strong>.</strong></em> As we saw in the introductory section, regression will involve a set of data that has some particular probability distribution. In summary, we have a population of data that we need to characterize.</p>
<p>And what elements are we looking for in particular, in the case of regression? We want to determine the relationship between an independent variable and a dependent variable that optimally <span>adjusts </span>to the provided data. When we find such a function<em><strong> </strong></em>between the described variables, it will be called the <strong>regression function</strong>.</p>
<p>There are a large number of function types available to help us model our current data, the most common example <span>being</span> the linear, polynomial, and exponential.</p>
<p>These techniques will aim to determine an objective function, which in our case will output a finite number of unknown optimum parameters of the function, called <strong>parametric regression</strong> techniques.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applications of regression</h1>
                </header>
            
            <article>
                
<p>Regression is normally applied in order to predict future variable values, and it's a very commonly employed technique for initial data modeling in data analysis projects, but it also can be used to optimize processes, finding common ground between related but dispersed data.</p>
<p>Here we will list a number of the possible application of regression analysis:</p>
<ul>
<li>In social sciences, to predict the future values of all kinds of metrics, such as unemployment and population</li>
<li>In economics, to predict future inflation rates, interest rates, and similar indicators</li>
<li>In earth sciences, to predict future phenomena, such as the thickness of the ozone layer</li>
<li>To help with all elements of the normal corporate dashboard, adding probabilistic estimations for production throughput, earnings, spending, and so on</li>
<li>Proving the dependencies and <span>relevancies</span> <span>between two phenomena</span></li>
<li>Finding the optimum mix of components in reaction experiments</li>
<li><span>Minimizing the risk portfolio</span></li>
<li>Understanding how sensitive a corporation’s sales are to changes in advertising expenditure</li>
<li>Seeing how a stock's price is affected by changes to the interest rate</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quantitative versus qualitative variables</h1>
                </header>
            
            <article>
                
<p>In day-to-day work with data, not all the elements we encounter are the same, and thus they require special treatment, depending on their properties. A very important distinction we can make to recognize how appropriate the problem variables are is by dividing the data types into quantitative and qualitative data variables using the following criteria:</p>
<ul>
<li><strong>Quantitative variables</strong>: In the realm of physical variables or measurements, we normally work with real numbers, or qualitative variables, because what matters the most is the quantity we are measuring. In this group, we have ordinal variables, that is, when we work with orders and rankings in an activity. Both of these variable types fall within the quantitative variables category.</li>
<li><strong>Qualitative variables:</strong> On the other hand, we have measurements that show which class a sample belongs to. This can't be expressed by a number, in the sense of a quantity; it is usually assigned a label, tag, or categorical value representing the group to which the sample belongs. We call these variables qualitative variables.</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="222" width="369" src="assets/0a815a32-cec0-4afd-adbd-043f0be978dd.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Reference table addressing the differences between quantitative and qualitative analysis</div>
<p>Now let's address the question of which types of variable are suitable to apply to regression problems.</p>
<p>The clear answer is quantitative variables, because the modeling of the data distribution can only be done through functions we use to detect a regular correspondence between those variables, and not on classes or types of elements. Regression requires one continuous output variable, which is only the case with quantitative metrics.</p>
<p>In the case of qualitative variables, we assign the data to classification problems because the very definition of it is to search for non-numeric labels or tags to assign to a sample. This is the mission of classification, which we will see in the following chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression</h1>
                </header>
            
            <article>
                
<p>So, it's time to start with the simplest yet still very useful abstraction for our data–a linear regression function.</p>
<p>In linear regression, we try to find a linear equation that minimizes the distance between the data points and the modeled line. <span>The model function takes the following form: </span></p>
<div class="mce-root CDPAlignCenter CDPAlign">y<sub>i</sub> = ßx<sub>i</sub> +α+ε<sub>i</sub></div>
<p class="mce-root">Here, <em>α</em> is the intercept and <em>ß</em> is the slope of the modeled line. The variable <em>x</em> is normally called the independent variable, and <em>y</em> the dependent one, but it can also be called the regressor and the response variables.</p>
<p>The ε<sub>i</sub> variable is a very interesting element, and it's the error or distance from the sample <em>i</em> to the regressed line.</p>
<div class="CDPAlignCenter CDPAlign"><img height="279" width="314" src="assets/7b75fbf6-d475-493d-91d9-c7426807bc65.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Depiction of the components of a regression line, including the original elements, the estimated ones (in red), and the error (ε)</div>
<p>The set of all those distances, calculated in the form of a function called the <em>cost</em> function, will give us, as a result of the solution process, the values of the unknown parameters that minimize the cost. Let's get to work on it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Determination of the cost function</h1>
                </header>
            
            <article>
                
<p>As with all machine learning techniques, the process of learning depends on a minimized loss function, which shows us how right or wrong we are when predicting an outcome, depending on the stage of learning we are in.</p>
<div class="mce-root packt_infobox">The most commonly used cost function for linear regression is called <em>least squares</em>.</div>
<p>Let's define <span>this cost function</span> taking, for simplification a 2D regression, where we have a list of number tuples <em>(x<sub>0</sub>, y<sub>0</sub>)</em>, <em>(x<sub>1</sub>, y<sub>1</sub>)</em> ... <em>(x<sub>n</sub>, y<sub>n</sub>)</em> and the values to find, which are <em>β<sub>0</sub></em>  and <em>β<sub>1</sub></em>. The least squares cost function in this case can be defined as follows:<span> </span> </p>
<div style="padding-left: 150px" class="mce-root CDPAlignLeft CDPAlign"><img height="74" width="270" src="assets/68886061-ae2a-4eb5-954c-92219745fc37.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Least squares function for a linear equation, using the standard variables β<sub>0</sub> and β<sub>1</sub> that will be used in the next sections</div>
<p>The summation for each of the elements gives a unique global number, which gives a global idea of the total differences between all the values (y<sub>i</sub>), and the corresponding point in our ideal regressing line (β<sub>0</sub> + β1<em>x</em><sub>i</sub>).</p>
<p>The rationale for this operation is pretty clear:</p>
<ul>
<li>The summation gives us a unique global number</li>
<li>The difference model-real point gives us the distance or L1 error</li>
<li>Squaring this gives us a positive number, which also penalizes distances in an non-linear way, passing the one-error limit, so the more errors we commit, the more willing we will be to increase our penalization rate</li>
</ul>
<p>Another way of phrasing this is that this process minimizes the sum of squared residuals, the residual being the difference between the value we get from the dataset and the expected value computed by the model, for the same input value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The many ways of minimizing errors</h1>
                </header>
            
            <article>
                
<p>The least squares error function has several ways of getting a solution:</p>
<ul>
<li>The analytical way</li>
<li>Using the covariance and correlation values</li>
<li>The most familiar way to the machine learning family of methods — the gradient descent way</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analytical approach</h1>
                </header>
            
            <article>
                
<p>The analytical approach employs several linear algebra techniques in order to get an exact solution.</p>
<div class="packt_infobox">We are presenting this technique in a very succinct way because it's not directly related to the machine learning techniques we are reviewing in this book. We are presenting it for completeness.</div>
<p>First of all, we represent the error in a function in a matrix form:</p>
<div style="padding-left: 210px" class="mce-root"><img height="33" width="162" src="assets/a982ccbd-7d43-4efa-9f55-564990c6a257.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Canonical form of the linear regression equation in the matrix form</div>
<p>Here, <em>J</em> is the cost function and has an analytical solution of:</p>
<div style="padding-left: 240px" class="mce-root"><img height="27" width="98" src="assets/26ba2a70-39a9-468f-88f9-ab93374edb34.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Analytical solution of the matrix form of linear regression</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pros and cons of the analytical approach</h1>
                </header>
            
            <article>
                
<p>The approach of using linear algebra techniques to calculate the minimum error solution is an easier one, given the fact that we can give a really simple representation that is deterministic, so there is no additional guessing involved after applying the operations.</p>
<p>But there are some possible problems with this approach:</p>
<ul>
<li>First, matrix inversion and multiplication are very computationally intensive operations. They typically have a lower bound of approximately <em>O(n<sup>2</sup></em><span><em>)</em> to <em>O(n<sup>3</sup>)</em>, so when the number of samples increases, the problem can become intractable.</span></li>
<li>Additionally, and depending on the implementation, this direct approach could also have limited accuracy, because we can normally use the limits of the floating point capacity of the current hardware.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Covariance/correlation method</h1>
                </header>
            
            <article>
                
<p>Now it's time to introduce a new way of estimating the coefficient of our regressing line, and in the process, we will learn additional statistical measures<span>, such as covariance and correlation</span><span>, which will also help us when analyzing a dataset for the first time and drawing our first conclusions.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Covariance</h1>
                </header>
            
            <article>
                
<p><strong>Covariance</strong> is a statistical term, and can be canonically defined as follows:</p>
<div class="mce-root packt_quote">A measure of the systematic relationship between a pair of random variables wherein a change in one variable is reciprocated by an equivalent change in another variable.</div>
<p class="mce-root">Covariance can take any value between -∞ to +∞, wherein a negative value is an indicator of a negative relationship, whereas a positive value represents a positive relationship. It also ascertains a linear relationship between the variables.</p>
<p class="mce-root">Therefore, when the value is zero, it indicates no direct linear relationship, and the values tend to form a blob-like distribution.</p>
<p class="mce-root">Covariance is not affected by the unit of measure, that is, there is no change in the strength of the relationship between two variables when changing units. Nevertheless, the value of covariance will change. It has the following formula, which needs the mean of each axis as a prerequisite:</p>
<div style="padding-left: 210px" class="mce-root"><img height="34" width="178" src="assets/fa14cc01-27e4-4d77-b042-68aaa589d1cc.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Correlation</h1>
                </header>
            
            <article>
                
<p>Remember when we described the process of normalization of a variable? We centered the variable by subtracting the mean and scaling it with the standard deviation of the dataset with the following formula:</p>
<div style="padding-left: 240px" class="mce-root"><img height="41" width="64" src="assets/3cf792e8-9f8e-456e-ae89-880e7c5e2ddb.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Analytical form of the data normalization operation</div>
<p>This will be the starting point of our analysis, and we will be extending it towards each axis with the correlation<span> </span>value.</p>
<p>The correlation value determines the degree to which two or more random variables move in tandem. During the study of two variables, if it has been observed that the movement of one variable is concordant with an equivalent movement in another variable, then the variables are said to be correlated, with the exact correlation value given by the following formula:</p>
<div style="padding-left: 180px"><img height="52" width="167" src="assets/0d9fb1b3-720a-4fd9-97fa-ef1a5326c698.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Canonical definition of correlation</div>
<p class="mce-root">As a real value-based metric, it can be of two types, positive or negative. The variables are positively or directly correlated when the two variables move in the same direction. When the two variables move in opposite direction, the correlation is negative or inverse.</p>
<p class="mce-root">The value of correlation lies between <em>-1</em> to <em>+1</em>, wherein values close to <em>+1</em> represent a strong positive correlation and values close to <em>-1</em> are an indicator of a strong negative correlation:</p>
<div class="CDPAlignCenter CDPAlign"><img height="582" width="716" src="assets/0c75050d-fd72-437f-b0e2-21759ec14479.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Graphic depiction of how the distribution of the samples affects the correlation values</div>
<p>There are other ways of measuring correlation. In this book, we will be talking mainly about linear correlation. There are other methods of studying non-linear correlation, which won't be covered in this book.</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="255" width="578" src="assets/ec163393-6f45-416d-bba8-1238a9438b08.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Depiction of the difference between linear correlation and non-linear correlation measures</div>
<div class="packt_infobox">Within the practical exercises of this chapter, you will find an implementation of both linear covariance and correlation.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Searching for the slope and intercept with covariance and correlation</h1>
                </header>
            
            <article>
                
<p><span>As we have known from the beginning, what we need is to find the equation of a line, representing the underlying data, in the following form:</span></p>
<div style="padding-left: 240px"><img height="31" width="97" src="assets/d10261e6-ae1e-43dd-9245-aed8cb9f4cde.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Approximate definition of a linear equation</div>
<p>As we know that this line passes through the average of all the points, we can estimate the intercept, with the only unknown being the estimated slope:</p>
<div style="padding-left: 240px" class="mce-root"><img height="37" width="115" src="assets/63f6700e-7d98-4c47-b89e-f2d66fd10257.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Derived definition of intercept</div>
<p><span>The slope represents the change in the dependent variable divided by the change in the independent variable. </span><span>In this case, we are dealing with variation in data rather than absolute differences between coordinates. </span></p>
<p><span>As the data is of non-uniform nature, we are defining the slope as the proportion of the variance in the independent variable that covaries with the dependent variable:</span></p>
<div style="padding-left: 240px" class="mce-root"><img height="56" width="103" src="assets/c12db851-a7b6-4b75-aca2-47ea376f313f.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Estimated slope coefficient</div>
<p><span>As it happens, if our data actually looks like a circular cloud when we plot it, our slope will become <em>zero</em>, suggesting no causal relationship between the variation in </span><em>x <span>and </span><em>y</em>,<span> </span></em>expressed in the following form:</p>
<div style="padding-left: 210px" class="mce-root"><img height="63" width="192" src="assets/c519abdc-078f-4cd4-aa61-4077e22947a9.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Expanded form of the estimated slope coefficient</div>
<p>Applying the formulas we have shown previously, we can finally simplify the expression of the slope of the estimated regression line to the following expression:</p>
<div style="padding-left: 240px" class="mce-root"><img height="62" width="89" src="assets/4fde6a3d-5fab-4bb1-a91c-c7db6eda7d31.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Final form of the slope coefficient</div>
<p>Here, <em>S<sub>y</sub></em> is the standard deviation in <em>y</em>, and <em>S<sub>x</sub></em> is the standard deviation in <em>x.</em></p>
<p>With the help of the remaining elements in the equation, we can simply derive the intercept based on the knowledge that the line will reach the mean dataset point:</p>
<div style="padding-left: 210px" class="mce-root"><img height="34" width="106" src="assets/c0d9968f-f485-406a-bd3b-5f974da7435c.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Final form of the approximated intercept coefficient</div>
<p>So, we are done with a very summarized expression of two preliminary forms of regression, which have also left many analysis elements for us to use. Now it's time to introduce the star of the current machine learning techniques, one that you will surely use in many projects as a practitioner, called <strong>gradient descent</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient descent</h1>
                </header>
            
            <article>
                
<p>It's time to talk about the method that will take us to the core of modern machine learning. The method explained here will be used with many of the more sophisticated models in a similar fashion, with increased difficulty but with the same principles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Some intuitive background</h1>
                </header>
            
            <article>
                
<p>To introduce gradient descent, we will first take a look at our objective—the fitting of a line function to a set of provided data. And what do we have as elements?</p>
<ul>
<li>A model function</li>
<li>An error function</li>
</ul>
<p>Another element that we could get is a representation of all the possible errors for any of the combinations of the parameters. That would be cool, right? But look at what such a function looks like just for a problem with a simple line as the solution. This curve represents <em>z= x<sup>2</sup> + y<sup>2</sup></em>, which follows the form of the least squares error function:</p>
<div class="CDPAlignCenter CDPAlign"><img height="361" width="563" src="assets/a1b2619c-d444-44e4-bb91-5876156fa5ad.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Least squares error surface, in the case of two variables. In the case of linear regression, they are the slope and the intersect</div>
<p><span>As you can see, calculating all the possible outcomes per line parameter would consume too much CPU time. But we have an advantage: we know that the surface of such a curve is <strong>c</strong></span><strong>onvex</strong><span> (a discussion beyond the scope of this book), so it roughly looks like a bowl, and it has a unique minimum value (as seen in the previous folder). This will spare us the problem of locating local points that look like a minimum, but in fact are just bumps on the surface.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The gradient descent loop</h1>
                </header>
            
            <article>
                
<p>So it's time to look for a method to converge to the minimum of a function, knowing only where I am on the surface, and possibly the gradient at the point on the surface I am standing on:</p>
<ul>
<li>Start at a random position (remember, we don't know anything about the surface yet)</li>
<li>Look for the direction of maximum change (as the function is convex, we know it will guide us to the minimum)</li>
<li><span>Advance over the error surface</span> in that direction, proportionally to the error amount</li>
<li>Adjust the starting point of the next step to the new point on the surface where we landed and repeat the process</li>
</ul>
<p>This method allows us to discover the path for the minimization of our values in an iterative way and in a limited time, when compared to a brute force method.</p>
<p>The process for two parameters and the least squares function goes like this:</p>
<div class="CDPAlignCenter CDPAlign"><img height="365" width="565" src="assets/8b1ff442-6ed4-42e6-9ce5-eb2d53028ba7.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Depiction of the gradient descent algorithm, <span>Beginning</span> from a starting high error point, and descending in the direction of maximum change</div>
<p>This gives us an idea of how the process functions work in a normal setting, when we use and choose good and appropriate initial parameters.</p>
<div class="packt_infobox">In the upcoming chapters, we will cover the process of gradient descent in more detail, including how choosing different elements (that we will call hyper-parameters) changes the behavior of the process.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Formalizing our concepts</h1>
                </header>
            
            <article>
                
<p>Now let's go over the mathematical side of our process, so we have a reference of all the parts involved, before using them in practice.</p>
<p><span>The elements or our equations are as follows:</span></p>
<ul>
<li>The linear function variables, <em>β</em><sub><em>0</em></sub> and <em>β<sub>1</sub></em> </li>
<li>The number of samples in the sampleset, <em>m</em></li>
<li>The different elements in the sampleset, <em>x<sup>(i)</sup></em> and <em>y<sup>(i)</sup></em></li>
</ul>
<p>Let's start with our error function,<span> </span><em>J</em>. It was defined in previous sections under the name of the least squares function. We will add, for practicality, the term 1/2m at the start of the equation, as follows:</p>
<div style="padding-left: 90px"><img height="32" width="355" src="assets/487739d9-a92a-4574-b71b-7a8f7976693c.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Least squares error function</div>
<p>Let's introduce a new operator, which is the basis of all the following work, gradient.</p>
<p>To build the concept of it, we have the following:</p>
<ul>
<li>A function of one or more independent variables</li>
<li>The partial derivative of the function for all independent variables</li>
</ul>
<p>As we already know how partial derivatives work at the moment, it's enough to say that the gradient is a vector containing all of the already mentioned partial derivatives<span>; in our case, it will be as follows:</span></p>
<div style="padding-left: 210px"><img height="78" width="179" src="assets/aaf9f35f-f5c2-4095-88a8-921b55522b1d.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Gradient of the error function</div>
<p>What's the purpose of such an operator? If we are able to calculate it, it will give us the direction of change of the whole function at a single point.</p>
<p>First, we calculate the partial derivative. You can try to derive it; basically, it uses the chain rule of deriving a squared expression and then multiplies it by the original expression. </p>
<p>In the second part of the equation, we simplify the linear function by the name of the model function, <em>h<sub>a</sub></em>:</p>
<div style="padding-left: 60px" class="mce-root"><img height="57" width="460" src="assets/c81125f1-7fcb-4559-91a4-40788b068f26.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Partial derivative of the error function for the β</span><sub>1</sub><span> variable</span></div>
<p><span>In the case of </span><em>β<sub>1</sub></em><span> we get the additional factor of the <em>x<sup>(i)</sup></em> element because the derivative of <em>β<sub>1</sub>x</em><sup><em>(i)</em> </sup>is <em>x<sup>(i)</sup></em>:</span></p>
<div style="padding-left: 90px" class="mce-root"><img height="56" width="456" src="assets/5c6376e9-d5cd-4a6e-8629-82cd539945c7.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Partial derivative of the error function for the β<sub>1</sub> variable</div>
<p>Now we introduce the recursion expression, which will provide (when iterating and if conditions are met) a combination of parameters that decrease the total error in a convergent way.</p>
<p>Here, we introduce a really important element: the step size, with the name α. What's the purpose of it? It will allow us to scale how much we will advance in one step. We will discover that not choosing the right amount of power can lead to disastrous consequences, including the divergence of the error to the infinite.</p>
<p>Note that the second formula only has the tiny difference of being multiplied by the current <em>x</em> value: </p>
<div style="padding-left: 90px" class="mce-root"><img height="135" width="410" src="assets/e90f1ce1-08a6-4af4-9111-125caf313f1a.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Recursion equation for the  model functions</div>
<p>So, we are ready to go! Now we will just add a bit of mathematical spice in order to produce a more compact representation of the algorithm. Let's now express the unknowns in vector form, so all the expressions will be expressed as a whole:</p>
<div style="padding-left: 270px" class="mce-root"><img height="57" width="77" src="assets/f828048c-302b-4b06-a2c2-f67d83f54625.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Expression of β in vector form</div>
<p>With this new expression, our recursion steps can be expressed in this simple and easy-to-remember expression:</p>
<div style="padding-left: 210px" class="mce-root"><img height="48" width="185" src="assets/6ad85c8a-f5a5-432c-a237-d03e03297180.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Expression of the gradient descent recursion in vector form</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Expressing recursion as a process</h1>
                </header>
            
            <article>
                
<p>The whole method of finding the minimum error can alternatively <span>be</span><span> </span><span>expressed in a flowchart, so that we can have all the elements in the same place and understand how easy it looks if we, for a moment don't take into account the somewhat complex analytic mechanisms:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="293" width="156" src="assets/5b239fa1-5f80-47f7-bcf6-70b9f2ff0d7e.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Flow diagram of the gradient descent method. Note the simple building blocks, without taking into account the subtle mathematics it involves</div>
<p>So, with this last procedural vision of the gradient descent process, we are ready to go on to the more practical parts of this chapter. We hope you enjoyed this journey towards finding an answer to the question: What's the best way to represent our data in a simple way? And rest assured we will use much more powerful tools in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Going practical – new tools for new methods</h1>
                </header>
            
            <article>
                
<p>In this section, we will introduce a new library that will help us with covariance and correlation, especially in the data visualization realm.</p>
<p>What is <strong>Seaborn</strong>?</p>
<p>Seaborn is a library for making attractive and informative statistical graphics in Python. Moreover, it also provides very useful multivariate analysis primitives, which will help you decide whether or not and how to apply determinate regression analysis to your data.</p>
<p class="mce-root">Some of the features that Seaborn offers are as follows:</p>
<ul>
<li class="mce-root">Several built-in themes of very high quality</li>
<li class="mce-root">Tools for choosing color palettes to make beautiful plots that reveal patterns in the data</li>
<li class="mce-root">Very important functions for visualizing univariate and bivariate distributions or for comparing them between subsets of data</li>
<li class="mce-root">Tools that fit and visualize linear regression models for different kinds of independent and dependent variables</li>
<li class="mce-root">Plotting functions which try to do something useful when called with a minimal set of arguments; they expose a number of customizable options through additional parameters</li>
</ul>
<p>An important additional feature is, given that Seaborn uses matplotlib, the graphics can be further tweaked using those tools and rendered with any of the matplotlib backends.</p>
<p>Now let's explore the most useful utilities that Seaborn will bring.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Useful diagrams for variable explorations – pairplot</h1>
                </header>
            
            <article>
                
<p>For the stage of data exploration, one of the most useful measures we can have is a graphical depiction of how all the features in the dataset interact, and discover the joint variations in an intuitive manner:</p>
<div class="CDPAlignCenter CDPAlign"><img height="497" width="494" class="alignnone size-full wp-image-690 image-border" src="assets/09c1ba22-b1bf-49f0-a9f6-c2ac41f11b70.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Pairplot for the variables in the Iris dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Correlation plot</h1>
                </header>
            
            <article>
                
<p>The correlation plot allows us to summarize the variable dependency in a much more succinct way, because it shows the direct correlation between variable pairs, using a color pallet. The diagonal values are of course 1, because all variables have a maximum correlation with themselves:</p>
<div class="CDPAlignCenter CDPAlign"><img height="580" width="665" src="assets/21467333-0c02-4339-9102-565e8c79026e.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Correlation plot of the San Francisco housing dataset.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data exploration and linear regression in practice</h1>
                </header>
            
            <article>
                
<p>In this section, we will start using one of the most well-known <em>toy</em> datasets, explore it, and select one of the dimensions to learn how to build a linear regression model for its values.<br/>
Let's start by importing all the libraries (<kbd>scikit-learn</kbd>, <kbd>seaborn</kbd>, and <kbd>matplotlib</kbd>); one of the excellent features of Seaborn is its ability to define very professional-looking style settings. In this case, we will use the <kbd>whitegrid</kbd> style:</p>
<pre class="mce-root"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets <span class="hljs-keyword">import</span> seaborn.apionly <span class="hljs-keyword">as</span> sns %matplotlib inline <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt sns.set(style=<span class="hljs-string">'whitegrid'</span>, context=<span class="hljs-string">'notebook'</span>)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Iris dataset</h1>
                </header>
            
            <article>
                
<p>It’s time to load the <em>Iris</em> dataset. This is one of the most well-known historical datasets. You will find it in many books and publications. Given the good properties of the data, it is useful for classification and regression examples. The Iris dataset (<a href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a>) contains 50 records for each of the three types of iris, 150 lines in a total over five fields. Each line is a measurement of the following:</p>
<ul>
<li>Sepal length in cm</li>
<li>Sepal width in cm</li>
<li>Petal length in cm</li>
<li>Petal width in cm</li>
</ul>
<p>The final field is the type of flower (<em>setosa</em>, <em>versicolor</em>, or <em>virginica</em>). Let’s use the <kbd>load_dataset</kbd> method to create a matrix of values from the dataset:</p>
<pre>iris2 = sns.load_dataset(<span class="hljs-string">'iris'</span>)</pre>
<p>In order to understand the dependencies between variables, we will implement the covariance operation. It will receive two arrays as parameters and will return the <kbd>covariance(x,y)</kbd> value:</p>
<pre><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">covariance</span> <span class="hljs-params">(X, Y)</span>:</span>
    xhat=np.mean(X)
    yhat=np.mean(Y)
    epsilon=<span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> x,y <span class="hljs-keyword">in</span> zip (X,Y):
        epsilon=epsilon+(x-xhat)*(y-yhat)
    <span class="hljs-keyword">return</span> epsilon/(len(X)-<span class="hljs-number">1</span>)</pre>
<p>Let's try the implemented function and compare it with the NumPy function. Note that we calculated <kbd>cov(a,b)</kbd>, and NumPy generated a matrix of all the combinations <kbd>cov(a,a)</kbd>, <kbd>cov(a,b)</kbd>, so our result should be equal to the values <kbd>(1,0)</kbd> and <kbd>(0,1)</kbd> of that matrix:</p>
<pre><span class="hljs-keyword">print</span> (covariance ([<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]))
<span class="hljs-keyword">print</span> (np.cov([<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]))<br/><span class="hljs-number"><br/>0.5</span>
[[ <span class="hljs-number">2.33333333</span>  <span class="hljs-number">0.5</span>       ]
 [ <span class="hljs-number">0.5</span>         <span class="hljs-number">1.</span>        ]]</pre>
<p>Having done a minimal amount of testing of the correlation function as defined earlier, receive two arrays, such as <kbd>covariance</kbd>, and use them to get the final value:</p>
<pre><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">correlation</span> <span class="hljs-params">(X, Y)</span>:</span>
    <span class="hljs-keyword">return</span> (covariance(X,Y)/(np.std(X,  ddof=<span class="hljs-number">1</span>)*np.std(Y,  ddof=<span class="hljs-number">1</span>))) <span class="hljs-comment">##We have to indicate ddof=1 the unbiased std</span></pre>
<p>Let’s test this function with two sample arrays, and compare this with the <kbd>(0,1)</kbd> and <kbd>(1,0)</kbd> values of the correlation matrix from NumPy:</p>
<pre><span class="hljs-keyword">print</span> (correlation ([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]))
<span class="hljs-keyword">print</span> (np.corrcoef ([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]))<br/><br/><span class="hljs-number">0.870388279778</span>
[[ <span class="hljs-number">1.</span>          <span class="hljs-number">0.87038828</span>]
 [ <span class="hljs-number">0.87038828</span>  <span class="hljs-number">1.</span>        ]]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting an intuitive idea with Seaborn pairplot</h1>
                </header>
            
            <article>
                
<p>A very good idea when starting worked on a problem is to get a graphical representation of all the possible variable combinations.</p>
<p>Seaborn’s <kbd>pairplot</kbd> function provides a complete graphical summary of all the variable pairs, represented as scatterplots, and a representation of the univariate distribution for the matrix diagonal.</p>
<p>Let’s look at how this plot type shows all the variables dependencies, and try to look for a linear relationship as a base to test our regression methods:</p>
<pre>sns.pairplot(iris2, size=<span class="hljs-number">3.0</span>)<br/><span class="hljs-tag">&lt;seaborn.axisgrid.PairGrid at 0x7f8a2a30e828&gt;<br/></span></pre>
<div class="CDPAlignCenter CDPAlign"><img height="613" width="608" class="alignnone size-full wp-image-690 image-border" src="assets/00ae62c1-69b8-41b1-8105-3989b5102bdf.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Pairplot of all the variables in the dataset.</div>
<p>Lets' select two variables that, from our initial analysis, have the property of being linearly dependent. They are <kbd>petal_width</kbd> and <kbd>petal_length</kbd>:</p>
<pre>X=iris2[<span class="hljs-string">'petal_width'</span>]
Y=iris2[<span class="hljs-string">'petal_length'</span>]</pre>
<p class="mce-root">Let’s now take a look at this variable combination, which shows a clear linear tendency:</p>
<pre>plt.scatter(X,Y)</pre>
<p>This is the representation of the chosen variables, in a scatter type graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="366" width="526" class="alignnone size-full wp-image-691 image-border" src="assets/492acb6c-06e9-40f5-9a6c-9261ab328337.png"/></div>
<p><span>This is the current distribution of data that we will try to model with our linear prediction function.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the prediction function</h1>
                </header>
            
            <article>
                
<p>First, let's define the function that will abstractedly represent the modeled data, in the form of a linear function, with the form <em>y=beta*x+alpha</em>:</p>
<pre><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(alpha, beta, x_i)</span>:</span>
    <span class="hljs-keyword">return</span> beta * x_i + alpha</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the error function</h1>
                </header>
            
            <article>
                
<p>It’s now time to define the function that will show us the difference between predictions and the expected output during training. As we will explain in depth in the next chapter, we have two main alternatives: measuring the absolute difference between the values (or L1), or measuring a variant of the square of the difference (or L2). Let’s define both versions, including the first formulation inside the second:</p>
<pre><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">error</span><span class="hljs-params">(alpha, beta, x_i, y_i)</span>:</span> <span class="hljs-comment">#L1</span>
    <span class="hljs-keyword">return</span> y_i - predict(alpha, beta, x_i)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sum_sq_e</span><span class="hljs-params">(alpha, beta, x, y)</span>:</span> <span class="hljs-comment">#L2</span>
    <span class="hljs-keyword">return</span> sum(error(alpha, beta, x_i, y_i) ** <span class="hljs-number">2</span>
               <span class="hljs-keyword">for</span> x_i, y_i <span class="hljs-keyword">in</span> zip(x, y))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Correlation fit</h1>
                </header>
            
            <article>
                
<p>Now, we will define a function implementing the correlation method to find the parameters for our regression:</p>
<pre><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">correlation_fit</span><span class="hljs-params">(x, y)</span>:</span>
    beta = correlation(x, y) * np.std(y, ddof=<span class="hljs-number">1</span>) / np.std(x,ddof=<span class="hljs-number">1</span>)
    alpha = np.mean(y) - beta * np.mean(x)
    <span class="hljs-keyword">return</span> alpha, beta</pre>
<p>Let’s then run the fitting function and print the guessed parameters:</p>
<pre>alpha, beta = correlation_fit(X, Y)
print(alpha)
print(beta)<br/><br/>1.08355803285
2.22994049512</pre>
<p>Let’s now graph the regressed line with the data in order to intuitively show the appropriateness of the solution:</p>
<pre>plt.scatter(X,Y)
xr=np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">3.5</span>)
plt.plot(xr,(xr*beta)+alpha)</pre>
<p><span>This is the final plot we will get with our recently calculated slope and intercept:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="381" width="548" class="alignnone size-full wp-image-692 image-border" src="assets/0873604a-4bd0-4782-ac23-c86db29e98fa.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final regressed line.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Polynomial regression and an introduction to underfitting and overfitting</h1>
                </header>
            
            <article>
                
<p>When looking for a model, one of the main characteristics we look for is the power of generalizing with a simple functional expression. When we increase the complexity of the model, it's possible that we are building a model that is good for the training data, but will be too optimized for that particular subset of data.</p>
<p>Underfitting, on the other hand, applies to situations where the model is too simple, such as this case, which can be represented fairly well with a simple linear model.</p>
<p>In the following example, we will work on the same problem as before, using the scikit-learn library to search higher-order polynomials to fit the incoming data with increasingly complex degrees.</p>
<p>Going beyond the normal threshold of a quadratic function, we will see how the function looks to fit every wrinkle in the data, but when we extrapolate, the values outside the normal range are clearly out of range:</p>
<div class="sourceCode">
<pre class="sourceCode python"><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge
<span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures
<span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline

ix<span class="op">=</span>iris2[<span class="st">'petal_width'</span>]
iy<span class="op">=</span>iris2[<span class="st">'petal_length'</span>]

<span class="co"># generate points used to represent the fitted function </span>
x_plot <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="fl">2.6</span>, <span class="dv">100</span>)

<span class="co"># create matrix versions of these arrays</span>
X <span class="op">=</span> ix[:, np.newaxis]
X_plot <span class="op">=</span> x_plot[:, np.newaxis]

plt.scatter(ix, iy, s<span class="op">=</span><span class="dv">30</span>, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">"training points"</span>)

<span class="cf">for</span> count, degree <span class="op">in</span> <span class="bu">enumerate</span>([<span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">20</span>]):
    model <span class="op">=</span> make_pipeline(PolynomialFeatures(degree), Ridge())
    model.fit(X, iy)
    y_plot <span class="op">=</span> model.predict(X_plot)
    plt.plot(x_plot, y_plot, label<span class="op">=</span><span class="st">"degree </span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span> degree)

plt.legend(loc<span class="op">=</span><span class="st">'upper left'</span>)
plt.show()</pre></div>
<p>The combined graph shows how the different polynomials' coefficients describe the data population in different ways. The 20 degree polynomial shows clearly how it adjusts perfectly for the trained dataset, and after the known values, it diverges almost spectacularly, going against the goal of generalizing for future data.</p>
<div class="CDPAlignCenter CDPAlign"><img height="358" width="521" class="alignnone size-full wp-image-716 image-border" src="assets/bf696898-ba33-4679-a61a-b6eb42ed6443.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Curve fitting of the initial dataset, with polynomials of increasing values.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression with gradient descent in practice</h1>
                </header>
            
            <article>
                
<p class="mce-root">So now we are working with gradient descent techniques in practice for the first time! The concepts we are now practicing will serve us well during the rest of the book. Let's start by importing the prerequisite library, as always. We will use NumPy for numeric processing, and Seaborn and matplotlib for representation:</p>
<div class="sourceCode">
<pre class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> seaborn <span class="im">as</span> sns
<span class="op">%</span>matplotlib inline
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
sns.<span class="bu">set</span>(style<span class="op">=</span><span class="st">'whitegrid'</span>, context<span class="op">=</span><span class="st">'notebook'</span>)</pre></div>
<p>The loss function will be the guide for us to know how well we are doing. As we saw in the theoretical section, the least squares method will be used.</p>
<div class="packt_infobox">You can review the <em>J</em> or loss function definition and properties in the previous sections.</div>
<p>So, this <kbd>least_squares</kbd> function will receive the current regression line parameters, <em><span class="math inline">b<sub>0</sub></span></em><span> </span>and<span> </span><span class="math inline">b<sub>0</sub></span>, and the data elements to measure how good our representation of reality is:</p>
<div class="sourceCode">
<pre class="sourceCode python"><span class="kw">def</span> least_squares(b0, b1, points):
    totalError <span class="op">=</span> <span class="dv">0</span>
    N<span class="op">=</span><span class="bu">float</span>(<span class="bu">len</span>(points))
    <span class="cf">for</span> x,y <span class="op">in</span> points:
        totalError <span class="op">+=</span> (y <span class="op">-</span> (b1 <span class="op">*</span> x <span class="op">+</span> b0)) <span class="op">**</span> <span class="dv">2</span>
    <span class="cf">return</span> totalError <span class="op">/</span> <span class="dv">2</span>.<span class="op">*</span>N</pre></div>
<p>Here, we will define each step of the recurrence. As parameters, we will receive the current<span> </span><span class="math inline"><em>b</em><sub>0</sub></span><span> </span>and<span> </span><span class="math inline"><em>b</em><sub>1</sub></span>, the points used to train the model, and the learning rate. On line five of the <kbd>step_gradient</kbd> function, we see the calculation of both gradients, and then we create the <kbd>new_b0</kbd> and <kbd>new_b1</kbd> variables, <span>updating their values in the error direction</span>, scaled by the learning rate. On the last line, we return the updated values and the current error level after all points have been used for the gradient:</p>
<div class="sourceCode">
<pre class="sourceCode python"><span class="kw">def</span> step_gradient(b0_current, b1_current, points, learningRate):
    b0_gradient <span class="op">=</span> <span class="dv">0</span>
    b1_gradient <span class="op">=</span> <span class="dv">0</span>
    N <span class="op">=</span> <span class="bu">float</span>(<span class="bu">len</span>(points))
    <span class="cf">for</span> x,y <span class="op">in</span> points:
        b0_gradient <span class="op">+=</span> (<span class="dv">1</span><span class="op">/</span>N) <span class="op">*</span> (y <span class="op">-</span> ((b1_current <span class="op">*</span> x) <span class="op">+</span> b0_current))
        b1_gradient <span class="op">+=</span> (<span class="dv">1</span><span class="op">/</span>N) <span class="op">*</span> x <span class="op">*</span> (y <span class="op">-</span> ((b1_current <span class="op">*</span> x) <span class="op">+</span> b0_current))
    new_b0 <span class="op">=</span> b0_current <span class="op">+</span> (learningRate <span class="op">*</span> b0_gradient)
    new_b1 <span class="op">=</span> b1_current <span class="op">+</span> (learningRate <span class="op">*</span> b1_gradient)
    <span class="cf">return</span> [new_b0, new_b1, least_squares(new_b0, new_b1, points)]</pre></div>
<p>Then, we define a function that will run a complete training outside the model so we can check all the combinations of parameters in one place. This function will initialize the parameters and will repeat the gradient step a fixed number of times:</p>
<div class="sourceCode">
<pre class="sourceCode python"><span class="kw">def</span> run_gradient_descent(points, starting_b0, starting_b1, learning_rate, num_iterations):
    b0 <span class="op">=</span> starting_b0
    b1 <span class="op">=</span> starting_b1
    slope<span class="op">=</span>[]
    intersect<span class="op">=</span>[]
    error<span class="op">=</span>[]
    <span class="cf">for</span> i <span class="op">in</span> <span class="bu">range</span>(num_iterations):
        b0, b1 , e<span class="op">=</span> step_gradient(b0, b1, np.array(points), learning_rate)
        slope.append(b1)
        intersect.append(b0)
        error.append(e)
    <span class="cf">return</span> [b0, b1, e, slope, intersect,error]</pre></div>
<div class="packt_infobox">This process could prove inefficient when the convergence rate is high, wasting precious CPU iterations. A more clever stop condition would consist of adding an acceptable error value, which would stop the iteration.</div>
<p>Well, time to try our model! Let's start loading the Iris dataset again, for reference, and as a means of checking the correctness of our results. We will use the <kbd>petal_width</kbd> and <kbd>petal_length</kbd> parameters, which we have already seen and decided they are good candidates for linear regression. The <kbd>dstack</kbd> command from NumPy allows us to merge the two columns, which we converted to a list to discard the column headers. The only caveat is that the resulting list has an unused extra dimension, which we discard using the <kbd>[0]</kbd> index selector:</p>
<div class="sourceCode">
<pre class="sourceCode python">iris <span class="op">=</span> sns.load_dataset(<span class="st">'iris'</span>)
X<span class="op">=</span>iris[<span class="st">'petal_width'</span>].tolist()
Y<span class="op">=</span>iris[<span class="st">'petal_length'</span>].tolist()
points<span class="op">=</span>np.dstack((X,Y))[<span class="dv">0</span>]</pre></div>
<p>So, let's try our model with what seem to be good initial parameters, a <kbd>0.0001</kbd> learning rate, initial parameters at <kbd>0</kbd>, and <kbd>1000</kbd> iterations; lets see how it behaves:</p>
<div class="sourceCode">
<pre class="sourceCode python">learning_rate <span class="op">=</span> <span class="fl">0.0001</span>
initial_b0 <span class="op">=</span> <span class="dv">0</span> 
initial_b1 <span class="op">=</span> <span class="dv">0</span> 
num_iterations <span class="op">=</span> <span class="dv">1000</span>
[b0, b1, e, slope, intersect, error] <span class="op">=</span> run_gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)<br/><br/>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))
plt.scatter(X,Y)
xr<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="fl">3.5</span>)
plt.plot(xr,(xr<span class="op">*</span>b1)<span class="op">+</span>b0)<span class="op">;</span>
plt.title(<span class="st">'Regression, alpha=0.001, initial values=(0,0), it=1000'</span>)<span class="op">;</span></pre></div>
<div class="figure CDPAlignCenter CDPAlign"><img height="361" width="476" src="assets/1bddad51-5230-49f5-b989-aad474339736.png"/>
<p>Well, that's bad; clearly, we are not yet there. Let's see what happened with the error during training:</p>
</div>
<div class="sourceCode">
<pre class="sourceCode python">plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))
xr<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="dv">1000</span>)
plt.plot(xr,np.array(error).transpose())<span class="op">;</span>
plt.title(<span class="st">'Error for 1000 iterations'</span>)<span class="op">;</span></pre></div>
<div class="figure CDPAlignCenter CDPAlign"><img height="346" width="487" src="assets/c854daf0-1fef-458e-ab5a-507a2bd34a3d.png"/></div>
<p>The process seems to be working, but it's a bit slow. Maybe we can try to increase the step by a factor of 10 to see if it converges quickly? Let's check:</p>
<div class="sourceCode">
<pre class="sourceCode python">learning_rate <span class="op">=</span> <span class="fl">0.001</span> <span class="co">#Last one was 0.0001</span>
initial_b0 <span class="op">=</span> <span class="dv">0</span> 
initial_b1 <span class="op">=</span> <span class="dv">0</span> 
num_iterations <span class="op">=</span> <span class="dv">1000</span>
[b0, b1, e, slope, intersect, error] <span class="op">=</span> run_gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)
plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))
xr<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="dv">1000</span>)
plt.plot(xr,np.array(error).transpose())<span class="op">;</span>
plt.title(<span class="st">'Error for 1000 iterations, increased step by tenfold'</span>)<span class="op">;<br/></span></pre></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="368" width="519" src="assets/41808f98-370f-4132-824a-ac418872dc86.png"/></div>
<p>That was better! The process converges much more quickly. Let's check how the regressed line looks now:</p>
<div class="sourceCode">
<pre class="sourceCode python">plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))
plt.scatter(X,Y)
xr<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="fl">3.5</span>)
plt.plot(xr,(xr<span class="op">*</span>b1)<span class="op">+</span>b0)<span class="op">;</span>
plt.title(<span class="st">'Regression, alpha=0.01, initial values=(0,0), it=1000'</span>)<span class="op">;</span></pre></div>
<div class="figure CDPAlignCenter CDPAlign"><img height="359" width="474" src="assets/dace0203-d296-4d39-8683-b5beae83a51b.png"/></div>
<p>Yes! It looks much better. We could think we are done, but a developer always wants to go faster. Let's see what would occur if we wanted to go faster, with an enormous step of <kbd>2</kbd>, for example:</p>
<div class="sourceCode">
<pre class="sourceCode python">learning_rate <span class="op">=</span> <span class="fl">0.85</span> <span class="co">#LAst one was 0.0001</span>
initial_b0 <span class="op">=</span> <span class="dv">0</span> 
initial_b1 <span class="op">=</span> <span class="dv">0</span> 
num_iterations <span class="op">=</span> <span class="dv">1000</span>
[b0, b1, e, slope, intersect, error] <span class="op">=</span> run_gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)
plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))
xr<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="dv">1000</span>)
plt.plot(xr,np.array(error).transpose())<span class="op">;</span>
plt.title(<span class="st">'Error for 1000 iterations, big step'</span>)<span class="op">;</span></pre></div>
<div class="CDPAlignCenter CDPAlign"><img height="355" width="477" src="assets/0ba9dc83-0d77-4b5c-9a5c-093df03947ea.png"/></div>
<p>This is a bad move; as you can see, the error finally went to infinity! What happens here? Simply, the steps we are taking are so radical that instead of slicing the imaginary bowl we described before, we are just jumping around the surface, and as the iterations advance, we began to escalate the accumulated errors without control. Another measure that could be taken is to improve our seed values, which, as you have seen started, with a value of <kbd>0</kbd>. This is a very bad idea in general for <span>this technique</span>, especially when you are working with data that is not normalized. There are more reasons for this, which you can find in more advanced literature. So, let's try to initialize the parameter on a pseudo-random location in order to allow the graphics to be the same across the code examples, and see what happens:</p>
<div class="sourceCode">
<pre class="sourceCode python">learning_rate <span class="op">=</span> <span class="fl">0.001</span> <span class="co">#Same as last time</span>
initial_b0 <span class="op">=</span> <span class="fl">0.8</span> <span class="co">#pseudo random value</span>
initial_b1 <span class="op">=</span> <span class="fl">1.5</span> <span class="co">#pseudo random value</span>
num_iterations <span class="op">=</span> <span class="dv">1000</span>
[b0, b1, e, slope, intersect, error] <span class="op">=</span> run_gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)
plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))
xr<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="dv">1000</span>)
plt.plot(xr,np.array(error).transpose())<span class="op">;</span>
plt.title(<span class="st">'Error for 1000 iterations, step 0.001, random initial parameter values'</span>)<span class="op">;</span></pre></div>
<div class="figure CDPAlignCenter CDPAlign"><img height="297" width="412" src="assets/6cd0d7cb-29fd-4ecd-8c3d-d72f9df03d12.png"/></div>
<p>As you can see, even if you have the same sloppy error rate, the initial error value decreases tenfold (from 2e5 to 2e4). Now let's try a final technique to improve the convergence of the parameters based on the normalization of the input values. As you have already studied in <a href="fa27740b-e9e0-4ad1-ab13-dfe57b30a956.xhtml" target="_blank">Chapter 2</a>, <em>The Learning Process,</em> it consists of centering and scaling the data. What's the effect of that operation on the data? Using a graphical image, when data is not normalized, the error surface tends to be shallow and the values oscillate a lot. The normalization transforms that data into a more deep surface, with more definite gradients towards the center:</p>
<div class="sourceCode">
<pre>learning_rate <span class="op">=</span> <span class="fl">0.001</span> <span class="co">#Same as last time</span>
initial_b0 <span class="op">=</span> <span class="fl">0.8</span> <span class="co">#pseudo random value</span>
initial_b1 <span class="op">=</span> <span class="fl">1.5</span> <span class="co">#pseudo random value</span>
num_iterations <span class="op">=</span> <span class="dv">1000</span>
x_mean <span class="op">=</span>np.mean(points[:,<span class="dv">0</span>])
y_mean <span class="op">=</span> np.mean(points[:,<span class="dv">1</span>])
x_std <span class="op">=</span> np.std(points[:,<span class="dv">0</span>])
y_std <span class="op">=</span> np.std(points[:,<span class="dv">1</span>])

X_normalized <span class="op">=</span> (points[:,<span class="dv">0</span>] <span class="op">-</span> x_mean)<span class="op">/</span>x_std
Y_normalized <span class="op">=</span> (points[:,<span class="dv">1</span>] <span class="op">-</span> y_mean)<span class="op">/</span>y_std

plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))
plt.scatter(X_normalized,Y_normalized)<br/><br/>&lt;matplotlib.collections.PathCollection at 0x7f9cad8f4240&gt;</pre></div>
<div class="CDPAlignCenter CDPAlign"><img height="289" width="412" src="assets/35b4c6f1-dc4f-4b94-820d-f884d3b3a0e1.png"/></div>
<p>Now that we have this set of clean and tidy data, let's try again with the last slow convergence parameters, and see what happens to the error minimization speed:</p>
<div class="sourceCode">
<pre class="sourceCode python">points<span class="op">=</span>np.dstack((X_normalized,Y_normalized))[<span class="dv">0</span>]
learning_rate <span class="op">=</span> <span class="fl">0.001</span> <span class="co">#Same as last time</span>
initial_b0 <span class="op">=</span> <span class="fl">0.8</span> <span class="co">#pseudo random value</span>
initial_b1 <span class="op">=</span> <span class="fl">1.5</span> <span class="co">#pseudo random value</span>
num_iterations <span class="op">=</span> <span class="dv">1000</span>
[b0, b1, e, slope, intersect, error] <span class="op">=</span> run_gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)
plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))
xr<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="dv">1000</span>)</pre>
<pre class="sourceCode python">plt.plot(xr,np.array(error).transpose())<span class="op">;</span>
plt.title(<span class="st">'Error for 1000 iterations, step 0.001, random initial parameter values, normalized initial values'</span>)<span class="op">;</span></pre></div>
<div class="CDPAlignCenter CDPAlign"><img height="289" width="467" src="assets/fd5f8a31-a6a3-40d4-bfa7-a795cc34616d.png"/></div>
<p>A very good starting point indeed! Just by normalizing the data, we have half the initial error values, and the error went down 20% after 1,000 iterations. The only thing we have to remember is to denormalize after we have the results, in order to have the initial scale and data center. So, that's all for now on gradient descent. We will be revisiting it in the next chapters for new challenges.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p>The way of this book is one of generalizations. In the first chapter, we began with simpler representations of the reality, and so simpler criteria for grouping or predicting information structures. </p>
<p>After having reviewed linear regression, which is used mainly to predict a real value following a modeled linear function, we will advance to a generalization of it, which will allow us to separate binary outcomes (indicating that a sample belongs to a class), starting from a previously fitted linear function. So let's get started with this technique, which will be of fundamental use in almost all the following chapters of this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem domain of linear regression and logistic regression</h1>
                </header>
            
            <article>
                
<p>To intuitively understand the problem domain of the logistic regression, we will employ a graphical representation.</p>
<p>In the first we show the linear fitting function, which is the main objective of the whole model building process, and at the bottom, the target data distribution. As you clearly see, data is now of a binary nature, and a sample belongs to one or another options, nothing in the middle. Additionally, we see that the modelling function is of a new type; we will later name it and study its properties. You may wonder what this has it to do with a linear function? Well, as we will see later, it will be inside of that s-like function, adapting its shape.</p>
<div class="CDPAlignCenter CDPAlign"><img height="376" width="355" src="assets/08c9647f-ab32-48d8-8a56-c50c4190de98.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Simplified depiction of the common data distributions where Linear or Logistic regression are applied.</div>
<p>Summarizing, Linear regression can be imagined as a continuum of increasingly growing values. The other is a domain where the output can have just two different values based on the <em>x</em> value. In the particular case shown in the image, we can see a clear trend towards one of the possible outcomes, as the independent variable increases, and the sigmoid function allows us to transition from two outcomes, which don't have a clear separation in time, which gives us an estimated probability in the overlap zone of the non-occurrence/occurrence zone.</p>
<p>In some ways the terminology is a bit confusing, given that we are doing a regression that is obtaining a continuous value, but in reality, the final objective is building a prediction for a classification problem with discrete variables.</p>
<p>The key here is to understand that we will obtain probabilities of an item pertaining to a class and not a totally discrete value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic function predecessor – the logit functions</h1>
                </header>
            
            <article>
                
<p>Before we study the logistic function, we will review the original function on which it is based, the logit function, which gives it some of its more general properties.</p>
<p>Essentially, when we talk about the logit function, we are working with the function of a random variable <em>p</em>; more specifically, one corresponding with a Bernoulli distribution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Link function</h1>
                </header>
            
            <article>
                
<p>As we are trying to build a <strong>generalized linear model</strong>, we want to start from a linear function and <span>obtain a mapping to a probability distribution </span><span>from the dependent variable.</span></p>
<p>Since the output type of our model is of a binary nature, the normally chosen distribution is the Bernoulli distribution, and the link function, leaning toward the logistic function, is the <strong>logit function</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logit function</h1>
                </header>
            
            <article>
                
<p>One of the possible variables that we could utilize is the natural logarithm of the odds, that <em>p</em> equals one. This function is called the logit function:</p>
<div style="padding-left: 270px" class="mce-root"><img height="43" width="131" src="assets/2ee45e25-bf32-4991-aad0-1ca557eab066.png"/></div>
<p>We can also call the logit function a log-odd function, because we are calculating the log of the odds <em>(p/1-p)</em> for a given probability <em>p</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logit function properties</h1>
                </header>
            
            <article>
                
<p>So, as we can visually infer, we are replacing <em>x</em> with a combination of the independent variables, no matter their values, and replacing <em>x</em> with any occurrence from minus infinity to infinity. We are scaling the response to be between <em>0</em> and <em>1</em>.</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="304" width="177" src="assets/580123ba-396c-452e-8ff2-5ad2f6b8979b.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Depiction of the main range characteristics of the logit function</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The importance of the logit inverse</h1>
                </header>
            
            <article>
                
<p>Suppose that we calculate the inverse of the logit function. The simple inverse transformation of the logit will give us the following expression:</p>
<div style="padding-left: 150px" class="mce-root"><img height="47" width="322" src="assets/fadf7d1d-7327-4ac1-af90-14a2c064d5d7.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Analytical definition of the logit function</div>
<p>This function is nothing less than a <strong>sigmoid function</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The sigmoid or logistic function</h1>
                </header>
            
            <article>
                
<p>The logistic function will represent the binary options we are representing in our new regression tasks. The logistic function is defined as follows (changing <span>the independent variable</span><span> from </span><span>α to</span> <em>t</em><span> for clarity):</span></p>
<div style="padding-left: 150px" class="mce-root"><img height="49" width="157" src="assets/55f11a91-4e1a-4fc9-815c-d1c3a170bb38.png"/></div>
<p class="CDPAlignCenter CDPAlignLeft CDPAlign">You will find this new figure common in the following sections, because it will be used very frequently as an activation function for neural networks and other applications. In the following figure, you will find the graphical representation of the sigmoid function:</p>
<div class="packt_figref CDPAlignCenter CDPAlign">
<div class="CDPAlignCenter packt_figref"><img height="228" width="342" src="assets/f6177f95-f5bc-4103-8581-894a78bd5007.png"/></div>
Standard sigmoid</div>
<p>How can we interpret and give this function a meaning for our modeling tasks? The normal interpretation of this equations is that <em>t</em> represents a simple independent variable, but we will improve this model, assuming that <em>t</em> is a linear function of a single explanatory variable <em>x</em> (the case where <em>t</em> is a linear combination of multiple explanatory variables is treated similarly), expressing it as follows:</p>
<div style="padding-left: 240px"><img height="30" width="98" src="assets/bc0dc978-01a0-44d4-a46e-efed47460598.png"/></div>
<p>So, we can start again from the original logit equation:</p>
<div style="padding-left: 210px" class="mce-root"><img height="52" width="218" src="assets/7d527bcb-6f1d-4939-a9d8-43d56d3e170c.png"/></div>
<p>We will reach to the regression equation, which will give us the regressed probability with the following equation:</p>
<div style="padding-left: 240px" class="mce-root"><img height="67" width="143" src="assets/0bd65ee9-f49d-45f8-8033-7480823ef98d.png"/></div>
<p>Note that <em>p</em> (hat) denotes an estimated probability. What will give us a measure of how approximate we are to the solution? Of course, a carefully chosen loss function!</p>
<p>The following image shows how the mapping from an infinite domain of possible outcomes which will be finally reduced to the <em>[0,1]</em> range, with <em>p</em> being the probability of the occurrence of the event being represented. This is shown in a simple schema, which is the  structure and domain transformation of the logit function (from a linear one to a probability modeled by a Sigmoid):</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="119" width="444" src="assets/d0643bfd-984e-4cb5-8beb-7adfc19ae8c1.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Function mapping for the logit of a linear equation, resulting in a sigmoid curve</div>
<p>What changes will affect the parameters of the linear function? They are the values that will change the central slope and the displacement from zero of the sigmoid function, allowing it to more exactly reduce the error between the regressed values and the real data points.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Properties of the logistic function</h1>
                </header>
            
            <article>
                
<p>Every curve in the function space can be described by the possible objectives it could be applied to. In the case of the logistic function, they are as follows:</p>
<ul>
<li>Model the probability of an event <em>p</em>, depending on one or more independent variables. For example, the probability of being awarded a prize, given previous qualifications</li>
<li>Estimate (this is the regression part) <em>p</em> for a determined observation, related to the possibility of the event not occurring.</li>
<li>Predict the effect of the change of independent variables using a binary response.</li>
<li>Classify observations by calculating the probability of an item being of a determined class.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multiclass application – softmax regression</h1>
                </header>
            
            <article>
                
<p>Up until now, we have been classifying in the case of only having two classes, or in probabilistic language, event occurrence probabilities <em>p</em>. But this logistic regression can also be conveniently generalized to account for many classes.</p>
<p>As we saw before, in logistic regression we assumed that the labels were binary (<em>y(i)∈{0,1})</em>, but softmax regression allows us to handle <em>y(i)∈{1,…,K}</em>, where <em>K</em> is the number of classes and the label <em>y</em> can take on <em>K</em> different values, rather than only two.</p>
<p>Given a test input <em>x</em>, we want to estimate the probability that <em>P(y=k|x)</em> for each value of <em>k=1,…,K</em>. The softmax regression will make this output a <em>K</em>-dimensional vector (whose elements sum to 1), giving us our <em>K</em> estimated probabilities:</p>
<div class="CDPAlignCenter CDPAlign"><img height="549" width="338" src="assets/ea26194c-e41f-48e6-8a94-de9c5a9e6376.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Comparison between the univariate logisic regression outcome and N classes softmax regression</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Practical example – cardiac disease modeling with logistic regression</h1>
                </header>
            
            <article>
                
<p><span>It's time to finally solve a practical example with the help of the very useful logistic regression. In this first exercise, we will work on predicting the probability of having coronary heart disease, based on the age of the population. It's a classic problem, which will be a good start for understanding this kind of regression analysis.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The CHDAGE dataset</h1>
                </header>
            
            <article>
                
<p>For the first simple example, we will use a very simple and often studied dataset, which was published in <em>Applied Logistic Regression</em>, from <em>David W. Hosmer</em>, <em>Jr. Stanley Lemeshow and </em><em>Rodney X. Sturdivant</em>. We list the age in years (<kbd>AGE</kbd>) and the presence or absence of evidence of significant <strong>coronary heart disease</strong> (<strong>CHD</strong>) for 100 subjects in a hypothetical study of risk factors for heart disease. The table also contains an identifier variable (<kbd>ID</kbd>) and an age group variable (<kbd>AGEGRP</kbd>).</p>
<p>The outcome variable is <kbd>CHD</kbd>, which is coded with a value of <kbd>0</kbd> to indicate that <kbd>CHD</kbd> is absent, or <kbd>1</kbd> to indicate that it is present in the individual. In general, any two values could be used, but we have found it most convenient to use zero and one. We refer to this dataset as the <kbd>CHDAGE</kbd> data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset format</h1>
                </header>
            
            <article>
                
<p>The <kbd>CHDAGE</kbd> dataset is a two-column CSV file that we will download from an external repository. In the first chapter, we used native TensorFlow methods to read the dataset. In this chapter, we will alternatively use a complementary and popular library to get the data. The cause for this new addition is that, given that the dataset only has 100 tuples, it is practical to just read it in one line, and also, we can get simple but powerful analysis methods for free from the pandas library.</p>
<p>In the first stage of this project, we will start loading an instance of the <kbd>CHDAGE</kbd> dataset. Then we will print vital statistics about the data, and then proceed to preprocessing. After doing some plots of the data, we will build a model composed of the activation function, which will be a softmax function, for the special case where it becomes a standard logistic regression, that is, when there are only two classes (existence or not of the illness).</p>
<p>Let's start by importing the required libraries:</p>
<div class="sourceCode">
<pre class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> pandas <span class="im">as</span> pd
<span class="im">from</span> sklearn <span class="im">import</span> datasets
<span class="im">from</span> sklearn <span class="im">import</span> linear_model
<span class="im">import</span> seaborn.apionly <span class="im">as</span> sns
<span class="op">%</span>matplotlib inline
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
sns.<span class="bu">set</span>(style<span class="op">=</span><span class="st">'whitegrid'</span>, context<span class="op">=</span><span class="st">'notebook'</span>)</pre></div>
<p>Let's read the dataset from the CSV original file using <kbd>read_csv</kbd> from pandas, and draw the data distribution using the scatter function of matplotlib. As we can see, there is a definite pattern through the years that correlates to the presence of cardiac disease with increasing age:</p>
<div class="sourceCode">
<pre class="sourceCode python">df <span class="op">=</span> pd.read_csv(<span class="st">"data/CHD.csv"</span>, header<span class="op">=</span><span class="dv">0</span>)
plt.figure() <span class="co"># Create a new figure</span>
plt.axis ([<span class="dv">0</span>,<span class="dv">70</span>,<span class="op">-</span><span class="fl">0.2</span>,<span class="fl">1.2</span>])
plt.title(<span class="st">'Original data'</span>)
plt.scatter(df[<span class="st">'age'</span>],df[<span class="st">'chd'</span>]) <span class="co">#Plot a scatter draw of the random datapoints</span></pre></div>
<div class="figure CDPAlignCenter CDPAlign">
<p>This is the current plot of the original data:</p>
</div>
<div class="figure CDPAlignCenter CDPAlign"><img height="290" width="416" src="assets/5f4526d5-8e60-43e6-830f-fecc8d7db99c.png"/></div>
<div class="figure CDPAlignCenter CDPAlign">
<p><span>Now we will create a logistic regression model using the logistic regression object from scikit-learn, and then we will call the <kbd>fit</kbd> function, which will create a sigmoid optimized to minimize the prediction error for our training data:</span></p>
</div>
<div class="sourceCode">
<pre>logistic <span class="op">=</span> linear_model.LogisticRegression(C<span class="op">=</span><span class="fl">1e5</span>)
logistic.fit(df[<span class="st">'age'</span>].reshape(<span class="dv">100</span>,<span class="dv">1</span>),df[<span class="st">'chd'</span>].reshape(<span class="dv">100</span>,<span class="dv">1</span>))<br/><br/>LogisticRegression(C=100000.0, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)</pre></div>
<p>Now it's time to represent the results. Here, we will generate a linear space from 10 to 90 years with 100 subdivisions.</p>
<p>For each sample of the domain, we will show the probability of occurrence (1) and not occurrence (0, or the inverse of the previous one).</p>
<p>Additionally, we will show the predictions along with the original data points, so we can match all the elements in a single graphic:</p>
<div class="sourceCode">
<pre class="sourceCode python">x_plot <span class="op">=</span> np.linspace(<span class="dv">10</span>, <span class="dv">90</span>, <span class="dv">100</span>)
oneprob<span class="op">=</span>[]
zeroprob<span class="op">=</span>[]
predict<span class="op">=</span>[]
plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>))
<span class="cf">for</span> i <span class="op">in</span> x_plot:
    oneprob.append (logistic.predict_proba(i)[<span class="dv">0</span>][<span class="dv">1</span>])<span class="op">;</span>
    zeroprob.append (logistic.predict_proba(i)[<span class="dv">0</span>][<span class="dv">0</span>])<span class="op">;</span>
    predict.append (logistic.predict(i)[<span class="dv">0</span>])<span class="op">;</span>

plt.plot(x_plot, oneprob)<span class="op">;</span>
plt.plot(x_plot, zeroprob)
plt.plot(x_plot, predict)<span class="op">;</span>
plt.scatter(df[<span class="st">'age'</span>],df[<span class="st">'chd'</span>])</pre></div>
<div class="CDPAlignCenter CDPAlign"><img height="428" width="442" class="alignnone size-full wp-image-722 image-border" src="assets/73dd7c53-d8cc-4c9a-afe2-a8b01d1e1c8c.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Simultaneous plot of the original data distribution, the modeling logistic curve, and its inverse</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we've reviewed the main ways to approach the problem of modeling data using simple and definite functions.</p>
<p>In the next chapter, we will be using more sophisticated models that can reach greater complexity and tackle higher-level abstractions, and can be very useful for the amazingly varied datasets that have emerged recently, starting with simple <strong>feedforward networks</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p><span>Galton, Francis, "<em>Regression towards mediocrity in hereditary stature.</em>" </span>The Journal of the Anthropological Institute of Great Britain and Ireland<span> 15 (1886): 246-263.</span></p>
<p><span>Walker, Strother H., and David B. Duncan, "<em>Estimation of the probability of an event as a function of several independent variables.</em>" </span>Biometrika<span> 54.1-2 (1967): 167-179.</span></p>
<p><span>Cox, David R, "<em>The regression analysis of binary sequences.</em>" </span>Journal of the Royal Statistical Society. Series B (Methodological)<span>(1958): 215-242.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>