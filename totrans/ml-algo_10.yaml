- en: Hierarchical Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to discuss a particular clustering technique called
    hierarchical clustering. Instead of working with the relationships existing in
    the whole dataset, this approach starts with a single entity containing all elements
    (divisive) or N separate elements (agglomerative), and proceeds by splitting or
    merging the clusters according to some specific criteria, which we're going to
    analyze and compare.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hierarchical clustering is based on the general concept of finding a hierarchy
    of partial clusters, built using either a bottom-up or a top-down approach. More
    formally, they are called:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agglomerative clustering**:  The process starts from the bottom (each initial
    cluster is made up of a single element) and proceeds by merging the clusters until
    a stop criterion is reached. In general, the target has a sufficiently small number
    of clusters at the end of the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Divisive clustering**:In this case, the initial state is a single cluster
    with all samples and the process proceeds by splitting the intermediate cluster
    until all elements are separated. At this point, the process continues with an
    aggregation criterion based on the dissimilarity between elements. A famous approach
    (which is beyond the scope of this book) called **DIANA** is described in Kaufman
    L., Roussew P.J., *Finding Groups In Data: An Introduction To Cluster Analysis*,
    Wiley.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn implements only the agglomerative clustering. However, this is
    not a real limitation because the complexity of divisive clustering is higher
    and the performances of agglomerative clustering are quite similar to the ones
    achieved by the divisive approach.
  prefs: []
  type: TYPE_NORMAL
- en: Agglomerative clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider the following dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/403e8d5c-2021-48a5-b640-1512d5f61bf3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We define **affinity**, a metric function of two arguments with the same dimensionality
    *m*. The most common metrics (also supported by scikit-learn) are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidean** or *L2*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/6068dda2-c6f1-4f82-93c4-280ca75d55b5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Manhattan** (also known as City Block) or *L1*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3c2e2e77-d772-4f7b-8661-4403eaa4e496.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Cosine distance**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d325da43-5b82-4426-8617-269e9039515c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Euclidean distance is normally a good choice, but sometimes it''s useful
    to a have a metric whose difference with the Euclidean one gets larger and larger.
    The Manhattan metric has this property; to show it, in the following figure there''s
    a plot representing the distances from the origin of points belonging to the line
    *y = x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99347e35-cd84-405f-9c60-9d3b5e92e451.png)'
  prefs: []
  type: TYPE_IMG
- en: The cosine distance, instead, is useful when we need a distance proportional
    to the angle between two vectors. If the direction is the same, the distance is
    null, while it is maximum when the angle is equal to 180° (meaning opposite directions).
    This distance can be employed when the clustering must not consider the *L2* norm
    of each point. For example, a dataset could contain bidimensional points with
    different scales and we need to group them into clusters corresponding to circular
    sectors. Alternatively, we could be interested in their position according to
    the four quadrants because we have assigned a specific meaning (invariant to the
    distance between a point and the origin) to each of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a metric has been chosen (let''s simply call it *d(x,y)*), the next step
    is defining a strategy (called **linkage**) to aggregate different clusters. There
    are many possible methods, but scikit-learn supports the three most common ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complete linkage**: For each pair of clusters, the algorithm computes and
    merges them to minimize the maximum distance between the clusters (in other words,
    the distance of the farthest elements):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f6e971dc-fe8a-4bc9-b8ca-42be837dcb97.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Average linkage**: It''s similar to complete linkage, but in this case the
    algorithm uses the average distance between the pairs of clusters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0dfa57ed-60c7-4d4c-996c-3d8be1184fd2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Ward''s linkage**: In this method, all clusters are considered and the algorithm
    computes the sum of squared distances within the clusters and merges them to minimize
    it. From a statistical viewpoint, the process of agglomeration leads to a reduction
    in the variance of each resulting cluster. The measure is:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b5095ee1-5ea0-40a3-b453-617cc1cda4b1.png)'
  prefs: []
  type: TYPE_IMG
- en: The Ward's linkage supports only the Euclidean distance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dendrograms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To better understand the agglomeration process, it's useful to introduce a graphical
    method called a **dendrogram**, which shows in a static way how the aggregations are
    performed, starting from the bottom (where all samples are separated) till the
    top (where the linkage is complete). Unfortunately, scikit-learn doesn't support
    them. However, SciPy (which is a mandatory requirement for it) provides some useful
    built-in functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating a dummy dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To avoid excessive complexity in the resulting plot, the number of samples
    has been kept very low. In the following figure, there''s a representation of
    the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9fc2356b-469f-4608-9b89-8ddb84aaa110.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can compute the dendrogram. The first step is computing a distance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We have chosen a Euclidean metric, which is the most suitable in this case.
    At this point, it''s necessary to decide which linkage we want. Let''s take Ward;
    however, all known methods are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s possible to create and visualize a dendrogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aeb9496e-6b92-4693-9997-536fa9fa4e81.png)'
  prefs: []
  type: TYPE_IMG
- en: In the *x* axis, there are the samples (numbered progressively), while the *y*
    axis represents the distance. Every arch connects two clusters that are merged
    together by the algorithm. For example, 23 and 24 are single elements merged together.
    The element 13 is then aggregated to the resulting cluster, and so the process
    continues.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, if we decide to cut the graph at the distance of 10, we get
    two separate clusters: the first one from 15 to 24 and the other one from 0 to
    20\. Looking at the previous dataset plot, all the points with *Y* < 10 are considered
    to be part of the first cluster, while the others belong to the second cluster.
    If we increase the distance, the linkage becomes very aggressive (particularly
    in this example with only a few samples) and with values greater than 27, only
    one cluster is generated (even if the internal variance is quite high!).'
  prefs: []
  type: TYPE_NORMAL
- en: Agglomerative clustering in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a more complex dummy dataset with 8 centers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A graphical representation is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8717c2a-bc36-4558-91e9-a159466d9e6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now perform an agglomerative clustering with different linkages (always
    keeping the Euclidean distance) and compare the results. Let''s start with a complete
    linkage (`AgglomerativeClustering` uses the method `fit_predict()` to train the
    model and transform the original dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A plot of the result (using both different markers and colors) is shown in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/abd002fc-e335-49f4-90b6-b3164b37f179.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The result is totally bad. This approach penalizes the inter-variance and merges
    cluster, which in most cases should be different. In the previous plot, the three
    clusters in the middle are quite fuzzy, and the probability of wrong placement
    is very high considering the variance of the cluster represented by dots. Let''s
    now consider the average linkage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e522106-8595-4b55-abd8-65a7588910ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the clusters are better defined, even if some of them could have
    become really small. It can also be useful to try other metrics (in particular
    *L1*) and compare the results. The last method, which is often the best (it''s
    the default one), is Ward''s linkage, that can be used only with a Euclidean metric
    (also the default one):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4badc80-5485-42c6-8e06-fa09fc61ce90.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, it's impossible to modify the metric so, as also suggested in
    the official scikit-learn documentation, a valid alternative could be the average
    linkage, which can be used with any affinity.
  prefs: []
  type: TYPE_NORMAL
- en: Connectivity constraints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'scikit-learn also allows specifying a connectivity matrix, which can be used
    as a constraint when finding the clusters to merge. In this way, clusters which
    are far from each other (non-adjacent in the connectivity matrix) are skipped.
    A very common method for creating such a matrix involves using the k-nearest neighbors
    graph function (implemented as `kneighbors_graph()`), that is based on the number
    of neighbors a sample has (according to a specific metric). In the following example,
    we consider a circular dummy dataset (often used in the official documentation
    also):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'A graphical representation is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d90add68-2585-4421-b9ab-1554e941611d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We start with unstructured agglomerative clustering based on average linkage
    and impose 20 clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we have used the method `fit()` because the class `AgglomerativeClustering`,
    after being trained, exposes the labels (cluster number) through the instance
    variable `labels_`and it''s easier to use this variable when the number of clusters
    is very high. A graphical plot of the result is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a133383d-5e1a-4660-9264-31b8bbc09e14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can try to impose a constraint with different values for *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plots are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f59b111-a461-4a2d-8118-6f62d97d9864.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, imposing a constraint (in this case, based on k-nearest neighbors)
    allows controlling how the agglomeration creates new clusters and can be a powerful
    tool for tuning the models, or for avoiding elements whose distance is large in
    the original space could be taken into account during the merging phase (this
    is particularly useful when clustering images).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kaufman L., Roussew P.J., *Finding Groups In Data: An Introduction To Cluster
    Analysis*, Wiley'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have presented hierarchical clustering, focusing our attention
    on the agglomerative version, which is the only one supported by scikit-learn.
    We discussed the philosophy, which is rather different to the one adopted by many
    other methods. In agglomerative clustering, the process begins by considering
    each sample as a single cluster and proceeds by merging the blocks until the number
    of desired clusters is reached. In order to perform this task, two elements are
    needed: a metric function (also called affinity) and a linkage criterion. The
    former is used to determine the distance between the elements, while the latter
    is a target function that is used to determine which clusters must be merged.'
  prefs: []
  type: TYPE_NORMAL
- en: We also saw how to visualize this process through dendrograms using SciPy. This
    technique is quite useful when it's necessary to maintain a complete control of
    the process and the final number of clusters is initially unknown (it's easier
    to decide where to cut the graph). We showed how to use scikit-learn to perform
    agglomerative clustering with different metrics and linkages and, at the end of
    the chapter, we also introduced the connectivity constraints that are useful when
    it's necessary to force the process to avoid merging clusters which are too far
    apart.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to introduce the recommendation systems, that
    are employed daily by many different systems to automatically suggest items to
    a user, according to his/her similarity to other users and their preferences.
  prefs: []
  type: TYPE_NORMAL
