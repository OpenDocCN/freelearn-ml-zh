<html><head></head><body>
		<div id="_idContainer102">
			<h1 id="_idParaDest-124"><a id="_idTextAnchor143"/>Chapter 7: Building Robust CI/CD Pipelines</h1>
			<p><a id="_idTextAnchor144"/>In this chapter, you will learn about continuous operations in the MLOps pipeline. The principles you will learn in this chapter are key to driving continuous deployments in a business context. To get a comprehensive understanding and first-hand experience, we will go through the concepts and hands-on implementation simultaneously. We will set up a CI/CD pipeline for the test environment while learning about components of <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) and <strong class="bold">continuous deployment</strong> (<strong class="bold">CD</strong>), pipeline testing, and releases and types of triggers. This will equip you with the skills to automate the deployment pipelines of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models for any given scenario on the cloud with continual learning abilities in tune with business. Let's start by looking at why we need CI/CD in MLOps after all. We will continue by exploring the other topics as follows:</p>
			<ul>
				<li><a id="_idTextAnchor145"/>Continuous integration, delivery, and deployment in MLOps</li>
				<li>Setting up a CI/CD pipeline and test environment (using Azure DevOps)</li>
				<li>Pipeline execution and testing</li>
				<li>Pipeline execution triggers</li>
			</ul>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor146"/>Continuous integration, delivery, and deployment in MLOps</h1>
			<p><strong class="bold">Automation</strong> is <a id="_idIndexMarker516"/>the <a id="_idIndexMarker517"/>primary<a id="_idIndexMarker518"/> reason for<a id="_idIndexMarker519"/> CI/CD in the MLOps workflow. The goal of enabling continuous delivery to the ML service is to maintain data and source code versions of the models, enable triggers to perform necessary jobs in parallel, build artifacts, and release deployments for production. Several cloud vendors are promoting DevOps services to monitor ML services and models in production, as well as orchestrate with other services on the cloud. Using CI and CD, we can enable continual learning, which is critical for a ML system's success. Without continual learning, a ML system is<a id="_idIndexMarker520"/> deemed to end up as a failed <strong class="bold">Proof of Concept</strong> (<strong class="bold">PoC</strong>). </p>
			<p class="author-quote">Only a model deployed with continual learning capabilities can bring business value.</p>
			<p>In order to learn to deploy a model in production with continual learning capabilities, we will explore CI, CD, and continuous delivery methods.</p>
			<p>As <a id="_idIndexMarker521"/>you <a id="_idIndexMarker522"/>can<a id="_idIndexMarker523"/> see<a id="_idIndexMarker524"/> in <em class="italic">Figure 7.1</em>, CI is key to CD and continuous delivery. Let's <a id="_idIndexMarker525"/>see <a id="_idIndexMarker526"/>how these three are interconnected:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B16572_07_01.jpg" alt="Figure 7.1 – Continuous integration, delivery, and deployment pipelines&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Continuous integration, delivery, and deployment pipelines</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor147"/>Continuous integration</h2>
			<p>CI aims to<a id="_idIndexMarker527"/> synchronize the application (ML pipeline and application) with the developer in real time. The developer's changes in commits or merges are validated by creating an application build on the go and by performing automated tests against the build. CI emphasizes automated testing with a focus on checking the application's robustness (if it is not broken or bugged) when new commits are merged to the master or main branch. Whenever a new commit is made to the master branch, a new build is created that is tested for robustness using automated testing. By automating this process, we can avoid delayed delivery of software and other integration challenges that can keep users waiting for days for the release. Automation and testing are <a id="_idIndexMarker528"/>at the heart of CI.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor148"/>Continuous delivery</h2>
			<p>Continuous delivery  extends<a id="_idIndexMarker529"/> from CI to ensure that the new changes or releases are deployed and efficiently brought to users; this is facilitated by automating testing and release processes. Automating testing and release processes enable developers and product managers to deploy the changes with one click of a button, enabling seamless control and supervision capabilities at any phase of the process. In the continuous delivery process, quite often, a human agent (from the QA team) is involved in approving a build (pass or fail) before deploying it in production (as shown in <em class="italic">Figure 7.1</em> in a continuous delivery pipeline). In a typical continuous delivery pipeline, a build goes through preliminary acceptance tests before getting deployed on the staging phase where a human agent supervises the performance using smoke tests and other suitable tests. </p>
			<p>Once the smoke tests have been passed, the human agent passes the build to be deployed in production. Automating the build and release process and having a human agent involved in the process ensures great quality as regards production and we can avoid some pitfalls that may go unnoticed with a fully automated pipeline. Using continuous delivery, a business can have full control over its release process and release a new build in small batches (easy to troubleshoot in the case of blockers or errors) or have a full release within a requisite time frame (daily, weekly, or monthly).</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor149"/>Continuous deployment</h2>
			<p>CD enables full automation and goes one step further than continuous delivery. All stages of build and release to your production are completely automated without any human intervention, unlike in continuous delivery. In such an automated pipeline, only a failed test can stop a new change from being deployed to production. Continuous deployment takes the pressure off the team to maintain the release pipeline and accelerates deployment straight to the customers enabling continual learning via feedback loops with customers. </p>
			<p>With such automation, there is no longer a release day for developers. It takes the pressure off them and they can just focus on building the software without worrying about tests and release management. Developers can build, test, and deploy the software at their convenience and can go live within minutes instead of waiting for release days or for human approval, which can delay the release of software to users by days and sometimes weeks. Continuous deployment ensures full automation to deploy and serve robust <a id="_idIndexMarker530"/>and scalable software to users. </p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor150"/>Setting up a CI/CD pipeline and the test environment (using Azure DevOps)</h1>
			<p>In the <a id="_idIndexMarker531"/>previous<a id="_idIndexMarker532"/> section, we <a id="_idIndexMarker533"/>went<a id="_idIndexMarker534"/> through the theory of CI, continuous delivery, and continuous deployment, and now it is time to see it in practice. Using Azure DevOps, we will set up a simple CI/CD pipeline of our own for the business problem (weather prediction), which we have been working on previously (in <a href="B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124"><em class="italic">Chapter 6</em></a>, <em class="italic">Key Principles for Deploying Your ML System</em>, in the Hands-on deployment section (for the business problem)). </p>
			<p>Azure DevOps is a service provided by Microsoft that facilitates source code management (version control), project management, CI, continuous delivery, and continuous deployment (automated builds, testing, and release capabilities). It also enables life cycle management for software applications. We will use Azure DevOps for hands-on training as it comes with seamless integration with the Azure ML service, which we have been using previously in <a href="B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124"><em class="italic">C</em><em class="italic">hapter 6</em></a>. You will experience the integration and syncing of both services to make deployments with ease. Let's get started. </p>
			<p>Go to your Azure DevOps project, <strong class="source-inline">Learn_MLOps</strong>. Go to the cloned repository and access the <strong class="source-inline">07_CICD_Pipeline</strong> folder. We will use these files (in the folder named <strong class="source-inline">07_CICD_Pipeline</strong>) as drivers to build a release pipeline:</p>
			<p class="source-code">Learn_MLOps</p>
			<p class="source-code">├──07_CICD_Pipeline</p>
			<p class="source-code">│   ├── AciDeploymentconfig.yml</p>
			<p class="source-code">    ├── AksDeploymentconfig.yml</p>
			<p class="source-code">    └── InferenceConfig.yml</p>
			<p class="source-code">    └── myenv.yml</p>
			<p class="source-code">    └── score.py</p>
			<p>We will deploy previously trained ML models (from <a href="B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Machine Learning Pipelines</em>) on two <a id="_idIndexMarker535"/>deployment targets: one is <strong class="bold">Azure Container Instances</strong> (<strong class="bold">ACI</strong>) for <a id="_idIndexMarker536"/>the test environment, and the second is an <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>) cluster for the production environment. The <strong class="source-inline">AciDeployment.yml</strong> file contains the configuration for the ACI deployment target, and the <strong class="source-inline">AksDeployment.yml</strong> file contains the configuration for the AKS cluster. <strong class="source-inline">InferenceConfig.yml</strong> points to inference artifacts such as <strong class="source-inline">score.py</strong> and <strong class="source-inline">myenv.yml</strong>. </p>
			<p>The functions<a id="_idIndexMarker537"/> defined in <strong class="source-inline">score.py</strong> will <a id="_idIndexMarker538"/>be used to pre-process <a id="_idIndexMarker539"/>the incoming data and infer the<a id="_idIndexMarker540"/> pre-processed data with the ML model to make predictions. The <strong class="source-inline">myenv.yml</strong> file is a configuration for the inference environment, for example, the Python version and packages to install within the environment. These files will be used as drivers to facilitate the release pipeline. Now that you have familiarized yourself with these files, let's begin by connecting the Azure ML service and the Azure DevOps project using a service principal.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor151"/>Creating a service principal</h2>
			<p>We need to sync <a id="_idIndexMarker541"/>Azure ML services and Azure DevOps in order to facilitate CI between both the services. Previously (in <a href="B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Machine Learning Pipelines</em>) we had developed and managed our ML models using Azure ML service, and we used the <strong class="source-inline">Learn_MLOps</strong> workspace. Now, we will connect the Azure ML workspace (named <strong class="source-inline">Learn_MLOps</strong>) with the Azure DevOps project (named <strong class="source-inline">Learn_MLOps</strong>) using a service principal. </p>
			<p>A service principal is an identity created for inter-application communication; it is a connection automation tool to access Azure resources. Service principal also takes care of the networking and connectivity aspects of your applications. Perform the following steps to set up a service principal for the pipelines:</p>
			<ol>
				<li>Go to <strong class="bold">Project Settings</strong> (on the bottom left of your screen) and select <strong class="bold">Service connections</strong>. Click the <strong class="bold">New service connection</strong> option/button to reveal the New service connection window, as shown in <em class="italic">Figure 7.2</em>:<div id="_idContainer083" class="IMG---Figure"><img src="image/B16572_07_02.jpg" alt="Figure 7.2 – New service principal connection&#13;&#10;"/></div><p class="figure-caption">Figure 7.2 – New service principal connection</p></li>
				<li>Select <strong class="bold">Azure Resource Manager</strong> for the connection type and proceed by clicking <strong class="bold">Next</strong>. Select <strong class="bold">Service principal (automatic)</strong> and proceed to the final step of creating a service principal.</li>
				<li>You will be prompted to create a new service connection. Set the scope as <strong class="bold">Machine Learning Workspace</strong> and point to the <strong class="bold">Subscription</strong>, <strong class="bold">Resource group</strong> and <strong class="bold">Machine Learning Workspace</strong> as shown in <em class="italic">Figure 7.3:</em><div id="_idContainer084" class="IMG---Figure"><img src="image/B16572_07_03.jpg" alt="Figure 7.3 – Final step in creating a service principal&#13;&#10;"/></div><p class="figure-caption">Figure 7.3 – Final step in creating a service principal</p></li>
				<li>Name the <a id="_idIndexMarker542"/>service principal in the <strong class="bold">Service connection name </strong>input box (e.g. <strong class="source-inline">mlops_sp</strong> as shown in <em class="italic">Figure 7.3</em>). Lastly, tick the checkbox (<strong class="bold">Grant access permission to all pipelines</strong>) and click <strong class="bold">Save</strong> to create the service principal.</li>
			</ol>
			<p>With this, your service principal with the given name (for example, <strong class="source-inline">mlops_sp</strong>) is ready to be used for orchestrating CI/CD pipelines. Next, we will install the extension used for the pipelines. </p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor152"/>Installing the extension to connect to the Azure ML workspace</h2>
			<p>Microsoft has<a id="_idIndexMarker543"/> developed an extension<a id="_idIndexMarker544"/> called <strong class="bold">Machine Learning</strong>. It is available in the Azure DevOps Marketplace. It is used to orchestrate models and artifacts from our desired Azure ML workspace. It lets us deploy models from the workspace to our desired deployment targets such as ACI or AKS. We will install the ML extension and use it to orchestrate the CI/CD pipeline. Perform the following steps to install the extension:</p>
			<ol>
				<li value="1">Go to the Marketplace to look for the <strong class="bold">Machine Learning</strong> extension. To go to the Marketplace, click on the bag icon in the top right of your screen, as shown in <em class="italic">Figure 7.4</em>:<div id="_idContainer085" class="IMG---Figure"><img src="image/B16572_07_04.jpg" alt="Figure 7.4 – Finding the Azure DevOps Marketplace&#13;&#10;"/></div><p class="figure-caption">Figure 7.4 – Finding the Azure DevOps Marketplace</p><p>After entering the Marketplace, you will be presented with multiple extensions to add to your Azure DevOps project. Next, we will search for the <strong class="bold">Machine Learning</strong> extension.</p></li>
				<li>Search for the <strong class="bold">Machine Learning</strong> extension and install the extension for free. Click the <strong class="bold">Get it free</strong> button to install the extension as shown in <em class="italic">Figure 7.5</em>:</li>
			</ol>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B16572_07_05.jpg" alt="Figure 7.5 – Installing the Machine Learning extension&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Installing the Machine Learning extension</p>
			<p>The <strong class="bold">Machine Learning</strong> extension will be installed upon clicking the <strong class="bold">Get it free</strong> button. After successful installation, you can use the <strong class="bold">Machine Learning</strong> extension to orchestrate<a id="_idIndexMarker545"/> jobs in the CI/CD pipeline. With these prerequisites, you are set to configure the continuous deployment or continuous delivery pipeline.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor153"/>Setting up a continuous integration and deployment pipeline for the test environment</h2>
			<p>In this <a id="_idIndexMarker546"/>section, we will <a id="_idIndexMarker547"/>configure <a id="_idIndexMarker548"/>the CI/CD pipeline for the staging <a id="_idIndexMarker549"/>environment (also called the test environment). We will use this pipeline to facilitate continual learning and automate deployments. Let's get started by going to <strong class="bold">Pipelines</strong> &gt;&gt; <strong class="bold">Releases</strong>, as shown in <em class="italic">Figure 7.6</em>: </p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B16572_07_06.jpg" alt="Figure 7.6 – Setting up your CI/CD pipeline  &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Setting up your CI/CD pipeline  </p>
			<p>Create a new pipeline in the <strong class="bold">Release</strong> section. Name your pipeline as you wish. For the demo, the pipeline has been named <strong class="source-inline">Port Weather ML Pipeline</strong>. Next, we will start connecting<a id="_idIndexMarker550"/> the requisite artifacts<a id="_idIndexMarker551"/> to enable the <a id="_idIndexMarker552"/>pipeline, such as the repository<a id="_idIndexMarker553"/> containing the code and the Azure ML workspace containing the models to deploy.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor154"/>Connecting artifacts to the pipeline</h2>
			<p>Connect to your Azure DevOps <a id="_idIndexMarker554"/>repository. The Azure DevOps<a id="_idIndexMarker555"/> repository serves as the central code repository to orchestrate deployments and operations on Azure DevOps. Hence, let's connect the repository (<strong class="source-inline">Learn_MLOps</strong>) to the release pipeline: </p>
			<ol>
				<li value="1">As shown in <em class="italic">Figure 7.7</em>, go to the <strong class="bold">Artifacts</strong> section, click <strong class="bold">Add</strong>, select <strong class="bold">Azure Repository</strong>, and then select the repository (for example, <strong class="screen-inline">Learn_MLOps</strong>) to connect with the release pipeline:<div id="_idContainer088" class="IMG---Figure"><img src="image/B16572_07_07.jpg" alt="Figure 7.7 – Connecting the Azure DevOps repository as an artifact  &#13;&#10;"/></div><p class="figure-caption">Figure 7.7 – Connecting the Azure DevOps repository as an artifact  </p></li>
				<li>Select the default branch (for example, <strong class="bold">master</strong>) and<a id="_idIndexMarker556"/> finally, click the <strong class="bold">Add</strong> button at the bottom of <a id="_idIndexMarker557"/>the screen to connect the repository to the release pipeline. After adding the repository, you will be able to see the repository name (<strong class="screen-inline">Learn_MLOps</strong>) and icon in the <strong class="bold">Artifacts</strong> section.</li>
				<li>Connect to your Azure ML workspace. To connect your Azure ML workspace to the release pipeline, go to the <strong class="bold">Artifacts</strong> section, click the <strong class="bold">Add</strong> button, and select <strong class="bold">Azure ML Model Artifact</strong> for the artifact type, as shown in <em class="italic">Figure 7.8</em>. Select the <strong class="bold">Service Endpoint</strong> (your Azure ML workspace, for example, <strong class="bold">mlops_ws</strong>) and the models to deploy. Let's select <strong class="bold">model-scaler</strong> as the model. We will use the <strong class="source-inline">scaler</strong> artifact previously registered in <a href="B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Machine Learning Pipelines,</em> to scale the incoming data using the standard:<div id="_idContainer089" class="IMG---Figure"><img src="image/B16572_07_08.jpg" alt="Figure 7.8 – Connecting the scaler as an artifact &#13;&#10;"/></div><p class="figure-caption">Figure 7.8 – Connecting the scaler as an artifact </p></li>
				<li>After selecting the <strong class="source-inline">model_scaler</strong> artifact, add<a id="_idIndexMarker558"/> the artifact to the release <a id="_idIndexMarker559"/>pipeline by clicking the <strong class="bold">Add</strong> button.  After adding the <strong class="source-inline">model_scaler</strong> artifact, you will be able to see the model's name (<strong class="source-inline">model_scaler</strong>) and a model icon in the <strong class="bold">Artifacts</strong> section, as shown in <em class="italic">Figure 7.9</em>. <p>In the same way, connect the <strong class="screen-inline">support_vector_classifier</strong> model to the release pipeline artifacts. Start by clicking the <strong class="bold">Add</strong> button on <strong class="bold">Artifacts</strong>, select <strong class="bold">Azure ML Model Artifact</strong>, point to the service endpoint (the service principal connected to your Azure ML workspace, for example: <strong class="source-inline">mlops_sp</strong>) and select the <strong class="screen-inline">support_vector_classifier</strong> model trained previously in <em class="italic">Chapter 4</em>, <em class="italic">Machine Learning Pipelines</em>. Add the model artifact to the pipeline by hitting the <strong class="bold">Add</strong> button: </p></li>
			</ol>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B16572_07_09.jpg" alt="Figure 7.9 – Connected artifacts&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – Connected artifacts</p>
			<p>After <a id="_idIndexMarker560"/>adding the <strong class="screen-inline">support_vector_classifier</strong> model, you <a id="_idIndexMarker561"/>will be able to see the model's name (<strong class="screen-inline">support_vector_classifier</strong>) and a model icon in the <strong class="bold">Artifacts</strong> section, as shown in <em class="italic">Figure 7.9</em>. </p>
			<p>Congratulations! We have all three desired artifacts (<strong class="source-inline">Learn_MLOps</strong>, <strong class="source-inline">scaler</strong>, and <strong class="bold">support_vector_classifier</strong>) connected to the release pipeline. We can use these artifacts to orchestrate the deployments in the pipeline. Next, get ready to configure the Staging/TEST environment!</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor155"/>Setting up a test environment</h2>
			<p>Let's set up a<a id="_idIndexMarker562"/> continuous integration and continuous deployment pipeline for the TEST environment in the pipeline. In this stage, we test the robustness of the service and perform various tests to validate the service readiness for production: </p>
			<ol>
				<li value="1">To get started, click on the <strong class="bold">Add a stage</strong> box in the <strong class="bold">Stages</strong> section and add an empty job (as shown in <em class="italic">Figure 7.6</em>) with the name <strong class="source-inline">DEV</strong> <strong class="source-inline">TEST</strong>. We will name the stage <strong class="source-inline">DEV</strong> <strong class="source-inline">TEST</strong> as this will be our development and testing environment. Ideally, both DEV and TEST are different stages, but for simplicity and avoiding repetitive implementation, we will merge them both. See the following <em class="italic">Figure 7.10</em>:<div id="_idContainer091" class="IMG---Figure"><img src="image/B16572_07_10.jpg" alt="Figure 7.10 – Setting up the DEV TEST stage&#13;&#10;"/></div><p class="figure-caption">Figure 7.10 – Setting up the DEV TEST stage</p></li>
				<li>After naming <a id="_idIndexMarker563"/>the stage, save the stage by clicking the <strong class="bold">Save</strong> button at the top. Every stage is a composition of a series of steps or jobs to check the robustness of the stage. Next, we will configure the jobs within the <strong class="bold">DEV TEST</strong> stage. A CI/CD job, in simple terms, is a process or script to execute or test deployments (for example, a job to deploy a model on the Kubernetes cluster). To configure jobs, click <a id="_idIndexMarker564"/>on the <strong class="bold">1 job, 0 task</strong> link in the <strong class="bold">DEV TEST</strong> stage, as shown in <em class="italic">Figure 7.11</em>:  <div id="_idContainer092" class="IMG---Figure"><img src="image/B16572_07_11.jpg" alt="Figure 7.11 – Configuring DEV TEST jobs&#13;&#10;"/></div><p class="figure-caption">Figure 7.11 – Configuring DEV TEST jobs</p><p>Upon <a id="_idIndexMarker565"/>clicking the <strong class="bold">1 job, 0 task</strong> link in the <strong class="bold">DEV TEST</strong> stage, you will have to add agent jobs. </p></li>
				<li>Add a task to the agent job by clicking <strong class="bold">+</strong> in the <strong class="bold">Agent job</strong> tab. We will use a pre-made template job named <strong class="bold">AzureML Model Deploy</strong>. Search and add <strong class="source-inline">AzureML model deploy</strong>, as shown in <em class="italic">Figure 7.12</em>:<div id="_idContainer093" class="IMG---Figure"><img src="image/B16572_07_12.jpg" alt="Figure 7.12 – Adding a job – AzureML Model Deploy&#13;&#10;"/></div><p class="figure-caption">Figure 7.12 – Adding a job – AzureML Model Deploy</p><p>Upon adding the <strong class="bold">AzureML Model Deploy</strong> job, you will be prompted to connect to <a id="_idIndexMarker566"/>your Azure ML workspace and the <strong class="source-inline">inferenceconfig</strong> file. </p></li>
				<li>Next, you will be prompted to enter the deployment information. As shown in <em class="italic">Figure 7.13</em>, point to your Azure ML workspace (for example, <strong class="screen-inline">mlops_ws</strong>) and set the <strong class="screen-inline">Model Source</strong> option to <strong class="bold">Model Artifact</strong> (as we are using the model artifacts generated previously when training and packaging models):</li>
			</ol>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B16572_07_13.jpg" alt="Figure 7.13 – Adding a job – Azure ML Model Deploy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13 – Adding a job – Azure ML Model Deploy</p>
			<p>Next, we will look at the <strong class="source-inline">inferenceConfig</strong> file and its functionality. The following snippet is taken<a id="_idIndexMarker567"/> from <strong class="source-inline">inferenceConfig.yml</strong> (in the repository). Here is a snapshot of <strong class="source-inline">inferenceConfig.yml</strong>:</p>
			<p><strong class="source-inline">inferenceConfig.yml</strong></p>
			<p class="source-code">entryScript: score.py</p>
			<p class="source-code">runtime: python</p>
			<p class="source-code">condaFile: myenv.yml</p>
			<p>It is a representation of the settings for a custom environment in which we will deploy our models. It points to the <strong class="source-inline">score.py</strong> file (previously created in <a href="B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124"><em class="italic">Chapter 6</em></a>, <em class="italic">Key Principles for Deploying Your ML System</em>) and the <strong class="source-inline">conda</strong> file <strong class="source-inline">myenv.yml</strong>, which defines the <strong class="source-inline">conda</strong> environment (packages and dependencies to install). Here is a snapshot of <strong class="source-inline">myenv.yml</strong>:</p>
			<p><strong class="source-inline">myenv.yml</strong></p>
			<p class="source-code">name: project_environment</p>
			<p class="source-code">dependencies:</p>
			<p class="source-code">  # The python interpreter version.</p>
			<p class="source-code">  # Currently Azure ML only supports 3.5.2 and later.</p>
			<p class="source-code">- python=3.6.2</p>
			<p class="source-code">- pip:</p>
			<p class="source-code">  - numpy</p>
			<p class="source-code">  - onnxruntime</p>
			<p class="source-code">  - joblib</p>
			<p class="source-code">  - azureml-core~=1.10.0</p>
			<p class="source-code">  - azureml-defaults~=1.10.0</p>
			<p class="source-code">  - scikit-learn==0.20.3</p>
			<p class="source-code">  - inference-schema</p>
			<p class="source-code">  - inference-schema[numpy-support]</p>
			<p class="source-code">  - azureml-monitoring</p>
			<p class="source-code">channels:</p>
			<p class="source-code">- anaconda</p>
			<p class="source-code">- conda-forge</p>
			<p>Both <a id="_idIndexMarker568"/>the <strong class="source-inline">score.py</strong> and <strong class="source-inline">myenv.yml</strong> files are tied up in the <strong class="source-inline">inferenceConfig.yml</strong> file to facilitate the deployment and inference of ML models. Proceed by selecting your inference configuration file (<strong class="source-inline">inferenceConfig.yml</strong>), as shown in <em class="italic">Figure 7.14</em>:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B16572_07_14.jpg" alt="Figure 7.14 – Selecting your inference configuration file&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14 – Selecting your inference configuration file</p>
			<p>After pointing to the <strong class="source-inline">inferenceConfig.yml</strong> file in your Azure DevOps repository, your basic<a id="_idIndexMarker569"/> configuration is done for the deployment. Lastly, we will configure the deployment information by pointing to the <strong class="bold">Model Deployment</strong> target, naming the deployment (web service), and pointing to the deployment configuration file (the deployment configuration file has configurations for deployment, with parameters such as computer type, autoscaler configurations, and other enablers for infrastructure definition). For the test environment, we will deploy the ML web service or endpoint in ACI. Here is a snapshot of the deployment config file (<strong class="source-inline">AciDeploymentConfig.yml</strong>) for ACI: </p>
			<p><strong class="source-inline">AciDeploymentConfig.yml</strong></p>
			<p class="source-code">computeType: ACI</p>
			<p class="source-code">containerResourceRequirements:</p>
			<p class="source-code">    cpu: 1</p>
			<p class="source-code">    memoryInGB: 1</p>
			<p class="source-code">authEnabled: False</p>
			<p class="source-code">sslEnabled: False</p>
			<p class="source-code">appInsightsEnabled: True</p>
			<p>It contains the infrastructural definition for provisioning the requisite compute for deployment, such as CPU units, memory in GB, and other authentication or security definitions. Let's select this deployment configuration file to set up the release pipeline for the staging<a id="_idIndexMarker570"/> environment, as shown in <em class="italic">Figure 7.15</em>: </p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B16572_07_15.jpg" alt="Figure 7.15 – Adding deployment information&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.15 – Adding deployment information</p>
			<p>After adding the deployment configuration file, save the job by clicking the <strong class="bold">Save</strong> button in the top right of the screen and then go to <strong class="bold">Pipelines</strong> &gt;&gt; <strong class="bold">Releases</strong> (on the left of your screen) to see your pipeline successfully set up. Let's continue from here to test the pipeline.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor156"/>Pipeline execution and testing </h1>
			<p>Now, it is <a id="_idIndexMarker571"/>time<a id="_idIndexMarker572"/> to test your pipeline and for that we will create a release and validate whether the pipeline release has executed successfully. The following steps will help you to test your pipeline:</p>
			<ol>
				<li value="1">Click on the <strong class="bold">Create release</strong> button to execute jobs configured on your pipeline. A popup will appear on the right of your screen (as shown in <em class="italic">Figure 7.16</em>) to view and select artifacts to deploy in your staging environment.</li>
				<li> Select the artifacts (<strong class="source-inline">_scaler</strong> and <strong class="source-inline">_support-vector-classifier</strong>) and select their versions. For simplicity, version 1 is recommended for both. <p>If you want to choose another version of your model or scaler make sure to change the path of your model and scaler in the <strong class="source-inline">score.py</strong> file (that is, insert the appropriate version number in the <strong class="source-inline">scaler</strong> and <strong class="source-inline">model</strong> paths <strong class="source-inline">model-scaler/{version number}/modelscaler.pkl</strong> and <strong class="source-inline">support-vector-classifier/ {version number} /svc.onnx</strong>. If you choose<a id="_idIndexMarker573"/> version 1, you don't have to worry about changing<a id="_idIndexMarker574"/> the code in <strong class="source-inline">score.py</strong> file as the paths contain version 1. </p></li>
				<li>After selecting artifacts and needed versions (version 1 is recommended), click on the <strong class="bold">Create</strong> button to create the release for your selected artifacts:<div id="_idContainer097" class="IMG---Figure"><img src="image/B16572_07_16.jpg" alt="Figure 7.16 – Creating a release&#13;&#10;"/></div><p class="figure-caption">Figure 7.16 – Creating a release</p></li>
				<li>Now the release pipeline (the CI/CD pipeline) is triggered to execute. All the steps defined in the pipeline will execute, such as downloading the artifacts, provisioning the ACI compute instance for deployment, and deploying the web service. Upon successful execution, you'll be notified with a green <a id="_idIndexMarker575"/>tick-mark <a id="_idIndexMarker576"/>on your release, as shown in <em class="italic">Figure 7.17</em>: <div id="_idContainer098" class="IMG---Figure"><img src="image/B16572_07_17.jpg" alt="Figure 7.17 – Monitoring releases&#13;&#10;"/></div><p class="figure-caption">Figure 7.17 – Monitoring releases</p></li>
				<li>You can monitor all your releases in the <strong class="bold">Releases</strong> section and manage the deployed web services from the Azure ML service. A successful release means all the steps in the jobs have been executed successfully and your artifacts (<strong class="source-inline">scaler</strong> and <strong class="source-inline">_support-vector-classifier</strong>) have been deployed as a web service on ACI, as shown in <em class="italic">Figure 7.18</em>: <div id="_idContainer099" class="IMG---Figure"><img src="image/B16572_07_18.jpg" alt="Figure 7.18 – Successful jobs in a release (test environment)&#13;&#10;"/></div><p class="figure-caption">Figure 7.18 – Successful jobs in a release (test environment)</p></li>
				<li>Finally, go<a id="_idIndexMarker577"/> and check your Azure ML workspace (from<a id="_idIndexMarker578"/> the <strong class="bold">Endpoints</strong> section) to view the deployed web service, as shown in <em class="italic">Figure 7.19</em>:</li>
			</ol>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B16572_07_19.jpg" alt="Figure 7.19 – Web service deployed on the Azure ML workspace&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.19 – Web service deployed on the Azure ML workspace</p>
			<p>We have successfully deployed a web service in the test environment. We can see the REST endpoint and the service name <strong class="bold">devtest-webservice</strong>. This brings us to the successful conclusion of the building and testing of the CI/CD pipeline for the test environment. Pipelines can be driven <a id="_idIndexMarker579"/>using triggers, and in the next section, we will look at what<a id="_idIndexMarker580"/> the triggers are and how we can use them to build optimal CI/CD pipelines. </p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor157"/>Pipeline execution triggers</h1>
			<p>In an effective CI/CD pipeline, process execution <a id="_idIndexMarker581"/>should be possible by means of multiple events or triggers. Having the option to trigger the pipeline by only regular events, such as code repository or push-or-pull requests, might be a handicap or limitation for the system. Having the option to trigger the pipeline process using multiple events enhances the flexibility and functionality of the CI/CD pipeline. Let's look at some types of triggers that can add value to the CI/CD pipeline<a id="_idIndexMarker582"/> process:</p>
			<ul>
				<li><strong class="bold">Artifactory triggers</strong><p>Artifacts <a id="_idIndexMarker583"/>are generated <a id="_idIndexMarker584"/>at different stages in the pipeline and development process. Generated artifacts, such as a trained model, metadata, uploaded Docker images, or any file that has been uploaded, can be triggered to execute a certain process in the CI/CD pipeline. Having such options can enable great flexibility and functionality for the CI/CD pipeline.</p></li>
				<li><strong class="bold">Docker Hub triggers</strong><p>Every time<a id="_idIndexMarker585"/> you push a new <a id="_idIndexMarker586"/>Docker image to a Docker Hub repository of your choice, a trigger in the CI/CD pipeline can be executed as per requirements. For example, when you upload a new Docker image to Docker Hub (or Azure Container Registry), the pipeline is triggered to deploy the Docker image as a web service. </p></li>
				<li><strong class="bold">Schedule triggers</strong><p>The pipeline<a id="_idIndexMarker587"/> process can be triggered<a id="_idIndexMarker588"/> following a specific time schedule. This type of trigger is very useful for a scheduled clean-up or cron jobs or any other workflow that needs to be run following a time interval; for example, a trigger for ML model retraining at 12:00 every day.</p></li>
				<li><strong class="bold">API triggers</strong><p>The purpose <a id="_idIndexMarker589"/>of API triggers is to integrate<a id="_idIndexMarker590"/> with external services (or any other application or service you have). This can be set up so your pipeline process is triggered based on an event on another system. For example, when the system admin comments <strong class="source-inline">retrain</strong> on a developer's platform, the pipeline can be triggered to retrain the existing deployed model. These triggers are facilitated using API calls.</p></li>
				<li><strong class="bold">Git triggers</strong><p>Git triggers<a id="_idIndexMarker591"/> are commonly used to trigger<a id="_idIndexMarker592"/> pipeline executions, for instance when new code is committed to a branch or a new pull request is made. When changes are made to a repository, then certain processes can be triggered in the pipeline as per requirements. </p></li>
			</ul>
			<p>Azure DevOps provides multiple trigger options (all of the above). Now, let's set up a Git trigger, based on the Git commit made to the repository:</p>
			<ol>
				<li value="1">Go to <strong class="bold">Pipelines</strong> &gt;&gt; <strong class="bold">Releases</strong> and click <strong class="screen-inline">Edit</strong> (in the top right <a id="_idIndexMarker593"/>of your screen) to edit the existing pipeline. </li>
				<li>Click on the repository artifact (named <strong class="screen-inline">_Learn_MLOps</strong>), as shown in <em class="italic">Figure 7.20</em>, and enable (by clicking on the toggle switch) the continuous deployment trigger. </li>
				<li>Add a branch filter by including the develop branch. This will trigger the pipeline to execute when changes or commits are made to the develop branch of the repository. For the test or staging stage, configure a Git trigger for the develop branch only (not the master or another branch). For production we can configure a Git trigger for the master branch. This way, we can separate the Git trigger branches for the test and production stages:<div id="_idContainer101" class="IMG---Figure"><img src="image/B16572_07_20.jpg" alt="Figure 7.20 – Enabling a Git trigger for the test environment &#13;&#10;"/></div><p class="figure-caption">Figure 7.20 – Enabling a Git trigger for the test environment </p></li>
				<li>Click on the <strong class="bold">Save</strong> button at <a id="_idIndexMarker594"/>the top to configure the Git trigger. Congratulations! You have successfully set up a continuous deployment Git trigger for your test environment. Whenever there are changes to the develop branch of the repository, the pipeline will be triggered to deploy a web service in the test (<strong class="bold">DEV TEST</strong>) environment. </li>
			</ol>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor158"/>Summary</h1>
			<p>In this chapter, we have learned the key principles of continuous operations in MLOps, primarily, continuous integration, delivery, and deployment. We have learned this by performing a hands-on implementation of setting up a CI/CD pipeline and test environment using Azure DevOps. We have tested the pipeline for execution robustness and finally looked into some triggers to enhance the functionality of the pipeline and also set up a Git trigger for the test environment. This chapter serves as the foundation for continual operations in MLOps and equips you with the skills to automate the deployment pipelines of ML models for any given scenario on the cloud, with continual learning abilities in tune with your business. </p>
			<p>In the next chapter, we will look into APIs, microservices, and what they have to offer for MLOps-based solutions.</p>
		</div>
	</body></html>