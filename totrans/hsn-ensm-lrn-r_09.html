<html><head></head><body>
<div class="book" title="Chapter&#xA0;9.&#xA0;Ensembling Regression Models" id="1R42S1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09" class="calibre1"/>Chapter 9. Ensembling Regression Models</h1></div></div></div><p class="calibre7">
<a class="calibre1" title="Chapter 3. Bagging" href="part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee">Chapters 3</a>, <span class="strong"><em class="calibre9">Bagging</em></span>, to <a class="calibre1" title="Chapter 8. Ensemble Diagnostics" href="part0057_split_000.html#1MBG21-2006c10fab20488594398dc4871637ee">Chapters 8</a>, <span class="strong"><em class="calibre9">Ensemble Diagnostics</em></span>, were devoted to learning different types of ensembling methods. The discussion was largely based on the classification problem. If the regressand/output of the supervised learning problem is a numeric variable, then we have a regression problem, which will be addressed here. The housing price problem is selected for demonstration purposes throughout the chapter, and the dataset is chosen from a <a id="id379" class="calibre1"/>Kaggle competition: <a class="calibre1" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/</a>. The data consists of numerous variables, including as many as 79 independent variables, with the price of the house as the output/dependent variable. The dataset needs some pre-processing as some variables have missing dates, some variables have lots of levels, with a few of them only occurring very rarely, and some variables have missing data in more than 20% of observations.</p><p class="calibre7">The pre-processing techniques will be succeeded by variable reduction methods and then we will fit important regression models: linear regression, neural network, and regression trees. An ensemble extension of the regression tree will first be provided, and then we will apply the bagging and random forest methods. Various boosting methods will be used to improve the prediction. Stacked ensemble methods will be applied in the concluding section.</p><p class="calibre7">In this chapter, we will cover the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Data pre-processing and visualization</li><li class="listitem">Variable reduction techniques</li><li class="listitem">Regression models</li><li class="listitem">Bagging and Random Forests for the regression data</li><li class="listitem">Boosting regression models</li><li class="listitem">Stacked ensemble methods for regression data</li></ul></div></div>

<div class="book" title="Chapter&#xA0;9.&#xA0;Ensembling Regression Models" id="1R42S1-2006c10fab20488594398dc4871637ee">
<div class="book" title="Technical requirements"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch09lvl1sec66" class="calibre1"/>Technical requirements</h1></div></div></div><p class="calibre7">We will need the following R packages for this chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="literal">adabag</code></li><li class="listitem"><code class="literal">caret</code></li><li class="listitem"><code class="literal">caretEnsemble</code></li><li class="listitem"><code class="literal">ClustofVar</code></li><li class="listitem"><code class="literal">FactoMinR</code></li><li class="listitem"><code class="literal">gbm</code></li><li class="listitem"><code class="literal">ipred</code></li><li class="listitem"><code class="literal">missForest</code></li><li class="listitem"><code class="literal">nnet</code></li><li class="listitem"><code class="literal">NeuralNetTools</code></li><li class="listitem"><code class="literal">plyr</code></li><li class="listitem"><code class="literal">rpart</code></li><li class="listitem"><code class="literal">RSADBE</code></li></ul></div></div></div>
<div class="book" title="Pre-processing the housing data"><div class="book" id="1S2JE2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec67" class="calibre1"/>Pre-processing the housing data</h1></div></div></div><p class="calibre7">The dataset was<a id="id380" class="calibre1"/> selected from <a class="calibre1" href="http://www.kaggle.com">www.kaggle.com</a> and the title of the project is <span class="strong"><strong class="calibre8">House Prices: Advanced Regression Techniques</strong></span>. The main files we will be using are <code class="literal">test.csv</code> and <code class="literal">train.csv</code>, and the files are available in the companion bundle package. A description <a id="id381" class="calibre1"/>of the variables can be found in the <code class="literal">data_description.txt</code> file. Further details, of course, can be obtained at <a class="calibre1" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/</a>. The train dataset contains 1460 observations, while the test dataset contains 1459 observations. The price of the property is known only in the train dataset and are not available for those in the test dataset. We will use the train dataset for model development only. The datasets are first loaded into an R session and a beginning inspection is done using the <code class="literal">read.csv</code>, <code class="literal">dim</code>, <code class="literal">names</code>, and <code class="literal">str</code> functions:</p><div class="informalexample"><pre class="programlisting">&gt; housing_train &lt;- read.csv("../Data/Housing/train.csv",
+                           row.names = 1,na.strings = "NA",
+                           stringsAsFactors = TRUE)
&gt; housing_test &lt;- read.csv("../Data/Housing/test.csv",
+                           row.names = 1,na.strings = "NA",
+                           stringsAsFactors = TRUE)
&gt; dim(housing_train)
[1] 1460   80
&gt; dim(housing_test)
[1] 1459   79
&gt; names(housing_train)
 [1] "MSSubClass"    "MSZoning"      "LotFrontage"   "LotArea"      
 [5] "Street"        "Alley"         "LotShape"      "LandContour"  
 [9] "Utilities"     "LotConfig"     "LandSlope"     "Neighborhood" 


[69] "X3SsnPorch"    "ScreenPorch"   "PoolArea"      "PoolQC"       
[73] "Fence"         "MiscFeature"   "MiscVal"       "MoSold"       
[77] "YrSold"        "SaleType"      "SaleCondition" "SalePrice" 
&gt; str(housing_train)
'data.frame':	1460 obs. of  80 variables:
 $ MSSubClass   : int  60 20 60 70 60 50 20 60 50 190 ...
 $ MSZoning     : Factor w/ 5 levels "C (all)","FV",..: 4 4 4 4 4 4 4 4 5 4 ...
 $ LotFrontage  : int  65 80 68 60 84 85 75 NA 51 50 ...
 $ LotArea      : int  8450 9600 11250 9550 14260 14115 10084 10382 6120 7420 ...
 $ Street       : Factor w/ 2 levels "Grvl","Pave": 2 2 2 2 2 2 2 2 2 2 ...
 $ Alley        : Factor w/ 2 levels "Grvl","Pave": NA NA NA NA NA NA NA NA NA NA ...


 $ MiscFeature  : Factor w/ 4 levels "Gar2","Othr",..: NA NA NA NA NA 3 NA 3 NA NA ...
 $ MiscVal      : int  0 0 0 0 0 700 0 350 0 0 ...
 $ MoSold       : int  2 5 9 2 12 10 8 11 4 1 ...
 $ YrSold       : int  2008 2007 2008 2006 2008 2009 2007 2009 2008 2008 ...
 $ SaleType     : Factor w/ 9 levels "COD","Con","ConLD",..: 9 9 9 9 9 9 9 9 9 9 ...
 $ SaleCondition: Factor w/ 6 levels "Abnorml","AdjLand",..: 5 5 5 1 5 5 5 5 1 5 ...
 $ SalePrice    : int  208500 181500 223500 140000 250000 143000 307000 200000 129900 118000 ...</pre></div><p class="calibre7">In this snippet, the <code class="literal">read.csv</code> function enabled importing the data from the comma-separated values file. The size of the <a id="id382" class="calibre1"/>imported data frame is evaluated using the <code class="literal">dim</code> function, while <code class="literal">names</code> gives us the variable names as stored in the original file. The <code class="literal">str</code> function gives a quick preview of the variable types and also gives a few of the observations.</p><p class="calibre7">The dimensions of the data frames give<a id="id383" class="calibre1"/> the number of variables and the number of observations. The details of all the variables can be found in the <code class="literal">data_description.txt</code> file. It can be seen that what we have on hand is a comprehensive dataset. Now, we ran the option of <code class="literal">na.strings = "NA"</code> in the <code class="literal">read.csv</code> import function, and quite naturally, this implied that we have missing data. When we have missing data in both the training and test data partitions, the author recommends combining the covariates in the partitions and then examining them further. The covariates are first combined and then we find the <a id="id384" class="calibre1"/>number of missing observations for each of the<a id="id385" class="calibre1"/> variables:</p><div class="informalexample"><pre class="programlisting">&gt; housing &lt;- rbind(housing_train[,1:79],housing_test)
&gt; dim(housing)
[1] 2919   79
&gt; sort(sapply(housing,function(x) sum(is.na(x))),dec=TRUE)
       PoolQC   MiscFeature         Alley         Fence   FireplaceQu 
         2909          2814          2721          2348          1420 
  LotFrontage   GarageYrBlt  GarageFinish    GarageQual    GarageCond 
          486           159           159           159           159 
   GarageType      BsmtCond  BsmtExposure      BsmtQual  BsmtFinType2 
          157            82            82            81            80 
 BsmtFinType1    MasVnrType    MasVnrArea      MSZoning     Utilities 
           79            24            23             4             2 
 BsmtFullBath  BsmtHalfBath    Functional   Exterior1st   Exterior2nd 
            2             2             2             1             1 
   BsmtFinSF1    BsmtFinSF2     BsmtUnfSF   TotalBsmtSF    Electrical 
            1             1             1             1             1 
  KitchenQual    GarageCars    GarageArea      SaleType    MSSubClass 
            1             1             1             1             0 
      LotArea        Street      LotShape   LandContour     LotConfig 
            0             0             0             0             0 

  OpenPorchSF EnclosedPorch    X3SsnPorch   ScreenPorch      PoolArea 
            0             0             0             0             0 
      MiscVal        MoSold        YrSold SaleCondition 
            0             0             0             0 </pre></div><p class="calibre7">The <code class="literal">rbind</code> function combines the data in the training and testing datasets. The <code class="literal">is.na(x)</code> code inspects the absence of the values for every element of <code class="literal">x</code>, and the <code class="literal">sum</code> applied tells us the number of missing observations for the variable. The function is then applied for every variable of <code class="literal">housing</code> using the <code class="literal">sapply</code> function. The count of missing observations for the variables is sorted in descending order using the <code class="literal">sort</code> function with the argument <code class="literal">dec=TRUE</code>, and hence it enables<a id="id386" class="calibre1"/> us to find the variables with the most missing numbers in the beginning.</p><p class="calibre7">The reader might be wondering about the rationale<a id="id387" class="calibre1"/> behind the collation of the observations. The intuitive reasoning behind the collation is that while some variables might have missing data, more in the training data than in the test data, or the other way around, it is important that the overall missing percentage does not exceed a certain threshold of the observations. Although we have missing data imputation techniques, using them when the missing data percentage is too high might cause us to miss out on the important patterns of the features. Consequently, we arbitrarily make a choice of restricting the variables if more than 10% of the values are missing. If the missing percentage of any variable exceeds 10%, we will avoid analyzing that variable further. First, we identify the variables that exceed 10%, and then we remove them from the master data frame. The following R code block gives us the desired result:</p><div class="informalexample"><pre class="programlisting">&gt; miss_variables &lt;- names(which(sapply(housing,
+         function(x) sum(is.na(x)))&gt;0.1*nrow(housing_train)))
&gt; miss_variables
 [1] "LotFrontage"  "Alley"        "FireplaceQu"  "GarageType"  
 [5] "GarageYrBlt"  "GarageFinish" "GarageQual"   "GarageCond"  
 [9] "PoolQC"       "Fence"        "MiscFeature" 
&gt; length(miss_variables)
[1] 11
&gt; housing[,miss_variables] &lt;- NULL
&gt; dim(housing)
[1] 2919   68</pre></div><p class="calibre7">The variables that have more than 10% missing observations are first identified and then stored in the <code class="literal">miss_variables</code> character vector, and we have 11 variables that meet this criterion. Such variables are eliminated with the <code class="literal">NULL</code> assignment for them.</p><p class="calibre7">Next, we find the number of levels (distinct) of factor variables. We define a function, <code class="literal">find_df</code>, which will find the number of levels of a factor variable. For numeric and integer variables, it will return <code class="literal">1</code>. The purpose of this exercise will become clear soon enough. The <code class="literal">find_df</code> function is created in the next block:</p><div class="informalexample"><pre class="programlisting">&gt; find_df &lt;- function(x){
+   if(class(x)=="numeric") mdf &lt;- 1
+   if(class(x)=="integer") mdf &lt;- 1
+   if(class(x) =="factor") mdf &lt;- length(levels(x))
+   if(class(x) =="character") mdf &lt;- length(unique(x))
+   return(mdf)
+ }
&gt; sapply(housing,find_df)
   MSSubClass      MSZoning       LotArea        Street      LotShape 
            1             4             1             2             3 
  LandContour     Utilities     LotConfig     LandSlope  Neighborhood 
            2             3             4             2            25 
   Condition1    Condition2      BldgType    HouseStyle   OverallQual 
            3             2             3             4             1 


   X3SsnPorch   ScreenPorch      PoolArea       MiscVal        MoSold 
            1             1             1             1             1 
       YrSold      SaleType SaleCondition 
            1             4             4 
&gt; dim(housing)
[1] 2919   68</pre></div><p class="calibre7">We need to inspect <code class="literal">67</code> variables, following<a id="id388" class="calibre1"/> the elimination of <code class="literal">11</code> variables with more than 10% missing <a id="id389" class="calibre1"/>observations. Some of these might not be factor variables. The <code class="literal">find_df</code> function shows that, for factor variables, the number of levels varies from 2-25. A quick problem now arises for the <code class="literal">Condition2</code> and <code class="literal">Exterior1st</code> variables:</p><div class="informalexample"><pre class="programlisting">&gt; round(table(housing$Condition2)/nrow(housing),2)
Artery  Feedr   Norm   PosA   PosN   RRAe   RRAn   RRNn 
  0.00   0.00   0.99   0.00   0.00   0.00   0.00   0.00 
&gt; round(table(housing$Exterior1st)/nrow(housing),2)
AsbShng AsphShn BrkComm BrkFace  CBlock CemntBd HdBoard ImStucc MetalSd 
   0.02    0.00    0.00    0.03    0.00    0.04    0.15    0.00    0.15 
Plywood   Stone  Stucco VinylSd Wd Sdng WdShing 
   0.08    0.00    0.01    0.35    0.14    0.02 </pre></div><p class="calibre7">In many practical problems, it appears that there will be <span class="strong"><strong class="calibre8">factor variables</strong></span> that have some levels that occur very infrequently. Now, if we have new levels in the test/validation partition, it is not possible to make predictions. From a statistical perspective, we have a technical problem: losing too many <span class="strong"><strong class="calibre8">degrees of freedom</strong></span>. A rudimentary approach is pursued here, and we will simply put together all the observations in the <code class="literal">Others</code> umbrella. A <code class="literal">Truncate_Factor</code> function is created, and this has two arguments: <code class="literal">x</code> and <code class="literal">alpha</code>. The <code class="literal">x</code> object is the variable to be given to the function, and <code class="literal">alpha</code> is the specified fraction below which any variable frequency<a id="id390" class="calibre1"/> would be pooled to obtain <code class="literal">Others</code>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note04" class="calibre1"/>Note</h3><p class="calibre7">If there are certain levels of a factor<a id="id391" class="calibre1"/> that are new in the test dataset, no analytical method will be able to incorporate the influence. Thus, in cases where we have too many infrequent levels, the chances of some levels not being included in the training dataset will be high and the prediction will not yield the output for the test observations.</p></div><p class="calibre7">The <code class="literal">Truncate_Factor</code> function is now created:</p><div class="informalexample"><pre class="programlisting">&gt; Truncate_Factor &lt;- function(x,alpha){
+   xc &lt;- as.character(x); n &lt;- length(x)
+   if(length(unique(x))&lt;=20) {
+     critical &lt;- n*alpha
	+     xc[xc %in% names(which((prop.table(table(xc)))&lt;alpha))] &lt;- "Others"
+   }
+   xc &lt;- as.factor(xc)
+   return(xc)
+ }
&gt; for(i in 1:ncol(housing)){
+   if(any(class(housing[,i]) == c('character','factor'))) 
+     housing[,i] = Truncate_Factor(housing[,i],0.05)
+ }
&gt; table(housing$Condition2)/nrow(housing)
  Norm Others 
  0.99   0.01 
&gt; table(housing$Exterior1st)/nrow(housing)
HdBoard MetalSd  Others Plywood VinylSd Wd Sdng 
  0.151   0.154   0.126   0.076   0.351   0.141 </pre></div><p class="calibre7">We can now see that the <code class="literal">Others</code> level is more frequent and if we randomly create partitions, it is very likely that the problem of unknown levels will not occur.</p><p class="calibre7">You may recollect that we have eliminated the variables that have excessive missing observations thus far. This does not mean that we are free of missing data, as can be quickly noticed:</p><div class="informalexample"><pre class="programlisting">&gt; sum(is.na(housing))
[1] 474
&gt; prod(dim(housing))
[1] 198492</pre></div><p class="calibre7">The <code class="literal">474</code> values can't be ignored. Missing data imputation is an important way of filling the missing values. Although the EM algorithm is a popular method to achieve that, we will apply the Random Forests<a id="id392" class="calibre1"/> technique to simulate the missing observations. The <code class="literal">missForest</code> package was introduced in <a class="calibre1" title="Chapter 4. Random Forests" href="part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee">Chapter 4</a>, <span class="strong"><em class="calibre9">Random Forests</em></span>, and an example was used to simulate the missing values. We will apply this function to the housing data frame. Since the<a id="id393" class="calibre1"/> default number of variables chosen in this function is <code class="literal">mtry=5</code> and we have 68 variables in housing, the number of variables chosen for splitting a node is changed to about p/3 and hence the option of <code class="literal">mtry=20</code> is seen in the next R block. On a machine with 8 GB of RAM, the next single-line code takes several hours to run. Next, we will apply the <code class="literal">missForest</code> function, save the imputed object for future reference, and create the test and training dataset with imputed values:</p><div class="informalexample"><pre class="programlisting">&gt; housing_impute &lt;- missForest(housing,maxiter = 10,ntree=500,mtry=20)
  missForest iteration 1 in progress...done!
  missForest iteration 2 in progress...done!
  missForest iteration 3 in progress...done!
  missForest iteration 4 in progress...done!
  missForest iteration 5 in progress...done!
  missForest iteration 6 in progress...done!
  missForest iteration 7 in progress...done!
There were 14 warnings (use warnings() to see them)
&gt; save(housing_impute,file=
+ '../Data/Housing/housing_covariates_impute.Rdata')
&gt; ht_imp &lt;- cbind(housing_impute$ximp[1:nrow(housing_train),],
+ housing_train$SalePrice)
&gt; save(ht_imp,file='../Data/Housing/ht_imp.Rdata')
&gt; htest_imp &lt;- housing_impute$ximp[(nrow(housing_train)+1):nrow(
+ housing),]
&gt; save(htest_imp,file='../Data/Housing/htest_imp.Rdata')</pre></div><p class="calibre7">The reader should certainly run the <code class="literal">missForest</code> code line on their local machine. However, to save time, the reader can also skip the line and then load the <code class="literal">ht_imp </code>and <code class="literal">htest_imp</code> objects from the code bundle. The next section will show a way of visualizing a large dataset and two data reduction methods.</p></div>

<div class="book" title="Visualization and variable reduction"><div class="book" id="1T1402-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec68" class="calibre1"/>Visualization and variable reduction</h1></div></div></div><p class="calibre7">In the previous section, the<a id="id394" class="calibre1"/> housing data underwent a lot of analytical pre-processing, and we are now ready to further analyze this. First, we begin with visualization. Since we have a lot of variables, the visualization on the R visual device is slightly difficult. As seen in earlier chapters, to visualize the<a id="id395" class="calibre1"/> random forests and other large, complex structures, we will initiate a PDF device and store the graphs in it. In the housing dataset, the main variable is the housing price and so we will first name the output variable <code class="literal">SalePrice</code>. We need to visualize the data in a way that facilitates the<a id="id396" class="calibre1"/> relationship between the numerous variables and the <code class="literal">SalePrice</code>. The independent variables can be either numeric or categorical. If the variables are numeric, a scatterplot will indicate the kind of relationship between the variable and the <code class="literal">SalePrice</code> regressand. If the independent variable is categorical/factor, we will visualize the boxplot at each level of the factor. The <code class="literal">pdf</code>, <code class="literal">plot</code>, and <code class="literal">boxplot</code> functions will help in generating the required plots:</p><div class="informalexample"><pre class="programlisting">&gt; load("../Data/Housing/ht_imp_author.Rdata")
&gt; names(ht_imp)[69] &lt;- "SalePrice"
&gt; SP &lt;- ht_imp$SalePrice
&gt; pdf("../Output/Visualizing_Housing_Data.pdf")
&gt; for(i in 1:68){
+   if(class(ht_imp[,i])=="numeric") {
+     plot(ht_imp[,i],SP,xlab=names(ht_imp)[i],ylab="Sales Price")
+     title(paste("Scatter plot of Sales Price against ", 
+ names(ht_imp)[i]))
+   }
+   if(class(ht_imp[,i])=="factor") {
+     boxplot(SP~ht_imp[,i],xlab=names(ht_imp)[i],ylab=
+ "Sales Price",notch=TRUE)
+     title(paste("Boxplot of Salesprice by ",names(ht_imp)[i]))
+   }
+ }
&gt; dev.off()
null device 
          1 </pre></div><p class="calibre7">The <code class="literal">ht_imp</code> object is loaded from the <code class="literal">ht_imp_author.Rdata</code> file. Note that, if you run the <code class="literal">missForest</code> function on your own and work on that file, then the results will be different from <code class="literal">ht_imp_author.Rdata</code>. The <code class="literal">pdf</code> function is known to initiate a file of the same name, as seen many times<a id="id397" class="calibre1"/> earlier. For the numeric variable, the <code class="literal">if</code> condition is checked and a scatter plot is displayed with the <code class="literal">xlab</code> taking the actual name of the variable as a name for the label along the <span class="strong"><em class="calibre9">x</em></span> axis. The <code class="literal">title</code> function slaps the output of the <code class="literal">paste</code> function, and the <code class="literal">paste</code> function ensures that we have a suitable title for the generated plot. Similar conditions are tested for the factor variables. We will now look at some of the interesting plots. The first plot of <code class="literal">SalePrice</code> with <code class="literal">MSSubClass (</code>see the <code class="literal">Visualizing_Housing_Data.pdf</code> file) is the following:</p><div class="mediaobject"><img src="../images/00372.jpeg" alt="Visualization and variable reduction" class="calibre10"/><div class="caption"><p class="calibre14">Figure 1: Scatter Plot of Sales Price against MSSubClass</p></div></div><p class="calibre11"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note05" class="calibre1"/>Note</h3><p class="calibre7">Note here that although we specified the <code class="literal">MSSubClass</code> variable as a numeric variable, the scatterplot does not give the same impression. Here, the values of the <code class="literal">MSSubClass</code> variable are cluttered around a specific point and then the scale jumps to the next value.</p></div><p class="calibre7">In short, it does not appear to be a <a id="id398" class="calibre1"/>continuous variable and this can be easily verified using the following:</p><div class="informalexample"><pre class="programlisting">&gt; table(ht_imp$MSSubClass)
20  30  40  45  50  60  70  75  80  85  90 120 160 180 190 
536  69   4  12 144 299  60  16  58  20  52  87  63  10  30 </pre></div><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: The reader should convert the <code class="literal">MSSubClass</code> variable to a factor and then apply <code class="literal">Truncate_Factor</code> to reduce the noise. Identify other numeric variables exhibiting this property in the <code class="literal">Visualizing_Housing_Data.pdf</code> file.</p><p class="calibre7">Let's now look at the boxplot for the <code class="literal">MSZoning</code> factor variable:</p><div class="mediaobject"><img src="../images/00373.jpeg" alt="Visualization and variable reduction" class="calibre10"/><div class="caption"><p class="calibre14">Figure 2: Box plots of Sales Price at Three Levels of MSZoning</p></div></div><p class="calibre11"> </p><p class="calibre7">The points beyond the whiskers indicate the presence<a id="id399" class="calibre1"/> of outliers. However, with complex problems, the interpretation is also likely to go awfully wrong. The notches are a useful trick in the display of boxplots. If the notches do not overlap for two levels of variables, it means that the levels are significant and the information is therefore useful, as seen in the display of the boxplot of <code class="literal">SalePrice</code> against the <code class="literal">MSZoning</code> levels.</p><p class="calibre7">The next display of the scatterplot of <code class="literal">SalePrice</code> against <code class="literal">LotArea</code> is taken up:</p><div class="mediaobject"><img src="../images/00374.jpeg" alt="Visualization and variable reduction" class="calibre10"/><div class="caption"><p class="calibre14">Figure 3: Scatter Plot of Sales Price against Lot Area</p></div></div><p class="calibre11"> </p><p class="calibre7">Clearly, the scatterplot shows that there is no meaningful relationship between the two variables <code class="literal">SalePrice</code> and <code class="literal">LotArea</code>. A different type of display is seen between <code class="literal">SalePrice</code> and <code class="literal">TotalBsmtSF</code> in the following figure:</p><div class="mediaobject"><img src="../images/00375.jpeg" alt="Visualization and variable reduction" class="calibre10"/><div class="caption"><p class="calibre14">Figure 4: Scatter Plot of Sales Price against TotalBsmtSF</p></div></div><p class="calibre11"> </p><p class="calibre7">We can clearly see an outlier in the <code class="literal">TotalBsmtSF</code> value at the extreme right of the figure. There is also a cluttering of <a id="id400" class="calibre1"/>values at 0 with <code class="literal">TotalBsmtSF</code>, which might be controlled by some other variable. Alternatively, it may be discovered that there is a zero-inflation of the variable and it therefore could be a mixture variable. Similarly, all other plots can be interpreted. The correlation between the <code class="literal">SalePrice</code> and other numeric variables is obtained next:</p><div class="informalexample"><pre class="programlisting">&gt; cor(ht_imp[sapply(ht_imp,is.numeric)])[,1]
   MSSubClass       LotArea   OverallQual   OverallCond     YearBuilt 
       1.0000       -0.1398        0.0326       -0.0593        0.0279 
 YearRemodAdd    MasVnrArea    BsmtFinSF1    BsmtFinSF2     BsmtUnfSF 
       0.0406        0.0206       -0.0698       -0.0656       -0.1408 
  TotalBsmtSF     X1stFlrSF     X2ndFlrSF  LowQualFinSF     GrLivArea 
      -0.2385       -0.2518        0.3079        0.0465        0.0749 
 BsmtFullBath  BsmtHalfBath      FullBath      HalfBath  BedroomAbvGr 
       0.0035       -0.0023        0.1316        0.1774       -0.0234 
 KitchenAbvGr  TotRmsAbvGrd    Fireplaces    GarageCars    GarageArea 
       0.2817        0.0404       -0.0456       -0.0401       -0.0987 
   WoodDeckSF   OpenPorchSF EnclosedPorch    X3SsnPorch   ScreenPorch 
      -0.0126       -0.0061       -0.0120       -0.0438       -0.0260 
     PoolArea       MiscVal        MoSold        YrSold     SalePrice 
       0.0083       -0.0077       -0.0136       -0.0214       -0.0843</pre></div><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Interpret all the relationships in the <code class="literal">Visualizing_Housing_Data.pdf</code> file and sort the correlations by their absolute value in the preceding R code.</p><p class="calibre7">We made use of the variable of interest for the <a id="id401" class="calibre1"/>visualization, and in turn this led to useful insights. As previously stated, <span class="strong"><em class="calibre9">p = 68</em></span> is a lot of covariates/independent variables. With big data, the complexity will increase in the north direction, and it is known that for many practical applications we have thousands of independent variables. While most visualization techniques are insightful, a shortcoming is that we seldom get insights into higher order relationships. For instance, when it comes to three or more variables, a relationship is seldom richly brought out in graphical displays. It is then important to deploy methods that will reduce the number of variables without being at the expense of information. The two<a id="id402" class="calibre1"/> methods of data reduction to be discussed here are <span class="strong"><strong class="calibre8">principal component analysis</strong></span> and <span class="strong"><strong class="calibre8">variable clustering</strong></span>.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Principal Component Analysis</strong></span> (<span class="strong"><strong class="calibre8">PCA</strong></span>) is a method drawn from the<a id="id403" class="calibre1"/> larger pool of <span class="strong"><strong class="calibre8">multivariate statistics</strong></span>. This<a id="id404" class="calibre1"/> is useful in data reduction as, given the original number of variables, it tries to give a new set of variables that covers most of the variance of the original data in as few new variables as possible. A brief explanation of PCA is given here.</p><p class="calibre7">Suppose we have a random vector of observations <span class="strong"><img src="../images/00376.jpeg" alt="Visualization and variable reduction" class="calibre15"/></span>. Given the random vector <span class="strong"><img src="../images/00377.jpeg" alt="Visualization and variable reduction" class="calibre15"/></span>, PCA finds a new vector of <span class="strong"><em class="calibre9">principal components</em></span> <span class="strong"><img src="../images/00378.jpeg" alt="Visualization and variable reduction" class="calibre15"/></span> such that each <span class="strong"><em class="calibre9">Yi</em></span> is a linear combination of <span class="strong"><img src="../images/00379.jpeg" alt="Visualization and variable reduction" class="calibre15"/></span>. Furthermore, the principal components are such that the variance of <span class="strong"><em class="calibre9">Y1</em></span> is higher than the variance of <span class="strong"><em class="calibre9">Y2</em></span> and both are uncorrelated; the variance of <span class="strong"><em class="calibre9">Y2</em></span> is higher than the variance of <span class="strong"><em class="calibre9">Y3</em></span> and <span class="strong"><em class="calibre9">Y1; Y2 </em></span>and<span class="strong"><em class="calibre9"> Y3 </em></span>are uncorrelated, and so forth. This relates to <span class="strong"><img src="../images/00380.jpeg" alt="Visualization and variable reduction" class="calibre15"/></span>, none of which are correlated with each other. The principal components are set up so that most of the variance of <span class="strong"><img src="../images/00381.jpeg" alt="Visualization and variable reduction" class="calibre15"/></span> is accumulated in the first few principal components (see Chapter 15 of Tattar, et al. (2016) for more information on this). As a result, we can achieve a lot of data reduction. However, the fundamental premise of PCA is that <span class="strong"><img src="../images/00382.jpeg" alt="Visualization and variable reduction" class="calibre15"/></span> is a vector of continuous random variables. In our dataset, we also have factor variables. Consequently, we<a id="id405" class="calibre1"/> can't use PCA for our purposes. A crude method is to ignore the factor variables and simply run the data reduction on the continuous variables. Instead, we would use <code class="literal">factor analysis for mixed data</code>, and the software functions to carry this out are available in the <code class="literal">FactoMineR</code> package.</p><p class="calibre7">Since data reduction only needs to be performed on the covariates and we do not have longitudinal data, the data reduction is applied on the entire set of observations available, and not only on the training dataset. The rationale for carrying out the data reduction on the entire dataset is the same as for truncating the number of levels of a factor variable. The <code class="literal">housing_impute</code> data frame is available in <code class="literal">housing_covariates_impute.Rdata</code>. We will first load it and then apply the <code class="literal">FAMD</code> function to carry out the factor analysis for mixed data:</p><div class="informalexample"><pre class="programlisting">&gt; load("../Data/Housing/housing_covariates_impute.Rdata")
&gt; housing_covariates &lt;- housing_impute$ximp
&gt; housing_cov_famd &lt;- FAMD(housing_covariates,ncp=68,graph=FALSE)
&gt; colnames(housing_cov_famd$eig) &lt;- c("Component","Variance",
+    "Cumulative")
&gt; housing_cov_famd$eig
            Component     Variance Cumulative
comp 1  12.2267562274 9.3334017003  9.33340170
comp 2   5.4502085801 4.1604645650 13.49386627
comp 3   4.5547218487 3.4768869074 16.97075317
comp 4   4.0710151565 3.1076451576 20.07839833
comp 5   3.1669428163 2.4175136002 22.49591193
comp 6   2.8331129142 2.1626816139 24.65859354
comp 7   2.6471571767 2.0207306692 26.67932421
comp 8   2.1871762983 1.6696002277 28.34892444
comp 9   2.1563067109 1.6460356572 29.99496010
comp 10  2.0083000432 1.5330534681 31.52801357


comp 66  0.7691341212 0.5871252834 80.58667899
comp 67  0.7648033308 0.5838193365 81.17049833
comp 68  0.7559712365 0.5770772798 81.74757561
&gt; windows(height=100,width=200)
&gt; pareto.chart(housing_cov_famd$eig[,2])</pre></div><p class="calibre7">In the <code class="literal">FAMD</code> function, the <code class="literal">ncp</code> option is set as equal to 68, since that is the number of variables we have. We would also like to look at how the principal components respond to the dataset. If the <code class="literal">graph=TRUE</code> option is<a id="id406" class="calibre1"/> selected, the function will display the related graphs. The <code class="literal">colnames</code> of <code class="literal">housing_cov_famd$eig</code> is changed as the default names don't do justice to the output it generates. We can see from the Eigen value analysis that the overall 68 components do not complete with the entire variation available in the data. Furthermore, even for the 50% of variance explained by the components, we need to pick 26 of them. As a consequence, the data reduction here does not seem to be productive. However, this does not mean that performance will be poor in the next set of analyses. When applying the <code class="literal">pareto.chart</code> function from the quality control package <code class="literal">qcc, on frequency data</code> gives a Pareto chart. As demonstrated by the percentages, it is clear that if we need 90% of the variance in the original variables to be explained by the principal components, then we will need nearly 60 principal components. Consequently, the number of variables reduced is only 8 and the interpretation is also an additional complexity. This is not good news. However, we will still save the data of principal components:</p><div class="informalexample"><pre class="programlisting">&gt; save(housing_cov_famd,file='../Data/Housing/Housing_FAMD.Rdata')
&gt; Housing_FAMD_Data &lt;- housing_cov_famd$ind$coord
&gt; save(Housing_FAMD_Data,file='../Data/Housing/
+ Housing_FAMD_Data.Rdata')</pre></div><div class="mediaobject"><img src="../images/00383.jpeg" alt="Visualization and variable reduction" class="calibre10"/><div class="caption"><p class="calibre14">Figure 5: Pareto Chart for Contribution of Principal Components</p></div></div><p class="calibre11"> </p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Explore the use of<a id="id407" class="calibre1"/> the <code class="literal">PCAmix</code> function from the R package <code class="literal">PCAmix</code> to reduce the number of variables through principal component analysis.</p></div>

<div class="book" title="Visualization and variable reduction">
<div class="book" title="Variable clustering"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec48" class="calibre1"/>Variable clustering</h2></div></div></div><p class="calibre7">Variables can be grouped together as<a id="id408" class="calibre1"/> we do with observations. To achieve this, we will use the <code class="literal">kmeansvar</code> function from the <code class="literal">ClustOfVar</code> package. The variable clustering package needs to be specified as the quantitative (numeric) variable separately, and the qualitative (factor) variables also need to be specified separately. In addition, we need to specify how many variable clusters we need. The <code class="literal">init</code> option helps in its specification here. The <code class="literal">is.numeric</code> and <code class="literal">is.factor</code> functions are used to identify the numeric and factor variables and the variable clusters are set up:</p><div class="informalexample"><pre class="programlisting">&gt; Housing_VarClust &lt;- kmeansvar(
+     X.quanti = housing_covariates[sapply(housing_covariates,
+                     is.numeric)],
+     X.quali = housing_covariates[sapply(housing_covariates,
+                       is.factor)],init=4)
Error: Some categorical variables have same names of categories,
             rename categories or use the option rename.level=TRUE to rename it automatically</pre></div><p class="calibre7">Oops! It is an error. It is important to recollect that all infrequent levels of factor variables have been labeled as Others. It might be the case that there are other levels that have the same name across variables, which is a very common label choice in survey data, including options such as Very Dissatisfied &lt; Dissatisfied &lt; OK &lt; Good &lt; Excellent. This choice of variable levels can be the same across multiple questions. However, we need the names of the levels to be distinct <a id="id409" class="calibre1"/>across all variables. A manual renaming of labels will be futile and an excessive waste of time. Consequently, we will approach the problem with a set of names that will be unique across the variables, namely the variable names themselves. The variable names will be concatenated with the variable levels and thus we will have distinct factor levels throughout. Using the <code class="literal">paste0</code> function and <code class="literal">mapvalues</code> from the <code class="literal">plyr</code> package, we will carry out the level renaming manipulation first and then apply <code class="literal">kmeansvar</code> again:</p><div class="informalexample"><pre class="programlisting">&gt; hc2 &lt;- housing_covariates
&gt; for(i in 1:ncol(hc2)){
+   if(class(hc2[,i])=="factor") {
+     hc2[,i] &lt;- mapvalues(hc2[,i],from=levels(hc2[,i]),
+     to=paste0(names(hc2)[i],"_",levels(hc2[,i])))
+   }
+ }
&gt; Housing_VarClust &lt;- kmeansvar(
+         X.quanti = hc2[sapply(hc2,is.numeric)],
+         X.quali = hc2[sapply(hc2,is.factor)], init=4)
&gt; Housing_VarClust$cluster
   MSSubClass       LotArea   OverallQual   OverallCond     YearBuilt 
            2             1             1             4             4 
 YearRemodAdd    MasVnrArea    BsmtFinSF1    BsmtFinSF2     BsmtUnfSF 
            4             3             1             2             4 

     BsmtCond  BsmtExposure  BsmtFinType1  BsmtFinType2       Heating 
            3             1             4             2             3 
    HeatingQC    CentralAir    Electrical   KitchenQual    Functional 
            4             1             4             4             4 
   PavedDrive      SaleType SaleCondition 
            1             4             4 
&gt; summary(Housing_VarClust)

Call:
kmeansvar(X.quanti = hc2[sapply(hc2, is.numeric)], X.quali = hc2[sapply(hc2,     is.factor)], init = 4)

number of iterations:  2

Data: 
   number of observations:  2919
   number of  variables:  68
        number of numerical variables:  34
        number of categorical variables:  34
   number of clusters:  4

Cluster  1 : 
             squared loading correlation
X1stFlrSF             0.6059       0.778
TotalBsmtSF           0.5913       0.769
OverallQual           0.5676       0.753

PoolArea              0.0166       0.129
MiscVal               0.0059       0.077
MoSold                0.0024       0.049


Cluster  2 : 
             squared loading correlation
X2ndFlrSF             0.8584      -0.927
HouseStyle            0.7734          NA
TotRmsAbvGrd          0.5185      -0.720

BsmtFinType2          0.0490          NA
BsmtFinSF2            0.0408       0.202
X3SsnPorch            0.0039       0.063

Cluster  3 : 
           squared loading correlation
MasVnrType         0.83189          NA
MasVnrArea         0.82585      -0.909
Heating            0.03532          NA
BsmtCond           0.02681          NA
Utilities          0.00763          NA
YrSold             0.00084       0.029

Cluster  4 : 
              squared loading correlation
Neighborhood           0.7955          NA
YearBuilt              0.7314      -0.855
BsmtQual               0.6792          NA

BsmtHalfBath           0.0087       0.093
Street                 0.0041          NA
Condition2             0.0015          NA

Gain in cohesion (in %):  11.56</pre></div><p class="calibre7">The important question is that although we have<a id="id410" class="calibre1"/> marked the variables in groups, how do we use them? The answer is provided in the coefficients of the variables within each group. To display the coefficients, run <code class="literal">$coef</code> in adjunct to the <code class="literal">clustvar</code> object <code class="literal">Housing_VarClust</code>:</p><div class="informalexample"><pre class="programlisting">&gt; Housing_VarClust$coef
$cluster1
                       [,1]
const              -7.1e+00
LotArea             2.1e-05
OverallQual         2.2e-01

CentralAir_N       -5.3e-01
CentralAir_Y        3.8e-02
PavedDrive_N       -5.2e-01
PavedDrive_Others  -2.8e-01
PavedDrive_Y        5.0e-02

$cluster2
                        [,1]
const                3.79789
MSSubClass          -0.00472
BsmtFinSF2           0.00066

HouseStyle_1.5Fin   -0.11967
HouseStyle_1Story    0.41892
HouseStyle_2Story   -0.69610
HouseStyle_Others    0.10816
BsmtFinType2_Others  0.33286
BsmtFinType2_Unf    -0.04491

$cluster3
                       [,1]
const              -33.1748
MasVnrArea          -0.0039
YrSold               0.0167

BsmtCond_TA         -0.0365
Heating_GasA        -0.0179
Heating_Others       1.1425

$cluster4
                          [,1]
const                 45.30644
OverallCond            0.09221
YearBuilt             -0.01009

SaleCondition_Normal   0.03647
SaleCondition_Others   0.20598
SaleCondition_Partial -0.58877</pre></div><p class="calibre7">Now, for the observations in the data, the corresponding variables are multiplied by the coefficients of the variable clusters to obtain a single vector for that variable cluster. Consequently, we will then have reduced <a id="id411" class="calibre1"/>the 68 variables to 4 variables.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Obtain the cluster variables for the <code class="literal">housing_covariates</code> data frame using the coefficients displayed previously.</p><p class="calibre7">The data pre-processing for the housing problem is now complete. In the next section, we will build the base learners for the regression data.</p></div></div>

<div class="book" title="Regression models"><div class="book" id="1TVKI2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec69" class="calibre1"/>Regression models</h1></div></div></div><p class="calibre7">Sir Francis Galton invented the simple linear regression model near the end of the nineteenth century. The example used <a id="id412" class="calibre1"/>looked at how a parent's height influences the height of their child. This study used data and laid the basis of regression analysis. The correlation between the height of parents and children is well known, and using data on 928 pairs of height measurements, a linear regression was developed by Galton. In an equivalent form, however, the method might have been in informal use before Galton officially invented it. The simple linear regression model consists of a single input (independent) variable and the output is also a single output.</p><p class="calibre7">In this supervised learning method, the target variable/output/dependent variable is a continuous variable, and it can also take values in intervals, including non-negative and real numbers. The input/independent variable has no restrictions and as such it can be numeric, categorical, or in any other form we used earlier for the classification problem. Interestingly though, linear regression models started much earlier than classification regression models such as logistic regression models. Machine learning problems are more often conceptualized based on the classification problem, and the ensemble methods, especially boosting, have been developed by using classification as the motive. The primary reason for this is that the error improvisation gives a nice intuition, and the secondary reason might be due to famous machine learning examples such as digit recognition, spam classification, and so on.</p><p class="calibre7">The simple linear regression extension is the multiple linear regression where we allow more than one independent variable. We will drop the convention of simple and multiple regression altogether and adhere<a id="id413" class="calibre1"/> simply to regression. As a base learner, the linear regression model is introduced first. Interesting datasets will be used to kick-start the linear regression model.</p></div>

<div class="book" title="Regression models">
<div class="book" title="Linear regression model"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec49" class="calibre1"/>Linear regression model</h2></div></div></div><p class="calibre7">In more formal terms, let <span class="strong"><img src="../images/00384.jpeg" alt="Linear regression model" class="calibre15"/></span> be a set of <span class="strong"><em class="calibre9">p</em></span> independent<a id="id414" class="calibre1"/> variables, and <span class="strong"><em class="calibre9">Y </em></span>be<a id="id415" class="calibre1"/>
<span class="strong"><em class="calibre9"> </em></span>the variable of interest. We need to understand the regressand <span class="strong"><em class="calibre9">Y</em></span> in terms of the regressors <span class="strong"><img src="../images/00385.jpeg" alt="Linear regression model" class="calibre15"/></span>. The linear regression model is given by:</p><div class="mediaobject"><img src="../images/00386.jpeg" alt="Linear regression model" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The relationship between <span class="strong"><em class="calibre9">Y</em></span> and the regressors is in linear form; <span class="strong"><img src="../images/00387.jpeg" alt="Linear regression model" class="calibre15"/></span> is the intercept term; <span class="strong"><img src="../images/00388.jpeg" alt="Linear regression model" class="calibre15"/></span> are the regression coefficients; and <span class="strong"><img src="../images/00389.jpeg" alt="Linear regression model" class="calibre15"/></span> is the error term. It needs to be mentioned that the linearity is in terms of the regression coefficients. It is also important to note that the regressors can come in any form and can sometimes be taken as other forms, including log, exponential, and quadratic. The error term <span class="strong"><img src="../images/00390.jpeg" alt="Linear regression model" class="calibre15"/></span> is often assumed to follow the normal distribution with unknown variance and zero mean. More details about the linear regression model can be found in Draper and Smith (1999), Chatterjee and Hadi (2012), and Montgomery, et al. (2005). For information on the implementation of this technique using R software, see Chapter 12 of Tattar, et al. (2016) or Chapter 6 of Tattar (2017).</p><p class="calibre7">First, we will explain the core notions of the linear regression model using the Galton dataset. The data is loaded from the <code class="literal">RSADBE</code> package and, using the <code class="literal">lm</code> function, we can build the model:</p><div class="informalexample"><pre class="programlisting">&gt; data(galton)
&gt; cor(galton)
       child parent
child   1.00   0.46
parent  0.46   1.00
&gt; plot(galton)
&gt; head(galton)
  child parent
1    62     70
2    62     68
3    62     66
4    62     64
5    62     64
6    62     68
&gt; cp_lm &lt;- lm(child~parent,data=galton)
&gt; summary(cp_lm)
Call:
lm(formula = child ~ parent, data = galton)

Residuals:
   Min     1Q Median     3Q    Max 
-7.805 -1.366  0.049  1.634  5.926 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  23.9415     2.8109    8.52   &lt;2e-16 ***
parent        0.6463     0.0411   15.71   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.2 on 926 degrees of freedom
Multiple R-squared:  0.21,	Adjusted R-squared:  0.21 
F-statistic:  247 on 1 and 926 DF,  p-value: &lt;2e-16</pre></div><div class="mediaobject"><img src="../images/00391.jpeg" alt="Linear regression model" class="calibre10"/><div class="caption"><p class="calibre14">Figure 6: Height of Child against Height of Parent - Scatterplot</p></div></div><p class="calibre11"> </p><p class="calibre7">What does this code block tell us? First, we will load the galton data from the RSADBE package and then look at the <code class="literal">cor</code> correlation<a id="id416" class="calibre1"/> between the height of parent and child. The correlation is <code class="literal">0.46</code>, which seems to be a strong, positive correlation. The plot scatterplot indicates the positive correlation too, and consequently we proceed to build the linear regression model of the height of the child as a function of the height of the parent. It is advisable to look at the p-value associated with the model first, which in this case is given in the last line of <code class="literal">summary(cp_lm)</code> as <code class="literal">&lt;2e-16</code>. The smaller p-value means that we reject the null hypothesis of the model being insignificant, and hence the current fitted model is useful. The p-values associated with the intercept and variable term are both <code class="literal">&lt;2e-16</code>, and that again means that the terms are significant. The regression coefficient of <code class="literal">0.6463</code> implies that if a parent is an inch taller, the child's height would increase by a magnitude of the regression coefficient.</p><p class="calibre7">The value of <code class="literal">Multiple R-squared</code> (technically simple R-squared) and <code class="literal">Adjusted R-squared</code> are both <code class="literal">0.21</code>, which is expected as we have a single variable in the model. The interpretation of R-squared is that if we multiply it by 100 (so 21%, in this case), the resulting number is the percentage of variation in the data (height of the child) as explained by the fitted value. The higher the value of this metric, the better the model is. In this example, it means that the height of the parent explains only about 21% of the variation of the child's height. This means that we need to consider other variables. In this case, one starting point might be to consider the height of both parents. The multiple R-square value will keep on increasing if you add more <a id="id417" class="calibre1"/>variables, and hence it is preferable to use the more robust adjusted R-square value. Is it possible to obtain a perfect R-square, for example 1, or 100%?</p><p class="calibre7">A dataset named <code class="literal">Mantel</code> is available online in the bundle package, and we will build a linear regression model to check for its R-square. To do this, we import the dataset and run the <code class="literal">lm</code> function over it:</p><div class="informalexample"><pre class="programlisting">&gt; Mantel &lt;- read.csv("../Data/Mantel.csv")
&gt; Mantel
   Y  X1   X2   X3
1  5   1 1004  6.0
2  6 200  806  7.3
3  8 -50 1058 11.0
4  9 909  100 13.0
5 11 506  505 13.1
&gt; Mantel_lm &lt;- lm(Y~.,data=Mantel)
&gt; summary(Mantel_lm)

Call:
lm(formula = Y ~ ., data = Mantel)

Residuals:
        1         2         3         4         5 
-2.49e-13  2.92e-13  3.73e-14 -3.89e-14 -4.14e-14 

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) -1.00e+03   2.73e-10 -3.67e+12  1.7e-13 ***
X1           1.00e+00   2.73e-13  3.67e+12  1.7e-13 ***
X2           1.00e+00   2.73e-13  3.67e+12  1.7e-13 ***
X3           1.33e-14   2.16e-13  6.00e-02     0.96    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.9e-13 on 1 degrees of freedom
Multiple R-squared:     1,	Adjusted R-squared:     1 
F-statistic: 4.99e+25 on 3 and 1 DF,  p-value: 1.04e-13</pre></div><p class="calibre7">Here, we can see that the R-square is perfect. Let's have some fun before we embark on the serious task of analyzing the housing price data.</p><p class="calibre7">For the <code class="literal">galton</code> dataset, we will add a new variable called <code class="literal">frankenstein</code>, and this variable will be the residuals from the fitted model <code class="literal">cp_lm</code>. A new dataset will be created, which will augment the <code class="literal">galton</code> dataset <a id="id418" class="calibre1"/>with the residuals; the linear model will then be fitted using the <code class="literal">lm</code> function and its R-square will be checked:</p><div class="informalexample"><pre class="programlisting">&gt; d2 &lt;- cbind(galton,residuals(cp_lm))
&gt; names(d2)
[1] "child"            "parent"           "residuals(cp_lm)"
&gt; names(d2) &lt;- c("child","parent","frankenstein")
&gt; cpf_lm &lt;- lm(child~.,d2)
&gt; summary(cpf_lm)
Call:
lm(formula = child ~ ., data = d2)
Residuals:
      Min        1Q    Median        3Q       Max 
-2.60e-15 -7.40e-16 -3.00e-16  2.10e-16  1.02e-13 
Coefficients:
             Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept)  2.39e+01   5.74e-15 4.17e+15   &lt;2e-16 ***
parent       6.46e-01   8.40e-17 7.69e+15   &lt;2e-16 ***
frankenstein 1.00e+00   6.71e-17 1.49e+16   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 4.6e-15 on 925 degrees of freedom
Multiple R-squared:     1,	Adjusted R-squared:     1 
F-statistic: 1.41e+32 on 2 and 925 DF,  p-value: &lt;2e-16
Warning message:
In summary.lm(cpf_lm) : essentially perfect fit: summary may be unreliable</pre></div><p class="calibre7">Don't ignore the warning function. You may recall that such a warning function was not displayed for the <code class="literal">Mantel</code> dataset. This is because this warning can be eliminated by adding a little noise to the <code class="literal">frankenstein</code> variable, consequently making him more monstrous:</p><div class="informalexample"><pre class="programlisting">&gt; d2$frankenstein &lt;- jitter(d2$frankenstein)
&gt; summary(lm(child~.,d2))
Call:
lm(formula = child ~ ., data = d2)
Residuals:
      Min        1Q    Median        3Q       Max 
-0.004072 -0.002052  0.000009  0.001962  0.004121 
Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.39e+01   2.92e-03    8210   &lt;2e-16 ***
parent       6.46e-01   4.27e-05   15143   &lt;2e-16 ***
frankenstein 1.00e+00   3.41e-05   29331   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 0.0023 on 925 degrees of freedom
Multiple R-squared:     1,	Adjusted R-squared:     1 
F-statistic: 5.45e+08 on 2 and 925 DF,  p-value: &lt;2e-16</pre></div><p class="calibre7">We have thus mastered the art of<a id="id419" class="calibre1"/> obtaining a perfect R-square. Playtime is over now; let's move on to the housing dataset. We previously saved the housing dataset for the train and test blocks as the <code class="literal">ht_imp.Rdata</code> and <code class="literal">htest_imp.Rdata</code> files. The author's filename version has been modified by renaming the filenames as <code class="literal">_author</code> to make things clearer. We then separate the training block into training and testing ones. Then, we use the <code class="literal">load</code> function to import the data, partition it with the <code class="literal">sample</code> function, and then use the <code class="literal">lm</code> function to build the regression model:</p><div class="informalexample"><pre class="programlisting">&gt; load("../Data/Housing/ht_imp_author.Rdata")
&gt; load("../Data/Housing/htest_imp_author.Rdata")
&gt; ls()
[1] "ht_imp"    "htest_imp"
&gt; Y &lt;- "SalePrice"
&gt; X &lt;- names(ht_imp)[-69]
&gt; set.seed(12345)
&gt; BV &lt;- sample(c("Build","Validate"),nrow(ht_imp),replace = TRUE,
+              prob=c(0.7,0.3))
&gt; HT_Build &lt;- ht_imp[BV=="Build",]
&gt; HT_Validate &lt;- ht_imp[BV=="Validate",]
&gt; HT_Formula &lt;- as.formula("SalePrice~.")
&gt; HT_LM_01 &lt;- lm(HT_Formula,data=HT_Build)
&gt; summary(HT_LM_01)

Call:
lm(formula = HT_Formula, data = HT_Build)

Residuals:
    Min      1Q  Median      3Q     Max 
-268498  -12222    -409   11351  240990 

Coefficients: (2 not defined because of singularities)
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -2.87e+03   1.53e+06    0.00  0.99850    
MSSubClass           -1.52e+02   7.95e+01   -1.91  0.05583 .  
MSZoningRL            8.55e+03   6.27e+03    1.36  0.17317    
MSZoningRM            1.20e+04   7.50e+03    1.60  0.11011    
LotArea               4.90e-01   1.21e-01    4.04  5.8e-05 ***
StreetPave            2.81e+04   1.70e+04    1.65  0.09979 .  
LotShapeOthers       -3.59e+03   6.12e+03   -0.59  0.55733    
LotShapeReg           1.25e+03   2.40e+03    0.52  0.60111    
LandContourOthers    -1.22e+04   3.99e+03   -3.05  0.00236 ** 
UtilitiesOthers      -5.76e+04   3.25e+04   -1.77  0.07637 .  
LotConfigCulDSac      1.21e+04   4.96e+03    2.44  0.01477 *  
LotConfigInside      -1.62e+03   2.58e+03   -0.63  0.52972    
LotConfigOthers      -1.28e+04   5.57e+03   -2.30  0.02144 *  

 
EnclosedPorch         6.95e+00   1.91e+01    0.36  0.71628    
X3SsnPorch            3.81e+01   3.87e+01    0.98  0.32497    
ScreenPorch           3.78e+01   2.01e+01    1.88  0.05988 .  
PoolArea              5.13e+01   2.60e+01    1.98  0.04842 *  
MiscVal               5.13e-02   6.57e+00    0.01  0.99377    
MoSold               -4.38e+02   3.67e+02   -1.19  0.23313    
YrSold               -1.01e+02   7.53e+02   -0.13  0.89376    
SaleTypeOthers       -4.88e+04   2.19e+04   -2.23  0.02598 *  
SaleTypeWD           -5.10e+04   2.20e+04   -2.32  0.02061 *  
SaleConditionNormal   1.93e+03   4.31e+03    0.45  0.65421    
SaleConditionOthers   1.87e+03   7.42e+03    0.25  0.80168    
SaleConditionPartial -3.21e+04   2.21e+04   -1.45  0.14641    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 28400 on 861 degrees of freedom
Multiple R-squared:  0.884,	Adjusted R-squared:  0.867 
F-statistic: 51.1 on 129 and 861 DF,  p-value: &lt;2e-16</pre></div><p class="calibre7">The accuracy assessment of the fitted linear model will be carried out after fitting three more base learners. The adjusted R-square value is about 87%. However, we have 68 variables, and we can see from the p-value of the previous summary that a lot of variables don't have p-values less than either 0.05 or 0.1. Consequently, we need to get rid of the insignificant variables. The step function can be slapped on many fitted regression models to eliminate the insignificant variables while retaining most of the model characteristics.</p><p class="calibre7">Running the step function in the R <a id="id420" class="calibre1"/>session leads to a huge display of output in the console. The initial output is lost to the space restrictions. Consequently, the author ran the script with the option of <span class="strong"><strong class="calibre8">Compile Report from R Script in RStudio</strong></span>, chose the option of MS Word as the report output format, and saved that file. An abbreviated version of the results from that file is given here:</p><div class="informalexample"><pre class="programlisting">## Start:  AIC=20446.87
## SalePrice ~ MSSubClass + MSZoning + LotArea + Street + LotShape + 
##     LandContour + Utilities + LotConfig + LandSlope + Neighborhood + 
##     Condition1 + Condition2 + BldgType + HouseStyle + OverallQual + 
##     OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl + 
##     Exterior1st + Exterior2nd + MasVnrType + MasVnrArea + ExterQual + 
##     ExterCond + Foundation + BsmtQual + BsmtCond + BsmtExposure + 
##     BsmtFinType1 + BsmtFinSF1 + BsmtFinType2 + BsmtFinSF2 + BsmtUnfSF + 
##     TotalBsmtSF + Heating + HeatingQC + CentralAir + Electrical + 
##     X1stFlrSF + X2ndFlrSF + LowQualFinSF + GrLivArea + BsmtFullBath + 
##     BsmtHalfBath + FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + 
##     KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + GarageCars + 
##     GarageArea + PavedDrive + WoodDeckSF + OpenPorchSF + EnclosedPorch + 
##     X3SsnPorch + ScreenPorch + PoolArea + MiscVal + MoSold + 
##     YrSold + SaleType + SaleCondition
## 
## 
## Step:  AIC=20446.87
## SalePrice ~ MSSubClass + MSZoning + LotArea + Street + LotShape + 
##     LandContour + Utilities + LotConfig + LandSlope + Neighborhood + 
##     Condition1 + Condition2 + BldgType + HouseStyle + OverallQual + 
##     OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl + 
##     Exterior1st + Exterior2nd + MasVnrType + MasVnrArea + ExterQual + 
##     ExterCond + Foundation + BsmtQual + BsmtCond + BsmtExposure + 
##     BsmtFinType1 + BsmtFinSF1 + BsmtFinType2 + BsmtFinSF2 + BsmtUnfSF + 
##     TotalBsmtSF + Heating + HeatingQC + CentralAir + Electrical + 
##     X1stFlrSF + X2ndFlrSF + LowQualFinSF + BsmtFullBath + BsmtHalfBath + 
##     FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + KitchenQual + 
##     TotRmsAbvGrd + Functional + Fireplaces + GarageCars + GarageArea + 
##     PavedDrive + WoodDeckSF + OpenPorchSF + EnclosedPorch + X3SsnPorch + 
##     ScreenPorch + PoolArea + MiscVal + MoSold + YrSold + SaleType + 
##     SaleCondition
## 
## 
## Step:  AIC=20446.87
## SalePrice ~ MSSubClass + MSZoning + LotArea + Street + LotShape + 
##     LandContour + Utilities + LotConfig + LandSlope + Neighborhood + 
##     Condition1 + Condition2 + BldgType + HouseStyle + OverallQual + 
##     OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl + 
##     Exterior1st + Exterior2nd + MasVnrType + MasVnrArea + ExterQual + 
##     ExterCond + Foundation + BsmtQual + BsmtCond + BsmtExposure + 
##     BsmtFinType1 + BsmtFinSF1 + BsmtFinType2 + BsmtFinSF2 + BsmtUnfSF + 
##     Heating + HeatingQC + CentralAir + Electrical + X1stFlrSF + 
##     X2ndFlrSF + LowQualFinSF + BsmtFullBath + BsmtHalfBath + 
##     FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + KitchenQual + 
##     TotRmsAbvGrd + Functional + Fireplaces + GarageCars + GarageArea + 
##     PavedDrive + WoodDeckSF + OpenPorchSF + EnclosedPorch + X3SsnPorch + 
##     ScreenPorch + PoolArea + MiscVal + MoSold + YrSold + SaleType + 
##     SaleCondition
## 
##                 Df  Sum of Sq        RSS   AIC
## - Exterior2nd    5 2.6926e+09 6.9890e+11 20441
## - HeatingQC      3 8.4960e+08 6.9706e+11 20442
## - MasVnrType     3 9.3578e+08 6.9714e+11 20442
## - OverallQual    1 3.2987e+10 7.2919e+11 20491
## - X2ndFlrSF      1 3.9790e+10 7.3600e+11 20500
## - Neighborhood  24 1.6770e+11 8.6391e+11 20613
## 
## Step:  AIC=20440.69
## SalePrice ~ MSSubClass + MSZoning + LotArea + Street + LotShape + 
##     LandContour + Utilities + LotConfig + LandSlope + Neighborhood + 
##     Condition1 + Condition2 + BldgType + HouseStyle + OverallQual + 
##     OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl + 
##     Exterior1st + MasVnrType + MasVnrArea + ExterQual + ExterCond + 
##     Foundation + BsmtQual + BsmtCond + BsmtExposure + BsmtFinType1 + 
##     BsmtFinSF1 + BsmtFinType2 + BsmtFinSF2 + BsmtUnfSF + Heating + 
##     HeatingQC + CentralAir + Electrical + X1stFlrSF + X2ndFlrSF + 
##     LowQualFinSF + BsmtFullBath + BsmtHalfBath + FullBath + HalfBath + 
##     BedroomAbvGr + KitchenAbvGr + KitchenQual + TotRmsAbvGrd + 
##     Functional + Fireplaces + GarageCars + GarageArea + PavedDrive + 
##     WoodDeckSF + OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch + 
##     PoolArea + MiscVal + MoSold + YrSold + SaleType + SaleCondition



## Step:  AIC=20386.81
## SalePrice ~ MSSubClass + LotArea + Street + LandContour + Utilities + 
##     LotConfig + LandSlope + Neighborhood + Condition1 + Condition2 + 
##     BldgType + HouseStyle + OverallQual + OverallCond + YearBuilt + 
##     RoofStyle + RoofMatl + Exterior1st + BsmtQual + BsmtCond + 
##     BsmtExposure + BsmtFinType1 + BsmtFinSF1 + BsmtFinType2 + 
##     X1stFlrSF + X2ndFlrSF + LowQualFinSF + BsmtFullBath + FullBath + 
##     HalfBath + KitchenAbvGr + KitchenQual + TotRmsAbvGrd + Functional + 
##     Fireplaces + GarageCars + WoodDeckSF + ScreenPorch + PoolArea + 
##     MoSold + SaleType
## 
##                Df  Sum of Sq        RSS   AIC
## &lt;none&gt;                       7.1467e+11 20387
## - KitchenAbvGr  1 1.4477e+09 7.1612e+11 20387
## - MoSold        1 1.6301e+09 7.1630e+11 20387
## - BldgType      2 3.1228e+09 7.1779e+11 20387
## - Utilities     1 1.7130e+09 7.1639e+11 20387
## - BsmtCond      1 1.7554e+09 7.1643e+11 20387
## - BsmtFinType2  1 1.8708e+09 7.1654e+11 20387
## - YearBuilt     1 2.0543e+09 7.1673e+11 20388
## - Street        1 2.1163e+09 7.1679e+11 20388
## - LowQualFinSF  1 2.1785e+09 7.1685e+11 20388
## - ScreenPorch   1 2.2387e+09 7.1691e+11 20388
## - MSSubClass    1 2.2823e+09 7.1695e+11 20388
## - LandSlope     1 2.5566e+09 7.1723e+11 20388
## - PoolArea      1 2.6036e+09 7.1728e+11 20388
## - Exterior1st   5 9.1221e+09 7.2379e+11 20389
## - Functional    1 3.4117e+09 7.1808e+11 20390
## - Condition1    2 4.9604e+09 7.1963e+11 20390
## - BsmtFinSF1    1 3.9442e+09 7.1862e+11 20390
## - Condition2    1 4.0659e+09 7.1874e+11 20390
## - RoofStyle     2 6.1817e+09 7.2085e+11 20391
## - HalfBath      1 5.3010e+09 7.1997e+11 20392
## - FullBath      1 5.4987e+09 7.2017e+11 20392
## - Fireplaces    1 6.0438e+09 7.2072e+11 20393
## - TotRmsAbvGrd  1 7.0166e+09 7.2169e+11 20395
## - LandContour   1 7.7036e+09 7.2238e+11 20395
## - WoodDeckSF    1 8.8947e+09 7.2357e+11 20397
## - LotConfig     3 1.2015e+10 7.2669e+11 20397
## - RoofMatl      1 9.0967e+09 7.2377e+11 20397
## - BsmtFullBath  1 9.4178e+09 7.2409e+11 20398
## - HouseStyle    3 1.2940e+10 7.2761e+11 20399
## - BsmtFinType1  5 1.7704e+10 7.3238e+11 20401
## - SaleType      2 1.5305e+10 7.2998e+11 20404
## - LotArea       1 1.4293e+10 7.2897e+11 20404
## - OverallCond   1 1.8131e+10 7.3280e+11 20410
## - BsmtQual      3 2.3916e+10 7.3859e+11 20413
## - X1stFlrSF     1 2.1106e+10 7.3578e+11 20414
## - BsmtExposure  3 2.8182e+10 7.4285e+11 20419
## - GarageCars    1 2.6886e+10 7.4156e+11 20421
## - KitchenQual   3 3.1267e+10 7.4594e+11 20423
## - OverallQual   1 3.7361e+10 7.5203e+11 20435
## - X2ndFlrSF     1 4.3546e+10 7.5822e+11 20443
## - Neighborhood 24 1.8921e+11 9.0389e+11 20572</pre></div><p class="calibre7">The <code class="literal">model</code> is summarized<a id="id421" class="calibre1"/> as follows:</p><div class="informalexample"><pre class="programlisting">&gt; summary(HT_LM_Final)

Call:
lm(formula = SalePrice ~ MSSubClass + LotArea + Street + LandContour + 
    Utilities + LotConfig + LandSlope + Neighborhood + Condition1 + 
    Condition2 + BldgType + HouseStyle + OverallQual + OverallCond + 
    YearBuilt + RoofStyle + RoofMatl + Exterior1st + BsmtQual + 
    BsmtCond + BsmtExposure + BsmtFinType1 + BsmtFinSF1 + BsmtFinType2 + 
    X1stFlrSF + X2ndFlrSF + LowQualFinSF + BsmtFullBath + FullBath + 
    HalfBath + KitchenAbvGr + KitchenQual + TotRmsAbvGrd + Functional + 
    Fireplaces + GarageCars + WoodDeckSF + ScreenPorch + PoolArea + 
    MoSold + SaleType, data = HT_Build)

Residuals:
    Min      1Q  Median      3Q     Max 
-272899  -11717     -42   11228  235349 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         -2.64e+05   1.78e+05   -1.48  0.13894    
MSSubClass          -1.27e+02   7.46e+01   -1.70  0.08965 .  
LotArea              4.75e-01   1.12e-01    4.25  2.3e-05 ***

MoSold              -4.99e+02   3.48e+02   -1.44  0.15136    
SaleTypeOthers      -1.69e+04   5.85e+03   -2.89  0.00396 ** 
SaleTypeWD          -1.76e+04   4.00e+03   -4.40  1.2e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 28100 on 904 degrees of freedom
Multiple R-squared:  0.881,	Adjusted R-squared:  0.87 
F-statistic: 78.1 on 86 and 904 DF,  p-value: &lt;2e-16</pre></div><p class="calibre7">The small module covering the <code class="literal">step</code> function is available in the <code class="literal">Housing_Step_LM.R</code> file and the output generated by<a id="id422" class="calibre1"/> using R Markdown is saved in the file named <code class="literal">Housing_Step_LM.docx</code>. The output of the <code class="literal">step</code> function runs over forty-three pages, but we don't have to inspect the variables left out at each step. It suffices to say that a lot of insignificant variables have been eliminated without losing the traits of the model. The accuracy assessment of the validated partition will be seen later. Next, we will extend the linear regression model to the nonlinear model and work out the neural networks.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Build linear regression models using the principal component and variable cluster variables. Does the accuracy – the R-square – with the set of relevant variables improve the linear regression model?</p></div></div>

<div class="book" title="Regression models">
<div class="book" title="Neural networks"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec50" class="calibre1"/>Neural networks</h2></div></div></div><p class="calibre7">The neural network architecture was introduced in the <span class="strong"><em class="calibre9">Statistical/machine learning models</em></span> section of <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>. Neural networks are capable of handling nonlinear relationships, the <a id="id423" class="calibre1"/>choice of the number of hidden neurons, the choice of transfer functions, and the learning rate (or decay rate) provides a great flexibility in building useful regression models. Haykin (2009) and Ripley (1996) provide two detailed explanations of the theory of neural networks.</p><p class="calibre7">We have looked at the use of <a id="id424" class="calibre1"/>neural networks for classification problems and have also seen the stack ensemble models in action. For the regression model, we need to tell the <code class="literal">nnet</code> function that the output/dependent variable is a continuous variable through the <code class="literal">linout=TRUE</code> option. Here, we will build a neural network with five hidden neurons, <code class="literal">size=5</code>, and run the function for a maximum of 100 iterations, <code class="literal">maxit=100</code>:</p><div class="informalexample"><pre class="programlisting">&gt; HT_NN &lt;- nnet(HT_Formula,data=HT_Build,linout=TRUE,maxit=100,size=5)
# weights:  666
initial  value 38535430702344.617187 
final  value 5951814083616.587891 
converged
&gt; summary(HT_NN)
a 131-5-1 network with 666 weights
options were - linear output units 
   b-&gt;h1   i1-&gt;h1   i2-&gt;h1   i3-&gt;h1   i4-&gt;h1   i5-&gt;h1   i6-&gt;h1   i7-&gt;h1 
-1.0e-02  6.5e-01 -8.0e-02  4.6e-01  5.0e-02 -4.0e-02  3.9e-01  1.3e-01 
  i8-&gt;h1   i9-&gt;h1  i10-&gt;h1  i11-&gt;h1  i12-&gt;h1  i13-&gt;h1  i14-&gt;h1  i15-&gt;h1 
 2.1e-01  4.6e-01  1.9e-01  5.2e-01 -6.6e-01  3.2e-01 -3.0e-02  2.2e-01 
 i16-&gt;h1  i17-&gt;h1  i18-&gt;h1  i19-&gt;h1  i20-&gt;h1  i21-&gt;h1  i22-&gt;h1  i23-&gt;h1 
-2.5e-01 -1.2e-01  3.3e-01 -2.8e-01 -4.6e-01 -3.8e-01 -4.1e-01 -3.2e-01 

 
-4.0e-01 -2.9e-01 -5.1e-01 -2.6e-01  2.5e-01 -6.0e-01  1.0e-02  1.5e-01 
i120-&gt;h5 i121-&gt;h5 i122-&gt;h5 i123-&gt;h5 i124-&gt;h5 i125-&gt;h5 i126-&gt;h5 i127-&gt;h5 
 3.7e-01 -2.0e-01  2.0e-01  1.0e-02 -3.3e-01 -2.4e-01 -1.9e-01  7.0e-01 
i128-&gt;h5 i129-&gt;h5 i130-&gt;h5 i131-&gt;h5 
-1.3e-01 -3.4e-01 -6.9e-01 -6.6e-01 
    b-&gt;o    h1-&gt;o    h2-&gt;o    h3-&gt;o    h4-&gt;o    h5-&gt;o 
 6.3e+04  6.3e+04  6.3e+04 -9.1e+04  4.7e-01 -8.4e+03 </pre></div><p class="calibre7">Note that the neural network architecture is not very useful. However, sometimes we are asked to display what we have <a id="id425" class="calibre1"/>built. Thus, we will use the <code class="literal">plotnet</code> function from the <code class="literal">NeuralNetTools</code> package to generate the network. Since there are too many variables (68 in this case), we save the plot to the <code class="literal">Housing_NN.pdf</code> PDF file and the reader can open it and zoom into the plot to inspect it:</p><div class="informalexample"><pre class="programlisting">&gt; pdf("../Output/Housing_NN.pdf",height = 25, width=60)
&gt; plotnet(HT_NN) # very chaotic network
&gt; dev.off()
RStudioGD 
        2</pre></div><p class="calibre7">The prediction of the neural network will be performed shortly.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise 1</strong></span>: Build neural networks with different decay options; the default is 0. Vary the decay value in the range of 0-0.2, with increments of 0.01, 0.05, and so on.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise 2</strong></span>: Improve the neural network fit using <code class="literal">reltol</code> values, decay values, and a combination of these variables.</p></div></div>

<div class="book" title="Regression models">
<div class="book" title="Regression tree"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec51" class="calibre1"/>Regression tree</h2></div></div></div><p class="calibre7">The regression tree forms the third base<a id="id426" class="calibre1"/> learner for the housing dataset and provides the decision tree structure for the<a id="id427" class="calibre1"/> regression problems. The advantages of the decision tree naturally get carried over to the regression tree. As seen in <a class="calibre1" title="Chapter 3. Bagging" href="part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee">Chapter 3</a>, <span class="strong"><em class="calibre9">Bagging</em></span>, the options for many decision trees are also available for the regression tree.</p><p class="calibre7">We will use the <code class="literal">rpart</code> function from the <code class="literal">rpart</code> library with the default settings to build the regression tree. Using the plot and text functions, we set up the regression tree:</p><div class="informalexample"><pre class="programlisting">&gt; HT_rtree &lt;- rpart(HT_Formula,data=HT_Build)
&gt; windows(height=100,width=100)
&gt; plot(HT_rtree,uniform = TRUE); text(HT_rtree)
&gt; HT_rtree$variable.importance
 OverallQual Neighborhood    YearBuilt    ExterQual  KitchenQual 
     3.2e+12      2.0e+12      1.7e+12      1.7e+12      1.4e+12 
  Foundation   GarageCars    GrLivArea   GarageArea    X1stFlrSF 
     1.3e+12      8.0e+11      6.9e+11      6.1e+11      3.8e+11 
   X2ndFlrSF  TotalBsmtSF TotRmsAbvGrd     BsmtQual   MasVnrArea 
     3.8e+11      3.2e+11      2.7e+11      2.7e+11      1.8e+11 
    FullBath     HalfBath   HouseStyle   BsmtFinSF1 YearRemodAdd 
     1.7e+11      1.3e+11      1.2e+11      1.1e+11      5.3e+10 
    MSZoning BsmtFinType1 BedroomAbvGr  Exterior1st BsmtFullBath 
     4.6e+10      4.4e+10      4.0e+10      2.4e+10      1.1e+10 
     LotArea 
     5.7e+09 </pre></div><div class="mediaobject"><img src="../images/00392.jpeg" alt="Regression tree" class="calibre10"/><div class="caption"><p class="calibre14">Figure 7: Regression Tree for the Sales Price of Houses</p></div></div><p class="calibre11"> </p><p class="calibre7">Which variables are important here? The answer<a id="id428" class="calibre1"/> to this is provided by the variable importance metric. We extract the variable importance from <code class="literal">HT_rtree</code> and the variable with the highest bar length is the most important of all the variables. We will now use the <code class="literal">barplot</code> function for the <code class="literal">HT_rtree</code>:</p><div class="informalexample"><pre class="programlisting">&gt; barplot(HT_rtree$variable.importance,las=2,yaxt="n")</pre></div><div class="mediaobject"><img src="../images/00393.jpeg" alt="Regression tree" class="calibre10"/><div class="caption"><p class="calibre14">Figure 8: Variable Importance of the Regression Tree of Housing Model</p></div></div><p class="calibre11"> </p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Explore the pruning options for a regression tree.</p><p class="calibre7">Next, we will look at the performance of the three base learners for the validation dataset.</p></div></div>

<div class="book" title="Regression models">
<div class="book" title="Prediction for regression models"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec52" class="calibre1"/>Prediction for regression models</h2></div></div></div><p class="calibre7">We separated the housing<a id="id429" class="calibre1"/> training dataset into two sections: train and validate. Now we will use the built models and check how well they are performing. We will do this by looking at the MAPE metric : <code class="literal">|Actual-Predicted|/Actual</code>. Using the <code class="literal">predict</code> function with the <code class="literal">newdata</code> option, the predictions are first obtained, and then the MAPE is calculated for the observations in the validated section of the data:</p><div class="informalexample"><pre class="programlisting">&gt; HT_LM_01_val_hat &lt;- predict(HT_LM_01,newdata = HT_Validate[,-69])
Warning message:
In predict.lm(HT_LM_01, newdata = HT_Validate[, -69]) :
  prediction from a rank-deficient fit may be misleading
&gt; mean(abs(HT_LM_01_val_hat - HT_Validate$SalePrice)/HT_Validate$SalePrice)
[1] 0.11
&gt; HT_LM_Final_val_hat &lt;- predict(HT_LM_Final,newdata = HT_Validate[,-69])
&gt; mean(abs(HT_LM_Final_val_hat - HT_Validate$SalePrice)/HT_Validate$SalePrice)
[1] 0.11
&gt; HT_NN_val_hat &lt;- predict(HT_NN,newdata = HT_Validate[,-69])
&gt; mean(abs(HT_NN_val_hat - HT_Validate$SalePrice)/HT_Validate$SalePrice)
[1] 0.37
&gt; HT_rtree_val_hat &lt;- predict(HT_rtree,newdata = HT_Validate[,-69])
&gt; mean(abs(HT_rtree_val_hat - HT_Validate$SalePrice)/HT_Validate$SalePrice)
[1] 0.17</pre></div><p class="calibre7">The linear regression model <code class="literal">HT_LM_01</code> and the most efficient linear model (by AIC) <code class="literal">HT_LM_Final</code> both give the same accuracy (up to two digits) and the MAPE is <code class="literal">0.11</code> for these two models. The neural network model <code class="literal">HT_NN </code>(with five hidden neurons) results in a MAPE of <code class="literal">0.37</code>, which is a bad result. This<a id="id430" class="calibre1"/> again reinforces the well-known fact that complexity does not necessarily mean accuracy. The accuracy of the regression tree <code class="literal">HT_rtree</code> is <code class="literal">0.17</code>.</p><p class="calibre7">The predicted prices are visualized in the following program:</p><div class="informalexample"><pre class="programlisting">&gt; windows(height = 100,width = 100)
&gt; plot(HT_Validate$SalePrice,HT_LM_01_val_hat,col="blue",
+      xlab="Sales Price",ylab="Predicted Value")
&gt; points(HT_Validate$SalePrice,HT_LM_Final_val_hat,col="green")
&gt; points(HT_Validate$SalePrice,HT_NN_val_hat,col="red")
&gt; points(HT_Validate$SalePrice,HT_rtree_val_hat,col="yellow")
&gt; legend(x=6e+05,y=4e+05,lty=3,
+        legend=c("Linear","Best Linear","Neural Network","Regression Tree"),
+        col=c("blue","green","red","yellow"))</pre></div><div class="mediaobject"><img src="../images/00394.jpeg" alt="Prediction for regression models" class="calibre10"/><div class="caption"><p class="calibre14">Figure 9: Predicting Housing Sales Prices</p></div></div><p class="calibre11"> </p><p class="calibre7">Now we have set up the<a id="id431" class="calibre1"/> base learners, it is time to build the ensembles out of them. We will now build ensemble models based on the homogeneous base learner of the decision tree.</p></div></div>
<div class="book" title="Bagging and Random Forests"><div class="book" id="1UU542-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec70" class="calibre1"/>Bagging and Random Forests</h1></div></div></div><p class="calibre7">
<a class="calibre1" title="Chapter 3. Bagging" href="part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee">Chapter 3</a>, <span class="strong"><em class="calibre9">Bagging</em></span>, and <a class="calibre1" title="Chapter 4. Random Forests" href="part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee">Chapter 4</a>, <span class="strong"><em class="calibre9">Random Forests</em></span>, demonstrate how to improve the stability and accuracy of the basic <a id="id432" class="calibre1"/>decision tree. In this section, we will primarily use the decision tree as base learners and create an ensemble of trees in the same way that we did in <a class="calibre1" title="Chapter 3. Bagging" href="part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee">Chapter 3</a>, <span class="strong"><em class="calibre9">Bagging</em></span>, and <a class="calibre1" title="Chapter 4. Random Forests" href="part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee">Chapter 4</a>, <span class="strong"><em class="calibre9">Random Forests</em></span>.</p><p class="calibre7">The <code class="literal">split</code> function is the <a id="id433" class="calibre1"/>primary difference between bagging and random forest algorithms for classification and regression<a id="id434" class="calibre1"/> trees. Thus, unsurprisingly, we can continue to use the same functions and packages for the regression problem as the counterparts that were used in the classification problem. We will first use the <code class="literal">bagging</code> function from the <code class="literal">ipred</code> package to set up the bagging algorithm for the housing data:</p><div class="informalexample"><pre class="programlisting">&gt; housing_bagging &lt;- bagging(formula = HT_Formula,data=ht_imp,nbagg=500,
+                            coob=TRUE,keepX=TRUE)
&gt; housing_bagging$err
[1] 35820</pre></div><p class="calibre7">The trees in the bagging object can be saved to a PDF file in the same way as in <a class="calibre1" title="Chapter 3. Bagging" href="part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee">Chapter 3</a>, <span class="strong"><em class="calibre9">Bagging</em></span>:</p><div class="informalexample"><pre class="programlisting">&gt; pdf("../Output/Housing_Bagging.pdf")
&gt; for(i in 1:500){
+   temp &lt;- housing_bagging$mtrees[[i]]
+   plot(temp$btree)
+   text(temp$btree,use.n=TRUE)
+ }
&gt; dev.off()
RStudioGD 
        2 </pre></div><p class="calibre7">Since variable importance is not given directly by the <code class="literal">ipred</code> package, and it is always an important measure to know which variables are important, we run a similar loop and program to what was used in <a class="calibre1" title="Chapter 3. Bagging" href="part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee">Chapter 3</a>, <span class="strong"><em class="calibre9">Bagging</em></span>, to get the variable importance plot:</p><div class="informalexample"><pre class="programlisting">&gt; VI &lt;- data.frame(matrix(0,nrow=500,ncol=ncol(ht_imp)-1))
&gt; vnames &lt;- names(ht_imp)[-69]
&gt; names(VI) &lt;- vnames
&gt; for(i in 1:500){
+   VI[i,] &lt;- as.numeric(housing_bagging$mtrees[[i]]$btree$variable.importance[vnames])
+ }
&gt; Bagging_VI &lt;- colMeans(VI,na.rm = TRUE)
&gt; Bagging_VI &lt;- sort(Bagging_VI,dec=TRUE)
&gt; barplot(Bagging_VI,las=2,yaxt="n")
&gt; title("Variable Importance of Bagging")</pre></div><div class="mediaobject"><img src="../images/00395.jpeg" alt="Bagging and Random Forests" class="calibre10"/><div class="caption"><p class="calibre14">Figure 10: Variable Importance Plot of the Bagging Algorithm for the Housing Data</p></div></div><p class="calibre11"> </p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Compare <span class="strong"><em class="calibre9">Figure 10</em></span> with <span class="strong"><em class="calibre9">Figure 8</em></span> to decide whether <a id="id435" class="calibre1"/>we have an overfitting <a id="id436" class="calibre1"/>problem in the regression tree.</p><p class="calibre7">Did bagging improve the prediction performance? This is the important criterion that we need to evaluate. Using the <code class="literal">predict</code> function with the <code class="literal">newdata</code> option, we again calculate the MAPE as follows:</p><div class="informalexample"><pre class="programlisting">&gt; HT_bagging_val_hat &lt;- predict(housing_bagging,newdata = HT_Validate[,-69])
&gt; mean(abs(HT_bagging_val_hat - HT_Validate$SalePrice)/HT_Validate$SalePrice)
[1] 0.13</pre></div><p class="calibre7">The simple regression tree had a MAPE of 17%, and now it is down to 13%. This leads us into the next exercise.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise:</strong></span> Use some of the pruning options with <code class="literal">rpart.control</code> to improve the performance of bagging.</p><p class="calibre7">The next step following bagging is the random forest. We will use the <code class="literal">randomForest</code> function from the package of the same name. Here, we explore 500 trees for this forest. For the regression data, the default<a id="id437" class="calibre1"/> setting for the number of covariates to be randomly <a id="id438" class="calibre1"/>sampled for splitting a node is <code class="literal">mtry = p/3</code>, where <code class="literal">p</code> is the number of covariates. We will use the default choice. The <code class="literal">randomForest</code> function is used to set up the tree ensemble and then <code class="literal">plot_rf</code>, defined in <a class="calibre1" title="Chapter 4. Random Forests" href="part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee">Chapter 4</a>, <span class="strong"><em class="calibre9">Random Forests</em></span>, is used to save the trees of the forest to a PDF file:</p><div class="informalexample"><pre class="programlisting">&gt; housing_RF &lt;- randomForest(formula=HT_Formula,data=ht_imp,ntree=500,
+                            replace=TRUE,importance=TRUE)
&gt; pdf("../Output/Housing_RF.pdf",height=100,width=500)
Error in pdf("../Output/Housing_RF.pdf", height = 100, width = 500) : 
  cannot open file '../Output/Housing_RF.pdf'
&gt; plot_RF(housing_RF)
[1] 1
[1] 2
[1] 3

[1] 498
[1] 499
[1] 500
&gt; dev.off()
null device 
          1 
&gt; windows(height=100,width=200)
&gt; varImpPlot(housing_RF2)</pre></div><p class="calibre7">The variable importance plot for the random forest is given next:</p><div class="mediaobject"><img src="../images/00396.jpeg" alt="Bagging and Random Forests" class="calibre10"/><div class="caption"><p class="calibre14">Figure 11: Variable Importance of the Random Forest for the Housing Data</p></div></div><p class="calibre11"> </p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Find the difference between the two variable importance plots <code class="literal">%lncMSE</code> and <code class="literal">IncNodePurity</code>. Also, compare the variable importance plot of the random forest with the bagging plot and comment on this.</p><p class="calibre7">How accurate is our forest? Using the <code class="literal">predict</code> function, we will get our answer:</p><div class="informalexample"><pre class="programlisting">&gt; HT_RF_val_hat &lt;- predict(housing_RF,newdata = HT_Validate[,-69])
&gt; mean(abs(HT_RF_val_hat - HT_Validate$SalePrice)/HT_Validate$SalePrice)
[1] 0.038</pre></div><p class="calibre7">This is simply brilliant stuff, and the<a id="id439" class="calibre1"/> random forest has significantly improved the accuracy by drastically reducing the MAPE from <code class="literal">0.17</code> to <code class="literal">0.038</code>. This is the outright winner of all the models built thus far.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: In spite of the increased accuracy, try to build forests based on pruned trees and calculate the accuracy.</p><p class="calibre7">Let's see how boosting changes the performance of the trees next.</p></div>
<div class="book" title="Boosting regression models" id="1VSLM1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec71" class="calibre1"/>Boosting regression models</h1></div></div></div><p class="calibre7">
<a class="calibre1" title="Chapter 5. The Bare Bones Boosting Algorithms" href="part0042_split_000.html#181NK1-2006c10fab20488594398dc4871637ee">Chapter 5</a>, <span class="strong"><em class="calibre9">Boosting</em></span>, introduced the boosting method for trees when we had a categorical variable of interest. The adaptation of boosting to the regression problem requires lot of computational changes. For more<a id="id440" class="calibre1"/> information, refer to papers by Zemel and Pitassi (2001), <a class="calibre1" href="http://papers.nips.cc/paper/1797-a-gradient-based-boosting-algorithm-for-regression-problems.pdf">http://papers.nips.cc/paper/1797-a-gradient-based-boosting-algorithm-for-regression-problems.pdf</a> , or Ridgeway, et al. (1999), <a class="calibre1" href="http://dimacs.rutgers.edu/Research/MMS/PAPERS/BNBR.pdf">http://dimacs.rutgers.edu/Research/MMS/PAPERS/BNBR.pdf</a>.</p><p class="calibre7">The <code class="literal">gbm</code> function from the <code class="literal">gbm</code> library will be used to boost the weak learners generated by using random forests. We generate a thousand trees, <code class="literal">n.trees=1e3</code>, and use the <code class="literal">shrinkage</code> factor of <code class="literal">0.05</code>, and then boost the regression trees using the gradient boosting algorithm for regression data:</p><div class="informalexample"><pre class="programlisting">&gt; housing_gbm &lt;- gbm(formula=HT_Formula,data=HT_Build,distribution = "gaussian",
+                    n.trees=1e3,shrinkage = 0.05,keep.data=TRUE,
+                    interaction.depth=1,
+                    cv.folds=3,n.cores = 1)
&gt; summary(housing_gbm)
                        var     rel.inf
OverallQual     OverallQual 29.22608012
GrLivArea         GrLivArea 18.85043432
Neighborhood   Neighborhood 13.79949556

PoolArea           PoolArea  0.00000000
MiscVal             MiscVal  0.00000000
YrSold               YrSold  0.00000000</pre></div><p class="calibre7">This summary gives the variable importance in descending order. The performance of the boosting can be looked into using the <code class="literal">gbm.perf</code> function and since our goal was always to generate a technique that performs well on new data, the out-of-bag curve is also laid over as follows:</p><div class="informalexample"><pre class="programlisting">&gt; windows(height=100,width=200)
&gt; par(mfrow=c(1,2))
&gt; gbm.perf(housing_gbm,method="OOB",plot.it=TRUE,
+                              oobag.curve = TRUE,overlay=TRUE)
[1] 135</pre></div><div class="mediaobject"><img src="../images/00397.jpeg" alt="Boosting regression models" class="calibre10"/><div class="caption"><p class="calibre14">Figure 12: Boosting Convergence for the Housing Data</p></div></div><p class="calibre11"> </p><p class="calibre7">The boosting method has converged <a id="id441" class="calibre1"/>at iteration <span class="strong"><strong class="calibre8">137</strong></span>. Next, we look at the performance of the boosting procedure on the validated data:</p><div class="informalexample"><pre class="programlisting">&gt; HT_gbm_val_hat &lt;- predict(housing_gbm,newdata = HT_Validate[,-69])
Using 475 trees...
&gt; mean(abs(HT_gbm_val_hat - HT_Validate$SalePrice)/HT_Validate$SalePrice)
[1] 0.11</pre></div><p class="calibre7">The MAPE has decreased from 17% to 11%. However, the random forest continues to be the most accurate model thus far.</p></div>
<div class="book" title="Stacking methods for regression models" id="20R681-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec72" class="calibre1"/>Stacking methods for regression models</h1></div></div></div><p class="calibre7">Linear regression models, neural <a id="id442" class="calibre1"/>networks, and regression trees are the three methods that <a id="id443" class="calibre1"/>will be stacked here. We will require the <code class="literal">caret</code> and <code class="literal">caretEnsemble</code> packages to do this task. The stacked ensemble methods have been introduced in detail in <a class="calibre1" title="Chapter 7. The General Ensemble Technique" href="part0051_split_000.html#1GKCM1-2006c10fab20488594398dc4871637ee">Chapter 7</a>, <span class="strong"><em class="calibre9">The General Ensemble Technique</em></span>. First, we specify the control parameters for the training task, specify the list of algorithms, and create the stacked ensemble:</p><div class="informalexample"><pre class="programlisting">&gt; control &lt;- trainControl(method="repeatedcv", number=10, repeats=3, 
+                         savePredictions=TRUE, classProbs=TRUE)
&gt; algorithmList &lt;- c('lm', 'rpart')
&gt; set.seed(12345)
&gt; Emodels &lt;- caretList(HT_Formula, data=HT_Build, trControl=control, 
+                      methodList=algorithmList,
+                      tuneList=list(
+                        nnet=caretModelSpec(method='nnet', trace=FALSE,
+                                            linout=TRUE)
+                        
+                      )
+                      )
There were 37 warnings (use warnings() to see them)</pre></div><p class="calibre7">The neural network is <a id="id444" class="calibre1"/>specified through <code class="literal">caretModelSpec</code>. <code class="literal">Emodels</code> needs to be<a id="id445" class="calibre1"/> resampled for further analysis:</p><div class="informalexample"><pre class="programlisting">&gt; Enresults &lt;- resamples(Emodels)
&gt; summary(Enresults)

Call:
summary.resamples(object = Enresults)

Models: nnet, lm, rpart 
Number of resamples: 30 

MAE 
       Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's
nnet  30462   43466  47098 47879   53335 58286    0
lm    16153   18878  20348 20138   21337 23865    0
rpart 30369   33946  35688 35921   37354 42437    0

RMSE 
       Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's
nnet  42598   66632  70197 69272   73089 85971    0
lm    22508   26137  29192 34347   39803 66875    0
rpart 38721   46508  50528 50980   55705 65337    0

Rsquared 
        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's
nnet  0.0064    0.16   0.32 0.31    0.44 0.74    4
lm    0.4628    0.77   0.85 0.81    0.88 0.92    0
rpart 0.4805    0.55   0.57 0.58    0.61 0.69    0

&gt; dotplot(Enresults)</pre></div><p class="calibre7">The <code class="literal">dotplot</code> is displayed <a id="id446" class="calibre1"/>next:</p><div class="mediaobject"><img src="../images/00398.jpeg" alt="Stacking methods for regression models" class="calibre10"/><div class="caption"><p class="calibre14">Figure 13: R-square, MAE, and RMSE for Housing Data</p></div></div><p class="calibre11"> </p><p class="calibre7">We can see from <span class="strong"><em class="calibre9">Figure 13</em></span> that the R-square is similar for the<a id="id447" class="calibre1"/> three models, although MAE and RMSE are significantly different across the three models. The model correlations can be found using the <code class="literal">modelCor</code> function:</p><div class="informalexample"><pre class="programlisting">&gt; modelCor(Enresults)
        nnet    lm rpart
nnet   1.000 0.033 -0.44
lm     0.033 1.000  0.29
rpart -0.441 0.288  1.00</pre></div><p class="calibre7">We now apply the ensemble method to the validation data:</p><div class="informalexample"><pre class="programlisting">&gt; HT_Validate_Predictions &lt;- rowMeans(predict(Emodels,newdata = HT_Validate))
Warning message:
In predict.lm(modelFit, newdata) :
  prediction from a rank-deficient fit may be misleading
&gt; mean(abs(HT_Validate_Predictions - HT_Validate$SalePrice)/HT_Validate$SalePrice)
[1] 0.16</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note06" class="calibre1"/>Note</h3><p class="calibre7">Note that the results from the<a id="id448" class="calibre1"/> neural network are default and we did not specify the size of the hidden layers. The MAPE of 16% is not desirable and we are better off using the random forest ensemble.</p></div><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Perform the stacked <a id="id449" class="calibre1"/>ensemble methods on the principal components and variable cluster data.</p></div>
<div class="book" title="Summary" id="21PMQ1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec73" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">In this chapter, we extended most of the models and methods learned earlier in the book. The chapter began with a detailed example of housing data, and we carried out the visualization and pre-processing. The principal component method helps in reducing data, and the variable clustering method also helps with the same task. Linear regression models, neural networks, and the regression tree were then introduced as methods that will serve as base learners. Bagging, boosting, and random forest algorithms are some methods that helped to improve the models. These methods are based on homogeneous ensemble methods. This chapter then closed with the stacking ensemble method for the three heterogeneous base learners.</p><p class="calibre7">A different data structure of censored observations will be the topic of the next chapter. Such data is referred to as survival data, and it commonly appears in the study of clinical trials.</p></div></body></html>