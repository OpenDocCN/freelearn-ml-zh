- en: Chapter 13. Ensemble learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。集成学习
- en: This chapter is the concluding chapter of all the learning methods we have learned
    from [Chapter 5](ch05.html "Chapter 5. Decision Tree based learning"), *Decision
    Tree based learning*. It is only apt to have this chapter as a closing chapter
    for the learning methods, as this learning method explains how effectively these
    methods can be used in a combination to maximize the outcome from the learners.
    Ensemble methods have an effective, powerful technique to achieve high accuracy
    across supervised and unsupervised solutions. Different models are efficient and
    perform very well in the selected business cases. It is important to find a way
    to combine the competing models into a committee, and there has been much research
    in this area with a fair degree of success. Also, as different views generate
    a large amount of data, the key aspect is to consolidate different concepts for
    intelligent decision making. Recommendation systems and stream-based text mining
    applications use ensemble methods extensively.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是所有从[第5章](ch05.html "第5章。基于决策树的学习")*基于决策树的学习*中学到的学习方法的总结章节。将本章作为学习方法的结尾章节是恰当的，因为这种学习方法解释了如何有效地将这些方法结合起来以最大化学习者的成果。集成方法在监督和无监督解决方案中具有有效、强大的技术，以实现高精度。不同的模型在选定的业务案例中效率高且表现良好。找到一种将竞争模型组合成委员会的方法很重要，在这方面已经进行了大量研究，并取得了相当程度的成功。此外，由于不同的观点产生了大量数据，关键是要巩固不同的概念以进行智能决策。推荐系统和基于流的文本挖掘应用广泛使用集成方法。
- en: There have been many independent studies done in supervised and unsupervised
    learning groups. The common theme observed was that many mixed models, when brought
    together, strengthened the weak models and brought in an overall better performance.
    One of the important goals of this chapter details a systematic comparison of
    different ensemble techniques that combine both supervised and unsupervised techniques
    and a mechanism to merge the results.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习和无监督学习小组中已经进行了许多独立研究。观察到的共同主题是，当许多混合模型结合在一起时，它们增强了弱模型，并带来了整体更好的性能。本章的一个重要目标是对不同的集成技术进行了系统比较，这些技术结合了监督和无监督技术，并介绍了一种合并结果的方法。
- en: '![Ensemble learning](img/B03980_13_01.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![集成学习](img/B03980_13_01.jpg)'
- en: 'This chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: An overview of ensemble methods based learning—the concept of *the wisdom of
    the crowd* and the key attributes.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法学习的概述——群体智慧的概念和关键属性。
- en: Core ensemble method taxonomy, real-world examples, and applications of ensemble
    learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心集成方法分类、现实世界示例和集成学习应用
- en: 'Ensemble method categories and various representative methods:'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法类别和多种代表性方法：
- en: '**Supervised ensemble methods** provide an overview and a detailed coverage
    of concepts such as bagging, boosting, gradient boosting methods, and Random decision
    trees and Random forests'
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督集成方法**概述和详细介绍了诸如袋装、提升、梯度提升方法和随机决策树和随机森林等概念。'
- en: '**Unsupervised ensemble methods** provide an overview of generative, direct
    and indirect methods that include clustering ensembles'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督集成方法**概述了包括聚类集成在内的生成、直接和间接方法。'
- en: Hands-on implementation exercises using Apache Mahout, R, Julia, Python (scikit-learn),
    and Apache Spark
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Mahout、R、Julia、Python（scikit-learn）和Apache Spark进行动手实现练习
- en: Ensemble learning methods
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习方法
- en: Ensemble, in general, means a group of things that are usually seen as a whole
    rather than in terms of the value as against the individual value. Ensembles follow
    a divide-and-conquer approach used to improve performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 集成，一般而言，意味着一组通常被视为整体而不是单个价值的事物。集成遵循分而治之的方法，用于提高性能。
- en: '![Ensemble learning methods](img/B03980_13_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![集成学习方法](img/B03980_13_02.jpg)'
- en: We will start understanding the specific algorithm with an introduction to the
    famous concept of the wisdom of the crowd.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从介绍著名的群体智慧概念开始，来理解具体的算法。
- en: The wisdom of the crowd
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 群体智慧
- en: Imperfect judgments when aggregated in a right way result in a collective intelligence,
    thus resulting in a superior outcome. The wisdom of the crowd is all about this
    collective intelligence.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当以正确的方式汇总时，不完美的判断会导致集体智慧，从而产生更优越的结果。群体智慧就是关于这种集体智慧。
- en: In general, the term crowd is usually associated with irrationality and the
    common perception that there is some influence, which sways the behavior of the
    crowd in the context of mobs and cults. However, the fact is that this need not
    always be negative and works well when working with collating intellect. The key
    concept of Wisdom of Crowds is that the decisions made by a group of people are
    always robust and accurate than those made by individuals. The ensemble learning
    methods of Machine learning have exploited this idea effectively to produce efficiency
    and accuracy in their results.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The term the wisdom of the crowd was coined by Galton in 1906\. Once he attended
    a farmer's fair where there was a contest to guess the weight of an ox that is
    butchered and dressed. The closest guess won the prize from a total of 800 contestants.
    He chose to collect all the responses and analyze them. When he took the average
    of the guesses, he was shocked to notice that they were very close to the actual
    value. This collective guess was both better than the contestant who won the prize
    and also proved to be the best in comparison to the guesses by cattle experts.
    The democracy of thoughts was a clear winner. For such a useful output, it is
    important that each contestant had his/her strong source of information. The independent
    guess provided by the contestant should not be influenced by his/her neighbor's
    guess, and also, there is an error-free mechanism to consolidate the guesses across
    the group. So in short, this is not an easy process. Another important aspect
    is also to the fact these guesses were superior to any individual expert's guess.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Some basic everyday examples include:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Google search results that usually have the most popular pages listed at the
    top
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a game like "Who wants to be a billionaire", the audience poll is used for
    the answering questions that the contestant has no knowledge about. Usually, the
    answer that is most voted by the crowd is the right answer.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The results of the wisdom of the crowd approach is not guaranteed. Following
    is the basic criteria for an optimal result using this approach:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregation**: There needs to be a foolproof way of consolidating individual
    responses into a collective response or judgment. Without this, the core purpose
    of collective views or responses across the board goes in vain.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence**: Within the crowd, there needs to be discipline around controlling
    the response from one entity over the rest in the crowd. Any influence would skew
    the response, thus impacting the accuracy.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decentralization**: Individual responses have their source and thrive on
    the limited knowledge.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The diversity of opinion**: It is important that each person has a response
    that is isolated; the response''s unusualness is still acceptable.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word ensemble means grouping. To build ensemble classifiers, we first need
    to build a set of classifiers from the training data, aggregate the predictions
    made by these classifiers, and predict a class label of a new record using this
    data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 术语“ensemble”意味着分组。要构建集成分类器，我们首先需要从训练数据中构建一组分类器，汇总这些分类器做出的预测，并使用这些数据预测新记录的类别标签。
- en: 'The following diagram depicts this process:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图解展示了这个过程：
- en: '![The wisdom of the crowd](img/B03980_13_03.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![群体智慧](img/B03980_13_03.jpg)'
- en: 'Technically, the core building blocks include a training set, an inducer, and
    an ensemble generator. Inducer handles defining classifiers for each of the sample
    training datasets. The ensemble generator creates classifiers and a combiner or
    aggregator that consolidates the responses across the combiners. With these building
    blocks and the relationships between them, we have the following properties that
    we will be using to categorize the ensemble methods. The next section covers these
    methods:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上，核心构建块包括训练集、诱导器和集成生成器。诱导器负责为每个样本训练数据集定义分类器。集成生成器创建分类器以及一个组合器或聚合器，该组合器或聚合器整合组合器之间的响应。有了这些构建块及其之间的关系，我们将使用以下属性来分类集成方法。下一节将介绍这些方法：
- en: '**Usage of a combiner**: This property defines the relationship between the
    ensemble generator and the combiner'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合器的使用**：这个属性定义了集成生成器和组合器之间的关系'
- en: '**Dependency between classifiers**: This property defines the degree to which
    the classifiers are dependent on each other'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器之间的依赖性**：这个属性定义了分类器相互依赖的程度'
- en: '**Generating diversity**: This property defines the procedure used to ensure
    diversity across combiners'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成多样性**：这个属性定义了确保组合器之间多样性的程序'
- en: '**The size of the ensemble**: This property denotes the number of classifiers
    used in the ensembles'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成的大小**：这个属性表示集成中使用的分类器数量'
- en: '**Cross inducers**: This property defines how the classifiers leverage the
    inducers. There are cases where the classifiers are built to work with a certain
    set of inducers'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉诱导器**：这个属性定义了分类器如何利用诱导器。有些情况下，分类器被构建成与特定的诱导器集一起工作'
- en: In summary, the building model ensembles first involves building experts and
    letting them provide a response/vote. The expected benefit is an improvement in
    prediction performance and produces a single global structure. Although, any interim
    results produced might end up being difficult to analyze.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，构建模型集成首先涉及构建专家并让他们提供响应/投票。预期的收益是预测性能的提高，并产生一个单一的全球结构。尽管如此，产生的任何中间结果可能最终难以分析。
- en: Let's look at how the performance of an aggregated/combined classifier works
    better in a comprehensive manner.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们全面地看看聚合/组合分类器的性能如何表现得更好。
- en: Let's consider three classifiers that have an error rate of 0.35(ԑ) or an accuracy
    of 0.65\. For each of the classifiers, the probability that the classifier goes
    wrong with its prediction is 35%.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑三个错误率为0.35（ε）或准确率为0.65的分类器。对于每个分类器，分类器预测错误的概率是35%。
- en: '![The wisdom of the crowd](img/B03980_13_04.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![群体智慧](img/B03980_13_04.jpg)'
- en: 'Given here is the truth table denoting the error rate of 0.35(35%) and the
    accuracy rate of 0.65(65%):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里给出的是真值表，表示错误率为0.35（35%）和准确率为0.65（65%）：
- en: '![The wisdom of the crowd](img/B03980_13_05.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![群体智慧](img/B03980_13_05.jpg)'
- en: After the three classifiers are combined, the class label of a test instance
    is predicted by using the majority vote process across the combiners to compute
    the probability that the ensemble classifier makes an error. This is depicted
    in the formula given below.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在三个分类器组合后，通过在组合器之间使用多数投票过程来预测测试实例的类别标签，计算集成分类器犯错的概率。这在下述公式中给出。
- en: '![The wisdom of the crowd](img/B03980_13_06.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![群体智慧](img/B03980_13_06.jpg)'
- en: Moreover, the accuracy is 71.83%. Very clearly, the error rate is lowered when
    aggregated across the classifiers. Now, if we extend this to 25 classifiers, the
    accuracy goes up to 94% as per the computation of the error rate (6%).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，准确率为71.83%。很明显，当在分类器之间汇总时，错误率会降低。现在，如果我们扩展到25个分类器，根据错误率（6%）的计算，准确率会上升到94%。
- en: '![The wisdom of the crowd](img/B03980_13_07.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![群体智慧](img/B03980_13_07.jpg)'
- en: Thus, the ensembles work as they give the bigger picture.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，集成工作得很好，因为它们提供了更大的视角。
- en: We have covered the criteria for the the wisdom of the crowd to work in the
    previous section. Let's now take the preceding case where we have 25 base classifiers,
    and see how the accuracy of the ensemble classifier changes for different error
    rates of the base classifier.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个章节中，我们已经讨论了群体智慧工作的标准。现在，让我们看看我们之前提到的25个基分类器的情况，看看集成分类器的准确率如何随着基分类器不同错误率的变化而变化。
- en: '![The wisdom of the crowd](img/B03980_13_08.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![群体智慧](img/B03980_13_08.jpg)'
- en: Tip
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The ensemble classifier's performance deteriorates and performs much worse than
    the base classifier in cases where the base classifier error rate is more than
    0.5.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当基分类器的错误率超过0.5时，集成分类器的性能会下降，并且比基分类器表现差得多。
- en: In the next section, we will cover some real-world use cases that apply ensemble
    methods.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍一些应用集成方法的现实世界用例。
- en: Key use cases
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键用例
- en: Some of the key real-world applications of ensemble learning methods are detailed
    and discussed in this section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细讨论了集成学习方法在现实世界中的关键应用。
- en: Recommendation systems
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推荐系统
- en: The purpose of recommendation systems is to produce significant or meaningful
    recommendations to a community of users around the products that would possibly
    interest them. Some examples include suggestions related to decision-making process
    such as what books to read on Amazon, what movies to watch on Netflix, or what
    news to read on a news website. The business domain, or the context and the characteristics
    of the business attributes are the primary inputs to the design of recommendation
    systems. The rating (on a scale of 1-5) that users provide for each of the films
    is a significant input as it records the degree to which users interact with the
    system. In addition to this, the details of the user (such as demographics and
    other personal or profile attributes) are also used by the recommender systems
    to identify a potential match between items and potential users.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统的目的是向可能感兴趣的用户社区产生有意义的推荐。一些例子包括与决策过程相关的建议，例如在亚马逊上阅读哪些书籍，在Netflix上观看哪些电影，或在新闻网站上阅读哪些新闻。业务领域，或业务属性的设计的上下文和特征是推荐系统设计的主要输入。用户为每部电影提供的评分（在1-5的范围内）是一个重要的输入，因为它记录了用户与系统的互动程度。此外，用户的详细信息（如人口统计和其他个人或配置文件属性）也被推荐系统用于识别项目与潜在用户之间的潜在匹配。
- en: 'The following screenshot is an example of a recommendation system result on
    Netflix:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是Netflix上推荐系统结果的示例：
- en: '![Recommendation systems](img/B03980_13_09.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![推荐系统](img/B03980_13_09.jpg)'
- en: Anomaly detection
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异常检测
- en: 'Anomaly detection or outlier detection is one of the most popular use cases
    or applications of ensemble learning methods. It is all about finding patterns
    in the data that look abnormal or unusual. Identifying anomalies is important
    as it may result in taking any decisive action. Some famous examples include (among
    many others):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测或离群值检测是集成学习方法最流行的用例或应用之一。这全部关于在数据中寻找异常或不寻常的模式。识别异常很重要，因为它可能导致采取任何决定性行动。一些著名的例子包括（许多其他例子中）：
- en: Fraud detection with credit cards
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信用卡欺诈检测
- en: Unusual disease detection in healthcare
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗保健中的罕见疾病检测
- en: Anomaly detection in aircraft engines
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 飞机发动机中的异常检测
- en: 'Let''s now expand on the example for anomaly detection in aircraft engines.
    Features of a plane engine which are considered to verify if it is anomalous or
    not are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在扩展关于飞机发动机异常检测的例子。以下是一些用于验证飞机发动机是否异常的特征：
- en: Heat generated(*x*[1])
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产生的热量(*x*[1])
- en: The intensity of vibration (*x*[2])
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 振动的强度 (*x*[2])
- en: With the dataset = *x*[(1)]*, x*[(2)] *… x*[(m)] marked, following are the anomalous
    and non-anomalous cases:![Anomaly detection](img/B03980_13_10.jpg)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据集 = *x*[(1)]*，x*[(2)] *… x*[(m)] 标记的情况下，以下是对异常和非异常案例的描述：![异常检测](img/B03980_13_10.jpg)
- en: Transfer learning
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移学习
- en: 'Traditionally, all the Machine learning algorithms assume learning to happen
    from scratch for every new learning problem. The assumption is that no previous
    learning will be leveraged. In cases where the domains for the learning problems
    relate, there will be some learnings from the past that can be acquired and used.
    Some common examples include:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，所有机器学习算法都假设每个新的学习问题都需要从头开始学习。这个假设是没有任何先前的学习会被利用。在学习问题的领域相关的情况下，将有一些从过去可以获取并使用的经验。一些常见的例子包括：
- en: The knowledge of French could help students learn Spanish
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 法语的知识可以帮助学生学习西班牙语
- en: The knowledge of mathematics could help students learn Physics
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学知识可以帮助学生学习物理
- en: The knowledge of driving a car could help drivers learn to drive a truck
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驾驶汽车的知识可以帮助驾驶员学习驾驶卡车
- en: 'In the Machine learning context, this refers to identifying and applying the
    knowledge accumulated from previous tasks to new tasks from a related domain.
    The key here is the ability to identify the commonality between the domains. Reinforcement
    learning and classification and regression problems apply transfer learning. The
    transfer learning process flow is as shown here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，这指的是从相关领域的新任务中识别和应用先前任务积累的知识。这里的关键是识别领域之间的共性。强化学习和分类与回归问题应用迁移学习。迁移学习的过程流程如图所示：
- en: '![Transfer learning](img/B03980_13_11.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![迁移学习](img/B03980_13_11.jpg)'
- en: Stream mining or classification
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流式挖掘或分类
- en: Mining data that comes in as streams has now become a key requirement for a
    wide range of applications with the growing technological advancements and social
    media.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随着技术进步和社交媒体的兴起，流式数据挖掘已成为众多应用的关键需求。
- en: 'The primary difference between the traditional learning is that the training
    and test data sets evolved in a distributed way as they come in streams. The goal
    of prediction now becomes a bit complex as the probabilities keep changing for
    changing timestamps, thus making this an ideal context for applying ensemble learning
    methods. The next diagram shows how *P(y)* changes with timestamp and changes
    to *P(x)* and *P (y|x)*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统学习的主要区别在于，随着流数据的到来，训练和测试数据集以分布式方式演变。预测的目标现在变得稍微复杂一些，因为随着时间戳的变化，概率也在不断变化，这使得应用集成学习方法成为理想的环境。下一个图表显示了
    *P(y)* 如何随时间戳和 *P(x)* 以及 *P (y|x)* 的变化而变化：
- en: '![Stream mining or classification](img/B03980_13_12.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![流式挖掘或分类](img/B03980_13_12.jpg)'
- en: With ensemble learning methods, the variance produced by single models is reduced,
    and the prediction or result is more accurate or robust as the distribution is
    evolving.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用集成学习方法，单个模型产生的方差减少，随着分布的演变，预测或结果更加准确或鲁棒。
- en: Ensemble methods
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成方法
- en: As discussed in the previous sections, ensemble methods are now proven to be
    powerful methods for improving the accuracy and robustness of the supervised,
    semi-supervised, and unsupervised solutions. Also, we have seen how the dynamics
    of decision-making are becoming complex as different sources have started generating
    enormous amounts of data continuously. Effective consolidation is now critical
    for successful and intelligent decision making.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几节所述，集成方法现在已被证明是提高监督学习、半监督学习和无监督学习解决方案的准确性和鲁棒性的强大方法。此外，我们也看到了随着不同来源开始持续产生大量数据，决策动态正变得越来越复杂。有效的整合现在是成功和智能决策的关键。
- en: The supervised and unsupervised ensemble methods share the same principles that
    involve combining the diverse base models that strengthen weak models. In the
    sections to follow, let's look at supervised, semi-supervised, and unsupervised
    techniques independently and in detail.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和无监督集成方法遵循相同的原理，涉及结合多样化的基础模型以增强弱模型。在接下来的章节中，我们将独立且详细地探讨监督、半监督和无监督技术。
- en: 'The following model depicts various learning categories and different algorithms
    that cover both, combining the learning and consensus approaches to the ensemble
    learning:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的模型描述了各种学习类别以及涵盖结合学习和共识方法的各种算法：
- en: '![Ensemble methods](img/B03980_13_13.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![集成方法](img/B03980_13_13.jpg)'
- en: 'Source: On the Power of Ensemble: Supervised and Unsupervised Methods Reconciled
    ([http://it.engineering.illinois.edu/ews/](http://it.engineering.illinois.edu/ews/))'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：关于集成方法的力量：监督学习和无监督方法达成一致([http://it.engineering.illinois.edu/ews/](http://it.engineering.illinois.edu/ews/))
- en: 'Before we go deeper into each of the ensemble techniques, let''s understand
    the difference between combining by learning versus combining by consensus:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨每种集成技术之前，让我们了解通过学习结合与通过共识结合之间的区别：
- en: '|   | Benefits | Downside |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|    | 优点 | 缺点 |'
- en: '| --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Combining by learning** |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **通过学习结合** |'
- en: Uses labeled data as a feedback mechanism
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标记数据作为反馈机制
- en: Has the potential to improve accuracy
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有潜力提高准确性
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Works only with labeled data
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅适用于标记数据
- en: There are chances of over-fitting
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在过拟合的风险
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Combining by consensus** |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **通过共识结合** |'
- en: Does not require labeled data
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要标记数据
- en: Has the potential to improve performance
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有提高性能的潜力
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The valued feedback from the labeled data is missing
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签数据的宝贵反馈信息缺失
- en: Works on the assumption that consensus is a good thing
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于共识是好事的假设
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Supervised ensemble methods
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督集成方法
- en: 'In the case of supervised learning methods, the input is always a labeled data.
    The *combining by learning* method includes boosting stack generalization and
    the rule ensembles techniques. The *combining by consensus* methods includes bagging,
    Random forests, and Random decision trees techniques. The following shows the
    process flow for combining by learning followed by another model for combining
    by consensus:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习方法的情况下，输入始终是标记数据。*通过学习组合*的方法包括Boosting堆叠泛化和规则集成技术。*通过共识组合*的方法包括Bagging、随机森林和随机决策树技术。以下展示了通过学习组合的过程流程，随后是另一个通过共识组合的模型：
- en: '![Supervised ensemble methods](img/B03980_13_14.jpg)![Supervised ensemble methods](img/B03980_13_15.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![监督集成方法](img/B03980_13_14.jpg)![监督集成方法](img/B03980_13_15.jpg)'
- en: 'The supervised ensemble method problem statement is defined as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 监督集成方法的问题陈述如下：
- en: The input data set is *D={x*[1]*, x*[2]*, …, x*[n]*}* the respective labels
    are *L={l*[1]*,l*[2]*,…,l*[n]*}*
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据集是 *D={x*[1]*, x*[2]*, …, x*[n]*}*，相应的标签是 *L={l*[1]*,l*[2]*,…,l*[n]*}*
- en: The ensemble method now generated a set of classifiers *C = {f*[1]*,f*[2]*,…,f*[k]*}*
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法现在生成了一组分类器 *C = {f*[1]*,f*[2]*,…,f*[k]*}*
- en: Finally, the combination of classifiers *f** minimizes generalization error
    as per the *f*(x)= ω*[1]*f*[1]*(x)+ ω*[2]*f*[2]*(x)+ ….+ ω*[k]*f*[k]*(x)* formula
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，分类器的组合 *f** 通过以下公式最小化泛化误差：*f*(x)= ω*[1]*f*[1]*(x)+ ω*[2]*f*[2]*(x)+ ….+ ω*[k]*f*[k]*(x)*
- en: Boosting
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Boosting
- en: Boosting is a pretty straightforward approach to calculating the output by applying
    a weighted average of all the outputs generated by multiple models. It is a framework
    for weak learners. The weights applied can be varied by using a powerful weighting
    formula to come up with a strong predictive model that addresses the pitfalls
    of these approaches and also works for a wider range of input data, using different
    narrowly tuned models.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting是一种相当直接的方法，通过应用多个模型生成的所有输出的加权平均来计算输出。它是一个弱学习者的框架。通过使用强大的加权公式，可以调整应用的权重，从而得出一个强大的预测模型，该模型解决了这些方法的缺陷，并且适用于更广泛的输入数据，使用不同的狭窄调整模型。
- en: 'Boosting has been successful in solving binary classification problems. This
    technique was introduced by Freund and Scaphire in the 1990s via the famous AdaBoost
    algorithm. Here listed are some key characteristics of this framework:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting在解决二元分类问题方面取得了成功。这项技术是由Freund和Scaphire在20世纪90年代通过著名的AdaBoost算法引入的。以下是该框架的一些关键特性：
- en: It combines several base classifiers that demonstrate improved performance in
    comparison to the base classifier
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它结合了多个基分类器，与基分类器相比，表现出改进的性能
- en: The weak learners are sequentially trained
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱学习器是按顺序训练的
- en: The data used for training each of the base classifiers is based on the performance
    of the previous classifier
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练每个基分类器所使用的数据基于先前分类器的性能
- en: Every classifier votes and contributes to the outcome
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个分类器投票并贡献于结果
- en: This framework works and uses the online algorithm strategy
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该框架工作并使用在线算法策略
- en: For every iteration, the weights are recomputed or redistributed where the incorrect
    classifiers will start to have their weights reduced
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每一次迭代，权重都会重新计算或重新分配，其中错误的分类器将开始减少其权重
- en: Correct classifiers receive more weight while incorrect classifiers have reduced
    weight
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确的分类器获得更多的权重，而错误的分类器则减少权重
- en: Boosting methods, though were originally designed for solving classification
    problems, are extended to handle regression as well
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boosting方法，虽然最初是为解决分类问题而设计的，但也被扩展到处理回归问题
- en: 'Boosting algorithm is stated next:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting算法如下所述：
- en: '![Boosting](img/B03980_13_16.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![Boosting](img/B03980_13_16.jpg)'
- en: 'Train a set of weak hypotheses: *h*[1]*, …, h*[T]*.*'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一组弱假设：*h*[1]*, …, h*[T]*.*
- en: Combine the hypothesis *H* as a weighted and majority vote of *T* weaker hypotheses.![Boosting](img/B03980_13_17.jpg)
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将假设 *H* 作为 *T* 个较弱假设的加权多数投票组合。![Boosting](img/B03980_13_17.jpg)
- en: Every iteration focuses on the misclassifications and recomputes the weights
    *D*[t]*(i)*.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次迭代都专注于错误分类，并重新计算权重 *D*[t]*(i)*。
- en: AdaBoost
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: AdaBoost
- en: AdaBoost is a linear classifier and constructs a stronger classifier *H(x)*
    as a linear combination of weaker functions *h*[t]*(x)*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![AdaBoost](img/B03980_13_18.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: 'The pictorial representation next demonstrates how the boosting framework works:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: All the data points are labeled into two classes *+1* and *-1* with equal weights—*1*.![AdaBoost](img/B03980_13_19.jpg)
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a *p (error)* and classify the data points as shown here:![AdaBoost](img/B03980_13_20.jpg)
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recompute the weights.![AdaBoost](img/B03980_13_21.jpg)
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have the weak classifiers participate again with a new problem set.![AdaBoost](img/B03980_13_22.jpg)![AdaBoost](img/B03980_13_23.jpg)
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A strong non-linear classifier is built using the weak classifiers iteratively.![AdaBoost](img/B03980_13_24.jpg)
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bagging
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Bagging is also called **Bootstrap Aggregation**. This technique of ensemble
    learning combines the *consensus* approach. There are three important steps in
    this technique:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Build the bootstrap sample that contains approximately 63.2% of the original
    records.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classify training using each bootstrap sample.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the majority voting and identify the class label of the ensemble classifier.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This process decreases the prediction variance by generating additional data
    generation, based on the original dataset by combining the datasets of the same
    size repetitively. The accuracy of the model increases with a decrease in variance
    and not by increasing dataset size. Following is the bagging algorithm:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging](img/B03980_13_25.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: 'As per the previous algorithm steps, an example flow of the bagging algorithm
    and the process is depicted here:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '**Training step**: For each iteration *t, t=1,…T*, create *N* samples from
    the training sets (this process is called bootstrapping), select a base model
    (for example, decision tree, neural networks, and so on), and train it using the
    samples built.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Testing step**: For every test cycle, predict by combining the results of
    all the *T* trained models. In the case of a classification problem, the majority
    of the voting approach is applied and for regression, it is the averaging approach.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some of the error computations are done as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging](img/B03980_13_26.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'Bagging works both in the over-fitting and under-fitting cases under the following
    conditions:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '**For under-fitting**: High bias and low variance case'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For over-fitting**: Small bias and large variance case'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of Bagging:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging](img/B03980_13_27.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: Wagging
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Wagging** is another variation of bagging. The entire dataset is used to
    train each model. Also, weights are assigned stochastically. So in short, wagging
    is bagging with additional weights that are assignment-based on the Poisson or
    exponential distribution. Following is the Wagging algorithm:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![Wagging](img/B03980_13_28.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: Random forests
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Radom forests is another ensemble learning method that combines multiple Decision
    trees. The following diagram represents the Random forest ensemble:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![Random forests](img/B03980_13_29.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/B03980_13_29.jpg)'
- en: 'Source: [https://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/](https://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/](https://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/)
- en: 'For a Random forest with T-trees, training of the decision tree classifiers
    is done as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有T个树的随机森林，决策树分类器的训练如下：
- en: Similar to the standard Bagging technique, a sample of *N* cases is defined
    with a random replacement to create a subset of the data that is about 62-66%
    of the comprehensive set.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与标准的Bagging技术类似，定义了一个随机替换的 *N* 个案例样本，以创建大约62-66%的全面数据集的子集。
- en: 'For each node, do as follows:'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个节点，执行以下操作：
- en: Select the *m* predictor variables in such a way that the identified variable
    gives the best split (binary split)
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以一种方式选择 *m* 个预测变量，使得识别的变量给出最佳的分割（二分分割）
- en: At the next node, choose other *m* variables that do the same
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一个节点，选择其他 *m* 个变量，它们执行相同的操作
- en: The value of the *m* can vary
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m* 的值可以变化'
- en: For Random splitter selection—*m=1*
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于随机分割选择—*m=1*
- en: 'For Breiman''s bagger: *m= total number of predictor variables*'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Breiman的袋装器：*m=预测变量总数*
- en: 'For Random forest, *m* is less than the number of predictor variables, and
    it can take three values: *½√m*, *√m*, and *2√m*'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于随机森林，*m* 小于预测变量的数量，它可以取三个值：*½√m*，*√m* 和 *2√m*
- en: Now, for every new input to the Random forest for prediction, the new value
    is run down through all the trees and an average, weighted average, or a voting
    majority is used to get the predicted value.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于随机森林预测的每个新输入，新值会通过所有树运行，并使用平均值、加权平均值或投票多数来获取预测值。
- en: Tip
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: In [Chapter 5](ch05.html "Chapter 5. Decision Tree based learning"), *Decision
    Tree based learning*, we have covered Random forests in detail.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html "第5章。基于决策树的学习")中，我们详细介绍了基于决策树的学习。
- en: Gradient boosting machines (GBM)
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 梯度提升机（GBM）
- en: GBMs are one of the highly adopted Machine learning algorithms. They are used
    to address classification and regression problems. The basis for GBMs is Decision
    trees, and they apply boosting technique where multiple weak algorithms are combined
    algorithmically to produce a strong learner. They are stochastic and gradient
    boosted, which means that they iteratively solve residuals.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: GBM是高度采用的机器学习算法之一。它们用于解决分类和回归问题。GBM的基础是决策树，它们应用提升技术，其中多个弱算法通过算法组合产生强学习器。它们是随机和梯度提升的，这意味着它们迭代地解决残差。
- en: They are known to be highly customizable as they can use a variety of loss functions.
    We have seen that the Random forests ensemble technique uses simple averaging
    against GBMs, which uses a practical strategy of the ensemble formation. In this
    strategy, new models are iteratively added to the ensemble where every iteration
    trains the weak modeler to identify the next steps.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 它们因其可以使用各种损失函数而闻名，可以高度定制。我们已经看到，随机森林集成技术使用简单的平均方法与GBM相比，GBM使用的是实用的集成形成策略。在这个策略中，新模型迭代地添加到集成中，其中每个迭代都训练弱模型以识别下一步。
- en: 'GBMs are flexible and are relatively more efficient than any other ensemble
    learning methods. The following table details the GBM algorithm:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: GBM灵活且比其他集成学习方法相对更高效。以下表格详细说明了GBM算法：
- en: '![Gradient boosting machines (GBM)](img/B03980_13_30.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![梯度提升机（GBM）](img/B03980_13_30.jpg)'
- en: '**Gradient boosted regression trees** (**GBRT**) are similar to GBMs that follow
    regression techniques.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升回归树**（**GBRT**）与遵循回归技术的GBM类似。'
- en: Unsupervised ensemble methods
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督集成方法
- en: 'As a part of unsupervised ensemble learning methods, one of the consensus-based
    ensembles is the clustering ensemble. The next diagram depicts the working of
    the clustering-based ensemble:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 作为无监督集成学习方法的一部分，基于共识的集成之一是聚类集成。以下图表描述了基于聚类的集成的工作原理：
- en: '![Unsupervised ensemble methods](img/B03980_13_31.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![无监督集成方法](img/B03980_13_31.jpg)'
- en: 'For a given unlabeled dataset *D={x*[1]*,x*[2]*,…,x*[n]*}*, a clustering ensemble
    computes a set of clusters *C = { C*[1]*,C*[2]*,…,C*[k]*}*, each of which maps
    the data to a cluster. A consensus-based unified cluster is formed. The following
    diagram depicts this flow:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的未标记数据集 *D={x*[1]*,x*[2]*,…,x*[n]*}*)，聚类集成计算一组聚类 *C = { C*[1]*,C*[2]*,…,C*[k]*}*)，每个聚类将数据映射到一个聚类。形成一个基于共识的统一聚类。以下图表描述了此流程：
- en: '![Unsupervised ensemble methods](img/B03980_13_32.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![无监督集成方法](img/B03980_13_32.jpg)'
- en: Implementing ensemble methods
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现集成方法
- en: Refer to the source code provided for this chapter to implement the ensemble
    learning methods (only supervised learning techniques). (source code path `.../chapter13/...`
    under each of the folders for the technology).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本章节提供的源代码以实现集成学习方法（仅限于监督学习技术）。(源代码路径 `.../chapter13/...` 位于每个技术文件夹下)。
- en: Using Mahout
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Mahout
- en: Refer to the folder `.../mahout/chapter13/ensembleexample/`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../mahout/chapter13/ensembleexample/`。
- en: Using R
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 R
- en: Refer to the folder `.../r/chapter13/ensembleexample/`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../r/chapter13/ensembleexample/`。
- en: Using Spark
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Spark
- en: Refer to the folder `.../spark/chapter13/ensembleexample/`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../spark/chapter13/ensembleexample/`。
- en: Using Python (Scikit-learn)
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python (Scikit-learn)
- en: Refer to the folder `.../python (scikit-learn)/chapter13/ensembleexample/`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../python (scikit-learn)/chapter13/ensembleexample/`。
- en: Using Julia
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Julia
- en: Refer to the folder `.../julia/chapter13/ensembleexample/`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../julia/chapter13/ensembleexample/`。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have covered the ensemble learning methods of Machine learning.
    We covered the concept of *the wisdom of the crowd*, how and when it is applied
    in the context of Machine learning, and how the accuracy and performance of the
    learners are improved. Specifically, we looked at some supervised ensemble learning
    techniques with some real-world examples. Finally, this chapter has source code
    examples for the gradient boosting algorithm using R, Python (scikit-learn), Julia,
    and Spark Machine learning tools and recommendation engines using the Mahout libraries.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了机器学习的集成学习方法。我们讨论了“群体智慧”的概念，以及如何在机器学习的背景下应用它，以及如何提高学习者的准确性和性能。具体来说，我们通过一些实际案例研究了某些监督集成学习技术。最后，本章提供了使用
    R、Python (scikit-learn)、Julia 和 Spark 机器学习工具以及使用 Mahout 库的推荐引擎的梯度提升算法的源代码示例。
- en: This chapter covers all the Machine learning methods and in the last chapter
    that follows, we will cover some advanced and upcoming architecture and technology
    strategies for Machine learning.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了所有机器学习方法，在接下来的最后一章中，我们将介绍一些机器学习的先进和新兴架构和技术策略。
