- en: Chapter 13. Ensemble learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is the concluding chapter of all the learning methods we have learned
    from [Chapter 5](ch05.html "Chapter 5. Decision Tree based learning"), *Decision
    Tree based learning*. It is only apt to have this chapter as a closing chapter
    for the learning methods, as this learning method explains how effectively these
    methods can be used in a combination to maximize the outcome from the learners.
    Ensemble methods have an effective, powerful technique to achieve high accuracy
    across supervised and unsupervised solutions. Different models are efficient and
    perform very well in the selected business cases. It is important to find a way
    to combine the competing models into a committee, and there has been much research
    in this area with a fair degree of success. Also, as different views generate
    a large amount of data, the key aspect is to consolidate different concepts for
    intelligent decision making. Recommendation systems and stream-based text mining
    applications use ensemble methods extensively.
  prefs: []
  type: TYPE_NORMAL
- en: There have been many independent studies done in supervised and unsupervised
    learning groups. The common theme observed was that many mixed models, when brought
    together, strengthened the weak models and brought in an overall better performance.
    One of the important goals of this chapter details a systematic comparison of
    different ensemble techniques that combine both supervised and unsupervised techniques
    and a mechanism to merge the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble learning](img/B03980_13_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of ensemble methods based learning—the concept of *the wisdom of
    the crowd* and the key attributes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core ensemble method taxonomy, real-world examples, and applications of ensemble
    learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ensemble method categories and various representative methods:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supervised ensemble methods** provide an overview and a detailed coverage
    of concepts such as bagging, boosting, gradient boosting methods, and Random decision
    trees and Random forests'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised ensemble methods** provide an overview of generative, direct
    and indirect methods that include clustering ensembles'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on implementation exercises using Apache Mahout, R, Julia, Python (scikit-learn),
    and Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble learning methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble, in general, means a group of things that are usually seen as a whole
    rather than in terms of the value as against the individual value. Ensembles follow
    a divide-and-conquer approach used to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble learning methods](img/B03980_13_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will start understanding the specific algorithm with an introduction to the
    famous concept of the wisdom of the crowd.
  prefs: []
  type: TYPE_NORMAL
- en: The wisdom of the crowd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imperfect judgments when aggregated in a right way result in a collective intelligence,
    thus resulting in a superior outcome. The wisdom of the crowd is all about this
    collective intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the term crowd is usually associated with irrationality and the
    common perception that there is some influence, which sways the behavior of the
    crowd in the context of mobs and cults. However, the fact is that this need not
    always be negative and works well when working with collating intellect. The key
    concept of Wisdom of Crowds is that the decisions made by a group of people are
    always robust and accurate than those made by individuals. The ensemble learning
    methods of Machine learning have exploited this idea effectively to produce efficiency
    and accuracy in their results.
  prefs: []
  type: TYPE_NORMAL
- en: The term the wisdom of the crowd was coined by Galton in 1906\. Once he attended
    a farmer's fair where there was a contest to guess the weight of an ox that is
    butchered and dressed. The closest guess won the prize from a total of 800 contestants.
    He chose to collect all the responses and analyze them. When he took the average
    of the guesses, he was shocked to notice that they were very close to the actual
    value. This collective guess was both better than the contestant who won the prize
    and also proved to be the best in comparison to the guesses by cattle experts.
    The democracy of thoughts was a clear winner. For such a useful output, it is
    important that each contestant had his/her strong source of information. The independent
    guess provided by the contestant should not be influenced by his/her neighbor's
    guess, and also, there is an error-free mechanism to consolidate the guesses across
    the group. So in short, this is not an easy process. Another important aspect
    is also to the fact these guesses were superior to any individual expert's guess.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some basic everyday examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Google search results that usually have the most popular pages listed at the
    top
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a game like "Who wants to be a billionaire", the audience poll is used for
    the answering questions that the contestant has no knowledge about. Usually, the
    answer that is most voted by the crowd is the right answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The results of the wisdom of the crowd approach is not guaranteed. Following
    is the basic criteria for an optimal result using this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregation**: There needs to be a foolproof way of consolidating individual
    responses into a collective response or judgment. Without this, the core purpose
    of collective views or responses across the board goes in vain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence**: Within the crowd, there needs to be discipline around controlling
    the response from one entity over the rest in the crowd. Any influence would skew
    the response, thus impacting the accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decentralization**: Individual responses have their source and thrive on
    the limited knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The diversity of opinion**: It is important that each person has a response
    that is isolated; the response''s unusualness is still acceptable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word ensemble means grouping. To build ensemble classifiers, we first need
    to build a set of classifiers from the training data, aggregate the predictions
    made by these classifiers, and predict a class label of a new record using this
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The wisdom of the crowd](img/B03980_13_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Technically, the core building blocks include a training set, an inducer, and
    an ensemble generator. Inducer handles defining classifiers for each of the sample
    training datasets. The ensemble generator creates classifiers and a combiner or
    aggregator that consolidates the responses across the combiners. With these building
    blocks and the relationships between them, we have the following properties that
    we will be using to categorize the ensemble methods. The next section covers these
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage of a combiner**: This property defines the relationship between the
    ensemble generator and the combiner'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependency between classifiers**: This property defines the degree to which
    the classifiers are dependent on each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generating diversity**: This property defines the procedure used to ensure
    diversity across combiners'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The size of the ensemble**: This property denotes the number of classifiers
    used in the ensembles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross inducers**: This property defines how the classifiers leverage the
    inducers. There are cases where the classifiers are built to work with a certain
    set of inducers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, the building model ensembles first involves building experts and
    letting them provide a response/vote. The expected benefit is an improvement in
    prediction performance and produces a single global structure. Although, any interim
    results produced might end up being difficult to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at how the performance of an aggregated/combined classifier works
    better in a comprehensive manner.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider three classifiers that have an error rate of 0.35(ԑ) or an accuracy
    of 0.65\. For each of the classifiers, the probability that the classifier goes
    wrong with its prediction is 35%.
  prefs: []
  type: TYPE_NORMAL
- en: '![The wisdom of the crowd](img/B03980_13_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Given here is the truth table denoting the error rate of 0.35(35%) and the
    accuracy rate of 0.65(65%):'
  prefs: []
  type: TYPE_NORMAL
- en: '![The wisdom of the crowd](img/B03980_13_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After the three classifiers are combined, the class label of a test instance
    is predicted by using the majority vote process across the combiners to compute
    the probability that the ensemble classifier makes an error. This is depicted
    in the formula given below.
  prefs: []
  type: TYPE_NORMAL
- en: '![The wisdom of the crowd](img/B03980_13_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Moreover, the accuracy is 71.83%. Very clearly, the error rate is lowered when
    aggregated across the classifiers. Now, if we extend this to 25 classifiers, the
    accuracy goes up to 94% as per the computation of the error rate (6%).
  prefs: []
  type: TYPE_NORMAL
- en: '![The wisdom of the crowd](img/B03980_13_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the ensembles work as they give the bigger picture.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered the criteria for the the wisdom of the crowd to work in the
    previous section. Let's now take the preceding case where we have 25 base classifiers,
    and see how the accuracy of the ensemble classifier changes for different error
    rates of the base classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![The wisdom of the crowd](img/B03980_13_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ensemble classifier's performance deteriorates and performs much worse than
    the base classifier in cases where the base classifier error rate is more than
    0.5.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover some real-world use cases that apply ensemble
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Key use cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some of the key real-world applications of ensemble learning methods are detailed
    and discussed in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendation systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of recommendation systems is to produce significant or meaningful
    recommendations to a community of users around the products that would possibly
    interest them. Some examples include suggestions related to decision-making process
    such as what books to read on Amazon, what movies to watch on Netflix, or what
    news to read on a news website. The business domain, or the context and the characteristics
    of the business attributes are the primary inputs to the design of recommendation
    systems. The rating (on a scale of 1-5) that users provide for each of the films
    is a significant input as it records the degree to which users interact with the
    system. In addition to this, the details of the user (such as demographics and
    other personal or profile attributes) are also used by the recommender systems
    to identify a potential match between items and potential users.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot is an example of a recommendation system result on
    Netflix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recommendation systems](img/B03980_13_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Anomaly detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Anomaly detection or outlier detection is one of the most popular use cases
    or applications of ensemble learning methods. It is all about finding patterns
    in the data that look abnormal or unusual. Identifying anomalies is important
    as it may result in taking any decisive action. Some famous examples include (among
    many others):'
  prefs: []
  type: TYPE_NORMAL
- en: Fraud detection with credit cards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unusual disease detection in healthcare
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection in aircraft engines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now expand on the example for anomaly detection in aircraft engines.
    Features of a plane engine which are considered to verify if it is anomalous or
    not are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Heat generated(*x*[1])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intensity of vibration (*x*[2])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the dataset = *x*[(1)]*, x*[(2)] *… x*[(m)] marked, following are the anomalous
    and non-anomalous cases:![Anomaly detection](img/B03980_13_10.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traditionally, all the Machine learning algorithms assume learning to happen
    from scratch for every new learning problem. The assumption is that no previous
    learning will be leveraged. In cases where the domains for the learning problems
    relate, there will be some learnings from the past that can be acquired and used.
    Some common examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: The knowledge of French could help students learn Spanish
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The knowledge of mathematics could help students learn Physics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The knowledge of driving a car could help drivers learn to drive a truck
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the Machine learning context, this refers to identifying and applying the
    knowledge accumulated from previous tasks to new tasks from a related domain.
    The key here is the ability to identify the commonality between the domains. Reinforcement
    learning and classification and regression problems apply transfer learning. The
    transfer learning process flow is as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Transfer learning](img/B03980_13_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Stream mining or classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mining data that comes in as streams has now become a key requirement for a
    wide range of applications with the growing technological advancements and social
    media.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary difference between the traditional learning is that the training
    and test data sets evolved in a distributed way as they come in streams. The goal
    of prediction now becomes a bit complex as the probabilities keep changing for
    changing timestamps, thus making this an ideal context for applying ensemble learning
    methods. The next diagram shows how *P(y)* changes with timestamp and changes
    to *P(x)* and *P (y|x)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream mining or classification](img/B03980_13_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With ensemble learning methods, the variance produced by single models is reduced,
    and the prediction or result is more accurate or robust as the distribution is
    evolving.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in the previous sections, ensemble methods are now proven to be
    powerful methods for improving the accuracy and robustness of the supervised,
    semi-supervised, and unsupervised solutions. Also, we have seen how the dynamics
    of decision-making are becoming complex as different sources have started generating
    enormous amounts of data continuously. Effective consolidation is now critical
    for successful and intelligent decision making.
  prefs: []
  type: TYPE_NORMAL
- en: The supervised and unsupervised ensemble methods share the same principles that
    involve combining the diverse base models that strengthen weak models. In the
    sections to follow, let's look at supervised, semi-supervised, and unsupervised
    techniques independently and in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following model depicts various learning categories and different algorithms
    that cover both, combining the learning and consensus approaches to the ensemble
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble methods](img/B03980_13_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: On the Power of Ensemble: Supervised and Unsupervised Methods Reconciled
    ([http://it.engineering.illinois.edu/ews/](http://it.engineering.illinois.edu/ews/))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go deeper into each of the ensemble techniques, let''s understand
    the difference between combining by learning versus combining by consensus:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Benefits | Downside |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Combining by learning** |'
  prefs: []
  type: TYPE_TB
- en: Uses labeled data as a feedback mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has the potential to improve accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Works only with labeled data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are chances of over-fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Combining by consensus** |'
  prefs: []
  type: TYPE_TB
- en: Does not require labeled data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has the potential to improve performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The valued feedback from the labeled data is missing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works on the assumption that consensus is a good thing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised ensemble methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the case of supervised learning methods, the input is always a labeled data.
    The *combining by learning* method includes boosting stack generalization and
    the rule ensembles techniques. The *combining by consensus* methods includes bagging,
    Random forests, and Random decision trees techniques. The following shows the
    process flow for combining by learning followed by another model for combining
    by consensus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Supervised ensemble methods](img/B03980_13_14.jpg)![Supervised ensemble methods](img/B03980_13_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The supervised ensemble method problem statement is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The input data set is *D={x*[1]*, x*[2]*, …, x*[n]*}* the respective labels
    are *L={l*[1]*,l*[2]*,…,l*[n]*}*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ensemble method now generated a set of classifiers *C = {f*[1]*,f*[2]*,…,f*[k]*}*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the combination of classifiers *f** minimizes generalization error
    as per the *f*(x)= ω*[1]*f*[1]*(x)+ ω*[2]*f*[2]*(x)+ ….+ ω*[k]*f*[k]*(x)* formula
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Boosting is a pretty straightforward approach to calculating the output by applying
    a weighted average of all the outputs generated by multiple models. It is a framework
    for weak learners. The weights applied can be varied by using a powerful weighting
    formula to come up with a strong predictive model that addresses the pitfalls
    of these approaches and also works for a wider range of input data, using different
    narrowly tuned models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boosting has been successful in solving binary classification problems. This
    technique was introduced by Freund and Scaphire in the 1990s via the famous AdaBoost
    algorithm. Here listed are some key characteristics of this framework:'
  prefs: []
  type: TYPE_NORMAL
- en: It combines several base classifiers that demonstrate improved performance in
    comparison to the base classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weak learners are sequentially trained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data used for training each of the base classifiers is based on the performance
    of the previous classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every classifier votes and contributes to the outcome
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This framework works and uses the online algorithm strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For every iteration, the weights are recomputed or redistributed where the incorrect
    classifiers will start to have their weights reduced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correct classifiers receive more weight while incorrect classifiers have reduced
    weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting methods, though were originally designed for solving classification
    problems, are extended to handle regression as well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boosting algorithm is stated next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Boosting](img/B03980_13_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Train a set of weak hypotheses: *h*[1]*, …, h*[T]*.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the hypothesis *H* as a weighted and majority vote of *T* weaker hypotheses.![Boosting](img/B03980_13_17.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Every iteration focuses on the misclassifications and recomputes the weights
    *D*[t]*(i)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: AdaBoost is a linear classifier and constructs a stronger classifier *H(x)*
    as a linear combination of weaker functions *h*[t]*(x)*.
  prefs: []
  type: TYPE_NORMAL
- en: '![AdaBoost](img/B03980_13_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The pictorial representation next demonstrates how the boosting framework works:'
  prefs: []
  type: TYPE_NORMAL
- en: All the data points are labeled into two classes *+1* and *-1* with equal weights—*1*.![AdaBoost](img/B03980_13_19.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a *p (error)* and classify the data points as shown here:![AdaBoost](img/B03980_13_20.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recompute the weights.![AdaBoost](img/B03980_13_21.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have the weak classifiers participate again with a new problem set.![AdaBoost](img/B03980_13_22.jpg)![AdaBoost](img/B03980_13_23.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A strong non-linear classifier is built using the weak classifiers iteratively.![AdaBoost](img/B03980_13_24.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Bagging is also called **Bootstrap Aggregation**. This technique of ensemble
    learning combines the *consensus* approach. There are three important steps in
    this technique:'
  prefs: []
  type: TYPE_NORMAL
- en: Build the bootstrap sample that contains approximately 63.2% of the original
    records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classify training using each bootstrap sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the majority voting and identify the class label of the ensemble classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This process decreases the prediction variance by generating additional data
    generation, based on the original dataset by combining the datasets of the same
    size repetitively. The accuracy of the model increases with a decrease in variance
    and not by increasing dataset size. Following is the bagging algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging](img/B03980_13_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As per the previous algorithm steps, an example flow of the bagging algorithm
    and the process is depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training step**: For each iteration *t, t=1,…T*, create *N* samples from
    the training sets (this process is called bootstrapping), select a base model
    (for example, decision tree, neural networks, and so on), and train it using the
    samples built.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Testing step**: For every test cycle, predict by combining the results of
    all the *T* trained models. In the case of a classification problem, the majority
    of the voting approach is applied and for regression, it is the averaging approach.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some of the error computations are done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging](img/B03980_13_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Bagging works both in the over-fitting and under-fitting cases under the following
    conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**For under-fitting**: High bias and low variance case'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For over-fitting**: Small bias and large variance case'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of Bagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging](img/B03980_13_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Wagging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Wagging** is another variation of bagging. The entire dataset is used to
    train each model. Also, weights are assigned stochastically. So in short, wagging
    is bagging with additional weights that are assignment-based on the Poisson or
    exponential distribution. Following is the Wagging algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wagging](img/B03980_13_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Random forests
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Radom forests is another ensemble learning method that combines multiple Decision
    trees. The following diagram represents the Random forest ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random forests](img/B03980_13_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/](https://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a Random forest with T-trees, training of the decision tree classifiers
    is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the standard Bagging technique, a sample of *N* cases is defined
    with a random replacement to create a subset of the data that is about 62-66%
    of the comprehensive set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each node, do as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the *m* predictor variables in such a way that the identified variable
    gives the best split (binary split)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At the next node, choose other *m* variables that do the same
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The value of the *m* can vary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Random splitter selection—*m=1*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For Breiman''s bagger: *m= total number of predictor variables*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For Random forest, *m* is less than the number of predictor variables, and
    it can take three values: *½√m*, *√m*, and *2√m*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, for every new input to the Random forest for prediction, the new value
    is run down through all the trees and an average, weighted average, or a voting
    majority is used to get the predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html "Chapter 5. Decision Tree based learning"), *Decision
    Tree based learning*, we have covered Random forests in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting machines (GBM)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: GBMs are one of the highly adopted Machine learning algorithms. They are used
    to address classification and regression problems. The basis for GBMs is Decision
    trees, and they apply boosting technique where multiple weak algorithms are combined
    algorithmically to produce a strong learner. They are stochastic and gradient
    boosted, which means that they iteratively solve residuals.
  prefs: []
  type: TYPE_NORMAL
- en: They are known to be highly customizable as they can use a variety of loss functions.
    We have seen that the Random forests ensemble technique uses simple averaging
    against GBMs, which uses a practical strategy of the ensemble formation. In this
    strategy, new models are iteratively added to the ensemble where every iteration
    trains the weak modeler to identify the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'GBMs are flexible and are relatively more efficient than any other ensemble
    learning methods. The following table details the GBM algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient boosting machines (GBM)](img/B03980_13_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Gradient boosted regression trees** (**GBRT**) are similar to GBMs that follow
    regression techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised ensemble methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a part of unsupervised ensemble learning methods, one of the consensus-based
    ensembles is the clustering ensemble. The next diagram depicts the working of
    the clustering-based ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised ensemble methods](img/B03980_13_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For a given unlabeled dataset *D={x*[1]*,x*[2]*,…,x*[n]*}*, a clustering ensemble
    computes a set of clusters *C = { C*[1]*,C*[2]*,…,C*[k]*}*, each of which maps
    the data to a cluster. A consensus-based unified cluster is formed. The following
    diagram depicts this flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised ensemble methods](img/B03980_13_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Implementing ensemble methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the source code provided for this chapter to implement the ensemble
    learning methods (only supervised learning techniques). (source code path `.../chapter13/...`
    under each of the folders for the technology).
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter13/ensembleexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter13/ensembleexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter13/ensembleexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python (Scikit-learn)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../python (scikit-learn)/chapter13/ensembleexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter13/ensembleexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered the ensemble learning methods of Machine learning.
    We covered the concept of *the wisdom of the crowd*, how and when it is applied
    in the context of Machine learning, and how the accuracy and performance of the
    learners are improved. Specifically, we looked at some supervised ensemble learning
    techniques with some real-world examples. Finally, this chapter has source code
    examples for the gradient boosting algorithm using R, Python (scikit-learn), Julia,
    and Spark Machine learning tools and recommendation engines using the Mahout libraries.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers all the Machine learning methods and in the last chapter
    that follows, we will cover some advanced and upcoming architecture and technology
    strategies for Machine learning.
  prefs: []
  type: TYPE_NORMAL
