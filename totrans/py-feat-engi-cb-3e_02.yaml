- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Encoding Categorical Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Home owner` variable with the values of `owner` and `non-owner` is categorical,
    and so is the `Marital status` variable with the values of `never married`, `married`,
    `divorced`, and `widowed`. In some categorical variables, the labels have an intrinsic
    order; for example, in the `Student''s grade` variable, the values of `A`, `B`,
    `C`, and `Fail` are ordered, with `A` being the highest grade and `Fail` being
    the lowest. These are called `City` variable, with the values of `London`, `Manchester`,
    `Bristol`, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: The values of categorical variables are often encoded as strings. To train most
    machine learning models, we need to transform those strings into numbers. The
    act of replacing strings with numbers is called **categorical encoding**. In this
    chapter, we will discuss multiple categorical encoding methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating binary variables through one-hot encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing one-hot encoding of frequent categories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing categories with counts or the frequency of observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing categories with ordinal numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing ordinal encoding based on the target value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing target mean encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding with the Weight of Evidence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping rare or infrequent categories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing binary encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use the `Matplotlib`, `pandas`, `NumPy`, `scikit-learn`,
    `feature-engine`, and Category Encoders Python libraries. If you need to install
    Python, the free Anaconda Python distribution ([https://www.anaconda.com/](https://www.anaconda.com/))
    includes most numerical computing libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '`feature-engine` can be installed with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you use Anaconda, you can install `feature-engine` with `conda`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To install Category Encoders, use `pip` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the **Credit Approval** dataset from the *UCI Machine Learning
    Repository* ([https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)), licensed
    under the CC BY 4.0 creative commons attribution: [https://creativecommons.org/licenses/by/4.0/legalcode](https://creativecommons.org/licenses/by/4.0/legalcode).
    You’ll find the dataset at this link: [http://archive.ics.uci.edu/dataset/27/credit+approval](http://archive.ics.uci.edu/dataset/27/credit+approval).'
  prefs: []
  type: TYPE_NORMAL
- en: 'I downloaded and modified the data as shown in this notebook: [https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/credit-approval-dataset.ipynb](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/credit-approval-dataset.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll find a copy of the modified data set in the accompanying GitHub repository:
    [https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Before encoding categorical variables, you might want to impute their missing
    data. Check out the imputation methods for categorical variables in [*Chapter
    1*](B22396_01.xhtml#_idTextAnchor020), *Imputing* *Missing Data*.
  prefs: []
  type: TYPE_NORMAL
- en: Creating binary variables through one-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`1` if the category is present, or `0` otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the one-hot encoded representation of the `Smoker`
    variable with the categories of `Smoker` and `Non-Smoker`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – One-hot encoded representation of the Smoker variable](img/B22396_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – One-hot encoded representation of the Smoker variable
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 2**.1*, from the `Smoker` variable, we can derive a binary
    variable for `Smoker`, which shows the value of `1` for smokers, or the binary
    variable for `Non-Smoker`, which takes the value of `1` for those who do not smoke.
  prefs: []
  type: TYPE_NORMAL
- en: For the `Color` categorical variable with the values of `red`, `blue`, and `green`,
    we can create three variables called `red`, `blue`, and `green`. These variables
    will be assigned a value of `1` if the observation corresponds to the respective
    color, and `0` if it does not.
  prefs: []
  type: TYPE_NORMAL
- en: 'A categorical variable with *k* unique categories can be encoded using *k-1*
    binary variables. For `Smoker`, *k* is *2* as it contains two labels (`Smoker`
    and `Non-Smoker`), so we only need one binary variable (*k - 1 = 1*) to capture
    all the information. For the `Color` variable, which has 3 categories (*k = 3*;
    `red`, `blue`, and `green`), we need 2 (*k - 1 = 2*) binary variables to capture
    all the information so that the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: If the observation is red, it will be captured by the `red` variable (`red`
    = `1`, `blue` = `0`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the observation is blue, it will be captured by the `blue` variable (`red`
    = `0`, `blue` = `1`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the observation is green, it will be captured by the combination of `red`
    and `blue` (`red` = `0`, `blue` = `0`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Encoding into *k-1* binary variables is well suited for linear models. There
    are a few occasions in which we may prefer to encode the categorical variables
    with *k* binary variables:'
  prefs: []
  type: TYPE_NORMAL
- en: When training decision trees, since they do not evaluate the entire feature
    space at the same time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When selecting features recursively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When determining the importance of each category within a variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will compare the one-hot encoding implementations of `pandas`,
    `scikit-learn`, and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s make a few imports and get the data ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and the `train_test_split` function from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the Credit Approval dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s separate the data into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s inspect the unique categories of the `A4` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see the unique values of `A4` in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: dummies = pd.get_dummies(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train["A4"], drop_first=True)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: dummies.head()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: With `pandas`’ `get_dummies()`, we can either ignore or encode missing data
    through the `dummy_na` parameter. By setting `dummy_na=True`, missing data will
    be encoded in a new binary variable. To encode the variable into *k* dummies,
    use `drop_first=False` instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see the output of *Step 5*, where each label is now a binary variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s encode all the categorical variables into *k-1* binaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas`’ `get_dummies()`will encode all variables of the object, string, or
    category type by default. To encode a subset of the variables, pass the variable
    names in a list to the `columns` parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect the first five rows of the resulting DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When encoding more than one variable, `get_dummies()` captures the variable
    name – say, `A1` – and places an underscore followed by the category name to identify
    the resulting binary variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the binary variables in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – A transformed DataFrame showing the numerical variables followed
    by the one-hot encoded representation of the categorical variables](img/B22396_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – A transformed DataFrame showing the numerical variables followed
    by the one-hot encoded representation of the categorical variables
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas`’ `get_dummies()`will create one binary variable per category seen
    in a DataFrame. Hence, if there are more categories in the train set than in the
    test set, `get_dummies()` will return more columns in the transformed train set
    than in the transformed test set, and vice versa. To avoid this, it is better
    to carry out one-hot encoding with `scikit-learn` or `feature-engine`.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s do one-hot encoding using `scikit-learn` instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the encoder and `ColumnTransformer` from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a list with the names of the categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the encoder to create *k-1* binary variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To encode variables into *k* dummies, set the `drop` parameter to `None`. To
    encode only binary variables into *k-1*, set the `drop` parameter to `if_binary`.
    The latter is useful because encoding binary variables into *k* dummies is redundant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s restrict the encoding to the categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s fit the encoder so that it identifies the categories to encode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s inspect the categories that will be represented with binary variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The transformer will add binary variables for the following categories:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Arrays with the categories that will be encoded into binary
    variables (one array per variable)](img/B22396_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Arrays with the categories that will be encoded into binary variables
    (one array per variable)
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn`’s `OneHotEncoder()` will only encode the categories learned
    from the train set. If there are new categories in the test set, we can instruct
    the encoder to ignore them, return an error, or replace them with an infrequent
    category, by setting the `handle_unknown` parameter to `ignore`, `error`, or `infrequent_if_exists`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s encode the categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make sure to inspect the result by executing `X_test_enc.head()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To get familiar with the output, let’s print the variable names of the resulting
    DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following image, we see the names of the variables in the transformed
    DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Arrays with the names of the variables in the resulting DataFrame](img/B22396_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Arrays with the names of the variables in the resulting DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`ColumnTransformer()` changes the name and order of the variables during the
    transformation. If the variable was encoded, it will append the `encoder` prefix
    and if the variable was not modified, it will append the `remainder` prefix.'
  prefs: []
  type: TYPE_NORMAL
- en: To wrap up the recipe, let’s perform one-hot encoding with `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the encoder from `f``eature-engine`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the encoder so that it returns *k-1* binary variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`feature-engine`’s `OneHotEncoder()` encodes all categorical variables by default.
    To encode a subset of the variables, pass the variable names in a list: `OneHotEncoder(variables=["A1",
    "A4"]`). To encode numerical variables, set the `ignore_format` parameter to `True`
    or cast the variables as objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the encoder to the train set so that it learns the categories and
    variables to encode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To encode binary variables into *k-1*, and other categorical variables into
    *k* dummies, set the `drop_last_binary` parameter to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore the variables that will be encoded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The transformer found and stored the variables of the object or categorical
    type, as shown in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s explore the categories for which dummy variables will be created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following dictionary contains the categories that will be encoded in each
    variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s encode the categorical variables in train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `X_train_enc.head()`, we will see the following DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Transformed DataFrame with numerical variables followed by the
    one-hot encoded representation of the categorical variables](img/B22396_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Transformed DataFrame with numerical variables followed by the
    one-hot encoded representation of the categorical variables
  prefs: []
  type: TYPE_NORMAL
- en: Note how the `A4` categorical variable was replaced with `A4_u`, `A4_y`, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can get the names of all the variables in the transformed dataset by executing
    `ohe_enc.get_feature_names_out()`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we performed a one-hot encoding of categorical variables using
    `pandas`, `scikit-learn`, and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas`’ `get_dummies()` replaced the categorical variables with a set of
    binary variables representing each of the categories. When used on the entire
    dataset, it returned the numerical variables, followed by the one-hot encoded
    representation of each seen category in every variable of type object, string,
    or categorical.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` will return binary variables for every category seen in a dataset.
    In practice, to avoid data leakage and anticipate deployment eventualities, we
    want to return dummy variables for categories seen in a training set only. So,
    it is safer to use `scikit-learn` and `feature-engine`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`OneHotEncoder()` from `scikit-learn` or `feature-engine` learned the categories
    that should be represented by binary variables from the train set when we applied
    `fit()`. With `transform()`, `scikit-learn` returned just the binary variables,
    whereas `feature-engine` returned the numerical variables followed by the one-hot
    encoded representation of the categorical ones.'
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn`’s `OneHotEncoder()` encodes all variables by default. To restrict
    the encoding to categorical variables, we used `ColumnTransformer()`. We set the
    output of `transform()`to `pandas` to obtain the resulting data as a DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding is suitable for linear models. It also expands the feature
    space. If your dataset contains many categorical variables or highly cardinal
    variables, you can restrict the number of binary variables by encoding the most
    frequent categories only. You can do this automatically with both `scikit-learn`
    and `feature-engine` as we describe in the *Performing one-hot encoding of frequent*
    *categories* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also perform one-hot encoding using the Category Encoders Python library:
    [https://contrib.scikit-learn.org/category_encoders/onehot.html](https://contrib.scikit-learn.org/category_encoders/onehot.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To limit the number of binary variables, we can choose which categories to
    encode and which to ignore; check out a Python demo in the following article:
    https://www.blog.trainindata.com/one-hot-encoding-categorical-variables/.'
  prefs: []
  type: TYPE_NORMAL
- en: Performing one-hot encoding of frequent categories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One-hot encoding represents each variable’s category with a binary variable.
    Hence, one-hot encoding of highly cardinal variables or datasets with multiple
    categorical features can expand the feature space dramatically. This, in turn,
    may increase the computational cost of using machine learning models or deteriorate
    their performance. To reduce the number of binary variables, we can perform one-hot
    encoding of the most frequent categories. One-hot encoding the top categories
    is equivalent to treating the remaining, less frequent categories as a single,
    unique category.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will implement one-hot encoding of the most popular categories
    using `pandas`, `Scikit-learn`, and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary Python libraries and get the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required Python libraries, functions, and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the Credit Approval dataset and divide it into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The most frequent categories need to be determined in the train set. This is
    to avoid data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect the unique categories of the `A6` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The unique values of `A6` are displayed in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train["A6"].value_counts().sort_values(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ascending=False).head(5)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A6
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c      93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: q      56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: w      48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: i      41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ff     38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A6 in a list by using the code in *Step 4* inside a list comprehension:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s add a binary variable per top category to a copy of the train and test
    sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s display the top `10` rows of the original and encoded variable, `A6`,
    in the train set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the output of *Step 7*, we can see the `A6` variable, followed by the binary
    variables:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s import the encoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the encoder to encode categories shown in at least `39` observations
    and limit the number of categories to encode to `5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s fit the transformer to the two high cardinal variables and then
    transform the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you execute `X_train_enc.head()` you’ll see the resulting DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Transformed DataFrame containing binary variables for those
    categories with at least 39 observations and an additional binary representing
    all remaining categories](img/B22396_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Transformed DataFrame containing binary variables for those categories
    with at least 39 observations and an additional binary representing all remaining
    categories
  prefs: []
  type: TYPE_NORMAL
- en: To wrap up the recipe, let’s encode the most frequent categories with `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up the one-hot encoder to encode the five most frequent categories
    of the `A6` and `A7` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The number of frequent categories to encode is arbitrarily determined by the
    user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the encoder to the train set so that it learns and stores the most
    frequent categories of `A6` and `A7`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s encode `A6` and `A7` in the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can view the new binary variables in the transformed DataFrame by executing
    `X_train_enc.head()`. You can also find the top five categories learned by the
    encoder by executing `ohe_enc.encoder_dict_`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first part of this recipe, we worked with the `A6` categorical variable.
    We inspected its unique categories with `pandas`’ `unique()`. Next, we counted
    the number of observations per category using `pandas`’ `value_counts()`, which
    returned a `pandas` series with the categories as the index and the number of
    observations as values. Next, we sorted the categories from the one with the most
    to the one with the least observations using `pandas`’ `sort_values()`. We then
    reduced the series to the five most popular categories by using `pandas`’ `head()`.
    We used this series in a list comprehension to capture the names of the most frequent
    categories. After that, we looped over each category, and with NumPy’s `where()`,
    we created binary variables by placing a value of `1` if the observation showed
    the category, or `0` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed how to use `OneHotEncoder()` from `scikit-learn` and `feature-engine`
    in the *Creating binary variables through one-hot encoding* recipe. Here, I will
    only highlight the parameters needed to encode the most frequent categories.
  prefs: []
  type: TYPE_NORMAL
- en: To encode frequent categories with `scikit-learn`, we set the `min_frequency`
    parameter to `39`. Hence, categories shown in less than `39` observations were
    grouped into an additional binary variable called `infrequent_sklearn`.
  prefs: []
  type: TYPE_NORMAL
- en: To encode frequent categories with `feature-engine`, we set the `top_categories`
    parameter to `5`. Hence, the transformer created binary variables for the 5 most
    frequent categories only. Less frequent categories will show a `0` in all the
    binary variables.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe is based on the winning solution of the **Knowledge Discovery and
    Data** (**KDD**) 2009 mining cup, *Winning the KDD Cup Orange Challenge with Ensemble
    Selection* (http://proceedings.mlr.press/v7/niculescu09/niculescu09.pdf), where
    the author's limited one-hot encoding to the 10 most frequent categories of each
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing categories with counts or the frequency of observations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In count with counts or frequency of observations” or frequency encoding, we
    replace the categories with the count or the fraction of observations showing
    that category. That is, if 10 out of 100 observations show the `blue` category
    for the `Color` variable, we would replace `blue` with `10` when doing count encoding,
    or with `0.1` if performing frequency encoding. These encoding methods are useful
    when there is a relationship between the category frequency and the target. For
    example, in sales, the frequency of a product may indicate its popularity.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If two different categories are present in the same number of observations,
    they will be replaced by the same value, which may lead to information loss.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will perform count and frequency encoding using `pandas`
    and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll start by encoding one variable with `pandas` and then we’ll automate
    the process with `feature-engine`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the Credit Approval dataset and divide it into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s with counts or frequency of observations” capture the number of observations
    per category of the `A7` variable in a dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To find the frequency instead, execute `X_train["A7"].value_counts(normalize=True).to_dict()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we execute `print(counts)`, we’ll see the count of observations per category
    of `A7`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s replace the categories in `A7` with the counts in a copy of the data
    sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Go ahead and inspect the data by executing `X_train_enc.head()` to corroborate
    that the categories have been replaced by the counts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To apply this procedure to multiple variables, we can use `feature-engine`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s set up the encoder so that it encodes all categorical variables with
    the count of observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`CountFrequencyEncoder()` will automatically find and encode all categorical
    variables in the train set. To encode only a subset of the variables, pass the
    variable names in a list to the `variables` argument. To encode with the frequency
    instead, use `encoding_method="frequency"`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the encoder to the train set so that it stores the number of observations
    per category per variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The encoder found the categorical variables automatically. Let’s check them
    out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous command returns the names of the categorical variables in the
    train set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s print the count of observations per category per variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous attribute stores the mappings that will be used to replace the
    categories:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Dictionary containing the number of observations per category,
    for each variable; these values will be used to encode the categorical variables](img/B22396_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Dictionary containing the number of observations per category,
    for each variable; these values will be used to encode the categorical variables
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s with counts or frequency of observations” replace the categories
    with counts in the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check out the result by executing `X_train_enc.head()`. The encoder returns
    `pandas` DataFrames with the strings of the categorical variables replaced with
    the counts of observations, leaving the variables ready to use in machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we replaced categories with the count of observations using
    `pandas` and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: Using `pandas`’ `value_counts()`, we determined the number of observations per
    category of the `A7` variable, and with `pandas`’ `to_dict()`, we captured these
    values in a with counts or frequency of observations” dictionary, where each key
    was a unique category, and each value the number of observations for that category.
    With `pandas`’ `map()` and using this dictionary, we replaced the categories with
    the observation counts in both the train and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The count of observations for the encoding should be obtained from the train
    set to avoid data leakage. Note that new categories in the test set will not have
    a corresponding mapping and hence will be replaced by `nan`. To avoid this, use
    f`eature-engine`. Alternatively, you can replace the `nan` with `0`.
  prefs: []
  type: TYPE_NORMAL
- en: To perform count encoding with `feature-engine`, we used `CountFrequencyEncoder()`
    and set `encoding_method` to `'count'`. We left the `variables` argument set to
    `None` so that the encoder automatically finds all the categorical variables in
    the dataset. With `fit()`, the transformer found the categorical variables and
    stored the observation counts per category in the `encoder_dict_` attribute. With
    `transform()`, the transformer replaced the categories with the counts, returning
    a `pandas` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If there are categories in the test set that were not present in the train set,
    the encoder will raise an error by default. You can make it ignore them, in which
    case they will appear as `nan`, or encode them as `0`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also carry out count and frequency encoding with the Python library
    Category Encoders: [https://contrib.scikit-learn.org/category_encoders/count.html](https://contrib.scikit-learn.org/category_encoders/count.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For some useful applications of count encoding, check out this article: [https://letsdatascience.com/frequency-encoding/](https://letsdatascience.com/frequency-encoding/).'
  prefs: []
  type: TYPE_NORMAL
- en: Replacing categories with ordinal numbers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ordinal encoding consists of replacing the categories with digits from *1* to
    *k* (or *0* to *k-1*, depending on the implementation), where *k* is the number
    of distinct categories of the variable. The numbers are assigned arbitrarily.
    Ordinal encoding is better suited for non-linear machine learning models, which
    can navigate through arbitrarily assigned numbers to find patterns that relate
    to the target.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will perform ordinal encoding using `pandas`, `scikit-learn`,
    and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s make the import and prepare the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and the data split function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the Credit Approval dataset and divide it into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To encode the `A7` variable, let’s make a dictionary of category-to-integer
    pairs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `print(ordinal_mapping)`, we will see the digits that will replace
    each category:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s replace the categories in a copy of the DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Go ahead and execute `print(X_train["A7"].head())` to see the result of the
    previous operation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we’ll carry out ordinal encoding using `scikit-learn`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s import the required classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Do not confuse `OrdinalEncoder()` with `LabelEncoder()` from `scikit-learn`.
    The former is intended to encode predictive features, whereas the latter is intended
    to modify the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up the encoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a list containing the categorical variables to encode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s restrict the encoding to the categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Remember to set `remainder` to `"passthrough"` to make the `ColumnTransformer()`
    return the un-transformed variables as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the encoder to the train set so that it creates and stores representations
    of categories to digits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: By executing `ct.named_transformers_["encoder"].categories_`, you can visualize
    the unique categories per variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s encode the categorical variables in the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Go ahead and execute `X_train_enc.head()` to check out the resulting DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`ColumnTransformer()` will mark the encoded variables by appending `encoder`
    to the variable name. The variables that were not modified show the `remainder`
    prefix.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s do ordinal encoding with `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the encoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the encoder so that it replaces categories with arbitrary integers
    in the categorical variables specified in *Step 7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`feature-engine`’s `OrdinalEncoder()` automatically finds and encodes all categorical
    variables if the `variables` parameter is `None`. Alternatively, it will encode
    the variables indicated in the list. In addition, it can assign the integers according
    to the target mean value (see the *Performing ordinal encoding based on the target*
    *value* recipe).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the encoder to the train set so that it learns and stores the category-to-integer
    mappings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The category to integer mappings are stored in the `encoder_dict_` attribute
    and can be accessed by executing `enc.encoder_dict_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s encode the categorical variables in the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`feature-engine` returns `pandas` DataFrames where the values of the original
    variables are replaced with numbers, leaving the DataFrame ready to use in machine
    learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we replaced categories with integers assigned arbitrarily.
  prefs: []
  type: TYPE_NORMAL
- en: We used `pandas`’ `unique()` to find the unique categories of the `A7` variable.
    Next, we created a dictionary of category-to-integer and passed it to `pandas`’
    `map()` to replace the strings in `A7` with the integers.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we carried out ordinal encoding using `scikit-learn`’s `OrdinalEncoder()`
    and used `ColumnTransformer()` to restrict the encoding to categorical variables.
    With `fit()`, the transformer created the category-to-integer mappings based on
    the categories in the train set. With `transform()`, the categories were replaced
    with integers. By setting the `remainder` parameter to `passthrough`, we made
    `ColumnTransformer()` concatenate the variables that are not encoded at the back
    of the encoded features.
  prefs: []
  type: TYPE_NORMAL
- en: To perform ordinal encoding with `feature-engine`, we used `OrdinalEncoder()`,
    indicating that the integers should be assigned arbitrarily through `encoding_method`,
    and passed a list with the variables to encode in the `variables` argument. With
    `fit()`, the encoder assigned integers to each variable’s categories, which were
    stored in the `encoder_dict_` attribute. These mappings were then used by the
    `transform()` method to replace the categories in the train and test sets, returning
    DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When a category in the test set is not present in the training set, it will
    not have a mapping to a digit. `OrdinalEncoder()` from `scikit-learn` and `feature-engine`
    will raise an error by default. However, they have the option to replace unseen
    categories with a user-defined value or `-``1`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn`’s `OrdinalEncoder()` can restrict the encoding to those categories
    with a minimum frequency. `feature-engine`’s `OrdinalEncoder()` can assign the
    numbers based on the target mean value, as we will see in the following recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also carry out ordinal encoding with `OrdinalEncoder()` from Category
    Encoders. Check it out at [http://contrib.scikit-learn.org/category_encoders/ordinal.html](http://contrib.scikit-learn.org/category_encoders/ordinal.html).
  prefs: []
  type: TYPE_NORMAL
- en: Performing ordinal encoding based on the target value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we replaced categories with integers, which were assigned
    arbitrarily. We can also assign integers to the categories given the target values.
    To do this, first, we calculate the mean value of the target per category. Next,
    we order the categories from the one with the lowest to the one with the highest
    target mean value. Finally, we assign digits to the ordered categories, starting
    with *0* to the first category up to *k-1* to the last category, where *k* is
    the number of distinct categories.
  prefs: []
  type: TYPE_NORMAL
- en: This encoding method creates a monotonic relationship between the categorical
    variable and the response and therefore makes the variables more adequate for
    use in linear models.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will encode categories while following the target value using
    `pandas` and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary Python libraries and get the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required Python libraries, functions, and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the Credit Approval dataset and divide it into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s determine the mean target value per category in `A7`, then sort the categories
    from that with the lowest to that with the highest target value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s repeat the computation in *Step 3*, but this time, let’s retain
    the ordered category names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To display the output of the preceding command, we can execute `print(ordered_labels)`:
    `Index([''o'', ''ff'', ''j'', ''dd'', ''v'', ''bb'', ''h'', ''n'', ''z'', ''Missing''],`
    `dtype=''object'', name=''A7'')`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s create a dictionary of category-to-integer pairs, using the ordered list
    we created in *Step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can visualize the result of the preceding code by executing `print(ordinal_mapping)`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_enc = X_train.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc = X_test.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train_enc["A7"] = X_train_enc["A7"].map(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ordinal_mapping)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc["A7"] = X_test_enc["A7"].map(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ordinal_mapping)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the test set contains a category that is not present in the train set, the
    preceding code will introduce `np.nan`.
  prefs: []
  type: TYPE_NORMAL
- en: To visualize the effect of this encoding, let’s plot the relationship of the
    categories of the `A7` variable with the target before and after the encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the mean target response per category of the `A7` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see the non-monotonic relationship between categories of `A7` and the
    target in the following plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Mean target value per category of A7 before the encoding](img/B22396_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Mean target value per category of A7 before the encoding
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the mean target value per category in the encoded variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The encoded variable shows a monotonic relationship with the target – the higher
    the mean target value, the higher the digit assigned to the category:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Mean target value per category of A7 after the encoding.](img/B22396_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Mean target value per category of A7 after the encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s perform ordered ordinal encoding using `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the encoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s set up the encoder so that it assigns integers based on the target
    mean value to all categorical variables in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`OrdinalEncoder()` will find and encode all categorical variables automatically.
    To restrict the encoding to a subset of variables, pass their names in a list
    to the `variables` argument. To encode numerical variables, set `ignore_format=True`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the encoder to the train set so that it finds the categorical variables,
    and then stores the category and integer mappings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s replace the categories with numbers in the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find the digits that will replace each category in the `encoder_dict_`
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the output of the transformation by executing `X_train_enc.head()`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we replaced the categories with integers according to the target
    mean.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of this recipe, we worked with the `A7` categorical variable.
    With `pandas`’ `groupby()`, we grouped the data based on the categories of `A7`,
    and with `pandas`’ `mean()`, we determined the mean value of the target for each
    of those categories. Next, we ordered the categories with `pandas`’ `sort_values()`
    from the ones with the lowest to the ones with the highest target mean response.
    The output of this operation was a `pandas` series, with the categories as indices
    and the target mean as values. With `pandas`’ `index`, we captured the ordered
    categories in an array; then, with Python dictionary comprehension, we created
    a dictionary of category-to-integer pairs. Finally, we used this dictionary to
    replace the category with integers using `pandas`’ `map()`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To avoid data leakage, we determine the category-to-integer mappings from the
    train set.
  prefs: []
  type: TYPE_NORMAL
- en: To perform the encoding with `feature-engine`, we used `OrdinalEncoder()`, setting
    the `encoding_method` to `ordered`. We left the argument variables set to `None`
    so that the encoder automatically detects all categorical variables in the dataset.
    With `fit()`, the encoder found the categorical variables and assigned digits
    to their categories according to the target mean value. The categorical variables’
    names and dictionaries with category-to-digit pairs were stored in the `variables_`
    and `encoder_dict_` attributes, respectively. Finally, using `transform()`, we
    replaced the categories with digits in the train and test sets, returning `pandas`
    DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For an implementation of this recipe with Category Encoders, visit this book’s
    GitHub repository: [https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-05-Ordered-ordinal-encoding.ipynb](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-05-Ordered-ordinal-encoding.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing target mean encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mean encoding** or **target encoding** maps each category to the probability
    estimate of the target attribute. If the target is binary, the numerical mapping
    is the posterior probability of the target conditioned to the value of the category.
    If the target is continuous, the numerical representation is given by the expected
    value of the target given the value of the category.'
  prefs: []
  type: TYPE_NORMAL
- en: In its simplest form, the numerical representation for each category is given
    by the mean value of the target variable for a particular category group. For
    example, if we have a `City` variable, with the categories of `London`, `Manchester`,
    and `Bristol`, and we want to predict the default rate (the target takes values
    of `0` and `1`); if the default rate for `London` is 30%, we replace `London`
    with `0.3`; if the default rate for `Manchester` is 20%, we replace `Manchester`
    with `0.2`; and so on. If the target is continuous – say we want to predict income
    – then we would replace `London`, `Manchester`, and `Bristol` with the mean income
    earned in each city.
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematical terms, if the target is binary, the replacement value, *S*,
    is determined like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the numerator is the number of observations with a target value of *1*
    for category *i* and the denominator is the number of observations with a category
    value of *i*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the target is continuous, *S*, this is determined by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mo>∑</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><msub><mi>n</mi><mi>i</mi></msub></mfrac></mrow></mrow></math>](img/2.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the numerator is the sum of the target across observations in category
    *i* and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/3.png)
    is the total number of observations in category *i*.
  prefs: []
  type: TYPE_NORMAL
- en: These formulas provide a good approximation of the target estimate if there
    is a sufficiently large number of observations with each category value – in other
    words, if ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/4.png)
    is large. However, in many datasets, there will be categories present in a few
    observations. In these cases, target estimates derived from the precedent formulas
    can be unreliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To mitigate poor estimates returned for rare categories, the target estimates
    can be determined as a mixture of two probabilities: those returned by the preceding
    formulas and the prior probability of the target based on the entire training.
    The two probabilities are *blended* using a weighting factor, which is a function
    of the category group size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mi>λ</mi><mfrac><msub><mi>n</mi><mrow><mi>i</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>)</mo></mrow></msub><msub><mi>n</mi><mi>i</mi></msub></mfrac><mo>+</mo><mo>(</mo><mn>1</mn><mo>−</mo><msub><mi>λ</mi><mi>i</mi></msub><mo>)</mo><mfrac><msub><mi>n</mi><mi>λ</mi></msub><mi>N</mi></mfrac></mrow></mrow></mrow></math>](img/5.png)'
  prefs: []
  type: TYPE_IMG
- en: In this formula, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">λ</mml:mi></mml:mrow></mml:msub></mml:math>](img/6.png) is
    the total number of cases where the target takes a value of *1*, *N* is the size
    of the train set, and *𝜆* is the weighting factor.
  prefs: []
  type: TYPE_NORMAL
- en: When the category group is large, *𝜆* tends to *1*, so more weight is given
    to the first term of the equation. When the category group size is small, then
    *𝜆* tends to *0*, so the estimate is mostly driven by the second term of the equation
    – that is, the target’s prior probability. In other words, if the group size is
    small, knowing the value of the category does not tell us anything about the value
    of the target.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weighting factor, *𝜆*, is determined differently in different open-source
    implementations. In Category Encoders, *𝜆* is a function of the group size, *k*,
    and a smoothing parameter, *f*, which controls the rate of transition between
    the first and second term of the preceding equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>λ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mrow><mrow><mo>−</mo><mo>(</mo><mi>n</mi><mo>−</mo><mi>k</mi><mo>)</mo></mrow></mrow><mo>/</mo><mi>f</mi></mrow></msup></mrow></mfrac></mrow></mrow></math>](img/7.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *k* is half of the minimal size for which we *fully trust* the first term
    of the equation. The *f* parameter is selected by the user either arbitrarily
    or with optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `scikit-learn` and `feature-engine`, *𝜆* is a function of the target variance
    for the entire dataset and within the category, and is determined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="normal">λ</mi><mo>=</mo><mfrac><mrow><mi>n</mi><mi>i</mi><mo>×</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo>+</mo><mi>n</mi><mi>i</mi><mo>×</mo><mi>t</mi></mrow></mfrac></mrow></mrow></math>](img/8.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *t* is the target variance in the entire dataset and *s* is the target
    variance within the category. Both implementations are equivalent, but it is important
    to know the equations because they will help you set up the parameters in the
    transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean encoding was designed to encode highly cardinal categorical variables
    without expanding the feature space. For more details, check out the following
    article: Micci-Barreca D. A., *Preprocessing Scheme for High-Cardinality Categorical
    Attributes in Classification and Prediction Problems*. ACM SIGKDD Explorations
    Newsletter, 2001.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will perform mean encoding using `scikit-learn` and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin with this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and the data split function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the Credit Approval dataset and divide it into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s import the transformers from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a list with the names of the categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the encoder to use the target variance to determine the weighting
    factor, as described at the beginning of the recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s restrict the imputation to categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s fit the encoder and transform the datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check out the result by executing `X_train_enc.head()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `fit_transform()` method of `scikit-learn`’s `TargetEncoder()` is not equivalent
    to applying `fit().transform()`. With `fit_transform()`, the resulting dataset
    is encoded based on partial fits over the training folds of a cross-validation
    scheme. This functionality was intentionally designed to prevent overfitting the
    machine learning model to the train set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s perform target encoding with `feature-engine`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the encoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the target mean encoder to encode all categorical variables while
    applying smoothing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`MeanEncoder()` does not apply smoothing by default. Make sure you set it to
    `auto` or to an integer to control the blend between prior and posterior target
    estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the transformer to the train set so that it learns and stores the
    mean target value per category per variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s encode the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The category-to-number pairs are stored as a dictionary of dictionaries in the
    `encoder_dict_` attribute. To display the stored parameters, execute `mean_enc.encoder_dict_.`
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we replaced the categories with the mean target value using
    `scikit-learn` and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: To encode with `scikit-learn`, we used `TargetEncoder()`, leaving the `smooth`
    parameter to its default value of `auto`. Like this, the transformer used the
    target variance to determine the weighting factor for the blend of probabilities.
    With `fit()`, the transformer learned the value it should use to replace the categories,
    and with `transform()`, it replaced the categories.
  prefs: []
  type: TYPE_NORMAL
- en: Note that for `TargetEncoder()`, the `fit()` method followed by `transform()`
    do not return the same dataset as the `fit_transform()`method. The latter encodes
    the training set based on mappings found with cross-validation. The idea is to
    use `fit_transform()` within a pipeline, so the machine learning model does not
    overfit. However, and here is where it gets confusing, the mappings stored in
    the `encodings_` attribute are the same after `fit()` and `fit_transform()`, and
    this is done intentionally so that when we apply `transform()` to a new dataset,
    we obtain the same result regardless of whether we apply `fit()` or `fit_transform()`to
    the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Unseen categories are encoded with the target mean by `scikit-learn`’s `TargetEncoder()`.
    `feature-engine`’s `MeanEncoder()` can either return an error, replace the unseen
    categories with `nan`, or with the target mean.
  prefs: []
  type: TYPE_NORMAL
- en: To perform the target encoding with `feature-engine`, we used `MeanEncoder(),`
    setting the `smoothing` parameter to `auto`. With `fit()`, the transformer found
    and stored the categorical variables and the values to encode each category. With
    `transform()`, it replaced the categories with numbers, returning `pandas` DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to implement target encoding with `pandas` or Category Encoders,
    check out the notebook in the accompanying GitHub repository: [https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an alternative way to return *better* target estimates when the category
    groups are small. The replacement value for each category is determined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>n</mi><mrow><mi>i</mi><mfenced
    open="(" close=")"><mrow><mi>Y</mi><mo>=</mo><mn>1</mn></mrow></mfenced></mrow></msub><mo>+</mo><mi>p</mi><mi>Y</mi><mi>x</mi><mi>m</mi></mrow><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>+</mo><mi>m</mi></mrow></mfrac></mrow></mrow></math>](img/9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>n</mi><mrow><mi>i</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>)</mo></mrow></msub></mrow></math>](img/10.png)is
    the target mean for category *i* and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/11.png)
    is the number of observations with category *i*. The target prior is given by
    *pY* and *m* is the weighting factor. With this adjustment, the only parameter
    that we have to set is the weight, *m*. If *m* is large, then more importance
    is given to the target’s prior probability. This adjustment affects target estimates
    for all categories but mostly for those with fewer observations because, in such
    cases, *m* could be much larger than ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/12.png)
    in the formula’s denominator.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This method is a good alternative to Category Encoders’ `TargetEncoder()` because,
    in Category Encoders’ implementation of target encoding, we need to optimize two
    parameters instead of one (as we did with `feature-engine` and `scikit-learn`)
    to control the smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an implementation of this encoding method using `MEstimateEncoder()`, visit
    this book’s GitHub repository: [https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-06-Target-mean-encoding.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding with Weight of Evidence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Weight of Evidence** (**WoE**) was developed primarily for credit and financial
    industries to facilitate variable screening and exploratory analysis and to build
    more predictive linear models to evaluate the risk of loan defaults.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The WoE is computed from the basic odds ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>WoE</mi><mo>=</mo><mi>log</mi><mrow><mrow><mo>(</mo><mfrac><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>s</mi></mrow><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>n</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>s</mi></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/13.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, positive and negative refer to the values of the target being *1* or *0*,
    respectively. The proportion of positive cases per category is determined as the
    sum of positive cases per category group divided by the total positive cases in
    the training set. The proportion of negative cases per category is determined
    as the sum of negative cases per category group divided by the total number of
    negative observations in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'WoE has the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: WoE = *0* if *p(positive)* / *p(negative)* = *1*; that is, if the outcome is
    random
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WoE > *0* if *p(positive)* > *p(negative)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WoE < *0* if *p(negative)* > *p(positive)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This allows us to directly visualize the predictive power of the category in
    the variable: the higher the WoE, the more likely the event will occur. If the
    WoE is positive, the event is likely to occur.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression models a binary response, *Y*, based on *X* predictor variables,
    assuming that there is a linear relationship between *X* and the log of odds of
    *Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>log</mi><mfenced
    open="(" close=")"><mfrac><mrow><mi>p</mi><mfenced open="(" close=")"><mrow><mi>Y</mi><mo>=</mo><mn>1</mn></mrow></mfenced></mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><mi>Y</mi><mo>=</mo><mn>0</mn></mrow></mfenced></mrow></mfrac></mfenced><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><msub><mi>X</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><msub><mi>X</mi><mn>2</mn></msub><mo>+</mo><mo>…</mo><mo>+</mo><msub><mi>b</mi><mi>n</mi></msub><msub><mi>X</mi><mi>n</mi></msub></mrow></mrow></math>](img/14.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *log (p(Y=1)/p(Y=0))* is the log of odds. As you can see, the WoE encodes
    the categories in the same scale – that is, the log of odds – as the outcome of
    the logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, by using WoE, the predictors are prepared and coded on the same scale,
    and the parameters in the logistic regression model – that is, the coefficients
    – can be directly compared.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will perform WoE encoding using `pandas` and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by making some imports and preparing the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the Credit Approval dataset and divide it into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s get the inverse of the target values to be able to calculate the negative
    cases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s determine the number of observations where the target variable takes
    a value of `1` or `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s calculate the numerator and denominator of the WoE’s formula, which
    we discussed earlier in this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s calculate the WoE per category:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can display the series with the category to WoE pairs by executing `print(woe)`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_enc = X_train.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc = X_test.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train_enc["A1"] = X_train_enc["A1"].map(woe)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc["A1"] = X_test_enc["A1"].map(woe)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s import the encoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s set up the encoder to encode three categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For rare categories, it might happen that `p(0)=0` or `p(1)=0`, and then the
    division or the logarithm is not defined. To avoid this, group infrequent categories
    as shown in the *Grouping rare or infrequent* *categories* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the transformer to the train set so that it learns and stores the
    WoE of the different categories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can display the dictionaries with the categories to WoE pairs by executing
    `woe_enc.encoder_dict_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s encode the three categorical variables in the train and test
    sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`feature-engine` returns `pandas` DataFrames, which contain the encoded categorical
    variables ready to use in machine learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we encoded categorical variables using the WoE with `pandas`
    and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: We combined the use of `pandas`’ `sum()` and `groupby()` and `numpy`’s `log()`
    to determine the WoE as we described at the beginning of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we automated the procedure with `feature-engine`. We used the `WoEEncoder()`,
    which learned the WoE per category with the `fit()` method, and then used `transform()`
    to replace the categories with the corresponding numbers.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For an implementation of WoE with Category Encoders, visit this book’s GitHub
    repository: [https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-07-Weight-of-evidence.ipynb](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch02-categorical-encoding/Recipe-07-Weight-of-evidence.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Grouping rare or infrequent categories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rare categories are those present only in a small fraction of the observations.
    There is no rule of thumb to determine how small a small fraction is, but typically,
    any value below 5% can be considered rare.
  prefs: []
  type: TYPE_NORMAL
- en: Infrequent labels often appear only on the train set or only on the test set,
    thus making the algorithms prone to overfitting or being unable to score an observation.
    In addition, when encoding categories to numbers, we only create mappings for
    those categories observed in the train set, so we won’t know how to encode new
    labels. To avoid these complications, we can group infrequent categories into
    a single category called `Rare` or `Other`.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will group infrequent categories using `pandas` and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary Python libraries and get the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary Python libraries, functions, and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the Credit Approval dataset and divide it into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s capture the fraction of observations per category in `A7` in a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see the percentage of observations per category of `A7`, expressed as
    decimals, in the following output after executing `print(freqs)`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a list containing the names of the categories present in more
    than 5% of the observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `print(frequent_cat)`, we will see the frequent categories of
    `A7`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_enc = X_train.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc = X_test.copy()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train_enc["A7"] = np.where(X_train["A7"].isin(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: frequent_cat), X_train["A7"], "Rare")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_enc["A7"] = np.where(X_test["A7"].isin(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: frequent_cat), X_test["A7"], "Rare")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s determine the percentage of observations in the encoded variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that the infrequent labels have now been re-grouped into the `Rare`
    category:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a rare label encoder that groups categories present in less than
    5% of the observations, provided that the categorical variable has more than four
    distinct values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s fit the encoder so that it finds the categorical variables and then learns
    their most frequent categories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Upon fitting, the transformer will raise warnings, indicating that many categorical
    variables have less than four categories, thus their values will not be grouped.
    The transformer just lets you know that this is happening.
  prefs: []
  type: TYPE_NORMAL
- en: We can display the frequent categories per variable by executing `rare_encoder.encoder_dict_`,
    as well as the variables that will be encoded by executing `rare_encoder.variables_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s group rare labels in the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have grouped rare labels, we are ready to encode the categorical
    variables, as we’ve done in the previous recipes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we grouped infrequent categories using `pandas` and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: We determined the fraction of observations per category of the `A7` variable
    using `pandas`’ `value_counts()` by setting the `normalize` parameter to `True`.
    Using list comprehension, we captured the names of the variables present in more
    than 5% of the observations. Finally, using NumPy’s `where()`, we searched each
    row of `A7`, and if the observation was one of the frequent categories in the
    list, which we checked using `pandas`’ `isin()`, its value was kept; otherwise,
    it was replaced with `Rare`.
  prefs: []
  type: TYPE_NORMAL
- en: We automated the preceding steps for multiple categorical variables using `feature-engine`’s
    `RareLabelEncoder()`. By setting `tol` to `0.05`, we retained categories present
    in more than 5% of the observations. By setting `n_categories` to `4`, we only
    grouped categories in variables with more than four unique values. With `fit()`,
    the transformer identified the categorical variables and then learned and stored
    their frequent categories. With `transform()`, the transformer replaced infrequent
    categories with the `Rare` string.
  prefs: []
  type: TYPE_NORMAL
- en: Performing binary encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`1` can be represented with the sequence of `1-0`, integer `2` with `0-1`,
    integer `3` with `1-1`, and integer `0` with `0-0`. The digits in the two positions
    of the binary string become the columns, which are the encoded representations
    of the original variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Table showing the steps required for binary encoding the color
    variable](img/B22396_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Table showing the steps required for binary encoding the color
    variable
  prefs: []
  type: TYPE_NORMAL
- en: Binary encoding encodes the data in fewer dimensions than one-hot encoding.
    In our example, the `Color` variable would be encoded into *k-1* categories by
    one-hot encoding – that is, three variables – but with binary encoding, we can
    represent the variable with only two features. More generally, we determine the
    number of binary features needed to encode a variable as *log2(number of distinct
    categories)*; in our example, *log2(4) = 2* binary features.
  prefs: []
  type: TYPE_NORMAL
- en: Binary encoding is an alternative method to one-hot encoding where we do not
    lose information about the variable, yet we obtain fewer features after the encoding.
    This is particularly useful when we have highly cardinal variables. For example,
    if a variable contains 128 unique categories, with one-hot encoding, we would
    need 127 features to encode the variable, whereas with binary encoding, we will
    only need *7 (log2(128)=7)*. Thus, this encoding prevents the feature space from
    exploding. In addition, binary-encoded features are also suitable for linear models.
    On the downside, the derived binary features lack human interpretability, so if
    we need to interpret the decisions made by our models, this encoding method may
    not be a suitable option.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to perform binary encoding using Category
    Encoders.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary Python libraries and get the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required Python libraries, functions, and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the Credit Approval dataset and divide it into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s inspect the unique categories in `A7`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that `A7` has 10 different categories:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: encoder = BinaryEncoder(cols=["A7"],
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: drop_invariant=True)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`BinaryEncoder()`, as well as other encoders from the Category Encoders package,
    allow us to select the variables to encode. We simply pass the column names in
    a list to the `cols` argument.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the transformer to the train set so that it calculates how many binary
    variables it needs and creates the variable-to-binary code representations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s encode `A7` in the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can display the top rows of the transformed train set by executing `print(X_train_enc.head())`,
    which returns the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.11 – DataFrame with the variables after binary encoding](img/B22396_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – DataFrame with the variables after binary encoding
  prefs: []
  type: TYPE_NORMAL
- en: Binary encoding returned four binary variables for `A7`, which are `A7_0`, `A7_1`,
    `A7_2`, and `A7_3`, instead of the nine that would have been returned by one-hot
    encoding.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we performed binary encoding using the Category Encoders package.
    We used `BinaryEncoder()` to encode the `A7` variable. With the `fit()` method,
    `BinaryEncoder()` created a mapping from a category to a set of binary columns,
    and with the `transform()` method, the encoder encoded the `A7` variable in both
    the train and test sets.
  prefs: []
  type: TYPE_NORMAL
