<html><head></head><body>
		<div id="_idContainer116">
			<h1 id="_idParaDest-127"><em class="italic"><a id="_idTextAnchor127"/>Chapter 7</em>: Understanding ML Models</h1>
			<p>Now that we have built a few models using H2O software, the next step before production is to understand how the model is making decisions. This has been termed variously as <strong class="bold">machine learning interpretability</strong> (<strong class="bold">MLI</strong>), <strong class="bold">explainable artificial intelligence</strong> (<strong class="bold">XAI</strong>), model explainability, and so on. The gist of all these terms is that building a model that predicts well is not enough. There is an inherent risk in deploying any model before fully trusting it. In this chapter, we outline a set of capabilities within H2O for explaining ML models.</p>
			<p>By the end of this chapter, you will be able to do the following:</p>
			<ul>
				<li>Select an appropriate model metric for evaluating your models.</li>
				<li>Explain what Shapley values are and how they can be used.</li>
				<li>Describe the differences between global and local explainability.</li>
				<li>Use multiple diagnostics to build understanding and trust in a model.</li>
				<li>Use global and local explanations along with model performance metrics to choose the best among a set of candidate models.</li>
				<li>Evaluate tradeoffs between model predictive performance, speed of scoring, and assumptions met in a single candidate model.</li>
			</ul>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Selecting model performance metrics</li>
				<li>Explaining models built in H2O (both globally and locally)</li>
				<li>Automated<a id="_idTextAnchor128"/> model documentation through H2O AutoDoc</li>
			</ul>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor129"/>Selecting model performance metrics</h1>
			<p>The<a id="_idIndexMarker508"/> most relevant question about any model is, <em class="italic">How well does it predict?</em> Regardless of any other positive properties that a model may possess, models that don't predict well are just not very useful. How to best measure predictive performance depends both on the specific problem being solved and the choices available to the data scientist. H2O provides multiple options for measuring model performance.</p>
			<p>For measuring predictive model performance in regression problems, H2O provides R<span class="superscript">2</span>, <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>), <strong class="bold">root mean squared error</strong> (<strong class="bold">RMSE</strong>), <strong class="bold">root mean squared logarithmic error</strong> (<strong class="bold">RMSLE</strong>), and <strong class="bold">mean absolute error </strong>(<strong class="bold">MAE</strong>) as metrics. MSE <a id="_idIndexMarker509"/>and RMSE<a id="_idIndexMarker510"/> are good default options, with RMSE being <a id="_idIndexMarker511"/>our preference because the metric is <a id="_idIndexMarker512"/>expressed in the same units as the predictions (rather than squared units, as in the case of MSE). All metrics based on squared error are sensitive to outliers in general. If robustness to outliers is a requirement, then MAE is a better choice. Finally, RMSLE is useful in the special case where under-prediction is worse than over-prediction. </p>
			<p>For classification <a id="_idIndexMarker513"/>models, H2O adds the Gini coefficient, absolute <strong class="bold">Matthews correlation coefficient</strong> (<strong class="bold">MCC</strong>), F1, F0.5, F2, Accuracy, Logloss, <strong class="bold">area under the ROC curve</strong> (<strong class="bold">AUC</strong>), <strong class="bold">area under the precision-recall curve</strong> (<strong class="bold">AUCPR</strong>), and <strong class="bold">Kolmogorov-Smirnov</strong> (<strong class="bold">KS</strong>) metrics. In<a id="_idIndexMarker514"/> our experience, AUC is <a id="_idIndexMarker515"/>the <a id="_idIndexMarker516"/>most commonly used metric in business. Because communication with business partners and executives is so vital to data scientists, we recommend using well-known metrics when their use is appropriate for the job. In the case of AUC, it does a good job with binary classification models when data is relatively balanced. AUCPR is a better choice for imbalanced data. </p>
			<p>TheLogloss metric, based on information theory, has some mathematical advantages. In particular, if you are interested in the predicted probabilities of class membership themselves and not just the predicted classifications, Logloss is a better choice of metric. Further documentation on these scoring options can be found at <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html</a>. </p>
			<p>Leaderboards created in AutoML for a classification problem include AUC, Logloss, AUCPR, mean per-class error, RMSE, and MSE as performance metrics. The leaderboard for the <strong class="source-inline">check</strong> AutoML object created in <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Model Building – Part 1,</em> is shown in <em class="italic">Figure 7.1</em> as an example:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B16721_07_01.jpg" alt="Figure 7.1 – An AutoML leaderboard for the check object &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – An AutoML leaderboard for the check object </p>
			<p>In addition<a id="_idIndexMarker517"/> to predictive performance, additional metrics of model performance may be important in an enterprise setting. Two of these included by default in <a id="_idIndexMarker518"/>AutoML leaderboards are the amount of time required to fit a model (<strong class="source-inline">training_time_ms</strong>) and the amount of time required to predict a single row of the data (<strong class="source-inline">predict_time_per_row_ms</strong>). </p>
			<p>In <em class="italic">Figure 7.1</em>, the best model according to both AUC and Logloss is a stacked ensemble of all models (the model in the top row). This model is also the slowest to score by an order of magnitude over any of the individual models. For streaming or real-time applications in particular, a model that cannot score quickly enough may automatically be disqualified as a candidate regardless of its predictive performance. </p>
			<p>We next address model explainability for understanding and evaluating our ML models.</p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor130"/>Explaining models built in H2O</h1>
			<p>Model performance<a id="_idIndexMarker519"/> metrics measured on our test data can tell us how well a model predicts and how fast it predicts. As mentioned in the chapter introduction, knowing that a model predicts well is not a sufficient reason to put it into production. Performance metrics alone cannot provide any insight into <em class="italic">why</em> the model is predicting as it is. If we don't understand why the model is predicting well, we have little hope of being able to anticipate conditions that would make the model not work well. The ability to explain a model's reasoning is a critical step prior to promoting it into production. This process can be described as gaining trust in the model. </p>
			<p>Explainability<a id="_idIndexMarker520"/> is<a id="_idIndexMarker521"/> typically divided into global and local components. Global explainability <a id="_idIndexMarker522"/>describes how the<a id="_idIndexMarker523"/> model works for an entire population. Gaining trust in a model is primarily a function of determining how it works globally. Local explanations <a id="_idIndexMarker524"/>operate instead on individual <a id="_idIndexMarker525"/>rows. They address questions such as how an individual prediction came about. The <strong class="source-inline">h2o.explain</strong> and <strong class="source-inline">h2o.explain_row</strong> methods bundle a set of explainability functions and visualizations for global and local explanations, respectively. </p>
			<p>We start this section with a simple introduction to Shapley values, one of the bedrock methods in model explainability, which can be confusing when first encountered. We cover global explanations for single models using <strong class="source-inline">h2o.explain</strong> and local explainability with <strong class="source-inline">h2o.explain_row</strong>. We then address global explanations for AutoML using <strong class="source-inline">h2o.explain</strong>, which we use to demonstrate the role of explainability in model selection. We illustrate the output of these methods using two models developed in <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Model Building – Part 1</em>. The first, <strong class="source-inline">gbm</strong>, is an individual baseline model built using <a id="_idIndexMarker526"/>default values with the H2O <strong class="bold">Gradient Boosting Machine (GBM)</strong> estimator. The second is an AutoML object, <strong class="source-inline">check</strong>. These models are chosen as examples only, acknowledging that the original baseline model was improved upon by multiple feature engineering and model optimization steps.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor131"/>A simple introduction to Shapley values</h2>
			<p>Shapley values <a id="_idIndexMarker527"/>have become an important part of ML explainability as a means for attributing the contribution of each feature to either overall or individual predictions. Shapley values are mathematically elegant and well-suited for the task of attribution. In this section, we provide a description of Shapley values: their origin, calculation, and how to use them for interpretation.</p>
			<p>Lloyd Shapley (1923-2016), a 2012 Nobel Prize winner in Economics, derived Shapley values in 1953 as the solution to a specific problem in game theory. Suppose a group of players working together receives a prize. How should that award be equitably divided amongst the players? </p>
			<p>Shapley started with mathematical axioms <a id="_idIndexMarker528"/>defining fairness: <strong class="bold">symmetry</strong> (players who contribute the same amount get the same payout), <strong class="bold">dummy</strong> (players who contribute nothing receive nothing), and <strong class="bold">additivity</strong> (if the game can be separated into <a id="_idIndexMarker529"/>additive<a id="_idIndexMarker530"/> parts, then you can decompose the payouts). The Shapley value is the unique mathematical solution<a id="_idIndexMarker531"/> that satisfies these axioms. In short, the Shapley value approach pays players in proportion to their marginal contributions.</p>
			<p>We next demonstrate the calculation of Shapley values for a couple of simple scenarios.</p>
			<h3>Shapley calculations illustrated – Two players</h3>
			<p>To illustrate<a id="_idIndexMarker532"/> the calculation of a Shapley value, consider the following simple example. Two musicians, John and Paul, performing on their own can earn £4 and £3, respectively. John and Paul playing together earn £10. How should they divide the £10? </p>
			<p>To calculate their marginal contributions, consider the number of ways these players can be sequenced. For two players, there are only two unique orderings: John is playing and is then joined by Paul, or Paul is playing and is then joined by John. This is illustrated in <em class="italic">Figure 7.2</em>:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B16721_07_02.jpg" alt="Figure 7.2 – Unique player sequences for John and Paul&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Unique player sequences for John and Paul</p>
			<p>The formulation as unique player sequences allows us to calculate Shapley values for each player. We illustrate the calculation of the Shapley value for John in <em class="italic">Figure 7.3</em>:</p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B16721_07_03.jpg" alt="Figure 7.3 – Sequence values for John&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Sequence values for John</p>
			<p>John is the<a id="_idIndexMarker533"/> first player present in sequence 1, thus the Shapley contribution is just the marginal value <em class="italic">v(J) = 4</em>. In the second sequence, John joins after Paul. The marginal value for John is the joint value of John and Paul, <em class="italic">v(JP)</em>, minus the marginal value of Paul, <em class="italic">v(P)</em>. In other words, <em class="italic">10 – 3 = 7</em>. The Shapley value for John is the average of the values for each sequence: <em class="italic">S(J) = 11/2 = 5.5</em>. Therefore, John should receive £5.50 of the £10 payment.</p>
			<p>The Shapley value for Paul is calculated in a similar fashion (obviously, it could also be calculated by subtraction). The sequence calculations are shown in <em class="italic">Figure 7.4</em>:</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B16721_07_04.jpg" alt="Figure 7.4 – Sequence values for Paul&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Sequence values for Paul</p>
			<p>In the first sequence in <em class="italic">Figure 7.4</em>, Paul joins after John, so the sequence value is the joint, <em class="italic">v(JP) = 10</em>, minus the marginal for John, <em class="italic">v(J) = 4</em>. The second sequence is just the marginal value of Paul: <em class="italic">v(P)=3</em>. The Shapley value for Paul is <em class="italic">S(P) = 9/2 = 4.5</em>.</p>
			<p>These calculations are easy and make sense with two players. Let's see what happens when we add a third player.</p>
			<h3>Shapley calculations illustrated – Three players</h3>
			<p>Suppose<a id="_idIndexMarker534"/> a third musician, George, joins John and Paul. George earns £2 on his own, £7 performing with John, £9 performing with Paul, and £20 when all three play together. For clarity, we summarize the earnings in <em class="italic">Figure 7.5</em>:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B16721_07_05.jpg" alt="Figure 7.5 – Earnings for John, Paul, and George&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Earnings for John, Paul, and George</p>
			<p>Because<a id="_idIndexMarker535"/> there are three players, there are 3! = 6 unique sequences in which John, Paul, and George can arrive. The calculations for the Shapley value for John in this three-player scenario are summarized in <em class="italic">Figure 7.6</em>:</p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B16721_07_06.jpg" alt="Figure 7.6 – Arrival sequences and values for calculating the Shapley value for John&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Arrival sequences and values for calculating the Shapley value for John</p>
			<p>In <em class="italic">Figure 7.6</em>, sequences 1 and 2 are straightforward: John is the first player, so the <em class="italic">v(J)</em> value is all that is needed. In sequences 3 and 5, John is the second player. The sequence values are calculated by taking the joint value of John and the first player and then subtracting the marginal value of that player. Sequences 4 and 6 are identical: John is the last player. His marginal contribution is calculated by taking the three-way interaction, <em class="italic">v(JPG)</em>, and subtracting the joint value of Paul and George, <em class="italic">v(PG)</em>. The Shapley value is <em class="italic">S(J) = 42/6 = 7</em>. </p>
			<p>We could continue and find the Shapley values for Paul and George in the same manner.</p>
			<h3>Calculating Shapley values for N players</h3>
			<p>As you can <a id="_idIndexMarker536"/>see, Shapley value calculations can quickly become overwhelming as the number of players, <em class="italic">N</em>, increases. The Shapley sequence calculations depend on knowing the values for the main effects and all the interactions from two-way to <em class="italic">N</em>-way, as in <em class="italic">Figure 7.5</em>. In addition, there are <em class="italic">N!</em> sequences to be solved. The computational task increases dramatically as the number of players increases.</p>
			<p>In the context of a predictive model, each feature is a player and the prediction is the shared prize. We can use Shapley values to attribute the impact of each feature on the final prediction. With some models having dozens or hundreds or possibly, even more, features, computing Shapley values in the real world is non-trivial. Fortunately, a combination of modern computing and mathematical shortcuts for computing Shapley values for certain families of models makes Shapley calculations tenable. </p>
			<p>Whether in simple examples as we have shown or in large complex ML models, the interpretation of Shapley values is the same.</p>
			<p>We next turn our attention to global explanations for single models.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor132"/>Global explanations for single models</h2>
			<p>We illustrat<a id="_idIndexMarker537"/>e single model explanations using the baseline GBM model built in <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Model Building –  Part 1</em>. We labeled this model <strong class="source-inline">gbm</strong> and documented its performance in <em class="italic">Figure 5.5</em> through to <em class="italic">Figure 5.10</em>. </p>
			<p>The basic command for global explanations is as follows:</p>
			<pre class="source-code">model_object.explain(test)</pre>
			<p>Here, <strong class="source-inline">test</strong> is the holdout test dataset used in model evaluation. Additional optional parameters include the following:</p>
			<ul>
				<li><strong class="source-inline">top_n_features</strong>: An integer indicating how many columns to use in column-based methods<a id="_idIndexMarker538"/> such as <strong class="bold">SHapley Additive exPlanations</strong> (<strong class="bold">SHAP</strong>)) and <strong class="bold">Partial Dependence Plots</strong> (<strong class="bold">PDP</strong>). Columns are <a id="_idIndexMarker539"/>ordered according to variable importance. The default value is <strong class="source-inline">5</strong>.</li>
				<li><strong class="source-inline">columns</strong>: A vector of column names to use in column-based methods as an alternative to <strong class="source-inline">top_n_features</strong>. </li>
				<li><strong class="source-inline">include_explanations</strong> or <strong class="source-inline">exclude_explanations</strong>: Respectively, <strong class="source-inline">include</strong> or <strong class="source-inline">exclude</strong> methods such as <strong class="source-inline">confusion_matrix</strong>, <strong class="source-inline">varimp</strong>, <strong class="source-inline">shap_summary</strong>, or <strong class="source-inline">pdp</strong>. </li>
			</ul>
			<p>For a <a id="_idIndexMarker540"/>single classification model such as <strong class="source-inline">gbm</strong>, this command will display the confusion matrix, variable importance plot, SHAP summary plot, and partial dependence plots for the top five variables in order of importance.</p>
			<p>We demonstrate this using our <strong class="source-inline">gbm</strong> model with the <strong class="source-inline">gbm.explain(test)</strong> command and discuss each display in turn. </p>
			<h3>The confusion matrix</h3>
			<p>The first <a id="_idIndexMarker541"/>output result is the confusion matrix, shown in <em class="italic">Figure 7.7</em>:</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B16721_07_07.jpg" alt="Figure 7.7 – Confusion matrix for the GBM baseline model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Confusion matrix for the GBM baseline model</p>
			<p>A nice feature of the <strong class="source-inline">explain</strong> method is that simple summary descriptions are provided for each display: <strong class="bold">Confusion matrix shows a predicted class vs an actual class</strong>. In <em class="italic">Figure 7.7</em>, the confusion matrix for <strong class="source-inline">gbm</strong> shows true negatives (17,616), false positives (2,087), false negatives (1,716), and true positives (2,057), along with a false <a id="_idIndexMarker542"/>positive rate (10.59%) and a false negative rate (45.48%). </p>
			<h3>The variable importance plot</h3>
			<p>The <a id="_idIndexMarker543"/>second visualization from the <strong class="source-inline">explain</strong> method is the variable importance plot, shown in <em class="italic">Figure 7.8</em>:</p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B16721_07_08.jpg" alt="Figure 7.8 – Variable importance plot for the GBM baseline model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – Variable importance plot for the GBM baseline model</p>
			<p>Note that the variable importance plot in <em class="italic">Figure 7.8</em> is identical to the plot displayed in <em class="italic">Figure 5.10</em> that we created manually using the <strong class="source-inline">varimp_plot</strong> command. Its inclusion here is one of the benefits of using the <strong class="source-inline">explain</strong> method.</p>
			<h3>The SHAP summary plot</h3>
			<p>The third visualization output by <strong class="source-inline">explain</strong> is a SHAP summary plot. <strong class="bold">SHAP</strong>, based on Shapley values, provides<a id="_idIndexMarker544"/> an informative view into black-box models. The SHAP summary plot for the GBM baseline model is shown in <em class="italic">Figure 7.9</em>:</p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B16721_07_09.jpg" alt="Figure 7.9 – SHAP summary plot for the GBM baseline model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – SHAP summary plot for the GBM baseline model</p>
			<p>Let's explain in a little more detail the<a id="_idIndexMarker545"/> SHAP summary plot in <em class="italic">Figure 7.9</em>. There is a lot going on in this informative plot:</p>
			<ul>
				<li>On the left-hand side, we have features (data columns) listed in order of decreasing feature importance based on Shapley values. (Note that Shapley feature importance rankings are not necessarily identical to the feature importance in <em class="italic">Figure 7.8</em>.) </li>
				<li>On the right-hand side, we have a normalized feature value scale going from <strong class="bold">0.0</strong> to <strong class="bold">1.0</strong> (blue to red as output by H2O). In other words, for each feature, we code the original data values by color: low original values as blue transitioning through purple for middling values, and ending with high original values as red (they show as varying shades of gray in this figure). </li>
				<li>The horizontal location of each observation is determined by its SHAP value. SHAP values measure the contribution of each feature to the prediction. Lower SHAP values are associated with lower predictions and higher values are associated with higher predictions.</li>
			</ul>
			<p>With that<a id="_idIndexMarker546"/> initial understanding, we can make the following observations: </p>
			<ul>
				<li>Features that have red values to the right and blue values to the left are positively correlated with the response. Since we are modeling the probability of a bad loan, features such as longer term (<strong class="source-inline">term</strong>) or higher revolving utilization (<strong class="source-inline">revol_util</strong>) are positively correlated with loan default. (Revolving credit utilization is essentially how large a customer's credit card balances are month-to-month.)</li>
				<li>Features with red values to the left and blue values to the right are negatively correlated with the response. So, for example, higher annual income (<strong class="source-inline">annual_inc</strong>) is negatively correlated with a loan going into default. </li>
			</ul>
			<p>These model observations from the SHAP summary plot make intuitive sense. You might expect someone who carries a larger credit card balance or makes a lower annual income to have an increased probability of defaulting on a loan. </p>
			<p>Note that we can get the same plot using the <strong class="source-inline">gbm.shap_summary_plot(test)</strong> command.</p>
			<h3>Partial dependence plots</h3>
			<p>The <a id="_idIndexMarker547"/>fourth visualization output by <strong class="source-inline">explain</strong> is a set of partial dependence plots. The specific plots shown depend on the <strong class="source-inline">top_n_features</strong> or <strong class="source-inline">columns</strong> optional parameters. By default, the top five features are shown in order of decreasing variable importance. <em class="italic">Figure 7.10</em> displays the partial dependence plots for the address state:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B16721_07_10.jpg" alt="Figure 7.10 – Partial dependence plot for address state&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10 – Partial dependence plot for address state</p>
			<p><em class="italic">Figure 7.11</em> displays <a id="_idIndexMarker548"/>the partial dependence plot for the revolving utilization variable:</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B16721_07_11.jpg" alt="Figure 7.11 – Partial dependence plot for revolving utilization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.11 – Partial dependence plot for revolving utilization</p>
			<p>The partial dependence plots<a id="_idIndexMarker549"/> output by <strong class="source-inline">explain</strong> include a representation of sample size (the shaded area starting from the bottom of the graph) overlayed by the mean response and its variability (the line surrounded by a shaded region). In the case of categorical variables, the mean response is a dot with bars indicating variability, as in <em class="italic">Figure 7.10</em>. In the case of numeric variables, the mean response is a dark line with lighter shading indicating variability, as in <em class="italic">Figure 7.11</em>.</p>
			<p>Note that we can create a partial dependence plot for any individual column using the following:</p>
			<pre class="source-code">gbm.pd_plot(test, column='revol_util')</pre>
			<h3>The global individual conditional expectation (ICE) plot</h3>
			<p>ICE plots <a id="_idIndexMarker550"/>will be introduced later in the <em class="italic">Local explanations for single models</em> section. However, for completeness, we include a global version of the ICE plot here. Note that this plot is not output by <strong class="source-inline">explain</strong>. The <strong class="source-inline">gbm.ice_plot(test, column='revol_util')</strong> command returns a global ICE plot as shown in <em class="italic">Figure 7.12</em>:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B16721_07_12.jpg" alt="Figure 7.12 – Global ICE plot for revolving utilization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.12 – Global ICE plot for revolving utilization</p>
			<p>The <a id="_idIndexMarker551"/>global ICE plot for a variable is an expansion of the partial dependence plot for that variable. The partial dependence plot displays how the mean response is related to the values of a specific variable. Shading, as shown in <em class="italic">Figure 7.11</em>, indicates the variability of the partial dependence line. The global ICE plot amplifies this by using multiple lines to represent the population. (In the case of categorical variables, the lines are replaced by points and the shading by bars.)</p>
			<p>As shown in <em class="italic">Figure 7.12</em>, the global ICE plot includes lines for the minimum (0th percentile), the deciles (10th percentile through 90th percentile by 10s), the maximum (100th percentile), and the partial dependence itself. This visually portrays the population much more accurately than partial dependence alone. Percentiles that parallel the partial dependence line correspond to segments of the population for which the partial dependence is a good representation. As is often the case, the behavior of the minimum and maximum of a population may be quite different than the mean behavior described by the partial dependence line. In <em class="italic">Figure 7.12</em>, there are three lines that are different from the others: the minimum, the maximum, and the 10th percentile. </p>
			<p>We next turn our attention to local explanations for single models.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor133"/>Local explanations for single models</h2>
			<p>The <strong class="source-inline">h2o.explain_row</strong> method <a id="_idIndexMarker552"/>allows a<a id="_idIndexMarker553"/> data scientist to investigate local explanations of a model. While global explanations are used for understanding how the model represents the overall population, local explanations give us the ability to interrogate a model on a per-row basis. This can be especially important in business when rows represent customers, as is the case in our Lending Club analysis. </p>
			<p>When predictive models are used to make decisions that impact customers directly (for instance, not approving a loan application or raising a customer's insurance rates), global explanations are not sufficient to satisfy business, legal, or regulatory requirements. This is where local explanations are critical.</p>
			<p>The <strong class="source-inline">explain_row</strong> method<a id="_idIndexMarker554"/> returns these local explanations for a specified <strong class="source-inline">row_index</strong> value. The <strong class="source-inline">gbm.explain_row(test, row_index=10)</strong> command provides a SHAP explanation plot and multiple ICE plots for columns based on variable importance. As with partial dependence plots, the <strong class="source-inline">top_n_features</strong> or <strong class="source-inline">columns</strong> parameters can optionally be provided.</p>
			<p>The resulting SHAP explanation plot is shown in <em class="italic">Figure 7.13</em>:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B16721_07_13.jpg" alt="Figure 7.13 – SHAP explanation for index = 10&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13 – SHAP explanation for index = 10</p>
			<p>SHAP explanations <a id="_idIndexMarker555"/>show the contributions of each variable to the overall prediction based on Shapley values. For the customer displayed in <em class="italic">Figure 7.13</em>, the positive SHAP values can be thought of as increasing the probability of loan default, while the negative SHAP values are those decreasing the probability of default. For this customer, the revolving utilization rate of 78.5% is the largest positive contributor to the predicted probability. The largest negative contributor is the annual income of 90,000, which decreases the probability of loan default more than any other variable. SHAP explanations can be used to provide <strong class="bold">reason codes</strong> that<a id="_idIndexMarker556"/> can help explain the model. Reason codes can also be used as the basis for sharing information directly with the customer, for instance, in adverse action codes that apply to some financial and insurance-related regulatory models. </p>
			<p>We next visit some of the ICE plots output from the <strong class="source-inline">explain_row</strong> method. </p>
			<h3>ICE plots for local explanations</h3>
			<p>ICE plots are <a id="_idIndexMarker557"/>individual or per-row counterparts of partial dependence plots. Just as partial dependence plots for a feature display the mean response of the target variable while varying the feature value, the ICE plot measures the target variable response while varying the feature value for a single row. Consider the ICE plot for the address state displayed in <em class="italic">Figure 7.14</em> as a result of the <strong class="source-inline">gbm.explain_row</strong> call:</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B16721_07_14.jpg" alt="Figure 7.14 – ICE plot for address state&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14 – ICE plot for address state</p>
			<p>The vertical dark dashed line in <em class="italic">Figure 7.14</em> represents the actual response for the row in question. In this case, the state is <strong class="bold">NJ</strong> (New Jersey) with a response of approximately 0.10. Had the state for this row been <strong class="bold">VA</strong> (Virginia), the response would have been lower (about 0.07). Had the state for this row instead been <strong class="bold">NV</strong> (Nevada), the response would have been higher, around 0.16.</p>
			<p>Consider next <em class="italic">Figure 7.15</em>, the ICE plot for the term of the loan:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B16721_07_15.jpg" alt="Figure 7.15 – ICE plot for the loan term&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.15 – ICE plot for the loan term</p>
			<p>The <a id="_idIndexMarker558"/>SHAP explanation value in <em class="italic">Figure 7.13</em> for a term of 36 months was the second-largest negative factor (it reduced the probability of loan default the most after annual income, which was the largest). According to <em class="italic">Figure 7.15</em>, a loan term of 60 months would have resulted in a default probability of slightly more than 0.35, significantly higher than the approximate 0.10 probability of default with a term of 36 months. While SHAP explanations and ICE plots are measuring two different things, their interpretations can be used jointly to understand the behavior of a particular prediction.</p>
			<p>The last ICE plot we consider is for revolving utilization, a numerical rather than categorical feature. This plot is shown in <em class="italic">Figure 7.16</em>:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B16721_07_16.jpg" alt="Figure 7.16 – ICE plot for revolving utilization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.16 – ICE plot for revolving utilization</p>
			<p>Revolving utilization <a id="_idIndexMarker559"/>was the most significant positive factor (increasing the probability of loan default) according to the SHAP explanations in <em class="italic">Figure 7.13</em>. The ICE plot in <em class="italic">Figure 7.16</em> shows the relationship between response and the value of revolving utilization. Had <strong class="source-inline">revol_util</strong> been 50%, the probability of loan default would have been reduced to approximately 0.08. At 20%, the probability of default would be approximately 0.05. If this customer were denied a loan, the high value of revolving utilization would be a defensible reason. Results of the corresponding ICE plot can be used to inform the customer of steps they could take to qualify for the loan.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor134"/>Global explanations for multiple models</h2>
			<p>In determining <a id="_idIndexMarker560"/>which model to promote into production, for instance, from an AutoML run, the data scientist could rely purely on predictive model metrics. This could mean simply promoting the model with the best AUC value. However, there is a lot of information that could be used to help in this decision, with predictive power being only one of multiple criteria. </p>
			<p>The global and local explain features of H2O provide additional information that is useful for evaluating models in conjunction with predictive attributes. We demonstrate it using the <strong class="source-inline">check</strong> AutoML object from <a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 5</em></a>,<em class="italic"> Advanced Model Building – Part 1</em>.</p>
			<p>The code to launch global explanations for multiple models is simply as follows:</p>
			<pre class="source-code">check.explain(test)</pre>
			<p>This results in a variable importance heatmap, model correlation heatmap, and multiple-model <a id="_idIndexMarker561"/>partial dependence plots. We will review each of these in order.</p>
			<h3>Variable importance heatmap</h3>
			<p>The variable importance heatmap <a id="_idIndexMarker562"/>visually combines the variable importance plots for multiple models by adding color as a dimension to be viewed along with variables (as rows) and models (as columns). The variable importance heatmap produced by <strong class="source-inline">check.explain</strong> is shown in <em class="italic">Figure 7.17</em>:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B16721_07_17.jpg" alt="Figure 7.17 – Variable importance heatmap for an AutoML object&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.17 – Variable importance heatmap for an AutoML object</p>
			<p>Variable importance values are coded as a color continuum from blue (cold) for low values to red (hot) for high values. The resulting figure is visually meaningful. In <em class="italic">Figure 7.17</em>, vertical bands correspond to each model and horizontal bands correspond to individual features. Vertical bands that are similar indicate a high level of correspondence between how models use their features. For instance, the <strong class="source-inline">XGBoost_1</strong> and <strong class="source-inline">XGBoost_2</strong> models (the last two columns) display similar patterns.</p>
			<p>You also see <a id="_idIndexMarker563"/>horizontal bands of similar color for variables such as <strong class="source-inline">delinq_2yrs</strong>, <strong class="source-inline">verification_status</strong>, or to a lesser extent, <strong class="source-inline">annual_inc</strong>. This indicates that all the candidate models treat these variables with comparable importance. The <strong class="source-inline">term</strong> variable in the last row is the most visually striking, being heterogeneous across models. These models don't agree on its absolute importance. However, you must be careful not to read too much into this. Notice that for <strong class="source-inline">term</strong>, the <em class="italic">relative</em> importance is the same for six of the ten models (all but the blue squares: <strong class="source-inline">DRF_1</strong>, <strong class="source-inline">GBM_4</strong>, <strong class="source-inline">XGBoost_2</strong>, and <strong class="source-inline">XGBoost_1</strong>). For these six models, <strong class="source-inline">term</strong> is the most important feature although its exact value varies widely. </p>
			<p>The code to create this display directly is as follows:</p>
			<pre class="source-code">check.varimp_heatmap()</pre>
			<p>Let's next consider the model correlation heatmap.</p>
			<h3>Model correlation heatmap</h3>
			<p>The <a id="_idIndexMarker564"/>variable importance heatmap allows us to compare multiple models in terms of how they view and use their component variables. The model correlation heatmap addresses a different question: <em class="italic">How correlated are the predictions from these different models?</em> To answer this, we turn to the model correlation heatmap in <em class="italic">Figure 7.18</em>:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B16721_07_18.jpg" alt="Figure 7.18 – Model correlation heatmap for an AutoML object&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.18 – Model correlation heatmap for an AutoML object</p>
			<p>The <a id="_idIndexMarker565"/>darkest blocks along the diagonal of <em class="italic">Figure 7.18</em> show a perfect correlation between a model and itself. Sequentially lighter shading describes decreasing correlation between models. How might you use this display to determine which model to promote to production? </p>
			<p>This is where business or regulatory constraints can come into play. In our example, <strong class="source-inline">StackedEnsemble_AllModels</strong> had the best model performance in terms of AUC. Suppose that we are not allowed to promote an ensemble model into production, for whatever reason. The single models that are most highly correlated with our best model include <strong class="source-inline">XGBoost_3</strong>, <strong class="source-inline">GBM_5</strong>, and <strong class="source-inline">GLM_1</strong>. These could then become candidates to promote into production, with a final decision based on additional criteria (perhaps the AUC value on the test set).</p>
			<p>If one of those additional criteria is native interpretability, then <strong class="source-inline">GLM_1</strong> for this AutoML object is the only choice. Note that interpretable models are indicated with a red-colored font in the model correlation heatmap.</p>
			<p>We can create this display directly using the following: </p>
			<pre class="source-code">check.model_correlation_heatmap(test)</pre>
			<p>Let's move on to introduce <a id="_idIndexMarker566"/>partial dependence plots for multiple models in the next subsection.</p>
			<h3>Multiple-model partial dependence plots</h3>
			<p>The <a id="_idIndexMarker567"/>third output from the <strong class="source-inline">explain</strong> method for multiple models is an extension of the partial dependence plot. For categorical variables, plot symbols and colors corresponding to different models are displayed on an individual plot. <em class="italic">Figure 7.19</em> is an example using the <strong class="source-inline">term</strong> variable:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B16721_07_19.jpg" alt="Figure 7.19 – Multiple model partial dependence plot for a loan term&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.19 – Multiple model partial dependence plot for a loan term</p>
			<p>For numeric variables, multiple models are represented by different colored lines on the same partial dependence plot. <em class="italic">Figure 7.20</em> is an example of this using the <strong class="source-inline">revol_util</strong> variable:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B16721_07_20.jpg" alt="Figure 7.20 – Multiple model partial dependence plot for revolving utilization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.20 – Multiple model partial dependence plot for revolving utilization</p>
			<p>In <em class="italic">Figure 7.19</em> and <em class="italic">Figure 7.20</em>, the competing models yield very similar results. This is not always the <a id="_idIndexMarker568"/>case. For example, <em class="italic">Figure 7.21</em> shows the multiple model partial dependence plot for annual income:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B16721_07_21.jpg" alt="Figure 7.21 – Multiple model partial dependence plot for annual income&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.21 – Multiple model partial dependence plot for annual income</p>
			<p>Although <a id="_idIndexMarker569"/>most of the models in <em class="italic">Figure 7.21</em> are similar for lower incomes, they diverge rather drastically as incomes increase. This is partially due to the very small sample sizes in the tails of the annual income distribution. The data scientist may also decide to disqualify certain models based on unrealistic or unreasonable tail behavior. For example, based on our experience, it does not make sense for loan default risk to increase as annual income increases. At worst, we would expect no relationship between income and default beyond a certain point. We are more likely to expect a monotonic decrease in loan default as income increases. Based on this reasoning, we would remove the models for the top two lines (<strong class="source-inline">DRF_1</strong> and <strong class="source-inline">GBM_1</strong>) from consideration.</p>
			<p>As with other <strong class="source-inline">explain</strong> methods, we can create this plot directly using the following command:</p>
			<pre class="source-code">check.pd_multi_plot(test, column='annual_inc')</pre>
			<p>We next visit model documentation.</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor135"/>Automated model documentation (H2O AutoDoc)</h1>
			<p>One of the important roles a data science team performs in an enterprise setting is documenting the history, attributes, and performance of models that are put into production. At a minimum, model documentation should be part of a data science team's best practices. More commonly in an enterprise setting, thorough model documentation or whitepapers are mandated to satisfy internal and external controls as well as regulatory or compliance requirements.</p>
			<p>As a rule, model documentation <a id="_idIndexMarker570"/>should be comprehensive enough to allow for the recreation of the model being documented. This entails identifying all data sources, including training and test data characteristics, specifying hardware system components, noting software versions, modeling code, software settings and seeds, modeling assumptions adopted, alternative models considered, performance metrics and appropriate diagnostics, and anything else necessary based on business or regulatory conditions. This process, while vital, is time-consuming and can be tedious. </p>
			<p><strong class="bold">H2O AutoDoc</strong> is a commercial software product that automatically creates comprehensive documentation for models built in H2O-3 and scikit-learn. A similar capability has existed in H2O.ai's <strong class="bold">Driverless AI</strong>, a <a id="_idIndexMarker571"/>commercial product that combines automatic feature engineering with enhanced AutoML to build and deploy supervised learning models. AutoDoc has been successfully used for documenting models now in production. We present a brief introduction to automatic document creation <a id="_idIndexMarker572"/>using AutoDoc here:</p>
			<ol>
				<li>After a model object has been created, we import the <strong class="source-inline">Config</strong> and <strong class="source-inline">render_autodoc</strong> modules into Python:<p class="source-code">from h2o_autodoc import Config</p><p class="source-code">from h2o_autodoc import render_autodoc</p></li>
				<li>Next, we will specify the output file path:<p class="source-code">config = Config(output_path = "autodoc_report.docx")</p></li>
				<li>Then, we will render the report by passing the configuration information and model object:<p class="source-code">doc_path = render_autodoc(h2o=h2o, config=config,</p><p class="source-code">                          model=gbm)</p></li>
				<li>Once the <a id="_idIndexMarker573"/>report is created, the location of the report can be indicated using the following:<p class="source-code">print(doc_path)</p></li>
			</ol>
			<p><em class="italic">Figure 7.22</em> shows the table of contents for a 44-page report created by H2O AutoDoc in Microsoft Word:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B16721_07_22.jpg" alt="Figure 7.22 – Table of contents for model documentation created by H2O AutoDoc&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.22 – Table of contents for model documentation created by H2O AutoDoc</p>
			<p>The advantages of thorough documentation produced in a consistent manner with a minimal amount of manual effort are self-evident. Output as either a Microsoft Word document or in markdown format, the reports can be individually edited and further customized. Report templates are also easily edited, allowing a data science team to have a different report structure for different uses: internal whitepaper or report for regulatory review, for example. The AutoDoc capability is consistently one of the best-loved features for H2O software for the enterprise.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor136"/>Summary</h1>
			<p>In this chapter, we reviewed multiple model performance metrics and learned how to choose one for evaluating a model's predictive performance. We introduced Shapley values through some simple examples to further understand their purpose and use in predictive model evaluation. Within H2O, we used the <strong class="source-inline">explain</strong> and <strong class="source-inline">explain_row</strong> commands to create global and local explanations for a single model. We learned how to interpret the resulting diagnostics and visualizations to gain trust in a model. For AutoML objects and other lists of models, we generated global and local explanations and saw how to use them alongside model performance metrics to weed out inappropriate candidate models. Putting it all together, we can now evaluate tradeoffs between model performance, scoring speed, and explanations in determining which model to put into production. Finally, we discussed the importance of model documentation and showed how H2O AutoDoc can automatically generate detailed documentation for any model built in H2O (or scikit-learn).</p>
			<p>In the next chapter, we will put everything we have learned about building and evaluating models in H2O together to create a deployment-ready model for predicting bad loans in the Lending Club data.</p>
		</div>
	</body></html>