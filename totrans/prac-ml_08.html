<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Clustering based learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Clustering based learning</h1></div></div></div><p>In this chapter, we will cover the clustering-based learning methods, and in specific the k-means clustering algorithm among others. Clustering-based learning is an unsupervised learning technique and thus works without a concrete definition of the target attribute. You will learn basics and the advanced concepts of this technique, and get hands-on implementation guidance in using Apache Mahout, R, Julia, Apache Spark, and Python to implement the k-means clustering algorithm.</p><p>The following figure depicts different learning models covered in this book, and the techniques highlighted in orange will be dealt in detail in this chapter:</p><div class="mediaobject"><img src="graphics/B03980_08_01.jpg" alt="Clustering based learning"/></div><p>The topics listed next are covered in depth in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The core principles and objectives of the clustering-based learning methods</li><li class="listitem" style="list-style-type: disc">How to represent clusters and understand the required distance measurement techniques</li><li class="listitem" style="list-style-type: disc">Learning in depth, the k-means clustering and choosing the right clustering algorithm and the rules of cluster evaluation. More importantly, choosing the right number of clusters.</li><li class="listitem" style="list-style-type: disc">An overview of hierarchical clustering, data standardization, discovering holes, and data regions.</li><li class="listitem" style="list-style-type: disc">Sample implementation using the Apache Mahout, R, Apache Spark, Julia, and Python (scikit-learn) libraries and modules.</li></ul></div><div class="section" title="Clustering-based learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec40"/>Clustering-based learning</h1></div></div></div><p>The<a id="id870" class="indexterm"/> clustering-based learning method is identified as an unsupervised learning task wherein the learning starts from no specific target attribute in mind, and the data is explored with a goal of finding intrinsic structures in them.</p><p>The following diagram represents the scope of the clustering-based learning method that will be covered in this chapter:</p><div class="mediaobject"><img src="graphics/B03980_08_02.jpg" alt="Clustering-based learning"/></div><p>The primary goal of the clustering technique is finding similar or homogenous groups in data that are called <a id="id871" class="indexterm"/>
<span class="strong"><strong>clusters</strong></span>. The way this is done is—data instances that are similar or, in short, are near to each other are grouped in one cluster, and the instances that are different are grouped into a different cluster. The following diagram shows a depiction of data points on a graph, and how the clusters are marked (in here, it is by pure intuition) by the three natural clusters:</p><div class="mediaobject"><img src="graphics/B03980_08_03.jpg" alt="Clustering-based learning"/></div><p>Thus, a cluster <a id="id872" class="indexterm"/>can be defined as a collection of objects that are similar to each other and dissimilar from the objects of another cluster. The following diagram depicts the clustering process:</p><div class="mediaobject"><img src="graphics/B03980_08_04.jpg" alt="Clustering-based learning"/></div><p>Some simple examples of clustering<a id="id873" class="indexterm"/> can be as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Shirts are grouped based on the sizes small (S), medium (M), large (L), extra large (XL), and so on</li><li class="listitem" style="list-style-type: disc">Target Marketing: grouping customers according to their similarities</li><li class="listitem" style="list-style-type: disc">Grouping text <a id="id874" class="indexterm"/>documents: The requirement here is to organize documents, and build a topic hierarchy based on their content similarities</li></ul></div><p>In fact, clustering techniques are very heavily used in many domains such as archeology, biology, marketing, insurance, libraries, financial services, and many others.</p></div></div>
<div class="section" title="Types of clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec41"/>Types of clustering</h1></div></div></div><p>Cluster analysis is <a id="id875" class="indexterm"/>all about the kind of algorithms that can be used to find clusters automatically given the data. There are primarily two classes of clustering algorithm; they are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The Hierarchical Clustering algorithms</li><li class="listitem" style="list-style-type: disc">The Partitional Clustering algorithms<div class="mediaobject"><img src="graphics/B03980_08_05.jpg" alt="Types of clustering"/></div></li></ul></div><p>The Hierarchical clustering algorithms define clusters that have a hierarchy, while the partitional clustering algorithms define clusters that divide the dataset into mutually disjoint partitions.</p><div class="section" title="Hierarchical clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec93"/>Hierarchical clustering</h2></div></div></div><p>The Hierarchical clustering <a id="id876" class="indexterm"/>is about defining clusters that have a hierarchy, and <a id="id877" class="indexterm"/>this is done either by iteratively merging smaller clusters into a larger cluster, or dividing a larger cluster into smaller clusters. This hierarchy of clusters that are produced by a clustering algorithm is called a <a id="id878" class="indexterm"/>
<span class="strong"><strong>dendogram</strong></span>. A dendogram is one of the ways in which the hierarchical clusters can be represented, and the user can realize different clustering based on the level at which the dendogram is defined. It uses a similarity scale that represents the distance between the clusters that were grouped from the larger cluster. The following diagram depicts a dendogram representation for the Hierarchical clusters:</p><div class="mediaobject"><img src="graphics/B03980_08_06.jpg" alt="Hierarchical clustering"/></div><p>There is another simple way of representing the Hierarchical clusters; that is, the Venn diagram. In this representation, we circle the data points that are a part of the cluster. The following diagram depicts a Venn representation for five data points:</p><div class="mediaobject"><img src="graphics/B03980_08_07.jpg" alt="Hierarchical clustering"/></div><p>There are two<a id="id879" class="indexterm"/> clustering algorithms in the Hierarchical <a id="id880" class="indexterm"/>clustering: the Agglomerative and Divisive clustering algorithms.</p><p>The Agglomerative clustering algorithm<a id="id881" class="indexterm"/> uses the bottom-up approach and merges a set of clusters into a larger cluster. The Divisive clustering algorithm<a id="id882" class="indexterm"/> uses the top-down approach and splits a cluster into subclusters. The identification of which cluster will be considered for merging or splitting is decided using greedy methods, and distance measurement becomes critical here. Let's have a quick recap of the instance-based learning methods from <a class="link" href="ch06.html" title="Chapter 6. Instance and Kernel Methods Based Learning">Chapter 6</a>, <span class="emphasis"><em>Instance and Kernel Methods Based Learning</em></span>. We have covered the Euclidean distance, Manhattan distance, and cosine similarity as some of the most commonly used metrics of similarity for numeric data, and hamming distance for non-numeric data. For the Hierarchical clustering, the actual data points are not required? only the distance measure matrix is sufficient, as the grouping is done based on the distances.</p><p>The Hierarchical clustering algorithm steps can be defined as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start with clusters such as <span class="emphasis"><em>S1={X1}, S2={X2} … Sm= {Xm}</em></span>.</li><li class="listitem">Find a set of the nearest clusters and merge them into a single cluster.</li><li class="listitem">Repeat the step 2 until the number of clusters formed is equal to a number defined.</li></ol></div></div><div class="section" title="Partitional clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec94"/>Partitional clustering</h2></div></div></div><p>Partitional clustering<a id="id883" class="indexterm"/> algorithms are different in comparison to the <a id="id884" class="indexterm"/>Hierarchical clustering algorithms as the clusters or partitions are generated and evaluated using a specific predefined criterion that is domain specific. Since each cluster formed is mutually exclusive, there can never be a hierarchical relationship between the clusters. In fact, every instance can be placed in one and only one of the <span class="emphasis"><em>k</em></span> clusters. The number of clusters (<span class="emphasis"><em>k</em></span>) to be formed is input to this algorithm, and this one set of <span class="emphasis"><em>k</em></span> clusters is the output of the partitional cluster algorithms. One of the most commonly used partitional clustering algorithms that we will be covering in this chapter is the k-means clustering algorithm.</p><p>Before we take a deep dive into the k-means clustering algorithm, let's have a quick definition stated here. With an input of <span class="emphasis"><em>k</em></span>, which denotes the number of expected clusters, <span class="emphasis"><em>k</em></span> centers or centroids will be defined that will facilitate defining the <span class="emphasis"><em>k</em></span> partitions. Based on these centers (centroids), the algorithm identifies the members and thus builds a partition followed by the recomputation of the new centers based on the identified members. This process is iterated until the clear, and optimal dissimilarities that make the partition really unique are exposed. Hence, the accuracy of the centroids is the key for the partition-based clustering algorithm to be successful. The following are the steps involved in the centroid-based partitional clustering algorithm:</p><p>Input: <span class="emphasis"><em>k</em></span> (the number of clusters) and <span class="emphasis"><em>d</em></span> (the data set with <span class="emphasis"><em>n</em></span> objects)</p><p>Output: Set of <span class="emphasis"><em>k</em></span> clusters that minimize the sum of dissimilarities of all the objects to the identified mediod (centroid)</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Identify the <span class="emphasis"><em>k</em></span> objects as the first set of centroids.</li><li class="listitem">Assign the remaining objects that are nearest to the centroid.</li><li class="listitem">Randomly select a non-centroid object and recompute the total points that will be swapped to form a new set of centroids, until you need no more swapping.</li></ol></div><p>The Hierarchical and partitional clustering techniques inherently have key differences in many aspects, and some of them include some basic assumptions; execution time assumptions, input parameters, and resultant clusters. Typically, partitional clustering is faster than Hierarchical clustering. While the hierarchical clustering can work with similarity measure alone, partitional clustering requires number of clusters and details around the initial centers. The Hierarchical clustering does not require any input parameters while the partitional clustering algorithms require an input value that indicates the number of clusters required to start running. The cluster definition for hierarchical clustering technique is more subjective as against the partitional clustering results in a exact and precise "k" cluster.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip07"/>Tip</h3><p>The quality of clustering depends on the chosen algorithm, distance function, and the application. A cluster quality is said to be the best when the inter-cluster distance is <span class="emphasis"><em>maximized,</em></span> and the intra-cluster distance is <span class="emphasis"><em>minimized</em></span>.</p></div></div></div></div>
<div class="section" title="The k-means clustering algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec42"/>The k-means clustering algorithm</h1></div></div></div><p>In this section, we will cover the <a id="id887" class="indexterm"/>k-means clustering algorithm in depth. The k-means is a partitional clustering algorithm.</p><p>Let the set of data points (or instances) be as follows:</p><p>
<span class="emphasis"><em>D = {x</em></span><sub>1</sub><span class="emphasis"><em>, x</em></span><sub>2</sub><span class="emphasis"><em>, …, x</em></span><sub>n</sub><span class="emphasis"><em>}</em></span>, where</p><p>
<span class="emphasis"><em>xi = (xi</em></span><sub>1</sub><span class="emphasis"><em>, xi</em></span><sub>2</sub><span class="emphasis"><em>, …, xi</em></span><sub>r</sub><span class="emphasis"><em>)</em></span>, is a vector in a real-valued space <span class="emphasis"><em>X ⊆ R</em></span><sub>r,</sub> and <span class="emphasis"><em>r</em></span> is the number of attributes in the data.</p><p>The k-means algorithm partitions the given data into <span class="emphasis"><em>k</em></span> clusters with each cluster having a center called a centroid.</p><p>
<span class="emphasis"><em>k</em></span> is specified by the user.</p><p>Given <span class="emphasis"><em>k</em></span>, the k-means algorithm works as follows:</p><p>Algorithm k-means (<span class="emphasis"><em>k</em></span>, <span class="emphasis"><em>D</em></span>)</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Identify the <span class="emphasis"><em>k</em></span> data points as the initial centroids (cluster centers).</li><li class="listitem">Repeat step 1.</li><li class="listitem">For each data point <span class="emphasis"><em>x ϵ D</em></span> do.</li><li class="listitem">Compute the distance from <span class="emphasis"><em>x</em></span> to the centroid.</li><li class="listitem">Assign <span class="emphasis"><em>x</em></span> to the closest centroid (a centroid represents a cluster).</li><li class="listitem">endfor</li><li class="listitem">Re-compute the centroids using the current cluster memberships until the stopping criterion is met.</li></ol></div><div class="section" title="Convergence or stopping criteria for the k-means clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec95"/>Convergence or stopping criteria for the k-means clustering</h2></div></div></div><p>The following list describes the<a id="id888" class="indexterm"/> convergence criteria for the k-means clustering algorithm:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There are zero or minimum number of reassignments for the data points to different clusters</li><li class="listitem" style="list-style-type: disc">There are zero or minimum changes of centroids</li><li class="listitem" style="list-style-type: disc">Otherwise, the decrease in the <a id="id889" class="indexterm"/><span class="strong"><strong>sum of squared error of prediction</strong></span> (<span class="strong"><strong>SSE</strong></span>) is minimum</li></ul></div><p>If <span class="emphasis"><em>C<sub>j</sub></em></span> is the <span class="emphasis"><em>j</em></span><sup>th</sup> cluster, then <span class="emphasis"><em>m</em></span><sub>j</sub> is the centroid of cluster <span class="emphasis"><em>C</em></span><sub>j</sub> (the mean vector of all the data points in <span class="emphasis"><em>C</em></span><sub>j</sub>), and if <span class="emphasis"><em>dist(x, m</em></span><sub>j</sub><span class="emphasis"><em>)</em></span> is the distance between the data point <span class="emphasis"><em>x</em></span> and centroid <span class="emphasis"><em>m</em></span><sub>j</sub> then the following example demonstrated using graphical representation explains the convergence criteria.</p><p>For example:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Identification of random k centers:<div class="mediaobject"><img src="graphics/B03980_08_08.jpg" alt="Convergence or stopping criteria for the k-means clustering"/></div></li><li class="listitem">Iteration 1: Compute centroids and assign the clusters:<div class="mediaobject"><img src="graphics/B03980_08_09.jpg" alt="Convergence or stopping criteria for the k-means clustering"/></div></li><li class="listitem">Iteration 2: Recompute centroids <a id="id890" class="indexterm"/>and reassign the clusters:<div class="mediaobject"><img src="graphics/B03980_08_10.jpg" alt="Convergence or stopping criteria for the k-means clustering"/></div></li><li class="listitem">Iteration 3: Recompute centroids and reassign the clusters:<div class="mediaobject"><img src="graphics/B03980_08_11.jpg" alt="Convergence or stopping criteria for the k-means clustering"/></div></li><li class="listitem">Terminate the <a id="id891" class="indexterm"/>process due to minimal changes to centroids or cluster reassignments.</li></ol></div><div class="section" title="K-means clustering on disk"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec93"/>K-means clustering on disk</h3></div></div></div><p>The k-means clustering algorithm<a id="id892" class="indexterm"/> can also be <a id="id893" class="indexterm"/>implemented with data on disk. This approach is used with large datasets that cannot be accommodated in memory. The strategy used here is to compute centroids incrementally by scanning the dataset only once for each iteration. The performance of this algorithm is determined by how well the number of iterations can be controlled. It is recommended that a limited set of iterations, less than 50 should be run. Although this version helps scaling, it is not the best algorithm for scaling up; there are other alternative clustering algorithms that scale-up, for example, BIRCH is one of them. The following algorithm describes the steps in disk the k-means algorithm:</p><p>Algorithm disk k-means (<span class="emphasis"><em>k</em></span>, <span class="emphasis"><em>D</em></span>)</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Choose the <span class="emphasis"><em>k</em></span> data points as the initial centroids (cluster centers) <span class="emphasis"><em>m</em></span><sub>j</sub>, where <span class="emphasis"><em>j = 1,2,3….k</em></span>.</li><li class="listitem">Repeat</li><li class="listitem">Initialize <span class="emphasis"><em>s</em></span><sub>j</sub><span class="emphasis"><em>=0</em></span>, where <span class="emphasis"><em>j=1,2,3….k</em></span>; (a vector with all the zero values).</li><li class="listitem">Initialize <span class="emphasis"><em>n</em></span><sub>j</sub><span class="emphasis"><em>=0</em></span>, where <span class="emphasis"><em>j=1,2,3….k</em></span>; (n<sub>j</sub> is number points in the cluster),</li><li class="listitem">For each data point <span class="emphasis"><em>x ϵ D</em></span> do.</li><li class="listitem"><span class="emphasis"><em>j = arg min dist(x, m</em></span><sub>j</sub><span class="emphasis"><em>)</em></span>.</li><li class="listitem">Assign <span class="emphasis"><em>x</em></span> to the cluster <span class="emphasis"><em>j</em></span>.</li><li class="listitem"><span class="emphasis"><em>s</em></span><sub>j</sub><span class="emphasis"><em> = s</em></span><sub>j</sub><span class="emphasis"><em> + x</em></span>.</li><li class="listitem"><span class="emphasis"><em>n</em></span><sub>j</sub><span class="emphasis"><em> = n</em></span><sub>j</sub><span class="emphasis"><em> + 1</em></span>.</li><li class="listitem">endfor.</li><li class="listitem"><span class="emphasis"><em>m</em></span><sub>i</sub><span class="emphasis"><em> = s</em></span><sub>j</sub><span class="emphasis"><em>/n</em></span><sub>j</sub>, where <span class="emphasis"><em>i=1,2,…k.</em></span></li><li class="listitem">Until the stopping, the criterion is met.</li></ol></div></div></div><div class="section" title="Advantages of the k-means approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec96"/>Advantages of the k-means approach</h2></div></div></div><p>The k-means way <a id="id894" class="indexterm"/>of unsupervised learning has many benefits; some of them are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The k-means clustering is popular and widely adopted due to its simplicity and ease of implementation.</li></ul></div><p>It is efficient and has <a id="id895" class="indexterm"/>optimal time complexity defined by <span class="emphasis"><em>O</em></span>(<span class="emphasis"><em>ikn</em></span>), where <span class="emphasis"><em>n</em></span> is the number of data points, <span class="emphasis"><em>k</em></span> is the number of clusters, and <span class="emphasis"><em>i</em></span> is the number of iterations. Since the <span class="emphasis"><em>l</em></span> and <span class="emphasis"><em>k</em></span> values are kept small, the k-means clustering can represent a linear expression too.</p></div><div class="section" title="Disadvantages of the k-means algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec97"/>Disadvantages of the k-means algorithm</h2></div></div></div><p>The following are the downsides or<a id="id896" class="indexterm"/> disadvantages of the k-means algorithm:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The value of <span class="emphasis"><em>k</em></span> is always a user input and is as good as the identified number <span class="emphasis"><em>k.</em></span></li><li class="listitem" style="list-style-type: disc">This algorithm is applicable only when the means are available, and in the case of categorical data the centroids are none other than the frequent values.</li><li class="listitem" style="list-style-type: disc">Clusters can never be elliptical and are always hyperspherical.</li><li class="listitem" style="list-style-type: disc">The clusters identified are very sensitive to the initially identified seeds, and can be different when to run multiple times with different random seeds involved. The following figure depicts how two different centroids can change the clusters. This can be achieved by iterative processing:<div class="mediaobject"><img src="graphics/B03980_08_12.jpg" alt="Disadvantages of the k-means algorithm"/></div></li><li class="listitem" style="list-style-type: disc">Again, k-means is very sensitive to outliers. Outliers can be the errors in the data recording or some special data points with very different values. The following diagram depicts the skew that an outlier can bring into the cluster formation. The first representation shows the ideal cluster, and the second one shows the undesirable cluster:<div class="mediaobject"><img src="graphics/B03980_08_13.jpg" alt="Disadvantages of the k-means algorithm"/></div><div class="mediaobject"><img src="graphics/B03980_08_14.jpg" alt="Disadvantages of the k-means algorithm"/></div></li></ul></div><p>Many of the algorithms<a id="id897" class="indexterm"/> and learning techniques that we have seen until now are sensitive to outliers. There are some standard techniques that can be employed.</p><p>One way is to get the outliers filtered from evaluation, and this requires us to apply some techniques to handle the noise in the data. The noise reduction techniques will be covered in the next chapters. In the case of k-means clustering, the removal of outliers can be done after a few iterations just to make sure the identified data points are really the outliers. Or, another way is to stick to a smaller sample of data on which the algorithm will be run. This way, the possibility of choosing an outlier will be minimal.</p></div><div class="section" title="Distance measures"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec98"/>Distance measures</h2></div></div></div><p>The distance measure is<a id="id898" class="indexterm"/> important in clustering algorithms. Reassigning data points to the clusters is determined by redefining the centroids. The following are some ways of measuring distance between two clusters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Single link</strong></span>: This <a id="id899" class="indexterm"/>method refers to measuring the distance between the two closest data points that belong to two different clusters. There can be noise in the data that might be considered with seriousness too.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Complete link</strong></span>: This<a id="id900" class="indexterm"/> method refers to measuring the distance between two farthest data points that belong to two different clusters. This method can make the clusters more sensitive to outliers.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Average link</strong></span>: This <a id="id901" class="indexterm"/>method uses the average distance measure of all the pairs of distances between the two clusters.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Centroids</strong></span>: This method <a id="id902" class="indexterm"/>refers to measuring the distance between the two clusters by measuring the distance between their centroids.</li></ul></div></div><div class="section" title="Complexity measures"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec99"/>Complexity measures</h2></div></div></div><p>Choosing the best clustering <a id="id903" class="indexterm"/>algorithm has always been a <a id="id904" class="indexterm"/>challenge. There are many algorithms available and both, accuracy and complexity measures are important for choosing the right algorithm. The single link method can help achieve <span class="emphasis"><em>O(n2)</em></span>; complete and average links can be done in <span class="emphasis"><em>O(n2logn)</em></span>. There are both advantages and limitations for each of the algorithms, and they work well in certain contexts of data distribution; no standard patterns in the data distribution make it a complex problem to solve. Hence, data preparation and standardization becomes an important aspect in Machine learning. Which distance measure would be an ideal choice can only be determined by implementing the different distance measures iteratively, and comparing the results across iterations. The clustering methods overall are highly dependent on the initial choices and can be subjective.</p></div></div>
<div class="section" title="Implementing k-means clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec43"/>Implementing k-means clustering</h1></div></div></div><p>Refer to the source code <a id="id905" class="indexterm"/>provided for this chapter for implementing the k-means clustering methods (only supervised learning techniques - source code path <code class="literal">.../chapter08/...</code> under each of the folders for the technology).</p><div class="section" title="Using Mahout"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec100"/>Using Mahout</h2></div></div></div><p>Refer <a id="id906" class="indexterm"/>to the <a id="id907" class="indexterm"/>folder <code class="literal">.../mahout/chapter8/k-meansexample/</code>.</p></div><div class="section" title="Using R"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec101"/>Using R</h2></div></div></div><p>Refer to<a id="id908" class="indexterm"/> the <a id="id909" class="indexterm"/>folder <code class="literal">.../r/chapter8/k-meansexample/</code>.</p></div><div class="section" title="Using Spark"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec102"/>Using Spark</h2></div></div></div><p>Refer to<a id="id910" class="indexterm"/> the <a id="id911" class="indexterm"/>folder <code class="literal">.../spark/chapter8/k-meansexample/</code>.</p></div><div class="section" title="Using Python (scikit-learn)"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec103"/>Using Python (scikit-learn)</h2></div></div></div><p>Refer to<a id="id912" class="indexterm"/> the <a id="id913" class="indexterm"/>folder <code class="literal">.../python-scikit-learn/chapter8/k-meansexample/</code>.</p></div><div class="section" title="Using Julia"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec104"/>Using Julia</h2></div></div></div><p>Refer to <a id="id914" class="indexterm"/>the <a id="id915" class="indexterm"/>folder <code class="literal">.../julia/chapter8/k-meansexample/</code>.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec44"/>Summary</h1></div></div></div><p>In this chapter, we have covered the clustering-based learning methods. We have taken a deep dive into the k-means clustering algorithm using an example. You have learned to implement k-means clustering using Mahout, R, Python, Julia, and Spark. In the next chapter, we will cover the Bayesian methods and in specific, the Naïve-Bayes algorithm.</p></div></body></html>