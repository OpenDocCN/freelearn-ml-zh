- en: Section 5 â€“ Clustering and Dimensionality Reduction with Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last two chapters of this book examines unsupervised learning models. These
    are models where there is no target to predict. Even without a target there are
    many insights that can be gleaned from our data. Dimension reduction with principal
    component analysis (PCA) allows us to capture the variance of our features with
    fewer components than the original number of features.
  prefs: []
  type: TYPE_NORMAL
- en: The components created with PCA can be used for visualizations, or to identify
    processes that are important but cannot really be captured well by each feature.
    PCA can also be used when we need to reduce the feature space in a supervised
    learning model. We will demonstrate how to create and evaluate a PCA in the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering helps us group instances by those which have more in common with
    each other than with those in any other group. This often reveals relationships
    that are not otherwise obvious. We look at two popular clustering algorithms,
    K-means and DBSCAN, in this chapter. Clustering works well when we are able to
    find the right hyperparameter values for our model -- the number of clusters (k)
    for k-means, and the value of epsilon for DBSCAN, which determines the size of
    the radius around core instances in a cluster. We will go over choosing the best
    hyperparameter values for these clustering algorithms in the final chapter of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B17978_15_ePub.xhtml#_idTextAnchor170), *Principal Component
    Analysis*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B17978_16_ePub.xhtml#_idTextAnchor177), *K-Means and DBSCAN
    Clustering*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
