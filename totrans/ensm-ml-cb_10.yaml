- en: Heterogeneous Ensemble Classifiers Using H2O
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipe:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Predicting credit card defaulters using heterogeneous ensemble classifiers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll showcase how to build heterogeneous ensemble classifier
    using H2O, which is an open source, distributed, in-memory, machine learning platform. There
    are a host of supervised and unsupervised algorithms available in H2O.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Among the supervised algorithms, H2O provides us with neural networks, random
    forest (RF), generalized linear models, a Gradient-Boosting Machine, a naive Bayes
    classifier, and XGBoost.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: H2O also provides us with a stacked ensemble method that aims to find the optimal
    combination of a collection of predictive algorithms using the stacking process.
    H2O's stacked ensemble supports both regression and classification.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Predicting credit card defaulters using heterogeneous ensemble classifiers
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use Taiwan's credit card payment defaulters data as an example. This
    is the same dataset we used earlier, in [Chapter 3](6a5a73fc-dba9-4903-a54a-6c79a8ee57b4.xhtml),
    *Resampling Methods*, to build a logistic regression model. In this recipe, we'll
    build multiple models using different algorithms, and finally, build a stacked
    ensemble model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This dataset contains information about credit card clients in Taiwan. This
    includes information to do with payment defaulters, customers' demographic factors,
    their credit data, and their payment history. The dataset is provided in GitHub.
    It is also available from its main source, the UCI ML Repository:[ https://bit.ly/2EZX6IC](https://bit.ly/2EZX6IC).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we''ll use the following supervised algorithms from H2O to
    build our models:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed random forest
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-boosting machine
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacked ensemble
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll see how to use these algorithms in Python and learn how to set some of
    the hyperparameters for each of the algorithms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll use Google Colab to build our model. In [Chapter 10](0d0517ac-d372-478f-ba6a-4ad4828b81a0.xhtml),
    *Heterogeneous Ensemble Classifiers Using H2O**,* we explained how to use Google
    Colaboratory in the *There's more* section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by installing H2O in Google Colab as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Executing the preceding command will show you a few instructions, with the
    final line showing the following message (the version number of H2O will be different
    depending on the latest version available):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We import all the required libraries, as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We''ll then initialize H2O:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Upon successful initialization, we''ll see the information shown in the following screenshot.
    This information might be different, depending on the environment:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/882ed571-c8ba-4490-a404-f16e26aabade.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: 'We''ll read our dataset from Google Drive. In order to do this, we first need
    to mount the drive:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It will instruct you to go to a URL to get the authorization code. You''ll
    need to click on the URL, copy the authorization code, and paste it. Upon successful
    mounting, you can read your file from the respective folder in Google Drive:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that with `h2o.import_file`, we create `h2o.frame.H2OFrame`. This is similar
    to a `pandas` DataFrame. However, in the case of a `pandas` DataFrame, the data
    is held in the memory, while in this case, the data is located on an H2O cluster.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'You can run similar methods on an H2O DataFrame as you can on pandas. For example,
    in order to see the first 10 observations in the DataFrame, you can use the following
    command:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To check the dimensions of the DataFrame, we use the following command:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In order to see all the column names, we run the following syntax:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the `pandas` DataFrame, we used `dtypes` to see the datatypes of each column.
    In the H2o DataFrame, we would use the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This gives us the following output. Note that the categorical variables appear
    as `''enum''`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/937331c5-ec80-4367-add7-ab7fd1c2be27.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'We have our target variable, `default.payment.next.month`, in the dataset.
    This tells us which customers have and have not defaulted on their payments. We
    want to see the distribution of the defaulters and non-defaulters:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives us the count of each class in the `default.payment.next.month` variable:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a2cdc5f-076e-4b13-bf8e-c0ef7bc4b80d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'We don''t need the `ID` column for predictive modeling, so we remove it from
    our DataFrame:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can see the distribution of the numeric variables using the `hist()` method:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following screenshot shows us the plotted variables . This can help us
    in our analysis of each of the variables:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23b6773e-bf76-460b-9529-95a123315127.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: 'To extend our analysis, we can see the distribution of defaulters and non-defaulters
    by gender, education, and marital status:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the following screenshot, we get to see the distribution of defaulters by
    different categories:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bae3882-f108-4d74-8219-38c232dbe93f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: 'We''ll now convert the categorical variables into factors:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We also encode the dichotomous target variable, `default.payment.next.month`,
    as a factor variable. After the conversion, we check the classes of the target
    variable with the `levels()` method:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We''ll then define our predictor and target variables:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We then split our DataFrame using the `split_frame()` method:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following code gives us two split output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the following screenshot, we get to see the following two splits:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c3138ad-28b8-46f5-b7d6-9d1ef46803f4.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: 'We separate the splits into train and test subsets:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How to do it...
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s move on to training our models using the algorithms we mentioned earlier
    in this chapter. We''ll start by training our **generalized linear model** (**GLM**)
    models. We''ll build three GLM models:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: A GLM model with default values for the parameters
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GLM model with Lambda search (regularization)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GLM model with grid search
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we will start with training our models in the following section.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s train our first model:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`H2OGeneralizedLinearEstimator` fits a generalized linear model. It takes in
    a response variable and a set of predictor variables.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '`H2OGeneralizedLinearEstimator` can handle both regression and classification
    tasks. In the case of a regression problem, it returns an `H2ORegressionModel` subclass,
    while for classification, it returns an `H2OBinomialModel` subclass.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'We created predictor and target variables in the *Getting ready* section. Pass
    the predictor and target variables to the model:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Train the GLM model using the `lambda_search` parameter:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`lambda_search` helps the GLM to find an optimal regularization parameter,
    λ. The `lambda_search` parameter takes in a Boolean value. When set to `True`,
    the GLM will first fit a model with the highest lambda value, which is known as
    **maximum regularization**. It then decreases this at each step until it reaches
    the minimum lambda. The resulting optimum model is based on the best lambda value.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the model using the GLM with a grid search:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We get the grid result sorted by the `auc` value with the `get_grid()` method:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the following screenshot, we can see the `auc` score for each model, which
    consists of different combinations of the `alpha` and `lambda` parameters:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4209530-ce0e-440d-ab7a-85f54f11e2e6.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: 'We can see the model metrics on our train data and our cross-validation data:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: From the preceding code block, you can evaluate the model metrics, which include
    `MSE`, `RMSE`, `Null` and `Residual Deviance`, `AUC`, and `Gini`, along with the
    `Confusion Matrix`. At a later stage, we will use the best model from the grid
    search for our stacked ensemble.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at the following image and evaluate the model metrics:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2e1b50c-c083-4d8c-a17b-37ff57893a35.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'Train the model using random forest. The code for random forest using default
    settings looks as follows:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To get the summary output of the model, use the following code:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Train the random forest model using a grid search. Set the hyperparameters
    as shown in the following code block:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Use the hyperparameters on `H2OGridSearch()` to train the `RF` model using
    `gridsearch`:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Sort the results by AUC score to see which model performs best:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Extract the best model from the grid search result:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In the following screenshot, we see the model metrics for the grid model on
    the train data and the cross-validation data:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/960c92f8-db18-427d-a08a-ee1a3750b8d7.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Train the model using GBM. Here''s how to train a GBM with the default settings:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Use a grid search on the GBM. To perform a grid search, set the hyperparameters
    as follows:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Use the hyperparameters on `H2OGridSearch()` to train the GBM model using grid
    search:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As with the earlier models, we can view the results sorted by AUC:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Extract the best model from the grid search:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We can use `H2OStackedEnsembleEstimator` to build a stacked ensemble ML model
    that can use the models we have built using H2O algorithms to improve the predictive
    performance. `H2OStackedEnsembleEstimator` helps us find the optimal combination
    of a collection of predictive algorithms.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a list of the best models from the earlier models that we built using
    grid search:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Set up a stacked ensemble model using `H2OStackedEnsembleEstimator`:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Evaluate the ensemble performance on the test data:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Compare the performance of the base learners on the `test` data. The following
    code tests the model performance of all the GLM models we''ve built:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following code tests the model performance of all the random forest models
    we''ve built:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following code tests the model performance of all the GBM models we''ve
    built:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To get the best AUC from the base learners, execute the following commands:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The following commands show the AUC from the stacked ensemble model:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: How it works...
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used Google Colab to train our models. After we installed H2O in Google Colab,
    we initialized the H2O instance. We also imported the required libraries.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: In order to use the H2O libraries, we imported `H2OGeneralizedLinearEstimator`,
    `H2ORandomForestEstimator`, and `H2OGradientBoostingEstimator` from `h2o.estimators`.
    We also imported `H2OStackedEnsembleEstimator` to train our model using a stacked
    ensemble.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: We mounted Google Drive and read our dataset using `h2o.import_file()`. This
    created an H2O DataFrame, which is very similar to a `pandas` DataFrame. Instead
    of holding it in the memory, however, the data is located in one of the remote
    H2O clusters.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: We then performed basic operations on the H2O DataFrame to analyze our data.
    We took a look at the dimensions, the top few rows, and the data types of each
    column. The `shape` attribute returned a tuple with the number of rows and columns.
    The `head()` method returned the top 10 observations. The `types` attribute returned
    the data types of each column.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Note that a categorical variable in an H2O DataFrame is marked as an `enum`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Our target variable was `default.payment.next.month`. With the `table()` method,
    we saw the distribution of both classes of our target variable. The `table()`
    method returned the count for classes `1` and `0` in this case.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: We didn't need the `ID` column, so we removed it using the `drop()` method with `axis=1` as
    a parameter. With `axis=1`, it dropped the columns. Otherwise, the default value
    of `axis=0` would have dropped the labels from the index.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: We analyzed the distribution of the numeric variables. There's no limit to how
    far you can explore your data. We also saw the distribution of both of the classes
    of our target variable by various categories, such as gender, education, and marriage.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: We then converted the categorical variables to factor type with the `asfactor()`
    method. This was done for the target variable as well.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: We created a list of predictor variables and target variables. We split our
    DataFrame into the train and test subsets with the `split_frame()` method.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: We passed ratios to the `split_frame()` method. In our case, we split the dataset
    into 70% and 30%. However, note that this didn't give an exact split of 70%-30%.
    H2O uses a probabilistic splitting method instead of using the exact ratios to
    split the dataset. This is to make the split more efficient on big data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: After we split our datasets into train and test subsets, we moved onto training
    our models. We used GLM, random forest, a **gradient-boosting machine** (**GBM**),
    and stacked ensembles to train the stacking model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: In the *How to do it...* section, in *Step 1* and *Step 2*, we showcased the
    code to train a GLM model with the default settings. We used cross-validation
    to train our model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, we trained a GLM model with `lambda_search`, which helps to find
    the optimal regularization parameter.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we used grid-search parameters to train our GLM model. We set our
    hyper-parameters and provided these to the `H2OGridSearch()` method. This helps
    us search for the optimum parameters across models. In the `H2OGridSearch()` method,
    we used the `RandomDiscrete` search-criteria strategy.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The default search-criteria strategy is Cartesian, which covers the entire space
    of hyperparameter combinations. The random discrete strategy carries out a random
    search of all the combinations of the hyperparameters provided.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, with the `get_grid()` method, we looked at the AUC score of each
    model built with different combinations of the parameters provided. In *Step 6*,
    we extracted the `best` model from the random grid search. We can also use the
    `print()` method on the best model to see the model performance metrics on both
    the train data and the cross-validation data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 7*, we trained a random forest model with default settings and looked
    at the summary of the resulting model in step 8\. In *Step 9* and *Step 10*, we
    showcased the code to train a random forest model using grid-search. We set multiple
    values for various acceptable hyper-parameters, such as `sample_rate`, `col_sample_rate_per_tree`,
    `max_depth`, and `ntrees`. `sample_rate` refers to row sampling without replacement.
    It takes a value between `0` and `1`, indicating the sampling percentage of the
    data. `col_sample_rate_per_tree` is the column sampling for each tree without
    replacement. `max_depth` is set to specify the maximum depth to which each tree
    should be built. Deeper trees may perform better on the training data but will
    take more computing time and may overfit and fail to generalize on unseen data. The
    `ntrees` parameter is used for tree-based algorithms to specify the number of
    trees to build on the model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 11* and *Step 12*, we printed the AUC score of each model generated
    by the grid-search and extracted the best model from it.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: We also trained GBM models to fit our data. In *Step 13*, we built the GBM using
    the default settings. In *Step 14*, we set the hyperparameter space for the grid
    search. We used this in *Step 15*, where we trained our GBM. In the GBM, we set
    values for hyperparameters, such as `learn_rate`, `sample_rate`, `col_sample_rate`,
    `max_depth`, and `ntrees`. The `learn_rate` parameter is used to specify the rate
    at which the GBM algorithm trains the model. A lower value for the `learn_rate`
    parameter is better and can help in avoiding overfitting, but can be costly in
    terms of computing time.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: In H2O, `learn_rate` is available in GBM and XGBoost.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 16* showed us the AUC score of each resulting model from the grid search.
    We extracted the best grid-searched GBM in *Step 17*.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 18* through to *Step 20*, we trained our stacked ensemble model using `H2OStackedEnsembleEstimator`
    from H2O. We evaluated the performance of the resulting model on the test data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In S*tep 21*, we evaluated all the GLM models we built on our test data. We
    did the same with all the models we trained using RF and GBM. *Step 22* gave us
    the model with the maximum AUC score. In *Step 23*, we evaluated the AUC score
    of the stacked ensemble model on the test data in order to compare the performance
    of the stacked ensemble model with the individual base learners.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that we used cross-validation to train all our models. We used the `nfolds`
    option to set the number of folds to use for cross-validation. In our example,
    we used `nfolds=5`, but we can also set it to higher numbers.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The number of folds needs to be the same across every models you build.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: With a value for `nfolds` specified, we can also provide a value for the `fold_assignment`
    parameters. `fold_assignment` takes values such as `auto`, `random`, `modulo`,
    and `stratified`. If we set it to `Auto`, the algorithm automatically chooses
    an option; currently, it chooses `Random`. With `fold_assignment` set to `Random`,
    it will enable a random split of the data into `nfolds` sets. When `fold_assignment` is
    set to `Modulo`, it uses a deterministic method to evenly split the data into
    `nfolds` that don't depend on the `seed` parameter.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: When we use cross-validation method to build models, ensure that you specify
    a `seed` value for all models or use `fold_assignment="Modulo"`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'In grid search, we used two parameters: `stopping_metric` and `stopping_rounds`.
    These parameters are available for GBM and random forest algorithms, but they
    aren''t available for GLM. `stopping_metric` specifies the metric to consider
    when early stopping is specified, which can be done by setting `stopping_rounds`
    to a value greater than zero.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: In our examples, we set `stopping_metric` to AUC and `stopping_rounds` to five.
    This means that the algorithm will measure the AUC before it stops training any
    further if the AUC doesn't improve in the specified number of rounds, which is
    five in our case.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: If `stopping_metric` is specified, `stopping_rounds` must be set as well. When `stopping_tolerance` is
    also set, the model will stop training after reaching the number of rounds mentioned
    in `stopping_rounds` if the model's `stopping_metric` doesn't improve by the `stopping_tolerance`
    value.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指定了`stopping_metric`，则必须设置`stopping_rounds`。当也设置了`stopping_tolerance`时，如果模型的`stopping_metric`没有通过`stopping_tolerance`值改善，模型将在达到`stopping_rounds`中提到的轮数后停止训练。
- en: See also
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: The H2O documentation is available at [http://docs.h2o.ai/](http://docs.h2o.ai/).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: H2O文档可在[http://docs.h2o.ai/](http://docs.h2o.ai/)找到。
