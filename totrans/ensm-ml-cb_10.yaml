- en: Heterogeneous Ensemble Classifiers Using H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting credit card defaulters using heterogeneous ensemble classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll showcase how to build heterogeneous ensemble classifier
    using H2O, which is an open source, distributed, in-memory, machine learning platform. There
    are a host of supervised and unsupervised algorithms available in H2O.
  prefs: []
  type: TYPE_NORMAL
- en: Among the supervised algorithms, H2O provides us with neural networks, random
    forest (RF), generalized linear models, a Gradient-Boosting Machine, a naive Bayes
    classifier, and XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: H2O also provides us with a stacked ensemble method that aims to find the optimal
    combination of a collection of predictive algorithms using the stacking process.
    H2O's stacked ensemble supports both regression and classification.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting credit card defaulters using heterogeneous ensemble classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use Taiwan's credit card payment defaulters data as an example. This
    is the same dataset we used earlier, in [Chapter 3](6a5a73fc-dba9-4903-a54a-6c79a8ee57b4.xhtml),
    *Resampling Methods*, to build a logistic regression model. In this recipe, we'll
    build multiple models using different algorithms, and finally, build a stacked
    ensemble model.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset contains information about credit card clients in Taiwan. This
    includes information to do with payment defaulters, customers' demographic factors,
    their credit data, and their payment history. The dataset is provided in GitHub.
    It is also available from its main source, the UCI ML Repository:[ https://bit.ly/2EZX6IC](https://bit.ly/2EZX6IC).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we''ll use the following supervised algorithms from H2O to
    build our models:'
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-boosting machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacked ensemble
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll see how to use these algorithms in Python and learn how to set some of
    the hyperparameters for each of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll use Google Colab to build our model. In [Chapter 10](0d0517ac-d372-478f-ba6a-4ad4828b81a0.xhtml),
    *Heterogeneous Ensemble Classifiers Using H2O**,* we explained how to use Google
    Colaboratory in the *There's more* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by installing H2O in Google Colab as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the preceding command will show you a few instructions, with the
    final line showing the following message (the version number of H2O will be different
    depending on the latest version available):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We import all the required libraries, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll then initialize H2O:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon successful initialization, we''ll see the information shown in the following screenshot.
    This information might be different, depending on the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/882ed571-c8ba-4490-a404-f16e26aabade.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll read our dataset from Google Drive. In order to do this, we first need
    to mount the drive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It will instruct you to go to a URL to get the authorization code. You''ll
    need to click on the URL, copy the authorization code, and paste it. Upon successful
    mounting, you can read your file from the respective folder in Google Drive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that with `h2o.import_file`, we create `h2o.frame.H2OFrame`. This is similar
    to a `pandas` DataFrame. However, in the case of a `pandas` DataFrame, the data
    is held in the memory, while in this case, the data is located on an H2O cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can run similar methods on an H2O DataFrame as you can on pandas. For example,
    in order to see the first 10 observations in the DataFrame, you can use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To check the dimensions of the DataFrame, we use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to see all the column names, we run the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `pandas` DataFrame, we used `dtypes` to see the datatypes of each column.
    In the H2o DataFrame, we would use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output. Note that the categorical variables appear
    as `''enum''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/937331c5-ec80-4367-add7-ab7fd1c2be27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have our target variable, `default.payment.next.month`, in the dataset.
    This tells us which customers have and have not defaulted on their payments. We
    want to see the distribution of the defaulters and non-defaulters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the count of each class in the `default.payment.next.month` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a2cdc5f-076e-4b13-bf8e-c0ef7bc4b80d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We don''t need the `ID` column for predictive modeling, so we remove it from
    our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the distribution of the numeric variables using the `hist()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows us the plotted variables . This can help us
    in our analysis of each of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23b6773e-bf76-460b-9529-95a123315127.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To extend our analysis, we can see the distribution of defaulters and non-defaulters
    by gender, education, and marital status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we get to see the distribution of defaulters by
    different categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bae3882-f108-4d74-8219-38c232dbe93f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll now convert the categorical variables into factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We also encode the dichotomous target variable, `default.payment.next.month`,
    as a factor variable. After the conversion, we check the classes of the target
    variable with the `levels()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll then define our predictor and target variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We then split our DataFrame using the `split_frame()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code gives us two split output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we get to see the following two splits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c3138ad-28b8-46f5-b7d6-9d1ef46803f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We separate the splits into train and test subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s move on to training our models using the algorithms we mentioned earlier
    in this chapter. We''ll start by training our **generalized linear model** (**GLM**)
    models. We''ll build three GLM models:'
  prefs: []
  type: TYPE_NORMAL
- en: A GLM model with default values for the parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GLM model with Lambda search (regularization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GLM model with grid search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we will start with training our models in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s train our first model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`H2OGeneralizedLinearEstimator` fits a generalized linear model. It takes in
    a response variable and a set of predictor variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '`H2OGeneralizedLinearEstimator` can handle both regression and classification
    tasks. In the case of a regression problem, it returns an `H2ORegressionModel` subclass,
    while for classification, it returns an `H2OBinomialModel` subclass.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We created predictor and target variables in the *Getting ready* section. Pass
    the predictor and target variables to the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the GLM model using the `lambda_search` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`lambda_search` helps the GLM to find an optimal regularization parameter,
    λ. The `lambda_search` parameter takes in a Boolean value. When set to `True`,
    the GLM will first fit a model with the highest lambda value, which is known as
    **maximum regularization**. It then decreases this at each step until it reaches
    the minimum lambda. The resulting optimum model is based on the best lambda value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the model using the GLM with a grid search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the grid result sorted by the `auc` value with the `get_grid()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we can see the `auc` score for each model, which
    consists of different combinations of the `alpha` and `lambda` parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4209530-ce0e-440d-ab7a-85f54f11e2e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see the model metrics on our train data and our cross-validation data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code block, you can evaluate the model metrics, which include
    `MSE`, `RMSE`, `Null` and `Residual Deviance`, `AUC`, and `Gini`, along with the
    `Confusion Matrix`. At a later stage, we will use the best model from the grid
    search for our stacked ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at the following image and evaluate the model metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2e1b50c-c083-4d8c-a17b-37ff57893a35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Train the model using random forest. The code for random forest using default
    settings looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the summary output of the model, use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the random forest model using a grid search. Set the hyperparameters
    as shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the hyperparameters on `H2OGridSearch()` to train the `RF` model using
    `gridsearch`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Sort the results by AUC score to see which model performs best:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the best model from the grid search result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we see the model metrics for the grid model on
    the train data and the cross-validation data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/960c92f8-db18-427d-a08a-ee1a3750b8d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Train the model using GBM. Here''s how to train a GBM with the default settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Use a grid search on the GBM. To perform a grid search, set the hyperparameters
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the hyperparameters on `H2OGridSearch()` to train the GBM model using grid
    search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'As with the earlier models, we can view the results sorted by AUC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the best model from the grid search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We can use `H2OStackedEnsembleEstimator` to build a stacked ensemble ML model
    that can use the models we have built using H2O algorithms to improve the predictive
    performance. `H2OStackedEnsembleEstimator` helps us find the optimal combination
    of a collection of predictive algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a list of the best models from the earlier models that we built using
    grid search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up a stacked ensemble model using `H2OStackedEnsembleEstimator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the ensemble performance on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare the performance of the base learners on the `test` data. The following
    code tests the model performance of all the GLM models we''ve built:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code tests the model performance of all the random forest models
    we''ve built:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code tests the model performance of all the GBM models we''ve
    built:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the best AUC from the base learners, execute the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The following commands show the AUC from the stacked ensemble model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used Google Colab to train our models. After we installed H2O in Google Colab,
    we initialized the H2O instance. We also imported the required libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In order to use the H2O libraries, we imported `H2OGeneralizedLinearEstimator`,
    `H2ORandomForestEstimator`, and `H2OGradientBoostingEstimator` from `h2o.estimators`.
    We also imported `H2OStackedEnsembleEstimator` to train our model using a stacked
    ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: We mounted Google Drive and read our dataset using `h2o.import_file()`. This
    created an H2O DataFrame, which is very similar to a `pandas` DataFrame. Instead
    of holding it in the memory, however, the data is located in one of the remote
    H2O clusters.
  prefs: []
  type: TYPE_NORMAL
- en: We then performed basic operations on the H2O DataFrame to analyze our data.
    We took a look at the dimensions, the top few rows, and the data types of each
    column. The `shape` attribute returned a tuple with the number of rows and columns.
    The `head()` method returned the top 10 observations. The `types` attribute returned
    the data types of each column.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a categorical variable in an H2O DataFrame is marked as an `enum`.
  prefs: []
  type: TYPE_NORMAL
- en: Our target variable was `default.payment.next.month`. With the `table()` method,
    we saw the distribution of both classes of our target variable. The `table()`
    method returned the count for classes `1` and `0` in this case.
  prefs: []
  type: TYPE_NORMAL
- en: We didn't need the `ID` column, so we removed it using the `drop()` method with `axis=1` as
    a parameter. With `axis=1`, it dropped the columns. Otherwise, the default value
    of `axis=0` would have dropped the labels from the index.
  prefs: []
  type: TYPE_NORMAL
- en: We analyzed the distribution of the numeric variables. There's no limit to how
    far you can explore your data. We also saw the distribution of both of the classes
    of our target variable by various categories, such as gender, education, and marriage.
  prefs: []
  type: TYPE_NORMAL
- en: We then converted the categorical variables to factor type with the `asfactor()`
    method. This was done for the target variable as well.
  prefs: []
  type: TYPE_NORMAL
- en: We created a list of predictor variables and target variables. We split our
    DataFrame into the train and test subsets with the `split_frame()` method.
  prefs: []
  type: TYPE_NORMAL
- en: We passed ratios to the `split_frame()` method. In our case, we split the dataset
    into 70% and 30%. However, note that this didn't give an exact split of 70%-30%.
    H2O uses a probabilistic splitting method instead of using the exact ratios to
    split the dataset. This is to make the split more efficient on big data.
  prefs: []
  type: TYPE_NORMAL
- en: After we split our datasets into train and test subsets, we moved onto training
    our models. We used GLM, random forest, a **gradient-boosting machine** (**GBM**),
    and stacked ensembles to train the stacking model.
  prefs: []
  type: TYPE_NORMAL
- en: In the *How to do it...* section, in *Step 1* and *Step 2*, we showcased the
    code to train a GLM model with the default settings. We used cross-validation
    to train our model.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, we trained a GLM model with `lambda_search`, which helps to find
    the optimal regularization parameter.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we used grid-search parameters to train our GLM model. We set our
    hyper-parameters and provided these to the `H2OGridSearch()` method. This helps
    us search for the optimum parameters across models. In the `H2OGridSearch()` method,
    we used the `RandomDiscrete` search-criteria strategy.
  prefs: []
  type: TYPE_NORMAL
- en: The default search-criteria strategy is Cartesian, which covers the entire space
    of hyperparameter combinations. The random discrete strategy carries out a random
    search of all the combinations of the hyperparameters provided.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, with the `get_grid()` method, we looked at the AUC score of each
    model built with different combinations of the parameters provided. In *Step 6*,
    we extracted the `best` model from the random grid search. We can also use the
    `print()` method on the best model to see the model performance metrics on both
    the train data and the cross-validation data.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 7*, we trained a random forest model with default settings and looked
    at the summary of the resulting model in step 8\. In *Step 9* and *Step 10*, we
    showcased the code to train a random forest model using grid-search. We set multiple
    values for various acceptable hyper-parameters, such as `sample_rate`, `col_sample_rate_per_tree`,
    `max_depth`, and `ntrees`. `sample_rate` refers to row sampling without replacement.
    It takes a value between `0` and `1`, indicating the sampling percentage of the
    data. `col_sample_rate_per_tree` is the column sampling for each tree without
    replacement. `max_depth` is set to specify the maximum depth to which each tree
    should be built. Deeper trees may perform better on the training data but will
    take more computing time and may overfit and fail to generalize on unseen data. The
    `ntrees` parameter is used for tree-based algorithms to specify the number of
    trees to build on the model.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 11* and *Step 12*, we printed the AUC score of each model generated
    by the grid-search and extracted the best model from it.
  prefs: []
  type: TYPE_NORMAL
- en: We also trained GBM models to fit our data. In *Step 13*, we built the GBM using
    the default settings. In *Step 14*, we set the hyperparameter space for the grid
    search. We used this in *Step 15*, where we trained our GBM. In the GBM, we set
    values for hyperparameters, such as `learn_rate`, `sample_rate`, `col_sample_rate`,
    `max_depth`, and `ntrees`. The `learn_rate` parameter is used to specify the rate
    at which the GBM algorithm trains the model. A lower value for the `learn_rate`
    parameter is better and can help in avoiding overfitting, but can be costly in
    terms of computing time.
  prefs: []
  type: TYPE_NORMAL
- en: In H2O, `learn_rate` is available in GBM and XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 16* showed us the AUC score of each resulting model from the grid search.
    We extracted the best grid-searched GBM in *Step 17*.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 18* through to *Step 20*, we trained our stacked ensemble model using `H2OStackedEnsembleEstimator`
    from H2O. We evaluated the performance of the resulting model on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: In S*tep 21*, we evaluated all the GLM models we built on our test data. We
    did the same with all the models we trained using RF and GBM. *Step 22* gave us
    the model with the maximum AUC score. In *Step 23*, we evaluated the AUC score
    of the stacked ensemble model on the test data in order to compare the performance
    of the stacked ensemble model with the individual base learners.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that we used cross-validation to train all our models. We used the `nfolds`
    option to set the number of folds to use for cross-validation. In our example,
    we used `nfolds=5`, but we can also set it to higher numbers.
  prefs: []
  type: TYPE_NORMAL
- en: The number of folds needs to be the same across every models you build.
  prefs: []
  type: TYPE_NORMAL
- en: With a value for `nfolds` specified, we can also provide a value for the `fold_assignment`
    parameters. `fold_assignment` takes values such as `auto`, `random`, `modulo`,
    and `stratified`. If we set it to `Auto`, the algorithm automatically chooses
    an option; currently, it chooses `Random`. With `fold_assignment` set to `Random`,
    it will enable a random split of the data into `nfolds` sets. When `fold_assignment` is
    set to `Modulo`, it uses a deterministic method to evenly split the data into
    `nfolds` that don't depend on the `seed` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: When we use cross-validation method to build models, ensure that you specify
    a `seed` value for all models or use `fold_assignment="Modulo"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In grid search, we used two parameters: `stopping_metric` and `stopping_rounds`.
    These parameters are available for GBM and random forest algorithms, but they
    aren''t available for GLM. `stopping_metric` specifies the metric to consider
    when early stopping is specified, which can be done by setting `stopping_rounds`
    to a value greater than zero.'
  prefs: []
  type: TYPE_NORMAL
- en: In our examples, we set `stopping_metric` to AUC and `stopping_rounds` to five.
    This means that the algorithm will measure the AUC before it stops training any
    further if the AUC doesn't improve in the specified number of rounds, which is
    five in our case.
  prefs: []
  type: TYPE_NORMAL
- en: If `stopping_metric` is specified, `stopping_rounds` must be set as well. When `stopping_tolerance` is
    also set, the model will stop training after reaching the number of rounds mentioned
    in `stopping_rounds` if the model's `stopping_metric` doesn't improve by the `stopping_tolerance`
    value.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The H2O documentation is available at [http://docs.h2o.ai/](http://docs.h2o.ai/).
  prefs: []
  type: TYPE_NORMAL
