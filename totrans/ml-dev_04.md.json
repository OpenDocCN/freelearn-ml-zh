["```py\nimport numpy as np from sklearn import datasets import seaborn.apionly as sns %matplotlib inline import matplotlib.pyplot as plt sns.set(style='whitegrid', context='notebook')\n```", "```py\niris2 = sns.load_dataset('iris')\n```", "```py\ndef covariance (X, Y):\n    xhat=np.mean(X)\n    yhat=np.mean(Y)\n    epsilon=0\n    for x,y in zip (X,Y):\n        epsilon=epsilon+(x-xhat)*(y-yhat)\n    return epsilon/(len(X)-1)\n```", "```py\nprint (covariance ([1,3,4], [1,0,2]))\nprint (np.cov([1,3,4], [1,0,2]))\n\n0.5\n[[ 2.33333333  0.5       ]\n [ 0.5         1\\.        ]]\n```", "```py\ndef correlation (X, Y):\n    return (covariance(X,Y)/(np.std(X,  ddof=1)*np.std(Y,  ddof=1))) ##We have to indicate ddof=1 the unbiased std\n```", "```py\nprint (correlation ([1,1,4,3], [1,0,2,2]))\nprint (np.corrcoef ([1,1,4,3], [1,0,2,2]))\n\n0.870388279778\n[[ 1\\.          0.87038828]\n [ 0.87038828  1\\.        ]]\n```", "```py\nsns.pairplot(iris2, size=3.0)\n<seaborn.axisgrid.PairGrid at 0x7f8a2a30e828>\n\n```", "```py\nX=iris2['petal_width']\nY=iris2['petal_length']\n```", "```py\nplt.scatter(X,Y)\n```", "```py\ndef predict(alpha, beta, x_i):\n    return beta * x_i + alpha\n```", "```py\ndef error(alpha, beta, x_i, y_i): #L1\n    return y_i - predict(alpha, beta, x_i)\n\ndef sum_sq_e(alpha, beta, x, y): #L2\n    return sum(error(alpha, beta, x_i, y_i) ** 2\n               for x_i, y_i in zip(x, y))\n```", "```py\ndef correlation_fit(x, y):\n    beta = correlation(x, y) * np.std(y, ddof=1) / np.std(x,ddof=1)\n    alpha = np.mean(y) - beta * np.mean(x)\n    return alpha, beta\n```", "```py\nalpha, beta = correlation_fit(X, Y)\nprint(alpha)\nprint(beta)\n\n1.08355803285\n2.22994049512\n```", "```py\nplt.scatter(X,Y)\nxr=np.arange(0,3.5)\nplt.plot(xr,(xr*beta)+alpha)\n```", "```py\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\nix=iris2['petal_width']\niy=iris2['petal_length']\n\n# generate points used to represent the fitted function \nx_plot = np.linspace(0, 2.6, 100)\n\n# create matrix versions of these arrays\nX = ix[:, np.newaxis]\nX_plot = x_plot[:, np.newaxis]\n\nplt.scatter(ix, iy, s=30, marker='o', label=\"training points\")\n\nfor count, degree in enumerate([3, 6, 20]):\n    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n    model.fit(X, iy)\n    y_plot = model.predict(X_plot)\n    plt.plot(x_plot, y_plot, label=\"degree %d\" % degree)\n\nplt.legend(loc='upper left')\nplt.show()\n```", "```py\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nsns.set(style='whitegrid', context='notebook')\n```", "```py\ndef least_squares(b0, b1, points):\n    totalError = 0\n    N=float(len(points))\n    for x,y in points:\n        totalError += (y - (b1 * x + b0)) ** 2\n    return totalError / 2.*N\n```", "```py\ndef step_gradient(b0_current, b1_current, points, learningRate):\n    b0_gradient = 0\n    b1_gradient = 0\n    N = float(len(points))\n    for x,y in points:\n        b0_gradient += (1/N) * (y - ((b1_current * x) + b0_current))\n        b1_gradient += (1/N) * x * (y - ((b1_current * x) + b0_current))\n    new_b0 = b0_current + (learningRate * b0_gradient)\n    new_b1 = b1_current + (learningRate * b1_gradient)\n    return [new_b0, new_b1, least_squares(new_b0, new_b1, points)]\n```", "```py\ndef run_gradient_descent(points, starting_b0, starting_b1, learning_rate, num_iterations):\n    b0 = starting_b0\n    b1 = starting_b1\n    slope=[]\n    intersect=[]\n    error=[]\n    for i in range(num_iterations):\n        b0, b1 , e= step_gradient(b0, b1, np.array(points), learning_rate)\n        slope.append(b1)\n        intersect.append(b0)\n        error.append(e)\n    return [b0, b1, e, slope, intersect,error]\n```", "```py\niris = sns.load_dataset('iris')\nX=iris['petal_width'].tolist()\nY=iris['petal_length'].tolist()\npoints=np.dstack((X,Y))[0]\n```", "```py\nlearning_rate = 0.0001\ninitial_b0 = 0 \ninitial_b1 = 0 \nnum_iterations = 1000\n[b0, b1, e, slope, intersect, error] = run_gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)\n\nplt.figure(figsize=(7,5))\nplt.scatter(X,Y)\nxr=np.arange(0,3.5)\nplt.plot(xr,(xr*b1)+b0);\nplt.title('Regression, alpha=0.001, initial values=(0,0), it=1000');\n```", "```py\nplt.figure(figsize=(7,5))\nxr=np.arange(0,1000)\nplt.plot(xr,np.array(error).transpose());\nplt.title('Error for 1000 iterations');\n```", "```py\nlearning_rate = 0.001 #Last one was 0.0001\ninitial_b0 = 0 \ninitial_b1 = 0 \nnum_iterations = 1000\n[b0, b1, e, slope, intersect, error] = run_gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)\nplt.figure(figsize=(7,5))\nxr=np.arange(0,1000)\nplt.plot(xr,np.array(error).transpose());\nplt.title('Error for 1000 iterations, increased step by tenfold');\n\n```", "```py\nplt.figure(figsize=(7,5))\nplt.scatter(X,Y)\nxr=np.arange(0,3.5)\nplt.plot(xr,(xr*b1)+b0);\nplt.title('Regression, alpha=0.01, initial values=(0,0), it=1000');\n```", "```py\nlearning_rate = 0.85 #LAst one was 0.0001\ninitial_b0 = 0 \ninitial_b1 = 0 \nnum_iterations = 1000\n[b0, b1, e, slope, intersect, error] = run_gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)\nplt.figure(figsize=(7,5))\nxr=np.arange(0,1000)\nplt.plot(xr,np.array(error).transpose());\nplt.title('Error for 1000 iterations, big step');\n```", "```py\nlearning_rate = 0.001 #Same as last time\ninitial_b0 = 0.8 #pseudo random value\ninitial_b1 = 1.5 #pseudo random value\nnum_iterations = 1000\n[b0, b1, e, slope, intersect, error] = run_gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)\nplt.figure(figsize=(7,5))\nxr=np.arange(0,1000)\nplt.plot(xr,np.array(error).transpose());\nplt.title('Error for 1000 iterations, step 0.001, random initial parameter values');\n```", "```py\nlearning_rate = 0.001 #Same as last time\ninitial_b0 = 0.8 #pseudo random value\ninitial_b1 = 1.5 #pseudo random value\nnum_iterations = 1000\nx_mean =np.mean(points[:,0])\ny_mean = np.mean(points[:,1])\nx_std = np.std(points[:,0])\ny_std = np.std(points[:,1])\n\nX_normalized = (points[:,0] - x_mean)/x_std\nY_normalized = (points[:,1] - y_mean)/y_std\n\nplt.figure(figsize=(7,5))\nplt.scatter(X_normalized,Y_normalized)\n\n<matplotlib.collections.PathCollection at 0x7f9cad8f4240>\n```", "```py\npoints=np.dstack((X_normalized,Y_normalized))[0]\nlearning_rate = 0.001 #Same as last time\ninitial_b0 = 0.8 #pseudo random value\ninitial_b1 = 1.5 #pseudo random value\nnum_iterations = 1000\n[b0, b1, e, slope, intersect, error] = run_gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)\nplt.figure(figsize=(7,5))\nxr=np.arange(0,1000)\n```", "```py\nplt.plot(xr,np.array(error).transpose());\nplt.title('Error for 1000 iterations, step 0.001, random initial parameter values, normalized initial values');\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn import linear_model\nimport seaborn.apionly as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nsns.set(style='whitegrid', context='notebook')\n```", "```py\ndf = pd.read_csv(\"data/CHD.csv\", header=0)\nplt.figure() # Create a new figure\nplt.axis ([0,70,-0.2,1.2])\nplt.title('Original data')\nplt.scatter(df['age'],df['chd']) #Plot a scatter draw of the random datapoints\n```", "```py\nlogistic = linear_model.LogisticRegression(C=1e5)\nlogistic.fit(df['age'].reshape(100,1),df['chd'].reshape(100,1))\n\nLogisticRegression(C=100000.0, class_weight=None, dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n```", "```py\nx_plot = np.linspace(10, 90, 100)\noneprob=[]\nzeroprob=[]\npredict=[]\nplt.figure(figsize=(10,10))\nfor i in x_plot:\n    oneprob.append (logistic.predict_proba(i)[0][1]);\n    zeroprob.append (logistic.predict_proba(i)[0][0]);\n    predict.append (logistic.predict(i)[0]);\n\nplt.plot(x_plot, oneprob);\nplt.plot(x_plot, zeroprob)\nplt.plot(x_plot, predict);\nplt.scatter(df['age'],df['chd'])\n```"]