- en: Scalable Computing for Data Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to use Azure in data science. We are going
    to look at how to prepare data, which includes cleaning and transforming it, creating
    engineering features, creating and training a machine learning model, and finally,
    making predictions using the machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: To build a machine learning algorithm with big data, we need to process a lot
    of data to train it. To do this, we need a lot of computing power. We also need
    the compute to be able to scale dynamically based on the load to serve these machine
    learning models at scale so that they can perform predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In the era before public clouds, we had to buy all of our hardware beforehand.
    We had to pay for it all, ...
  prefs: []
  type: TYPE_NORMAL
- en: Different scalable compute options in Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two types of scaling possible in Azure to help us scale our CPUs
    and/or GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertical scaling**: Increasing the number of CPU cores and the memory of
    the VM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal scaling**: Increasing the number of VMs in the cluster, while
    keeping the CPU cores and the memory the same in each virtual machine. When we
    use horizontal scaling, we may have to write additional code to perform parallel
    or distributed computing using a supported framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to DSVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A DSVM is a Linux or Windows container that is made up of popular data science
    tools and frameworks. It can help us quickly get started with data analytics and
    data science. It supports a number of OSes; currently, we can create a DSVM with
    Windows 2012, Windows 2016, Ubuntu Linux, and CentOS Linux. The idea behind DSVMs
    is to decrease the time it takes to create a working environment.
  prefs: []
  type: TYPE_NORMAL
- en: A DSVM has preconfigured libraries that are tested for compatibility. It is
    updated every few months so that it uses the latest compatible version of tools,
    libraries, and frameworks. It supports GPU VMs and comes pre-installed with the
    necessary drivers. A DSVM also includes tutorials and guides on how to get started.
  prefs: []
  type: TYPE_NORMAL
- en: A DSVM is likely ...
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning a DSVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now go through the steps required to create a DSVM. In this example,
    we will create a Windows 2016 DSVM, but the steps are similar if you want to create
    a Linux DSVM:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Azure portal at [https://portal.azure.com](https://portal.azure.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on Create a resource, and then go to AI + Machine Learning and click
    on Data Science Virtual Machine – Windows 2016:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fe0efb54-7a49-44da-a69e-440cfb3b7e95.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, we can search for `data science virtual machine` in the Azure
    portal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4218cb20-5eae-4eb2-915c-c82d8bc1c627.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A window will appear that will show you some of the key characteristics of
    the DSVM that you choose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f0bc8ff3-867a-455d-b423-edd84e394aea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can create a DSVM quickly by providing the necessary information. This includes
    the Resource group, the Region, a Username, and a Password:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/05d93bb1-32ba-429e-8451-19eef79d5aa0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An important parameter that we need to consider is the VM SIZE. Here, we have
    the option of selecting a CPU- and/or a GPU-based VM. We have lots of options
    based on our CPU, our memory, and our networking requirements. A sample screenshot
    of different types of VMs is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f8457362-aa48-4828-9035-39a5a45f70ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Wait for a few minutes while Azure creates our virtual machine. Once it is ready,
    we will get a notification in the portal.
  prefs: []
  type: TYPE_NORMAL
- en: 'After our VM has been created, we can access it using RDP or SSH, depending
    on the OS. From the overview window, we can look at different metrics when our
    VM is running, such as CPU usage or disk operations. These metrics can help us
    to decide whether we need to resize our VM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4eb6bf0-af66-452e-b22c-aa6035326594.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also configure the VM to shut down automatically using the Auto-shutdown option
    from the portal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/feac6770-f34f-4538-a661-f21d8d2d7ebe.png)'
  prefs: []
  type: TYPE_IMG
- en: After we log in to our DSVM, we can start using its tools as if it were our
    local machine. For more information about each of the tools, please refer to the
    following website: [https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/provision-vm#important-directories-on-the-vm](https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/provision-vm#important-directories-on-the-vm).
  prefs: []
  type: TYPE_NORMAL
- en: After using the VM, we can stop it from the portal to avoid paying for it unnecessarily.
    When we want to use it in the future, we can start it again without having to
    recreate it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now move on to look at a variant of a DSVM – a DLVM.
  prefs: []
  type: TYPE_NORMAL
- en: DLVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A DLVM is a special type of DSVM that has a base image that is customized for
    deep learning. We can use either Windows 2016 or Ubuntu Linux as the OS. It has
    pre-installed frameworks, tools, and tutorials to get you started quickly with
    deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Provisioning a DLVM is similar to provisioning a DSVM. We need to select a
    GPU-based VM for a DLVM:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Azure portal and search for `deep learning virtual machine`. We can
    create a DLVM in the same way as we created a DSVM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A window will pop up, asking us to provide a name and select an OS, username,
    password, resource group, location, and so on. The next window requires us to
    select a GPU-based VM. As shown in the following screenshot, the VM sizes that
    are not available in your region ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Batch AI service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DSVMs and DLVMs are good at carrying out single node-based computing. In scenarios
    where we need to distribute training, however, we can use the Batch AI service,
    which allows us to focus on training instead of having to worry about managing
    the cluster. A Batch AI service has VMs that use the same base image as the DSVM,
    meaning that all the libraries, tools, and frameworks that are available in a
    DSVM are available in the Batch AI service as well. The Batch AI service allows
    us to use parallel training and GPU-based VMs for deep learning, and we can also
    deploy a Docker container to a Batch AI node. When using the Batch AI service,
    we can mount our Azure Blob or Azure Data Lake Storage with our cluster. This
    means that we can train with a huge amount of data without having to copy the
    data to the cluster because it can be streamed instead.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the Batch AI service is only available as a preview,
    so there may be new features when the service is  available generally.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning a Batch AI service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now proceed to create a Batch AI service:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the Azure portal at [https://portal.azure.com](https://portal.azure.com).
    Go to Create a resource and search for `batch ai`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a594316f-47c7-4cad-8995-3662c5f33d09.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next screen, we can create our Batch AI service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The screen after that asks for the name, the subscription, the resource group,
    and the location for our Batch AI service workspace. The workspace is where the
    cluster, experiments, and file server can be added.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Batch AI Cluster will provide the computing power for running your experiments.
    We can provide basic information, such as the number of VMs, the VM type, the
    user account details, and the network ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ACI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a data scientist, after we have trained our machine learning model, we may
    want to deploy the model as a web service for real-time or batch scoring. When
    we train our machine learning model, we use a certain framework and libraries.
    In most cases, the same environment should be available in our deployment environment.
    Containers are a fast and simple way to create such an environment, in which we
    can host our model and dependencies. Containers can be created easily with ACI.
    As data scientists, we can use AML to deploy our machine learning model as a web
    service to an ACI. This way, we can development test our model and then deploy
    it in production. For more details on ACI, refer to the following website: [https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview](https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview).
  prefs: []
  type: TYPE_NORMAL
- en: AKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is an open source project for managing applications that are hosted
    in containers. AKS is a managed Kubernetes service that makes it easy to use Kubernetes.
    Once you test your machine learning model, you can deploy it to a scalable cluster
    with AKS. You can then have a scalable web service for your machine learning model.
    You can use AML to deploy your model to AKS. For more details on AKS, refer to
    the following website: [https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about the different types of compute options
    that are available for you to scale your machine learning training and deployment.
    We learned about the vertical and horizontal scaling compute options that can
    be used to provide you with a large amount of compute power for your machine learning
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the machine learning server, which
    can be used for on-premise deployments.
  prefs: []
  type: TYPE_NORMAL
