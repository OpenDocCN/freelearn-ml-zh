- en: Getting Started with Data Mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are collecting information about our world on a scale that has never been
    seen before in the history of humanity. Along with this trend, we are now placing
    more day-to-day importance on the use of this information in everyday life. We
    now expect our computers to translate web pages into other languages, predict
    the weather with high accuracy, suggest books we would like, and to diagnose our
    health issues. These expectations will grow into the future, both in application
    breadth and efficacy. **Data Mining** is a methodology that we can employ to train
    computers to make decisions with data and forms the backbone of many high-tech
    systems of today.
  prefs: []
  type: TYPE_NORMAL
- en: The **Python** programming language is fast growing in popularity, for a good
    reason. It gives the programmer flexibility, it has many modules to perform different
    tasks, and Python code is usually more readable and concise than in any other
    languages. There is a large and an active community of researchers, practitioners,
    and beginners using Python for data mining.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce data mining with Python. We will cover the
    following topics
  prefs: []
  type: TYPE_NORMAL
- en: What is data mining and where can we use it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Python-based environment to perform data mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of affinity analysis, recommending products based on purchasing habits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of (a classic) classification problem, predicting the plant species
    based on its measurement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing data mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data mining provides a way for a computer to learn how to make decisions with
    data. This decision could be predicting tomorrow's weather, blocking a spam email
    from entering your inbox, detecting the language of a website, or finding a new
    romance on a dating site. There are many different applications of data mining,
    with new applications being discovered all the time.
  prefs: []
  type: TYPE_NORMAL
- en: Data mining is part algorithm design, statistics, engineering, optimization,
    and computer science. However, combined with these *base* skills in the area,
    we also need to apply **domain knowledge (expert knowledge)**of the area we are
    applying the data mining. Domain knowledge is critical for going from good results
    to great results. Applying data mining effectively usually requires this domain-specific
    knowledge to be integrated with the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Most data mining applications work with the same **high-level** view, where
    a model learns from some data and is applied to other data, although the details
    often change quite considerably.
  prefs: []
  type: TYPE_NORMAL
- en: Data mining applications involve creating data sets and tuning the algorithm
    as explained in the following steps
  prefs: []
  type: TYPE_NORMAL
- en: 'We start our data mining process by creating a dataset, describing an aspect
    of the real world. Datasets comprise of the following two aspects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Samples**: These are objects in the real world, such as a book, photograph,
    animal, person, or any other object. Samples are also referred to as observations,
    records or rows, among other naming conventions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Features**: These are descriptions or measurements of the samples in our
    dataset. Features could be the length, frequency of a specific word, the number
    of legs on an animal, date it was created, and so on. Features are also referred
    to as variables, columns, attributes or covariant, again among other naming conventions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step is tuning the data mining algorithm. Each data mining algorithm
    has parameters, either within the algorithm or supplied by the user. This tuning
    allows the algorithm to learn how to make decisions about the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a simple example, we may wish the computer to be able to categorize people
    as *short* or *tall*. We start by collecting our dataset, which includes the heights
    of different people and whether they are considered short or tall:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Person** | **Height** | **Short or tall?** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 155cm | Short |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 165cm | Short |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 175cm | Tall |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 185cm | Tall |'
  prefs: []
  type: TYPE_TB
- en: As explained above, the next step involves tuning the parameters of our algorithm.
    As a simple algorithm; if the height is more than *x*, the person is tall. Otherwise,
    they are short. Our training algorithms will then look at the data and decide
    on a good value for *x*. For the preceding data, a reasonable value for this threshold
    would be 170 cm. A person taller than 170 cm is considered tall by the algorithm.
    Anyone else is considered short by this measure. This then lets our algorithm
    classify new data, such as a person with height 167 cm, even though we may have
    never seen a person with those measurements before.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding data, we had an obvious feature type. We wanted to know if
    people are short or tall, so we collected their heights. This feature engineering
    is a critical problem in data mining. In later chapters, we will discuss methods
    for choosing good features to collect in your dataset. Ultimately, this step often
    requires some expert domain knowledge or at least some trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will introduce data mining through Python. In some cases, we
    choose clarity of code and workflows, rather than the most optimized way to perform
    every task. This clarity sometimes involves skipping some details that can improve
    the algorithm's speed or effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python and the Jupyter Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover installing Python and the environment that we
    will use for most of the book, the **Jupyter** Notebook. Furthermore, we will
    install the **NumPy** module, which we will use for the first set of examples.
  prefs: []
  type: TYPE_NORMAL
- en: The Jupyter Notebook was, until very recently, called the IPython Notebook.
    You'll notice the term in web searches for the project. Jupyter is the new name,
    representing a broadening of the project beyond using just Python.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Python programming language is a fantastic, versatile, and an easy to use
    language.
  prefs: []
  type: TYPE_NORMAL
- en: For this book, we will be using Python 3.5, which is available for your system
    from the Python Organization's website [https://www.python.org/downloads/](https://www.python.org/downloads/).
    However, I recommend that you use Anaconda to install Python, which you can download
    from the official website at [https://www.continuum.io/downloads](https://www.continuum.io/downloads).
  prefs: []
  type: TYPE_NORMAL
- en: There will be two major versions to choose from, Python 3.5 and Python 2.7\.
    Remember to download and install Python 3.5, which is the version tested throughout
    this book. Follow the installation instructions on that website for your system.
    If you have a strong reason to learn version 2 of Python, then do so by downloading
    the Python 2.7 version. Keep in mind that some code may not work as in the book,
    and some workarounds may be needed.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, I assume that you have some knowledge of programming and Python
    itself. You do not need to be an expert with Python to complete this book, although
    a good level of knowledge will help. I will not be explaining general code structures
    and syntax in this book, except where it is different from what is considered
    *normal* python coding practice.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not have any experience with programming, I recommend that you pick
    up the *Learning Python* book from Packt Publishing, or the book *Dive Into Python*,
    available online at [www.diveintopython3.net](http://www.diveintopython3.net)
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python organization also maintains a list of two online tutorials for those
    new to Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For non-programmers who want to learn to program through the Python language:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://wiki.python.org/moin/BeginnersGuide/NonProgrammers](https://wiki.python.org/moin/BeginnersGuide/NonProgrammers)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For programmers who already know how to program, but need to learn Python specifically:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://wiki.python.org/moin/BeginnersGuide/Programmers](https://wiki.python.org/moin/BeginnersGuide/Programmers)'
  prefs: []
  type: TYPE_NORMAL
- en: Windows users will need to set an environment variable to use Python from the
    command line, where other systems will usually be immediately executable. We set
    it in the following steps
  prefs: []
  type: TYPE_NORMAL
- en: First, find where you install Python 3 onto your computer; the default location
    is `C:\Python35`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, enter this command into the command line (cmd program): set the environment
    to `PYTHONPATH=%PYTHONPATH%;C:\Python35`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remember to change the `C:\Python35` if your installation of Python is in a
    different folder.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have Python running on your system, you should be able to open a command
    prompt and can run the following code to be sure it has installed correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that we will be using the dollar sign ($) to denote that a command that
    you type into the terminal (also called a shell or `cmd` on Windows). You do not
    need to type this character (or retype anything that already appears on your screen).
    Just type in the rest of the line and press Enter.
  prefs: []
  type: TYPE_NORMAL
- en: After you have the above `"Hello, world!"` example running, exit the program
    and move on to installing a more advanced environment to run Python code, the
    Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.5 will include a program called **pip**, which is a package manager
    that helps to install new libraries on your system. You can verify that `pip`
    is working on your system by running the `$ pip freeze` command, which tells you
    which packages you have installed on your system. Anaconda also installs their
    package manager, `conda`, that you can use. If unsure, use `conda` first, use
    `pip` only if that fails.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Jupyter Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Jupyter** is a platform for Python development that contains some tools and
    environments for running Python and has more features than the standard interpreter.
    It contains the powerful Jupyter Notebook, which allows you to write programs
    in a web browser. It also formats your code, shows output, and allows you to annotate
    your scripts. It is a great tool for exploring datasets and we will be using it
    as our main environment for the code in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the Jupyter Notebook on your computer, you can type the following
    into a command line prompt (not into Python):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You will not need administrator privileges to install this, as Anaconda keeps
    packages in the user's directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the Jupyter Notebook installed, you can launch it with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this command will do two things. First, it will create a Jupyter Notebook
    instance - the backend - that will run in the command prompt you just used. Second,
    it will launch your web browser and connect to this instance, allowing you to
    create a new notebook. It will look something like the following screenshot (where
    you need to replace `/home/bob` with your current working directory):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_001.png)'
  prefs: []
  type: TYPE_IMG
- en: To stop the Jupyter Notebook from running, open the command prompt that has
    the instance running (the one you used earlier to run the `jupyter notebook  `
    command). Then, press *Ctrl* + *C* and you will be prompted `Shutdown this notebook
    server (y/[n])?`. Type *y* and press *Enter* and the Jupyter Notebook will shut
    down.
  prefs: []
  type: TYPE_NORMAL
- en: Installing scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `scikit-learn` package is a machine learning library, written in Python
    (but also containing code in other languages). It contains numerous algorithms,
    datasets, utilities, and frameworks for performing machine learning. Scikit-learnis
    built upon the scientific python stack, including libraries such as the `NumPy`
    and `SciPy` for speed. Scikit-learn is fast and scalable in many instances and
    useful for all skill ranges from beginners to advanced research users. We will
    cover more details of scikit-learn in [Chapter 2](b453da40-2d85-4978-aca0-3121de2f984e.xhtml),
    *Classifying with scikit-learn Estimators.*
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `scikit-learn`, you can use the `conda` utility that comes with
    Python 3, which will also install the `NumPy` and `SciPy` libraries if you do
    not already have them. Open a terminal with administrator/root privileges and
    enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Users of major Linux distributions such as Ubuntu or Red Hat may wish to install
    the official package from their package manager.
  prefs: []
  type: TYPE_NORMAL
- en: Not all distributions have the latest versions of scikit-learn, so check the
    version before installing it. The minimum version needed for this book is 0.14\.
    My recommendation for this book is to use Anaconda to manage this for you, rather
    than installing using your system's package manager.
  prefs: []
  type: TYPE_NORMAL
- en: Those wishing to install the latest version by compiling the source, or view
    more detailed installation instructions, can go to [http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html)
    and refer the official documentation on installing scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: A simple affinity analysis example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we jump into our first example. A common use case for data
    mining is to improve sales, by asking a customer who is buying a product if he/she
    would like another similar product as well. You can perform this analysis through
    affinity analysis, which is the study of when things exist together, namely. correlate
    to each other.
  prefs: []
  type: TYPE_NORMAL
- en: To repeat the now-infamous phrase taught in statistics classes, *correlation
    is not causation*. This phrase means that the results from affinity analysis cannot
    give a cause. In our next example, we perform affinity analysis on product purchases.
    The results indicate that the products are purchased together, but not that buying
    one product causes the purchase of the other. The distinction is important, critically
    so when determining how to use the results to affect a business process, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: What is affinity analysis?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Affinity analysis is a type of data mining that gives similarity between samples
    (objects). This could be the similarity between the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Users** on a website, to provide varied services or targeted advertising'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Items** to sell to those users, to provide recommended movies or products'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human genes**, to find people that share the same ancestors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can measure affinity in several ways. For instance, we can record how frequently
    two products are purchased together. We can also record the accuracy of the statement
    when a person buys object 1 and when they buy object 2\. Other ways to measure
    affinity include computing the similarity between samples, which we will cover
    in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Product recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the issues with moving a traditional business online, such as commerce,
    is that tasks that used to be done by humans need to be automated for the online
    business to scale and compete with existing automated businesses. One example
    of this is up-selling, or selling an extra item to a customer who is already buying.
    Automated product recommendations through data mining are one of the driving forces
    behind the e-commerce revolution that is turning billions of dollars per year
    into revenue.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we are going to focus on a basic product recommendation service.
    We design this based on the following idea: when two items are historically purchased
    together, they are more likely to be purchased together in the future. This sort
    of thinking is behind many product recommendation services, in both online and
    offline businesses.'
  prefs: []
  type: TYPE_NORMAL
- en: A very simple algorithm for this type of product recommendation algorithm is
    to simply find any historical case where a user has brought an item and to recommend
    other items that the historical user brought. In practice, simple algorithms such
    as this can do well, at least better than choosing random items to recommend.
    However, they can be improved upon significantly, which is where data mining comes
    in.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify the coding, we will consider only two items at a time. As an example,
    people may buy bread and milk at the same time at the supermarket. In this early
    example, we wish to find simple rules of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If a person buys product X, then they are likely to purchase product Y*'
  prefs: []
  type: TYPE_NORMAL
- en: More complex rules involving multiple items will not be covered such as people
    buying sausages and burgers being more likely to buy tomato sauce.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset with NumPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset can be downloaded from the code package supplied with the book,
    or from the official GitHub repository at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/dataPipelineAU/LearningDataMiningWithPython2](https://github.com/dataPipelineAU/LearningDataMiningWithPython2)'
  prefs: []
  type: TYPE_NORMAL
- en: Download this file and save it on your computer, noting the path to the dataset.
    It is easiest to put it in the directory you'll run your code from, but we can
    load the dataset from anywhere on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, I recommend that you create a new folder on your computer
    to store your dataset and code. From here, open your Jupyter Notebook, navigate
    to this folder, and create a new notebook.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we are going to use for this example is a NumPy two-dimensional
    array, which is a format that underlies most of the examples in the rest of the
    book. The array looks like a table, with rows representing different samples and
    columns representing different features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cells represent the value of a specific feature of a specific sample. To
    illustrate, we can load the dataset with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter the previous code into the first cell of your (Jupyter) Notebook. You
    can then run the code by pressing Shift + Enter (which will also add a new cell
    for the next section of code). After the code is run, the square brackets to the
    left-hand side of the first cell will be assigned an incrementing number, letting
    you know that this cell has completed. The first cell should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_1-1.png)'
  prefs: []
  type: TYPE_IMG
- en: For code that will take more time to run, an asterisk will be placed here to
    denote that this code is either running or scheduled to run. This asterisk will
    be replaced by a number when the code has completed running (including if the
    code completes because it failed).
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset has 100 samples and five features, which we will need to know
    for the later code. Let''s extract those values using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you choose to store the dataset somewhere other than the directory your Jupyter
    Notebooks are in, you will need to change the `dataset_filename` value to the
    new location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can show some of the rows of the dataset to get an understanding of
    the data. Enter the following line of code into the next cell and run it, to print
    the first five lines of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will show you which items were bought in the first five transactions
    listed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Downloading the example code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com/)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you could visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you. I''ve also setup a GitHub
    repository that contains a live version of the code, along with new fixes, updates
    and so on. You can retrieve the code and datasets at the repository here: [https://github.com/dataPipelineAU/LearningDataMiningWithPython2](https://github.com/dataPipelineAU/LearningDataMiningWithPython2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read the dataset can by looking at each row (horizontal line) at a
    time. The first row `(0, 1, 0, 0, 0)` shows the items purchased in the first transaction.
    Each column (vertical row) represents each of the items. They are bread, milk,
    cheese, apples, and bananas, respectively. Therefore, in the first transaction,
    the person bought cheese, apples, and bananas, but not bread or milk. Add the
    following line in a new cell to allow us to turn these feature numbers into actual
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Each of these features contains binary values, stating only whether the items
    were purchased and not how many of them were purchased. A*1* indicates that *at
    least 1* item was bought of this type, while a *0* indicates that absolutely none
    of that item was purchased. For a real world dataset, using exact figures or a
    larger threshold would be required.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a simple ranking of rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We wish to find rules of the type *If a person buys product X, then they are
    likely to purchase product Y.* We can quite easily create a list of all the rules
    in our dataset by simply finding all occasions when two products are purchased
    together. However, we then need a way to determine good rules from bad ones allowing
    us to choose specific products to recommend.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can evaluate rules of this type in many ways, on which we will focus on
    two: **support** and **confidence.**'
  prefs: []
  type: TYPE_NORMAL
- en: Support is the number of times that a rule occurs in a dataset, which is computed
    by simply counting the number of samples for which the rule is valid. It can sometimes
    be normalized by dividing by the total number of times the premise of the rule
    is valid, but we will simply count the total for this implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The **premise** is the requirements for a rule to be considered active. The
    **conclusion** is the output of the rule. For the example *if a person buys an
    apple, they also buy a banana*, the rule is only valid if the premise happens
    - a person buys an apple. The rule's conclusion then states that the person will
    buy a banana.
  prefs: []
  type: TYPE_NORMAL
- en: While the support measures how often a rule exists, confidence measures how
    accurate they are when they can be used. You can compute this by determining the
    percentage of times the rule applies when the premise applies. We first count
    how many times a rule applies to our data and divide it by the number of samples
    where the premise (the `if` statement) occurs.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we will compute the support and confidence for the rule *if a
    person buys apples, they also buy bananas.*
  prefs: []
  type: TYPE_NORMAL
- en: 'As the following example shows, we can tell whether someone bought apples in
    a transaction, by checking the value of `sample[3]`, where we assign a sample
    to a row of our matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, we can check if bananas were bought in a transaction by seeing if
    the value of `sample[4]` is equal to 1 (and so on). We can now compute the number
    of times our rule exists in our dataset and, from that, the confidence and support.
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to compute these statistics for all rules in our database. We will
    do this by creating a dictionary for both *valid rules* and *invalid rules*. The
    key to this dictionary will be a tuple (premise and conclusion). We will store
    the indices, rather than the actual feature names. Therefore, we would store (3
    and 4) to signify the previous rule *If a person buys apples, they will also buy
    bananas.* If the premise and conclusion are given, the rule is considered valid.
    While if the premise is given but the conclusion is not, the rule is considered
    invalid for that sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help us to compute the confidence and support for
    all possible rules:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first set up some dictionaries to store the results. We will use `defaultdict`
    for this, which sets a default value if a key is accessed that doesn''t yet exist.
    We record the number of valid rules, invalid rules, and occurrences of each premise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Next, we compute these values in a large loop. We iterate over each sample in
    the dataset and then loop over each feature as a premise. When again loop over
    each feature as a possible conclusion, mapping the relationship premise to conclusion.
    If the sample contains a person who bought the premise and the conclusion, we
    record this in `valid_rules`. If they did not purchase the conclusion product,
    we record this in `invalid_rules`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For sample in X:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If the premise is valid for this sample (it has a value of *1*), then we record
    this and check each conclusion of our rule. We skip over any conclusion that is
    the same as the premise-this would give us rules such as: *if a person buys Apples,
    then they buy Apples*, which obviously doesn''t help us much.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now completed computing the necessary statistics and can now compute
    the *support* and *confidence* for each rule. As before, the support is simply
    our `valid_rules` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can compute the confidence in the same way, but we must loop over each rule
    to compute this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a dictionary with the support and confidence for each rule. We
    can create a function that will print out the rules in a readable format. The
    signature of the rule takes the premise and conclusion indices, the support and
    confidence dictionaries we just computed, and the features array that tells us
    what the `features` mean. Then we print out the `Support` and `Confidence` of
    this rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test the code by calling it in the following way-feel free to experiment
    with different premises and conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Ranking to find the best rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we can compute the support and confidence of all rules, we want to
    be able to find the *best* rules. To do this, we perform a ranking and print the
    ones with the highest values. We can do this for both the support and confidence
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the rules with the highest support, we first sort the support dictionary.
    Dictionaries do not support ordering by default; the `items()` function gives
    us a list containing the data in the dictionary. We can sort this list using the
    `itemgetter` class as our key, which allows for the sorting of nested lists such
    as this one. Using `itemgetter(1)` allows us to sort based on the values. `Setting
    reverse=True` gives us the highest values first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then print out the top five rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, we can print the top rules based on confidence. First, compute the
    sorted confidence list and then print them out using the same method as before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Two rules are near the top of both lists. The first is *If a person buys apples,
    they will also buy cheese*, and the second is *If a person buys cheese, they will
    also buy bananas*. A store manager can use rules like these to organize their
    store. For example, if apples are on sale this week, put a display of cheeses
    nearby. Similarly, it would make little sense to put both bananas on sale at the
    same time as cheese, as nearly 66 percent of people buying cheese will probably
    buy bananas -our sale won't increase banana purchases all that much.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jupyter Notebook will display graphs inline, right in the notebook. Sometimes,
    however, this is not always configured by default. To configure Jupyter Notebook
    to display graphs inline, use the following line of code: `%matplotlib inline`'
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize the results using a library called `matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to start with a simple line plot showing the confidence values
    of the rules, in order of confidence. `matplotlib` makes this easy - we just pass
    in the numbers, and it will draw up a simple but effective plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_01_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the previous graph, we can see that the first five rules have decent confidence,
    but the efficacy drops quite quickly after that. Using this information, we might
    decide to use just the first five rules to drive business decisions. Ultimately
    with exploration techniques like this, the result is up to the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data mining has great exploratory power in examples like this. A person can
    use data mining techniques to explore relationships within their datasets to find
    new insights. In the next section, we will use data mining for a different purpose:
    prediction and classification.'
  prefs: []
  type: TYPE_NORMAL
- en: A simple classification example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the affinity analysis example, we looked for correlations between different
    variables in our dataset. In classification, we have a single variable that we
    are interested in and that we call the **class** (also called the target). In
    the earlier example, if we were interested in how to make people buy more apples,
    we would explore the rules related to apples and use those to inform our decisions.
  prefs: []
  type: TYPE_NORMAL
- en: What is classification?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Classification is one of the largest uses of data mining, both in practical
    use and in research. As before, we have a set of samples that represents objects
    or things we are interested in classifying. We also have a new array, the class
    values. These class values give us a categorization of the samples. Some examples
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determining the species of a plant by looking at its measurements. The class
    value here would be: *Which species is this?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Determining if an image contains a dog. The class would be: *Is there a dog
    in this image?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Determining if a patient has cancer, based on the results of a specific test.
    The class would be: *Does this patient have cancer?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While many of the examples previous are binary (yes/no) questions, they do not
    have to be, as in the case of plant species classification in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of classification applications is to train a model on a set of samples
    with known classes and then apply that model to new unseen samples with unknown
    classes. For example, we want to train a spam classifier on my past e-mails, which
    I have labeled as spam or not spam. I then want to use that classifier to determine
    whether my next email is spam, without me needing to classify it myself.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and preparing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset we are going to use for this example is the famous Iris database
    of plant classification. In this dataset, we have 150 plant samples and four measurements
    of each: **sepal length**, **sepal width**, **petal length**, and **petal width**
    (all in centimeters). This classic dataset (first used in 1936!) is one of the
    classic datasets for data mining. There are three classes: **Iris Setosa**, **Iris
    Versicolour**, and **Iris Virginica**. The aim is to determine which type of plant
    a sample is, by examining its measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `scikit-learn` library contains this dataset built-in, making the loading
    of the dataset straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You can also `print(dataset.DESCR)` to see an outline of the dataset, including
    some details about the features.
  prefs: []
  type: TYPE_NORMAL
- en: The features in this dataset are continuous values, meaning they can take any
    range of values. Measurements are a good example of this type of feature, where
    a measurement can take the value of 1, 1.2, or 1.25 and so on. Another aspect
    of continuous features is that feature values that are close to each other indicate
    similarity. A plant with a sepal length of 1.2 cm is like a plant with a Sepal
    width of 1.25 cm.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast are categorical features. These features, while often represented
    as numbers, cannot be compared in the same way. In the Iris dataset, the class
    values are an example of a categorical feature. The class 0 represents Iris Setosa;
    class 1 represents Iris Versicolour, and class 2 represents Iris Virginica. The
    numbering doesn't mean that Iris Setosa is more similar to Iris Versicolour than
    it is to Iris Virginica-despite the class value being more similar. The numbers
    here represent categories. All we can say is whether categories are the same or
    different.
  prefs: []
  type: TYPE_NORMAL
- en: There are other types of features too, which we will cover in later chapters.
    These include pixel intensity, word frequency and n-gram analysis.
  prefs: []
  type: TYPE_NORMAL
- en: While the features in this dataset are continuous, the algorithm we will use
    in this example requires categorical features. Turning a continuous feature into
    a categorical feature is a process called discretization.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple discretization algorithm is to choose some threshold, and any values
    below this threshold are given a value 0\. Meanwhile, any above this are given
    the value 1\. For our threshold, we will compute the mean (average) value for
    that feature. To start with, we compute the mean for each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The result from this code will be an array of length 4, which is the number
    of features we have. The first value is the mean of the values for the first feature
    and so on. Next, we use this to transform our dataset from one with continuous
    features to one with discrete categorical features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We will use this new `X_d` dataset (for *X discretized*) for our **training
    and testing**, rather than the original dataset (*X*).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the OneR algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**OneR** is a simple algorithm that simply predicts the class of a sample by
    finding the most frequent class for the feature values. **OneR** is shorthand
    for *One Rule*, indicating we only use a single rule for this classification by
    choosing the feature with the best performance. While some of the later algorithms
    are significantly more complex, this simple algorithm has been shown to have good
    performance in some real-world datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm starts by iterating over every value of every feature. For that
    value, count the number of samples from each class that has that feature value.
    Record the most frequent class of the feature value, and the error of that prediction.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a feature has two values, *0* and *1*, we first check all samples
    that have the value *0*. For that value, we may have 20 in Class *A*, 60 in Class
    *B*, and a further 20 in Class *C*. The most frequent class for this value is
    *B*, and there are 40 instances that have different classes. The prediction for
    this feature value is *B* with an error of 40, as there are 40 samples that have
    a different class from the prediction. We then do the same procedure for the value
    *1* for this feature, and then for all other feature value combinations.
  prefs: []
  type: TYPE_NORMAL
- en: Once these combinations are computed, we compute the error for each feature
    by summing up the errors for all values for that feature. The feature with the
    lowest total error is chosen as the *One Rule* and then used to classify other
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, we will first create a function that computes the class prediction
    and error for a specific feature value. We have two necessary imports, `defaultdict`
    and `itemgetter`, that we used in earlier code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the function definition, which needs the dataset, classes,
    the index of the feature we are interested in, and the value we are computing.
    It loops over each sample, and counts the number of time each feature value corresponds
    to a specific class. We then choose the most frequent class for the current feature/value
    pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As a final step, we also compute the error of this rule. In the `OneR` algorithm,
    any sample with this feature value would be predicted as being the most frequent
    class. Therefore, we compute the error by summing up the counts for the other
    classes (not the most frequent). These represent training samples that result
    in error or an incorrect classification.
  prefs: []
  type: TYPE_NORMAL
- en: With this function, we can now compute the error for an entire feature by looping
    over all the values for that feature, summing the errors, and recording the predicted
    classes for each value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function needs the dataset, classes, and feature index we are interested
    in. It then iterates through the different values and finds the most accurate
    feature value to use for this specific feature, as the rule in `OneR`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Let's have a look at this function in a little more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'After some initial tests, we then find all the unique values that the given
    feature takes. The indexing in the next line looks at the whole column for the
    given feature and returns it as an array. We then use the set function to find
    only the unique values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create our dictionary that will store the predictors. This dictionary
    will have feature values as the keys and classification as the value. An entry
    with key 1.5 and value 2 would mean that, when the feature has a value set to
    1.5, classify it as belonging to class 2\. We also create a list storing the errors
    for each feature value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As the main section of this function, we iterate over all the unique values
    for this feature and use our previously defined `train_feature_value` function
    to find the most frequent class and the error for a given feature value. We store
    the results as outlined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we compute the total errors of this rule and return the predictors
    along with this value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Testing the algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we evaluated the affinity analysis algorithm of the earlier section, our
    aim was to explore the current dataset. With this classification, our problem
    is different. We want to build a model that will allow us to classify previously
    unseen samples by comparing them to what we know about the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, we split our machine-learning workflow into two stages: training
    and testing. In training, we take a portion of the dataset and create our model.
    In testing, we apply that model and evaluate how effectively it worked on the
    dataset. As our goal is to create a model that can classify previously unseen
    samples, we cannot use our testing data for training the model. If we do, we run
    the risk of **overfitting**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting is the problem of creating a model that classifies our training
    dataset very well but performs poorly on new samples. The solution is quite simple:
    never use training data to test your algorithm. This simple rule has some complex
    variants, which we will cover in later chapters; but, for now, we can evaluate
    our `OneR` implementation by simply splitting our dataset into two small datasets:
    a training one and a testing one. This workflow is given in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `scikit-learn` library contains a function to split data into training
    and testing components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This function will split the dataset into two sub-datasets, per a given ratio
    (which by default uses 25 percent of the dataset for testing). It does this randomly,
    which improves the confidence that the algorithm will perform as expected in real
    world environments (where we expect data to come in from a random distribution):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have two smaller datasets: `Xd_train` contains our data for training
    and `Xd_test` contains our data for testing. `y_train` and `y_test` give the corresponding
    class values for these datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: We also specify a `random_state`. Setting the random state will give the same
    split every time the same value is entered. It will *look* random, but the algorithm
    used is deterministic, and the output will be consistent. For this book, I recommend
    setting the random state to the same value that I do, as it will give you the
    same results that I get, allowing you to verify your results. To get truly random
    results that change every time you run it, set `random_state` to `None`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute the predictors for all the features for our dataset. Remember
    to only use the training data for this process. We iterate over all the features
    in the dataset and use our previously defined functions to train the predictors
    and compute the errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we find the best feature to use as our *One Rule*, by finding the feature
    with the lowest error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create our `model` by storing the predictors for the best feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model is a dictionary that tells us which feature to use for our *One Rule*
    and the predictions that are made based on the values it has. Given this model,
    we can predict the class of a previously unseen sample by finding the value of
    the specific feature and using the appropriate predictor. The following code does
    this for a given sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Often we want to predict several new samples at one time, which we can do using
    the following function. It simply uses the above code, but iterate over all the
    samples in a dataset, obtaining the prediction for each sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'For our `testing` dataset, we get the predictions by calling the following
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then compute the accuracy of this by comparing it to the known classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This algorithm gives an accuracy of 65.8 percent, which is not bad for a single
    rule!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced data mining using Python. If you could run the
    code in this section (note that the full code is available in the supplied code
    package), then your computer is set up for much of the rest of the book. Other
    Python libraries will be introduced in later chapters to perform more specialized
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We used the Jupyter Notebook to run our code, which allows us to immediately
    view the results of a small section of the code. Jupyter Notebook is a useful
    tool that will be used throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: We introduced a simple affinity analysis, finding products that are purchased
    together. This type of exploratory analysis gives an insight into a business process,
    an environment, or a scenario. The information from these types of analysis can
    assist in business processes, find the next big medical breakthrough, or create
    the next artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in this chapter, there was a simple classification example using the `OneR`
    algorithm. This simple algorithm simply finds the best feature and predicts the
    class that most frequently had this value in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To expand on the outcomes of this chapter, think about how you would implement
    a variant of `OneR` that can take multiple feature/value pairs into consideration.
    Take a shot at implementing your new algorithm and evaluating it. Remember to
    test your algorithm on a separate dataset to the training data. Otherwise, you
    run the risk of over fitting your data.
  prefs: []
  type: TYPE_NORMAL
- en: Over the next few chapters, we will expand on the concepts of classification
    and affinity analysis. We will also introduce classifiers in the scikit-learn
    package and use them to do our machine learning, rather than writing the algorithms
    ourselves.
  prefs: []
  type: TYPE_NORMAL
