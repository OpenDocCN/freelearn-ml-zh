<html><head></head><body>
<div id="_idContainer021">
<h1 class="chapter-number" id="_idParaDest-29"><a id="_idTextAnchor028"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/><span class="koboSpan" id="kobo.2.1">From Model-Centric to Data-Centric – ML’s Evolution</span></h1>
<p><span class="koboSpan" id="kobo.3.1">By now, you might be thinking: if data-centricity is essential to the further evolution of AI and ML, how come model-centricity is the </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">dominant approach?</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">This is a very relevant question to ask, and one we will answer in this chapter. </span><span class="koboSpan" id="kobo.5.2">To understand what it takes to shift to a data-centric approach, we must understand the forces that have led to model-centricity being the predominant approach, and how to </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">overcome them.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">We will start this chapter by exploring why the evolution of AI and ML has predominately followed a model-centric approach, before diving into the huge opportunity that can be unlocked </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">through data-centricity.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">Throughout this chapter, we will challenge the notion that ML requires big datasets and that more data is always better. </span><span class="koboSpan" id="kobo.9.2">There is a long tail of </span><em class="italic"><span class="koboSpan" id="kobo.10.1">small data</span></em><span class="koboSpan" id="kobo.11.1"> ML use cases that open up when we shift our mindset from </span><em class="italic"><span class="koboSpan" id="kobo.12.1">bigger data</span></em><span class="koboSpan" id="kobo.13.1"> to </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.14.1">better data</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.16.1">By the end of this chapter, you will have a clear understanding of the progression of ML to date, and know what it takes to build on the current paradigm and achieve even better results </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">with ML.</span></span></p>
<p><span class="koboSpan" id="kobo.18.1">In this chapter, we will cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.20.1">Exploring why ML development ended up being </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">mostly model-centric</span></span></li>
<li><span class="koboSpan" id="kobo.22.1">The opportunity for </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">small-data ML</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">Why we need data-centric ML more </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">than ever</span></span></li>
</ul>
<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/><span class="koboSpan" id="kobo.26.1">Exploring why ML development ended up being mostly model-centric</span></h1>
<p><span class="koboSpan" id="kobo.27.1">A short history lesson is in order to truly appreciate why a data-centric approach</span><a id="_idIndexMarker032"/><span class="koboSpan" id="kobo.28.1"> is the key to unlocking the full potential </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">of ML.</span></span></p>
<p><span class="koboSpan" id="kobo.30.1">The fields of data science and ML have achieved significant advancements since the earliest attempts to make electronic computers act </span><em class="italic"><span class="koboSpan" id="kobo.31.1">intelligently</span></em><span class="koboSpan" id="kobo.32.1">. </span><span class="koboSpan" id="kobo.32.2">The </span><em class="italic"><span class="koboSpan" id="kobo.33.1">intelligent</span></em><span class="koboSpan" id="kobo.34.1"> tasks performed by most smartphones today were nearly unimaginable at the turn of the 21</span><span class="superscript"><span class="koboSpan" id="kobo.35.1">st</span></span><span class="koboSpan" id="kobo.36.1"> century. </span><span class="koboSpan" id="kobo.36.2">Moreover, we are producing more data every single day than was created from the beginning of human civilization to the 21</span><span class="superscript"><span class="koboSpan" id="kobo.37.1">st</span></span><span class="koboSpan" id="kobo.38.1"> century – and we’re doing so at an estimated growth rate of 23% </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">per annum</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.40.1">1</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.42.1">Despite these incredible developments in technology and data volumes, some elements of data science are very old. </span><span class="koboSpan" id="kobo.42.2">Statistics and data analysis have been in use for centuries and the mathematical components of today’s ML models were mostly developed long before the advent of </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">digital computers.</span></span></p>
<p><span class="koboSpan" id="kobo.44.1">For our purposes, the history of ML and AI</span><a id="_idIndexMarker033"/><span class="koboSpan" id="kobo.45.1"> starts with the introduction of the first electronic calculation machines during World </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">War II.</span></span></p>
<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/><span class="koboSpan" id="kobo.47.1">The 1940s to 1970s – the early days</span></h2>
<p><span class="koboSpan" id="kobo.48.1">Historian and former US Army officer Adrian R. </span><span class="koboSpan" id="kobo.48.2">Lewis wrote in his book </span><em class="italic"><span class="koboSpan" id="kobo.49.1">The American Culture of War</span></em><span class="koboSpan" id="kobo.50.1"> that “war created the conditions for great advances in technology… without war, men would not traverse oceans in hours, travel in space, or </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">microwave popcorn</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.52.1">2</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">.”</span></span></p>
<p><span class="koboSpan" id="kobo.54.1">This was indeed the case during World War II, and in the decades that followed. </span><span class="koboSpan" id="kobo.54.2">Huge leaps were made in computer science, cryptology, and hardware technology, as fighting nations around the world were racing each other for dominance on </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">every front.</span></span></p>
<p><span class="koboSpan" id="kobo.56.1">In the 1940s and 1950s, innovations such as compilers, semiconductor transistors, integrated circuits, and computer chips made digital electronic computers capable of performing more complex processes (until this point, a </span><em class="italic"><span class="koboSpan" id="kobo.57.1">computer</span></em><span class="koboSpan" id="kobo.58.1"> was predominately the job title of mathematically gifted humans employed to perform complex calculations</span><span class="superscript"><span class="koboSpan" id="kobo.59.1">3</span></span><span class="koboSpan" id="kobo.60.1">). </span><span class="koboSpan" id="kobo.60.2">This, in turn, led to some early innovations that underpin today’s </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">ML models.</span></span></p>
<p><span class="koboSpan" id="kobo.62.1">In 1943, American scientists Walter Pitts and Warren McCullough created the world’s first computational model for neural networks. </span><span class="koboSpan" id="kobo.62.2">This formed the basis for other innovations in AI, including Arthur Samuel’s self-improving checkers-playing program in 1952 and the </span><strong class="bold"><span class="koboSpan" id="kobo.63.1">perceptron</span></strong><span class="koboSpan" id="kobo.64.1">, a </span><a id="_idIndexMarker034"/><span class="koboSpan" id="kobo.65.1">neural network for classifying images funded by the US Navy and IBM </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">in 1958.</span></span></p>
<p><span class="koboSpan" id="kobo.67.1">In 1950, British mathematician and computer scientist Alan Turing introduced the </span><em class="italic"><span class="koboSpan" id="kobo.68.1">Turing test</span></em><span class="koboSpan" id="kobo.69.1"> for </span><a id="_idIndexMarker035"/><span class="koboSpan" id="kobo.70.1">assessing a computer’s ability to perform intelligent operations comparable to those of humans. </span><span class="koboSpan" id="kobo.70.2">The test was often used as a benchmark for the </span><em class="italic"><span class="koboSpan" id="kobo.71.1">intelligence</span></em><span class="koboSpan" id="kobo.72.1"> of a computer and became very influential to the philosophy of AI </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">in general.</span></span></p>
<p><span class="koboSpan" id="kobo.74.1">The expansion of ML research continued throughout the 1960s, with the development of the nearest neighbor algorithm being one of the most noticeable advances. </span><span class="koboSpan" id="kobo.74.2">The work of Stanford researchers Thomas Cover and Peter Hart formed the basis for the rise of the k-nearest neighbor algorithm as a powerful statistical </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">classification method</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.76.1">4</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.78.1">In 1965, co-founder of </span><a id="_idIndexMarker036"/><span class="koboSpan" id="kobo.79.1">Fairchild Semiconductor and Intel, Gordon Moore proposed that processing power and hard drive storage for computers would double every two years, also known as </span><em class="italic"><span class="koboSpan" id="kobo.80.1">Moore’s law</span></em><span class="superscript"><span class="koboSpan" id="kobo.81.1">5</span></span><span class="koboSpan" id="kobo.82.1">. </span><span class="koboSpan" id="kobo.82.2">Even</span><a id="_idIndexMarker037"/><span class="koboSpan" id="kobo.83.1"> though Moore’s law proved to be reasonably accurate, it would take many decades to reach a point where vast amounts of data could be processed at a reasonable speed </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">and cost.</span></span></p>
<p><span class="koboSpan" id="kobo.85.1">To put things into perspective, IBM’s leading product in 1970 was the System/370 Model 145, which had 500 KB of RAM and 233 MB of hard disk space</span><span class="superscript"><span class="koboSpan" id="kobo.86.1">6</span></span><span class="koboSpan" id="kobo.87.1">. </span><span class="koboSpan" id="kobo.87.2">The computer took up a whole room and cost $705,775 to $1,783,000</span><span class="superscript"><span class="koboSpan" id="kobo.88.1">7</span></span><span class="koboSpan" id="kobo.89.1">, circa $5 to $13 million in today’s inflation-adjusted dollars. </span><span class="koboSpan" id="kobo.89.2">At the time of writing, the latest iPhone 14 has 12,000 times the amount of RAM and up to 2,200 times the amount of hard disk space of the System/370 Model 145, depending on the </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">iPhone configuration</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.91.1">8</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<span class="koboSpan" id="kobo.93.1"><img alt="Figure 2.1 – The IBM System/370 Model 145. Everything in this picture is part of the computer’s operation (except the clock on the wall). Source: Jean Weber/INRA, DIST" src="image/B19297_02_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.94.1">Figure 2.1 – The IBM System/370 Model 145. </span><span class="koboSpan" id="kobo.94.2">Everything in this picture is part of the computer’s operation (except the clock on the wall). </span><span class="koboSpan" id="kobo.94.3">Source: Jean Weber/INRA, DIST</span></p>
<p><span class="koboSpan" id="kobo.95.1">Most of the 1970s </span><a id="_idIndexMarker038"/><span class="koboSpan" id="kobo.96.1">are widely recognized as a period of “AI Winter” – a period with very little ground-breaking research or developments in the field of AI. </span><span class="koboSpan" id="kobo.96.2">The business world saw little short-term potential in AI, mainly because computer processing power and data storage capacity were underdeveloped and </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">prohibitively expensive.</span></span></p>
<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/><span class="koboSpan" id="kobo.98.1">The 1980s to 1990s – the rise of personal computing and the internet</span></h2>
<p><span class="koboSpan" id="kobo.99.1">In 1982, IBM</span><a id="_idIndexMarker039"/><span class="koboSpan" id="kobo.100.1"> introduced the first personal computer (IBM PC), which sparked a revolution in computer technology at work and in people’s homes. </span><span class="koboSpan" id="kobo.100.2">It also led to the meteoric rise of companies such as Apple, Microsoft, Hewlett-Packard, Intel, and many other hardware and software enterprises that rode the wave of </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">technological innovation.</span></span></p>
<p><span class="koboSpan" id="kobo.102.1">The increased ability to digitize processes and information also amplified the corporate world’s interest in using stored data for analytical purposes. </span><span class="koboSpan" id="kobo.102.2">Relational databases became mainstream, at the expense of network and hierarchical </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">database models</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.104.1">9</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.106.1">The query language SQL was developed in the 1970s; throughout the 1980s, it became widely accepted as the main database language, achieving the ISO and ANSI certifications </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">in 1986</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.108.1">10</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.110.1">The explosion in digital information created a need for new techniques to make sense of data from a statistical point of view. </span><span class="koboSpan" id="kobo.110.2">Stanford University researchers developed the first software to generate classification and regression trees in 1984, and innovations such as the lexical database WordNet created the early foundations for text analysis and natural </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">language processing.</span></span></p>
<p><span class="koboSpan" id="kobo.112.1">Personal computers continued to replace typewriters and mainframes into the 1990s, which allowed for the World Wide Web to be formed in 1991. </span><span class="koboSpan" id="kobo.112.2">Websites, blogs, internet forums, emails, instant messages, and VoIP calls created yet another explosion in the volume, variety, and velocity </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">of data.</span></span></p>
<p><span class="koboSpan" id="kobo.114.1">As a result, new methods for organizing more complex and disparate types of data evolved. </span><span class="koboSpan" id="kobo.114.2">Gradient boosting algorithms such as AdaBoost and gradient boosting machines were developed by Stanford researchers throughout the late nineties, paving the way for search engines to rank all sorts </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">of information.</span></span></p>
<p><span class="koboSpan" id="kobo.116.1">The rise of the internet also created a huge business opportunity for those who could organize the information on it. </span><span class="koboSpan" id="kobo.116.2">Companies such as Amazon, Alibaba, Yahoo!, and Google were founded during this period to fight for dominance in e-commerce and web search. </span><span class="koboSpan" id="kobo.116.3">These companies saw enormous potential in computer science, AI, and ML and invested heavily in developing algorithms to manage their vast stores </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">of information.</span></span></p>
<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/><span class="koboSpan" id="kobo.118.1">The 2000s – the rise of tech giants</span></h2>
<p><span class="koboSpan" id="kobo.119.1">ML research </span><a id="_idIndexMarker040"/><span class="koboSpan" id="kobo.120.1">picked up pace throughout the 2000s, whether it be in universities or corporate </span><strong class="bold"><span class="koboSpan" id="kobo.121.1">research and development</span></strong><span class="koboSpan" id="kobo.122.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.123.1">R&amp;D</span></strong><span class="koboSpan" id="kobo.124.1">) departments. </span><span class="koboSpan" id="kobo.124.2">Computer processing power had finally reached a point where large-scale data processing was feasible for most corporations </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">and researchers.</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">While internet search engine providers were busy developing algorithms to sort and categorize the ever-growing information being published online, university researchers were creating new tools and techniques that would fuel the evolution </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">of ML.</span></span></p>
<p><span class="koboSpan" id="kobo.128.1">In 2003, The R Foundation was created to develop and support the open source ML tool and programming language R. </span><span class="koboSpan" id="kobo.128.2">As a freely available and open source programming language for statistical computing and graphics</span><span class="superscript"><span class="koboSpan" id="kobo.129.1">11</span></span><span class="koboSpan" id="kobo.130.1">, R significantly lowered the barrier to entry for researchers looking to use statistical programming in their work and for data enthusiasts wanting to practice and learn </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">ML techniques.</span></span></p>
<p><span class="koboSpan" id="kobo.132.1">Random Forest algorithms were introduced in 2001 and later patented in 2006 by statisticians and ML pioneers Leo Breiman from the University of California, Berkley, and Adele Cutler from Utah </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">State University</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.134.1">12</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.136.1">Stanford</span><a id="_idIndexMarker041"/><span class="koboSpan" id="kobo.137.1"> professor Fei-Fei Li introduced the ImageNet project in 2008 as a free and open image database for training object recognition models</span><span class="superscript"><span class="koboSpan" id="kobo.138.1">13</span></span><span class="koboSpan" id="kobo.139.1">. </span><span class="koboSpan" id="kobo.139.2">The database was created to provide a high-quality, standardized dataset for object categorization models to be trained and benchmarked on. </span><span class="koboSpan" id="kobo.139.3">At the time of writing, ImageNet contains more than 14 million labeled images, organized according to the </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">WordNet hierarchy.</span></span></p>
<p><span class="koboSpan" id="kobo.141.1">This period also saw the meteoric rise of the network-based business model as a way to create internet dominance. </span><span class="koboSpan" id="kobo.141.2">Social media platforms such as LinkedIn, Facebook, Twitter, and YouTube were launched during this period and became supernational tech giants by using ML algorithms to organize information and content created by </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">their users.</span></span></p>
<p><span class="koboSpan" id="kobo.143.1">As data volumes exploded, so did the need for cheap and flexible data storage. </span><span class="koboSpan" id="kobo.143.2">Cloud compute and storage services such as AWS, Dropbox, and Google Drive were launched, while universities joined forces with Google and IBM to establish server farms that could be used for data-intensive research</span><span class="superscript"><span class="koboSpan" id="kobo.144.1">14</span></span><span class="koboSpan" id="kobo.145.1">. </span><span class="koboSpan" id="kobo.145.2">Increasingly, the availability of processing power was now based on the user’s economic justification rather than </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">technical limitations.</span></span></p>
<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/><span class="koboSpan" id="kobo.147.1">2010–now – big data drives AI innovation</span></h2>
<p><span class="koboSpan" id="kobo.148.1">Network-based </span><a id="_idIndexMarker042"/><span class="koboSpan" id="kobo.149.1">businesses continued to define the direction for the internet and ML development. </span><span class="koboSpan" id="kobo.149.2">Search engines, social media platforms, and software and hardware providers invested heavily in R&amp;D activities surrounding AI. </span><span class="koboSpan" id="kobo.149.3">As an example, the Google Brain research team was founded in 2011 to provide cutting-edge AI research on </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">big data.</span></span></p>
<p><span class="koboSpan" id="kobo.151.1">New network-based companies were disrupting industries such as taxis, hotels, travel services, payments, restaurant and food services, media, music, banking, consumer retail, and education – utilizing digital platforms, ML, and vast amounts of consumer data as their powerful </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">competitive advantage.</span></span></p>
<p><span class="koboSpan" id="kobo.153.1">Traditional research institutions formed tight collaborations with big tech companies, resulting in big leaps in deep learning techniques for audio and image recognition, natural language understanding, anomaly detection, synthetic data generation, and </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">much more.</span></span></p>
<p><span class="koboSpan" id="kobo.155.1">By 2017, three out of four teams competing in the annual ImageNet Challenge achieved greater than 95% accuracy, proving that image recognition algorithms were now </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">highly advanced.</span></span></p>
<p><span class="koboSpan" id="kobo.157.1">Powerful algorithms for generating new data were also developed during this golden decade of AI. </span><span class="koboSpan" id="kobo.157.2">In 2014, a researcher from the Google Brain team named Ian Goodfellow invented the </span><strong class="bold"><span class="koboSpan" id="kobo.158.1">Generative Adversarial Network</span></strong><span class="koboSpan" id="kobo.159.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.160.1">GAN</span></strong><span class="koboSpan" id="kobo.161.1">), a neural network that works by pairing two models against </span><a id="_idIndexMarker043"/><span class="koboSpan" id="kobo.162.1">each other</span><span class="superscript"><span class="koboSpan" id="kobo.163.1">15</span></span><span class="koboSpan" id="kobo.164.1">. </span><span class="koboSpan" id="kobo.164.2">Another form of generative model framework, the </span><strong class="bold"><span class="koboSpan" id="kobo.165.1">Generative Pre-trained Transformer</span></strong><span class="koboSpan" id="kobo.166.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.167.1">GPT</span></strong><span class="koboSpan" id="kobo.168.1">), entered</span><a id="_idIndexMarker044"/><span class="koboSpan" id="kobo.169.1"> the scene in 2018, courtesy of the OpenAI </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">research lab.</span></span></p>
<p><span class="koboSpan" id="kobo.171.1">With generative models in operation, it was now possible to produce </span><em class="italic"><span class="koboSpan" id="kobo.172.1">human-like</span></em><span class="koboSpan" id="kobo.173.1"> outputs such as text snippets, images, artwork, music, and deepfakes – audio and video impersonations of someone’s voice </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">and mannerisms.</span></span></p>
<p><span class="koboSpan" id="kobo.175.1">As </span><em class="italic"><span class="koboSpan" id="kobo.176.1">big data</span></em><span class="koboSpan" id="kobo.177.1">, </span><em class="italic"><span class="koboSpan" id="kobo.178.1">ML</span></em><span class="koboSpan" id="kobo.179.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.180.1">AI</span></em><span class="koboSpan" id="kobo.181.1"> became part of the vernacular, the demand for analysts, data scientists, data engineers, and other data professionals increased substantially. </span><span class="koboSpan" id="kobo.181.2">In 2011, job listings for data scientists increased by 15,000% year on year</span><span class="superscript"><span class="koboSpan" id="kobo.182.1">16</span></span><span class="koboSpan" id="kobo.183.1">. </span><span class="koboSpan" id="kobo.183.2">The massive enthusiasm for the potential of data and ML caused analytics pioneers Tom Davenport and DJ Patil to label data science as </span><em class="italic"><span class="koboSpan" id="kobo.184.1">the sexiest job of the 21</span></em><span class="superscript"><span class="koboSpan" id="kobo.185.1">st</span></span><em class="italic"><span class="koboSpan" id="kobo.186.1"> century</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.187.1">in 2012</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.188.1">17</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.190.1">Millions of data enthusiasts around the world sought out places to learn the latest ML and data mining techniques. </span><span class="koboSpan" id="kobo.190.2">Platforms such as Kaggle and Coursera allowed millions of users to learn through open online courses, enter ML contests, access quality datasets, and </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">share knowledge.</span></span></p>
<p><span class="koboSpan" id="kobo.192.1">On the tooling front, the proliferation of freely downloadable software programs and packages running on R, Python, or SQL made it relatively easy to access advanced data science techniques at a </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">low cost:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<span class="koboSpan" id="kobo.194.1"><img alt="Figure 2.2 – A history of ML from 1940 to now" src="image/B19297_02_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.195.1">Figure 2.2 – A history of ML from 1940 to now</span></p>
<p><span class="koboSpan" id="kobo.196.1">As the </span><a id="_idIndexMarker045"/><span class="koboSpan" id="kobo.197.1">advancements in information technology, data, and AI converged during AI’s golden decade of 2010 to 2020, ML model architectures have matured significantly. </span><span class="koboSpan" id="kobo.197.2">At this point, most of the opportunities to create better models lie in improving </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">data quality.</span></span></p>
<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/><span class="koboSpan" id="kobo.199.1">Model-centricity was the logical evolutionary outcome</span></h2>
<p><span class="koboSpan" id="kobo.200.1">The last eight decades of data science history have followed a logical evolutionary path that has led to model-centricity</span><a id="_idIndexMarker046"/><span class="koboSpan" id="kobo.201.1"> being the principal approach </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">to ML.</span></span></p>
<p><span class="koboSpan" id="kobo.203.1">The ideas and </span><a id="_idIndexMarker047"/><span class="koboSpan" id="kobo.204.1">mathematical concepts behind ML were imagined long before the technology was mature enough to match them. </span><span class="koboSpan" id="kobo.204.2">Before the 1990s, computers were not powerful enough to allow university researchers to evolve the field of ML substantially. </span><span class="koboSpan" id="kobo.204.3">These technical limitations also meant that there was limited research conducted for commercial gain by private enterprises during </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">this period.</span></span></p>
<p><span class="koboSpan" id="kobo.206.1">At the advent of the internet era in the early 1990s, hardware and software solutions were beginning to be advanced enough to eliminate these age-old limitations. </span><span class="koboSpan" id="kobo.206.2">The internet also sparked an information revolution that increased the volume and variety of available data enormously. </span><span class="koboSpan" id="kobo.206.3">All of a sudden, ML was not just financially viable, it became the driving force behind tech companies such as Amazon, Yahoo!, and Google. </span><span class="koboSpan" id="kobo.206.4">With more digital information available than ever before, there was a need to advance the way we interpreted and modeled various kinds of data. </span><span class="koboSpan" id="kobo.206.5">In other words, ML research needed a model-centric focus first </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">and foremost.</span></span></p>
<p><span class="koboSpan" id="kobo.208.1">Throughout the 2000s, a new kind of business model came to dominate our lives. </span><span class="koboSpan" id="kobo.208.2">Network-based digital businesses such as social media platforms, search engines, software creators, and online marketplaces created platforms where users could create and interact with content and products. </span><span class="koboSpan" id="kobo.208.3">By applying ML to massive amounts of user-generated data, these businesses watched and optimized every interaction along </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">the way.</span></span></p>
<p><span class="koboSpan" id="kobo.210.1">These “AI-first” big tech businesses were less constrained by data quality or volume. </span><span class="koboSpan" id="kobo.210.2">Their constraints lay mostly in fast and affordable compute and storage capacity, and the sophistication of ML techniques. </span><span class="koboSpan" id="kobo.210.3">Through in-house research, partnerships with universities, and strategic investments in promising AI technologies, big tech companies have been able to drive the agenda for ML development over the last two decades. </span><span class="koboSpan" id="kobo.210.4">What these companies needed primarily was a </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">model-centric approach.</span></span></p>
<p><span class="koboSpan" id="kobo.212.1">As a result of the model-centric research that has occurred since the mid-1990s, we now have algorithms that can organize all the world’s information, identify individuals in a crowd, drive vehicles in open traffic, recognize and generate sound, speech, and imagery, and much more. </span><span class="koboSpan" id="kobo.212.2">Our ability to make accurate models </span><em class="italic"><span class="koboSpan" id="kobo.213.1">given the input data</span></em><span class="koboSpan" id="kobo.214.1"> is very advanced thanks to this period </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">of innovation.</span></span></p>
<p><span class="koboSpan" id="kobo.216.1">As data continued to become a more ubiquitous asset, there was a sudden strong need to train more data scientists and other data professionals. </span><span class="koboSpan" id="kobo.216.2">Today, there is no shortage of learning opportunities through online learning platforms, university courses, and ML competitions, but they typically have one thing in common: the initial input dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">is predefined.</span></span></p>
<p><span class="koboSpan" id="kobo.218.1">It makes a lot of sense to teach ML on a fixed dataset. </span><span class="koboSpan" id="kobo.218.2">Without a replicable output, it is difficult to verify whether learners have mastered a particular technique, or benchmark different models against each other. </span><span class="koboSpan" id="kobo.218.3">However, the natural consequence is that learning is centered around model improvement through model-centric tasks such as model selection, hyperparameter tuning, feature engineering, and other enhancements of the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.219.1">existing</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.220.1"> dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.221.1">Model-centric skills </span><a id="_idIndexMarker048"/><span class="koboSpan" id="kobo.222.1">must be mastered by experienced data scientists, but they are just the foundation of a data-centric paradigm. </span><span class="koboSpan" id="kobo.222.2">This is because ML progress comes in </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">four parts:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.224.1">Improving </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">computer power</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.226.1">Improving algorithms</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.227.1">Improving data</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.228.1">Improving measurement</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.229.1">So far, we have made huge progress on items 1 and 2, to a point where they are largely a solved problem for the majority of ML use cases. </span><span class="koboSpan" id="kobo.229.2">Most of the opportunity now lies in evolving our approach to improving data and measurement. </span><span class="koboSpan" id="kobo.229.3">When we improve our data, we can build better models, but we also unlock the long tail of ML use cases that are often out of reach because we only have a few thousand rows (or less) of data to build our </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">models on.</span></span></p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/><span class="koboSpan" id="kobo.231.1">Unlocking the opportunity for small data ML</span></h1>
<p><span class="koboSpan" id="kobo.232.1">The group of </span><a id="_idIndexMarker049"/><span class="koboSpan" id="kobo.233.1">tech companies famously labeled </span><em class="italic"><span class="koboSpan" id="kobo.234.1">The Big Nine</span></em><span class="koboSpan" id="kobo.235.1"> by author Amy Webb</span><span class="superscript"><span class="koboSpan" id="kobo.236.1">18</span></span><span class="koboSpan" id="kobo.237.1"> are examples of consumer internet companies that have leveraged big data and AI to build world dominance. </span><span class="koboSpan" id="kobo.237.2">Amazon, Apple, Alibaba, Baidu, Meta, Google, IBM, Microsoft, and Tencent dominate in the digital era because they utilize enormous amounts of user data to power their </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">AI systems.</span></span></p>
<p><span class="koboSpan" id="kobo.239.1">As network-based </span><em class="italic"><span class="koboSpan" id="kobo.240.1">AI-first</span></em><span class="koboSpan" id="kobo.241.1"> businesses, they have amassed customers on an unprecedented scale because users are happy to co-create and share their data, so long as it is a net benefit to them. </span><span class="koboSpan" id="kobo.241.2">For the Big Nine, getting enough modeling data is rarely a problem, and investing in the most advanced ML capabilities is a virtuous circle that enables more </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">market dominance.</span></span></p>
<p><span class="koboSpan" id="kobo.243.1">For most other organizations – and ML use cases – this sort of scale is unachievable. </span><span class="koboSpan" id="kobo.243.2">As we explored in </span><a href="B19297_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.244.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.245.1">, </span><em class="italic"><span class="koboSpan" id="kobo.246.1">Exploring Data-Centric Machine Learning</span></em><span class="koboSpan" id="kobo.247.1"> the long tail of ML opportunities doesn’t offer the option to build models on large volumes of training data because of the </span><a id="_idIndexMarker050"/><span class="No-Break"><span class="koboSpan" id="kobo.248.1">following challenges:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.249.1">The lack of training data observations</span></strong><span class="koboSpan" id="kobo.250.1">: Datasets are smaller in the long tail – typically in the order of only a few thousand rows or less. </span><span class="koboSpan" id="kobo.250.2">On top of that, most organizations are capturing data in the non-digitized physical world, which makes it harder to capture and finetune some </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">data points.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.252.1">Dirty data</span></strong><span class="koboSpan" id="kobo.253.1">: Unlike network-based </span><em class="italic"><span class="koboSpan" id="kobo.254.1">AI-first</span></em><span class="koboSpan" id="kobo.255.1"> businesses, most organizations generate data through a large variety of sources such as internal (but externally developed) IT systems, third-party platforms, and manual collection by staff or customers. </span><span class="koboSpan" id="kobo.255.2">This creates a complex patchwork of data sources that come with a variety of data </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">quality challenges.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.257.1">Risk of bias and unfairness in high-stakes domains</span></strong><span class="koboSpan" id="kobo.258.1">: Poor data quality in high-stakes domains such as healthcare, legal services, education, public safety, and crime prevention may lead to disastrous impacts on individuals or vulnerable populations. </span><span class="koboSpan" id="kobo.258.2">For example, predicting whether a person has cancer based on medical images is a high-stakes activity – recommending the next video to watch based on your YouTube history video </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">is not.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.260.1">Model complexity and lack of economies of scale</span></strong><span class="koboSpan" id="kobo.261.1">: Even though there is plenty of value to be found in the long tail, individual ML projects typically need a lot of customization to deal with distinct scenarios. </span><span class="koboSpan" id="kobo.261.2">Customization is costly as it creates an accumulation of many models, datasets, and processes that must be maintained pre- and </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">post-model implementation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.263.1">The need for domain expertise in data and model development</span></strong><span class="koboSpan" id="kobo.264.1">: The combination of small datasets, higher stakes, and more complex scenarios makes it difficult to build ML models without the involvement of subject-matter experts during data collection, labeling and validation, model development, </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">and testing.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.266.1">It is important to note that many companies have the opportunity to unlock significant value with </span><em class="italic"><span class="koboSpan" id="kobo.267.1">small data</span></em><span class="koboSpan" id="kobo.268.1"> ML. </span><span class="koboSpan" id="kobo.268.2">For</span><a id="_idIndexMarker051"/><span class="koboSpan" id="kobo.269.1"> example, only a few organizations will have individual ML projects worth $50 million or more, but many more organizations will have 50 potential ML opportunities worth $1 million each. </span><span class="koboSpan" id="kobo.269.2">In practice, this means we must get maximum value out of our raw material if we want smaller projects to become feasible and </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">financially viable.</span></span></p>
<p><span class="koboSpan" id="kobo.271.1">Dr Andrew Ng, CEO and founder of Landing AI, summarizes these challenges </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">as follows</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.273.1">19</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">:</span></span></p>
<p class="author-quote"><span class="koboSpan" id="kobo.275.1">“In the consumer software Internet, we could train a handful of ML models to serve a billion users. </span><span class="koboSpan" id="kobo.275.2">In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models.”</span></p>
<p class="author-quote"><span class="koboSpan" id="kobo.276.1">“In many industries, where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. </span><span class="koboSpan" id="kobo.276.2">Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.”</span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.277.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.278.1">.3</span></em><span class="koboSpan" id="kobo.279.1"> illustrates the challenge and opportunity of </span><em class="italic"><span class="koboSpan" id="kobo.280.1">small data</span></em><span class="koboSpan" id="kobo.281.1"> ML. </span><span class="koboSpan" id="kobo.281.2">While the low-hanging fruits of big data/high-value ML use cases have been picked by </span><em class="italic"><span class="koboSpan" id="kobo.282.1">AI-first</span></em><span class="koboSpan" id="kobo.283.1"> businesses, the long tail of small data/moderate value is underexploited. </span><span class="koboSpan" id="kobo.283.2">In reality, most ML use cases exist in the long tail of smaller datasets and low economies of scale. </span><span class="koboSpan" id="kobo.283.3">A strong focus on data quality is needed to make ML useful when datasets </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">are small:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<span class="koboSpan" id="kobo.285.1"><img alt="Figure 2.3 – ﻿The long tail of ML opportunities" src="image/B19297_02_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.286.1">Figure 2.3 – The long tail of ML opportunities</span></p>
<p><span class="koboSpan" id="kobo.287.1">In the next section, we will explore the challenges in working with smaller and more complex datasets, and how you can </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">overcome them.</span></span></p>
<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/><span class="koboSpan" id="kobo.289.1">Why we need data-centric AI more than ever</span></h1>
<p><span class="koboSpan" id="kobo.290.1">The leading organizations </span><a id="_idIndexMarker052"/><span class="koboSpan" id="kobo.291.1">in AI, such as the Big Nine, have achieved incredible results with ML since the turn of the century, but how is AI being used in the </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">long tail?</span></span></p>
<p><span class="koboSpan" id="kobo.293.1">A 2020 survey published by MIT Sloan Management Review and Boston Consulting Group concluded that most companies struggle to turn their vision for AI into reality. </span><span class="koboSpan" id="kobo.293.2">In a survey of over 3,000 business leaders from 29 industries in 112 countries, 70% of respondents understood how AI can generate business value and 57% had piloted or productionized AI solutions. </span><span class="koboSpan" id="kobo.293.3">However, only 1 in 10 had been able to generate significant financial benefits </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">with AI.</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.295.1">20</span></span></span></p>
<p><span class="koboSpan" id="kobo.296.1">The survey authors found that companies that were realizing significant financial benefits with AI had built their success on </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">two pillars:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.298.1">They had a solid foundation of the right data, technology, </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">and talent.</span></span></li>
<li><span class="koboSpan" id="kobo.300.1">They had defined several effective ways for humans and AI to work and learn together. </span><span class="koboSpan" id="kobo.300.2">In other words, they had created an iterative feedback loop between humans and AI, going from data collection and curation to </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">solution deployment.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.302.1">Why are these two pillars critical to success with ML and AI? </span><span class="koboSpan" id="kobo.302.2">Because the ML model is only a small part of an </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">ML system.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">In 2015, Google researchers Sculley et al.</span><span class="superscript"><span class="koboSpan" id="kobo.305.1">21</span></span><span class="koboSpan" id="kobo.306.1"> published a seminal paper called </span><em class="italic"><span class="koboSpan" id="kobo.307.1">Hidden Technical Debt in Machine Learning Systems</span></em><span class="koboSpan" id="kobo.308.1">, in which they describe how “</span><em class="italic"><span class="koboSpan" id="kobo.309.1">only a small fraction of real-world ML systems are composed of the ML code… the required surrounding infrastructure is vast </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.310.1">and complex.</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">”</span></span></p>
<p><span class="koboSpan" id="kobo.312.1">In traditional information technology jargon, </span><em class="italic"><span class="koboSpan" id="kobo.313.1">technical debt</span></em><span class="koboSpan" id="kobo.314.1"> refers to the long-term costs incurred by cutting corners in the software</span><a id="_idIndexMarker053"/><span class="koboSpan" id="kobo.315.1"> development life cycle. </span><span class="koboSpan" id="kobo.315.2">It’s the hardcoded logic, the missing documentation, the lack of integration with other platforms, inefficient code, and anything else that is a roadblock to better system performance and future improvements. </span><span class="koboSpan" id="kobo.315.3">Technical debt can be “paid down” by removing </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">these issues.</span></span></p>
<p><span class="koboSpan" id="kobo.317.1">ML systems are different in that they can carry technical debt in code, but they also have the added complexity that technical debt may exist in the data components of the system. </span><span class="koboSpan" id="kobo.317.2">Input data is the foundational ingredient in the system and the data is variable. </span><span class="koboSpan" id="kobo.317.3">Because ML models are driven by weighted impacts from many features in both data and code, a change in one variable may change the logical structure of the rest of the model. </span><span class="koboSpan" id="kobo.317.4">This is also known as the </span><a id="_idIndexMarker054"/><span class="koboSpan" id="kobo.318.1">CACE principle: </span><em class="italic"><span class="koboSpan" id="kobo.319.1">Changing Anything </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.320.1">Changes Everything</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.322.1">As illustrated in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.323.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.324.1">.4</span></em><span class="koboSpan" id="kobo.325.1">, a </span><a id="_idIndexMarker055"/><span class="koboSpan" id="kobo.326.1">productionized ML system is much more than the model code. </span><span class="koboSpan" id="kobo.326.2">In a typical ML project, it is estimated that only 5-10% of the overall system is the model code</span><span class="superscript"><span class="koboSpan" id="kobo.327.1">22</span></span><span class="koboSpan" id="kobo.328.1">. </span><span class="koboSpan" id="kobo.328.2">The remaining 90-95% of the solution is related to data </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">and infrastructure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<span class="koboSpan" id="kobo.330.1"><img alt="Figure 2.4 – ML systems are much more than code. Source: Adapted from Sculley et al﻿., 2015" src="image/B19297_02_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.331.1">Figure 2.4 – ML systems are much more than code. </span><span class="koboSpan" id="kobo.331.2">Source: Adapted from Sculley et al., 2015</span></p>
<p><span class="koboSpan" id="kobo.332.1">As Sculley et al. </span><span class="koboSpan" id="kobo.332.2">described, the </span><a id="_idIndexMarker056"/><span class="koboSpan" id="kobo.333.1">data collection and curation activities in an ML solution are often significantly more resource-intensive than direct model development activities. </span><span class="koboSpan" id="kobo.333.2">Given this, data engineering should be a data scientist’s best friend. </span><span class="koboSpan" id="kobo.333.3">Yet, there is a disconnect between the importance of data quality and how most ML solutions are developed </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">in practice.</span></span></p>
<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/><span class="koboSpan" id="kobo.335.1">The cascading effects of data quality</span></h2>
<p><span class="koboSpan" id="kobo.336.1">In 2021, Google researchers </span><a id="_idIndexMarker057"/><span class="koboSpan" id="kobo.337.1">Sambasivan et al.</span><span class="superscript"><span class="koboSpan" id="kobo.338.1">23</span></span><span class="koboSpan" id="kobo.339.1"> conducted a research study of the practices of 53 ML practitioners from the US, India, and East and West Africa working in a variety of industries. </span><span class="koboSpan" id="kobo.339.2">The study participants were selected from high-stakes domains such as healthcare, agriculture, finance, public safety, environmental conversation, </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">and education.</span></span></p>
<p><span class="koboSpan" id="kobo.341.1">The purpose of the study was to identify and describe the downstream impact of data quality on ML systems and present empirical evidence of what they call </span><em class="italic"><span class="koboSpan" id="kobo.342.1">data cascades</span></em><span class="koboSpan" id="kobo.343.1"> – compounding negative effects stemming from data </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">quality issues.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.345.1">Data cascades</span></strong><span class="koboSpan" id="kobo.346.1"> are </span><a id="_idIndexMarker058"/><span class="koboSpan" id="kobo.347.1">caused by conventional model-centric ML practices that undervalue data quality and typically lead to invisible and delayed impacts on model performance – in other words, ML-specific technical debt. </span><span class="koboSpan" id="kobo.347.2">According to the researchers, data cascades are highly prevalent, with 92% of ML practitioners in the study experiencing one or more data cascades in a </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">given project.</span></span></p>
<p><span class="koboSpan" id="kobo.349.1">The causes of data </span><a id="_idIndexMarker059"/><span class="koboSpan" id="kobo.350.1">cascades fit into four categories explained in the </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">following subsections.</span></span></p>
<h3><span class="koboSpan" id="kobo.352.1">The perceived low value of data work and lack of reward systems</span></h3>
<p><span class="koboSpan" id="kobo.353.1">There are often two underlying reasons for the lack of available data in the long tail of </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">ML opportunities:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.355.1">Firstly, the events being modeled are bespoke and rare, so there is a physical limit to the amount of data that can be collected for a given </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">use case</span></span></li>
<li><span class="koboSpan" id="kobo.357.1">Secondly, data collection and curation activities are considered relatively expensive and difficult, especially when they involve </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">manual collection</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.359.1">In truth, most data-related work is not done by data scientists. </span><span class="koboSpan" id="kobo.359.2">The roles directly responsible for creating, collecting, and curating data are often performing these tasks as a secondary duty in their job. </span><span class="koboSpan" id="kobo.359.3">The responsibility of collecting high-quality data is frequently at odds with other duties because of competing priorities, time constraints, technical limitations of collection systems, or simply a lack of understanding of how to carry out good </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">data collection.</span></span></p>
<p><span class="koboSpan" id="kobo.361.1">Take, for example, a hospital nurse who is responsible for a wide variety of tasks relating to the care of patients, some of which are data collection. </span><span class="koboSpan" id="kobo.361.2">High-quality data in healthcare has the potential to create huge benefits for patients and healthcare providers around the world if it can be aggregated and generalized through ML. </span><span class="koboSpan" id="kobo.361.3">However, for the individual nurse, there is more incentive to do the minimum required to document patient status and medical interventions, so more time can be spent on primary patient care. </span><span class="koboSpan" id="kobo.361.4">The typical result of this kind of scenario is suboptimal data collection in terms of depth of detail and consistency </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">of labeling.</span></span></p>
<p><span class="koboSpan" id="kobo.363.1">ML practitioners face a similar challenge further downstream. </span><span class="koboSpan" id="kobo.363.2">Sambasivan et al. </span><span class="koboSpan" id="kobo.363.3">describe how business and project goals such as cost, revenue, time to market, and competitive pressures lead data scientists to hurry through model development, leaving insufficient room for data quality and ethics concerns. </span><span class="koboSpan" id="kobo.363.4">As one practitioner states, </span><em class="italic"><span class="koboSpan" id="kobo.364.1">everyone wants to do the model work, not the </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.365.1">data work</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.367.1">Lack of cross-functional collaboration</span></h3>
<p><span class="koboSpan" id="kobo.368.1">When it comes to</span><a id="_idIndexMarker060"/><span class="koboSpan" id="kobo.369.1"> high-stakes or bespoke ML projects, subject-matter experts are often critical participants in upstream data collection as well as the ultimate consumers of </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">model outputs.</span></span></p>
<p><span class="koboSpan" id="kobo.371.1">On the face of it, subject-matter experts should be very willing to participate actively in ML projects because they get to reap the benefits of useful models. </span><span class="koboSpan" id="kobo.371.2">However, the opposite is often </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">the case.</span></span></p>
<p><span class="koboSpan" id="kobo.373.1">A requirement to collect additional information for ML purposes typically means that data collectors and curators have to work harder to get their job done. </span><span class="koboSpan" id="kobo.373.2">It can be difficult for frontline workers with limited data literacy to appreciate the importance of data collection, and unfortunately, the cascading effect of this conduct shows up much later in the project life cycle – often </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">after deployment.</span></span></p>
<p><span class="koboSpan" id="kobo.375.1">Data scientists should also play a critical role in data collection as they will make many decisions on how to interpret and manipulate datasets during model development. </span><span class="koboSpan" id="kobo.375.2">Therefore, an ML practitioner’s curiosity and willingness to understand the technical and social contexts of a given domain is a critical part of any project’s success. </span><span class="koboSpan" id="kobo.375.3">It is the invisible glue that makes ML solutions relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">and accurate.</span></span></p>
<p><span class="koboSpan" id="kobo.377.1">Unfortunately, data scientists often lack domain-specific expertise and rely on subject-matter experts to validate their interpretation of datasets. </span><span class="koboSpan" id="kobo.377.2">If ML practitioners do not constantly question their assumptions, rely too heavily on their technical expertise, and take the accuracy of input data for granted, they will miss the finer points of the context they’re trying to model. </span><span class="koboSpan" id="kobo.377.3">When this happens, ML projects will suffer from </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">data cascades.</span></span></p>
<p><span class="koboSpan" id="kobo.379.1">Insufficient cross-functional collaboration results in costly project challenges such as additional data collection, misinterpretation of results, and lack of trust in ML as a relevant solution to a </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">given problem.</span></span></p>
<h3><span class="koboSpan" id="kobo.381.1">Educational and knowledge gaps for ML practitioners</span></h3>
<p><span class="koboSpan" id="kobo.382.1">Even the most technically skilled ML practitioners may fail to build useful models for real-life scenarios if they lack end-to-end knowledge of ML pipelines. </span><span class="koboSpan" id="kobo.382.2">Unfortunately, most learning paths for data scientists lack appropriate attention to data </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">engineering practices.</span></span></p>
<p><span class="koboSpan" id="kobo.384.1">Graduate programs and online training courses are built on clean datasets, but real life is full of dirty data. </span><span class="koboSpan" id="kobo.384.2">Data scientists are simply not trained in building ML solutions from scratch, including data collection design, data management, and data governance processes, training data collectors, cleaning dirty data, and building </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">domain knowledge.</span></span></p>
<p><span class="koboSpan" id="kobo.386.1">As a result, data engineering and MLOps practices are poorly understood and under-appreciated by those who are directly responsible for turning raw data into </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">useful insights.</span></span></p>
<h3><span class="koboSpan" id="kobo.388.1">Lack of measurement of and accountability for data quality</span></h3>
<p><span class="koboSpan" id="kobo.389.1">Conventional ML </span><a id="_idIndexMarker061"/><span class="koboSpan" id="kobo.390.1">practices rely on statistical accuracy tests, such as </span><em class="italic"><span class="koboSpan" id="kobo.391.1">precision</span></em><span class="koboSpan" id="kobo.392.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.393.1">recall</span></em><span class="koboSpan" id="kobo.394.1">, as proxies for model </span><em class="italic"><span class="koboSpan" id="kobo.395.1">and</span></em><span class="koboSpan" id="kobo.396.1"> data quality. </span><span class="koboSpan" id="kobo.396.2">These measures don’t provide any direct information on the quality of a dataset as it pertains to representing specific events and relevant situational context. </span><span class="koboSpan" id="kobo.396.3">The lack of standardized approaches for identifying and rectifying data quality issues early in the process makes data improvement work reactive, as opposed to planned and aligned to </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">project goals.</span></span></p>
<p><span class="koboSpan" id="kobo.398.1">The much-used management phrase </span><em class="italic"><span class="koboSpan" id="kobo.399.1">what gets measured gets managed</span></em><span class="koboSpan" id="kobo.400.1"> is also true in a data quality setting. </span><span class="koboSpan" id="kobo.400.2">Without appropriate processes in place for identifying data quality issues, it is difficult to incentivize and assign accountability to individuals for good </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">data collection.</span></span></p>
<p><span class="koboSpan" id="kobo.402.1">The importance of assigning accountability for data quality in high-stakes domains is underpinned by the fact that model accuracy typically has to be very high, based on small datasets. </span><span class="koboSpan" id="kobo.402.2">For example, a poorly performing model in a low-risk and data-rich industry, such as online retailing or digital advertising, can be modified relatively quickly given the automated and persistent nature of </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">data collection.</span></span></p>
<p><span class="koboSpan" id="kobo.404.1">ML models deployed in the long tail are often harder to validate because of a much lower frequency of events. </span><span class="koboSpan" id="kobo.404.2">At the same time, high-stakes domains typically demand a higher model accuracy threshold. </span><span class="koboSpan" id="kobo.404.3">Online advertisers can probably live with an accuracy score of 75%, but a model built for cancer diagnosis typically has to have an error rate of less than 1% to </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">be viable.</span></span></p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/><span class="koboSpan" id="kobo.406.1">Avoiding data cascades and technical debt</span></h2>
<p><span class="koboSpan" id="kobo.407.1">The </span><a id="_idIndexMarker062"/><span class="koboSpan" id="kobo.408.1">pervasiveness of</span><a id="_idIndexMarker063"/><span class="koboSpan" id="kobo.409.1"> data cascades highlights a larger underlying problem: the dominant conventions in ML development are drawn from the practices of </span><em class="italic"><span class="koboSpan" id="kobo.410.1">big data</span></em><span class="koboSpan" id="kobo.411.1"> companies. </span><span class="koboSpan" id="kobo.411.2">These practices have been developed in an environment of plentiful and expendable data where each user has one account</span><span class="superscript"><span class="koboSpan" id="kobo.412.1">24</span></span><span class="koboSpan" id="kobo.413.1">. </span><span class="koboSpan" id="kobo.413.2">Combine this with a culture of </span><em class="italic"><span class="koboSpan" id="kobo.414.1">move fast and break things</span></em><span class="superscript"><span class="koboSpan" id="kobo.415.1">25</span></span><span class="koboSpan" id="kobo.416.1"> while viewing data work as undesirable drudgery, and you have an approach that will fail in most </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">high-stakes domains.</span></span></p>
<p><span class="koboSpan" id="kobo.418.1">The</span><a id="_idIndexMarker064"/><span class="koboSpan" id="kobo.419.1"> cascading effects of poor data are opaque and hard to track in </span><a id="_idIndexMarker065"/><span class="koboSpan" id="kobo.420.1">any standardized way, even though they occur frequently and persistently. </span><span class="koboSpan" id="kobo.420.2">Fortunately, data cascades are also fixable. </span><span class="koboSpan" id="kobo.420.3">Sambasivan et al. </span><span class="koboSpan" id="kobo.420.4">define the concept of </span><em class="italic"><span class="koboSpan" id="kobo.421.1">data excellence</span></em><span class="koboSpan" id="kobo.422.1"> as</span><a id="_idIndexMarker066"/><span class="koboSpan" id="kobo.423.1"> the solution: a cultural shift toward recognizing data management as a core business discipline and establishing the right processes and incentives for those who are a part of the </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">ML pipeline.</span></span></p>
<p><span class="koboSpan" id="kobo.425.1">As data professionals, it’s up to us to decide whether ML should remain a tool for the few or whether it’s time to allow projects with smaller financial value or higher stakes to become viable. </span><span class="koboSpan" id="kobo.425.2">To do this, we must strive for </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">data excellence.</span></span></p>
<p><span class="koboSpan" id="kobo.427.1">Now, let’s summarize the key takeaways from </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">this chapter.</span></span></p>
<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/><span class="koboSpan" id="kobo.429.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.430.1">In this chapter, we reviewed the history of ML to give us a clear understanding of why model-centric ML is the dominant approach today. </span><span class="koboSpan" id="kobo.430.2">We also learned how a model-centric approach limits us from unlocking the potential value tied up in the long tale of </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">ML opportunities.</span></span></p>
<p><span class="koboSpan" id="kobo.432.1">By now, you should have a strong appreciation for why data-centricity is needed for the discipline of ML to achieve its full potential but also recognize that it will require substantial effort to make the shift. </span><span class="koboSpan" id="kobo.432.2">To become an effective data-centric ML practitioner, old habits must be broken and new </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">ones formed.</span></span></p>
<p><span class="koboSpan" id="kobo.434.1">Now, it’s time to start exploring the tools and techniques to make that shift. </span><span class="koboSpan" id="kobo.434.2">In the next chapter, we will discuss the principles of data-centric ML and the techniques and approaches associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">each principle.</span></span></p>
<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/><span class="koboSpan" id="kobo.436.1">References</span></h1>
<ol>
<li><a href="https://www.idc.com/getdoc.jsp?containerId=prUS47560321"><span class="koboSpan" id="kobo.437.1">https://www.idc.com/getdoc.jsp?containerId=prUS47560321</span></a><span class="koboSpan" id="kobo.438.1">, viewed on 23 </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">September 2022</span></span></li>
<li><span class="koboSpan" id="kobo.440.1">Lewis, A. </span><span class="koboSpan" id="kobo.440.2">R., 2006, </span><em class="italic"><span class="koboSpan" id="kobo.441.1">The American Culture of War</span></em><span class="koboSpan" id="kobo.442.1">, Routledge, New </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">York, USA</span></span></li>
<li><a href="https://www.nasa.gov/feature/when-the-computer-wore-a-skirt-langley-s-computers-1935-1970"><span class="koboSpan" id="kobo.444.1">https://www.nasa.gov/feature/when-the-computer-wore-a-skirt-langley-s-computers-1935-1970</span></a><span class="koboSpan" id="kobo.445.1">, viewed on 23 </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">September 2022</span></span></li>
<li><a href="https://www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/"><span class="koboSpan" id="kobo.447.1">https://www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/</span></a><span class="koboSpan" id="kobo.448.1">, viewed on 23 </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">September 2022</span></span></li>
<li><a href="http://large.stanford.edu/courses/2012/ph250/lee1/docs/Excepts_A_Conversation_with_Gordon_Moore.pdf"><span class="koboSpan" id="kobo.450.1">http://large.stanford.edu/courses/2012/ph250/lee1/docs/Excepts_A_Conversation_with_Gordon_Moore.pdf</span></a><span class="koboSpan" id="kobo.451.1">, viewed on 23 </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">September 2022</span></span></li>
<li><a href="https://www.businessinsider.com/ibm-1970-mainframe-specs-are-ridiculous-today-2014-5"><span class="koboSpan" id="kobo.453.1">https://www.businessinsider.com/ibm-1970-mainframe-specs-are-ridiculous-today-2014-5</span></a><span class="koboSpan" id="kobo.454.1">, viewed on 22 </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">September 2022</span></span></li>
<li><a href="https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP3145.html"><span class="koboSpan" id="kobo.456.1">https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP3145.html</span></a><span class="koboSpan" id="kobo.457.1">, viewed on 22 </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">September 2022</span></span></li>
<li><a href="https://www.apple.com/au/iphone-14/specs/"><span class="koboSpan" id="kobo.459.1">https://www.apple.com/au/iphone-14/specs/</span></a><span class="koboSpan" id="kobo.460.1">, viewed on 23 </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">September 2022</span></span></li>
<li><a href="https://www.quickbase.com/articles/timeline-of-database-history"><span class="koboSpan" id="kobo.462.1">https://www.quickbase.com/articles/timeline-of-database-history</span></a><span class="koboSpan" id="kobo.463.1">, viewed on 24 </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">September 2022</span></span></li>
<li><a href="https://www.dataversity.net/brief-history-database-management/"><span class="koboSpan" id="kobo.465.1">https://www.dataversity.net/brief-history-database-management/</span></a><span class="koboSpan" id="kobo.466.1">, viewed on 24 </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">September 2022</span></span></li>
<li><a href="https://www.r-project.org/about.html"><span class="koboSpan" id="kobo.468.1">https://www.r-project.org/about.html</span></a><span class="koboSpan" id="kobo.469.1">, viewed on 24 </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">September 2022</span></span></li>
<li><a href="https://www.historyofdatascience.com/leo-breiman-statistics-at-the-service-of-others/"><span class="koboSpan" id="kobo.471.1">https://www.historyofdatascience.com/leo-breiman-statistics-at-the-service-of-others/</span></a><span class="koboSpan" id="kobo.472.1">, viewed on 24 </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">September 2022</span></span></li>
<li><a href="https://www.image-net.org/about.php"><span class="koboSpan" id="kobo.474.1">https://www.image-net.org/about.php</span></a><span class="koboSpan" id="kobo.475.1">, viewed on 24 </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">September 2022</span></span></li>
<li><a href="https://www.dataversity.net/brief-history-cloud-computing/"><span class="koboSpan" id="kobo.477.1">https://www.dataversity.net/brief-history-cloud-computing/</span></a><span class="koboSpan" id="kobo.478.1">, viewed on 25 </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">September 2022</span></span></li>
<li><a href="https://thenextweb.com/news/2010-2019-the-rise-of-deep-learning"><span class="koboSpan" id="kobo.480.1">https://thenextweb.com/news/2010-2019-the-rise-of-deep-learning</span></a><span class="koboSpan" id="kobo.481.1">, viewed on 25 </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">September 2022</span></span></li>
<li><a href="https://www.dataversity.net/brief-history-data-science/"><span class="koboSpan" id="kobo.483.1">https://www.dataversity.net/brief-history-data-science/</span></a><span class="koboSpan" id="kobo.484.1">, viewed on 25 </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">September 2022</span></span></li>
<li><a href="https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century"><span class="koboSpan" id="kobo.486.1">https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century</span></a><span class="koboSpan" id="kobo.487.1">, viewed on 25 </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">September 2022</span></span></li>
<li><span class="koboSpan" id="kobo.489.1">Webb, A., 2019, </span><em class="italic"><span class="koboSpan" id="kobo.490.1">The Big Nine: How Tech Titans and Their Thinking Machines Could Warp Humanity</span></em><span class="koboSpan" id="kobo.491.1">, Hachette Book Group, New </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">York, USA</span></span></li>
<li><a href="https://spectrum.ieee.org/andrew-ng-data-centric-ai"><span class="koboSpan" id="kobo.493.1">https://spectrum.ieee.org/andrew-ng-data-centric-ai</span></a><span class="koboSpan" id="kobo.494.1">, viewed on 25 </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">September 2022</span></span></li>
<li><span class="koboSpan" id="kobo.496.1">Ransbotham, S., Khodabandeh, S., Kiron, D., Candelon, F., Chu, M., and LaFountain, B., </span><em class="italic"><span class="koboSpan" id="kobo.497.1">Expanding AI’s Impact With Organizational Learning</span></em><span class="koboSpan" id="kobo.498.1">, MIT Sloan Management Review and Boston Consulting Group, </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">October 2020</span></span></li>
<li><a href="https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf"><span class="koboSpan" id="kobo.500.1">https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</span></a><span class="koboSpan" id="kobo.501.1">, Sculley et al., 2015, viewed 23 </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">July 2022,</span></span></li>
<li><span class="koboSpan" id="kobo.503.1">Yang, K., 2022, </span><em class="italic"><span class="koboSpan" id="kobo.504.1">Landing AI – Moving Beyond the Software </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.505.1">Industry</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">, </span></span><a href="https://community.ai-infrastructure.org/public/videos/landing-ai-ai-moving-beyond-the-software-industry-2022-09-30"><span class="No-Break"><span class="koboSpan" id="kobo.507.1">https://community.ai-infrastructure.org/public/videos/landing-ai-ai-moving-beyond-the-software-industry-2022-09-30</span></span></a></li>
<li><span class="koboSpan" id="kobo.508.1">Sambasivan, N., Kapania, S., Highfill, H., Akrong, D., Paritosh, P., Aroyo, L., 2021, </span><em class="italic"><span class="koboSpan" id="kobo.509.1">Everyone wants to do the model work, not the data work:</span></em> <em class="italic"><span class="koboSpan" id="kobo.510.1">Data Cascades in </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.511.1">High-Stakes AI</span></em></span></li>
<li><a href="https://hbr.org/2019/01/the-era-of-move-fast-and-break-things-is-over"><span class="koboSpan" id="kobo.512.1">https://hbr.org/2019/01/the-era-of-move-fast-and-break-things-is-over</span></a><span class="koboSpan" id="kobo.513.1">, viewed on 8 </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">October 2022</span></span></li>
</ol>
</div>


<div class="Content" id="_idContainer022">
<h1 id="_idParaDest-43" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor042"/><span class="koboSpan" id="kobo.1.1">Part 2: The Building Blocks of Data-Centric ML</span></h1>
<p><span class="koboSpan" id="kobo.2.1">In this part, we lay the groundwork for data-centric ML with four key principles that underpin this approach, giving you essential context before exploring specific techniques. </span><span class="koboSpan" id="kobo.2.2">Then we explore human-centric and non-technical approaches to data quality, examining how expert knowledge, trained labelers, and clear instructions can enhance your </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">ML output.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B19297_03.xhtml#_idTextAnchor043"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 3</span></em></a><em class="italic"><span class="koboSpan" id="kobo.7.1">, Principles of Data-Centric ML</span></em></li>
<li><a href="B19297_04.xhtml#_idTextAnchor056"><em class="italic"><span class="koboSpan" id="kobo.8.1">Chapter 4</span></em></a><em class="italic"><span class="koboSpan" id="kobo.9.1">, Data Labeling Is a Collaborative Process</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer023">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer024">
</div>
</div>
</body></html>