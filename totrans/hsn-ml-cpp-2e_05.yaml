- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Anomaly detection** is where we search for unexpected values in a given dataset.
    An anomaly is a deviation in system behavior or data value from the standard or
    expected value. Anomalies are also known as outliers, errors, deviations, and
    exceptions. They can occur in data that’s of a diverse nature and structure as
    a result of technical failures, accidents, deliberate hacks, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many methods and algorithms we can use to search for anomalies in
    various types of data. These methods use different approaches to solve the same
    problem. There are unsupervised, supervised, and semi-supervised algorithms. However,
    in practice, unsupervised methods are the most popular. The **unsupervised anomaly
    detection** technique detects anomalies in unlabeled test datasets, under the
    assumption that most of the dataset is normal. It does this by searching for data
    points that are unlikely to fit the rest of the dataset. Unsupervised algorithms
    are more popular because of the nature of anomaly events, which are significantly
    rare compared to normal or expected data, so it is usually very difficult to get
    a suitably labeled dataset for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Broadly speaking, anomaly detection applies to a wide range of areas, such as
    intrusion detection, fraud detection, fault detection, health monitoring, event
    detection (in sensor networks), and the detection of environmental disruptions.
    Often, anomaly detection is used as the preprocessing step for data preparation,
    before the data is passed on to other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: So, in this chapter, we’ll discuss the most popular unsupervised algorithms
    for anomaly detection and its applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the applications of anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning approaches for anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of using different C++ libraries for anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The list of software that you’ll need to complete the examples in this chapter
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Shogun-toolbox` library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Shark-ML` library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dlib` library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PlotCpp` library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern C++ compiler with C++17 support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CMake build system version >= 3.8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be found at the following GitHub repo:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter05](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter05)'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the applications of anomaly detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two areas in data analysis look for anomalies: **outlier detection** and **novelty
    detection**.'
  prefs: []
  type: TYPE_NORMAL
- en: A *new object* or *novelty* is an object that differs in its properties from
    objects in the training dataset. Unlike an outlier, the new object is not in the
    dataset itself, but it can appear at any point after a system has started working.
    Its task is to detect when it appears. For example, if we were to analyze existing
    temperature measurements and identify abnormally high or low values, then we would
    be detecting outliers. On the other hand, if we were to create an algorithm that,
    for every new measurement, evaluates the temperature’s similarity to past values
    and identifies significantly unusual ones, then we would be detecting novelties.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reasons for outliers appearing include data errors, the presence of noise,
    misclassified objects, and foreign objects from other datasets or distributions.
    Let’s explain two of the most obscure types of outliers: data errors and data
    from different distributions. Data errors can broadly refer to inaccuracies in
    measurements, rounding errors, and incorrect entries. An example of an object
    belonging to a different distribution is measurements that have come from a broken
    sensor. This is because these values will belong to a range that may be different
    from what was expected.'
  prefs: []
  type: TYPE_NORMAL
- en: Novelties usually appear as a result of fundamentally new object behavior. For
    example, if our objects are computer system behavior descriptions, then after
    a virus has penetrated the computer and deleted some information from these descriptions,
    they will be rendered as novelties. Another example of a novelty could be a new
    group of customers that behave differently from others but have some similarities
    to other customers. The main feature of novelty objects is that they are new,
    in that it’s impossible to have information about all possible virus infections
    or breakdowns in the training set. Creating such a training dataset is a complicated
    process and often does not make sense. However, fortunately, we can obtain a large
    enough dataset by focusing on the ordinary (regular) operations of the system
    or mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, the task of anomaly detection is similar to the task of **classification**,
    but there is an essential difference: **class imbalances**. For example, equipment
    failures (anomalies) are significantly rarer than having the equipment functioning
    normally.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can observe anomalies in different kinds of data. In the following graph,
    we can see an example of anomalies in a numeric series:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Example of anomalies in a numeric series](img/B19849_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Example of anomalies in a numeric series
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see anomalies in graphs; these anomalies can
    be as edges as well as vertices (see elements marked with a lighter color):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Anomalies in graphs](img/B19849_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Anomalies in graphs
  prefs: []
  type: TYPE_NORMAL
- en: 'The following text shows anomalies in a sequence of characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AABBCCCAABBCCCAACABBBCCCAABB`'
  prefs: []
  type: TYPE_NORMAL
- en: The quality or performance of anomaly detection tasks can be estimated, just
    like classification tasks can, by using, for example, **Area Under the Receiver
    Operating Characteristic** **Curve** (**AUC-ROC**).
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed what anomalies are, so let’s see what approaches there are
    to detect them.
  prefs: []
  type: TYPE_NORMAL
- en: Learning approaches for anomaly detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll look at the most popular and straightforward methods
    we can use for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting anomalies with statistical tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Statistical tests** are usually used to catch extreme values for individual
    features. The general name for this type of test is **extreme-value analysis**.
    An example of such a test is the use of the Z-score measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi></mrow><mi>δ</mi></mfrac></mrow></mrow></math>](img/1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B19849_Formula_022.png) is a sample from the dataset, *µ* is
    the mean of all samples from the dataset, and ![](img/B19849_Formula_032.png)
    is the standard deviation of samples in the dataset. A *Z*-score value tells us
    how many standard deviations a data point is distant from the mean. So, by choosing
    the appropriate threshold value, we can filter some values as anomalies. Any data
    points with a *Z*-score greater than the threshold will be considered anomalies
    or unusual values in the dataset. Typically, values above `3` or below `-3` are
    considered anomalies, but you can adjust this threshold based on your specific
    project requirements. The following graph shows which values from some type of
    normally distributed data can be treated as anomalies or outliers by using the
    Z-score test:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Z-score anomaly detection](img/B19849_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Z-score anomaly detection
  prefs: []
  type: TYPE_NORMAL
- en: 'One important concept that we should mention is extreme values—the maximum
    and minimum values from the given dataset. It is important to understand that
    extreme values and anomalies are different concepts. The following is a small
    data sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[1, 39, 2, 1, 101, 2, 1, 100, 1, 3, 101, 1, 3, 100, 101,` `100, 100]`'
  prefs: []
  type: TYPE_NORMAL
- en: We can consider the value `39` as an anomaly, but not because it is a maximal
    or minimal value. It is crucial to understand that an anomaly needn’t be an extreme
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Although extreme values are not anomalies in general, in some cases, we can
    adapt methods of extreme-value analysis to the needs of anomaly detection. However,
    this depends on the task at hand and should be carefully analyzed by **machine
    learning** (**ML**) practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting anomalies with the Local Outlier Factor method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The distance measurement-based methods are widely used for solving different
    ML problems, as well as for anomaly detection. These methods assume that there
    is a specific metric in the object space that helps us find anomalies. The general
    assumption when we use distance-based methods for anomaly detection is that the
    anomaly only has a few neighbors, while a normal point has many. Therefore, for
    example, the distance to the *k*th neighbor can serve as a good measure of anomalies,
    as reflected in the **Local Outlier Factor** (**LOF**) method. This method is
    based on estimating the density of objects that have been checked for anomalies.
    Objects lying in the areas of lowest density are considered anomalies or outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of the LOF method over other methods is that it works in conjunction
    with the local density of objects. Therefore, the LOF effectively identifies outliers
    even when there are objects of different classes in the dataset that may not be
    considered anomalies during training. For example, let’s assume that there is
    a distance, *k*-distance (*A*), from the object *(A)* to the *k*th nearest neighbor.
    Note that the set of *k* nearest neighbors includes all objects within this distance.
    We denote the set of *k* nearest neighbors as *N*k*(A)*. This distance is used
    to determine the reachability distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_042.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If point *A* lies among *k* neighbors of point *B*, then *reachability-distance*
    will be equal to the *k-distance* of point *B*. Otherwise, it will be equal to
    the exact distance between points *A* and *B*, which is given by the `dist` function.
    The local reachability density of an object *A* is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_052.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Local reachability density is the inverse of the average reachability distance
    of the object, *A*, from its neighbors. Note that this is not the average reachability
    distance of neighbors from *A* (which, by definition, should have been k-distance(*A*)),
    but is the distance at which *A* can be reached from its neighbors. The local
    reachability densities are then compared with the local reachability densities
    of the neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_062.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The provided formula gives the average local reachability density of the neighbors,
    divided by the local reachability density of the object itself:'
  prefs: []
  type: TYPE_NORMAL
- en: A value of approximately `1` means that the object can be compared with its
    neighbors (and therefore it is not an outlier)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value less than `1` indicates a dense area (objects have many neighbors)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value significantly larger than `1` indicates anomalies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The disadvantage of this method is the fact that the resulting values are difficult
    to interpret. A value of `1` or less indicates that a point is purely internal,
    but there is no clear rule by which a point will be an outlier. In one dataset,
    the value `1.1` may indicate an outlier. However, in another dataset with a different
    set of parameters (for example, if there is data with sharp local fluctuations),
    the value `2` may also indicate internal objects. These differences can also occur
    within a single dataset due to the locality of the method.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting anomalies with isolation forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea of an isolation forest is based on the `anomaly_score` value of the
    algorithm, which is the depth of the leaves in the constructed tree. The following
    formula shows how the anomaly score can be calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_072.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_082.png) is the path length of the observation,
    ![](img/B19849_Formula_091.png), ![](img/B19849_Formula_102.png) is an average
    of ![](img/B19849_Formula_113.png) from a collection of isolation trees, ![](img/B19849_Formula_124.png)
    is the average path length of the unsuccessful search in a binary search tree,
    and ![](img/B19849_Formula_132.png) is the number of external nodes.
  prefs: []
  type: TYPE_NORMAL
- en: We’re assuming that it is common for anomalies to appear in leaves with a low
    depth, which is close to the root, but for regular objects, the tree will build
    several more levels. The number of such levels is proportional to the size of
    the cluster. Consequently, `anomaly_score` is proportional to the points lying
    in it.
  prefs: []
  type: TYPE_NORMAL
- en: 'This assumption means that objects from clusters of small sizes (which are
    potentially anomalies) will have a lower `anomaly_score` value than those from
    clusters of regular data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Isolation forest visualization](img/B19849_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Isolation forest visualization
  prefs: []
  type: TYPE_NORMAL
- en: The isolation forest method is widely used and implemented in various libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting anomalies with One-Class Support Vector Machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The support vector method is a binary classification method based on using a
    **hyperplane** to divide objects into classes. The dimensions of the hyperplane
    are always chosen so that they’re less than the dimensions of the original space.
    In ![](img/B19849_Formula_143.png), for example, a hyperplane is an ordinary two-dimensional
    plane. The distance from the hyperplane to each class should be as short as possible.
    The vectors that are closest to the separating hyperplane are called support vectors.
    In practice, cases where the data can be divided by a hyperplane—in other words,
    linear cases—are quite rare. In this case, all the elements of the training dataset
    are embedded in the higher dimension space, ![](img/B19849_Formula_152.png), using
    a special mapping. In this case, the mapping is chosen so that in the new space,
    ![](img/B19849_Formula_152.png), the dataset is linearly separable. Such mapping
    is based on kernel functions and is usually named the Kernel trick; it will be
    discussed more precisely in [*Chapter 6*](B19849_06.xhtml#_idTextAnchor301).
  prefs: []
  type: TYPE_NORMAL
- en: '**One-Class Support Vector Machine** (**OCSVM**) is an adaptation of the support
    vector method that focuses on anomaly detection. OCSVM differs from the standard
    version of **Support Vector Machine** (**SVM**) in a way that the resulting optimization
    problem includes an improvement for determining a small percentage of predetermined
    anomalous values, which allows this method to be used to detect anomalies. These
    anomalous values lie between the starting point and the optimal separating hyperplane.
    All other data belonging to the same class falls on the opposite side of the optimal
    separating hyperplane.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s also another type of OCSVM method that uses a spherical, instead of
    a planar (or linear), approach. The algorithm obtains a spherical boundary, in
    the feature space, around the data. The volume of this hypersphere is minimized
    to reduce the effect of incorporating outliers in the solution.
  prefs: []
  type: TYPE_NORMAL
- en: A **spherical mapping** is appropriate when the data has a spherical shape,
    such as when the data points are distributed evenly around the origin. A **planar
    (or linear) mapping** is more appropriate when the data has a planar shape, such
    as when the data points lie on a line or plane. Also, other kernel functions can
    be used to map the data into a higher-dimensional space where it is linearly separable.
    The choice of kernel function depends on the nature of the data.
  prefs: []
  type: TYPE_NORMAL
- en: OCSVM assigns a label, which is the distance from the test data point to the
    optimal hyperplane. Positive values in the OCSVM output represent normal behavior
    (with higher values representing greater normality), while negative values represent
    anomalous behavior (the lower the value, the more significant the anomaly).
  prefs: []
  type: TYPE_NORMAL
- en: In order to assign a label, the OCSVM first trains on a dataset that consists
    of only normal or expected behavior. This dataset is called the **positive class**.
    The OCSVM then tries to find a hyperplane that maximizes the distance between
    the positive class and the origin. This hyperplane is called the **decision boundary**.
  prefs: []
  type: TYPE_NORMAL
- en: Once the decision boundary has been found, any new data point that falls outside
    of this boundary is considered an anomaly or outlier. The OCSVM assigns a label
    of `anomaly` to these data points.
  prefs: []
  type: TYPE_NORMAL
- en: Density estimation approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most popular approaches to anomaly detection is density estimation,
    which involves estimating the probability distribution of normal data and then
    flagging observations as anomalies if they fall outside the expected range. The
    basic idea behind density estimation is to fit a model to the data that represents
    the underlying distribution of normal behavior. This model can be a simple parametric
    distribution such as **Gaussian** or a more complex non-parametric model such
    as **kernel density estimation** (**KDE**). Once the model is trained on normal
    data, it can be used to estimate the density of new observations. Observations
    with low density are considered anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several advantages to using a density estimation approach:'
  prefs: []
  type: TYPE_NORMAL
- en: It is flexible and can handle a wide range of data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not require labeled data for training, making it suitable for **unsupervised**
    **learning** (**UL**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can detect both point anomalies (observations that are significantly different
    from the rest) and contextual anomalies (observations that do not follow the normal
    pattern within a specific context)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, there are also some challenges with this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: The choice of model and parameters can affect the performance of the algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers can influence the estimated density, leading to false positives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm may not be able to detect anomalies that are not well represented
    in the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, density estimation is a powerful tool for anomaly detection that can
    be customized to suit the specific needs of an application. By carefully selecting
    the model and tuning the parameters, it is possible to achieve high accuracy and
    precision in detecting anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Using multivariate Gaussian distribution for anomaly detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s assume we have some samples ![](img/B19849_Formula_172.png) in a dataset
    and that they are labeled and normally distributed (Gaussian distribution). In
    such a case, we can use distribution properties to detect anomalies. Let’s assume
    that the function ![](img/B19849_Formula_182.png) gives us the probability of
    a sample being normal. A high probability corresponds to a regular sample, while
    a low probability corresponds to an anomaly. We can, therefore, choose thresholds
    to distinguish between regular values and anomalies with the following **anomaly**
    **model formula**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If [![](img/B19849_Formula_201.png)] and ![](img/B19849_Formula_212.png) follows
    the Gaussian distribution with the mean, ![](img/B19849_Formula_222.png), and
    the variance, ![](img/B19849_Formula_231.png), it is denoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_242.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following formula gives the probability of ![](img/B19849_Formula_251.png)
    in a Gaussian distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_261.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_272.png) is the mean and ![](img/B19849_Formula_283.png)
    is the variance (![](img/B19849_Formula_291.png) is the standard deviation). This
    formula is known as parametrized **probability density function** (**PDF**), and
    it describes the relative likelihood of different outcomes in a continuous random
    variable. It is used to model the distribution of a random variable and provides
    information about the probability of observing a specific value or range of values
    for that variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll introduce an example of the general approach we follow for anomaly
    detection with Gaussian distribution density estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we’re given a new example, ![](img/B19849_Formula_302.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the features, ![](img/B19849_Formula_312.png), that are regular, meaning
    they determine anomalous behavior.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the ![](img/B19849_Formula_322.png)and ![](img/B19849_Formula_33.png) parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute ![](img/B19849_Formula_34.png) using an equation to calculate the probability
    of ![](img/B19849_Formula_35.png) in a Gaussian distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine if ![](img/B19849_Formula_04.png) is an anomaly by comparing it with
    the threshold, ![](img/B19849_Formula_37.png); see the anomaly model formula.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following graph shows an example of Gaussian distribution density estimation
    for normally distributed data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Gaussian density estimation visualization](img/B19849_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Gaussian density estimation visualization
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, we assume that selected features are independent, but usually,
    in real data, there are some correlations between them. In such a case, we should
    use a multivariate Gaussian distribution model instead of a univariate one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following formula gives the probability of ![](img/B19849_Formula_38.png)
    in a multivariate Gaussian distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_391.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B19849_Formula_40.png) is the mean, ![](img/B19849_Formula_41.png)
    is the correlation matrix, and ![](img/B19849_Formula_421.png) is the determinant
    of the matrix, ![](img/B19849_Formula_41.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graphs shows the difference between the univariate and the multivariate
    Gaussian distribution estimation models for a dataset with correlated data. Notice
    how distribution boundaries cover the regular data with a darker color, while
    anomalies are marked with a lighter color:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Univariate and multivariate Gaussian distributions](img/B19849_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Univariate and multivariate Gaussian distributions
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the multivariate Gaussian distribution can take into account
    correlations in the data and adapt its shape to them. This characteristic allows
    us to detect anomalies correctly for types of data whose distribution follows
    a Gaussian (normal) distribution shape. Also, we can see one of the advantages
    of this approach: results can be easily visualized in two or three dimensions,
    providing a clear understanding of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at another approach for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: KDE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the KDE approach, our goal is to approximate a complex mixture of random
    distributions around point samples into a single function. So, the main idea is
    to center a probability distribution function at each data point and then take
    their average. It means that each discrete point in our dataset is replaced by
    an extended probability distribution, called a **kernel**. The probability density
    at any given point is then estimated as the sum of the kernel functions centered
    at each discrete point. If the point is close to many other points its estimated
    probability density will be larger than if it is far away from any sample point.
    This approach can be used to find anomalies as points where the estimated density
    is minimal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we can formulate the KDE function for univariate kernels as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, Kh is a smoothed kernel defined with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, h is the bandwidth parameter that defines the width of a kernel. The bandwidth
    parameter controls the degree of smoothness or roughness of the kernel function.
    A larger bandwidth results in a smoother function, while a smaller bandwidth leads
    to a more rugged one. Choosing the right bandwidth is crucial for achieving good
    generalization performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'K can be, for example, a Gaussian kernel function, which is the most common
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_471.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graph shows a plot of KDE of Gaussian distribution made from
    individual kernels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – KDE of Gaussian distribution](img/B19849_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – KDE of Gaussian distribution
  prefs: []
  type: TYPE_NORMAL
- en: You can see in this graph that the density value will be the maximum for points
    around the value `3` because most sample points are located in this region. Another
    cluster of points is located around point `8`, but their density is quite a bit
    smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Before training the KDE model, it is important to preprocess the data to ensure
    that it is suitable for the KDE algorithm. This may include scaling the data so
    that all features have similar ranges, removing outliers or anomalies, and transforming
    the data if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Handling high-dimensional data efficiently in KDE can be challenging due to
    the **curse of dimensionality** (**CoD**). One approach is to use dimensionality
    reduction techniques such as **principal component analysis** (**PCA**) or **t-distributed
    stochastic neighbor embedding** (**t-SNE**) to reduce the number of dimensions
    before applying KDE. Another approach is to use sparse KDE algorithms that only
    consider a subset of data points when calculating the density estimate for a given
    location.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will look at another approach for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Density estimation trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **density estimation tree** (**DET**) algorithm also can be used to detect
    anomalies by thresholding the density value for certain sample points. This is
    a non-parametric technique based on decision tree construction. The main advantage
    of this algorithm is the fast analytical complexity of density estimation at any
    given point, its *O(log n)* time, where n is the number of points in the tree.
    The tree is constructed iteratively in a top-to-bottom approach. Each leaf, *t*,
    is divided into two sub-leaves *t*l and *t*r by maximizing residual gain *s* that
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_481.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *R(t)* is the tree loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_491.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*N* is the number of candidates the *t* leaf contains, and *V* is the leaf’s
    volume. Then, the actual density for a leaf *t* can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_501.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, to estimate a density value for a given point, we have to determine to which
    leaf it belongs and then get the leaf’s density.
  prefs: []
  type: TYPE_NORMAL
- en: In the current section, we discussed various anomaly detection approaches, and
    in the following sections, we will see how to use various C++ libraries to deal
    with the anomaly detection task.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of using different C++ libraries for anomaly detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll look at some examples of how to implement the algorithms
    we described previously for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: C++ implementation of the isolation forest algorithm for anomaly detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Isolation forest algorithms** can be easily implemented in pure C++ because
    their logic is pretty straightforward. Also, there are no implementations of this
    algorithm in popular C++ libraries. Let’s assume that our implementation will
    only be used with two-dimensional data. We are going to detect anomalies in a
    range of samples where each sample contains the same number of features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because our dataset is large enough, we can define a wrapper for the actual
    data container. This allows us to reduce the number of copy operations we perform
    on the actual data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `DatasetRange` type holds a reference to the vector of `Sample` type objects
    and to the container of indices that point to the samples in the dataset. These
    indices define the exact dataset objects that this `DatasetRange` object points
    to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define the elements of the isolation tree, with the first one being
    the `Node` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This type is a regular tree-node structure. The following members are specific
    to the isolation tree algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '`split_col`: This is the index of the feature column where the algorithm caused
    a split'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split_value`: This is the value of the feature where the algorithm caused
    a split'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size`: This is the number of underlying items for the node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_external`: This is the flag that indicates whether the node is a leaf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking the `Node` type as a basis, we can define the procedure of building an
    isolation tree. We aggregate this procedure with the auxiliary `IsolationTree`
    type. Because the current algorithm is based on random splits, the auxiliary data
    is the random engine object.
  prefs: []
  type: TYPE_NORMAL
- en: 'We only need to initialize this object once, and then it will be shared among
    all tree-type objects. This approach allows us to make the results of the algorithm
    reproducible in the case of constant seeding. Furthermore, it makes debugging
    the randomized algorithm much simpler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll do the most critical work in the `MakeIsolationTree()` method,
    which is used in the constructor to initialize the root data member:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Initially, we checked the termination conditions to stop the splitting process.
    If we meet them, we return a new node marked as an external leaf. Otherwise, we
    start splitting the passed data range. For splitting, we randomly select the `feature`
    column and determine the unique values of the selected feature. Then, we randomly
    select a value from an interval between the `max` and `min` values among the feature
    values from all the samples. After we make these random selections, we compare
    the values of the selected splitting feature to all the samples from the input
    data range and put their indices into two lists. One list is for values higher
    than the splitting values, while another list is for values that are lower than
    them. Then, we return a new tree node initialized with references to the left
    and right nodes, which are initialized with recursive calls to the `MakeIsolationTree()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another vital method of the `IsolationTree` type is the `PathLength()` method.
    We use it for anomaly score calculations. It takes the sample as an input parameter
    and returns the amortized path length to the corresponding tree leaf from the
    root node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `PathLength()` method finds the leaf node during tree traversal based on
    sample feature values. These values are used to select a tree traversal direction
    based on the current node-splitting values. During each step, this method also
    increases the resulting height. The result of this method is a sum of the actual
    tree traversal height and the value returned from the call to the `CalcC()` function,
    which then returns the average path’s length of unsuccessful searches in a binary
    search tree of equal height to the leaf node. The `CalcC()` function can be implemented
    in the following way, according to the formula from the original paper, which
    describes the isolation forest algorithm (you can find a reference to this in
    the *Further* *reading* section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The final part of the algorithm’s implementation is the creation of the forest.
    The forest is an array of trees built from a limited number of samples, randomly
    chosen from the original dataset. The number of samples used to build the tree
    is a hyperparameter of this algorithm. Furthermore, this implementation uses heuristics
    as the stopping criteria, in that it is a maximum tree height `hlim` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how it is used in the tree-building procedure. The `hlim` value is
    calculated only once, and the following code shows this. Moreover, it is based
    on the number of samples that are used to build a single tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The tree forest is built in the constructor of the `IsolationForest` type.
    We also calculated the value of the average path length of the unsuccessful search
    in a binary search tree for all of the samples in the constructor. We use this
    forest in the `AnomalyScore()` method for the actual process of anomaly detection.
    It implements the formula for the anomaly score value for a given sample. It returns
    a value that can be interpreted in the following way: if the returned value is
    close to `1`, then the sample has anomalous features, while if the value is less
    than `0.5`, then we can assume that the sample is a normal one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how we can use this algorithm. Furthermore, it uses
    `Dlib` primitives for the dataset’s representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we converted and merged the given datasets for the
    container that’s suitable for our algorithm. Then, we initialized the object of
    the `IsolationForest` type, which immediately builds the isolation forest with
    the following hyperparameters: the number of trees is 100, and the number of samples
    used for one tree is 50.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we called the `AnomalyScore()` method for each sample from the dataset
    in order to detect anomalies with thresholds and return their values. In the following
    graph, we can see the result of anomaly detection after using the isolation forest
    algorithm. The points labeled as `1 cls` are the anomalies:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.8 – Anomaly detect\uFEFFion with isolation forest algorithm](img/B19849_05_08.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Anomaly detection with isolation forest algorithm
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to implement the isolation forest algorithm
    from scratch. The following section will show you how to use the `Dlib` library
    for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Dlib library for anomaly detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Dlib` library provides a couple of implemented algorithms that we can
    use for anomaly detection: the OCSVM model and the multivariate Gaussian model.'
  prefs: []
  type: TYPE_NORMAL
- en: OCSVM with Dlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is only one algorithm that’s implemented in the `Dlib` library straight
    out of the box: OCSVM. There is a `svm_one_class_trainer` class in this library
    that can be used to train the corresponding algorithm, which should be configured
    with a kernel object, and the `nu` parameter, which controls the smoothness (in
    other words, the degree to which it controls the ratio between generalization
    and overfitting) of the solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most widely used kernel is based on the Gaussian distribution and is known
    as the `radial_basis_kernel` class. Typically, we represent datasets in the `Dlib`
    library as a C++ vector of separate samples. Therefore before using this `trainer`
    object, we have to convert a matrix dataset into a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the training process is a decision function object of the `decision_function<kernel_type>`
    class that we can use for single sample classification. Objects of this type can
    be used as a regular function. The result of a decision function is the distance
    from the normal class boundary, so the most distant samples can be classified
    as anomalies. The following graph shows an example of how the OCSVM algorithm
    from the `Dlib` library works. Note that the dots labeled as `1 cls` correspond
    to anomalies:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.9 – Anomaly\uFEFF detection with Dlib OCSVM implementation](img/B19849_05_09.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Anomaly detection with Dlib OCSVM implementation
  prefs: []
  type: TYPE_NORMAL
- en: We can see that OCSVM solved the task well and detected anomalies that are very
    interpretable. In the next section, we will see how to use a multivariate Gaussian
    model to detect anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate Gaussian model with Dlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the linear algebra facilities of the `D``lib` library (or any other library,
    for that matter), we can implement anomaly detection with the multivariate Gaussian
    distribution approach. The following example shows how to implement this approach
    with `Dlib` linear algebra routines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The idea of this approach is to define a function that returns the probability
    of appearing, given a sample in a dataset. To implement such a function, we calculate
    the statistical characteristics of the training dataset. In the first step, we
    calculate the mean values of each feature and store them in the one-dimensional
    matrix. Then, we calculate the covariance matrix for the training samples using
    the formula for the correlation matrix that was given in the prior theoretical
    section named *Density estimation approach*. Next, we determine the correlation
    matrix determinant and inverse version. We define a lambda function named `prob`
    to calculate the probability of a single sample using the formula provided in
    the *Using multivariate Gaussian* *distribution* section.
  prefs: []
  type: TYPE_NORMAL
- en: For large datasets, the computational complexity of calculating the covariance
    matrix can become a significant factor in the overall runtime of an ML model.
    Furthermore, optimizing the calculation of the covariance matrix for large datasets
    requires a combination of techniques, including sparsity, parallelization, approximation,
    and efficient algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We also define a probability threshold to separate anomalies. It determines
    the boundary between normal and anomalous behavior, and it plays a crucial role
    in the classification of samples as anomalies or normal. Engineers must carefully
    consider their application’s requirements and adjust the threshold accordingly
    to achieve the desired level of sensitivity. For example, in security applications
    where false alarms are costly, a higher threshold might be preferred to minimize
    false positives. In contrast, in medical diagnoses where missing a potential anomaly
    could have serious consequences, a lower threshold might be more appropriate to
    ensure that no true anomalies go undetected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we iterate over all the examples (including the training and testing
    datasets) to find out how the algorithm separates regular samples from anomalies.
    In the following graph, we can see the result of this separation. The dots labeled
    as `1 cls` are anomalies:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.10 – Anomaly detection w\uFEFFith Dlib multivariate Gaussian distribution](img/B19849_05_10.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Anomaly detection with Dlib multivariate Gaussian distribution
  prefs: []
  type: TYPE_NORMAL
- en: We see that this approach found fewer anomalies than the previous one, so you
    should be aware that some methods will not work well with your data and it will
    make sense to try different methods. In the following section, we will see how
    to use a multivariate Gaussian model from the `mlpack` library for the same task.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate Gaussian model with mlpack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We already discussed in the previous chapter the `GMM` and the `EMFit` classes
    that exist in the `mlpack` library. The **Expectation-Maximization with Fit**
    (**EMFit**) algorithm is an ML technique used to estimate the parameters of a
    **gaussian mixture model** (**GMM**). It works by iteratively optimizing the parameters
    to fit the data. We can use them not only for solving clustering tasks but also
    for anomaly detection. There will only the one difference: we have to specify
    only one cluster for the training. So, GMM class initialization will look like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The initializations for the `KMeans` and the `EMFit` algorithms will be the
    same as in the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The values for `max_iterations` and the convergence tolerance variables influence
    the training process by determining how long the algorithm runs and when it stops.
    A higher number of trials may lead to more accurate results but also increase
    computational time. The convergence tolerance determines how close the parameters
    must be to their previous values before the algorithm stops. If the tolerance
    is too low, the algorithm may never converge, while if it is too high, it may
    converge to a suboptimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can use the `Probability` method of the `gmm` object to test some
    new data. The only new action we have to take is to define a probability threshold
    that will be used to check if a new sample belongs to the original data distribution
    or if it’s an anomaly. This can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Having this threshold, the usage of the `Probability` method will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we defined a lambda function that can be applied to any dataset defined
    as a matrix. In this function, we have used a simple loop over all samples in
    which we applied the `Probability` method for a single sample and compared the
    returned value with the threshold. If the probability value is too low, we mark
    the sample as an anomaly by adding its coordinates to the `plotting_cluster[1]`
    object. To plot the anomaly detection result, we used the same approach as was
    described in the previous chapter. The following code shows how to use the function
    we defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We applied the `detect` function to both sets of data: the `normal` one that
    was used for the training and the new one, `test`. You can see the anomaly detection
    result in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.11 – Anomaly detection with \uFEFFmlpack multivariate Gaussian distribution](img/B19849_05_11.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Anomaly detection with mlpack multivariate Gaussian distribution
  prefs: []
  type: TYPE_NORMAL
- en: The two outliers were detected. You can try to change the probability threshold
    to see how the decision boundary will be changed and what objects will be classified
    as anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will see how to use the KDE algorithm implementation
    from the `mlpack` library.
  prefs: []
  type: TYPE_NORMAL
- en: KDE with mlpack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The KDE algorithm in the `mlpack` library is implemented in the `KDE` class.
    This class can be specialized with several template parameters; the most important
    ones are `KernelType`, `MetricType`, and `TreeType`. Let’s use the Gaussian kernel,
    the Euclidean distance as the metric, and KD-Tree for the tree type. A tree data
    structure is used to optimize the algorithm’s computational complexity. For each
    query point, the algorithm will apply a kernel function to each reference point,
    so the computational complexity can be *O(N^2)* in the naive implementation for
    *N* query points and *N* reference points. The tree optimization avoids many similar
    calculations, because kernel function values decrease with distance, but it also
    introduces some level of approximation. The following code snippet shows how to
    define a `KDE` object for our sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Due to the approximations used in the algorithm, the API allows us to define
    relative and absolute error tolerances. Relative and absolute error tolerances
    control the level of approximation in the KDE estimate. A higher tolerance allows
    for more approximation, which can reduce computational complexity but also decrease
    accuracy. Conversely, a lower tolerance requires more computation but can result
    in a more accurate estimate.
  prefs: []
  type: TYPE_NORMAL
- en: The **relative error tolerance** parameter specifies the maximum relative error
    allowed between the true density and the estimated density at any point. It is
    used to determine the optimal bandwidth for the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: The **absolute error tolerance** parameter sets the maximum absolute error allowed
    between the true density and the estimated density over the entire domain. It
    can be used to ensure that the estimated density is within a certain range of
    the true density.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our sample, we defined only the absolute one. The next step is to train
    our algorithm object with the normal data without anomalies; it can be done as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that different algorithms in the `mlpack` library use mostly the
    same API. Then, we can define a function to classify the given data as normal
    or as an anomaly. The following code shows its definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We defined a lambda function that takes the data matrix and passes it to the
    `Evaluate` method of the `kde` object. This method evaluated and assigned density
    value estimation for every sample in the given data matrix. Then, we just compared
    those estimations with the `density_threshold` value to decide if the sample was
    normal or an anomaly. Samples with low-density values were classified as anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: To select an optimal threshold, you need to balance the trade-off between sensitivity
    and specificity based on your specific use case. If you prioritize detecting all
    anomalies, you may want to set a lower threshold to increase the number of true
    positives, even if it means accepting more false positives. Conversely, if you
    prioritize minimizing false alarms, you might choose a higher threshold, which
    could miss some anomalies but reduce the number of false positives. In practice,
    selecting an optimal density threshold often involves experimenting with different
    values and evaluating the results using metrics such as precision, recall, and
    F1 score. Additionally, domain knowledge and expert input can help guide the selection
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function also prepares data for plotting in the same manner as we did
    earlier. The following code shows how to plot two datasets with normal and outlier
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the anomaly detection result in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.12 – Anomaly detection with KDE from \uFEFFmlpack](img/B19849_05_12.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Anomaly detection with KDE from mlpack
  prefs: []
  type: TYPE_NORMAL
- en: We can see that using the KDE method, we can find the two outliers, as we detected
    in the previous sections. The points labeled with `1 cls` are outliers. By changing
    the density threshold, you can see how the decision boundary will be changed.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will see how to use the DET algorithm implementation
    from the `mlpack` library.
  prefs: []
  type: TYPE_NORMAL
- en: DET with mlpack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The DET method from the `mlpack` library is implemented in the `DTree` class.
    To start working with it, we have to make a copy of the training normal data because
    an object of the `DTree` class will change the input data order. The data ordering
    is changed because `mlpack` creates a tree data structure directly on the given
    data object. The following code snippet shows how to define such an object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Due to the input data reordering the algorithms, the API is required to provide
    a mapping of the indices that will show the relation of new indices to the old
    ones. Such mapping can be initialized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we just stored the original relation of the data indices to the input
    data. Later, this mapping will be updated by the algorithm. Now, can build the
    DET by calling the `Grow` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The two main parameters of the `Grow` method are `max_leaf_size` and `min_leaf_size`.
    They should be turned manually after a series of experiments or with some prior
    knowledge of the dataset characteristics. It can be tricky to estimate those parameters
    with automated techniques such as cross-validation because, for anomaly detection
    tasks, we don’t usually have enough data marked as anomalies. So, the values for
    this example were chosen manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having an initialized DET, we can use the `ComputeValue` method to estimate
    a density for some given data sample. If we choose a density threshold value,
    we can detect anomalies just by comparison with this value. We used the same approach
    in other algorithms too. The following code snippet shows how to use a threshold
    to distinguish between normal and anomalous data and build a data structure for
    result plotting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We defined a `detect` function that simply iterates over the input data matrix
    columns and applies the `ComputeValue` method for every given sample to get the
    density estimate. Then, this function compares a value with `density_threshold`,
    and if the density is big enough, puts the sample in the first plotting cluster.
    Otherwise, the sample comes to the second plotting cluster. We can apply this
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `normal` and `test` are matrices with the normal and anomalous data samples,
    correspondingly. The following graph shows the detection result plotting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Anomaly detection with DET algorithm](img/B19849_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Anomaly detection with DET algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'You may notice that this method classified more data points as anomalies compared
    to previous methods. To change this detection result, you can play with three
    parameters: the density threshold and leaf `min` and `max` sizes. This method
    with more complex turning may be useful for datasets where you don’t know the
    data distribution rule (the kernel form) or it’s hard to write code for it. The
    same goes for when you have normal data with several clusters with different distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we examined anomalies in data. We discussed several approaches
    to anomaly detection and looked at two kinds of anomalies: outliers and novelties.
    We considered the fact that anomaly detection is primarily a UL problem, but despite
    this, some algorithms require labeled data, while others are semi-supervised.
    The reason for this is that, generally, there is a tiny number of positive examples
    (that is, anomalous samples) and a large number of negative examples (that is,
    standard samples) in anomaly detection tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we don’t usually have enough positive samples to train algorithms.
    That is why some solutions use labeled data to improve algorithm generalization
    and precision. On the contrary, **supervised learning** (**SL**) usually requires
    a large number of positive and negative examples, and their distribution needs
    to be balanced.
  prefs: []
  type: TYPE_NORMAL
- en: Also, notice that the task of detecting anomalies does not have a single formulation
    and that it is often interpreted differently, depending on the nature of the data
    and the goal of the concrete task. Moreover, choosing the correct anomaly detection
    method depends primarily on the task, data, and available a priori information.
    We also learned that different libraries can give slightly different results,
    even for the same algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will discuss dimension reduction methods. Such
    methods help us to reduce the dimensionality of data with high dimensionality
    into a new representation of data with lower dimensionality while preserving essential
    information from the original data.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Anomaly detection learning resources: [https://github.com/yzhao062/anomaly-detection-resources](https://github.com/yzhao062/anomaly-detection-resources)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Outlier Detection with One-Class SVMs: An Application to Melanoma* *Prognosis*:
    [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041295/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041295/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Isolation forest: [https://feitonyliu.files.wordpress.com/2009/07/liu-iforest.pdf](https://feitonyliu.files.wordpress.com/2009/07/liu-iforest.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ram, Parikshit & Gray, Alexander. (2011). *Density estimation trees*. *Proceedings
    of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*.
    627-635\. 10.1145/2020408.2020507: [https://www.researchgate.net/publication/221654618_Density_estimation_trees](https://www.researchgate.net/publication/221654618_Density_estimation_trees)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tutorial on KDE: [https://faculty.washington.edu/yenchic/18W_425/Lec6_hist_KDE.pdf](https://faculty.washington.edu/yenchic/18W_425/Lec6_hist_KDE.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
