- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Anomaly Detection
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常检测
- en: '**Anomaly detection** is where we search for unexpected values in a given dataset.
    An anomaly is a deviation in system behavior or data value from the standard or
    expected value. Anomalies are also known as outliers, errors, deviations, and
    exceptions. They can occur in data that’s of a diverse nature and structure as
    a result of technical failures, accidents, deliberate hacks, and more.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**异常检测**是我们在一个给定的数据集中寻找意外值的地方。异常是系统行为或数据值与标准或预期值之间的偏差。异常也被称为异常值、错误、偏差和例外。它们可能由于技术故障、事故、故意黑客攻击等原因，出现在性质和结构多样化的数据中。'
- en: There are many methods and algorithms we can use to search for anomalies in
    various types of data. These methods use different approaches to solve the same
    problem. There are unsupervised, supervised, and semi-supervised algorithms. However,
    in practice, unsupervised methods are the most popular. The **unsupervised anomaly
    detection** technique detects anomalies in unlabeled test datasets, under the
    assumption that most of the dataset is normal. It does this by searching for data
    points that are unlikely to fit the rest of the dataset. Unsupervised algorithms
    are more popular because of the nature of anomaly events, which are significantly
    rare compared to normal or expected data, so it is usually very difficult to get
    a suitably labeled dataset for anomaly detection.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用许多方法和算法来搜索各种类型数据中的异常。这些方法使用不同的方法来解决相同的问题。有无监督、监督和半监督算法。然而，在实践中，无监督方法是最受欢迎的。**无监督异常检测**技术检测未标记的测试数据集中的异常，假设数据集的大部分是正常的。它是通过寻找不太可能适合其余数据集的数据点来做到这一点的。无监督算法之所以更受欢迎，是因为异常事件的发生频率与正常或预期数据相比显著较低，因此通常很难获得适合异常检测的适当标记的数据集。
- en: Broadly speaking, anomaly detection applies to a wide range of areas, such as
    intrusion detection, fraud detection, fault detection, health monitoring, event
    detection (in sensor networks), and the detection of environmental disruptions.
    Often, anomaly detection is used as the preprocessing step for data preparation,
    before the data is passed on to other algorithms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义上讲，异常检测适用于广泛的领域，如入侵检测、欺诈检测、故障检测、健康监测、事件检测（在传感器网络中）以及环境破坏的检测。通常，异常检测被用作数据准备的前处理步骤，在数据传递给其他算法之前。
- en: So, in this chapter, we’ll discuss the most popular unsupervised algorithms
    for anomaly detection and its applications.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将讨论最流行的无监督异常检测算法及其应用。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Exploring the applications of anomaly detection
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索异常检测的应用
- en: Learning approaches for anomaly detection
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测的学习方法
- en: Examples of using different C++ libraries for anomaly detection
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的 C++ 库进行异常检测的示例
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The list of software that you’ll need to complete the examples in this chapter
    is as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章示例所需的软件列表如下：
- en: '`Shogun-toolbox` library'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Shogun-toolbox` 库'
- en: '`Shark-ML` library'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Shark-ML` 库'
- en: '`Dlib` library'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dlib` 库'
- en: '`PlotCpp` library'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PlotCpp` 库'
- en: Modern C++ compiler with C++17 support
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 C++17 的现代 C++ 编译器
- en: CMake build system version >= 3.8
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMake 构建系统版本 >= 3.8
- en: 'The code files for this chapter can be found at the following GitHub repo:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter05](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter05)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在以下 GitHub 仓库中找到：[https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter05](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter05)
- en: Exploring the applications of anomaly detection
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索异常检测的应用
- en: 'Two areas in data analysis look for anomalies: **outlier detection** and **novelty
    detection**.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析中的两个领域寻找异常：**异常值检测**和**新颖性检测**。
- en: A *new object* or *novelty* is an object that differs in its properties from
    objects in the training dataset. Unlike an outlier, the new object is not in the
    dataset itself, but it can appear at any point after a system has started working.
    Its task is to detect when it appears. For example, if we were to analyze existing
    temperature measurements and identify abnormally high or low values, then we would
    be detecting outliers. On the other hand, if we were to create an algorithm that,
    for every new measurement, evaluates the temperature’s similarity to past values
    and identifies significantly unusual ones, then we would be detecting novelties.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *新对象* 或 *新颖性* 是一个与训练数据集中的对象在属性上不同的对象。与异常不同，新对象本身不在数据集中，但它可以在系统开始工作后的任何时刻出现。它的任务是检测其出现。例如，如果我们分析现有的温度测量值并识别异常高或低的值，那么我们就是在检测异常。另一方面，如果我们创建一个算法，对于每次新的测量，评估温度与过去值的相似性并识别显著异常的值，那么我们就是在检测新颖性。
- en: 'The reasons for outliers appearing include data errors, the presence of noise,
    misclassified objects, and foreign objects from other datasets or distributions.
    Let’s explain two of the most obscure types of outliers: data errors and data
    from different distributions. Data errors can broadly refer to inaccuracies in
    measurements, rounding errors, and incorrect entries. An example of an object
    belonging to a different distribution is measurements that have come from a broken
    sensor. This is because these values will belong to a range that may be different
    from what was expected.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 异常出现的原因包括数据错误、噪声的存在、误分类的对象，以及来自其他数据集或分布的外来对象。让我们解释两种最不为人知的异常类型：数据错误和来自不同分布的数据。数据错误可以广泛地指代测量不准确、舍入误差和不正确的输入。属于不同分布的对象的例子是来自损坏传感器的测量值。这是因为这些值将属于可能与预期不同的范围。
- en: Novelties usually appear as a result of fundamentally new object behavior. For
    example, if our objects are computer system behavior descriptions, then after
    a virus has penetrated the computer and deleted some information from these descriptions,
    they will be rendered as novelties. Another example of a novelty could be a new
    group of customers that behave differently from others but have some similarities
    to other customers. The main feature of novelty objects is that they are new,
    in that it’s impossible to have information about all possible virus infections
    or breakdowns in the training set. Creating such a training dataset is a complicated
    process and often does not make sense. However, fortunately, we can obtain a large
    enough dataset by focusing on the ordinary (regular) operations of the system
    or mechanism.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 新颖性通常是由于根本新的对象行为而产生的。例如，如果我们的对象是计算机系统行为描述，那么在病毒侵入计算机并从这些描述中删除一些信息后，它们将被视为新颖性。另一个新颖性的例子可能是一群与其他客户行为不同但与其他客户有相似之处的客户。新颖性对象的主要特征是它们是新的，因为在训练集中不可能有关于所有可能的病毒感染或故障的信息。创建这样的训练数据集是一个复杂的过程，通常也没有意义。然而，幸运的是，我们可以通过关注系统或机制的普通（常规）操作来获得足够大的数据集。
- en: 'Often, the task of anomaly detection is similar to the task of **classification**,
    but there is an essential difference: **class imbalances**. For example, equipment
    failures (anomalies) are significantly rarer than having the equipment functioning
    normally.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，异常检测的任务与分类的任务相似，但有一个本质的区别：**类别不平衡**。例如，设备故障（异常）比设备正常工作的情况要罕见得多。
- en: 'We can observe anomalies in different kinds of data. In the following graph,
    we can see an example of anomalies in a numeric series:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在不同类型的数据中观察到异常。在下面的图中，我们可以看到一个数值序列中异常的例子：
- en: '![Figure 5.1 – Example of anomalies in a numeric series](img/B19849_05_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 数值序列中异常的例子](img/B19849_05_01.jpg)'
- en: Figure 5.1 – Example of anomalies in a numeric series
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 数值序列中异常的例子
- en: 'In the following diagram, we can see anomalies in graphs; these anomalies can
    be as edges as well as vertices (see elements marked with a lighter color):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们可以看到图中存在的异常；这些异常可以是边也可以是顶点（见用较浅颜色标记的元素）：
- en: '![Figure 5.2 – Anomalies in graphs](img/B19849_05_02.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 图中的异常](img/B19849_05_02.jpg)'
- en: Figure 5.2 – Anomalies in graphs
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 图中的异常
- en: 'The following text shows anomalies in a sequence of characters:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的文本展示了字符序列中的异常：
- en: '`AABBCCCAABBCCCAACABBBCCCAABB`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`AABBCCCAABBCCCAACABBBCCCAABB`'
- en: The quality or performance of anomaly detection tasks can be estimated, just
    like classification tasks can, by using, for example, **Area Under the Receiver
    Operating Characteristic** **Curve** (**AUC-ROC**).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用，例如**受试者工作特征曲线下面积**（**AUC-ROC**）来估计异常检测任务的品质或性能，就像分类任务可以一样。
- en: We have discussed what anomalies are, so let’s see what approaches there are
    to detect them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了什么是异常，那么让我们看看有哪些方法可以用来检测它们。
- en: Learning approaches for anomaly detection
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常检测的学习方法
- en: In this section, we’ll look at the most popular and straightforward methods
    we can use for anomaly detection.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨我们用于异常检测的最流行和最直接的方法。
- en: Detecting anomalies with statistical tests
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用统计测试检测异常
- en: '**Statistical tests** are usually used to catch extreme values for individual
    features. The general name for this type of test is **extreme-value analysis**.
    An example of such a test is the use of the Z-score measure:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**统计测试**通常用于捕捉单个特征的极端值。这类测试的通用名称是**极端值分析**。这种测试的一个例子是使用Z分数度量：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi></mrow><mi>δ</mi></mfrac></mrow></mrow></math>](img/1.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi></mrow><mi>δ</mi></mfrac></mrow></mrow></math>](img/1.png)'
- en: 'Here, ![](img/B19849_Formula_022.png) is a sample from the dataset, *µ* is
    the mean of all samples from the dataset, and ![](img/B19849_Formula_032.png)
    is the standard deviation of samples in the dataset. A *Z*-score value tells us
    how many standard deviations a data point is distant from the mean. So, by choosing
    the appropriate threshold value, we can filter some values as anomalies. Any data
    points with a *Z*-score greater than the threshold will be considered anomalies
    or unusual values in the dataset. Typically, values above `3` or below `-3` are
    considered anomalies, but you can adjust this threshold based on your specific
    project requirements. The following graph shows which values from some type of
    normally distributed data can be treated as anomalies or outliers by using the
    Z-score test:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<img src="img/B19849_Formula_022.png" alt=""/>](img/B19849_Formula_022.png)是数据集的一个样本，*µ*是数据集中所有样本的平均值，![<img
    src="img/B19849_Formula_032.png" alt=""/>](img/B19849_Formula_032.png)是数据集中样本的标准差。一个*Z*分数值告诉我们数据点与平均值有多少个标准差。因此，通过选择适当的阈值值，我们可以过滤掉一些异常值。任何*Z*分数大于阈值的点将被视为异常值或数据集中的异常值。通常，大于`3`或小于`-3`的值被视为异常，但你可以根据你特定的项目需求调整这个阈值。以下图表显示了使用Z分数测试可以将哪些来自某种正态分布数据的值视为异常或离群值：
- en: '![Figure 5.3 – Z-score anomaly detection](img/B19849_05_03.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – Z分数异常检测](img/B19849_05_03.jpg)'
- en: Figure 5.3 – Z-score anomaly detection
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – Z分数异常检测
- en: 'One important concept that we should mention is extreme values—the maximum
    and minimum values from the given dataset. It is important to understand that
    extreme values and anomalies are different concepts. The following is a small
    data sample:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该提到的一个重要概念是极端值——给定数据集的最大值和最小值。重要的是要理解，极端值和异常是不同的概念。以下是一个小的数据样本：
- en: '`[1, 39, 2, 1, 101, 2, 1, 100, 1, 3, 101, 1, 3, 100, 101,` `100, 100]`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`[1, 39, 2, 1, 101, 2, 1, 100, 1, 3, 101, 1, 3, 100, 101,` `100, 100]`'
- en: We can consider the value `39` as an anomaly, but not because it is a maximal
    or minimal value. It is crucial to understand that an anomaly needn’t be an extreme
    value.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`39`视为异常值，但不是因为它是最大或最小值。重要的是要理解，异常不一定是极端值。
- en: Although extreme values are not anomalies in general, in some cases, we can
    adapt methods of extreme-value analysis to the needs of anomaly detection. However,
    this depends on the task at hand and should be carefully analyzed by **machine
    learning** (**ML**) practitioners.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然极端值通常不是异常，但在某些情况下，我们可以将极端值分析方法适应于异常检测的需求。然而，这取决于具体任务，并且应该由**机器学习**（**ML**）从业者仔细分析。
- en: Detecting anomalies with the Local Outlier Factor method
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用局部离群因子方法检测异常
- en: The distance measurement-based methods are widely used for solving different
    ML problems, as well as for anomaly detection. These methods assume that there
    is a specific metric in the object space that helps us find anomalies. The general
    assumption when we use distance-based methods for anomaly detection is that the
    anomaly only has a few neighbors, while a normal point has many. Therefore, for
    example, the distance to the *k*th neighbor can serve as a good measure of anomalies,
    as reflected in the **Local Outlier Factor** (**LOF**) method. This method is
    based on estimating the density of objects that have been checked for anomalies.
    Objects lying in the areas of lowest density are considered anomalies or outliers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于距离测量的方法被广泛用于解决不同的机器学习问题，以及用于异常检测。这些方法假设在对象空间中存在一个特定的度量标准，有助于我们找到异常。当我们使用基于距离的方法进行异常检测时的一般假设是，异常值只有少数邻居，而正常点有许多邻居。因此，例如，到第
    *k* 个邻居的距离可以作为一个很好的异常值度量，正如在**局部异常因子**（**LOF**）方法中所反映的那样。这种方法基于估计已检查异常的对象密度。位于最低密度区域的对象被认为是异常值或异常。
- en: 'The advantage of the LOF method over other methods is that it works in conjunction
    with the local density of objects. Therefore, the LOF effectively identifies outliers
    even when there are objects of different classes in the dataset that may not be
    considered anomalies during training. For example, let’s assume that there is
    a distance, *k*-distance (*A*), from the object *(A)* to the *k*th nearest neighbor.
    Note that the set of *k* nearest neighbors includes all objects within this distance.
    We denote the set of *k* nearest neighbors as *N*k*(A)*. This distance is used
    to determine the reachability distance:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他方法相比，LOF 方法的优势在于它与对象的局部密度相结合。因此，LOF 可以有效地识别异常值，即使在数据集中存在不同类别的对象，这些对象在训练期间可能不被视为异常。例如，假设从对象
    *(A)* 到其第 *k* 个最近邻居的距离为 *k*-distance (*A*)。请注意，*k* 个最近邻居的集合包括所有在这个距离内的对象。我们将 *k*
    个最近邻居的集合表示为 *N*k*(A)*。这个距离用于确定可达距离：
- en: '![](img/B19849_Formula_042.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_042.jpg)'
- en: 'If point *A* lies among *k* neighbors of point *B*, then *reachability-distance*
    will be equal to the *k-distance* of point *B*. Otherwise, it will be equal to
    the exact distance between points *A* and *B*, which is given by the `dist` function.
    The local reachability density of an object *A* is defined as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果点 *A* 位于点 *B* 的 *k* 个邻居之间，则 *可达距离* 将等于点 *B* 的 *k*-distance。否则，它将等于点 *A* 和
    *B* 之间的确切距离，该距离由 `dist` 函数给出。对象 *A* 的局部可达密度定义为以下：
- en: '![](img/B19849_Formula_052.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_052.jpg)'
- en: 'Local reachability density is the inverse of the average reachability distance
    of the object, *A*, from its neighbors. Note that this is not the average reachability
    distance of neighbors from *A* (which, by definition, should have been k-distance(*A*)),
    but is the distance at which *A* can be reached from its neighbors. The local
    reachability densities are then compared with the local reachability densities
    of the neighbors:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 局部可达密度是对象 *A* 从其邻居的平均可达距离的倒数。请注意，这并不是从 *A* 到邻居的平均可达距离（根据定义，应该是 k-distance(*A*)），而是
    *A* 可以从其邻居到达的距离。然后比较局部可达密度与邻居的局部可达密度：
- en: '![](img/B19849_Formula_062.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_062.jpg)'
- en: 'The provided formula gives the average local reachability density of the neighbors,
    divided by the local reachability density of the object itself:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的公式给出了邻居的平均局部可达密度，除以对象的局部可达密度：
- en: A value of approximately `1` means that the object can be compared with its
    neighbors (and therefore it is not an outlier)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大约为 `1` 的值意味着对象可以与其邻居进行比较（因此它不是异常值）
- en: A value less than `1` indicates a dense area (objects have many neighbors)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小于 `1` 的值表示密集区域（对象有许多邻居）
- en: A value significantly larger than `1` indicates anomalies
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明显大于 `1` 的值表示异常
- en: The disadvantage of this method is the fact that the resulting values are difficult
    to interpret. A value of `1` or less indicates that a point is purely internal,
    but there is no clear rule by which a point will be an outlier. In one dataset,
    the value `1.1` may indicate an outlier. However, in another dataset with a different
    set of parameters (for example, if there is data with sharp local fluctuations),
    the value `2` may also indicate internal objects. These differences can also occur
    within a single dataset due to the locality of the method.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是结果值难以解释。`1`或更小的值表示一个点完全是内部的，但没有明确的规则可以确定一个点是否为异常值。在一个数据集中，`1.1`的值可能表示异常值。然而，在另一个具有不同参数集的数据集中（例如，如果存在具有尖锐局部波动的数据），`2`的值也可能表示内部对象。这些差异也可能由于方法的局部性而在单个数据集中发生。
- en: Detecting anomalies with isolation forest
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用独立森林检测异常
- en: 'The idea of an isolation forest is based on the `anomaly_score` value of the
    algorithm, which is the depth of the leaves in the constructed tree. The following
    formula shows how the anomaly score can be calculated:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 独立森林（Isolation Forest）这一想法基于算法的`anomaly_score`值，即构建的树中叶子的深度。以下公式展示了如何计算异常分数：
- en: '![](img/B19849_Formula_072.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_072.jpg)'
- en: Here, ![](img/B19849_Formula_082.png) is the path length of the observation,
    ![](img/B19849_Formula_091.png), ![](img/B19849_Formula_102.png) is an average
    of ![](img/B19849_Formula_113.png) from a collection of isolation trees, ![](img/B19849_Formula_124.png)
    is the average path length of the unsuccessful search in a binary search tree,
    and ![](img/B19849_Formula_132.png) is the number of external nodes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B19849_Formula_082.png)是观察值的路径长度，![](img/B19849_Formula_091.png)，![](img/B19849_Formula_102.png)是从一组隔离树中得到的![](img/B19849_Formula_113.png)的平均值，![](img/B19849_Formula_124.png)是在二叉搜索树中搜索失败的平均路径长度，而![](img/B19849_Formula_132.png)是外部节点的数量。
- en: We’re assuming that it is common for anomalies to appear in leaves with a low
    depth, which is close to the root, but for regular objects, the tree will build
    several more levels. The number of such levels is proportional to the size of
    the cluster. Consequently, `anomaly_score` is proportional to the points lying
    in it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设异常通常出现在深度较低的叶子中，这些叶子靠近根节点，但对于常规对象，树将构建更多几个层级。这种层级的数量与簇的大小成正比。因此，`anomaly_score`与位于其中的点的数量成正比。
- en: 'This assumption means that objects from clusters of small sizes (which are
    potentially anomalies) will have a lower `anomaly_score` value than those from
    clusters of regular data:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个假设意味着来自小尺寸簇（可能是异常值）的对象将比来自常规数据簇的对象具有更低的`anomaly_score`值：
- en: '![Figure 5.4 – Isolation forest visualization](img/B19849_05_04.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – 独立森林可视化](img/B19849_05_04.jpg)'
- en: Figure 5.4 – Isolation forest visualization
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 独立森林可视化
- en: The isolation forest method is widely used and implemented in various libraries.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 独立森林方法被广泛使用并在各种库中得到实现。
- en: Detecting anomalies with One-Class Support Vector Machine
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用单类支持向量机检测异常
- en: The support vector method is a binary classification method based on using a
    **hyperplane** to divide objects into classes. The dimensions of the hyperplane
    are always chosen so that they’re less than the dimensions of the original space.
    In ![](img/B19849_Formula_143.png), for example, a hyperplane is an ordinary two-dimensional
    plane. The distance from the hyperplane to each class should be as short as possible.
    The vectors that are closest to the separating hyperplane are called support vectors.
    In practice, cases where the data can be divided by a hyperplane—in other words,
    linear cases—are quite rare. In this case, all the elements of the training dataset
    are embedded in the higher dimension space, ![](img/B19849_Formula_152.png), using
    a special mapping. In this case, the mapping is chosen so that in the new space,
    ![](img/B19849_Formula_152.png), the dataset is linearly separable. Such mapping
    is based on kernel functions and is usually named the Kernel trick; it will be
    discussed more precisely in [*Chapter 6*](B19849_06.xhtml#_idTextAnchor301).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量法是一种基于使用**超平面**将对象划分为类别的二分类方法。超平面的维度总是选择小于原始空间的维度。例如，在![](img/B19849_Formula_143.png)中，超平面是一个普通的二维平面。超平面到每个类别的距离应尽可能短。最接近分离超平面的向量被称为支持向量。在实践中，可以通过超平面划分的数据——换句话说，线性情况——相当罕见。在这种情况下，训练数据集的所有元素都嵌入到更高维的空间![](img/B19849_Formula_152.png)中，使用特殊的映射。在这种情况下，映射的选择使得在新空间![](img/B19849_Formula_152.png)中，数据集是线性可分的。这种映射基于核函数，通常称为核技巧；它将在[*第6章*](B19849_06.xhtml#_idTextAnchor301)中更详细地讨论。
- en: '**One-Class Support Vector Machine** (**OCSVM**) is an adaptation of the support
    vector method that focuses on anomaly detection. OCSVM differs from the standard
    version of **Support Vector Machine** (**SVM**) in a way that the resulting optimization
    problem includes an improvement for determining a small percentage of predetermined
    anomalous values, which allows this method to be used to detect anomalies. These
    anomalous values lie between the starting point and the optimal separating hyperplane.
    All other data belonging to the same class falls on the opposite side of the optimal
    separating hyperplane.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**单类支持向量机**（**OCSVM**）是对支持向量方法的一种改进，专注于异常检测。OCSVM与标准版本的支持向量机（**SVM**）的不同之处在于，所得到的优化问题包括对确定一小部分预定的异常值的改进，这使得该方法可用于检测异常。这些异常值位于起点和最优分离超平面之间。属于同一类的所有其他数据都落在最优分离超平面的另一侧。'
- en: There’s also another type of OCSVM method that uses a spherical, instead of
    a planar (or linear), approach. The algorithm obtains a spherical boundary, in
    the feature space, around the data. The volume of this hypersphere is minimized
    to reduce the effect of incorporating outliers in the solution.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一种OCSVM方法，它使用的是球形方法，而不是平面（或线性）方法。该算法在特征空间中获取数据周围的球形边界。通过最小化这个超球体的体积来减少在解中包含异常值的影响。
- en: A **spherical mapping** is appropriate when the data has a spherical shape,
    such as when the data points are distributed evenly around the origin. A **planar
    (or linear) mapping** is more appropriate when the data has a planar shape, such
    as when the data points lie on a line or plane. Also, other kernel functions can
    be used to map the data into a higher-dimensional space where it is linearly separable.
    The choice of kernel function depends on the nature of the data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据具有球形形状时，例如数据点均匀分布在原点周围时，**球形映射**是合适的。当数据具有平面形状时，例如数据点位于一条线或平面上时，**平面（或线性）映射**更合适。此外，还可以使用其他核函数将数据映射到更高维的空间，使其线性可分。核函数的选择取决于数据的性质。
- en: OCSVM assigns a label, which is the distance from the test data point to the
    optimal hyperplane. Positive values in the OCSVM output represent normal behavior
    (with higher values representing greater normality), while negative values represent
    anomalous behavior (the lower the value, the more significant the anomaly).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: OCSVM分配一个标签，即测试数据点到最优超平面的距离。OCSVM输出中的正值表示正常行为（值越高表示正常性越强），而负值表示异常行为（值越低表示异常越显著）。
- en: In order to assign a label, the OCSVM first trains on a dataset that consists
    of only normal or expected behavior. This dataset is called the **positive class**.
    The OCSVM then tries to find a hyperplane that maximizes the distance between
    the positive class and the origin. This hyperplane is called the **decision boundary**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分配标签，OCSVM首先在一个只包含正常或预期行为的数据集上训练。这个数据集被称为**正类**。然后OCSVM试图找到一个超平面，该超平面最大化正类与原点之间的距离。这个超平面被称为**决策边界**。
- en: Once the decision boundary has been found, any new data point that falls outside
    of this boundary is considered an anomaly or outlier. The OCSVM assigns a label
    of `anomaly` to these data points.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到决策边界，任何落在这个边界之外的新数据点都被认为是异常或离群值。OCSVM将这些数据点分配一个`异常`标签。
- en: Density estimation approach
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 密度估计方法
- en: One of the most popular approaches to anomaly detection is density estimation,
    which involves estimating the probability distribution of normal data and then
    flagging observations as anomalies if they fall outside the expected range. The
    basic idea behind density estimation is to fit a model to the data that represents
    the underlying distribution of normal behavior. This model can be a simple parametric
    distribution such as **Gaussian** or a more complex non-parametric model such
    as **kernel density estimation** (**KDE**). Once the model is trained on normal
    data, it can be used to estimate the density of new observations. Observations
    with low density are considered anomalies.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测中最受欢迎的方法之一是密度估计，它涉及估计正常数据的概率分布，然后如果观察值落在预期范围之外，则将其标记为异常。密度估计背后的基本思想是拟合一个模型来表示正常行为的潜在分布。这个模型可以是一个简单的参数分布，如**高斯**，或者一个更复杂的非参数模型，如**核密度估计**（**KDE**）。一旦模型在正常数据上训练，就可以用它来估计新观察值的密度。密度低的观察值被认为是异常。
- en: 'There are several advantages to using a density estimation approach:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用密度估计方法有几个优点：
- en: It is flexible and can handle a wide range of data types
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是灵活的，可以处理各种数据类型
- en: It does not require labeled data for training, making it suitable for **unsupervised**
    **learning** (**UL**)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不需要训练标签数据，这使得它适合于**无监督学习**（**UL**）
- en: It can detect both point anomalies (observations that are significantly different
    from the rest) and contextual anomalies (observations that do not follow the normal
    pattern within a specific context)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以检测点异常（与其它观察值显著不同的观察值）和上下文异常（在特定上下文中不遵循正常模式的观察值）
- en: 'However, there are also some challenges with this approach:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法也存在一些挑战：
- en: The choice of model and parameters can affect the performance of the algorithm
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型和参数的选择可能会影响算法的性能
- en: Outliers can influence the estimated density, leading to false positives
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离群值可能会影响估计的密度，导致假阳性
- en: The algorithm may not be able to detect anomalies that are not well represented
    in the training data
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法可能无法检测在训练数据中表征不佳的异常
- en: Overall, density estimation is a powerful tool for anomaly detection that can
    be customized to suit the specific needs of an application. By carefully selecting
    the model and tuning the parameters, it is possible to achieve high accuracy and
    precision in detecting anomalies.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，密度估计是异常检测的一个强大工具，可以根据应用的具体需求进行定制。通过仔细选择模型和调整参数，可以在检测异常时实现高准确性和精确度。
- en: Using multivariate Gaussian distribution for anomaly detection
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多元高斯分布进行异常检测
- en: 'Let’s assume we have some samples ![](img/B19849_Formula_172.png) in a dataset
    and that they are labeled and normally distributed (Gaussian distribution). In
    such a case, we can use distribution properties to detect anomalies. Let’s assume
    that the function ![](img/B19849_Formula_182.png) gives us the probability of
    a sample being normal. A high probability corresponds to a regular sample, while
    a low probability corresponds to an anomaly. We can, therefore, choose thresholds
    to distinguish between regular values and anomalies with the following **anomaly**
    **model formula**:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在数据集中有一些样本 ![](img/B19849_Formula_172.png)，并且它们被标记并且呈正态分布（高斯分布）。在这种情况下，我们可以使用分布特性来检测异常。假设函数
    ![](img/B19849_Formula_182.png)给出了一个样本为正常的概率。高概率对应于常规样本，而低概率对应于异常。因此，我们可以选择阈值来区分常规值和异常，以下是一个**异常模型公式**：
- en: '![](img/B19849_Formula_19.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_19.jpg)'
- en: 'If [![](img/B19849_Formula_201.png)] and ![](img/B19849_Formula_212.png) follows
    the Gaussian distribution with the mean, ![](img/B19849_Formula_222.png), and
    the variance, ![](img/B19849_Formula_231.png), it is denoted as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 [![](img/B19849_Formula_201.png)] 和 ![](img/B19849_Formula_212.png) 符合具有均值
    ![](img/B19849_Formula_222.png) 和方差 ![](img/B19849_Formula_231.png) 的高斯分布，则表示如下：
- en: '![](img/B19849_Formula_242.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_242.jpg)'
- en: 'The following formula gives the probability of ![](img/B19849_Formula_251.png)
    in a Gaussian distribution:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式给出了高斯分布中 ![](img/B19849_Formula_251.png) 的概率：
- en: '![](img/B19849_Formula_261.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_261.jpg)'
- en: Here, ![](img/B19849_Formula_272.png) is the mean and ![](img/B19849_Formula_283.png)
    is the variance (![](img/B19849_Formula_291.png) is the standard deviation). This
    formula is known as parametrized **probability density function** (**PDF**), and
    it describes the relative likelihood of different outcomes in a continuous random
    variable. It is used to model the distribution of a random variable and provides
    information about the probability of observing a specific value or range of values
    for that variable.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B19849_Formula_272.png) 是均值，![](img/B19849_Formula_283.png) 是方差（![](img/B19849_Formula_291.png)
    是标准差）。这个公式被称为参数化的**概率密度函数**（**PDF**），它描述了连续随机变量不同结果的相对可能性。它用于模拟随机变量的分布，并提供有关观察特定值或该变量的值范围的概率的信息。
- en: 'Next, we’ll introduce an example of the general approach we follow for anomaly
    detection with Gaussian distribution density estimation:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍我们用于高斯分布密度估计异常检测的一般方法的示例：
- en: Let’s say we’re given a new example, ![](img/B19849_Formula_302.png).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们得到了一个新的例子，![](img/B19849_Formula_302.png)。
- en: Select the features, ![](img/B19849_Formula_312.png), that are regular, meaning
    they determine anomalous behavior.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择特征，![](img/B19849_Formula_312.png)，它们是规则的，意味着它们决定了异常行为。
- en: Fit the ![](img/B19849_Formula_322.png)and ![](img/B19849_Formula_33.png) parameters.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整 ![](img/B19849_Formula_322.png) 和 ![](img/B19849_Formula_33.png) 参数。
- en: Compute ![](img/B19849_Formula_34.png) using an equation to calculate the probability
    of ![](img/B19849_Formula_35.png) in a Gaussian distribution.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个计算 ![](img/B19849_Formula_35.png) 在高斯分布中概率的方程来计算 ![](img/B19849_Formula_34.png)。
- en: Determine if ![](img/B19849_Formula_04.png) is an anomaly by comparing it with
    the threshold, ![](img/B19849_Formula_37.png); see the anomaly model formula.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将其与阈值 ![](img/B19849_Formula_37.png) 进行比较来确定 ![](img/B19849_Formula_04.png)
    是否是异常；参见异常模型公式。
- en: 'The following graph shows an example of Gaussian distribution density estimation
    for normally distributed data:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了正态分布数据的 Gaussian 分布密度估计示例：
- en: '![Figure 5.5 – Gaussian density estimation visualization](img/B19849_05_05.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 高斯密度估计可视化](img/B19849_05_05.jpg)'
- en: Figure 5.5 – Gaussian density estimation visualization
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 高斯密度估计可视化
- en: In this approach, we assume that selected features are independent, but usually,
    in real data, there are some correlations between them. In such a case, we should
    use a multivariate Gaussian distribution model instead of a univariate one.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们假设所选特征是独立的，但在实际数据中，它们之间通常存在一些相关性。在这种情况下，我们应该使用多元高斯分布模型而不是单变量模型。
- en: 'The following formula gives the probability of ![](img/B19849_Formula_38.png)
    in a multivariate Gaussian distribution:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式给出了多元高斯分布中 ![](img/B19849_Formula_38.png) 的概率：
- en: '![](img/B19849_Formula_391.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_391.jpg)'
- en: 'Here, ![](img/B19849_Formula_40.png) is the mean, ![](img/B19849_Formula_41.png)
    is the correlation matrix, and ![](img/B19849_Formula_421.png) is the determinant
    of the matrix, ![](img/B19849_Formula_41.png):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B19849_Formula_40.png) 是均值，![](img/B19849_Formula_41.png) 是相关矩阵，![](img/B19849_Formula_421.png)
    是矩阵的行列式，![](img/B19849_Formula_41.png)：
- en: '![](img/B19849_Formula_44.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_44.jpg)'
- en: 'The following graphs shows the difference between the univariate and the multivariate
    Gaussian distribution estimation models for a dataset with correlated data. Notice
    how distribution boundaries cover the regular data with a darker color, while
    anomalies are marked with a lighter color:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了具有相关数据的数据集的单变量和多变量高斯分布估计模型之间的差异。注意分布边界如何用较深的颜色覆盖常规数据，而异常用较浅的颜色标记：
- en: '![Figure 5.6 – Univariate and multivariate Gaussian distributions](img/B19849_05_06.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 单变量和多变量高斯分布](img/B19849_05_06.jpg)'
- en: Figure 5.6 – Univariate and multivariate Gaussian distributions
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 单变量和多变量高斯分布
- en: 'We can see that the multivariate Gaussian distribution can take into account
    correlations in the data and adapt its shape to them. This characteristic allows
    us to detect anomalies correctly for types of data whose distribution follows
    a Gaussian (normal) distribution shape. Also, we can see one of the advantages
    of this approach: results can be easily visualized in two or three dimensions,
    providing a clear understanding of the data.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，多元高斯分布可以考虑到数据中的相关性，并适应其形状。这一特性使我们能够正确地检测出那些分布形状遵循高斯（正态）分布的数据类型的异常。此外，我们还可以看到这种方法的一个优点：结果可以很容易地在二维或三维中进行可视化，从而对数据有清晰的理解。
- en: In the next section, we will look at another approach for anomaly detection.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨另一种异常检测的方法。
- en: KDE
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KDE
- en: In the KDE approach, our goal is to approximate a complex mixture of random
    distributions around point samples into a single function. So, the main idea is
    to center a probability distribution function at each data point and then take
    their average. It means that each discrete point in our dataset is replaced by
    an extended probability distribution, called a **kernel**. The probability density
    at any given point is then estimated as the sum of the kernel functions centered
    at each discrete point. If the point is close to many other points its estimated
    probability density will be larger than if it is far away from any sample point.
    This approach can be used to find anomalies as points where the estimated density
    is minimal.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 KDE 方法中，我们的目标是将围绕点样本的复杂随机分布近似为一个单一函数。因此，主要思想是在每个数据点处集中一个概率分布函数，然后取它们的平均值。这意味着我们数据集中的每个离散点都被一个扩展的概率分布所取代，称为**核**。在任意给定点的概率密度被估计为所有以每个离散点为中心的核函数的总和。如果点靠近许多其他点，其估计的概率密度将大于如果它远离任何样本点的情况。这种方法可以用来寻找异常点，即估计密度最小的地方。
- en: 'Mathematically, we can formulate the KDE function for univariate kernels as
    follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以将单变量核的 KDE 函数表述如下：
- en: '![](img/B19849_Formula_45.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![公式 45](img/B19849_Formula_45.jpg)'
- en: 'Here, Kh is a smoothed kernel defined with the following formula:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Kh 是通过以下公式定义的平滑核：
- en: '![](img/B19849_Formula_46.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![公式 46](img/B19849_Formula_46.jpg)'
- en: Here, h is the bandwidth parameter that defines the width of a kernel. The bandwidth
    parameter controls the degree of smoothness or roughness of the kernel function.
    A larger bandwidth results in a smoother function, while a smaller bandwidth leads
    to a more rugged one. Choosing the right bandwidth is crucial for achieving good
    generalization performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，h 是带宽参数，它定义了核的宽度。带宽参数控制着核函数的平滑度或粗糙度。较大的带宽会导致更平滑的函数，而较小的带宽则会导致更崎岖的函数。选择合适的带宽对于实现良好的泛化性能至关重要。
- en: 'K can be, for example, a Gaussian kernel function, which is the most common
    one:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: K 可以是，例如，高斯核函数，这是最常见的一种：
- en: '![](img/B19849_Formula_471.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![公式 471](img/B19849_Formula_471.jpg)'
- en: 'The following graph shows a plot of KDE of Gaussian distribution made from
    individual kernels:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了由单个核构成的 Gaussian 分布的 KDE 图：
- en: '![Figure 5.7 – KDE of Gaussian distribution](img/B19849_05_07.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – Gaussian 分布的 KDE](img/B19849_05_07.jpg)'
- en: Figure 5.7 – KDE of Gaussian distribution
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – Gaussian 分布的 KDE
- en: You can see in this graph that the density value will be the maximum for points
    around the value `3` because most sample points are located in this region. Another
    cluster of points is located around point `8`, but their density is quite a bit
    smaller.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从这张图中看到，密度值将在值 `3` 附近的点达到最大，因为大多数样本点都位于这个区域。另一个点群位于点 `8` 附近，但它们的密度要小得多。
- en: Before training the KDE model, it is important to preprocess the data to ensure
    that it is suitable for the KDE algorithm. This may include scaling the data so
    that all features have similar ranges, removing outliers or anomalies, and transforming
    the data if necessary.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 KDE 模型之前，预处理数据以确保其适合 KDE 算法是非常重要的。这可能包括对数据进行缩放，以便所有特征具有相似的范围，移除异常值或异常点，以及在必要时转换数据。
- en: Handling high-dimensional data efficiently in KDE can be challenging due to
    the **curse of dimensionality** (**CoD**). One approach is to use dimensionality
    reduction techniques such as **principal component analysis** (**PCA**) or **t-distributed
    stochastic neighbor embedding** (**t-SNE**) to reduce the number of dimensions
    before applying KDE. Another approach is to use sparse KDE algorithms that only
    consider a subset of data points when calculating the density estimate for a given
    location.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于**维度诅咒**（**CoD**），在KDE中有效地处理高维数据可能具有挑战性。一种方法是在应用KDE之前使用降维技术，如**主成分分析**（**PCA**）或**t分布随机邻域嵌入**（**t-SNE**）来减少维度数量。另一种方法是使用稀疏KDE算法，在计算给定位置的密度估计时仅考虑数据点的子集。
- en: In the following section, we will look at another approach for anomaly detection.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨另一种异常检测方法。
- en: Density estimation trees
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密度估计树
- en: 'The **density estimation tree** (**DET**) algorithm also can be used to detect
    anomalies by thresholding the density value for certain sample points. This is
    a non-parametric technique based on decision tree construction. The main advantage
    of this algorithm is the fast analytical complexity of density estimation at any
    given point, its *O(log n)* time, where n is the number of points in the tree.
    The tree is constructed iteratively in a top-to-bottom approach. Each leaf, *t*,
    is divided into two sub-leaves *t*l and *t*r by maximizing residual gain *s* that
    is defined as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**密度估计树**（**DET**）算法也可以通过阈值化某些样本点的密度值来检测异常。这是一种基于决策树构建的非参数技术。该算法的主要优点是在任何给定点的密度估计的快速分析复杂度，其时间复杂度为*O(log
    n)*，其中n是树中点的数量。树是按从上到下的方式迭代构建的。每个叶子节点*t*通过最大化以下定义的残差增益*s*被分为两个子叶子节点*t*l和*t*r：'
- en: '![](img/B19849_Formula_481.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_481.jpg)'
- en: 'Here, *R(t)* is the tree loss function:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*R(t)*是树损失函数：
- en: '![](img/B19849_Formula_491.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_491.jpg)'
- en: '*N* is the number of candidates the *t* leaf contains, and *V* is the leaf’s
    volume. Then, the actual density for a leaf *t* can be calculated as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*是*t*叶子节点包含的候选者数量，*V*是叶子节点的体积。然后，叶子节点*t*的实际密度可以按以下方式计算：'
- en: '![](img/B19849_Formula_501.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_501.jpg)'
- en: So, to estimate a density value for a given point, we have to determine to which
    leaf it belongs and then get the leaf’s density.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了估计给定点的密度值，我们必须确定它属于哪个叶子节点，然后获取该叶子节点的密度。
- en: In the current section, we discussed various anomaly detection approaches, and
    in the following sections, we will see how to use various C++ libraries to deal
    with the anomaly detection task.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了各种异常检测方法，在接下来的几节中，我们将看到如何使用各种C++库来处理异常检测任务。
- en: Examples of using different C++ libraries for anomaly detection
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用不同C++库进行异常检测的示例
- en: In this section, we’ll look at some examples of how to implement the algorithms
    we described previously for anomaly detection.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何实现我们之前描述的用于异常检测的算法的一些示例。
- en: C++ implementation of the isolation forest algorithm for anomaly detection
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C++实现用于异常检测的隔离森林算法
- en: '**Isolation forest algorithms** can be easily implemented in pure C++ because
    their logic is pretty straightforward. Also, there are no implementations of this
    algorithm in popular C++ libraries. Let’s assume that our implementation will
    only be used with two-dimensional data. We are going to detect anomalies in a
    range of samples where each sample contains the same number of features.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**隔离森林算法**可以很容易地在纯C++中实现，因为它们的逻辑非常直接。此外，在流行的C++库中还没有这个算法的实现。让我们假设我们的实现将仅用于二维数据。我们将检测包含相同数量特征的样本范围内的异常。'
- en: 'Because our dataset is large enough, we can define a wrapper for the actual
    data container. This allows us to reduce the number of copy operations we perform
    on the actual data:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集足够大，我们可以定义一个实际数据容器的包装器。这允许我们减少对实际数据执行的复制操作数量：
- en: '[PRE0]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `DatasetRange` type holds a reference to the vector of `Sample` type objects
    and to the container of indices that point to the samples in the dataset. These
    indices define the exact dataset objects that this `DatasetRange` object points
    to.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`DatasetRange`类型持有对`Sample`类型对象数组的引用，以及对指向数据集中样本的索引容器的引用。这些索引定义了此`DatasetRange`对象指向的确切数据集对象。'
- en: 'Next, we define the elements of the isolation tree, with the first one being
    the `Node` type:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义隔离树元素，第一个是`Node`类型：
- en: '[PRE1]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This type is a regular tree-node structure. The following members are specific
    to the isolation tree algorithm:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此类型是常规树节点结构。以下成员是隔离树算法特有的：
- en: '`split_col`: This is the index of the feature column where the algorithm caused
    a split'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_col`：这是算法导致分割的特征列的索引'
- en: '`split_value`: This is the value of the feature where the algorithm caused
    a split'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_value`：这是算法导致分割的特征值'
- en: '`size`: This is the number of underlying items for the node'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size`：这是节点的底层项目数量'
- en: '`is_external`: This is the flag that indicates whether the node is a leaf'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_external`：这是一个标志，表示节点是否为叶节点'
- en: Taking the `Node` type as a basis, we can define the procedure of building an
    isolation tree. We aggregate this procedure with the auxiliary `IsolationTree`
    type. Because the current algorithm is based on random splits, the auxiliary data
    is the random engine object.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以`Node`类型为基础，我们可以定义构建隔离树的程序。我们将此程序与辅助`IsolationTree`类型相结合。因为当前算法基于随机分割，辅助数据是随机引擎对象。
- en: 'We only need to initialize this object once, and then it will be shared among
    all tree-type objects. This approach allows us to make the results of the algorithm
    reproducible in the case of constant seeding. Furthermore, it makes debugging
    the randomized algorithm much simpler:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要初始化此对象一次，然后它将在所有树类型对象之间共享。这种方法允许我们在固定种子的情况下使算法的结果可重复。此外，它使随机化算法的调试变得更加简单：
- en: '[PRE2]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we’ll do the most critical work in the `MakeIsolationTree()` method,
    which is used in the constructor to initialize the root data member:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行`MakeIsolationTree()`方法中最关键的工作，该方法在构造函数中用于初始化根数据成员：
- en: '[PRE3]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Initially, we checked the termination conditions to stop the splitting process.
    If we meet them, we return a new node marked as an external leaf. Otherwise, we
    start splitting the passed data range. For splitting, we randomly select the `feature`
    column and determine the unique values of the selected feature. Then, we randomly
    select a value from an interval between the `max` and `min` values among the feature
    values from all the samples. After we make these random selections, we compare
    the values of the selected splitting feature to all the samples from the input
    data range and put their indices into two lists. One list is for values higher
    than the splitting values, while another list is for values that are lower than
    them. Then, we return a new tree node initialized with references to the left
    and right nodes, which are initialized with recursive calls to the `MakeIsolationTree()`
    method.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，我们检查终止条件以停止分割过程。如果我们遇到这些条件，我们将返回一个标记为外部叶节点的新的节点。否则，我们开始分割传递的数据范围。对于分割，我们随机选择`feature`列并确定所选特征的唯一值。然后，我们从所有样本的特征值中随机选择一个介于`max`和`min`值之间的值。在我们做出这些随机选择之后，我们将所选分割特征值与输入数据范围中的所有样本进行比较，并将它们的索引放入两个列表中。一个列表用于高于分割值的值，而另一个列表用于低于它们的值。然后，我们返回一个新树节点，该节点初始化为对左右节点的引用，这些节点通过递归调用`MakeIsolationTree()`方法进行初始化。
- en: 'Another vital method of the `IsolationTree` type is the `PathLength()` method.
    We use it for anomaly score calculations. It takes the sample as an input parameter
    and returns the amortized path length to the corresponding tree leaf from the
    root node:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`IsolationTree`类型的另一个重要方法是`PathLength()`方法。我们使用它进行异常分数计算。它接受样本作为输入参数，并返回从根节点到相应树叶节点的平均路径长度：'
- en: '[PRE4]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `PathLength()` method finds the leaf node during tree traversal based on
    sample feature values. These values are used to select a tree traversal direction
    based on the current node-splitting values. During each step, this method also
    increases the resulting height. The result of this method is a sum of the actual
    tree traversal height and the value returned from the call to the `CalcC()` function,
    which then returns the average path’s length of unsuccessful searches in a binary
    search tree of equal height to the leaf node. The `CalcC()` function can be implemented
    in the following way, according to the formula from the original paper, which
    describes the isolation forest algorithm (you can find a reference to this in
    the *Further* *reading* section):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`PathLength()` 方法根据样本特征值在树遍历过程中找到叶节点。这些值用于根据当前节点分裂值选择树遍历方向。在每一步中，此方法还会增加结果的高度。此方法的结果是实际树遍历高度和从
    `CalcC()` 函数调用返回的值的总和，该函数返回与叶节点等高的二叉搜索树中未成功搜索的平均路径长度。`CalcC()` 函数可以按照原始论文中的公式实现，该公式描述了隔离森林算法（你可以在
    *进一步* *阅读* 部分找到参考）：'
- en: '[PRE5]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The final part of the algorithm’s implementation is the creation of the forest.
    The forest is an array of trees built from a limited number of samples, randomly
    chosen from the original dataset. The number of samples used to build the tree
    is a hyperparameter of this algorithm. Furthermore, this implementation uses heuristics
    as the stopping criteria, in that it is a maximum tree height `hlim` value.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 算法实现的最后部分是森林的创建。森林是从原始数据集中随机选择的有限数量的样本构建的树数组。用于构建树的样本数量是该算法的超参数。此外，此实现使用启发式作为停止标准，即它是最大树高度
    `hlim` 值。
- en: 'Let’s see how it is used in the tree-building procedure. The `hlim` value is
    calculated only once, and the following code shows this. Moreover, it is based
    on the number of samples that are used to build a single tree:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它在树构建过程中的使用。`hlim` 值只计算一次，以下代码展示了这一点。此外，它基于用于构建单个树的样本数量：
- en: '[PRE6]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The tree forest is built in the constructor of the `IsolationForest` type.
    We also calculated the value of the average path length of the unsuccessful search
    in a binary search tree for all of the samples in the constructor. We use this
    forest in the `AnomalyScore()` method for the actual process of anomaly detection.
    It implements the formula for the anomaly score value for a given sample. It returns
    a value that can be interpreted in the following way: if the returned value is
    close to `1`, then the sample has anomalous features, while if the value is less
    than `0.5`, then we can assume that the sample is a normal one.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 树森林是在 `IsolationForest` 类的构造函数中构建的。我们还在构造函数中计算了所有样本在二叉搜索树中未成功搜索的平均路径长度。我们在 `AnomalyScore()`
    方法中使用此森林来进行实际的异常检测过程。它实现了给定样本的异常分数值的公式。它返回的值可以按以下方式解释：如果返回的值接近 `1`，则样本具有异常特征；如果值小于
    `0.5`，则我们可以假设该样本是正常的。
- en: 'The following code shows how we can use this algorithm. Furthermore, it uses
    `Dlib` primitives for the dataset’s representation:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了我们如何使用此算法。此外，它使用 `Dlib` 原语来表示数据集：
- en: '[PRE7]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the preceding example, we converted and merged the given datasets for the
    container that’s suitable for our algorithm. Then, we initialized the object of
    the `IsolationForest` type, which immediately builds the isolation forest with
    the following hyperparameters: the number of trees is 100, and the number of samples
    used for one tree is 50.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们将适合我们算法的容器中的给定数据集进行了转换和合并。然后，我们初始化了 `IsolationForest` 类的对象，它立即使用以下超参数构建隔离森林：树的数量是
    100，用于构建一棵树的样本数量是 50。
- en: 'Finally, we called the `AnomalyScore()` method for each sample from the dataset
    in order to detect anomalies with thresholds and return their values. In the following
    graph, we can see the result of anomaly detection after using the isolation forest
    algorithm. The points labeled as `1 cls` are the anomalies:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们为数据集中的每个样本调用了 `AnomalyScore()` 方法，以使用阈值检测异常并返回它们的值。在下面的图中，我们可以看到使用隔离森林算法进行异常检测后的结果。标记为
    `1 cls` 的点是异常点：
- en: "![Figure 5.8 – Anomaly detect\uFEFFion with isolation forest algorithm](img/B19849_05_08.jpg)"
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 使用隔离森林算法进行异常检测](img/B19849_05_08.jpg)'
- en: Figure 5.8 – Anomaly detection with isolation forest algorithm
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 使用隔离森林算法进行异常检测
- en: In this section, we learned how to implement the isolation forest algorithm
    from scratch. The following section will show you how to use the `Dlib` library
    for anomaly detection.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何从头实现隔离森林算法。下一节将向您展示如何使用 `Dlib` 库进行异常检测。
- en: Using the Dlib library for anomaly detection
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Dlib 库进行异常检测
- en: 'The `Dlib` library provides a couple of implemented algorithms that we can
    use for anomaly detection: the OCSVM model and the multivariate Gaussian model.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib` 库提供了一些实现算法，我们可以使用它们进行异常检测：OCSVM 模型和多元高斯模型。'
- en: OCSVM with Dlib
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dlib 中的 OCSVM
- en: 'There is only one algorithm that’s implemented in the `Dlib` library straight
    out of the box: OCSVM. There is a `svm_one_class_trainer` class in this library
    that can be used to train the corresponding algorithm, which should be configured
    with a kernel object, and the `nu` parameter, which controls the smoothness (in
    other words, the degree to which it controls the ratio between generalization
    and overfitting) of the solution.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib` 库中直接实现的一个算法是 OCSVM。在这个库中有一个 `svm_one_class_trainer` 类，可以用来训练相应的算法，这应该配置一个核对象，以及
    `nu` 参数，它控制解决方案的平滑度（换句话说，它控制泛化与过拟合之间比率的程度）。'
- en: 'The most widely used kernel is based on the Gaussian distribution and is known
    as the `radial_basis_kernel` class. Typically, we represent datasets in the `Dlib`
    library as a C++ vector of separate samples. Therefore before using this `trainer`
    object, we have to convert a matrix dataset into a vector:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最广泛使用的核函数基于高斯分布，被称为 `radial_basis_kernel` 类。通常，我们在 `Dlib` 库中将数据集表示为单独样本的 C++
    向量。因此，在使用此 `trainer` 对象之前，我们必须将矩阵数据集转换为向量：
- en: '[PRE8]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The result of the training process is a decision function object of the `decision_function<kernel_type>`
    class that we can use for single sample classification. Objects of this type can
    be used as a regular function. The result of a decision function is the distance
    from the normal class boundary, so the most distant samples can be classified
    as anomalies. The following graph shows an example of how the OCSVM algorithm
    from the `Dlib` library works. Note that the dots labeled as `1 cls` correspond
    to anomalies:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程的结果是 `decision_function<kernel_type>` 类的决策函数对象，我们可以用它进行单样本分类。此类对象可以用作常规函数。决策函数的结果是到正常类边界的距离，因此最远的样本可以分类为异常。以下图表展示了
    `Dlib` 库中的 OCSVM 算法的工作示例。注意，标记为 `1 cls` 的点对应于异常：
- en: "![Figure 5.9 – Anomaly\uFEFF detection with Dlib OCSVM implementation](img/B19849_05_09.jpg)"
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 使用 Dlib OCSVM 实现的异常检测](img/B19849_05_09.jpg)'
- en: Figure 5.9 – Anomaly detection with Dlib OCSVM implementation
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 使用 Dlib OCSVM 实现的异常检测
- en: We can see that OCSVM solved the task well and detected anomalies that are very
    interpretable. In the next section, we will see how to use a multivariate Gaussian
    model to detect anomalies.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，OCSVM 很好地解决了任务并检测到了非常可解释的异常。在下一节中，我们将看到如何使用多元高斯模型来检测异常。
- en: Multivariate Gaussian model with Dlib
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dlib 中的多元高斯模型
- en: 'Using the linear algebra facilities of the `D``lib` library (or any other library,
    for that matter), we can implement anomaly detection with the multivariate Gaussian
    distribution approach. The following example shows how to implement this approach
    with `Dlib` linear algebra routines:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Dlib` 库的线性代数功能（或任何其他库，实际上也是如此），我们可以使用多元高斯分布方法实现异常检测。以下示例展示了如何使用 `Dlib` 线性代数例程实现这种方法：
- en: '[PRE9]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The idea of this approach is to define a function that returns the probability
    of appearing, given a sample in a dataset. To implement such a function, we calculate
    the statistical characteristics of the training dataset. In the first step, we
    calculate the mean values of each feature and store them in the one-dimensional
    matrix. Then, we calculate the covariance matrix for the training samples using
    the formula for the correlation matrix that was given in the prior theoretical
    section named *Density estimation approach*. Next, we determine the correlation
    matrix determinant and inverse version. We define a lambda function named `prob`
    to calculate the probability of a single sample using the formula provided in
    the *Using multivariate Gaussian* *distribution* section.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的思路是定义一个函数，该函数返回给定数据集中样本出现的概率。为了实现这样的函数，我们计算训练数据集的统计特征。在第一步中，我们计算每个特征的均值并将它们存储在一个一维矩阵中。然后，我们使用先前理论部分中给出的相关矩阵公式来计算训练样本的协方差矩阵，该部分命名为*密度估计方法*。接下来，我们确定相关矩阵的行列式和逆矩阵。我们定义一个名为`prob`的lambda函数，使用*使用多元高斯分布*部分提供的公式来计算单个样本的概率。
- en: For large datasets, the computational complexity of calculating the covariance
    matrix can become a significant factor in the overall runtime of an ML model.
    Furthermore, optimizing the calculation of the covariance matrix for large datasets
    requires a combination of techniques, including sparsity, parallelization, approximation,
    and efficient algorithms.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大数据集，计算协方差矩阵的计算复杂度可以成为机器学习模型整体运行时间中的一个重要因素。此外，优化大数据集的协方差矩阵计算需要结合多种技术，包括稀疏性、并行化、近似和高效算法。
- en: We also define a probability threshold to separate anomalies. It determines
    the boundary between normal and anomalous behavior, and it plays a crucial role
    in the classification of samples as anomalies or normal. Engineers must carefully
    consider their application’s requirements and adjust the threshold accordingly
    to achieve the desired level of sensitivity. For example, in security applications
    where false alarms are costly, a higher threshold might be preferred to minimize
    false positives. In contrast, in medical diagnoses where missing a potential anomaly
    could have serious consequences, a lower threshold might be more appropriate to
    ensure that no true anomalies go undetected.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义一个概率阈值来区分异常值。它决定了正常行为和异常行为之间的边界，并在将样本分类为异常或正常时起着至关重要的作用。工程师必须仔细考虑他们应用程序的要求并相应地调整阈值，以达到所需的灵敏度水平。例如，在安全应用中，误报代价高昂，可能更倾向于选择更高的阈值以最小化误报。相反，在医学诊断中，错过潜在异常可能具有严重后果，因此可能更合适选择较低的阈值以确保没有真正的异常未被检测到。
- en: 'Then, we iterate over all the examples (including the training and testing
    datasets) to find out how the algorithm separates regular samples from anomalies.
    In the following graph, we can see the result of this separation. The dots labeled
    as `1 cls` are anomalies:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们遍历所有示例（包括训练和测试数据集），以找出算法如何将常规样本与异常值分开。在下面的图中，我们可以看到这种分离的结果。标记为`1 cls`的点代表异常值：
- en: "![Figure 5.10 – Anomaly detection w\uFEFFith Dlib multivariate Gaussian distribution](img/B19849_05_10.jpg)"
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图5.10 – 使用Dlib多变量高斯分布进行异常检测](img/B19849_05_10.jpg)'
- en: Figure 5.10 – Anomaly detection with Dlib multivariate Gaussian distribution
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 – 使用Dlib多变量高斯分布进行异常检测
- en: We see that this approach found fewer anomalies than the previous one, so you
    should be aware that some methods will not work well with your data and it will
    make sense to try different methods. In the following section, we will see how
    to use a multivariate Gaussian model from the `mlpack` library for the same task.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这种方法找到的异常比之前少，因此你应该意识到某些方法可能不会很好地适用于你的数据，尝试不同的方法是有意义的。在下一节中，我们将看到如何使用`mlpack`库中的多变量高斯模型来完成相同的任务。
- en: Multivariate Gaussian model with mlpack
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用mlpack的多变量高斯模型
- en: 'We already discussed in the previous chapter the `GMM` and the `EMFit` classes
    that exist in the `mlpack` library. The **Expectation-Maximization with Fit**
    (**EMFit**) algorithm is an ML technique used to estimate the parameters of a
    **gaussian mixture model** (**GMM**). It works by iteratively optimizing the parameters
    to fit the data. We can use them not only for solving clustering tasks but also
    for anomaly detection. There will only the one difference: we have to specify
    only one cluster for the training. So, GMM class initialization will look like
    the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在上一章讨论了存在于`mlpack`库中的`GMM`和`EMFit`类。**期望最大化与拟合**（**EMFit**）算法是一种机器学习技术，用于估计**高斯混合模型**（**GMM**）的参数。它通过迭代优化参数以拟合数据来工作。我们不仅可以用它们来解决聚类任务，还可以用于异常检测。唯一的区别是：我们只需要为训练指定一个簇。因此，GMM类的初始化将如下所示：
- en: '[PRE10]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The initializations for the `KMeans` and the `EMFit` algorithms will be the
    same as in the previous example:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`KMeans`和`EMFit`算法的初始化将与前一个示例相同：'
- en: '[PRE11]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The values for `max_iterations` and the convergence tolerance variables influence
    the training process by determining how long the algorithm runs and when it stops.
    A higher number of trials may lead to more accurate results but also increase
    computational time. The convergence tolerance determines how close the parameters
    must be to their previous values before the algorithm stops. If the tolerance
    is too low, the algorithm may never converge, while if it is too high, it may
    converge to a suboptimal solution.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_iterations`和收敛容差变量的值会影响训练过程，通过确定算法运行多长时间以及何时停止。更高的试验次数可能会导致更准确的结果，但也会增加计算时间。收敛容差决定了参数在算法停止之前必须接近其先前值有多近。如果容差太低，算法可能永远不会收敛，而如果它太高，它可能收敛到一个次优解。'
- en: 'Then, we can use the `Probability` method of the `gmm` object to test some
    new data. The only new action we have to take is to define a probability threshold
    that will be used to check if a new sample belongs to the original data distribution
    or if it’s an anomaly. This can be done as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`gmm`对象的`Probability`方法来测试一些新的数据。我们唯一需要采取的新行动是定义一个概率阈值，该阈值将用于检查新样本是否属于原始数据分布，或者它是否是异常。可以这样做：
- en: '[PRE12]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Having this threshold, the usage of the `Probability` method will be the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个阈值，`Probability`方法的用法如下：
- en: '[PRE13]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here, we defined a lambda function that can be applied to any dataset defined
    as a matrix. In this function, we have used a simple loop over all samples in
    which we applied the `Probability` method for a single sample and compared the
    returned value with the threshold. If the probability value is too low, we mark
    the sample as an anomaly by adding its coordinates to the `plotting_cluster[1]`
    object. To plot the anomaly detection result, we used the same approach as was
    described in the previous chapter. The following code shows how to use the function
    we defined:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个可以应用于任何以矩阵定义的数据集的lambda函数。在这个函数中，我们使用了一个简单的循环，遍历所有样本，并对单个样本应用`Probability`方法，并将返回的值与阈值进行比较。如果概率值太低，我们通过将其坐标添加到`plotting_cluster[1]`对象中来将该样本标记为异常。为了绘制异常检测结果，我们使用了与上一章中描述的相同的方法。以下代码显示了如何使用我们定义的函数：
- en: '[PRE14]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We applied the `detect` function to both sets of data: the `normal` one that
    was used for the training and the new one, `test`. You can see the anomaly detection
    result in the following graph:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`detect`函数应用于两组数据：用于训练的`normal`数据和新的`test`数据。您可以在以下图表中看到异常检测结果：
- en: "![Figure 5.11 – Anomaly detection with \uFEFFmlpack multivariate Gaussian distribution](img/B19849_05_11.jpg)"
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图5.11 – 使用mlpack多元高斯分布进行异常检测](img/B19849_05_11.jpg)'
- en: Figure 5.11 – Anomaly detection with mlpack multivariate Gaussian distribution
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 – 使用mlpack多元高斯分布进行异常检测
- en: The two outliers were detected. You can try to change the probability threshold
    to see how the decision boundary will be changed and what objects will be classified
    as anomalies.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 两个异常值被检测到。您可以尝试更改概率阈值，看看决策边界将如何改变，以及哪些对象将被分类为异常。
- en: In the following section, we will see how to use the KDE algorithm implementation
    from the `mlpack` library.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用`mlpack`库中的KDE算法实现。
- en: KDE with mlpack
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用mlpack的核密度估计（KDE）
- en: 'The KDE algorithm in the `mlpack` library is implemented in the `KDE` class.
    This class can be specialized with several template parameters; the most important
    ones are `KernelType`, `MetricType`, and `TreeType`. Let’s use the Gaussian kernel,
    the Euclidean distance as the metric, and KD-Tree for the tree type. A tree data
    structure is used to optimize the algorithm’s computational complexity. For each
    query point, the algorithm will apply a kernel function to each reference point,
    so the computational complexity can be *O(N^2)* in the naive implementation for
    *N* query points and *N* reference points. The tree optimization avoids many similar
    calculations, because kernel function values decrease with distance, but it also
    introduces some level of approximation. The following code snippet shows how to
    define a `KDE` object for our sample:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack`库中的KDE算法是在`KDE`类中实现的。这个类可以用几个模板参数进行特殊化；其中最重要的参数是`KernelType`、`MetricType`和`TreeType`。让我们使用高斯核，欧几里得距离作为度量，KD-Tree作为树类型。使用树数据结构来优化算法的计算复杂度。对于每个查询点，算法将对每个参考点应用核函数，因此，在原始实现中，对于N个查询点和N个参考点，计算复杂度可以达到*O(N^2)*。树优化避免了许多类似的计算，因为核函数值随着距离的增加而减小，但它也引入了一定程度的近似。下面的代码片段展示了如何为我们的样本定义一个`KDE`对象：'
- en: '[PRE15]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Due to the approximations used in the algorithm, the API allows us to define
    relative and absolute error tolerances. Relative and absolute error tolerances
    control the level of approximation in the KDE estimate. A higher tolerance allows
    for more approximation, which can reduce computational complexity but also decrease
    accuracy. Conversely, a lower tolerance requires more computation but can result
    in a more accurate estimate.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算法中使用了近似，API允许我们定义相对和绝对误差容限。相对和绝对误差容限控制KDE估计中的近似程度。更高的容限允许更多的近似，这可以降低计算复杂度，但也会降低精度。相反，更低的容限需要更多的计算，但可能导致更准确的估计。
- en: The **relative error tolerance** parameter specifies the maximum relative error
    allowed between the true density and the estimated density at any point. It is
    used to determine the optimal bandwidth for the kernel.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**相对误差容限**参数指定了在任何点上真实密度与估计密度之间允许的最大相对误差。它用于确定核的最佳带宽。'
- en: The **absolute error tolerance** parameter sets the maximum absolute error allowed
    between the true density and the estimated density over the entire domain. It
    can be used to ensure that the estimated density is within a certain range of
    the true density.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**绝对误差容限**参数设置在整个域内真实密度与估计密度之间允许的最大绝对误差。它可以用来确保估计密度在真实密度的某个范围内。'
- en: 'For our sample, we defined only the absolute one. The next step is to train
    our algorithm object with the normal data without anomalies; it can be done as
    follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的样本，我们只定义了绝对误差容限。下一步是用正常数据（无异常）训练我们的算法对象；可以这样做：
- en: '[PRE16]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can see that different algorithms in the `mlpack` library use mostly the
    same API. Then, we can define a function to classify the given data as normal
    or as an anomaly. The following code shows its definition:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，`mlpack`库中的不同算法主要使用相同的API。然后，我们可以定义一个函数来将给定的数据分类为正常或异常。下面的代码展示了其定义：
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We defined a lambda function that takes the data matrix and passes it to the
    `Evaluate` method of the `kde` object. This method evaluated and assigned density
    value estimation for every sample in the given data matrix. Then, we just compared
    those estimations with the `density_threshold` value to decide if the sample was
    normal or an anomaly. Samples with low-density values were classified as anomalies.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个lambda函数，它接受数据矩阵并将其传递给`kde`对象的`Evaluate`方法。此方法评估并分配给给定数据矩阵中的每个样本密度值估计。然后，我们只需将这些估计与`density_threshold`值进行比较，以决定样本是否为正常或异常。密度值低的样本被分类为异常。
- en: To select an optimal threshold, you need to balance the trade-off between sensitivity
    and specificity based on your specific use case. If you prioritize detecting all
    anomalies, you may want to set a lower threshold to increase the number of true
    positives, even if it means accepting more false positives. Conversely, if you
    prioritize minimizing false alarms, you might choose a higher threshold, which
    could miss some anomalies but reduce the number of false positives. In practice,
    selecting an optimal density threshold often involves experimenting with different
    values and evaluating the results using metrics such as precision, recall, and
    F1 score. Additionally, domain knowledge and expert input can help guide the selection
    process.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择一个最优的阈值，你需要根据你的具体用例在敏感性和特异性之间进行权衡。如果你优先考虑检测所有异常，你可能想要设置一个较低的阈值以增加真阳性的数量，即使这意味着接受更多的假阳性。相反，如果你优先考虑最小化误报，你可能选择一个较高的阈值，这可能会错过一些异常，但会减少假阳性的数量。在实践中，选择最优的密度阈值通常涉及对不同值进行实验，并使用如精确度、召回率和F1分数等指标评估结果。此外，领域知识和专家意见可以帮助指导选择过程。
- en: 'This function also prepares data for plotting in the same manner as we did
    earlier. The following code shows how to plot two datasets with normal and outlier
    data:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数还以我们之前的方式准备绘图数据。以下代码显示了如何绘制包含正常和异常数据的两个数据集：
- en: '[PRE18]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can see the anomaly detection result in the following graph:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下图表中看到异常检测的结果：
- en: "![Figure 5.12 – Anomaly detection with KDE from \uFEFFmlpack](img/B19849_05_12.jpg)"
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图5.12 – 使用mlpack的KDE进行异常检测](img/B19849_05_12.jpg)'
- en: Figure 5.12 – Anomaly detection with KDE from mlpack
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 – 使用mlpack的KDE进行异常检测
- en: We can see that using the KDE method, we can find the two outliers, as we detected
    in the previous sections. The points labeled with `1 cls` are outliers. By changing
    the density threshold, you can see how the decision boundary will be changed.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用KDE方法，我们可以找到两个异常值，正如我们在前面的章节中检测到的。标记为`1 cls`的点为异常值。通过改变密度阈值，你可以看到决策边界将如何改变。
- en: In the following section, we will see how to use the DET algorithm implementation
    from the `mlpack` library.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用`mlpack`库中的DET算法实现。
- en: DET with mlpack
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DET with mlpack
- en: 'The DET method from the `mlpack` library is implemented in the `DTree` class.
    To start working with it, we have to make a copy of the training normal data because
    an object of the `DTree` class will change the input data order. The data ordering
    is changed because `mlpack` creates a tree data structure directly on the given
    data object. The following code snippet shows how to define such an object:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack`库中的DET方法在`DTree`类中实现。要开始使用它，我们必须复制训练正常数据，因为`DTree`类的对象会改变输入数据的顺序。数据顺序的改变是因为`mlpack`直接在给定的数据对象上创建树数据结构。以下代码片段显示了如何定义这样的对象：'
- en: '[PRE19]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Due to the input data reordering the algorithms, the API is required to provide
    a mapping of the indices that will show the relation of new indices to the old
    ones. Such mapping can be initialized as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输入数据重新排序的算法，API需要提供一个索引映射，该映射将显示新索引与旧索引之间的关系。这种映射可以初始化如下：
- en: '[PRE20]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, we just stored the original relation of the data indices to the input
    data. Later, this mapping will be updated by the algorithm. Now, can build the
    DET by calling the `Grow` method, as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是存储了数据索引与输入数据之间的原始关系。稍后，这种映射将由算法更新。现在，可以通过调用`Grow`方法来构建DET，如下所示：
- en: '[PRE21]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The two main parameters of the `Grow` method are `max_leaf_size` and `min_leaf_size`.
    They should be turned manually after a series of experiments or with some prior
    knowledge of the dataset characteristics. It can be tricky to estimate those parameters
    with automated techniques such as cross-validation because, for anomaly detection
    tasks, we don’t usually have enough data marked as anomalies. So, the values for
    this example were chosen manually.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`Grow`方法的两个主要参数是`max_leaf_size`和`min_leaf_size`。它们应该在一系列实验或对数据集特征的先验知识的基础上手动调整。使用如交叉验证等自动化技术估计这些参数可能会很棘手，因为在异常检测任务中，我们通常没有足够的数据被标记为异常。因此，这个示例中的值是手动选择的。'
- en: 'Having an initialized DET, we can use the `ComputeValue` method to estimate
    a density for some given data sample. If we choose a density threshold value,
    we can detect anomalies just by comparison with this value. We used the same approach
    in other algorithms too. The following code snippet shows how to use a threshold
    to distinguish between normal and anomalous data and build a data structure for
    result plotting:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We defined a `detect` function that simply iterates over the input data matrix
    columns and applies the `ComputeValue` method for every given sample to get the
    density estimate. Then, this function compares a value with `density_threshold`,
    and if the density is big enough, puts the sample in the first plotting cluster.
    Otherwise, the sample comes to the second plotting cluster. We can apply this
    function as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here, `normal` and `test` are matrices with the normal and anomalous data samples,
    correspondingly. The following graph shows the detection result plotting:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Anomaly detection with DET algorithm](img/B19849_05_13.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Anomaly detection with DET algorithm
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'You may notice that this method classified more data points as anomalies compared
    to previous methods. To change this detection result, you can play with three
    parameters: the density threshold and leaf `min` and `max` sizes. This method
    with more complex turning may be useful for datasets where you don’t know the
    data distribution rule (the kernel form) or it’s hard to write code for it. The
    same goes for when you have normal data with several clusters with different distributions.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we examined anomalies in data. We discussed several approaches
    to anomaly detection and looked at two kinds of anomalies: outliers and novelties.
    We considered the fact that anomaly detection is primarily a UL problem, but despite
    this, some algorithms require labeled data, while others are semi-supervised.
    The reason for this is that, generally, there is a tiny number of positive examples
    (that is, anomalous samples) and a large number of negative examples (that is,
    standard samples) in anomaly detection tasks.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we don’t usually have enough positive samples to train algorithms.
    That is why some solutions use labeled data to improve algorithm generalization
    and precision. On the contrary, **supervised learning** (**SL**) usually requires
    a large number of positive and negative examples, and their distribution needs
    to be balanced.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Also, notice that the task of detecting anomalies does not have a single formulation
    and that it is often interpreted differently, depending on the nature of the data
    and the goal of the concrete task. Moreover, choosing the correct anomaly detection
    method depends primarily on the task, data, and available a priori information.
    We also learned that different libraries can give slightly different results,
    even for the same algorithms.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will discuss dimension reduction methods. Such
    methods help us to reduce the dimensionality of data with high dimensionality
    into a new representation of data with lower dimensionality while preserving essential
    information from the original data.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论降维方法。这些方法帮助我们将高维数据降低到新的低维数据表示，同时保留原始数据中的关键信息。
- en: Further reading
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Anomaly detection learning resources: [https://github.com/yzhao062/anomaly-detection-resources](https://github.com/yzhao062/anomaly-detection-resources)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测学习资源：[https://github.com/yzhao062/anomaly-detection-resources](https://github.com/yzhao062/anomaly-detection-resources)
- en: '*Outlier Detection with One-Class SVMs: An Application to Melanoma* *Prognosis*:
    [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041295/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041295/)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一类SVMs的异常检测：黑色素瘤预后应用*：[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041295/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041295/)'
- en: 'Isolation forest: [https://feitonyliu.files.wordpress.com/2009/07/liu-iforest.pdf](https://feitonyliu.files.wordpress.com/2009/07/liu-iforest.pdf)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隔离森林：[https://feitonyliu.files.wordpress.com/2009/07/liu-iforest.pdf](https://feitonyliu.files.wordpress.com/2009/07/liu-iforest.pdf)
- en: 'Ram, Parikshit & Gray, Alexander. (2011). *Density estimation trees*. *Proceedings
    of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*.
    627-635\. 10.1145/2020408.2020507: [https://www.researchgate.net/publication/221654618_Density_estimation_trees](https://www.researchgate.net/publication/221654618_Density_estimation_trees)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ram, Parikshit & Gray, Alexander. (2011). *密度估计树*. *ACM SIGKDD国际知识发现和数据挖掘会议论文集*.
    627-635\. 10.1145/2020408.2020507: [https://www.researchgate.net/publication/221654618_Density_estimation_trees](https://www.researchgate.net/publication/221654618_Density_estimation_trees)'
- en: 'Tutorial on KDE: [https://faculty.washington.edu/yenchic/18W_425/Lec6_hist_KDE.pdf](https://faculty.washington.edu/yenchic/18W_425/Lec6_hist_KDE.pdf)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KDE教程：[https://faculty.washington.edu/yenchic/18W_425/Lec6_hist_KDE.pdf](https://faculty.washington.edu/yenchic/18W_425/Lec6_hist_KDE.pdf)
