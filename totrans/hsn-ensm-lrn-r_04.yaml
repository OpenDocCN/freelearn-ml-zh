- en: Chapter 4. Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapter introduced bagging as an ensembling technique based on
    homogeneous base learners, with the decision tree serving as a base learner. A
    slight shortcoming of the bagging method is that the bootstrap trees are correlated.
    Consequently, although the variance of predictions is reduced, the bias will persist.
    Breiman proposed randomly sampling the covariates and independent variables at
    each split, and this method then went on to help in decorrelating the bootstrap
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: In the first section of this chapter, the random forest algorithm is introduced
    and illustrated. The notion of variable importance is crucial to decision trees
    and all of their variants, and a section is devoted to clearly illustrating this
    concept. Do the random forests perform better than bagging? An answer will be
    provided in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Breiman laid out the importance of proximity plots in the context of random
    forests, and we will delve into this soon enough. An algorithm as complex as this
    will have a lot of nitty-gritty details, and some of these will be illustrated
    through programs and real data. Missing data is almost omnipresent and we will
    undertake the task of imputing missing values using random forests. Although a
    random forest is primarily a supervised learning technique, it can also be used
    for clustering observations regarding the data, and this topic will be the concluding
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core topics of this chapter are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The Random Forest algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable importance for decision trees and random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing random forests with bagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of proximity plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest details, nitty-gritty, and nuances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling missing data by using random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering with random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the following libraries in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kernlab`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`randomForest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`randomForestExplainer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rpart`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, generalized the decision tree using the bootstrap
    principle. Before we embark on a journey with random forests, we will quickly
    review the history of decision trees and highlight some of their advantages and
    drawbacks. The invention of decision trees followed through a culmination of papers,
    and the current form of the trees can be found in detail in Breiman, et al. (1984).
    Breiman''s method is popularly known as **C**lassification **a**nd **R**egression
    **T**rees, aka **CART**. Around the late 1970s and early 1980s, Quinlan invented
    an algorithm called C4.5 independently of Breiman. For more information, see Quinlan
    (1984). To a large extent, the current form of decision trees, bagging, and random
    forests is owed to Breiman. A somewhat similar approach is also available in an
    algorithm popularly known by the abbreviation CHAID, which stands for **Ch**i-square
    **A**utomatic **I**nteraction **D**etector. An in-depth look at CART can be found
    in Hastie, et al. (2009), and a statistical perspective can be found in Berk (2016).
    An excellent set of short notes can also be found in Seni and Elder (2010). Without
    any particular direction, we highlight some advantages and drawbacks of CART:'
  prefs: []
  type: TYPE_NORMAL
- en: Trees automatically address the problem of variable selection since at each
    split, they look for the variable that gives the best split in the regressand,
    and thus a tree eliminates variables that are not useful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trees do not require data processing. This means that we don't have to consider
    transformation, rescaling, and/or weight-of-evidence preprocessing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trees are computationally scalable and the time complexity is manageable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trees give a metric called variable importance that is based on the contribution
    of the variable to error reduction across all the splits of the trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trees efficiently handle missing values and if an observation has a missing
    value, the tree will continue to use the available values of the observation.
    Handling missing data is often enabled by the notion of a surrogate split.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trees have fewer parameters to manage, as seen in the previous chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trees have a simple top-down interpretation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trees with great depth tend to be almost unbiased.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interaction effect is easily identified among the variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its drawback is that the fitted model is not continuous and it will have sharp
    edges. Essentially, trees are piecewise constant regression models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trees can't approximate low interaction target functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The greedy search approach to trees results in high variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first extension of the trees was seen in the bagging algorithm discussed
    in the previous chapter. Suppose we have N observations. For each bootstrap sample,
    we draw N observations with replacement. How many observations are likely to be
    common between two bootstrap samples? Let''s write a simple program to find it
    first, using the simple `sample` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This program needs explanation. The number of *N* observations varies from 1000
    to 10000 with an increment of 1000, and we run *B = 1e3 = 1000* bootstrap iterations.
    Now, for a fixed size of *N*, we draw two samples with replacement of size *N*,
    see how many observations are common between them, and divide it by *N*. The average
    of the *B = 1000* samples is the probability of finding a common observation between
    two samples. Equivalently, it gives the common observation percentage between
    two samples.
  prefs: []
  type: TYPE_NORMAL
- en: The bootstrap probability clearly shows that about 40% of observations will
    be common between any two trees. Consequently, the trees will be correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Chapter 15, Hastie, et al. (2009) points out that the bagging trees are
    IID trees and hence the expectation of any one tree is the same as the expectation
    of any other tree. Consequently, the bias of the bagged trees is the same as that
    of the individual trees. Thus, variance reduction is the only improvement provided
    by bagging. Suppose that we have B independent and identically distributed IID
    random variables with a variance of ![Random Forests](img/00179.jpeg). The sample
    average has a variance of ![Random Forests](img/00180.jpeg). However, if we know
    that the variables are only identically distributed and that there is a positive
    pairwise correlation of ![Random Forests](img/00181.jpeg), then the variance of
    the sample average is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random Forests](img/00182.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that as the number of B samples increase, the second term vanishes and
    the first term remains. Thus, we see that the correlatedness of the bagged trees
    restricts the benefits of averaging. This motivated Breiman to innovate in a way
    that subsequent trees will not be correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Breiman''s solution is that before each split, select *m < p* number of input
    variables at random for splitting. This lays the foundation of random forests,
    where we shake the data to improve the performance. Note that merely *shaking*
    does not guarantee improvement. This trick helps when we have highly nonlinear
    estimators. The formal random forest algorithm, following Hastie, et al. (2009)
    and Berk (2016), is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Draw a random sample of size N with replacement from the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a random sample without replacement of the predictors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct the first recursive partition of the data in the usual way.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2 for each subsequent split until the tree is as large as desired.
    Importantly, do not prune. Compute each terminal node proportion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop the out-of-bag (OOB) data down the tree and store the assigned class to
    each observation, along with each observation's predictor values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1-5 a large number of times, say 1000.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using only the class assigned to each observation when that observation is OOB,
    count the number of times over the trees that the observation is classified in
    one category and the number of times over trees it is classified in the other
    category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign each case to a category by a majority vote over the set of trees when
    that case is OOB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From his practical experience, Breiman recommends randomly selecting a number
    of covariates at each split as ![Random Forests](img/00183.jpeg) with a minimum
    node size of 1 for a classification problem, whereas the recommendation for a
    regression problem is ![Random Forests](img/00184.jpeg) with a minimum node size
    of 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `randomForest` R package for software implementation. The German
    Credit data will be used for further analysis. If you remember, in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    the accuracy obtained by using the basic classification tree was 70%. We will
    set up the German credit data using the same settings as earlier, and we will
    build the random forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `randomForest` function applies over the `formula` and `data` as seen earlier.
    Here, we have specified the number of trees to be 500 with `ntree=500`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we compare this random result with the bagging result from the previous
    chapter, the accuracy obtained there was only `0.78`. Here, we have `p = 19` covariates,
    and so we will try to increase the number of covariates sampled for a split at
    `8`, and see how it performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'An increase of `0.01`, or about 1%, might appear meager. However, in a banking
    context, this accuracy will translate into millions of dollars. We will use the
    usual `plot` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The following graph is the output of the preceding code executed using the `plot`
    function`:`
  prefs: []
  type: TYPE_NORMAL
- en: '![Random Forests](img/00185.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Error rate of the Random Forest for the German Credit data'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have three curves: the error rate for OOB, the error rate for the good class,
    and the error rate for the bad class. Note that the error rate stabilizes at around
    100 trees. Using the loss matrix, it might be possible to reduce the gap between
    the three curves. Ideally, the three curves should be as close as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Create random forests with the options of `split` criteria, `loss`
    matrix, `minsplit`, and different `mtry`. Examine the error rate curves and prepare
    a summary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualize the random forest! Where are the trees? Apparently, we need to do
    a lot of exercises to extract trees out of the fitted `randomForest` object. A
    new function, `plot_RF`, has been defined in the `Utilities.R` file and we will
    display it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random Forests](img/00186.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The `plot_RF` function first obtains the number of `$ntree` trees in the forest.
    It will then run through a `for` loop. In each iteration of the loop, it will
    extract information related to that tree with the `getTree` function and create
    a new `dendogram` object. The `dendogram` is then visualized, and is nothing but
    the tree. Furthermore, the `print` command is optional and can be muted out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Four arbitrarily chosen trees from the forest in the PDF file are displayed
    in the following figure, Trees of the Random Forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random Forests](img/00187.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Trees of the Random Forest'
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick visit is paid to the Pima Indians diabetes problem. In the accuracy
    table of [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    we could see that the accuracy for the decision tree was 0.7588, or 75.88%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we have an improved accuracy of 0.7704 – 0.7588 = 0.0116, or about 1.2%.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Obtain the error rate plot of the Pima Indian Diabetes problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Variable importance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statistical models, say linear regression and logistic regression, indicate
    which variables are significant with measures such as p-value and t-statistics.
    In a decision tree, a split is caused by a single variable. If the specification
    of the number of variables for the surrogate splits, a certain variable may appear
    as the split criteria more than once in the tree and some variables may never
    appear in the tree splits at all. During each split, we select the variable that
    leads to the maximum reduction in impurity, and the contribution of a variable
    across the tree splits would also be different. The overall improvement across
    each split of the tree (by the reduction in impurity for the classification tree
    or by the improvement in the split criterion) is referred to as the *variable
    importance*. In the case of ensemble methods such as bagging and random forest,
    the variable importance is measured for each tree in the technique. While the
    concept of variable importance is straightforward, its computational understanding
    is often unclear. This is primarily because a formula or an expression is not
    given in mathematical form. The idea is illustrated next through simple code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kyphosis` dataset from the `rpart` package consists of four variables,
    and the target variable here is named `Kyphosis`, indicating the presence of the
    kyphosis type of deformation following an operation. The three explanatory variables
    are `Age`, `Number`, and `Start`. We build a classification tree with zero surrogate
    variables for the split criteria with the `maxsurrogate=0` option. The choice
    of zero surrogates ensures that we have only one variable at a split. The tree
    is set up and visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Variable importance](img/00188.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Kyphosis Classification Tree'
  prefs: []
  type: TYPE_NORMAL
- en: In the no-surrogate tree, the first split variable is `Start`, with a terminal
    leaf on the right part of the split. The left side/partition further splits again
    with the `Start` variable, with a terminal node/leaf on the left side and a split
    on the later right side. In the next two split points, we use only the `Age` variable,
    and the `Number` variable is not used anywhere in the tree. Thus, we expect the
    `Number` variable to have zero importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `$variable.importance` on the fitted classification tree, we obtain the
    variable importance of the three explanatory variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the `Number` variable is not shown as having any importance. The
    importance of `Start` is given as `7.783` and `Age` as `2.961`. To understand
    how R has computed these values, run the `summary` function on the classification
    tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Four lines of output have been highlighted in the summary output, and each line
    contains the information about the split, the best of improvement offered by each
    of the variables, and the variable selected at the split. Thus, for the `Start`
    variable, the first highlighted line shows the improvement at `6.762` and the
    second line shows `1.021`. By adding these, we get `6.762 + 1.021 = 7.783`, which
    is the same as the output given from the `$variable.importance` extractor. Similarly,
    the last two highlighted lines show the contribution of `Age` as `1.274 + 1.714
    = 2.961`. Thus, we have clearly outlined the computation of the variable importance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Create a new classification tree, say `KC2`, and allow a surrogate
    split. Using the `summary` function, verify the computations associated with the
    variable importance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `VarImpPlot` function from the `randomForest` package gives us a dot chart
    plot of the variable importance measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'A visual display is given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variable importance](img/00189.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Variable Importance Plots for German and Pima Indian Diabetes Random
    Forests'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the five most important variables for classifying the German credit as
    good or bad are `amount`, `checking`, `age`, `duration`, and `purpose`. For the
    Pima Indian Diabetes classification, the three most important variables are `glucose`,
    `mass`, and `age`.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at the notion of the proximity measure next.
  prefs: []
  type: TYPE_NORMAL
- en: Proximity plots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to Hastie, et al. (2009), "*one of the advertised outputs of a random
    forest is a proximity plot"* (see page 595). But what are proximity plots? If
    we have *n* observations in the training dataset, a proximity matrix of order
    ![Proximity plots](img/00190.jpeg) is created. Here, the matrix is initialized
    with all the values at 0\. Whenever a pair of observations such as OOB occur jointly
    in the terminal node of a tree, the proximity count is increased by 1\. The proximity
    matrix is visualized using the multidimensional scaling method, a concept beyond
    the scope of this chapter, where the proximity matrix is represented in two dimensions.
    The proximity plots give an indication of which points are closer to each other
    from the perspective of the random forest.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the earlier creation of random forests, we had not specified the option
    of a proximity matrix. Thus, we will first create the random forest using the
    option of proximity as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The options `proximity=TRUE,cob.prox=TRUE` are important to obtain the `proximity`
    matrix. We then simply make use of the `MDSplot` graphical function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Proximity plots](img/00191.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The multidimensional plot for the proximity matrix of an RF'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is easier to find which observation is closest to a given observation from
    the proximity data perspective, and not the Euclidean distance, using the `which.max`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the observations numbered `657` in the training dataset (and `962` in
    the overall dataset) are closest to the first observation. Note that the overall
    position is because of the name extracted from the sample function. The `which.max`
    function is useful for finding the maximum position in an array.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that most often, the graphical display using the `MDSplot` function
    results in a similar star-shape display. The proximity matrix also helps in carrying
    out cluster analysis, as will be seen in the concluding section of the chapter.
    Next, we will cover the parameters of a random forest in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest nuances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `GC_Random_Forest.pdf` file consists of the 500 trees which serve as the
    homogeneous learners in the random forest ensemble. It is well known that a decision
    tree has a nice and clear interpretation. This is because it shows how one traverses
    the path to a terminal node. The random selection of features at each split and
    the bootstrap samples lead to the setting up of the random forest. Refer to the
    figure *Trees of the Random Forest*, which depicts trees numbered `78`, `176`,
    `395`, and `471`. The first split across the four trees is respectively `purpose`,
    `amount`, `property`, and `duration`. The second split for the first left side
    of these four trees is `employed`, `resident`, `purpose`, and `amount`, respectively.
    It is a cumbersome exercise to see which variables are meaningful over the others.
    We know that the earlier a variable appears, the higher its importance is. The
    question that then arises is, with respect to a random forest, how do we find
    the depth distribution of the variables? This and many other points are addressed
    through a powerful random forest package available as `randomForestExplainer`,
    and it is not an exaggeration that this section would not have been possible without
    this awesome package.
  prefs: []
  type: TYPE_NORMAL
- en: 'By applying the `min_depth_distribution` function on the random forest object,
    we get the depth distribution of the variables. Using `plot_min_depth_distribution`,
    we then get the plot of minimum depth distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding code block is *Minimum Depth Distribution of German
    Random Forest*, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random Forest nuances](img/00192.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Minimum Depth Distribution of German Random Forest'
  prefs: []
  type: TYPE_NORMAL
- en: 'From previous figure, it is clear that the `checking` variable appears more
    often as the primary split, followed by `savings`, `purpose`, `amount`, and `duration`.
    Consequently, we get a useful depiction through the minimum depth distribution
    plot. Further analyses are possible by using the `measure_importance` function,
    which gives us various measures of importance for the variables of the random
    forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We are warned here that the random forest has not been grown with the option
    of `localImp = TRUE`, which is central to obtaining the measures. Thus, we create
    a new random forest with this option, and then run the `measure_importance` function
    on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output has a wider format, and hence we provide it in an image format and
    display the result vertically in *Analysis of Variable Importance Measure*. We
    can see that the `measure_importance` function gives a lot of information on the
    average minimum depth, number of nodes across the 500 trees that the variable
    appears as node, the average decrease in accuracy, the Gini decrease, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see from the output that if the mean minimum depth is higher, the associated
    p-value is also higher and hence the variable is insignificant. For example, the
    variables `coapp`, `depends`, `existcr`, `foreign`, and `telephon` have a higher
    mean minimum depth, and their p-value is also 1 in most cases. Similarly, lower
    values of `gini_decrease` are also associated with higher p-values, and this indicates
    the insignificance of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random Forest nuances](img/00193.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Analysis of Variable Importance Measure'
  prefs: []
  type: TYPE_NORMAL
- en: 'The importance measure object `GC2_RF_VIM` can be used for further analyses.
    For the `no_of_nodes` measure, we can compare the various metrics from the previous
    variable importance measures. For instance, we would like to see how the `times_a_root`
    values for the variables turns out against the mean minimum depth. Similarly,
    we would like to analyze other measures. By applying the `plot_multi_way_importance`
    graphical function on this object, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Random Forest nuances](img/00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Multi-way Importance Plot for the German Credit Data'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the `times_a_root` values of the variables are plotted against the mean
    minimum depth, `mean_min_depth`, while keeping the number of nodes to their size.
    The non-top variables are black, while the top variables are blue. Similarly,
    we plot `gini_decrease`, `no_of_trees` and `p_value` against `mean_min_depth`
    in the preceding figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The correlation between the five measures is depicted next, using the `plot_importance_ggpairs`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Random Forest nuances](img/00195.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Relationship Between the Measures of Importance'
  prefs: []
  type: TYPE_NORMAL
- en: Since the measures are strongly correlated, either positively or negatively,
    we need to have all five of these measures to understand random forests.
  prefs: []
  type: TYPE_NORMAL
- en: 'A great advantage of the tree structure is the interpretation of interaction
    between the variables. For instance, if the split in a parent is by one variable,
    and by another variable in the daughter node, we can conclude that there is interaction
    between these two variables. Again, the question arises for the random forests.
    Using the `important_variables` and `min_depth_interactions`, we can obtain the
    interactions among the variables of a random forest as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output that will be obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random Forest nuances](img/00196.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Minimum Depth Interaction for German Random Forest'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can easily find the interaction variables of the random forest.
  prefs: []
  type: TYPE_NORMAL
- en: The `randomForestExplainer` R package is very powerful and helps us to carry
    out many diagnostics after obtaining the random forests. Without post diagnostics,
    we cannot evaluate any fitted model. Consequently, the reader is advised to carry
    out most of the steps learned in this section in their implementation of random
    forests.
  prefs: []
  type: TYPE_NORMAL
- en: We will compare a random forest with the bagging procedure in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Carry out the diagnostics for the random forest built for the
    Pima Indian Diabetes problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparisons with bagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When comparing the random forest results with the bagging counterpart for the
    German credit data and Pima Indian Diabetes datasets, we did not see much improvement
    in the accuracy over the validated partition of the data. A potential reason might
    be that the variability reduction achieved by bagging is at the optimum reduced
    variance, and that any bias improvement will not lead to an increase in the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We consider a dataset to be available from the R core package `kernlab`. The
    dataset is spam and it has a collection of 4601 emails with labels that state
    whether the email is spam or non-spam. The dataset has a good collection of 57
    variables derived from the email contents. The task is to build a good classifier
    for the spam identification problem. The dataset is quickly partitioned into training
    and validation partitions, as with earlier problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we will build the simple classification tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The classification tree gives a modest accuracy of about 90%. We will then
    apply `randomForest` and build the random forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Bagging can be performed with the `randomForest` function. The trick is to
    ask the random forest to use all the variables while setting up a split. Thus,
    the choice of `mtry=ncol(spal_TestX)` will select all the variables and bagging
    is then easily performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The increase in accuracy is also reflected in the accuracy plots, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparisons with bagging](img/00197.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Random Forest and Bagging Comparisons for the Spam Classification
    Problem'
  prefs: []
  type: TYPE_NORMAL
- en: We will look at some niche applications of random forests in the concluding
    two sections.
  prefs: []
  type: TYPE_NORMAL
- en: Missing data imputation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Missing data is a menace! It pops up out of nowhere and blocks analysis until
    it is properly taken care of. The statistical technique of the expectation-maximization
    algorithm, or simply the EM algorithm, needs a lot of information on the probability
    distributions, structural relationship, and in-depth details of statistical models.
    However, an approach using the EM algorithm is completely ruled out here. Random
    forests can be used to overcome the missing data problem.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `missForest` R package to fix the missing data problem whenever
    we come across it in the rest of the book. The algorithm for the `missForest`
    function and other details can be found at [https://academic.oup.com/bioinformatics/article/28/1/112/219101](https://academic.oup.com/bioinformatics/article/28/1/112/219101).
    For any variable/column with missing data, the technique is to build a random
    forest for that variable and obtain the OOB prediction as the imputation error
    estimates. Note that the function can handle continuous as well as categorical
    missing values. The creators of the package have enabled the functions with parallel
    run capability to save time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take a simple dataset from [https://openmv.net/info/travel-times](https://openmv.net/info/travel-times),
    and there are missing values in the data. The data consists of `13` variables
    and `205` observations. Of the `13` variables available, only the `FuelEconomy`
    variable has missing values. Let''s explore the dataset in the R terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be seen that there are `19` observations with missing values. The `sapply`
    function tells us that all `19` observations have missing values for the `FuelEconomy`
    variable only. The `missForest` function is now deployed in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We have now imputed the missing values. It needs to be noted that the imputed
    values should make sense and should not look out of place. In [Chapter 9](part0062_split_000.html#1R42S1-2006c10fab20488594398dc4871637ee
    "Chapter 9. Ensembling Regression Models"), *Ensembling Regression Models*, we
    will use the `missForest` function to impute a lot of missing values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: How can the imputed values be validated? Use the `prodNA` function
    from the `missForest` package and puncture good values with missing data. Using
    the `missForest` function, get the imputed values and compare them with the original
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: The proximity matrix tells us how close the observations are from the random
    forest perspective. If we have information about the observations neighborhood,
    we can carry out a cluster analysis. As a by-product of using the proximity matrix,
    we can now also use random forests for unsupervised problems.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering with Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests can be set up without the target variable. Using this feature,
    we will calculate the proximity matrix and use the OOB proximity values. Since
    the proximity matrix gives us a measure of closeness between the observations,
    it can be converted into clusters using hierarchical clustering methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the setup of `y = NULL` in the `randomForest` function. The options
    of `proximity=TRUE` and `oob.prox=TRUE` are specified to ensure that we obtain
    the required proximity matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the `hclust` function with the option of `ward.D2` to carry out
    the hierarchical cluster analysis on the proximity matrix of dissimilarities.
    The `cutree` function divides the `hclust` object into `k = 6` number of clusters.
    Finally, the `table` function and the visuals give an idea of how good the clustering
    has been by using the random forests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a diagram illustrating clustering using random forests:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering with Random Forest](img/00198.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Clustering Using Random Forests'
  prefs: []
  type: TYPE_NORMAL
- en: Although the clusters provided by the random forests do not fit the label identification
    problem, we will take them as a starting point. It needs to be understood that
    random forests can be used properly for cluster analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests were created as an improvement on the bagging method. As an example
    of the homogeneous ensemble method, we saw how the forests help in obtaining higher
    accuracy. Visualization and variable importance for random forests were thoroughly
    detailed. We also saw a lot of diagnostic methods that can be used after fitting
    a random forest. The method was then compared with bagging. Novel applications
    of random forest for missing data imputation and cluster analysis were also demonstrated.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at boosting, which is a very important ensemble.
  prefs: []
  type: TYPE_NORMAL
