<html><head></head><body><div class="calibre1" title="Chapter&#xA0;10.&#xA0;Estimating Projective Relations in Images"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title"><a id="ch10" class="calibre6"/>Chapter 10. Estimating Projective Relations in Images</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div class="calibre1"><ul class="itemizedlist"><li class="listitem">Computing the fundamental matrix of an image pair</li><li class="listitem">Matching images using random sample consensus</li><li class="listitem">Computing a homography between two images</li><li class="listitem">Detecting a planar target in images</li></ul></div><div class="calibre1" title="Introduction"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch10lvl1sec61" class="calibre6"/>Introduction</h1></div></div></div><p class="calibre8">Images are generally produced using a digital camera, which captures a scene by projecting light going through its lens onto an image sensor. The fact that an image is formed by the projection of a 3D scene onto a 2D plane implies the existence of important relationships both between a scene and its image and between different images of the same scene. Projective geometry is the tool that is used to describe and characterize, in mathematical terms, the process of image formation. In this chapter, we will introduce you to some of the fundamental projective relations that exist in multi-view imagery and explain how these can be used in computer vision programming. But, before we start the recipes, let's explore the basic concepts related to scene projection and image formation.</p><div class="calibre1" title="Image formation"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec182" class="calibre6"/>Image formation</h2></div></div></div><p class="calibre8">Fundamentally, the process used to produce images has not changed since the beginning of photography. The light coming from an observed scene is captured by a camera through a frontal aperture, and the captured light rays hit an image plane (or an image sensor) located at the back of the camera. Additionally, a lens is used to concentrate the rays coming from the different scene elements. This process is illustrated by the following figure:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Image formation" src="graphics/image_10_001.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Here, <code class="literal">do</code> is the distance from the lens to the observed object, <code class="literal">di</code> is the distance from the lens to the image plane, and <code class="literal">f</code> is the focal length of the lens. These quantities are related by the so-called <span><strong class="calibre15">thin lens equation</strong></span>:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Image formation" src="graphics/B05388_10_15-2.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">In computer vision, this camera model can be simplified in a number of ways. Firstly, we can neglect the effect of the lens by considering that we have a camera with an infinitesimal aperture since, in theory, this does not change the image appearance. (However, by doing so, we ignore the focusing effect by creating an image with an infinite depth of field.) In this case, therefore, only the central ray is considered. Secondly, since most of the time we have <code class="literal">do&gt;&gt;di</code>, we can assume that the image plane is located at the focal distance. Finally, we can note from the geometry of the system that the image on the plane is inverted. We can obtain an identical but upright image by simply positioning the image plane in front of the lens. Obviously, this is not physically feasible, but from a mathematical point of view, this is completely equivalent. This simplified model is often referred to as the <span><strong class="calibre15">pinhole camera </strong></span>
<span><strong class="calibre15">model</strong></span>, and it is represented as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Image formation" src="graphics/image_10_003.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">From this model, and using the law of similar triangles, we can easily derive the basic projective equation that relates a photographed object with its image:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Image formation" src="graphics/B05388_10_16-2.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The size (<code class="literal">hi</code>) of the image of an object (of physical height <code class="literal">
ho</code>) is therefore inversely proportional to its distance (<code class="literal">do</code>) from the camera, which is naturally true. In general, this relation describes where a 3D scene point will be projected on the image plane given the geometry of the camera. More specifically, if we assume that the reference frame is positioned at the focal point, then a 3D scene point located at position <code class="literal">(X,Y,Z)</code> will be projected onto the image plane at <code class="literal">(x,y)=(fX/Z,fY/Z)</code>. Here, the <code class="literal">Z</code> coordinate corresponds with the depth of the point (or distance to camera, denoted by <code class="literal">do</code> in the previous equation). This relation can be rewritten in a simple matrix form through the introduction of homogeneous coordinates, in which 2D points are represented by 3-vectors, and 3D points are represented by 4-vectors (the extra coordinate is simply an arbitrary scale factor <code class="literal">s</code> that needs to be removed when a 2D coordinate needs to be extracted from a homogeneous 3-vector):</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Image formation" src="graphics/B05388_10_17-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This <code class="literal">3x4</code> matrix is called the projection matrix. In cases where the reference frame is not aligned with the focal point, then rotation <code class="literal">r</code> and translation <code class="literal">t</code> matrices must be introduced. The role of these ones is simply to express the projected 3D point into a camera-centric reference frame, which is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Image formation" src="graphics/B05388_10_18-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The first matrix of this equation is said to contain the intrinsic parameters of the camera (here, only the focal length, but the next chapter will introduce a few more intrinsic parameters). The second matrix contains the extrinsic parameters that are the parameters that relate the camera to the exterior world. It should be noted that, in practice, image coordinates are expressed in pixels while 3D coordinates are expressed in world measurements (for example, meters). This aspect will be explored in <a href="ch11.html" title="Chapter 11. Reconstructing 3D Scenes">
Chapter 11
</a>, <span><em class="calibre16">Reconstructing 3D Scenes</em></span>.</p></div></div></div>
<div class="calibre1" title="Computing the fundamental matrix of an image pair"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch10lvl1sec62" class="calibre6"/>Computing the fundamental matrix of an image pair</h1></div></div></div><p class="calibre8">The introductory section of this chapter presented the projective equation, describing how a scene point projects onto the image plane of a single camera. In this recipe, we will explore the projective relationship that exists between two images that display the same scene. These two images could have been obtained by moving a camera to two different locations to take pictures from two viewpoints, or by using two cameras, each of them taking a different picture of the scene. When those two cameras are separated by a rigid baseline, we use the term <span><strong class="calibre15">stereovision</strong></span>.</p><div class="calibre1" title="Getting ready"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec183" class="calibre6"/>Getting ready</h2></div></div></div><p class="calibre8">Let's now consider two pinhole cameras observing a given scene point, as shown in the following figure:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Getting ready" src="graphics/image_10_007.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">We learned that we can find the image <code class="literal">x</code> of a 3D point <code class="literal">X</code> by tracing a line joining this 3D point with the camera's center. Conversely, the scene point that has its image at position <span><strong class="calibre15">x</strong></span> on the image plane can be located anywhere on this line in the 3D space. This implies that, if we want to find the corresponding point of a given image point in another image, we need to search along the projection of this line onto the second image plane. This imaginary line is called the <span><strong class="calibre15">epipolar line</strong></span> of point <code class="literal">x</code>. It defines a fundamental constraint that must satisfy two corresponding points; that is, the match of a given point must lie on the epipolar line of this point in the other view, and the exact orientation of this epipolar line depends on the respective position of the two cameras. In fact, the configuration of the set of possible epipolar lines characterizes the geometry of a two-view system.</p><p class="calibre8">Another observation that can be made from the geometry of this two-view system is that all the epipolar lines pass through the same point. This point corresponds to the projection of one camera's center onto the other camera (points <code class="literal">e</code> and <code class="literal">e'</code> in the above figure). This special point is called an <span><strong class="calibre15">epipole</strong></span>.</p><p class="calibre8">Mathematically, the relationship between an image point and its corresponding epipolar line can be expressed using a <code class="literal">3x3</code> matrix, as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Getting ready" src="graphics/B05388_10_19-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">In projective geometry, a 2D line is also represented by a 3-vector. It corresponds to the set of 2D points <code class="literal">(x',y')</code>, that satisfy the equation <code class="literal">l1'x'+ l2'y'+ l3'= 0</code> (the prime superscript denotes that this line belongs to the second image). Consequently, the matrix <code class="literal">F</code>, called the fundamental matrix, maps a 2D image point in one view to an epipolar line in the other view.</p></div><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec184" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The fundamental matrix of an image pair can be estimated by solving a set of equations that involve a certain number of known matched points between the two images. The minimum number of such matches is seven. In order to illustrate the fundamental matrix estimation process, we selected seven good matches from the matching results of SIFT features, as presented in the previous chapter.</p><p class="calibre8">These matches will be used to compute the fundamental matrix using the <code class="literal">cv::findFundamentalMat</code> OpenCV function. The image pair with its selected matches is shown here:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_10_009.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">These matches are stored in a <code class="literal">cv::DMatch</code> vector pointing to indexes of <code class="literal">cv::keypoint</code> instances. These keypoints first need to be converted into <code class="literal">cv::Point2f</code> in order to be used with <code class="literal">cv::findFundamentalMat</code>. An OpenCV function can be used to this end:</p><pre class="programlisting">    // Convert keypoints into Point2f 
    std::vector&lt;cv::Point2f&gt; selPoints1, selPoints2; 
    std::vector&lt;int&gt; pointIndexes1, pointIndexes2; 
    cv::KeyPoint::convert(keypoints1,selPoints1,pointIndexes1); 
    cv::KeyPoint::convert(keypoints2,selPoints2,pointIndexes2); 
</pre><p class="calibre8">The two resulting vectors <code class="literal">selPoints1</code> and <code class="literal">selPoints2</code> contain the corresponding point coordinates in the two images. The <code class="literal">pointIndexes1</code> and <code class="literal">pointIndexes2</code> vectors contain the indexes of the keypoints to be converted. The call to the <code class="literal">cv::findFundamentalMat</code> function is then as follows:</p><pre class="programlisting">    // Compute F matrix from 7 matches 
    cv::Mat fundamental= cv::findFundamentalMat(  
                             selPoints1,      // 7 points in first image 
                             selPoints2,      // 7 points in second image 
                             cv::FM_7POINT);  // 7-point method 
</pre><p class="calibre8">One way to visually verify the validity of the fundamental matrix is to draw the epipolar lines of some selected points. Another OpenCV function allows the epipolar lines of a given set of points to be computed. Once these have been computed, they can be drawn using the <code class="literal">cv::line</code> function. The following lines of code accomplish these two steps (that is, computing and drawing epipolar lines on the image on the right from the points in the image on the left):</p><pre class="programlisting">    // draw the left points corresponding epipolar 
    // lines in right image  
    std::vector&lt;cv::Vec3f&gt; lines1; 
    cv::computeCorrespondEpilines(  
                     selPoints1,  // image points  
                     1,           // in image 1 (can also be 2) 
                     fundamental, // F matrix 
                     lines1);     // vector of epipolar lines 
    // for all epipolar lines 
    for (vector&lt;cv::Vec3f&gt;::const_iterator it= lines1.begin(); 
                it!=lines1.end(); ++it) { 
      // draw the line between first and last column 
      cv::line(image2, cv::Point(0,-(*it)[2]/(*it)[1]),
               cv::Point(image2.cols,
                         -((*it)[2]+(*it)[0]*image2.cols)/(*it)[1]), 
                         cv::Scalar(255,255,255)); 
    } 
</pre><p class="calibre8">The epipolar lines of the left image are obtained in a similar way. The following image shows these lines:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_10_010.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Remember that the epipole of one image is at the intersection of all its epipolar lines. This one is the projection of the other camera's center. Note that the epipolar lines can intersect (and often do) outside of the image boundaries. In the case of our example, the epipole of the second image is at the location where the first camera would be visible if the two images were taken at the same instant. Note also that the results can be quite unstable when the fundamental matrix is computed from only seven matches. Indeed, substituting one match for another could lead to a significantly different set of epipolar lines.</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec185" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">We previously explained that, for a point in one image, the fundamental matrix gives the equation of the line on which its corresponding point in the other view should be found. If the corresponding point of a point <code class="literal">(x,y)</code> is <code class="literal">(x',y')</code>, suppose we have <code class="literal">F</code>, the fundamental matrix, between the two views. Since <code class="literal">(x',y')</code> lies on the epipolar line given by multiplying <code class="literal">F</code> by <code class="literal">(x,y)</code> expressed in homogenous coordinates, we must then have the following equation:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/B05388_10_20-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This equation expresses the relationship between two corresponding points and is known as the <span><strong class="calibre15">epipolar constraint</strong></span>. Using this equation, it becomes possible to estimate the entries of the matrix using known matches. Since the entries of the <code class="literal">F</code> matrix are given up to a scale factor, there are only eight entries to be estimated (the ninth one can be arbitrarily set to <code class="literal">1</code>). Each match contributes to one equation. Therefore, with eight known matches, the matrix can be fully estimated by solving the resulting set of linear equations. This is what is done when you use the <code class="literal">cv::FM_8POINT</code> flag with the <code class="literal">cv::findFundamentalMat</code> function. Note that, in this case, it is possible (and preferable) to input more than eight matches. The obtained over-determined system of linear equations can then be solved in a mean-square sense.</p><p class="calibre8">To estimate the fundamental matrix, an additional constraint can also be exploited. Mathematically, the <code class="literal">F</code> matrix maps a 2D point to a 1D pencil of lines (that is, lines that intersect at a common point). The fact that all these epipolar lines pass through this unique point (that is, the epipole) imposes a constraint on the matrix. This constraint reduces the number of matches required to estimate the fundamental matrix to seven. In mathematical terms, we say that the fundamental matrix has 7 degrees of freedom and is therefore of rank-2. Unfortunately, in this case, the set of equations becomes nonlinear, with up to three possible solutions (in this case, <code class="literal">cv::findFundamentalMat</code> will return a fundamental matrix of the size <code class="literal">9x3</code>, that is, three <code class="literal">3x3</code> matrices stacked up). The seven-match solution of the <code class="literal">F</code> matrix estimation can be invoked in OpenCV by using the <code class="literal">cv::FM_7POINT</code> flag. This is what we did in the example in the preceding section.</p><p class="calibre8">Lastly, it should be mentioned that the choice of an appropriate set of matches in the image is important to obtain an accurate estimation of the fundamental matrix. In general, the matches should be well distributed across the images and include points at different depths in the scene. Otherwise, the solution will become unstable. In particular, the selected scene points should not be coplanar, as the fundamental matrix (in this case) becomes degenerated.</p></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec186" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem"><span><em class="calibre16">Multiple View Geometry in Computer Vision</em></span>, Cambridge University Press, 2004, <span><em class="calibre16">R. Hartley</em></span> and <span><em class="calibre16">A. Zisserma</em></span>n, is the most complete reference on projective geometry in computer vision</li><li class="listitem">The <span><em class="calibre16">Matching images using random sample </em></span><span><em class="calibre16">consensus</em></span> recipe explains how a fundamental matrix can be robustly estimated from a larger match set</li><li class="listitem">The <span><em class="calibre16">Computing a homography between two images</em></span> recipe explains why a fundamental matrix cannot be computed when the matched points are coplanar, or are the result of a pure rotation</li></ul></div></div></div>
<div class="calibre1" title="Matching images using random sample consensus"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch10lvl1sec63" class="calibre6"/>Matching images using random sample consensus</h1></div></div></div><p class="calibre8">When two cameras observe the same scene, they see the same elements but under different viewpoints. We have already studied the feature point matching problem in the previous chapter. In this recipe, we come back to this problem, and we will learn how to exploit the epipolar constraint introduced in the previous recipe to match image features more reliably.</p><p class="calibre8">The principle that we will follow is simple: when we match feature points between two images, we only accept those matches that fall on corresponding epipolar lines. However, to be able to check this condition, the fundamental matrix must be known, but we need good matches to estimate this matrix. This seems to be a chicken-and-egg problem. However, in this recipe, we propose a solution in which the fundamental matrix and a set of good matches will be jointly computed.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec187" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The objective is to be able to compute a fundamental matrix and a set of good matches between two views. To do so, all the found feature point correspondences will be validated using the epipolar constraint introduced in the previous recipe. To this end, we have created a class that encapsulates the different steps of the proposed robust matching process:</p><pre class="programlisting">    class RobustMatcher { 
     private: 
      // pointer to the feature point detector object 
      cv::Ptr&lt;cv::FeatureDetector&gt; detector; 
      // pointer to the feature descriptor extractor object 
      cv::Ptr&lt;cv::DescriptorExtractor&gt; descriptor; 
      int normType; 
      float ratio;         // max ratio between 1st and 2nd NN 
      bool refineF;        // if true will refine the F matrix 
      bool refineM;        // if true will refine the matches  
      double distance;     // min distance to epipolar 
      double confidence;   // confidence level (probability) 
 
     public: 
 
      RobustMatcher(const cv::Ptr&lt;cv::FeatureDetector&gt; &amp;detector,         
                    const cv::Ptr&lt;cv::DescriptorExtractor&gt; &amp;descriptor=  
                              cv::Ptr&lt;cv::DescriptorExtractor&gt;()):
                    detector(detector), descriptor(descriptor),                
                    normType(cv::NORM_L2), ratio(0.8f),  
                    refineF(true), refineM(true),  
                    confidence(0.98), distance(1.0) { 
 
          // in this case use the associated descriptor 
          if (!this-&gt;descriptor) {  
            this-&gt;descriptor = this-&gt;detector; 
        }  
      } 
</pre><p class="calibre8">Users of this class simply supply the feature detector and descriptor instances of their choice. These ones can also be specified using the defined <code class="literal">setFeatureDetector</code> and <code class="literal">setDescriptorExtractor</code> setter methods.</p><p class="calibre8">The main method is the match method, which returns matches, detected keypoints, and the estimated fundamental matrix. The method proceeds in four distinct steps (explicitly identified in the comments of the following code), which we will now explore:</p><pre class="programlisting">    // Match feature points using RANSAC 
    // returns fundamental matrix and output match set 
    cv::Mat match(cv::Mat&amp; image1, cv::Mat&amp; image2,     // input images 
                  std::vector&lt;cv::DMatch&gt;&amp; matches,     // output matches 
                  std::vector&lt;cv::KeyPoint&gt;&amp; keypoints1,//output keypoints 
                  std::vector&lt;cv::KeyPoint&gt;&amp; keypoints2) {  
 
       // 1. Detection of the feature points 
      detector-&gt;detect(image1,keypoints1); 
      detector-&gt;detect(image2,keypoints2); 
 
      // 2. Extraction of the feature descriptors 
      cv::Mat descriptors1, descriptors2; 
      descriptor-&gt;compute(image1,keypoints1,descriptors1); 
      descriptor-&gt;compute(image2,keypoints2,descriptors2); 
 
      // 3. Match the two image descriptors 
      // (optionally apply some checking method) 
    
      // Construction of the matcher with crosscheck 
      cv::BFMatcher matcher(normType,   //distance measure 
                            true);      //crosscheck flag 
      // match descriptors 
      std::vector&lt;cv::DMatch&gt; outputMatches; 
      matcher.match(descriptors1,descriptors2,outputMatches); 
 
      // 4. Validate matches using RANSAC 
      cv::Mat fundamental= ransacTest(outputMatches,        
                                      keypoints1, keypoints2,   
                                      matches); 
      // return the found fundamental matrix 
      return fundamental; 
    } 
</pre><p class="calibre8">The first two steps simply detect the feature points and compute their descriptors. Next, we proceed to feature matching using the <code class="literal">cv::BFMatcher</code> class, as we did in the previous chapter. We use the crosscheck flag to obtain matches of better quality.</p><p class="calibre8">The fourth step is the new concept introduced in this recipe. It consists of an additional filtering test that will this time use the fundamental matrix in order to reject matches that do not obey the epipolar constraint. This test is based on the <code class="literal">RANSAC</code> method that can compute the fundamental matrix even when outliers are present in the match set (this method will be explained in the next section).</p><p class="calibre8">Using our <code class="literal">RobustMatcher</code> class, the robust matching of an image pair is then easily accomplished by the following calls:</p><pre class="programlisting">    // Prepare the matcher (with default parameters) 
    // SIFT detector and descriptor 
    RobustMatcher rmatcher(cv::xfeatures2d::SIFT::create(250)); 
 
    // Match the two images 
    std::vector&lt;cv::DMatch&gt; matches; 
 
    std::vector&lt;cv::KeyPoint&gt; keypoints1, keypoints2; 
    cv::Mat fundamental = rmatcher.match(image1, image2,    
                                         matches,         
                                         keypoints1, keypoints2); 
</pre><p class="calibre8">This results in <code class="literal">54</code> matches that are shown in the following screenshot:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_10_012.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Most of the time, the resulting matches will be good matches. However, a few false matches might remain; these are ones that accidently fell on the corresponding epipolar lines of the computed fundamental matrix.</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec188" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">In the preceding recipe, we learned that it is possible to estimate the fundamental matrix associated with an image pair from a number of feature point matches. Obviously, to be exact, this match set must be made up of only good matches. However, in a real context, it is not possible to guarantee that a match set obtained by comparing the descriptors of the detected feature points will be completely exact. This is why a fundamental matrix estimation method based on the <span><strong class="calibre15">RANSAC</strong></span> (<span><strong class="calibre15">RANdom SAmpling Consensus</strong></span>) strategy has been introduced.</p><p class="calibre8">The RANSAC algorithm aims to estimate a given mathematical entity from a data set that may contain a number of outliers. The idea is to randomly select some data points from the set and perform the estimation with only those. The number of selected points should be the minimum number of points required to estimate the mathematical entity. In the case of the fundamental matrix, eight matched pairs is the minimum number (in fact, the real minimum is seven matches, but the 8-point linear algorithm is faster to compute). Once the fundamental matrix is estimated from these eight random matches, all the other matches in the match set are tested against the epipolar constraint that derives from this matrix. All the matches that fulfill this constraint (that is, matches for which the corresponding feature is at a short distance from its epipolar line) are identified. These matches form the support set of the computed fundamental matrix.</p><p class="calibre8">The central idea behind the RANSAC algorithm is that, the larger the support set, the higher the probability that the computed matrix is the right one. Conversely, if one (or more) of the randomly selected matches is an incorrect match, then the computed fundamental matrix will also be incorrect, and its support set will be expected to be small. This process is repeated a number of times and, in the end, the matrix with the largest support will be retained as the most probable one.</p><p class="calibre8">Therefore, our objective is to pick eight random matches several times so that eventually we select eight good ones, which should give us a large support set. Depending on the proportion of incorrect matches in the entire data set, the probability of selecting a set of eight correct matches will differ. However, we know that, the more selections we make, the higher our confidence will be that we have at least one good match set among those selections. More precisely, if we assume that the match set is made of <code class="literal">w%</code> inliers (good matches), then the probability that we select eight good matches is <code class="literal">w<sup class="calibre20">8</sup>.</code> Consequently, the probability that a selection contains at least one incorrect match is <code class="literal">(1-w<sup class="calibre20">8</sup>)</code>. If we make <code class="literal">k</code> selections, the probability of having one random set that contains good matches only is <code class="literal">1-(1-w<sup class="calibre20">8</sup>)<sup class="calibre20">k</sup></code>.</p><p class="calibre8">This is the confidence probability <code class="literal">c</code>, and we want this probability to be as high as possible, since we need at least one good set of matches in order to obtain the correct fundamental matrix. Therefore, when running the RANSAC algorithm, one needs to determine the number of <code class="literal">k</code> selections that need to be made in order to obtain a given confidence level.</p><p class="calibre8">The use of the RANSAC method to estimate the fundamental matrix is done inside the <code class="literal">ransacTest</code> method of our <code class="literal">RobustMatcher</code> class:</p><pre class="programlisting">    // Identify good matches using RANSAC 
    // Return fundamental matrix and output matches 
    cv::Mat ransacTest(const std::vector&lt;cv::DMatch&gt;&amp; matches,
                       std::vector&lt;cv::KeyPoint&gt;&amp; keypoints1,  
                       std::vector&lt;cv::KeyPoint&gt;&amp; keypoints2,  
                       std::vector&lt;cv::DMatch&gt;&amp; outMatches) { 
 
      // Convert keypoints into Point2f 
      std::vector&lt;cv::Point2f&gt; points1, points2; 
      for (std::vector&lt;cv::DMatch&gt;::const_iterator it= matches.begin(); 
           it!= matches.end(); ++it) { 
 
        // Get the position of left keypoints 
        points1.push_back(keypoints1[it-&gt;queryIdx].pt); 
        // Get the position of right keypoints 
        points2.push_back(keypoints2[it-&gt;trainIdx].pt); 
      } 
 
      // Compute F matrix using RANSAC 
      std::vector&lt;uchar&gt; inliers(points1.size(),0); 
      cv::Mat fundamental=  
         cv::findFundamentalMat( points1,
                         points2,       // matching points 
                         inliers,       // match status (inlier or outlier)   
                         cv::FM_RANSAC, // RANSAC method 
                         distance,      // distance to epipolar line 
                         confidence);   // confidence probability 
      
      // extract the surviving (inliers) matches 
      std::vector&lt;uchar&gt;::const_iterator itIn= inliers.begin(); 
      std::vector&lt;cv::DMatch&gt;::const_iterator itM= matches.begin(); 
      // for all matches 
      for ( ;itIn!= inliers.end(); ++itIn, ++itM) { 
        if (*itIn) { // it is a valid match 
        outMatches.push_back(*itM); 
      } 
    } 
    return fundamental; 
   } 
</pre><p class="calibre8">This code is a bit long because the keypoints need to be converted into <code class="literal">cv::Point2f</code> before the F matrix computation. When using the <code class="literal">cv::findFundamentalMat</code> function with the <code class="literal">cv::FM_RANSAC</code> method, two extra parameters are provided. One of these extra parameters is the confidence level, which determines the number of iterations to be made (by default, it is <code class="literal">0.99</code>). The other parameter is the maximum distance to the epipolar line for a point to be considered as an inlier. All of the matched pairs in which a point is at a greater distance from its epipolar line than the distance specified will be reported as an outlier. The function also returns <code class="literal">std::vector</code> of the character value, indicating that the corresponding match in the input set has been identified either as an outlier (<code class="literal">0</code>) or as an inlier (<code class="literal">1</code>). This explains the last loop of our method that extracts the good matches from the original match set.</p><p class="calibre8">The more good matches you have in your initial match set, the higher the probability that RANSAC will give you the correct fundamental matrix. This is why we applied the crosscheck filter when matching the feature points. You could have also used the ratio test presented in the previous recipe in order to further improve the quality of the final match set. It is just a question of balancing the computational complexity, the final number of matches, and the required level of confidence that the obtained match set will contain only exact matches.</p></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec189" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">The result of the robust matching process presented in this recipe is: 1) an estimate of the fundamental matrix computed using the eight selected matches that have the largest support and 2) the match set included in this support set. Using this information, it is possible to refine these results in two ways.</p><div class="calibre1" title="Refining the fundamental matrix"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch10lvl3sec41" class="calibre6"/>Refining the fundamental matrix</h3></div></div></div><p class="calibre8">Since we now have a match set of good quality, as a last step, it might be a good idea to use all of them to re-estimate the fundamental matrix. We already mentioned that a linear 8-point algorithm to estimate this matrix exists. We can, therefore, obtain an over-determined system of equations that will solve the fundamental matrix in a least-squares sense. This step can be added the end of our <code class="literal">ransacTest</code> function:</p><pre class="programlisting">    // Convert the keypoints in support set into Point2f  
    points1.clear(); 
    points2.clear(); 
    for (std::vector&lt;cv::DMatch&gt;::const_iterator it=  
                                      outMatches.begin(); 
         it!= outMatches.end(); ++it) { 
      // Get the position of left keypoints 
      points1.push_back(keypoints1[it-&gt;queryIdx].pt); 
      // Get the position of right keypoints 
      points2.push_back(keypoints2[it-&gt;trainIdx].pt); 
    } 
 
    // Compute 8-point F from all accepted matches 
    fundamental= cv::findFundamentalMat(  
                      points1,points2, // matching points 
                      cv::FM_8POINT);  // 8-point method solved using SVD 
</pre><p class="calibre8">The <code class="literal">cv::findFundamentalMat</code> function does indeed accept more than <code class="literal">8</code> matches by solving the linear system of equations using singular value decomposition.</p></div><div class="calibre1" title="Refining the matches"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch10lvl3sec42" class="calibre6"/>Refining the matches</h3></div></div></div><p class="calibre8">We learned that in a two-view system, every point must lie on the epipolar line of its corresponding point. This is the epipolar constraint expressed by the fundamental matrix. Consequently, if you have a good estimate of a fundamental matrix, you can use this epipolar constraint to correct the obtained matches by forcing them to lie on their epipolar lines. This can be easily done by using the <code class="literal">cv::correctMatches</code> OpenCV function:</p><pre class="programlisting">    std::vector&lt;cv::Point2f&gt; newPoints1, newPoints2; 
    // refine the matches 
    correctMatches(fundamental,             // F matrix 
                   points1, points2,        // original position 
                   newPoints1, newPoints2); // new position 
</pre><p class="calibre8">This function proceeds by modifying the position of each corresponding point so that it satisfies the epipolar constraint while minimizing the cumulative (squared) displacement.</p></div></div></div>
<div class="calibre1" title="Computing a homography between two images"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch10lvl1sec64" class="calibre6"/>Computing a homography between two images</h1></div></div></div><p class="calibre8">The first recipe of this chapter showed you how to compute the fundamental matrix of an image pair from a set of matches. In projective geometry, another very useful mathematical entity also exists. This one can be computed from multi-view imagery and, as we will see, is a matrix with special properties.</p><div class="calibre1" title="Getting ready"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec190" class="calibre6"/>Getting ready</h2></div></div></div><p class="calibre8">Again, let's consider the projective relation between a 3D point and its image on a camera, which we presented in the introduction section of this chapter. Basically, we learned that this equation relates a 3D point to its image using the intrinsic properties of the camera and the position of that camera (specified with a rotation and a translation component). If we now carefully examine this equation, we realize that there are two special situations of particular interest. The first situation is when two views of a scene are separated by a pure rotation. We can then observe that the fourth column of the extrinsic matrix will be made up of <code class="literal">0</code>s (that is, the translation is null):</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Getting ready" src="graphics/B05388_10_21-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">As a result, the projective relation in this special case becomes a <code class="literal">3x3</code> matrix. A similarly interesting situation also occurs when the object we observe is a plane. In this specific case, we can assume without loss of generality that the points on this plane will be located at <code class="literal">Z=0</code>. As a result, we obtain the following equation:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Getting ready" src="graphics/B05388_10_22-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This zero coordinate of the scene points will then cancel the third column of the projective matrix, which will then again become a <code class="literal">3x3</code> matrix. This special matrix is called a <span><strong class="calibre15">homography</strong></span>, and it implies that, under special circumstances (here, a pure rotation or a planar object), a world point can be related to its image by a linear relation. In addition, because this matrix is invertible, you can also relate an image point on one view directly to its corresponding point on the other view, given that these two views are separated by a pure rotation, or are imaging a planar object. The homographic relation is then of the following form:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Getting ready" src="graphics/B05388_10_23-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Here, <code class="literal">H</code> is a <code class="literal">3x3</code> matrix. This relation holds up to a scale factor represented here by the <code class="literal">s</code> scalar value. Once this matrix is estimated, all the points in one view can be transferred to a second view using this relation. This is the property that will be exploited in this recipe and the next one. Note that, as a side effect of the homography relation, the fundamental matrix becomes undefined in these cases.</p></div><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec191" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Suppose that we have two images separated by a pure rotation. This happens, for example, when you take pictures of a building or a landscape by rotating yourself; as you are sufficiently far away from your subject, the translational component is, in this case, negligible. These two images can be matched using the features of your choice and the <code class="literal">cv::BFMatcher</code> function.</p><p class="calibre8">The result is something like this:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_10_016.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Then, as we did in the previous recipe, we will apply a RANSAC step that will this time involve the estimation of a homography based on a match set (which obviously contains a good number of outliers). This is done by using the <code class="literal">cv::findHomography</code> function, which is very similar to the <code class="literal">cv::findFundamentalMat</code> function:</p><pre class="programlisting">    // Find the homography between image 1 and image 2 
    std::vector&lt;char&gt; inliers; 
    cv::Mat homography= cv::findHomography(       
                            points1,
                            points2,    // corresponding points 
                            inliers,    // outputed inliers matches  
                            cv::RANSAC, // RANSAC method 
                            1.);     //max distance to reprojection point 
</pre><p class="calibre8">Recall that a homography exists (instead of a fundamental matrix) because our two images are separated by a pure rotation. We display here the inlier keypoints as identified by the <code class="literal">inliers</code> argument of the function:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_10_017.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The homography is a <code class="literal">3x3</code> invertible matrix. Therefore, once it has been computed, you can transfer image points from one image to the other. In fact, you can do this for every pixel of an image. Consequently, you can transfer a complete image to the point of view of a second image. This process is called image <span><strong class="calibre15">mosaicing</strong></span> or image <span><strong class="calibre15">stitching</strong></span> and is often used to build a large panorama from multiple images. An OpenCV function that does exactly this is as follows:</p><pre class="programlisting">    // Warp image 1 to image 2 
    cv::Mat result; 
    cv::warpPerspective(image1,       // input image 
                        result,       // output image 
                        homography,   // homography 
                        cv::Size(2*image1.cols,image1.rows));  
                        // size of output image 
</pre><p class="calibre8">Once this new image is obtained, it can be appended to the other image in order to expand the view (since the two images are now from the same point of view):</p><pre class="programlisting">    // Copy image 1 on the first half of full image 
    cv::Mat half(result,cv::Rect(0,0,image2.cols,image2.rows)); 
    image2.copyTo(half);    // copy image2 to image1 roi 
</pre><p class="calibre8">The following image is the result:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_10_018.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec192" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">When two views are related by a homography, it becomes possible to determine where a given scene point on one image is found on the other image. This property becomes particularly interesting for the points in one image that fall outside the image boundaries of the other. Indeed, since the second view shows a portion of the scene that is not visible in the first image, you can use the homography in order to expand the image by reading the color value of the additional pixels in the other image. That's how we were able to create a new image that is an expansion of our second image in which extra columns were added to the right-hand side.</p><p class="calibre8">The homography computed by <code class="literal">cv::findHomography</code> is the one that maps the points in the first image to the points in the second image. This homography can be computed from a minimum of four matches and the RANSAC algorithm is again used here. Once the homography with the best support is found, the <code class="literal">cv::findHomography</code> method refines it using all the identified inliers.</p><p class="calibre8">Now, in order to transfer the points of image <code class="literal">1</code> to image <code class="literal">2</code>, what we need is, in fact, the inverse homography. This is exactly what the <code class="literal">cv::warpPerspective</code> function is doing by default; that is, it uses the inverse of the homography provided as the input to get the color value of each point of the output image (this is what we called backward mapping in <a href="ch02.html" title="Chapter 2. Manipulating Pixels">
Chapter 2
</a>, <span><em class="calibre16">Manipulating Pixels</em></span>). When an output pixel is transferred to a point outside the input image, a black value (<code class="literal">0</code>) is simply assigned to this pixel. Note that a <code class="literal">cv::WARP_INVERSE_MAP</code> flag can be specified as the optional fifth argument in <code class="literal">cv::warpPerspective</code> if you want to use direct homography instead of the inverted one during the pixel transfer process.</p></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec193" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">The <code class="literal">contrib</code> package of OpenCV offers a complete stitching solution that can produce high-quality panoramas from multiple images.</p><div class="calibre1" title="Generating image panoramas with the cv::Stitcher module"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch10lvl3sec43" class="calibre6"/>Generating image panoramas with the cv::Stitcher module</h3></div></div></div><p class="calibre8">The mosaic we obtained in this recipe is good but still contains some defects. The alignment of the images is not perfect and we can clearly see the cut between the two images because the brightness and contrast in the two images are not the same. Fortunately, there is now a stitching solution in OpenCV that looks at all these aspects and tries to produce a panorama of optimal quality. This solution is quite complex and elaborated but, at its core, it relies on the principles learned in this recipe. That is, matching feature points in images and robustly estimating a homography. In addition, the solution estimates the intrinsic and extrinsic camera parameters to ensure a better alignment. It also nicely blends the images together by compensating for the difference in exposure conditions. The high-level call of this function is as follows:</p><pre class="programlisting">    // Read input images 
    std::vector&lt;cv::Mat&gt; images; 
    images.push_back(cv::imread("parliament1.jpg")); 
    images.push_back(cv::imread("parliament2.jpg")); 
 
    cv::Mat panorama;   // output panorama 
    // create the stitcher 
    cv::Stitcher stitcher = cv::Stitcher::createDefault(); 
    // stitch the images 
    cv::Stitcher::Status status = stitcher.stitch(images, panorama); 
 
</pre><p class="calibre8">Numerous parameters in the instance can be adjusted to obtain high-quality results. Interested readers should explore this package in more depth in order to learn more about it. In our case, the result obtained is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Generating image panoramas with the cv::Stitcher module" src="graphics/image_10_019.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Obviously, in general, an arbitrary number of input images can be used to compose a large panorama.</p></div></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec194" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The <span><em class="calibre16">Remapping an image</em></span> recipe in <a href="ch02.html" title="Chapter 2. Manipulating Pixels">
Chapter 2
</a>, <span><em class="calibre16">Manipulating Pixels</em></span>, discusses the concept of backward mapping</li><li class="listitem">The <span><em class="calibre16">Automatic panoramic image stitching using invariant</em></span> features article by <span><em class="calibre16">M. Brown</em></span> and <span><em class="calibre16">D. Lowe</em></span> in <span><em class="calibre16">International Journal of Computer Vision,</em></span>74, 1, 2007, describes a complete method for building panoramas from multiple images</li></ul></div></div></div>
<div class="calibre1" title="Detecting a planar target in images"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch10lvl1sec65" class="calibre6"/>Detecting a planar target in images</h1></div></div></div><p class="calibre8">In the previous recipe, we explained how homographies can be used to stitch together images separated by a pure rotation to create a panorama. In this recipe, we also learned that different images of a plane also generate homographies between views. We will now see how we can make use of this fact to recognize a planar object in an image.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec195" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Suppose you want to detect the occurrence of a planar object in an image. This object could be a poster, painting, logo, signage, and so on. Based on what we have learned in this chapter, the strategy would consist of detecting feature points on this planar object and to try to match them with the feature points in the image. These matches would then be validated using a robust matching scheme similar to the one we used previously, but this time based on a homography. If the number of valid matches is high, then this must mean that our planar object is visible in the current image.</p><p class="calibre8">In this recipe, our mission is to detect the occurrence of the first edition of our book in an image, more specifically, the following image:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_10_020-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Let's define a <code class="literal">TargetMatcher</code> class that is very similar to our <code class="literal">RobustMatcher</code> class:</p><pre class="programlisting">    class TargetMatcher { 
      private: 
      // pointer to the feature point detector object 
      cv::Ptr&lt;cv::FeatureDetector&gt; detector; 
      // pointer to the feature descriptor extractor object 
      cv::Ptr&lt;cv::DescriptorExtractor&gt; descriptor; 
      cv::Mat target;           // target image 
      int normType;             // to compare descriptor vectors 
      double distance;          // min reprojection error 
      int numberOfLevels;       // pyramid size 
      double scaleFactor;       // scale between levels 
      // the pyramid of target images and its keypoints 
      std::vector&lt;cv::Mat&gt; pyramid; 
      std::vector&lt;std::vector&lt;cv::KeyPoint&gt;&gt; pyrKeypoints; 
      std::vector&lt;cv::Mat&gt; pyrDescriptors; 
</pre><p class="calibre8">The reference image of the planar object to be matched is held by the <code class="literal">target</code> attribute. As it will be explained in the next section, feature points will be detected in a pyramid of images of the target successively down-sampled. The matching methods are similar to the ones of the <code class="literal">RobustMatcher</code> class, except that they include <code class="literal">cv::findHomography</code> instead of <code class="literal">cv::findFundamentalMat</code> in the <code class="literal">ransacTest</code> method.</p><p class="calibre8">To use the <code class="literal">TargetMatcher</code> class, a specific feature point detector and descriptor must be instantiated and passed to the constructor:</p><pre class="programlisting">    // Prepare the matcher 
    TargetMatcher tmatcher(cv::FastFeatureDetector::create(10), 
                           cv::BRISK::create()); 
    tmatcher.setNormType(cv::NORM_HAMMING); 
</pre><p class="calibre8">Here, we selected the FAST detector in conjunction with the BRISK descriptor because they are quick to compute. Then, you must specify the target to be detected:</p><pre class="programlisting">    // set the target image 
    tmatcher.setTarget(target); 
</pre><p class="calibre8">In our case, this is the following image:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_10_021.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">You can detect this target in an image by calling the <code class="literal">detectTarget</code> method:</p><pre class="programlisting">    // match image with target 
    tmatcher.detectTarget(image, corners); 
</pre><p class="calibre8">This method returns the position of the four corners of the target in the image (if found). Lines can then be drawn to visually validate the detection:</p><pre class="programlisting">   // draw the target corners on the image 
    if (corners.size() == 4) { // we have a detection 
 
      cv::line(image, cv::Point(corners[0]),  
               cv::Point(corners[1]),
               cv::Scalar(255, 255, 255), 3); 
      cv::line(image, cv::Point(corners[1]),  
               cv::Point(corners[2]), 
               cv::Scalar(255, 255, 255), 3); 
      cv::line(image, cv::Point(corners[2]),  
               cv::Point(corners[3]),
               cv::Scalar(255, 255, 255), 3); 
      cv::line(image, cv::Point(corners[3]),  
               cv::Point(corners[0]),
               cv::Scalar(255, 255, 255), 3); 
    } 
</pre><p class="calibre8">The result is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_10_022.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec196" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">Since we do not know what the size of the target in the image is, we have decided to build a pyramid made of the target image in different sizes. Another option would have been to use scale-invariant features. At each level of our pyramid, the size of the target image is reduced by a certain factor (attribute <code class="literal">scaleFactor</code>, <code class="literal">0.9</code> by default) and the pyramid is made of a number of levels (attribute <code class="literal">numberOfLevels</code>, <code class="literal">8</code> by default). Feature points are detected for each level of the pyramid:</p><pre class="programlisting">    // Set the target image 
    void setTarget(const cv::Mat t) { 
 
      target= t; 
      createPyramid(); 
    } 
    // create a pyramid of target images 
    void createPyramid() { 
 
      // create the pyramid of target images 
      pyramid.clear(); 
      cv::Mat layer(target); 
      for (int i = 0;  
           i &lt; numberOfLevels; i++) { // reduce size at each layer 
        pyramid.push_back(target.clone()); 
        resize(target, target, cv::Size(), scaleFactor, scaleFactor); 
      } 
 
      pyrKeypoints.clear(); 
      pyrDescriptors.clear(); 
      // keypoint detection and description in pyramid 
      for (int i = 0; i &lt; numberOfLevels; i++) { 
        // detect target keypoints at level i 
        pyrKeypoints.push_back(std::vector&lt;cv::KeyPoint&gt;()); 
        detector-&gt;detect(pyramid[i], pyrKeypoints[i]); 
        // compute descriptor at level i 
        pyrDescriptors.push_back(cv::Mat()); 
        descriptor-&gt;compute(pyramid[i],  
                            pyrKeypoints[i],
                            pyrDescriptors[i]); 
      } 
    } 
</pre><p class="calibre8">The <code class="literal">detectTarget</code> method then proceeds onto three steps. Firstly, interest points are detected in the input image. Secondly, this image is robustly matched with each image of the target pyramid. The level with the highest number of inliers is retained. If this one has a sufficiently high number of surviving matches, then we have found the target. The third step consists of reprojecting the four corners of the target to the correct scale onto the input image using the found homography and the <code class="literal">cv::getPerspectiveTransform</code> function:</p><pre class="programlisting">    // detect the defined planar target in an image 
    // returns the homography and 
    // the 4 corners of the detected target 
    cv::Mat detectTarget( 
                  const cv::Mat&amp; image, // position of the 
                                        // target corners (clock-wise) 
                  std::vector&lt;cv::Point2f&gt;&amp; detectedCorners) { 
 
      // 1. detect image keypoints 
      std::vector&lt;cv::KeyPoint&gt; keypoints; 
      detector-&gt;detect(image, keypoints); 
      // compute descriptors 
      cv::Mat descriptors; 
      descriptor-&gt;compute(image, keypoints, descriptors); 
 
      std::vector&lt;cv::DMatch&gt; matches; 
      cv::Mat bestHomography; 
      cv::Size bestSize; 
      int maxInliers = 0; 
      cv::Mat homography; 
 
      // Construction of the matcher   
      cv::BFMatcher matcher(normType); 
 
      // 2. robustly find homography for each pyramid level 
      for (int i = 0; i &lt; numberOfLevels; i++) { 
        // find a RANSAC homography between target and image 
        matches.clear(); 
        // match descriptors 
        matcher.match(pyrDescriptors[i], descriptors, matches); 
        // validate matches using RANSAC 
        std::vector&lt;cv::DMatch&gt; inliers; 
        homography = ransacTest(matches, pyrKeypoints[i],  
                                keypoints, inliers); 
 
        if (inliers.size() &gt; maxInliers) { // we have a better H 
          maxInliers = inliers.size(); 
          bestHomography = homography; 
          bestSize = pyramid[i].size(); 
        } 
 
      } 
 
      // 3. find the corner position on the image using best homography 
      if (maxInliers &gt; 8) { // the estimate is valid 
 
        //target corners at best size 
        std::vector&lt;cv::Point2f&gt; corners; 
        corners.push_back(cv::Point2f(0, 0)); 
        corners.push_back(cv::Point2f(bestSize.width - 1, 0)); 
        corners.push_back(cv::Point2f(bestSize.width - 1,  
                                      bestSize.height - 1)); 
        corners.push_back(cv::Point2f(0, bestSize.height - 1)); 
 
        // reproject the target corners 
        cv::perspectiveTransform(corners, detectedCorners, bestHomography); 
      } 
 
      return bestHomography; 
    } 
</pre><p class="calibre8">The following image shows the matching results obtained in the case of our example:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_10_023.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch10lvl2sec197" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The <span><em class="calibre16">Fast and robust homography scheme for real-time planar target detection</em></span> article by <span><em class="calibre16">H. Bazargani</em></span>, <span><em class="calibre16">O. Bilaniuk</em></span> and <span><em class="calibre16">R. Laganière</em></span> in <span><em class="calibre16">Journal of Real-Time Image Processing</em></span>, May 2015, describes a method to detecting a planar target in real-time. It also describes the <code class="literal">cv::RHO</code> method for the <code class="literal">cv::findHomography</code> function.</li></ul></div></div></div></body></html>