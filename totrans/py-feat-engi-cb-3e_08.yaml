- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating New Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding new features to a dataset can help machine learning models learn patterns
    and important details in the data. For example, in finance, the **disposable income**,
    which is the *total income* minus the *acquired debt* for any one month, might
    be more relevant for credit risk than just the income or the acquired debt. Similarly,
    the *total acquired debt* of a person across financial products, such as a car
    loan, a mortgage, and credit cards, might be more important to estimate the credit
    risk than any debt considered individually. In these examples, we use domain knowledge
    to craft new variables, and these variables are created by adding or subtracting
    existing features.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, a variable may not have a linear or monotonic relationship with
    the target, but a polynomial combination might. For example, if our variable has
    a quadratic relationship with the target, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>y</mi><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></mrow></math>](img/36.png),
    we can convert that into a linear relationship by squaring the original variable.
    We can also help linear models better understand the relationships between variables
    and targets by transforming the predictors through splines, or by using decision
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of crafting additional features to train simpler models, such
    as linear or logistic regression, is that both the features and the models remain
    interpretable. We can explain the reasons driving a model’s output to management,
    clients, and regulators, adding a layer of transparency to our machine learning
    pipelines. In addition, simpler models tend to be faster to train and easier to
    deploy and maintain.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will create new features by transforming or combining variables
    with mathematical functions, splines, and decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Combining features with mathematical functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing features to reference variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing polynomial expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining features with decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating periodic features from cyclical variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating spline features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use the `pandas`, `numpy`, `matplotlib`, `scikit-learn`,
    and `feature-engine` Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Combining features with mathematical functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'New features can be created by combining existing variables with mathematical
    and statistical functions. Taking an example from the finance industry, we can
    calculate the total debt of a person by summing up their debt across individual
    financial products, such as car loan, mortgage, or credit card debt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Total debt = car loan debt + credit card debt +* *mortgage debt*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also derive other insightful features using alternative statistical
    operations. For example, we can determine the maximum debt of a customer across
    financial products or the average time a user spends on a website:'
  prefs: []
  type: TYPE_NORMAL
- en: '*maximum debt = max(car loan balance, credit card balance,* *mortgage balance)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*average time on website = mean(time spent on homepage, time spent on about
    page, time spent on* *FAQ page)*'
  prefs: []
  type: TYPE_NORMAL
- en: We can, in principle, use any mathematical or statistical operation to create
    new features, such as the product, mean, standard deviation, or maximum or minimum
    values. In this recipe, we will implement these mathematical operations using
    `pandas` and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While, in the recipe, we can show you how to combine features with mathematical
    functions, we can’t do justice to the use of domain knowledge in deciding which
    function to apply, as that varies with every domain. So, we will leave that with
    you.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use the breast cancer dataset from `scikit-learn`. The
    features are computed from digitized images of breast cells and describe the characteristics
    of their cell nuclei, in terms of smoothness, concavity, symmetry, and compactness,
    among others. Each row contains information about the morphology of cell nuclei
    in a tissue sample. The target variable indicates whether the tissue sample corresponds
    to cancerous cells. The idea is to predict whether the tissue samples belong to
    benign or malignant breast cells, based on their cell nuclei morphology.
  prefs: []
  type: TYPE_NORMAL
- en: 'To become familiar with the dataset, run the following commands in a Jupyter
    notebook or Python console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block should print out a description of the dataset and an
    interpretation of its variables.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will create new features by combining variables using multiple
    mathematical operations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by loading the necessary libraries, classes, and data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, load the breast cancer dataset into a `pandas` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the following code lines, we will create new features by combining variables
    using multiple mathematical operations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s begin by creating a list with the subset of the features that we want
    to combine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The features in *step 3* represent the mean characteristics of cell nuclei in
    the images. It might be useful to obtain the mean across all examined characteristics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s get the mean value of the features and then display the resulting feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output shows the mean value of the features from *step 3*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, to capture the general variability of the cell nuclei, let’s determine
    the standard deviation of the mean characteristics, and then display the resulting
    feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output shows the standard deviation of the features from step
    3:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When we craft new features based on domain knowledge, we know exactly how we
    want to combine the variables. We could also combine features with multiple operations
    and then evaluate whether they are predictive, using, for example, a feature selection
    algorithm or deriving feature importance from the machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make a list containing mathematical functions that we want to use to
    combine the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s apply the functions from *step 6* to combine the features from *step
    3*, capturing the resulting variables in a new DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `df_t.head()`, we will see the DataFrame with the newly created
    features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.1 – A DataFrame with the newly created features](img/B22396_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – A DataFrame with the newly created features
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` `agg` can apply multiple functions to combine features. It can take
    a list of strings with the function names, as we did in *step 7*; a list of NumPy
    functions, such as `np.log`; and Python functions that you create.'
  prefs: []
  type: TYPE_NORMAL
- en: We can create the same features that we created with `pandas` automatically
    by using `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a list by using the name of the output features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up `MathFeatures()` to apply the functions in *step 6* to the features
    from *step 3*, naming the new features with the strings from *step 8*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s add the new features to the original DataFrame, capturing the result
    in a new variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can display the input and output features by executing `df_t[features +`
    `new_feature_names].head()`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2 – DataFrame with the input features and the newly created variables](img/B22396_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – DataFrame with the input features and the newly created variables
  prefs: []
  type: TYPE_NORMAL
- en: While `pandas` `agg` returns a DataFrame with the features resulting from the
    operation, `feature-engine` goes one step further, by concatenating the new features
    to the original DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`pandas` has many built-in operations to apply mathematical and statistical
    computations to a group of variables. To combine features mathematically, we first
    made a list containing the names of the features we wanted to combine. Then, we
    determined the mean and standard deviation of those features by using `pandas`
    `mean()` and `std()`. We could also apply any of the `sum()`, `prod()`, `max()`,
    and `min()` methods, which return the sum, product, maximum, and minimum values
    of those features, respectively. To perform these operations across the columns,
    we added the `axis=1` argument within the methods.'
  prefs: []
  type: TYPE_NORMAL
- en: With pandas `agg()`, we applied several mathematical functions simultaneously.
    It takes as arguments a list of strings, corresponding to the functions to apply
    and the `axis` that the functions should be applied to, which can be either `1`
    for columns or `0` for rows. As a result, pandas `agg()` returned a `pandas` DataFrame,
    resulting from applying the mathematical functions to the groups of features.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we created the same features by combining variables with `feature-engine`.
    We used the `MathFeatures()` transformer, which takes the features to combine
    and the functions to apply as input; it also has the option to indicate the names
    of the resulting features. When we used `fit()`, the transformer did not learn
    parameters but checked that the variables were indeed numerical. The `transform()`
    method triggered the use of `pandas.agg` under the hood, applying the mathematical
    functions to create the new variables.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To find out more about the mathematical operations supported by `pandas`, visit
    [https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats).
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about `pandas` `aggregate`, check out [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.aggregate.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.aggregate.html).
  prefs: []
  type: TYPE_NORMAL
- en: Comparing features to reference variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipe, *Combining features with mathematical functions*, we
    created new features by applying mathematical or statistical functions, such as
    the sum or the mean, to a group of variables. Some mathematical operations, however,
    such as subtraction or division, are performed *between* features. These operations
    are useful to derive ratios, such as the *debt-to-income ratio*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*debt-to-income ratio = total debt /* *total income*'
  prefs: []
  type: TYPE_NORMAL
- en: 'These operations are also useful to compute differences, such as the *disposable
    income*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*disposable income = income -* *total debt*'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to create new features by subtracting or dividing
    variables with `pandas` and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In the recipe, we will show you how to create features with subtraction and
    division. We hope that the examples, relating to the financial sector, shed some
    light on how to use domain knowledge to decide which features to combine and how.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by loading the necessary Python libraries and the breast cancer
    dataset from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the necessary libraries, classes, and data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the breast cancer dataset into a `pandas` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the breast cancer dataset, some features capture the worst and the mean characteristics
    of the cell nuclei of breast cells. For example, for each image (that is, for
    each row), we have the worst compactness observed in all nuclei and the mean compactness
    of all nuclei. A feature that captures the difference between the worst and the
    mean value could predict malignancy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s capture the difference between two features, the `worst compactness`
    and `mean compactness` of cell nuclei, in a new variable and display its values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see the difference between these feature values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can perform the same calculation by executing `df["difference"] = df["worst
    compactness"] - (``df["mean compactness"])`.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the ratio between the worst and the average characteristic of the
    cell nuclei might be indicative of malignancy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a new feature with the ratio between the worst and mean radius
    of the nuclei, and then display its values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see the values corresponding to the ratio between
    the features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can calculate the ratio by executing an alternative command, `df["quotient"]
    = df["worst radius"] / (``df["mean radius"])`.
  prefs: []
  type: TYPE_NORMAL
- en: We can also capture the ratio and difference between every nuclei morphology
    characteristic and the mean radius or mean area of the nuclei. Let’s begin by
    capturing these subsets of variables into lists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make a list of the features in the numerator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a list of the features in the denominator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can create features by dividing the features in *step 5* by one of the features
    in *step 6* with `pandas`, by executing `df[features].div(df["mean radius"])`.
    For subtraction, we’d execute `df[features].sub(df["mean radius"])`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up the `feature-engine` library’s `RelativeFeatures()` so that it
    subtracts or divides every feature from *step 5* with respect to the features
    from *step 6*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Subtracting the features from *step 5* and *step 6* does not make biological
    sense, but we will do it anyway to demonstrate the use of the `RelativeFeatures()`
    transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add the new features to the DataFrame and capture the result in a new
    variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s capture the names of the new features in a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_names_in_` is a common attribute in `scikit-learn` and `feature-engine`
    transformers and stores the name of the variables from the DataFrame used to fit
    the transformer. In other words, it stores the names of the input features. When
    using `transform()`, the transformers check that the features from the new input
    dataset match those used during training. In *step 9*, we leverage this attribute
    to find the additional variables added to the data after the transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we execute `print(new_features)`, we will see a list with the names of the
    features created by `ReferenceFeatures()`. Note that the features contain the
    variables on the left- and right-hand sides of the mathematical equation, plus
    the function that was applied to them to create the new feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can display the first five rows of the resulting variables by executing
    `df_t[new_features].head()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – A DataFrame with the newly created features](img/B22396_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – A DataFrame with the newly created features
  prefs: []
  type: TYPE_NORMAL
- en: '`feature-engine` adds new features as columns at the right of the original
    DataFrame and automatically adds variable names to those features. By doing so,
    `feature-engine` automates much of the manual work that we would do with `pandas`.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`pandas` has many built-in operations to compare a feature or a group of features
    to a reference variable. In this recipe, we used pandas `sub()` and `div()` to
    determine the difference or the ratio between two variables, or a subset of variables
    and one reference feature.'
  prefs: []
  type: TYPE_NORMAL
- en: To subtract one variable from another, we applied `sub()` to a `pandas` series
    with the first variable, passing the `pandas` series with the second variable
    as an argument to `sub()`. This operation returned a third `pandas` series with
    the difference between the first and second variables. To divide one variable
    from another, we used `div()`, which works identically to `sub()` – that is, it
    divides the variable on the left by the variable passed as an argument of `div()`.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we combined several variables with two reference variables automatically
    via subtraction or division, by utilizing `ReferenceFeatures()` from `Feature-engine`.
    The `ReferenceFeatures()` transformer takes the variables to be combined, the
    reference variables, and the functions to use to combine them. When using `fit()`,
    the transformer did not learn about parameters but checked that the variables
    were numerical. Executing `transform()` added the new features to the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`ReferenceFeatures()` can also add, multiply, get the modulo, or get the power
    of a group of variables relating to a second group of reference variables. You
    can find out more in its documentation: [https://feature-engine.readthedocs.io/en/latest/api_doc/creation/RelativeFeatures.html](https://feature-engine.readthedocs.io/en/latest/api_doc/creation/RelativeFeatures.html).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To learn more about the binary operations supported by `pandas`, visit [https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#binary-operator-functions](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#binary-operator-functions).
  prefs: []
  type: TYPE_NORMAL
- en: Performing polynomial expansion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Simple models, such as linear and logistic regression, can capture complex
    patterns if we feed them the right features. Sometimes, we can create powerful
    features by combining the variables in our datasets with themselves or with other
    variables. For example, in the following figure, we can see that the target, *y*,
    has a quadratic relation with the variable, *x*, and as shown in the left panel,
    a linear model is not able to capture that relationship accurately:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 8.4 – A linear model fit to predict a target, y, from a feature, x,\
    \ which has a quadratic relationship to the target, before and after squaring\
    \ x. In the left panel: the model offers a poor fit by using the original variable;\
    \ in the right panel, the model offers a better fit, based on the squar\uFEFF\
    e of the original variable](img/B22396_08_4.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4 – A linear model fit to predict a target, y, from a feature, x,
    which has a quadratic relationship to the target, before and after squaring x.
    In the left panel: the model offers a poor fit by using the original variable;
    in the right panel, the model offers a better fit, based on the square of the
    original variable'
  prefs: []
  type: TYPE_NORMAL
- en: This linear model has a quadratic relationship to the target, before and after
    squaring *x*. However, if we square *x*, or, in other words, if we create a second-degree
    polynomial of the feature, the linear model can accurately predict the target,
    *y*, from the square of *x*, as we see in the right panel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another classical example in which a simple feature can make a simple model,
    such as logistic regression, understand the underlying relationship in the data
    is the **XOR** situation. In the left panel of the following diagram, we see how
    the target class is distributed across the values of *x1* and *x2* (the class
    is highlighted with different color shades):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – An illustration of the XOR relationship and how combining features
    allows a full class separation](img/B22396_08_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – An illustration of the XOR relationship and how combining features
    allows a full class separation
  prefs: []
  type: TYPE_NORMAL
- en: If both features are positive, or both features are negative, then the class
    is 1, but if the features take different signs, then the class is 0 (left panel).
    Logistic regression will not be able to pick this pattern from each individual
    feature because, as we can see in the middle panel, there is significant class
    overlap across the values of the feature – in this case, x1\. However, multiplying
    x1 by x2 creates a feature that allows a logistic regression to predict the classes
    accurately because x3, as can we see in the right panel, allows the classes to
    be clearly separated.
  prefs: []
  type: TYPE_NORMAL
- en: With similar logic, polynomial combinations of the same or different variables
    can return new variables that convey additional information and capture feature
    interaction thereby resulting in useful inputs for linear models. With huge datasets,
    analyzing every possible variable combination is not always possible. But we can
    create several polynomial variables automatically, using, for example, `scikit-learn`,
    and we can let the model decide which variables are useful. In this recipe, we
    will learn how to create multiple features through polynomial combinations using
    scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Polynomial expansion serves to automate the creation of new features, capture
    feature interaction, and potential non-linear relationships between the original
    variables and the target. To create polynomial features, we need to determine
    which features to combine and which polynomial degree to use.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While determining the features to combine or the degree of the polynomial combination
    is not an easy task, keep in mind that high polynomial degrees will result in
    a lot of new features and may lead to overfitting. In general, we keep the degree
    low, to a maximum of 2 or 3.
  prefs: []
  type: TYPE_NORMAL
- en: The `PolynomialFeatures()` transformer from `scikit-learn` creates polynomial
    combinations of the features with a degree less than or equal to a user-specified
    degree, automatically.
  prefs: []
  type: TYPE_NORMAL
- en: To follow up easily with this recipe, let’s first understand the output of `PolynomialFeatures()`
    when used to create second- and third-degree polynomial combinations of three
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second-degree polynomial combinations of three variables – *a*, *b*, and *c*
    – will return the following new features:'
  prefs: []
  type: TYPE_NORMAL
- en: '*1, a, b, c, ab, ac, bc, a2,* *b2, c2*'
  prefs: []
  type: TYPE_NORMAL
- en: From the previous features, *a*, *b*, and *c* are the original variables; *ab*,
    *ac*, and *bc* are the products of those features; and *a2*, *b2*, and *c2* are
    the squared values of the original features. `PolynomialFeatures()` also returns
    the bias term *1*, which we would probably exclude when creating features.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The resulting features – *ab*, *ac*, and *bc* – are called **interactions**
    or feature interactions of **degree 2**. The degree reflects the number of variables
    combined. The result combines a maximum of two variables because we indicated
    a second-degree polynomial as the maximum allowed combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Third-degree polynomial combinations of the three variables – *a*, *b*, and
    *c* – will return the following new features:'
  prefs: []
  type: TYPE_NORMAL
- en: '*1, a, b, c, ab, ac, bc, abc, a2b, a2c, b2a, b2c, c2a, c2b, a3,* *b3, c3*'
  prefs: []
  type: TYPE_NORMAL
- en: Among the returned features, in addition to those returned by the second-degree
    polynomial combination, we now have the third-degree combinations of the features
    with themselves (*a3*, *b3*, and *c3*), the squared values of every feature combined
    linearly with a second feature (*a2b*, *a2c*, *b2a*, *b2c*, *c2a*, and *c2b*),
    and the product of the three features (*abc*). Note how we have all possible interactions
    of degrees 1, 2, and 3 and the bias term *1*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the output of the polynomial expansion implemented by
    `scikit-learn`, let’s jump into the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will create features with polynomial expansion using a toy
    dataset to become familiar with the resulting variables. Creating features with
    the polynomial expansion of a real dataset is identical to what we will discuss
    in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required libraries, classes, and data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set `scikit-learn` library’s `set_output` API globally so that all transformers
    return a DataFrame as a result of the `transform()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a DataFrame containing one variable, with values from 1 to 10:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up `PolynomialFeatures()` to create all possible combinations up
    to a third-degree polynomial of the single variable and exclude the bias term
    from the result – that is, we will exclude the value *1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create the polynomial combinations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `dft`, we’ll see a DataFrame with the original feature, followed
    by its values squared, and then its values to the power of three:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.6 – A DataFrame with the polynomial expansion of the third degree
    of a single variable](img/B22396_08_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – A DataFrame with the polynomial expansion of the third degree of
    a single variable
  prefs: []
  type: TYPE_NORMAL
- en: If, instead of returning a DataFrame, `PolynomialFeatures()` returns a NumPy
    array and you want to obtain the names of the features in the array, you can do
    so by executing `poly.get_feature_names_out()`, which returns `array(['var', 'var^2',
    '``var^3'], dtype=object)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s plot the new feature values against the original variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following diagram, we can see the relationship between the polynomial
    features and the original variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.7 – The relationship between the features resulting from polynomial
    expansion and the original variable](img/B22396_08_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – The relationship between the features resulting from polynomial
    expansion and the original variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add two additional variables to our toy dataset, with values from 1 to
    10:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s combine the three features in the dataset with polynomial expansion
    up to the second degree, but this time, we will only return features produced
    by combining at least two different variables – that is, the interaction features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `dft`, we will see the features resulting from the polynomial
    expansion, which contain the original features, plus all possible combinations
    of the three variables but without the quadratic terms, as we set the transformer
    to return only the interaction between features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.8 – A DataFrame with the result of creating features with polynomial\
    \ expansion but retaining only the interactio\uFEFFn between variables](img/B22396_08_8.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – A DataFrame with the result of creating features with polynomial
    expansion but retaining only the interaction between variables
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and create third-degree polynomial combinations of the features, returning
    only the interactions or all possible features to get a better sense of the output
    of `PolynomialFeatures()`.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve learned how to create new features by combining existing variables
    with themselves or other features in data. Creating features via polynomial expansion
    using a real dataset is, in essence, identical.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to combine only a subset of features, you can select the features
    to combine by utilizing `ColumnTransformer()`, as we will demonstrate in the *There’s
    more…* section ahead in this recipe, or by using `SklearnTransformerWrapper()`
    from `feature-engine`, as you can see in the accompanying GitHub repository: [https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch08-creation/Recipe3-PolynomialExpansion.ipynb](https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch08-creation/Recipe3-PolynomialExpansion.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we created features by using polynomial combinations of a feature
    with itself or among three variables. To create these polynomial features, we
    used `PolynomialFeatures()` from `scikit-learn`. By default, `PolynomialFeatures()`
    generates a new feature matrix consisting of all polynomial combinations of the
    features in the data, with a degree less than or equal to the user-specified `degree`.
    By setting `degree` to `3`, we created all possible polynomial combinations of
    a degree of 3 or smaller. To retain the combination of a feature with itself,
    we set the `interaction_only` parameter to `False`. To avoid returning the bias
    term, we set the `include_bias` parameter to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Setting the `interaction_only` parameter to `True` returns the interaction terms
    only – that is, the variables resulting from combinations of two or more variables.
  prefs: []
  type: TYPE_NORMAL
- en: The `fit()` method determined all of the possible feature combinations based
    on the parameters specified. At this stage, the transformer did not perform actual
    mathematical computations. The `transform()` method performed the mathematical
    computations with the features to create the new variables. With the `get_feature_names()`
    method, we could identify the terms of the expansion – that is, how each new feature
    was calculated.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 2*, we set `scikit-learn` library’s `set_output` API `pandas` DataFrames
    as a result of the `transform()` method. scikit-learn transformers return `NumPy`
    arrays by default. The new `set_output` API allows us to change the container
    of the result to a `pandas` or a `polars` DataFrame. We can set the output individually
    every time we set up a transformer – for example, by using `poly = PolynomialFeatures().set_output(transform="pandas")`.
    Alternatively, as we did in this recipe, we can set the global configuration,
    and then every time we set up a new transformer, it will return a `pandas` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create features by performing polynomial expansion on a subset of variables
    in the breast cancer dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the necessary libraries, classes, and data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, load the data and separate it into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make a list with the features to combine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up `PolynomialFeatures()` to create all possible combinations up to the
    third degree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up the column transformer to create features only from those specified
    in *step 3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the polynomial features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And that’s it. By executing `ct.get_feature_names_out()`, we obtain the names
    of the new features.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`ColumnTransformer()` will append the word `poly` to the resulting variables,
    which is the name we gave to the step within `ColumnTransformer()` in *step 5*.
    I am not a huge fan of this behavior because it makes data analysis harder, as
    you need to keep track of the variable name changes. To avoid variable name changes,
    you can use `feature-engine`’s `SklearnTransformerWrapper()` instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Combining features with decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the winning solution of the **Knowledge Discovery and Data** Mining (**KDD**)
    competition in 2009, the authors created new features by combining two or more
    variables using decision trees. When examining the variables, they noticed that
    some features had a high level of mutual information with the target yet low correlation,
    indicating that the relationship with the target was not linear. While these features
    were predictive when used in tree-based algorithms, linear models could not take
    advantage of them. Hence, to use these features in linear models, they replaced
    the features with the outputs of decision trees trained on the individual features,
    or combinations of two or three variables, to return new features with a monotonic
    relationship with the target.
  prefs: []
  type: TYPE_NORMAL
- en: In short, combining features with decision trees is useful for creating features
    that show a monotonic relationship with the target, which is useful for making
    accurate predictions using linear models. The procedure consists of training a
    decision tree using a subset of the features – typically, one, two, or three at
    a time – and then using the prediction of the tree as a new feature.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more details about this procedure and the overall winning solution
    of the 2009 KDD data competition in this article: [http://proceedings.mlr.press/v7/niculescu09/niculescu09.pdf](http://proceedings.mlr.press/v7/niculescu09/niculescu09.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that we can automate the creation of features using trees,
    using `feature-engine`, and in this recipe, we will learn how to do so.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll combine features with decision trees, using the California
    housing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by importing `pandas` and the required functions, classes, and
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset into a `pandas` DataFrame and remove
    the `Latitude` and `Longitude` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Separate the dataset into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check out Pearson’s correlation coefficient between the features and the target,
    which is a measure of a linear relationship:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that, apart from `MedInc`, most variables
    do not show a strong linear relationship with the target; the correlation coefficient
    is smaller than 0.5:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a grid of hyperparameters to optimize each decision tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `feature-engine` library’s `DecisionTreeFeatures()` allows us to add features
    resulting from the predictions of a decision tree, trained on one or more features.
    There are many ways in which we can instruct the transformer to combine the features.
    We’ll start by creating all possible combinations between two variables.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make a list with the two features that we want to use as inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up `DecisionTreeFeatures()` to create all possible combinations between
    the features from *step 6*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We set `regression` to `True` because the target in this dataset is continuous.
    If you have a binary target or are performing classification, set it to `False`.
    Make sure to select an evaluation metric (`scoring`) that is suitable for your
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the transformer so that it trains the decision trees on the input features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you wonder which features have been used to train decision trees, you can
    inspect them like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that `DecisionTreeFeatures()` has trained
    three decision trees – two by using the single features, `AveRooms` and `AveBedrms`,
    and one by using both features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`DecisionTreeFeatures()` also stores the decision trees. You can check them
    out by executing `dtf.estimators_`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, add the features to the training and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make a list with the name of the new features (the transformer appends the
    word `tree` to the feature names):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, display the features that were added to the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see the first five rows of the new features,
    resulting from the decision trees trained in *step 8*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![    Figure 8.9 – A portion of the testing set containing the features derived
    from the decision trees](img/B22396_08_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – A portion of the testing set containing the features derived from
    the decision trees
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the power of this transformation, calculate Pearson’s correlation
    coefficient between the new features and the target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that the correlation between the new variables
    and the target is greater than the correlation shown by the original features
    (compare these values with those of *step 4*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you want to combine specific features instead of getting all possible combinations
    between variables, you can do so by specifying the input features in tuples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a tuple of tuples with the different features that we want to use as
    input for decision trees:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to pass these tuples to the `features_to_combine` parameter of
    `DecisionTreeFeatures()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We fitted the transformer in the previous step, so we can go ahead and add
    the features to training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the new features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see the new features derived from predictions
    of decision trees in the test set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![    Figure 8.10 – A portion of the testing set containing the features derived
    from the decision trees](img/B22396_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – A portion of the testing set containing the features derived from
    the decision trees
  prefs: []
  type: TYPE_NORMAL
- en: To wrap up the recipe, we’ll compare the performance of a Lasso linear regression
    model trained using the original features with one using the features derived
    from the decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `Lasso` and the `cross_validate` function from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up a Lasso regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train and evaluate the model using the original data with cross-validation,
    and then print out the resulting *r*-squared:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see the *r*-squared of the Lasso regression
    model trained using the original features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, train a Lasso regression model with the features derived from the
    decision trees and evaluate it with cross-validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that the performance of the Lasso regression
    model trained based of the tree-derived features is better; the *r*-square is
    greater than that from *step 20*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: I hope I’ve given you a flavor of the power of combining features with decision
    trees and how to do so with `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we created new features based on the predictions of decision
    trees trained on one or more variables. We used `DecisionTreeFeatures()` from
    `Feature-engine` to automate the process of training the decision trees with cross-validation
    and hyperparameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '`DecisionTreeFeatures()` trains decision trees using grid-search under the
    hood. Hence, you can pass a grid of hyperparameters to optimize the tree, or the
    transformer will optimize just the depth, which, in any case, is the most important
    parameter in a decision tree. You can also change the metric you want to optimize
    through the `scoring` parameter and the cross-validation scheme you want to use
    through the `cv` parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: The most exciting feature of `DecisionTreeFeatures()` is its ability to infer
    the feature combinations to create tree-derived features, which is regulated through
    the `features_to_combine` parameter. If you pass an integer to this parameter
    – say, for example, `3`, `DecisionTreeFeatures()` will create all possible combinations
    of 1, 2, and 3 features and use these to train the decision trees. Instead of
    an integer, you can pass a list of integers – say, `[2,3]` – in which case, `DecisionTreeFeatures()`
    will create all possible combinations of 2 and 3 features. You can also specify
    which features you want to combine and how by passing the feature combinations
    in tuples, as we did in *step 14*.
  prefs: []
  type: TYPE_NORMAL
- en: With `fit()`, `DecisionTreeFeatures()` finds the feature combinations and trains
    the decision trees. With `transform()`, `DecisionTreeFeatures()` adds the features
    resulting from the decision trees to the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are training regression or multi-class classification, the new features
    will be either the prediction of the continuous target or the class. If you are
    training a binary classification model, the new features will result from the
    probability of class 1.
  prefs: []
  type: TYPE_NORMAL
- en: After adding the new features, we compared their relationship to the target
    by analyzing Pearson’s correlation coefficient, which returned a measure of linear
    association. We saw that the features derived from trees had a greater correlation
    coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to know more about what mutual information is and how to calculate
    it, check out this article: [https://www.blog.trainindata.com/mutual-information-with-python/](https://www.blog.trainindata.com/mutual-information-with-python/).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating periodic features from cyclical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some features are periodic – for example, the hours in a day, the months in
    a year, and the days in a week. They all start at a certain value (say, January),
    go up to a certain other value (say, December), and then start over from the beginning.
    Some features are numeric, such as the hours, and some can be represented with
    numbers, such as the months, with values of 1 to 12\. Yet, this numeric representation
    does not capture the periodicity or cyclical nature of the variable. For example,
    December (12) is closer to January (1) than June (6); however, this relationship
    is not captured by the numerical representation of the feature. But we could change
    it if we transformed these variables with the sine and cosine, two naturally periodic
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding cyclical features with the sine and cosine functions allows linear
    models to leverage the cyclical nature of features and reduce their modeling error.
    In this recipe, we will create new features from periodic variables that capture
    the cyclical nature of time.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Trigonometric functions, such as sine and cosine, are periodic, with values
    cycling between -1 and 1 every 2π cycles, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Sine and cosine functions](img/B22396_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Sine and cosine functions
  prefs: []
  type: TYPE_NORMAL
- en: 'We can capture the periodicity of a cyclical variable by applying a trigonometric
    transformation after normalizing the variable values between 0 and 2π:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>sin</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mi>sin</mi><mrow><mrow><mo>(</mo><mn>2</mn><mi>π</mi><mfrac><mi>X</mi><msub><mi>X</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/37.png)'
  prefs: []
  type: TYPE_IMG
- en: Dividing the variable’s values by its maximum will normalize it between 0 and
    1 (assuming that the minimum value is 0), and multiplying it by 2π will rescale
    the variable between 0 and 2π.
  prefs: []
  type: TYPE_NORMAL
- en: Should we use sine? Or should we use cosine? The thing is, we need to use both
    to encode all the values of the variables unequivocally. Since sine and cosine
    circle between 0 and 1, they will take a value of 0 for more than one value of
    *x*. For example, the sine of 0 returns 0, and so does the sine of π. So, if we
    encode a variable with just the sine, we wouldn’t be able to distinguish between
    the values 0 and π anymore. However, because the sine and the cosine are out of
    phase, the cosine of 0 returns 1, whereas the cosine of π returns -1\. Hence,
    by encoding the variable with the two functions, we are now able to distinguish
    between 0 and 1, which would take (0,1) and (0,-1) as values for the sine and
    cosine functions, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will first transform the `hour` variable in a toy DataFrame
    with the sine and the cosine to get a sense of the new variable representation.
    Then, we will automate feature creation from multiple cyclical variables using
    `feature-engine`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by importing the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a toy DataFrame with one variable – `hour` – with values between 0 and
    23:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create two features using the sine and cosine transformations, after
    normalizing the variable values between 0 and 2π:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `df.head()`, we will see the original and new features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.12 – A DataFrame with the hour variable and the new features obtaine\uFEFF\
    d through the sine and cosine transformations](img/B22396_08_12.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – A DataFrame with the hour variable and the new features obtained
    through the sine and cosine transformations
  prefs: []
  type: TYPE_NORMAL
- en: 'Make a scatter plot between the hour and its sine-transformed values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following plot, we can see how the values of the hour circle between
    -1 and 1, just like the sine function after the transformation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.13 – A sca\uFEFFtter plot of th\uFEFFe hour versus its sine transformed\
    \ values](img/B22396_08_13.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – A scatter plot of the hour versus its sine transformed values
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, make a scatter plot between the hour and its cosine transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following plot, we can see how the values of the hour circle between
    -1 and 1, just like the cosine function after the transformation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.14 – A scatter plot of the hour versus its cosine-transformed values](img/B22396_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – A scatter plot of the hour versus its cosine-transformed values
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can reconstitute the cyclical nature of the hour, which is now captured
    by the two new features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the values of the sine versus the cosine of the hour, and overlay the
    original values of the hour using a color map:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following plot, we can see how the two trigonometric transformations
    of the hour reflect the cyclical nature of the hour, in a plot that reminds us
    of a clock:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.15 – A scatter plot of the trigonometric transformation of the hour](img/B22396_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – A scatter plot of the trigonometric transformation of the hour
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The code implementation and idea for this plot were taken from scikit-learn’s
    documentation: [https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#trigonometric-features](https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#trigonometric-features).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the nature and effect of the transformation, let’s create
    new features using the sine and cosine transformations from multiple variables
    automatically. We will use the `feature-engine` library’s `CyclicalFeatures()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `CyclicalFeatures()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a toy DataFrame that contains the `hour`, `month`, and `week`
    variables, whose values vary between 0 and 23, 1 and 12, and 0 and 6, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `df.head()`, we will see the first five rows of the toy DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.16 – The\uFEFF toy DataFrame with three cyclical features](img/B22396_08_16.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – The toy DataFrame with three cyclical features
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up the transformer to create the sine and cosine features from these variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: By setting `variables` to `None`, `CyclicalFeatures()` will create trigonometric
    features from all numerical variables. To create trigonometric features from a
    subset of variables, we can pass the variables’ names in a list to the `variables`
    parameter. We can retain or drop the original variables after creating the cyclical
    features using the `drop_original` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To finish, add the features to the DataFrame and capture the result in a new
    variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `dft.head()`, we will see the original and new features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.17 – DataFrame with cyclical features plus the features created
    through the sine and cosine functions](img/B22396_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – DataFrame with cyclical features plus the features created through
    the sine and cosine functions
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it – we’ve created features by using the sine and cosine transformation
    automatically from multiple variables and added them directly to the original
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we encoded cyclical features with values obtained from the sine
    and cosine functions, applied to the normalized values of the variable. First,
    we normalized the variable values between 0 and 2 π. To do this, we divided the
    variable values by the variable maximum value, which we obtained with `pandas.max()`,
    to scale the variables between 0 and 1\. Then, we multiplied those values by 2π,
    using `numpy.pi`. Finally, we used `np.sin` and `np.cos` to apply the sine and
    cosine transformations, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: To automate this procedure for multiple variables, we used the `Feature-engine`
    library’s `CyclicalFeatures()`. With `fit()`, the transformer learned the maximum
    values of each variable, and with `transform()`, it added the features resulting
    from the sine and cosine transformations to the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In theory, to apply the sine and cosine transformation, we need to scale the
    original variable between 0 and 1\. Dividing by the maximum of the variable will
    only result in this scaling if the minimum value is 0\. scikit-learn’s documentation
    and `Feature-engine`’s current implementation divide the variable by its maximum
    value (or an arbitrary period), without paying too much attention to whether the
    variable starts at 0\. In practice, you won’t see a big difference in the resulting
    variables if you divide the hour feature by 23 or 24, or the month feature by
    12 or 11\. Discussions are underway on whether Feature-engine’s implementation
    should be updated, so the default behavior might change by the time this book
    is published. Check out the documentation for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Creating spline features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear models expect a linear relationship between the predictor variables and
    the target. However, we can use linear models to model non-linear effects if we
    first transform the features. In the *Performing polynomial expansion* recipe,
    we saw how we can unmask linear patterns by creating features with polynomial
    functions. In this recipe, we will discuss the use of splines.
  prefs: []
  type: TYPE_NORMAL
- en: Splines are used to mathematically reproduce flexible shapes. They consist of
    piecewise low-degree polynomial functions. To create splines, we must place knots
    at several values of *x*. These knots indicate where the pieces of the function
    join. Then, we fit low-degree polynomials to the data between two consecutive
    knots.
  prefs: []
  type: TYPE_NORMAL
- en: There are several types of splines, such as smoothing splines, regression splines,
    and B-splines. scikit-learn supports the use of B-splines to create features.
    The procedure to fit and, therefore, return the spline values for a certain variable,
    based on a polynomial degree and the number of knots, exceeds the scope of this
    recipe. For more details, check out the resources in the *See also* section of
    this recipe. In this recipe, we’ll get a sense of what splines are and how we
    can use them to improve the performance of linear models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s get a sense of what splines are. In the following figure, on the left,
    we can see a spline with a degree of 1\. It consists of two linear pieces – one
    from 2.5 to 5 and the other from 5 to 7.5\. There are three knots – 2.5, 5, and
    7.5\. Outside the interval between 2.5 and 7.5, the spline takes a value of 0\.
    The latter is characteristic of splines; they are only non-negative between certain
    values. On the right panel of the figure, we can see three splines of degree 1\.
    We can construct as many splines as we want by introducing more knots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – The splines with a degree of 1](img/B22396_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – The splines with a degree of 1
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, on the left, we can see a quadratic spline, also known
    as a spline with a degree of 2\. It is based on four adjacent knots – 0, 2.5,
    5, and 7.5\. On the right-hand side of the figure, we can see several splines
    of degree 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19 – The splines with a degree of 2](img/B22396_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – The splines with a degree of 2
  prefs: []
  type: TYPE_NORMAL
- en: We can use splines to model non-linear functions, and we will learn how to do
    this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will use splines to model the sine function. Once we get
    a sense of what splines are and how we can use them to fit non-linear relationships
    through a linear model, we will use splines for regression in a real dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea to model the sine function with splines was taken from scikit-learn’s
    documentation: [https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html](https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the necessary libraries and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a training set, `X`, with 20 values between -1 and 11, and the target
    variable, `y`, which is the sine of `X`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the relationship between `X` and `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following plot, we can see the sine function of `X`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.20 – The relationship \uFEFFbetween the predictor and the target\
    \ variable, where y = sine(x)](img/B22396_08_20.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – The relationship between the predictor and the target variable,
    where y = sine(x)
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit a linear model to predict `y` from `X` by utilizing a Ridge regression,
    and then obtain the predictions of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, plot the relationship between `X` and `y`, and overlay the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following figure, we can see that the linear model makes a very poor
    fit of the non-linear relationship between `X` and `y`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.21 – The linear fit between X and y](img/B22396_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.21 – The linear fit between X and y
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, set up `SplineTransformer()` to obtain spline features from `X`, by utilizing
    third-degree polynomials and five knots at equidistant places within the values
    of `X`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain the spline features and convert the NumPy array into a `pandas` DataFrame,
    adding the names of the spline basis functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By executing `X_df.head()`, we can see the spline features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.22 – A DataFrame with the splines](img/B22396_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.22 – A DataFrame with the splines
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`SplineTransformer()` returns a feature matrix consisting of `n_splines = n_knots
    + degree –` `1`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, plot the splines against the values of `X`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following figure, we can see the relationship between the different
    splines and the values of the predictor variable, `X`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.23 – Spli\uFEFFnes plotted against the values of the predictor variable,\
    \ X](img/B22396_08_23.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.23 – Splines plotted against the values of the predictor variable,
    X
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, fit a linear model to predict `y` from the spline features obtained from
    `X`, by utilizing a Ridge regression, and then obtain the predictions of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, plot the relationship between `X` and `y`, and overlay the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following figure, we can see that by utilizing spline features as input,
    the Ridge regression can better predict the shape of `y`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.24 – The predictions of a linear model, based on splines overlaid
    over the true relationship between X and y](img/B22396_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.24 – The predictions of a linear model, based on splines overlaid over
    the true relationship between X and y
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the number of knots or the degree of the polynomial increases the
    flexibility of the spline curves. Try creating splines from higher polynomial
    degrees and see how the Ridge regression predictions change.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand what the spline features are and how we can use them
    to predict non-linear effects, let’s try them out on a real dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import some additional classes and functions from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the California housing dataset and drop two of the variables, which we
    won’t use for modeling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, we will fit a Ridge regression to predict house prices based on the
    existing variables, by utilizing cross-validation, and then obtain the performance
    of the model to set up the benchmark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see the model performance, where the values
    are the *R*-squared:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: spl = SplineTransformer(degree=3, n_knots=50)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ct = ColumnTransformer(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[("splines", spl, ['
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"AveRooms", "AveBedrms", "Population",'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"AveOccup"]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )],
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: remainder="passthrough",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ct.fit(X, y)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we need to use `ColumnTransformer()` to obtain features from a
    subset of variables in the data. With `remainder=passthrough`, we ensure that
    the variables that are not used as templates for the splines – that is, `MedInc`
    and `HouseAge` – are also returned in the resulting DataFrame. To check out the
    features resulting from this step, execute `ct.get_feature_names_out()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, fit a Ridge regression to predict house prices based on `MedInc`, `HouseAge`,
    and the spline features, using cross-validation, and then obtain the performance
    of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see the model performance, where the values
    are the *R*-squared:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, by using splines in place of some of the original variables,
    we can improve the performance of the linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we created new features based on splines. First, we used a toy
    variable, with values from -1 to 11, and then we obtained splines from a real
    dataset. The procedure in both cases was identical – we used `SplineTransformer()`
    from `scikit-learn`. The `SplineTransformer()` transformer takes the `degree`
    property of the polynomial and the number of knots (`n_knots`) as input and returns
    the splines that better fit the data. The knots are placed at equidistant values
    of `X` by default, but through the `knots` parameter, we can choose to uniformly
    distribute them to the quantiles of `X` instead, or we can pass an array with
    the specific values of `X` that should be used as knots.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The number, spacing, and position of the knots are arbitrarily set by the user
    and are the parameters that influence the shape of the splines the most. When
    using splines in regression models, we can optimize these parameters in a randomized
    search with cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: With `fit()`, the transformer computes the knots of the splines. With `transform()`,
    it returns the array of B-splines. The transformer returns `n_splines=n_knots
    + degree –` `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, like most scikit-learn transformers, `SplineTransformer()` also
    now has the option to return `pandas` and polars DataFrames in addition to NumPy
    arrays, a behavior that can be modified through the `set_output()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we used `ColumnTransformer()` to derive splines from a subset of features.
    Because we set `remainder` to `passthrough`, `ColumnTransformer()` concatenated
    the features that were not used to obtain splines to the resulting matrix of splines.
    By doing this, we fitted a Ridge regression with the splines, plus the `MedInc`
    and `HouseAge` variables, and managed to improve the linear model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To find out more about the math underlying B-splines, check out the following
    articles:'
  prefs: []
  type: TYPE_NORMAL
- en: Perperoglou, et al. *A review of spline function procedures in R* ([https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0666-3](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0666-3)).
    BMC Med Res Methodol 19, 46 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eilers and Marx. *Flexible Smoothing with B-splines and* *Penalties* ([https://projecteuclid.org/journals/statistical-science/volume-11/issue-2/Flexible-smoothing-with-B-splines-and-penalties/10.1214/ss/1038425655.full](https://projecteuclid.org/journals/statistical-science/volume-11/issue-2/Flexible-smoothing-with-B-splines-and-penalties/10.1214/ss/1038425655.full)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For an example of how to use B-splines to model time series data, check out
    the following page in the `scikit-learn` library’s documentation: [https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#periodic-spline-features](https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#periodic-spline-features).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
