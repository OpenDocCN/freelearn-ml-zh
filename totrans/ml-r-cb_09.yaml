- en: Chapter 9. Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering data with hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cutting a tree into clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering data with the k-means method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing a bivariate cluster plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing clustering methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting silhouette information from clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtaining optimum clusters for k-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering data with the density-based method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering data with the model-based method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing a dissimilarity matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating clusters externally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is a technique used to group similar objects (close in terms of distance)
    together in the same group (cluster). Unlike supervised learning methods (for
    example, classification and regression) covered in the previous chapters, a clustering
    analysis does not use any label information, but simply uses the similarity between
    data features to group them into clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering can be widely adapted in the analysis of businesses. For example,
    a marketing department can use clustering to segment customers by personal attributes.
    As a result of this, different marketing campaigns targeting various types of
    customers can be designed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The four most common types of clustering methods are hierarchical clustering,
    k-means clustering, model-based clustering, and density-based clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hierarchical clustering**: It creates a hierarchy of clusters, and presents
    the hierarchy in a dendrogram. This method does not require the number of clusters
    to be specified at the beginning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**k-means clustering**: It is also referred to as flat clustering. Unlike hierarchical
    clustering, it does not create a hierarchy of clusters, and it requires the number
    of clusters as an input. However, its performance is faster than hierarchical
    clustering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model-based clustering**: Both hierarchical clustering and k-means clustering
    use a heuristic approach to construct clusters, and do not rely on a formal model.
    Model-based clustering assumes a data model and applies an EM algorithm to find
    the most likely model components and the number of clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Density-based clustering**: It constructs clusters in regard to the density
    measurement. Clusters in this method have a higher density than the remainder
    of the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following recipes, we will discuss how to use these four clustering techniques
    to cluster data. We discuss how to validate clusters internally, using within
    clusters the sum of squares, average silhouette width, and externally, with ground
    truth.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering data with hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hierarchical clustering adopts either an agglomerative or divisive method to
    build a hierarchy of clusters. Regardless of which approach is adopted, both first
    use a distance similarity measure to combine or split clusters. The recursive
    process continues until there is only one cluster left or you cannot split more
    clusters. Eventually, we can use a dendrogram to represent the hierarchy of clusters.
    In this recipe, we will demonstrate how to cluster customers with hierarchical
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will perform hierarchical clustering on customer data, which
    involves segmenting customers into different groups. You can download the data
    from this Github page: [https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9](https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to cluster customer data into a hierarchy of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to load data from `customer.csv` and save it into `customer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then examine the dataset structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you should normalize the customer data into the same scale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use agglomerative hierarchical clustering to cluster the customer
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can use the `plot` function to plot the dendrogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00150.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The dendrogram of hierarchical clustering using "ward.D2"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Additionally, you can use the single method to perform hierarchical clustering
    and see how the generated dendrogram differs from the previous:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00151.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The dendrogram of hierarchical clustering using "single"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hierarchical clustering is a clustering technique that tries to build a hierarchy
    of clusters iteratively. Generally, there are two approaches to build hierarchical
    clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agglomerative hierarchical clustering**: This is a bottom-up approach. Each
    observation starts in its own cluster. We can then compute the similarity (or
    the distance) between each cluster and then merge the two most similar ones at
    each iteration until there is only one cluster left.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Divisive hierarchical clustering**: This is a top-down approach. All observations
    start in one cluster, and then we split the cluster into the two least dissimilar
    clusters recursively until there is one cluster for each observation:![How it
    works...](img/00152.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An illustration of hierarchical clustering
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before performing hierarchical clustering, we need to determine how similar
    the two clusters are. Here, we list some common distance functions used for the
    measurement of similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single linkage**: This refers to the shortest distance between two points
    in each cluster:![How it works...](img/00153.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete linkage**: This refers to the longest distance between two points
    in each cluster:![How it works...](img/00154.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average linkage**: This refer to the average distance between two points
    in each cluster (where ![How it works...](img/00155.jpeg) is the size of cluster
    ![How it works...](img/00156.jpeg) and ![How it works...](img/00157.jpeg) is the
    size of cluster ![How it works...](img/00158.jpeg)):![How it works...](img/00159.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ward method**: This refers to the sum of the squared distance from each point
    to the mean of the merged clusters (where ![How it works...](img/00160.jpeg) is
    the mean vector of ![How it works...](img/00161.jpeg)):![How it works...](img/00162.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we perform hierarchical clustering on customer data. First,
    we load the data from `customer.csv`, and then load it into the customer data
    frame. Within the data, we find five variables of customer account information,
    which are ID, number of visits, average expense, sex, and age. As the scale of
    each variable varies, we use the scale function to normalize the scale.
  prefs: []
  type: TYPE_NORMAL
- en: After the scales of all the attributes are normalized, we perform hierarchical
    clustering using the `hclust` function. We use the Euclidean distance as distance
    metrics, and use Ward's minimum variance method to perform agglomerative clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we use the `plot` function to plot the dendrogram of hierarchical clusters.
    We specify `hang` to display labels at the bottom of the dendrogram, and use `cex`
    to shrink the label to 70 percent of the normal size. In order to compare the
    differences using the `ward.D2` and `single` methods to generate a hierarchy of
    clusters, we draw another dendrogram using `single` in the preceding figure (step
    6).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can choose a different distance measure and method while performing hierarchical
    clustering. For more details, you can refer to the documents for `dist` and `hclust`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this recipe, we use `hclust` to perform agglomerative hierarchical clustering;
    if you would like to perform divisive hierarchical clustering, you can use the
    `diana` function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can use `diana` to perform divisive hierarchical clustering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can use `summary` to obtain the summary information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can plot a dendrogram and banner with the `plot` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you are interested in drawing a horizontal dendrogram, you can use the `dendextend`
    package. Use the following procedure to generate a horizontal dendrogram:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the `dendextend` and `magrittr` packages (if your R
    version is 3.1 and above, you do not have to install and load the `magrittr` package):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up the dendrogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, plot the horizontal dendrogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![There''s more...](img/00163.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The horizontal dendrogram
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Cutting trees into clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a dendrogram, we can see the hierarchy of clusters, but we have not grouped
    data into different clusters yet. However, we can determine how many clusters
    are within the dendrogram and cut the dendrogram at a certain tree height to separate
    the data into different groups. In this recipe, we demonstrate how to use the
    `cutree` function to separate the data into a given number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to perform the `cutree` function, you need to have the previous recipe
    completed by generating the hclust object, `hc`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to cut the hierarchy of clusters into a given number
    of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, categorize the data into four groups:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then examine the cluster labels for the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Count the number of data within each cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can visualize how data is clustered with the red rectangle border:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00164.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Using the red rectangle border to distinguish different clusters within the
    dendrogram
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can determine the number of clusters from the dendrogram in the preceding
    figure. In this recipe, we determine there should be four clusters within the
    tree. Therefore, we specify the number of clusters as `4` in the `cutree` function.
    Besides using the number of clusters to cut the tree, you can specify the `height`
    as the cut tree parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can output the cluster labels of the data and use the `table` function
    to count the number of data within each cluster. From the counting table, we find
    that most of the data is in cluster 4\. Lastly, we can draw red rectangles around
    the clusters to show how data is categorized into the four clusters with the `rect.hclust`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides drawing rectangles around all hierarchical clusters, you can place
    a red rectangle around a certain cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![There''s more...](img/00165.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Drawing a red rectangle around a certain cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, you can color clusters in different colors with a red rectangle around
    the clusters by using the `dendextend` package. You have to complete the instructions
    outlined in the *There''s more* section of the previous recipe and perform the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Color the branch according to the cluster it belongs to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then add a red rectangle around the clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![There''s more...](img/00166.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Drawing red rectangles around clusters within a horizontal dendrogram
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, you can add a line to show the tree cutting location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![There''s more...](img/00167.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Drawing a cutting line within a horizontal dendrogram
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clustering data with the k-means method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: k-means clustering is a flat clustering technique, which produces only one partition
    with *k* clusters. Unlike hierarchical clustering, which does not require a user
    to determine the number of clusters at the beginning, the k-means method requires
    this to be determined first. However, k-means clustering is much faster than hierarchical
    clustering as the construction of a hierarchical tree is very time consuming.
    In this recipe, we will demonstrate how to perform k-means clustering on the customer
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the customer dataset as the input data
    source to perform k-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to cluster the `customer` dataset with the k-means
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can use `kmeans` to cluster the customer data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then inspect the center of each cluster using `barplot`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00168.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The barplot of centers of different attributes in four clusters
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lastly, you can draw a scatter plot of the data and color the points according
    to the clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00169.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The scatter plot showing data colored with regard to its cluster label
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'k-means clustering is a method of partitioning clustering. The goal of the
    algorithm is to partition n objects into *k* clusters, where each object belongs
    to the cluster with the nearest mean. The objective of the algorithm is to minimize
    the **within-cluster sum of squares** (**WCSS**). Assuming *x* is the given set
    of observations, S = ![How it works...](img/00170.jpeg) denotes *k* partitions,
    and ![How it works...](img/00171.jpeg) is the mean of ![How it works...](img/00172.jpeg),
    then we can formulate the WCSS function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00173.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The process of k-means clustering can be illustrated by the following five
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify the number of *k* clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly create k partitions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the center of the partitions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Associate objects closest to the cluster center.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2, 3, and 4 until the WCSS changes very little (or is minimized).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to use k-means clustering to cluster customer
    data. In contrast to hierarchical clustering, k-means clustering requires the
    user to input the number of *K*. In this example, we use *K=4*. Then, the output
    of a fitted model shows the size of each cluster, the cluster means of four generated
    clusters, the cluster vectors with regard to each data point, the within cluster
    sum of squares by the clusters, and other available components.
  prefs: []
  type: TYPE_NORMAL
- en: Further, you can draw the centers of each cluster in a bar plot, which will
    provide more details on how each attribute affects the clustering. Lastly, we
    plot the data point in a scatter plot and use the fitted cluster labels to assign
    colors with regard to the cluster label.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In k-means clustering, you can specify the algorithm used to perform clustering
    analysis. You can specify either Hartigan-Wong, Lloyd, Forgy, or MacQueen as the
    clustering algorithm. For more details, please use the `help` function to refer
    to the document for the `kmeans` function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Drawing a bivariate cluster plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we employed the k-means method to fit data into clusters.
    However, if there are more than two variables, it is impossible to display how
    data is clustered in two dimensions. Therefore, you can use a bivariate cluster
    plot to first reduce variables into two components, and then use components, such
    as axis and circle, as clusters to show how data is clustered. In this recipe,
    we will illustrate how to create a bivariate cluster plot.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the `customer` dataset as the input
    data source to draw a bivariate cluster plot.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to draw a bivariate cluster plot:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and load the cluster package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then draw a bivariate cluster plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00174.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The bivariate clustering plot of the customer dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can also zoom into the bivariate cluster plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00175.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The zoom-in of the bivariate clustering plot
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we draw a bivariate cluster plot to show how data is clustered.
    To draw a bivariate cluster plot, we first need to install the `cluster` package
    and load it into R. We then use the `clusplot` function to draw a bivariate cluster
    plot from a customer dataset. In the `clustplot` function, we can set `shade`
    to `TRUE` and `color` to `TRUE` to display a cluster with colors and shades. As
    per the preceding figure (step 2) we found that the bivariate uses two components,
    which explains 85.01 percent of point variability, as the x-axis and y-axis. The
    data points are then scattered on the plot in accordance with component 1 and
    component 2\. Data within the same cluster is circled in the same color and shade.
  prefs: []
  type: TYPE_NORMAL
- en: Besides drawing the four clusters in a single plot, you can use `rect` to add
    a rectangle around a specific area within a given x-axis and y-axis range. You
    can then zoom into the plot to examine the data within each cluster by using `xlim`
    and `ylim` in the `clusplot` function.
  prefs: []
  type: TYPE_NORMAL
- en: There's more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `clusplot` function uses `princomp` and `cmdscale` to reduce the original
    feature dimension to the principal component. Therefore, one can see how data
    is clustered in a single plot with these two components as the x-axis and y-axis.
    To learn more about `princomp` and `cmdscale`, one can use the `help` function
    to view related documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'For those interested in how to use `cmdscale` to reduce the dimensions, please
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![There''s more](img/00176.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The scatter plot of data with regard to scaled dimensions
  prefs: []
  type: TYPE_NORMAL
- en: Comparing clustering methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After fitting data into clusters using different clustering methods, you may
    wish to measure the accuracy of the clustering. In most cases, you can use either
    intracluster or intercluster metrics as measurements. We now introduce how to
    compare different clustering methods using `cluster.stat` from the `fpc` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to perform a clustering method comparison, one needs to have the previous
    recipe completed by generating the `customer` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to compare clustering methods:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the `fpc` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You then need to use hierarchical clustering with the `single` method to cluster
    customer data and generate the object `hc_single`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use hierarchical clustering with the `complete` method to cluster customer
    data and generate the object `hc_complete`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use k-means clustering to cluster customer data and generate the
    object `km`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, retrieve the cluster validation statistics of either clustering method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Most often, we focus on using `within.cluster.ss` and `avg.silwidth` to validate
    the clustering method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can generate the cluster statistics of each clustering method and
    list them in a table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we demonstrate how to validate clusters. To validate a clustering
    method, we often employ two techniques: intercluster distance and intracluster
    distance. In these techniques, the higher the intercluster distance, the better
    it is, and the lower the intracluster distance, the better it is. In order to
    calculate related statistics, we can apply `cluster.stat` from the fpc package
    on the fitted clustering object.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the output, the `within.cluster.ss` measurement stands for the within
    clusters sum of squares, and avg.silwidth represents the average silhouette width.
    The `within.cluster.ss` measurement shows how closely related objects are in clusters;
    the smaller the value, the more closely related objects are within the cluster.
    On the other hand, a silhouette is a measurement that considers how closely related
    objects are within the cluster and how clusters are separated from each other.
    Mathematically, we can define the silhouette width for each point *x* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00177.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *a(x)* is the average distance between *x* and all
    other points within the cluster, and *b(x)* is the minimum of the average distances
    between x and the points in the other clusters. The silhouette value usually ranges
    from *0* to *1*; a value closer to *1* suggests the data is better clustered.
  prefs: []
  type: TYPE_NORMAL
- en: The summary table generated in the last step shows that the complete hierarchical
    clustering method outperforms a single hierarchical clustering method and k-means
    clustering in `within.cluster.ss` and `avg.silwidth`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `kmeans` function also outputs statistics (for example, `withinss` and
    `betweenss`) for users to validate a clustering method:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Extracting silhouette information from clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Silhouette information is a measurement to validate a cluster of data. In the
    previous recipe, we mentioned that the measurement of a cluster involves the calculation
    of how closely the data is clustered within each cluster, and measures how far
    different clusters are apart from each other. The silhouette coefficient combines
    the measurement of the intracluster and intercluster distance. The output value
    typically ranges from *0* to *1*; the closer to *1*, the better the cluster is.
    In this recipe, we will introduce how to compute silhouette information.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to extract the silhouette information from a cluster, you need to have
    the previous recipe completed by generating the `customer` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to compute the silhouette information:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `kmeans` to generate a k-means object, `km`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then compute the silhouette information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can plot the silhouette information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00178.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The silhouette plot of the k-means clustering result
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to use the silhouette plot to validate clusters.
    You can first retrieve the silhouette information, which shows cluster sizes,
    the average silhouette widths, and individual silhouette widths. The silhouette
    coefficient is a value ranging from *0* to *1*; the closer to *1*, the better
    the quality of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we use the `plot` function to draw a silhouette plot. The left-hand
    side of the plot shows the number of horizontal lines, which represent the number
    of clusters. The right-hand column shows the mean similarity of the plot of its
    own cluster minus the mean similarity of the next similar cluster. The average
    silhouette width is presented at the bottom of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For those interested in how silhouettes are computed, please refer to the Wikipedia
    entry for **Silhouette Value**: [http://en.wikipedia.org/wiki/Silhouette_%28clustering%29](http://en.wikipedia.org/wiki/Silhouette_%28clustering%29)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtaining the optimum number of clusters for k-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While k-means clustering is fast and easy to use, it requires *k* to be the
    input at the beginning. Therefore, we can use the sum of squares to determine
    which *k* value is best for finding the optimum number of clusters for k-means.
    In the following recipe, we will discuss how to find the optimum number of clusters
    for the k-means clustering method.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to find the optimum number of clusters, you need to have the previous
    recipe completed by generating the `customer` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to find the optimum number of clusters for the
    k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, calculate the within sum of squares (`withinss`) of different numbers
    of clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use a line plot to plot the within sum of squares with a different
    number of `k`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00179.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The line plot of the within sum of squares with regard to the different number
    of k
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you can calculate the average silhouette width (avg.silwidth) of different
    numbers of clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use a line plot to plot the average silhouette width with a different
    number of `k`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00180.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The line plot of average silhouette width with regard to the different number
    of k
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Retrieve the maximum number of clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to find the optimum number of clusters by
    iteratively getting within the sum of squares and the average silhouette value.
    For the within sum of squares, lower values represent clusters with better quality.
    By plotting the within sum of squares in regard to different number of `k`, we
    find that the elbow of the plot is at `k=4`.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, we also compute the average silhouette width based on the
    different numbers of clusters using `cluster.stats`. Also, we can use a line plot
    to plot the average silhouette width with regard to the different numbers of clusters.
    The preceding figure (step 4) shows the maximum average silhouette width appears
    at `k=4`. Lastly, we use `which.max` to obtain the value of k to determine the
    location of the maximum average silhouette width.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For those interested in how the within sum of squares is computed, please refer
    to the Wikipedia entry of **K-means clustering**: [http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering data with the density-based method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an alternative to distance measurement, you can use a density-based measurement
    to cluster data. This method finds an area with a higher density than the remaining
    area. One of the most famous methods is DBSCAN. In the following recipe, we will
    demonstrate how to use DBSCAN to perform density-based clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use simulated data generated from the `mlbench` package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to perform density-based clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the `fpc` and `mlbench` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use the `mlbench` library to draw a Cassini problem graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00181.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The Cassini problem graph
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you can cluster data with regard to its density measurement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the data in a scatter plot with different cluster labels as the color:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00182.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The data scatter plot colored with regard to the cluster label
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can also use `dbscan` to predict which cluster the data point belongs to.
    In this example, first make three inputs in the matrix `p`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then predict which cluster the data belongs to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Density-based clustering uses the idea of density reachability and density
    connectivity, which makes it very useful in discovering a cluster in nonlinear
    shapes. Before discussing the process of density-based clustering, some important
    background concepts must be explained. Density-based clustering takes two parameters
    into account: `eps` and `MinPts`. `eps` stands for the maximum radius of the neighborhood;
    `MinPts` denotes the minimum number of points within the `eps` neighborhood. With
    these two parameters, we can define the core point as having points more than
    `MinPts` within `eps`. Also, we can define the board point as having points less
    than `MinPts`, but is in the neighborhood of the core points. Then, we can define
    the core object as if the number of points in the `eps`-neighborhood of `p` is
    more than `MinPts`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we have to define the reachability between two points. We can
    say that a point, `p`, is directly density reachable from another point, `q`,
    if q is within the `eps`-neighborhood of `p` and `p` is a core object. Then, we
    can define that a point, `p`, is generic and density reachable from the point
    `q`, if there exists a chain of points, p[1],p[2]...,p[n], where p[1] = q, p[n]
    = p, and p[i]+1 is directly density reachable from pi with regard to Eps and `MinPts`
    for 1 <= i <= n:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00183.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Point p and q is density reachable
  prefs: []
  type: TYPE_NORMAL
- en: 'With a preliminary concept of density-based clustering, we can then illustrate
    the process of DBSCAN, the most popular density-based clustering, as shown in
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly select a point, `p`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve all the points that are density-reachable from `p` with regard to `Eps`
    and `MinPts`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If `p` is a core point, then a cluster is formed. Otherwise, if it is a board
    point and no points are density reachable from `p`, the process will mark the
    point as noise and continue visiting the next point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the process until all points have been visited.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to use the DBSCAN density-based method to
    cluster customer data. First, we have to install and load the `mlbench` and `fpc`
    libraries. The `mlbench` package provides many methods to generate simulated data
    with different shapes and sizes. In this example, we generate a Cassini problem
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we perform `dbscan` on a Cassini dataset to cluster the data. We specify
    the reachability distance as 0.2, the minimum reachability number of points to
    `2`, the progress reporting as null, and use distance as a measurement. The clustering
    method successfully clusters data into three clusters with sizes of 200, 200,
    and 100\. By plotting the points and cluster labels on the plot, we see that three
    sections of the Cassini graph are separated in different colors.
  prefs: []
  type: TYPE_NORMAL
- en: The `fpc` package also provides a `predict` function, and you can use this to
    predict the cluster labels of the input matrix. Point c(0,0) is classified into
    cluster 3, point c(0, -1.5) is classified into cluster 1, and point c(1,1) is
    classified into cluster 2.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `fpc` package contains flexible procedures of clustering, and has useful
    clustering analysis functions. For example, you can generate a discriminant projection
    plot using the `plotcluster` function. For more information, please refer to the
    following document:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Clustering data with the model-based method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast to hierarchical clustering and k-means clustering, which use a heuristic
    approach and do not depend on a formal model. Model-based clustering techniques
    assume varieties of data models and apply an EM algorithm to obtain the most likely
    model, and further use the model to infer the most likely number of clusters.
    In this recipe, we will demonstrate how to use the model-based method to determine
    the most likely number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to perform a model-based method to cluster customer data, you need
    to have the previous recipe completed by generating the customer dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to perform model-based clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, please install and load the library `mclust`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then perform model-based clustering on the `customer` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, you can press 1 to obtain the BIC against a number of components:![How
    to do it...](img/00184.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot of BIC against number of components
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, you can press 2 to show the classification with regard to different combinations
    of features:![How to do it...](img/00185.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot showing classification with regard to different combinations of features
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Press 3 to show the classification uncertainty with regard to different combinations
    of features:![How to do it...](img/00186.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot showing classification uncertainty with regard to different combinations
    of features
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, press 4 to plot the density estimation:![How to do it...](img/00187.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A plot of density estimation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, you can press 0 to plot density to exit the plotting menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lastly, use the `summary` function to obtain the most likely model and number
    of clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instead of taking a heuristic approach to build a cluster, model-based clustering
    uses a probability-based approach. Model-based clustering assumes that the data
    is generated by an underlying probability distribution and tries to recover the
    distribution from the data. One common model-based approach is using finite mixture
    models, which provide a flexible modeling framework for the analysis of the probability
    distribution. Finite mixture models are a linearly weighted sum of component probability
    distribution. Assume the data *y=(y[1],y[2]…y[n])* contains n independent and
    multivariable observations; G is the number of components; the likelihood of finite
    mixture models can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00188.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Where ![How it works...](img/00189.jpeg) and ![How it works...](img/00190.jpeg)
    are the density and parameters of the *k*th component in the mixture, and ![How
    it works...](img/00191.jpeg) (![How it works...](img/00192.jpeg) and ![How it
    works...](img/00193.jpeg)) is the probability that an observation belongs to the
    *k*th component.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of model-based clustering has several steps: First, the process
    selects the number and types of component probability distribution. Then, it fits
    a finite mixture model and calculates the posterior probabilities of a component
    membership. Lastly, it assigns the membership of each observation to the component
    with the maximum probability.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to use model-based clustering to cluster
    data. We first install and load the `Mclust` library into R. We then fit the customer
    data into the model-based method by using the `Mclust` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the data is fit into the model, we plot the model based on clustering
    results. There are four different plots: BIC, classification, uncertainty, and
    density plots. The BIC plot shows the BIC value, and one can use this value to
    choose the number of clusters. The classification plot shows how data is clustered
    in regard to different dimension combinations. The uncertainty plot shows the
    uncertainty of classifications in regard to different dimension combinations.
    The density plot shows the density estimation in contour.'
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the `summary` function to obtain the most likely model and
    the most possible number of clusters. For this example, the most possible number
    of clusters is five, with a BIC value equal to -556.1142.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For those interested in detail on how `Mclust` works, please refer to the following
    source: C. Fraley, A. E. Raftery, T. B. Murphy and L. Scrucca (2012). *mclust
    Version 4 for R: Normal Mixture Modeling for Model-Based Clustering, Classification,
    and Density Estimation*. *Technical Report No. 597*, Department of Statistics,
    University of Washington.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing a dissimilarity matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A dissimilarity matrix can be used as a measurement for the quality of a cluster.
    To visualize the matrix, we can use a heat map on a distance matrix. Within the
    plot, entries with low dissimilarity (or high similarity) are plotted darker,
    which is helpful to identify hidden structures in the data. In this recipe, we
    will discuss some techniques that are useful to visualize a dissimilarity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to visualize the dissimilarity matrix, you need to have the previous
    recipe completed by generating the customer dataset. In addition to this, a k-means
    object needs to be generated and stored in the variable `km`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to visualize the dissimilarity matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the `seriation` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then use `dissplot` to visualize the dissimilarity matrix in a heat
    map:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00194.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: A dissimilarity plot of k-means clustering
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, apply `dissplot` on hierarchical clustering in the heat map:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00195.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: A dissimilarity plot of hierarchical clustering
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we use a dissimilarity plot to visualize the dissimilarity matrix.
    We first install and load the package `seriation`, and then apply the `dissplot`
    function on the k-means clustering output, generating the preceding figure (step
    2).
  prefs: []
  type: TYPE_NORMAL
- en: It shows that clusters similar to each other are plotted darker, and dissimilar
    combinations are plotted lighter. Therefore, we can see clusters against their
    corresponding clusters (such as cluster 4 to cluster 4) are plotted diagonally
    and darker. On the other hand, clusters dissimilar to each other are plotted lighter
    and away from the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, we can apply the `dissplot` function on the output of hierarchical
    clustering. The generated plot in the figure (step 3) shows the similarity of
    each cluster in a single heat map.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides using `dissplot` to visualize the dissimilarity matrix, one can also
    visualize a distance matrix by using the `dist` and `image` functions. In the
    resulting graph, closely related entries are plotted in red. Less related entries
    are plotted closer to white:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '![There''s more...](img/00196.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A distance matrix plot of customer dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to plot both a dendrogram and heat map to show how data is clustered,
    you can use the `heatmap` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '![There''s more...](img/00197.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A heat map with dendrogram on the column and row side
  prefs: []
  type: TYPE_NORMAL
- en: Validating clusters externally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides generating statistics to validate the quality of the generated clusters,
    you can use known data clusters as the ground truth to compare different clustering
    methods. In this recipe, we will demonstrate how clustering methods differ with
    regard to data with known clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will continue to use handwriting digits as clustering inputs;
    you can find the figure on the author''s Github page: [https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9](https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to cluster digits with different clustering techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to install and load the package `png`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, please read images from `handwriting.png` and transform the read data
    into a scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00198.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: A scatter plot of handwriting digits
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform a k-means clustering method on the handwriting digits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00199.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: k-means clustering result on handwriting digits
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, perform the `dbscan` clustering method on the handwriting digits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00200.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: DBSCAN clustering result on handwriting digits
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how different clustering methods work in regard
    to a handwriting dataset. The aim of the clustering is to separate 1 and 7 into
    different clusters. We perform different techniques to see how data is clustered
    in regard to the k-means and DBSCAN methods.
  prefs: []
  type: TYPE_NORMAL
- en: To generate the data, we use the Windows application `paint.exe` to create a
    PNG file with dimensions of 28 x 28 pixels. We then read the PNG data using the
    `readPNG` function and transform the read PNG data points into a scatter plot,
    which shows the handwriting digits in 17.
  prefs: []
  type: TYPE_NORMAL
- en: After the data is read, we perform clustering techniques on the handwriting
    digits. First, we perform k-means clustering, where `k=2` on the dataset. Since
    k-means clustering employs distance measures, the constructed clusters cover the
    area of both the 1 and 7 digits. We then perform DBSCAN on the dataset. As DBSCAN
    is a density-based clustering technique, it successfully separates digit 1 and
    digit 7 into different clusters.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are interested in how to read various graphic formats in R, you may
    refer to the following document:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
