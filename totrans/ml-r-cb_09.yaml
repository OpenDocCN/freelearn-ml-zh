- en: Chapter 9. Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。聚类
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Clustering data with hierarchical clustering
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用层次聚类聚类数据
- en: Cutting a tree into clusters
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将树木切割成簇
- en: Clustering data with the k-means method
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用k-means方法聚类数据
- en: Drawing a bivariate cluster plot
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制双变量簇图
- en: Comparing clustering methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较聚类方法
- en: Extracting silhouette information from clustering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从聚类中提取轮廓信息
- en: Obtaining optimum clusters for k-means
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取k-means的最佳簇
- en: Clustering data with the density-based method
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于密度的方法聚类数据
- en: Clustering data with the model-based method
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于模型的方法聚类数据
- en: Visualizing a dissimilarity matrix
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化差异矩阵
- en: Validating clusters externally
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部验证簇
- en: Introduction
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Clustering is a technique used to group similar objects (close in terms of distance)
    together in the same group (cluster). Unlike supervised learning methods (for
    example, classification and regression) covered in the previous chapters, a clustering
    analysis does not use any label information, but simply uses the similarity between
    data features to group them into clusters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种用于将相似对象（在距离上接近）分组到同一组（簇）中的技术。与前面章节中涵盖的监督学习方法（例如，分类和回归）不同，聚类分析不使用任何标签信息，而是简单地使用数据特征之间的相似性将它们分组到簇中。
- en: Clustering can be widely adapted in the analysis of businesses. For example,
    a marketing department can use clustering to segment customers by personal attributes.
    As a result of this, different marketing campaigns targeting various types of
    customers can be designed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以广泛应用于商业分析。例如，营销部门可以使用聚类根据个人属性对客户进行细分。因此，可以设计针对不同类型客户的差异化营销活动。
- en: 'The four most common types of clustering methods are hierarchical clustering,
    k-means clustering, model-based clustering, and density-based clustering:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的四种聚类方法为层次聚类、k-means聚类、基于模型聚类和基于密度的聚类：
- en: '**Hierarchical clustering**: It creates a hierarchy of clusters, and presents
    the hierarchy in a dendrogram. This method does not require the number of clusters
    to be specified at the beginning.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次聚类**：它创建簇的层次结构，并以树状图的形式展示。这种方法在开始时不需要指定簇的数量。'
- en: '**k-means clustering**: It is also referred to as flat clustering. Unlike hierarchical
    clustering, it does not create a hierarchy of clusters, and it requires the number
    of clusters as an input. However, its performance is faster than hierarchical
    clustering.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k-means聚类**：也称为平面聚类。与层次聚类不同，它不创建簇的层次结构，并且需要一个输入的簇数量。然而，它的性能比层次聚类快。'
- en: '**Model-based clustering**: Both hierarchical clustering and k-means clustering
    use a heuristic approach to construct clusters, and do not rely on a formal model.
    Model-based clustering assumes a data model and applies an EM algorithm to find
    the most likely model components and the number of clusters.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型聚类**：层次聚类和k-means聚类都使用启发式方法构建簇，并且不依赖于正式模型。基于模型聚类假设数据模型，并应用EM算法来找到最可能的模型组件和簇的数量。'
- en: '**Density-based clustering**: It constructs clusters in regard to the density
    measurement. Clusters in this method have a higher density than the remainder
    of the dataset.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于密度的聚类**：它根据密度测量构建簇。这种方法中的簇比数据集的其余部分具有更高的密度。'
- en: In the following recipes, we will discuss how to use these four clustering techniques
    to cluster data. We discuss how to validate clusters internally, using within
    clusters the sum of squares, average silhouette width, and externally, with ground
    truth.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的菜谱中，我们将讨论如何使用这四种聚类技术来聚类数据。我们讨论了如何使用簇内平方和、平均轮廓宽度和外部真实情况来内部验证簇。
- en: Clustering data with hierarchical clustering
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用层次聚类聚类数据
- en: Hierarchical clustering adopts either an agglomerative or divisive method to
    build a hierarchy of clusters. Regardless of which approach is adopted, both first
    use a distance similarity measure to combine or split clusters. The recursive
    process continues until there is only one cluster left or you cannot split more
    clusters. Eventually, we can use a dendrogram to represent the hierarchy of clusters.
    In this recipe, we will demonstrate how to cluster customers with hierarchical
    clustering.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类采用聚合或划分方法来构建簇的层次结构。无论采用哪种方法，两者首先都使用距离相似度度量来合并或分割簇。递归过程持续进行，直到只剩下一个簇或无法再分割簇。最终，我们可以使用树状图来表示簇的层次结构。在这个菜谱中，我们将演示如何使用层次聚类对客户进行聚类。
- en: Getting ready
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we will perform hierarchical clustering on customer data, which
    involves segmenting customers into different groups. You can download the data
    from this Github page: [https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9](https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将对客户数据进行层次聚类，这涉及到将客户分割成不同的组。您可以从这个GitHub页面下载数据：[https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9](https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9)。
- en: How to do it...
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to cluster customer data into a hierarchy of clusters:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤将客户数据聚类到簇的层次结构中：
- en: 'First, you need to load data from `customer.csv` and save it into `customer`:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您需要从`customer.csv`加载数据并将其保存到`customer`中：
- en: '[PRE0]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can then examine the dataset structure:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以检查数据集的结构：
- en: '[PRE1]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, you should normalize the customer data into the same scale:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您应该将客户数据归一化到相同的尺度：
- en: '[PRE2]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can then use agglomerative hierarchical clustering to cluster the customer
    data:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用聚合层次聚类对客户数据进行聚类：
- en: '[PRE3]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Lastly, you can use the `plot` function to plot the dendrogram:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以使用`plot`函数绘制树状图：
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![How to do it...](img/00150.jpeg)'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00150.jpeg)'
- en: The dendrogram of hierarchical clustering using "ward.D2"
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用"ward.D2"的层次聚类树状图
- en: 'Additionally, you can use the single method to perform hierarchical clustering
    and see how the generated dendrogram differs from the previous:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，您可以使用单方法进行层次聚类，并查看生成的树状图与之前的区别：
- en: '[PRE5]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![How to do it...](img/00151.jpeg)'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00151.jpeg)'
- en: The dendrogram of hierarchical clustering using "single"
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用"single"的层次聚类树状图
- en: How it works...
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作...
- en: 'Hierarchical clustering is a clustering technique that tries to build a hierarchy
    of clusters iteratively. Generally, there are two approaches to build hierarchical
    clusters:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是一种聚类技术，它试图通过迭代构建簇的层次结构。通常，有两种方法来构建层次簇：
- en: '**Agglomerative hierarchical clustering**: This is a bottom-up approach. Each
    observation starts in its own cluster. We can then compute the similarity (or
    the distance) between each cluster and then merge the two most similar ones at
    each iteration until there is only one cluster left.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合层次聚类**：这是一种自下而上的方法。每个观测值最初都在自己的簇中。然后我们可以计算每个簇之间的相似度（或距离），并在每次迭代中将两个最相似的簇合并，直到只剩下一个簇。'
- en: '**Divisive hierarchical clustering**: This is a top-down approach. All observations
    start in one cluster, and then we split the cluster into the two least dissimilar
    clusters recursively until there is one cluster for each observation:![How it
    works...](img/00152.jpeg)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**划分层次聚类**：这是一种自上而下的方法。所有观测值最初都在一个簇中，然后我们递归地将簇分割成两个最不相似的簇，直到每个观测值对应一个簇：![如何工作...](img/00152.jpeg)'
- en: An illustration of hierarchical clustering
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层次聚类的示意图
- en: 'Before performing hierarchical clustering, we need to determine how similar
    the two clusters are. Here, we list some common distance functions used for the
    measurement of similarity:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行层次聚类之前，我们需要确定两个簇之间的相似度。在这里，我们列出了一些常用的相似度测量距离函数：
- en: '**Single linkage**: This refers to the shortest distance between two points
    in each cluster:![How it works...](img/00153.jpeg)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单连接**：这指的是每个簇中两点之间的最短距离：![如何工作...](img/00153.jpeg)'
- en: '**Complete linkage**: This refers to the longest distance between two points
    in each cluster:![How it works...](img/00154.jpeg)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全连接**：这指的是每个簇中两点之间的最长距离：![如何工作...](img/00154.jpeg)'
- en: '**Average linkage**: This refer to the average distance between two points
    in each cluster (where ![How it works...](img/00155.jpeg) is the size of cluster
    ![How it works...](img/00156.jpeg) and ![How it works...](img/00157.jpeg) is the
    size of cluster ![How it works...](img/00158.jpeg)):![How it works...](img/00159.jpeg)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均链接**：这指的是每个簇中两点之间的平均距离（其中![如何工作...](img/00155.jpeg)是簇![如何工作...](img/00156.jpeg)的大小，而![如何工作...](img/00157.jpeg)是簇![如何工作...](img/00158.jpeg)的大小）：![如何工作...](img/00159.jpeg)'
- en: '**Ward method**: This refers to the sum of the squared distance from each point
    to the mean of the merged clusters (where ![How it works...](img/00160.jpeg) is
    the mean vector of ![How it works...](img/00161.jpeg)):![How it works...](img/00162.jpeg)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**沃德方法**：这指的是每个点到合并簇均值的平方距离之和（其中![如何工作...](img/00160.jpeg)是![如何工作...](img/00161.jpeg)的均值向量）：![如何工作...](img/00162.jpeg)'
- en: In this recipe, we perform hierarchical clustering on customer data. First,
    we load the data from `customer.csv`, and then load it into the customer data
    frame. Within the data, we find five variables of customer account information,
    which are ID, number of visits, average expense, sex, and age. As the scale of
    each variable varies, we use the scale function to normalize the scale.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们对客户数据进行层次聚类。首先，我们从`customer.csv`加载数据，然后将其加载到客户数据框中。在数据中，我们找到五个客户账户信息的变量，它们是ID、访问次数、平均消费、性别和年龄。由于每个变量的尺度不同，我们使用尺度函数来归一化尺度。
- en: After the scales of all the attributes are normalized, we perform hierarchical
    clustering using the `hclust` function. We use the Euclidean distance as distance
    metrics, and use Ward's minimum variance method to perform agglomerative clustering.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在对所有属性的尺度进行归一化后，我们使用`hclust`函数执行层次聚类。我们使用欧几里得距离作为距离度量，并使用沃德的最小方差方法进行聚合聚类。
- en: Finally, we use the `plot` function to plot the dendrogram of hierarchical clusters.
    We specify `hang` to display labels at the bottom of the dendrogram, and use `cex`
    to shrink the label to 70 percent of the normal size. In order to compare the
    differences using the `ward.D2` and `single` methods to generate a hierarchy of
    clusters, we draw another dendrogram using `single` in the preceding figure (step
    6).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`plot`函数绘制层次簇的树状图。我们指定`hang`以在树状图的底部显示标签，并使用`cex`将标签缩小到正常大小的70%。为了比较使用`ward.D2`和`single`方法生成的簇层次结构，我们在前面的图中使用`single`绘制另一个树状图（步骤6）。
- en: There's more...
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You can choose a different distance measure and method while performing hierarchical
    clustering. For more details, you can refer to the documents for `dist` and `hclust`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行层次聚类时，您可以选择不同的距离度量和方法。有关更多详细信息，您可以参考`dist`和`hclust`的文档：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this recipe, we use `hclust` to perform agglomerative hierarchical clustering;
    if you would like to perform divisive hierarchical clustering, you can use the
    `diana` function:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们使用`hclust`执行聚合层次聚类；如果您想执行分裂层次聚类，可以使用`diana`函数：
- en: 'First, you can use `diana` to perform divisive hierarchical clustering:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您可以使用`diana`执行分裂层次聚类：
- en: '[PRE7]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, you can use `summary` to obtain the summary information:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以使用`summary`获取摘要信息：
- en: '[PRE8]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Lastly, you can plot a dendrogram and banner with the `plot` function:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以使用`plot`函数绘制树状图和横幅：
- en: '[PRE9]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you are interested in drawing a horizontal dendrogram, you can use the `dendextend`
    package. Use the following procedure to generate a horizontal dendrogram:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想绘制水平树状图，可以使用`dendextend`包。使用以下步骤生成水平树状图：
- en: 'First, install and load the `dendextend` and `magrittr` packages (if your R
    version is 3.1 and above, you do not have to install and load the `magrittr` package):'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，安装并加载`dendextend`和`magrittr`包（如果您的R版本是3.1及以上，您不需要安装和加载`magrittr`包）：
- en: '[PRE10]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Set up the dendrogram:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置树状图：
- en: '[PRE11]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, plot the horizontal dendrogram:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，绘制水平树状图：
- en: '[PRE12]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![There''s more...](img/00163.jpeg)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![还有更多...](img/00163.jpeg)'
- en: The horizontal dendrogram
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 水平树状图
- en: Cutting trees into clusters
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将树切割成簇
- en: In a dendrogram, we can see the hierarchy of clusters, but we have not grouped
    data into different clusters yet. However, we can determine how many clusters
    are within the dendrogram and cut the dendrogram at a certain tree height to separate
    the data into different groups. In this recipe, we demonstrate how to use the
    `cutree` function to separate the data into a given number of clusters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在树状图中，我们可以看到簇的层次结构，但我们还没有将数据分组到不同的簇中。然而，我们可以确定树状图中有多少簇，并在某个树高切割树状图以将数据分离到不同的组中。在这个菜谱中，我们展示了如何使用`cutree`函数将数据分离成给定的簇数。
- en: Getting ready
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In order to perform the `cutree` function, you need to have the previous recipe
    completed by generating the hclust object, `hc`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行`cutree`函数，你需要完成之前的步骤，通过生成hclust对象`hc`。
- en: How to do it...
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to cut the hierarchy of clusters into a given number
    of clusters:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤将簇的层次结构切割成给定的簇数：
- en: 'First, categorize the data into four groups:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，将数据分类为四个组：
- en: '[PRE13]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can then examine the cluster labels for the data:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以检查数据的簇标签：
- en: '[PRE14]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Count the number of data within each cluster:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个簇内的数据数量：
- en: '[PRE15]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, you can visualize how data is clustered with the red rectangle border:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以使用红色矩形边框可视化数据的聚类情况：
- en: '[PRE16]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![How to do it...](img/00164.jpeg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00164.jpeg)'
- en: Using the red rectangle border to distinguish different clusters within the
    dendrogram
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用红色矩形边框来区分树状图中不同的簇
- en: How it works...
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We can determine the number of clusters from the dendrogram in the preceding
    figure. In this recipe, we determine there should be four clusters within the
    tree. Therefore, we specify the number of clusters as `4` in the `cutree` function.
    Besides using the number of clusters to cut the tree, you can specify the `height`
    as the cut tree parameter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从前一个图中的树状图中确定簇的数量。在这个菜谱中，我们确定树中应该有四个簇。因此，我们在`cutree`函数中将簇数指定为`4`。除了使用簇数来切割树，你还可以指定`height`作为切割树的参数。
- en: Next, we can output the cluster labels of the data and use the `table` function
    to count the number of data within each cluster. From the counting table, we find
    that most of the data is in cluster 4\. Lastly, we can draw red rectangles around
    the clusters to show how data is categorized into the four clusters with the `rect.hclust`
    function.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以输出数据的簇标签，并使用`table`函数计算每个簇内的数据数量。从计数表中，我们发现大部分数据都在簇4中。最后，我们可以使用`rect.hclust`函数在簇周围绘制红色矩形，以显示数据如何被分类到四个簇中。
- en: There's more...
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Besides drawing rectangles around all hierarchical clusters, you can place
    a red rectangle around a certain cluster:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在所有层次簇周围绘制矩形，你还可以在某个簇周围放置红色矩形：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![There''s more...](img/00165.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多...](img/00165.jpeg)'
- en: Drawing a red rectangle around a certain cluster.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个簇周围绘制红色矩形。
- en: 'Also, you can color clusters in different colors with a red rectangle around
    the clusters by using the `dendextend` package. You have to complete the instructions
    outlined in the *There''s more* section of the previous recipe and perform the
    following steps:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以使用`dendextend`包通过在簇周围添加红色矩形以不同颜色着色簇。你必须完成前一个菜谱中*还有更多*部分中概述的说明，并执行以下步骤：
- en: 'Color the branch according to the cluster it belongs to:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据所属的簇给分支上色：
- en: '[PRE18]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can then add a red rectangle around the clusters:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以在簇周围添加红色矩形：
- en: '[PRE19]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![There''s more...](img/00166.jpeg)'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![还有更多...](img/00166.jpeg)'
- en: Drawing red rectangles around clusters within a horizontal dendrogram
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在水平树状图中绘制簇周围的红色矩形
- en: 'Finally, you can add a line to show the tree cutting location:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以添加一条线来显示树木切割的位置：
- en: '[PRE20]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![There''s more...](img/00167.jpeg)'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![还有更多...](img/00167.jpeg)'
- en: Drawing a cutting line within a horizontal dendrogram
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在水平树状图中绘制切割线
- en: Clustering data with the k-means method
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k-means方法聚类数据
- en: k-means clustering is a flat clustering technique, which produces only one partition
    with *k* clusters. Unlike hierarchical clustering, which does not require a user
    to determine the number of clusters at the beginning, the k-means method requires
    this to be determined first. However, k-means clustering is much faster than hierarchical
    clustering as the construction of a hierarchical tree is very time consuming.
    In this recipe, we will demonstrate how to perform k-means clustering on the customer
    dataset.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类是一种平面聚类技术，它只产生一个包含 *k* 个聚类的分区。与不需要用户在开始时确定聚类数量的层次聚类不同，k-means 方法需要首先确定这一点。然而，由于构建层次树非常耗时，k-means
    聚类比层次聚类快得多。在本食谱中，我们将演示如何在客户数据集上执行 k-means 聚类。
- en: Getting ready
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will continue to use the customer dataset as the input data
    source to perform k-means clustering.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将继续使用客户数据集作为输入数据源来执行 k-means 聚类。
- en: How to do it...
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to cluster the `customer` dataset with the k-means
    method:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下步骤使用 k-means 方法对 `customer` 数据集进行聚类：
- en: 'First, you can use `kmeans` to cluster the customer data:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您可以使用 `kmeans` 对客户数据进行聚类：
- en: '[PRE21]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can then inspect the center of each cluster using `barplot`:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用 `barplot` 检查每个聚类的中心：
- en: '[PRE22]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![How to do it...](img/00168.jpeg)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00168.jpeg)'
- en: The barplot of centers of different attributes in four clusters
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 四个聚类中不同属性的中心的条形图
- en: 'Lastly, you can draw a scatter plot of the data and color the points according
    to the clusters:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以在数据上绘制散点图，并根据聚类对点进行着色：
- en: '[PRE23]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![How to do it...](img/00169.jpeg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00169.jpeg)'
- en: The scatter plot showing data colored with regard to its cluster label
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 显示数据根据其聚类标签着色的散点图
- en: How it works...
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'k-means clustering is a method of partitioning clustering. The goal of the
    algorithm is to partition n objects into *k* clusters, where each object belongs
    to the cluster with the nearest mean. The objective of the algorithm is to minimize
    the **within-cluster sum of squares** (**WCSS**). Assuming *x* is the given set
    of observations, S = ![How it works...](img/00170.jpeg) denotes *k* partitions,
    and ![How it works...](img/00171.jpeg) is the mean of ![How it works...](img/00172.jpeg),
    then we can formulate the WCSS function as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类是一种分区聚类方法。算法的目标是将 n 个对象划分为 *k* 个聚类，其中每个对象属于最近的均值所在的聚类。算法的目标是最小化 **聚类内平方和**（**WCSS**）。假设
    *x* 是给定的观察值集合，S = ![工作原理...](img/00170.jpeg) 表示 *k* 个分区，而 ![工作原理...](img/00171.jpeg)
    是 ![工作原理...](img/00172.jpeg) 的均值，那么我们可以将 WCSS 函数表述如下：
- en: '![How it works...](img/00173.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/00173.jpeg)'
- en: 'The process of k-means clustering can be illustrated by the following five
    steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类的过程可以通过以下五个步骤来表示：
- en: Specify the number of *k* clusters.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定 *k* 个聚类的数量。
- en: Randomly create k partitions.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机创建 *k* 个分区。
- en: Calculate the center of the partitions.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算分区中心。
- en: Associate objects closest to the cluster center.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将对象与聚类中心最接近的关联起来。
- en: Repeat steps 2, 3, and 4 until the WCSS changes very little (or is minimized).
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2、3 和 4，直到 WCSS 变化很小（或是最小化）。
- en: In this recipe, we demonstrate how to use k-means clustering to cluster customer
    data. In contrast to hierarchical clustering, k-means clustering requires the
    user to input the number of *K*. In this example, we use *K=4*. Then, the output
    of a fitted model shows the size of each cluster, the cluster means of four generated
    clusters, the cluster vectors with regard to each data point, the within cluster
    sum of squares by the clusters, and other available components.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们展示了如何使用 k-means 聚类对客户数据进行聚类。与层次聚类不同，k-means 聚类需要用户输入 *K* 的数量。在本例中，我们使用
    *K=4*。然后，拟合模型的输出显示了每个聚类的规模、四个生成聚类的聚类均值、每个数据点的聚类向量、聚类内的平方和以及其他可用组件。
- en: Further, you can draw the centers of each cluster in a bar plot, which will
    provide more details on how each attribute affects the clustering. Lastly, we
    plot the data point in a scatter plot and use the fitted cluster labels to assign
    colors with regard to the cluster label.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以在条形图中绘制每个聚类的中心，这将提供更多关于每个属性如何影响聚类的细节。最后，我们在散点图中绘制数据点，并使用拟合的聚类标签根据聚类标签分配颜色。
- en: See also
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: 'In k-means clustering, you can specify the algorithm used to perform clustering
    analysis. You can specify either Hartigan-Wong, Lloyd, Forgy, or MacQueen as the
    clustering algorithm. For more details, please use the `help` function to refer
    to the document for the `kmeans` function:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在k-means聚类中，您可以指定用于执行聚类分析的算法。您可以指定Hartigan-Wong、Lloyd、Forgy或MacQueen作为聚类算法。有关更多详细信息，请使用`help`函数参考`kmeans`函数的文档：
- en: '[PRE24]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Drawing a bivariate cluster plot
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制双变量聚类图
- en: In the previous recipe, we employed the k-means method to fit data into clusters.
    However, if there are more than two variables, it is impossible to display how
    data is clustered in two dimensions. Therefore, you can use a bivariate cluster
    plot to first reduce variables into two components, and then use components, such
    as axis and circle, as clusters to show how data is clustered. In this recipe,
    we will illustrate how to create a bivariate cluster plot.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个配方中，我们使用了k-means方法将数据拟合到聚类中。然而，如果有超过两个变量，就无法在二维中显示数据的聚类方式。因此，您可以使用双变量聚类图首先将变量降低为两个成分，然后使用轴和圆等成分作为聚类来显示数据的聚类方式。在本配方中，我们将说明如何创建双变量聚类图。
- en: Getting ready
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will continue to use the `customer` dataset as the input
    data source to draw a bivariate cluster plot.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将继续使用`customer`数据集作为输入数据源来绘制双变量聚类图。
- en: How to do it...
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to draw a bivariate cluster plot:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以绘制双变量聚类图：
- en: 'Install and load the cluster package:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并加载聚类包：
- en: '[PRE25]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can then draw a bivariate cluster plot:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以绘制双变量聚类图：
- en: '[PRE26]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![How to do it...](img/00174.jpeg)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00174.jpeg)'
- en: The bivariate clustering plot of the customer dataset
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 客户数据集的双变量聚类图
- en: 'You can also zoom into the bivariate cluster plot:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以放大双变量聚类图：
- en: '[PRE27]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![How to do it...](img/00175.jpeg)'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00175.jpeg)'
- en: The zoom-in of the bivariate clustering plot
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双变量聚类图的放大
- en: How it works...
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we draw a bivariate cluster plot to show how data is clustered.
    To draw a bivariate cluster plot, we first need to install the `cluster` package
    and load it into R. We then use the `clusplot` function to draw a bivariate cluster
    plot from a customer dataset. In the `clustplot` function, we can set `shade`
    to `TRUE` and `color` to `TRUE` to display a cluster with colors and shades. As
    per the preceding figure (step 2) we found that the bivariate uses two components,
    which explains 85.01 percent of point variability, as the x-axis and y-axis. The
    data points are then scattered on the plot in accordance with component 1 and
    component 2\. Data within the same cluster is circled in the same color and shade.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们绘制双变量聚类图以显示数据的聚类方式。要绘制双变量聚类图，我们首先需要安装`cluster`包并将其加载到R中。然后，我们使用`clusplot`函数从客户数据集中绘制双变量聚类图。在`clustplot`函数中，我们可以将`shade`设置为`TRUE`，将`color`设置为`TRUE`以显示带有颜色和阴影的聚类。根据前面的图（步骤2），我们发现双变量使用两个成分作为x轴和y轴，这两个成分解释了85.01%的点变异性。然后，数据点根据成分1和成分2在图上散布。同一聚类内的数据以相同颜色和阴影的圆圈表示。
- en: Besides drawing the four clusters in a single plot, you can use `rect` to add
    a rectangle around a specific area within a given x-axis and y-axis range. You
    can then zoom into the plot to examine the data within each cluster by using `xlim`
    and `ylim` in the `clusplot` function.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在单个图中绘制四个聚类外，您还可以使用`rect`在给定x轴和y轴范围内的特定区域内添加一个矩形。然后，您可以使用`clusplot`函数中的`xlim`和`ylim`来放大图表，以检查每个聚类内的数据。
- en: There's more
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容
- en: 'The `clusplot` function uses `princomp` and `cmdscale` to reduce the original
    feature dimension to the principal component. Therefore, one can see how data
    is clustered in a single plot with these two components as the x-axis and y-axis.
    To learn more about `princomp` and `cmdscale`, one can use the `help` function
    to view related documents:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`clusplot`函数使用`princomp`和`cmdscale`将原始特征维度降低到主成分。因此，可以看到数据如何在这两个成分作为x轴和y轴的单个图中聚类。要了解更多关于`princomp`和`cmdscale`的信息，可以使用`help`函数查看相关文档：'
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For those interested in how to use `cmdscale` to reduce the dimensions, please
    perform the following steps:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对如何使用`cmdscale`进行降维感兴趣的人，请执行以下步骤：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![There''s more](img/00176.jpeg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![更多内容](img/00176.jpeg)'
- en: The scatter plot of data with regard to scaled dimensions
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 关于缩放维度的数据散点图
- en: Comparing clustering methods
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较聚类方法
- en: After fitting data into clusters using different clustering methods, you may
    wish to measure the accuracy of the clustering. In most cases, you can use either
    intracluster or intercluster metrics as measurements. We now introduce how to
    compare different clustering methods using `cluster.stat` from the `fpc` package.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的聚类方法将数据拟合到簇中后，您可能希望测量聚类的准确性。在大多数情况下，您可以使用簇内或簇间指标作为测量标准。我们现在介绍如何使用`fpc`包中的`cluster.stat`比较不同的聚类方法。
- en: Getting ready
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In order to perform a clustering method comparison, one needs to have the previous
    recipe completed by generating the `customer` dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行聚类方法比较，需要完成前面的配方，生成`customer`数据集。
- en: How to do it...
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to compare clustering methods:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以比较聚类方法：
- en: 'First, install and load the `fpc` package:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，安装并加载`fpc`包：
- en: '[PRE30]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You then need to use hierarchical clustering with the `single` method to cluster
    customer data and generate the object `hc_single`:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您需要使用`single`方法进行层次聚类以聚类客户数据并生成对象`hc_single`：
- en: '[PRE31]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Use hierarchical clustering with the `complete` method to cluster customer
    data and generate the object `hc_complete`:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`complete`方法进行层次聚类以聚类客户数据并生成对象`hc_complete`：
- en: '[PRE32]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You can then use k-means clustering to cluster customer data and generate the
    object `km`:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以使用k-means聚类将客户数据进行聚类并生成对象`km`：
- en: '[PRE33]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, retrieve the cluster validation statistics of either clustering method:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，检索任何聚类方法的聚类验证统计信息：
- en: '[PRE34]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Most often, we focus on using `within.cluster.ss` and `avg.silwidth` to validate
    the clustering method:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，我们关注使用`within.cluster.ss`和`avg.silwidth`来验证聚类方法：
- en: '[PRE35]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we can generate the cluster statistics of each clustering method and
    list them in a table:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以生成每种聚类方法的聚类统计信息并将它们列在表中：
- en: '[PRE36]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works...
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作...
- en: 'In this recipe, we demonstrate how to validate clusters. To validate a clustering
    method, we often employ two techniques: intercluster distance and intracluster
    distance. In these techniques, the higher the intercluster distance, the better
    it is, and the lower the intracluster distance, the better it is. In order to
    calculate related statistics, we can apply `cluster.stat` from the fpc package
    on the fitted clustering object.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们展示了如何验证聚类。为了验证聚类方法，我们通常采用两种技术：簇间距离和簇内距离。在这些技术中，簇间距离越高，越好；簇内距离越低，越好。为了计算相关统计信息，我们可以将`fpc`包中的`cluster.stat`应用于拟合的聚类对象。
- en: 'From the output, the `within.cluster.ss` measurement stands for the within
    clusters sum of squares, and avg.silwidth represents the average silhouette width.
    The `within.cluster.ss` measurement shows how closely related objects are in clusters;
    the smaller the value, the more closely related objects are within the cluster.
    On the other hand, a silhouette is a measurement that considers how closely related
    objects are within the cluster and how clusters are separated from each other.
    Mathematically, we can define the silhouette width for each point *x* as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，`within.cluster.ss`测量表示簇内平方和，而`avg.silwidth`表示平均轮廓宽度。`within.cluster.ss`测量表示簇内对象的相关性；值越小，簇内相关对象越紧密。另一方面，轮廓是一个考虑簇内对象的相关性和簇之间分离程度的测量。数学上，我们可以定义每个点*x*的轮廓宽度如下：
- en: '![How it works...](img/00177.jpeg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/00177.jpeg)'
- en: In the preceding equation, *a(x)* is the average distance between *x* and all
    other points within the cluster, and *b(x)* is the minimum of the average distances
    between x and the points in the other clusters. The silhouette value usually ranges
    from *0* to *1*; a value closer to *1* suggests the data is better clustered.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*a(x)*是*x*与簇内所有其他点的平均距离，而*b(x)*是*x*与其他簇中点的平均距离的最小值。轮廓值通常在*0*到*1*之间；接近*1*的值表明数据聚类得更好。
- en: The summary table generated in the last step shows that the complete hierarchical
    clustering method outperforms a single hierarchical clustering method and k-means
    clustering in `within.cluster.ss` and `avg.silwidth`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步生成的摘要表显示，完全层次聚类方法在`within.cluster.ss`和`avg.silwidth`方面优于单一层次聚类方法和k-means聚类。
- en: See also
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: 'The `kmeans` function also outputs statistics (for example, `withinss` and
    `betweenss`) for users to validate a clustering method:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kmeans`函数也输出统计信息（例如，`withinss`和`betweenss`），供用户验证聚类方法：'
- en: '[PRE37]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Extracting silhouette information from clustering
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从聚类中提取轮廓信息
- en: Silhouette information is a measurement to validate a cluster of data. In the
    previous recipe, we mentioned that the measurement of a cluster involves the calculation
    of how closely the data is clustered within each cluster, and measures how far
    different clusters are apart from each other. The silhouette coefficient combines
    the measurement of the intracluster and intercluster distance. The output value
    typically ranges from *0* to *1*; the closer to *1*, the better the cluster is.
    In this recipe, we will introduce how to compute silhouette information.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓信息是用于验证数据聚类的测量指标。在之前的步骤中，我们提到聚类的测量涉及计算数据在每个聚类内聚类的紧密程度，以及衡量不同聚类之间的距离。轮廓系数结合了簇内和簇间距离的测量。输出值通常在
    *0* 到 *1* 之间；越接近 *1*，聚类越好。在本步骤中，我们将介绍如何计算轮廓信息。
- en: Getting ready
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In order to extract the silhouette information from a cluster, you need to have
    the previous recipe completed by generating the `customer` dataset.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从聚类中提取轮廓信息，你需要完成之前的步骤，通过生成 `customer` 数据集。
- en: How to do it...
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to compute the silhouette information:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来计算轮廓信息：
- en: 'Use `kmeans` to generate a k-means object, `km`:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `kmeans` 生成一个 k-means 对象，`km`：
- en: '[PRE38]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can then compute the silhouette information:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以计算轮廓信息：
- en: '[PRE39]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, you can plot the silhouette information:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以绘制轮廓信息图：
- en: '[PRE40]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![How to do it...](img/00178.jpeg)'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00178.jpeg)'
- en: The silhouette plot of the k-means clustering result
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: k-means聚类结果的轮廓图
- en: How it works...
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we demonstrate how to use the silhouette plot to validate clusters.
    You can first retrieve the silhouette information, which shows cluster sizes,
    the average silhouette widths, and individual silhouette widths. The silhouette
    coefficient is a value ranging from *0* to *1*; the closer to *1*, the better
    the quality of the cluster.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本步骤中，我们展示了如何使用轮廓图来验证聚类。你可以首先检索轮廓信息，它显示了聚类大小、平均轮廓宽度和个体轮廓宽度。轮廓系数是一个介于 *0* 到 *1*
    之间的值；越接近 *1*，聚类质量越好。
- en: Lastly, we use the `plot` function to draw a silhouette plot. The left-hand
    side of the plot shows the number of horizontal lines, which represent the number
    of clusters. The right-hand column shows the mean similarity of the plot of its
    own cluster minus the mean similarity of the next similar cluster. The average
    silhouette width is presented at the bottom of the plot.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 `plot` 函数绘制轮廓图。图的左侧显示了水平线的数量，代表聚类的数量。图的右侧列显示了其自身聚类图与下一个相似聚类图平均相似度的差值。平均轮廓宽度在图的底部显示。
- en: See also
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: 'For those interested in how silhouettes are computed, please refer to the Wikipedia
    entry for **Silhouette Value**: [http://en.wikipedia.org/wiki/Silhouette_%28clustering%29](http://en.wikipedia.org/wiki/Silhouette_%28clustering%29)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于那些对如何计算轮廓感兴趣的人，请参考维基百科上的**轮廓值**条目：[http://zh.wikipedia.org/wiki/轮廓_%28聚类%29](http://zh.wikipedia.org/wiki/轮廓_%28聚类%29)
- en: Obtaining the optimum number of clusters for k-means
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取 k-means 的最佳聚类数量
- en: While k-means clustering is fast and easy to use, it requires *k* to be the
    input at the beginning. Therefore, we can use the sum of squares to determine
    which *k* value is best for finding the optimum number of clusters for k-means.
    In the following recipe, we will discuss how to find the optimum number of clusters
    for the k-means clustering method.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 k-means 聚类速度快且易于使用，但它需要在开始时输入 *k* 值。因此，我们可以使用平方和来确定哪个 *k* 值最适合找到 k-means
    的最佳聚类数量。在下面的步骤中，我们将讨论如何找到 k-means 聚类方法的最佳聚类数量。
- en: Getting ready
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In order to find the optimum number of clusters, you need to have the previous
    recipe completed by generating the `customer` dataset.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最佳聚类数量，你需要完成之前的步骤，通过生成 `customer` 数据集。
- en: How to do it...
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to find the optimum number of clusters for the
    k-means clustering:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以找到 k-means 聚类最佳聚类数量：
- en: 'First, calculate the within sum of squares (`withinss`) of different numbers
    of clusters:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，计算不同数量聚类的内部平方和 (`withinss`)：
- en: '[PRE41]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You can then use a line plot to plot the within sum of squares with a different
    number of `k`:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用线图绘制不同 `k` 值的内部平方和：
- en: '[PRE42]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![How to do it...](img/00179.jpeg)'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00179.jpeg)'
- en: The line plot of the within sum of squares with regard to the different number
    of k
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于不同 k 值的内部平方和的线图
- en: 'Next, you can calculate the average silhouette width (avg.silwidth) of different
    numbers of clusters:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以计算不同数量聚类的平均轮廓宽度（avg.silwidth）：
- en: '[PRE43]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You can then use a line plot to plot the average silhouette width with a different
    number of `k`:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用线图来绘制不同`k`值的平均轮廓宽度：
- en: '[PRE44]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![How to do it...](img/00180.jpeg)'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00180.jpeg)'
- en: The line plot of average silhouette width with regard to the different number
    of k
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与不同数量的k相关的平均轮廓宽度的线图
- en: 'Retrieve the maximum number of clusters:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取最大聚类数量：
- en: '[PRE45]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: How it works...
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we demonstrate how to find the optimum number of clusters by
    iteratively getting within the sum of squares and the average silhouette value.
    For the within sum of squares, lower values represent clusters with better quality.
    By plotting the within sum of squares in regard to different number of `k`, we
    find that the elbow of the plot is at `k=4`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们通过迭代地获取平方和与平均轮廓值之和来演示如何找到最佳聚类数量。对于平方和，较低的值表示质量更好的聚类。通过绘制不同数量`k`的平方和，我们发现图表的肘部在`k=4`。
- en: On the other hand, we also compute the average silhouette width based on the
    different numbers of clusters using `cluster.stats`. Also, we can use a line plot
    to plot the average silhouette width with regard to the different numbers of clusters.
    The preceding figure (step 4) shows the maximum average silhouette width appears
    at `k=4`. Lastly, we use `which.max` to obtain the value of k to determine the
    location of the maximum average silhouette width.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们使用`cluster.stats`根据不同数量的聚类计算平均轮廓宽度。我们还可以使用线图来绘制不同数量聚类对应的平均轮廓宽度。前面的图（步骤4）显示最大平均轮廓宽度出现在`k=4`。最后，我们使用`which.max`来获取k的值，以确定最大平均轮廓宽度的位置。
- en: See also
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考信息
- en: 'For those interested in how the within sum of squares is computed, please refer
    to the Wikipedia entry of **K-means clustering**: [http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于那些对平方和如何计算感兴趣的人，请参阅维基百科上的**K-means聚类**条目：[http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering)
- en: Clustering data with the density-based method
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于密度的方法进行聚类
- en: As an alternative to distance measurement, you can use a density-based measurement
    to cluster data. This method finds an area with a higher density than the remaining
    area. One of the most famous methods is DBSCAN. In the following recipe, we will
    demonstrate how to use DBSCAN to perform density-based clustering.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 作为距离测量的替代，你可以使用基于密度的测量来聚类数据。这种方法找到一个比剩余区域密度更高的区域。最著名的方法之一是DBSCAN。在下面的配方中，我们将演示如何使用DBSCAN进行基于密度的聚类。
- en: Getting ready
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use simulated data generated from the `mlbench` package.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用由`mlbench`包生成的模拟数据。
- en: How to do it...
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to perform density-based clustering:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以执行基于密度的聚类：
- en: 'First, install and load the `fpc` and `mlbench` packages:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，安装并加载`fpc`和`mlbench`包：
- en: '[PRE46]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You can then use the `mlbench` library to draw a Cassini problem graph:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用`mlbench`库绘制Cassini问题图：
- en: '[PRE47]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![How to do it...](img/00181.jpeg)'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00181.jpeg)'
- en: The Cassini problem graph
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Cassini问题图
- en: 'Next, you can cluster data with regard to its density measurement:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以根据密度的测量来对数据进行聚类：
- en: '[PRE48]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Plot the data in a scatter plot with different cluster labels as the color:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以不同的聚类标签作为颜色绘制散点图中的数据：
- en: '[PRE49]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![How to do it...](img/00182.jpeg)'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00182.jpeg)'
- en: The data scatter plot colored with regard to the cluster label
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据聚类标签着色的数据散点图
- en: 'You can also use `dbscan` to predict which cluster the data point belongs to.
    In this example, first make three inputs in the matrix `p`:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以使用`dbscan`来预测数据点属于哪个聚类。在这个例子中，首先在矩阵`p`中输入三个输入：
- en: '[PRE50]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You can then predict which cluster the data belongs to:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以预测数据属于哪个聚类：
- en: '[PRE51]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works...
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Density-based clustering uses the idea of density reachability and density
    connectivity, which makes it very useful in discovering a cluster in nonlinear
    shapes. Before discussing the process of density-based clustering, some important
    background concepts must be explained. Density-based clustering takes two parameters
    into account: `eps` and `MinPts`. `eps` stands for the maximum radius of the neighborhood;
    `MinPts` denotes the minimum number of points within the `eps` neighborhood. With
    these two parameters, we can define the core point as having points more than
    `MinPts` within `eps`. Also, we can define the board point as having points less
    than `MinPts`, but is in the neighborhood of the core points. Then, we can define
    the core object as if the number of points in the `eps`-neighborhood of `p` is
    more than `MinPts`.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的聚类利用密度可达性和密度连通性的概念，这使得它在发现非线性形状的聚类中非常有用。在讨论基于密度的聚类过程之前，必须解释一些重要的背景概念。基于密度的聚类考虑两个参数：`eps`和`MinPts`。`eps`代表邻域的最大半径；`MinPts`表示`eps`邻域内点的最小数量。有了这两个参数，我们可以定义核心点为在`eps`邻域内有超过`MinPts`个点的点。此外，我们还可以定义边界点为点数少于`MinPts`，但位于核心点邻域内的点。然后，我们可以定义核心对象为如果点`p`的`eps`邻域内的点数超过`MinPts`。
- en: 'Furthermore, we have to define the reachability between two points. We can
    say that a point, `p`, is directly density reachable from another point, `q`,
    if q is within the `eps`-neighborhood of `p` and `p` is a core object. Then, we
    can define that a point, `p`, is generic and density reachable from the point
    `q`, if there exists a chain of points, p[1],p[2]...,p[n], where p[1] = q, p[n]
    = p, and p[i]+1 is directly density reachable from pi with regard to Eps and `MinPts`
    for 1 <= i <= n:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们必须定义两点之间的可达性。我们可以这样说，如果点`q`位于点`p`的`eps`邻域内，并且`p`是一个核心对象，那么点`p`是直接从点`q`密度可达的。然后，我们可以定义，如果存在一个点的链，p[1]，p[2]，...，p[n]，其中p[1]
    = q，p[n] = p，并且对于1 <= i <= n，p[i]+1相对于Eps和`MinPts`直接从pi密度可达，那么点`p`是通用且从点`q`密度可达的：
- en: '![How it works...](img/00183.jpeg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/00183.jpeg)'
- en: Point p and q is density reachable
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 点p和q是密度可达的
- en: 'With a preliminary concept of density-based clustering, we can then illustrate
    the process of DBSCAN, the most popular density-based clustering, as shown in
    these steps:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在对基于密度的聚类有一个初步概念之后，我们可以通过以下步骤来阐述最流行的基于密度的聚类算法DBSCAN的过程，如图所示：
- en: Randomly select a point, `p`.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个点，`p`。
- en: Retrieve all the points that are density-reachable from `p` with regard to `Eps`
    and `MinPts`.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据`Eps`和`MinPts`检索所有从`p`密度可达的点。
- en: If `p` is a core point, then a cluster is formed. Otherwise, if it is a board
    point and no points are density reachable from `p`, the process will mark the
    point as noise and continue visiting the next point.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`p`是一个核心点，那么就形成了一个簇。否则，如果它是边界点，并且没有点从`p`密度可达，则过程将标记该点为噪声，并继续访问下一个点。
- en: Repeat the process until all points have been visited.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复此过程，直到所有点都被访问。
- en: In this recipe, we demonstrate how to use the DBSCAN density-based method to
    cluster customer data. First, we have to install and load the `mlbench` and `fpc`
    libraries. The `mlbench` package provides many methods to generate simulated data
    with different shapes and sizes. In this example, we generate a Cassini problem
    graph.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了如何使用基于密度的DBSCAN方法来聚类客户数据。首先，我们必须安装和加载`mlbench`和`fpc`库。`mlbench`包提供了许多方法来生成不同形状和大小的模拟数据。在这个例子中，我们生成一个Cassini问题图。
- en: Next, we perform `dbscan` on a Cassini dataset to cluster the data. We specify
    the reachability distance as 0.2, the minimum reachability number of points to
    `2`, the progress reporting as null, and use distance as a measurement. The clustering
    method successfully clusters data into three clusters with sizes of 200, 200,
    and 100\. By plotting the points and cluster labels on the plot, we see that three
    sections of the Cassini graph are separated in different colors.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在Cassini数据集上执行`dbscan`以聚类数据。我们指定可达距离为0.2，点的最小可达数为2，进度报告为null，并使用距离作为测量标准。聚类方法成功地将数据聚为三个大小分别为200、200和100的簇。通过在图上绘制点和簇标签，我们看到Cassini图的三部分被不同颜色分开。
- en: The `fpc` package also provides a `predict` function, and you can use this to
    predict the cluster labels of the input matrix. Point c(0,0) is classified into
    cluster 3, point c(0, -1.5) is classified into cluster 1, and point c(1,1) is
    classified into cluster 2.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`fpc`包还提供了一个`predict`函数，您可以使用此函数预测输入矩阵的聚类标签。点c(0,0)被分类到聚类3，点c(0, -1.5)被分类到聚类1，点c(1,1)被分类到聚类2。'
- en: See also
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'The `fpc` package contains flexible procedures of clustering, and has useful
    clustering analysis functions. For example, you can generate a discriminant projection
    plot using the `plotcluster` function. For more information, please refer to the
    following document:'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fpc`包包含灵活的聚类过程，并具有有用的聚类分析函数。例如，您可以使用`plotcluster`函数生成判别投影图。有关更多信息，请参阅以下文档：'
- en: '[PRE52]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Clustering data with the model-based method
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于模型的方法进行聚类数据
- en: In contrast to hierarchical clustering and k-means clustering, which use a heuristic
    approach and do not depend on a formal model. Model-based clustering techniques
    assume varieties of data models and apply an EM algorithm to obtain the most likely
    model, and further use the model to infer the most likely number of clusters.
    In this recipe, we will demonstrate how to use the model-based method to determine
    the most likely number of clusters.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用启发式方法且不依赖于正式模型的层次聚类和k-means聚类相比，基于模型聚类技术假设各种数据模型，并应用EM算法以获得最可能的模型，并进一步使用该模型推断最可能的聚类数量。在本配方中，我们将演示如何使用基于模型的方法来确定最可能的聚类数量。
- en: Getting ready
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In order to perform a model-based method to cluster customer data, you need
    to have the previous recipe completed by generating the customer dataset.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行基于模型的方法对客户数据进行聚类，您需要完成之前的配方，生成客户数据集。
- en: How to do it...
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to perform model-based clustering:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以执行基于模型聚类：
- en: 'First, please install and load the library `mclust`:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，请安装并加载库`mclust`：
- en: '[PRE53]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'You can then perform model-based clustering on the `customer` dataset:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以在`customer`数据集上执行基于模型聚类：
- en: '[PRE54]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Then, you can press 1 to obtain the BIC against a number of components:![How
    to do it...](img/00184.jpeg)
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以按1键获取BIC与组件数量的关系：![如何操作...](img/00184.jpeg)
- en: Plot of BIC against number of components
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BIC与组件数量的关系图
- en: Then, you can press 2 to show the classification with regard to different combinations
    of features:![How to do it...](img/00185.jpeg)
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以按2键显示关于不同特征组合的分类：![如何操作...](img/00185.jpeg)
- en: Plot showing classification with regard to different combinations of features
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据不同特征组合的分类图
- en: Press 3 to show the classification uncertainty with regard to different combinations
    of features:![How to do it...](img/00186.jpeg)
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按3键显示关于不同特征组合的分类不确定性：![如何操作...](img/00186.jpeg)
- en: Plot showing classification uncertainty with regard to different combinations
    of features
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据不同特征组合的分类不确定性图
- en: Next, press 4 to plot the density estimation:![How to do it...](img/00187.jpeg)
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，按4键绘制密度估计：![如何操作...](img/00187.jpeg)
- en: A plot of density estimation
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 密度估计图
- en: Then, you can press 0 to plot density to exit the plotting menu.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以按0键绘制密度并退出绘图菜单。
- en: 'Lastly, use the `summary` function to obtain the most likely model and number
    of clusters:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`summary`函数获取最可能的模型和聚类数量：
- en: '[PRE55]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: How it works...
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作...
- en: 'Instead of taking a heuristic approach to build a cluster, model-based clustering
    uses a probability-based approach. Model-based clustering assumes that the data
    is generated by an underlying probability distribution and tries to recover the
    distribution from the data. One common model-based approach is using finite mixture
    models, which provide a flexible modeling framework for the analysis of the probability
    distribution. Finite mixture models are a linearly weighted sum of component probability
    distribution. Assume the data *y=(y[1],y[2]…y[n])* contains n independent and
    multivariable observations; G is the number of components; the likelihood of finite
    mixture models can be formulated as:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于启发式方法构建聚类不同，基于模型聚类使用基于概率的方法。基于模型聚类假设数据是由潜在的概率分布生成的，并试图从数据中恢复分布。一种常见的基于模型的方法是使用有限混合模型，它为概率分布的分析提供了一个灵活的建模框架。有限混合模型是组件概率分布的线性加权总和。假设数据*y=(y[1],y[2]…y[n])*包含n个独立的多变量观测值；G是组件数量；有限混合模型的似然可以表示为：
- en: '![How it works...](img/00188.jpeg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/00188.jpeg)'
- en: Where ![How it works...](img/00189.jpeg) and ![How it works...](img/00190.jpeg)
    are the density and parameters of the *k*th component in the mixture, and ![How
    it works...](img/00191.jpeg) (![How it works...](img/00192.jpeg) and ![How it
    works...](img/00193.jpeg)) is the probability that an observation belongs to the
    *k*th component.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '![如何工作...](img/00189.jpeg) 和 ![如何工作...](img/00190.jpeg) 是混合模型中第 *k* 个成分的密度和参数，而
    ![如何工作...](img/00191.jpeg) ([![如何工作...](img/00192.jpeg)] 和 ![如何工作...](img/00193.jpeg))
    是观察属于第 *k* 个成分的概率。'
- en: 'The process of model-based clustering has several steps: First, the process
    selects the number and types of component probability distribution. Then, it fits
    a finite mixture model and calculates the posterior probabilities of a component
    membership. Lastly, it assigns the membership of each observation to the component
    with the maximum probability.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的聚类过程有几个步骤：首先，过程选择组件概率分布的数量和类型。然后，它拟合一个有限混合模型并计算组件成员的后验概率。最后，它将每个观察的成员分配给概率最大的组件。
- en: In this recipe, we demonstrate how to use model-based clustering to cluster
    data. We first install and load the `Mclust` library into R. We then fit the customer
    data into the model-based method by using the `Mclust` function.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了如何使用基于模型的聚类方法对数据进行聚类。我们首先在R中安装并加载`Mclust`库。然后，我们使用`Mclust`函数将客户数据拟合到基于模型的方法中。
- en: 'After the data is fit into the model, we plot the model based on clustering
    results. There are four different plots: BIC, classification, uncertainty, and
    density plots. The BIC plot shows the BIC value, and one can use this value to
    choose the number of clusters. The classification plot shows how data is clustered
    in regard to different dimension combinations. The uncertainty plot shows the
    uncertainty of classifications in regard to different dimension combinations.
    The density plot shows the density estimation in contour.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 数据拟合到模型后，我们根据聚类结果绘制模型。有四种不同的图：BIC图、分类图、不确定性和密度图。BIC图显示了BIC值，人们可以使用这个值来选择簇的数量。分类图显示了数据如何根据不同的维度组合进行聚类。不确定图显示了根据不同维度组合的分类不确定性。密度图显示了轮廓中的密度估计。
- en: You can also use the `summary` function to obtain the most likely model and
    the most possible number of clusters. For this example, the most possible number
    of clusters is five, with a BIC value equal to -556.1142.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用`summary`函数来获取最可能的模型和最可能的簇数量。对于这个例子，最可能的簇数量是五个，BIC值为-556.1142。
- en: See also
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For those interested in detail on how `Mclust` works, please refer to the following
    source: C. Fraley, A. E. Raftery, T. B. Murphy and L. Scrucca (2012). *mclust
    Version 4 for R: Normal Mixture Modeling for Model-Based Clustering, Classification,
    and Density Estimation*. *Technical Report No. 597*, Department of Statistics,
    University of Washington.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于那些对`Mclust`如何工作感兴趣的人，请参阅以下来源：C. Fraley, A. E. Raftery, T. B. Murphy 和 L.
    Scrucca (2012). *mclust Version 4 for R: Normal Mixture Modeling for Model-Based
    Clustering, Classification, and Density Estimation*. *技术报告第597号*，华盛顿大学统计学系。'
- en: Visualizing a dissimilarity matrix
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化相似性矩阵
- en: A dissimilarity matrix can be used as a measurement for the quality of a cluster.
    To visualize the matrix, we can use a heat map on a distance matrix. Within the
    plot, entries with low dissimilarity (or high similarity) are plotted darker,
    which is helpful to identify hidden structures in the data. In this recipe, we
    will discuss some techniques that are useful to visualize a dissimilarity matrix.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 相似性矩阵可以用作衡量聚类质量的一个指标。为了可视化矩阵，我们可以在距离矩阵上使用热图。在图中，低相似性（或高相似性）的条目以较深的颜色绘制，这有助于识别数据中的隐藏结构。在这个菜谱中，我们将讨论一些可视化相似性矩阵的有用技术。
- en: Getting ready
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In order to visualize the dissimilarity matrix, you need to have the previous
    recipe completed by generating the customer dataset. In addition to this, a k-means
    object needs to be generated and stored in the variable `km`.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化相似性矩阵，你需要完成之前的菜谱，生成客户数据集。此外，还需要生成并存储在变量`km`中的k-means对象。
- en: How to do it...
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to visualize the dissimilarity matrix:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以可视化相似性矩阵：
- en: 'First, install and load the `seriation` package:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，安装并加载`seriation`包：
- en: '[PRE56]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'You can then use `dissplot` to visualize the dissimilarity matrix in a heat
    map:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用`dissplot`在热图上可视化相似性矩阵：
- en: '[PRE57]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![How to do it...](img/00194.jpeg)'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00194.jpeg)'
- en: A dissimilarity plot of k-means clustering
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: k-means聚类的相似性图
- en: 'Next, apply `dissplot` on hierarchical clustering in the heat map:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在热图中应用`dissplot`于层次聚类：
- en: '[PRE58]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![How to do it...](img/00195.jpeg)'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00195.jpeg)'
- en: A dissimilarity plot of hierarchical clustering
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层次聚类的相似度图
- en: How it works...
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we use a dissimilarity plot to visualize the dissimilarity matrix.
    We first install and load the package `seriation`, and then apply the `dissplot`
    function on the k-means clustering output, generating the preceding figure (step
    2).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用不相似度图来可视化不相似度矩阵。我们首先安装并加载`seriation`包，然后对k-means聚类输出应用`dissplot`函数，生成前面的图（步骤2）。
- en: It shows that clusters similar to each other are plotted darker, and dissimilar
    combinations are plotted lighter. Therefore, we can see clusters against their
    corresponding clusters (such as cluster 4 to cluster 4) are plotted diagonally
    and darker. On the other hand, clusters dissimilar to each other are plotted lighter
    and away from the diagonal.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示，相似度高的聚类用较深的颜色表示，而不同组合的聚类用较浅的颜色表示。因此，我们可以看到对应聚类（如聚类4到聚类4）是斜对角且颜色较深的。另一方面，彼此不相似的聚类用较浅的颜色表示，并且远离对角线。
- en: Likewise, we can apply the `dissplot` function on the output of hierarchical
    clustering. The generated plot in the figure (step 3) shows the similarity of
    each cluster in a single heat map.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以在层次聚类的输出上应用`dissplot`函数。图中的生成图（步骤3）显示了每个聚类的相似性。
- en: There's more...
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Besides using `dissplot` to visualize the dissimilarity matrix, one can also
    visualize a distance matrix by using the `dist` and `image` functions. In the
    resulting graph, closely related entries are plotted in red. Less related entries
    are plotted closer to white:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用`dissplot`来可视化不相似度矩阵外，还可以使用`dist`和`image`函数来可视化距离矩阵。在生成的图中，密切相关项用红色表示。不太相关项则更接近白色：
- en: '[PRE59]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '![There''s more...](img/00196.jpeg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多...](img/00196.jpeg)'
- en: A distance matrix plot of customer dataset
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 客户数据集的距离矩阵图
- en: 'In order to plot both a dendrogram and heat map to show how data is clustered,
    you can use the `heatmap` function:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制树状图和热图以显示数据的聚类方式，您可以使用`heatmap`函数：
- en: '[PRE60]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![There''s more...](img/00197.jpeg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多...](img/00197.jpeg)'
- en: A heat map with dendrogram on the column and row side
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 列和行侧带有树状图的热图
- en: Validating clusters externally
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部验证聚类
- en: Besides generating statistics to validate the quality of the generated clusters,
    you can use known data clusters as the ground truth to compare different clustering
    methods. In this recipe, we will demonstrate how clustering methods differ with
    regard to data with known clusters.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 除了生成统计信息来验证生成的聚类的质量外，您还可以使用已知的数据聚类作为基准来比较不同的聚类方法。在这个菜谱中，我们将演示聚类方法在已知聚类数据上的差异。
- en: Getting ready
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we will continue to use handwriting digits as clustering inputs;
    you can find the figure on the author''s Github page: [https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9](https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9).'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将继续使用手写数字作为聚类输入；您可以在作者的GitHub页面上找到该图：[https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9](https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9)。
- en: How to do it...
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to cluster digits with different clustering techniques:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以使用不同的聚类技术对数字进行聚类：
- en: 'First, you need to install and load the package `png`:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您需要安装并加载`png`包：
- en: '[PRE61]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Then, please read images from `handwriting.png` and transform the read data
    into a scatter plot:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，请从`handwriting.png`读取图像，并将读取的数据转换为散点图：
- en: '[PRE62]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![How to do it...](img/00198.jpeg)'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00198.jpeg)'
- en: A scatter plot of handwriting digits
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 手写数字的散点图
- en: 'Perform a k-means clustering method on the handwriting digits:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对手写数字执行k-means聚类方法：
- en: '[PRE63]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '![How to do it...](img/00199.jpeg)'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00199.jpeg)'
- en: k-means clustering result on handwriting digits
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 手写数字的k-means聚类结果
- en: 'Next, perform the `dbscan` clustering method on the handwriting digits:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，对手写数字执行`dbscan`聚类方法：
- en: '[PRE64]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![How to do it...](img/00200.jpeg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/00200.jpeg)'
- en: DBSCAN clustering result on handwriting digits
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 手写数字的DBSCAN聚类结果
- en: How it works...
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we demonstrate how different clustering methods work in regard
    to a handwriting dataset. The aim of the clustering is to separate 1 and 7 into
    different clusters. We perform different techniques to see how data is clustered
    in regard to the k-means and DBSCAN methods.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了不同的聚类方法在处理手写数据集时的效果。聚类的目的是将 1 和 7 分离到不同的簇中。我们执行不同的技术来观察数据在 k-means
    和 DBSCAN 方法下的聚类情况。
- en: To generate the data, we use the Windows application `paint.exe` to create a
    PNG file with dimensions of 28 x 28 pixels. We then read the PNG data using the
    `readPNG` function and transform the read PNG data points into a scatter plot,
    which shows the handwriting digits in 17.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成数据，我们使用 Windows 应用程序 `paint.exe` 创建一个 28 x 28 像素的 PNG 文件。然后我们使用 `readPNG`
    函数读取 PNG 数据，并将读取的 PNG 数据点转换为散点图，该图显示了 17 中的手写数字。
- en: After the data is read, we perform clustering techniques on the handwriting
    digits. First, we perform k-means clustering, where `k=2` on the dataset. Since
    k-means clustering employs distance measures, the constructed clusters cover the
    area of both the 1 and 7 digits. We then perform DBSCAN on the dataset. As DBSCAN
    is a density-based clustering technique, it successfully separates digit 1 and
    digit 7 into different clusters.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据读取之后，我们对手写数字进行聚类技术处理。首先，我们在数据集上执行 k-means 聚类，其中 `k=2`。由于 k-means 聚类使用距离度量，构建的簇覆盖了
    1 和 7 数字区域。然后我们对数据集执行 DBSCAN。由于 DBSCAN 是一种基于密度的聚类技术，它成功地将数字 1 和数字 7 分离到不同的簇中。
- en: See also
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'If you are interested in how to read various graphic formats in R, you may
    refer to the following document:'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对如何在 R 中读取各种图形格式感兴趣，你可以参考以下文档：
- en: '[PRE65]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
