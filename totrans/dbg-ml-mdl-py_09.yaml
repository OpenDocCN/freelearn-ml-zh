- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing and Debugging for Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might have gotten excited about training and testing a machine learning
    model without thinking about the unexpected behavior of your model in production
    and how your model fits into a bigger technology. Most academic courses don’t
    go through details of strategies to test models, assess their qualities, and monitor
    their performance pre-deployment and in production. There are important concepts
    and techniques in testing and debugging models for production that we will review
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration testing of machine learning pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and validating live performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model assertion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about the importance of infrastructure
    and integration testing, as well as model monitoring and assertion. You will have
    also learned how to use Python libraries so that you can benefit from them in
    your projects.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python library requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` >= 1.2.2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` >= 1.22.4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytest` >= 7.2.2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You must also have basic knowledge of the machine learning life cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter09](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Infrastructure testing refers to the process of verifying and validating the
    various components and systems involved in deploying, managing, and scaling machine
    learning models. This includes testing software, hardware, and other resources
    that make up the infrastructure that supports machine learning workflows. Infrastructure
    testing in machine learning helps you ensure that models are trained, deployed,
    and maintained effectively. It provides you with reliable models in a production
    environment. Regular infrastructure testing can help you detect and fix issues
    early and reduce the risk of failures during deployment and in the production
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the important aspects of infrastructure testing in machine
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data pipeline testing**: This ensures that the data pipelines responsible
    for data collection, selection, and wrangling are working correctly and efficiently.
    This helps maintain data quality and consistency for training, testing, and deploying
    your machine learning models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training and evaluation**: This validates the functionality of the
    model training process, such as hyperparameter tuning and model evaluation. This
    process eliminates unexpected issues in training and evaluation to achieve a reliable
    and responsible model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment and serving**: This tests the process of deploying the trained
    model in a production environment, ensuring that the serving infrastructure, such
    as API endpoints, is working correctly and can handle the expected request load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and observability**: This tests the monitoring and logging systems
    that provide insights into the performance and behavior of the machine learning
    infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration testing**: This verifies that all components of the machine learning
    infrastructure, such as data pipelines, model training systems, and deployment
    platforms, are working together seamlessly and without conflicts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability testing**: This evaluates the ability of the infrastructure to
    scale up or down in response to changing requirements, such as increased data
    volume, higher user traffic, or more complex models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and compliance testing**: This ensures that the machine learning
    infrastructure meets necessary security requirements, data protection regulations,
    and privacy standards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you understand the importance and benefits of infrastructure testing,
    you are ready to learn about the related tools that can help you in model deployment
    and infrastructure management.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as Code tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Infrastructure as Code** (**IaC**) and configuration management tools such
    as **Chef**, **Puppet**, and **Ansible** can be used to automate the deployment,
    configuration, and management of software and hardware infrastructures. These
    tools could help us ensure consistency and reliability across different environments.
    Let’s understand how Chef, Puppet, and Ansible work, and how they can help you
    in your projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chef** ([https://www.chef.io/products/chef-infrastructure-management](https://www.chef.io/products/chef-infrastructure-management)):
    Chef is an open source configuration management tool that relies on a client-server
    model, where the Chef server stores the desired configuration, and the Chef client
    applies it to the nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Puppet** ([https://www.puppet.com/](https://www.puppet.com/)): Puppet is
    another open source configuration management tool that works in a client-server
    model or as a standalone application. Puppet enforces desired configurations across
    nodes by periodically pulling them from the Puppet master server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ansible** ([https://www.ansible.com/](https://www.ansible.com/)): Ansible
    is an open source and easy-to-use configuration management, orchestration, and
    automation tool that communicates and applies configurations to nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools primarily focus on infrastructure management and automation, but
    they also have modules or plugins to perform basic testing and validation of the
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure testing tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Test Kitchen, ServerSpec, and InSpec are infrastructure testing tools we can
    use to verify and validate the desired configuration and behavior of our infrastructures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test Kitchen** ([https://github.com/test-kitchen/test-kitchen](https://github.com/test-kitchen/test-kitchen)):
    Test Kitchen is an integration testing framework mainly used with Chef but can
    also work with other IaC tools such as Ansible and Puppet. It allows you to test
    your infrastructure code on different platforms and configurations. Test Kitchen
    creates temporary instances on various platforms (using drivers such as Docker
    or cloud providers), converges your infrastructure code, and runs tests against
    the configured instances. You can use Test Kitchen with different testing frameworks,
    such as ServerSpec or InSpec, to define your tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ServerSpec** ([https://serverspec.org/](https://serverspec.org/)): ServerSpec
    is a **behavior-driven development** (**BDD**) testing framework for infrastructure.
    It allows you to write tests in a human-readable language. ServerSpec tests the
    desired state of your infrastructure by executing commands on the target system
    and checking the output against the expected results. You can use ServerSpec with
    Test Kitchen or other IaC tools to ensure that your infrastructure is configured
    correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**InSpec** ([https://github.com/inspec/inspec](https://github.com/inspec/inspec)):
    InSpec, developed by Chef, is an open source infrastructure testing framework.
    It defines tests and compliance rules in a human-readable language. You can run
    InSpec tests independently or in conjunction with tools such as Test Kitchen,
    Chef, or other IaC platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools ensure that our IaC and configuration management setups work as
    expected before deployment to achieve consistency and reliability across different
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure testing using Pytest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also use Pytest, which we used for unit testing in the previous chapter
    for infrastructure testing. Let’s assume we write test functions that should start
    with the `test_` prefix in a Python file called `test_infrastructure.py`. We can
    use Python libraries such as `paramiko`, `requests`, or `socket` to interact with
    our infrastructure (for example, making API calls, connecting to servers, and
    so on). For example, we can test whether a web server is responding with status
    code 200:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can run the tests that we explained in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Other techniques besides infrastructure testing can help you in preparing your
    models for a successful deployment, such as integration testing, which we will
    cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing of machine learning pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we train a machine learning model, we need to evaluate how well it interacts
    with the other components of a larger system it belongs to. Integration testing
    helps us in validating that the model works correctly within the overall application
    or infrastructure and meets the desired performance criteria. Some of the important
    components of integration testing to rely on in our machine learning projects
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing data pipelines**: We need to evaluate that the data preprocessing
    components before model training, such as data wrangling, are consistent between
    the training and deployment stages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing APIs**: If our machine learning model is exposed through an API,
    we can test the API endpoints to ensure it handles requests and responses correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing model deployment**: We can use integration testing to assess the
    model’s deployment process, whether it’s deployed as a standalone service, within
    a container, or embedded in an application. This process helps us ensure that
    the deployment environment provides the necessary resources, such as CPU, memory,
    and storage, and that the model can be updated if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing interactions with other components**: We need to verify that our
    machine learning model works seamlessly with databases, user interfaces, or third-party
    services. This may include testing how the model’s predictions are stored, displayed,
    or used within the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing end-to-end functionality**: We can use end-to-end tests that simulate
    real-world scenarios and user interactions to validate that the model’s predictions
    are accurate, reliable, and useful in the context of the overall application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can benefit from integration testing to ensure a smooth deployment and reliable
    operation in real-world applications. There are several tools and libraries we
    can use to create robust integration tests for our machine learning models in
    Python. *Table 9.1* shows some of the popular tools for integration testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tool** | **Brief Description** | **URL** |'
  prefs: []
  type: TYPE_TB
- en: '| Pytest | A framework widely used for unit and integration testing in Python
    | [https://docs.pytest.org/en/7.2.x/](https://docs.pytest.org/en/7.2.x/) |'
  prefs: []
  type: TYPE_TB
- en: '| Postman | An API testing tool that’s used for testing the interaction between
    machine learning models and RESTful APIs | [https://www.postman.com/](https://www.postman.com/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Requests | A Python library that tests APIs and services by sending HTTP
    requests | [https://requests.readthedocs.io/en/latest/](https://requests.readthedocs.io/en/latest/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Locust | A load testing tool that allows you to simulate user behavior and
    test the performance and scalability of your machine learning models under various
    load conditions | [https://locust.io/](https://locust.io/) |'
  prefs: []
  type: TYPE_TB
- en: '| Selenium | A browser automation tool you can use to test the end-to-end functionality
    of web applications that utilize machine learning models | [https://www.selenium.dev/](https://www.selenium.dev/)
    |'
  prefs: []
  type: TYPE_TB
- en: Table 9.1 – Popular tools for integration testing
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing using pytest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we want to practice integration testing using `pytest` for a simple Python
    application with two components: a database and a service, both of which retrieve
    data from the database. Let’s assume we have the `database.py` and `service.py`
    script files:'
  prefs: []
  type: TYPE_NORMAL
- en: 'database.py:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'service.py:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will write an integration test using `pytest` to ensure that the `UserService`
    component works correctly with the `Database` component. First, we need to write
    our tests in a test script file, called `test_integration.py`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The defined `test_get_user_name` function tests the interaction between the
    `UserService` and `Database` components by checking whether the `get_user_name`
    method returns the correct usernames for different user IDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the test, we can execute the following command in the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Integration testing using pytest and requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can combine the `requests` and `pytest` Python libraries to perform integration
    testing on our machine learning APIs. We can use the `requests` library to send
    HTTP requests and the `pytest` library to write test cases. Let’s suppose we have
    a machine learning API with the following endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the API accepts a JSON payload with input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns a JSON response with the predicted price:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to create a test script file called `test_integration.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the test, we can execute the following command in the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we defined a test function called `test_predict_house_price`
    that sends a POST request (that is, an HTTP method used to submit data to a server
    to create or update a resource) to the API with the input data as a JSON payload.
    The test function then checks the API response’s status code, content type, and
    predicted price value. If you want to try this with a real API you have, replace
    the example URL with the actual API endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the testing strategies we covered in this chapter, you can benefit
    from model monitoring and assertion to have successful deployments and reliable
    models in production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and validating live performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use monitoring and logging mechanisms during deployment to track the
    model’s performance and detect potential issues. We can regularly evaluate the
    deployed model to ensure it continues to meet performance criteria, or other criteria,
    such as being unbiased, that we defined for it. We can also benefit from the information
    coming from model monitoring to update or retrain the model as needed. Here are
    three important concepts in this subject regarding differences between modeling
    before deployment and in production:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data variance**: The data that is used in model training and testing goes
    through the steps of data wrangling and all the cleaning and reformatting needed.
    However, the data that is given to the deployed model – that is, the data coming
    from the user to the model – might not go through the same data processes, which
    then causes variations in the model results in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data drift**: Data drift happens if the characteristics and meaning of features
    or independent variables in production differ from those in the modeling stage.
    Imagine you used a third-party tool to generate a score for the health or financial
    situation of people. The algorithm behind that tool could change over time, and
    its range and meaning will not be the same when your model gets used in production.
    If you have not updated your model accordingly, then your model will not work
    as expected as the meaning of the value of the features will not be the same between
    the data used for training and the user data after deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concept drift**: Concept drift is about any change in the definition of output
    variables. For example, real decision boundaries between training data and production
    could be different because of concept drift, meaning the effort made in training
    might result in a decision boundary far from reality in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to MLflow, which was introduced in the previous chapter, there
    are Python and libraries tools (as listed in *Table 9.2*) that you can use to
    monitor the performance, I/O data, and infrastructure of machine learning models,
    helping you maintain model quality and reliability in production environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tool** | **Brief Description** | **URL** |'
  prefs: []
  type: TYPE_TB
- en: '| Alibi Detect | An open source Python library that focuses on outlier, adversarial,
    and drift detection | [https://github.com/SeldonIO/alibi-detect](https://github.com/SeldonIO/alibi-detect)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Evidently | An open source Python library for analyzing and monitoring machine
    learning models that offers various model evaluation techniques, such as data
    drift detection and model performance monitoring | [https://github.com/evidentlyai/evidently](https://github.com/evidentlyai/evidently)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ELK Stack | **Elasticsearch**, **Logstash**, **and Kibana** (**ELK**) is
    a popular stack for collecting, processing, and visualizing logs and metrics from
    various sources, including machine learning models | [https://www.elastic.co/elk-stack](https://www.elastic.co/elk-stack)
    |'
  prefs: []
  type: TYPE_TB
- en: '| WhyLabs | A platform that provides observability and monitoring for machine
    learning models | [https://whylabs.ai/](https://whylabs.ai/) |'
  prefs: []
  type: TYPE_TB
- en: Table 9.2 – Popular tools for machine learning model monitoring and drift detection
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also benefit from some statistical and visualization techniques for
    detecting and addressing data and concept drifts. Here are some examples of such
    methods for data drift evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical tests**: We can use hypothesis tests, such as the *Kolmogorov-Smirnov
    test*, *Chi-squared test*, or *Mann-Whitney U test*, to determine whether the
    distribution of input data has changed significantly over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distribution metrics**: We can use distribution metrics, such as mean, standard
    deviation, quantiles, and other summary statistics, to compare training data and
    the new data in production. Significant differences in these metrics may indicate
    data drift.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualization**: We can use visualization techniques such as histograms,
    boxplots, or scatter plots for the input features of the training data and the
    new data in production to help identify changes in the data distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature importance**: We can monitor changes in feature importance values.
    If feature importance in the new data differs significantly from those in the
    training data, it may indicate data drift.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distance metrics**: We can measure the difference between the training data
    and the new data distributions using distance metrics such as *Kullback-Leibler
    divergence* or *Jensen-Shannon divergence*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model assertion is another technique, as you will learn next, that helps you
    in building and deploying reliable machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Model assertion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use traditional programming assertion in machine learning modeling to
    ensure that the model is behaving as expected. Model assertions can help us detect
    issues early on, such as input data drift or other unexpected behaviors that might
    affect the model’s performance. We can consider model assertions as a set of rules
    that get checked during the model’s training, validation, or even during deployment
    to ensure that the model’s predictions meet the predefined conditions. Model assertions
    can help us in many ways, such as detecting issues with the model or input data,
    allowing us to address them before they impact the model’s performance. They can
    also help us maintain the model’s performance. Here are two examples of model
    assertions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input data assertions**: These can check that the input features fall within
    an expected range or have the correct data type. For example, if a model predicts
    house prices based on the number of rooms, you might assert that the number of
    rooms is always a positive integer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output data assertions**: These can check that the model’s predictions meet
    certain conditions or constraints. For example, in a binary classification problem,
    you might assert that the predicted probability is between 0 and 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s go through a simple example of model assertion in Python. In this example,
    we will use a simple linear regression model from `scikit-learn` to predict house
    prices based on the number of rooms, using a toy dataset. First, let’s create
    a toy dataset and train the linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s define our model assertions so that they do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Check that the input (number of rooms) is a positive integer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check that the predicted house price is within the expected range.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s the code that does these things:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use the defined model assertion functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `assert_input` function checks whether the input data (that is, the number
    of rooms) is an integer and is positive. The `assert_output` function checks whether
    the predicted house price is within a specified range (for example, between 50,000
    and 350,000 in this example). The previous code doesn’t give any `AssertionError`
    assertion as it meets the criteria defined in the model assertion functions. Let’s
    say that, instead of `3`, which is an integer, we use a string, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we get the following `AssertionError`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s say we define the output range for `assert_output` so that it’s between
    `50000` and `150000` and use the model predictions for a house with `3` bedrooms,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following `AssertionError`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Model assertion is another technique, side by side with model monitoring, that
    helps ensure the reliability of our models.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have come to the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about important concepts for test-driven development,
    including infrastructure and integration testing. You learned about the available
    tools and libraries to implement these two types of testing. We also went through
    examples in which you learned how to use the `pytest` library for both infrastructure
    and integration testing. You also learned about model monitoring and model assertion
    as two other important topics for assessing the behavior of our models before
    and in production. These techniques and tools help you in designing strategies
    so that you have a successful deployment and reliable models in production environments.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about reproducibility, an important concept
    in proper machine learning modeling, and how you can use data and model versioning
    to achieve reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you explain the difference between data and concept drifts?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can model assertions help you in developing reliable machine learning models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some examples of components of integration testing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we use Chef, Puppet, and Ansible?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kang, Daniel, et al. *Model assertions for monitoring and improving ML models*.
    Proceedings of Machine Learning and Systems 2 (2020): 481-496.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
