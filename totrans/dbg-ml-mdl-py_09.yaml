- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing and Debugging for Production
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might have gotten excited about training and testing a machine learning
    model without thinking about the unexpected behavior of your model in production
    and how your model fits into a bigger technology. Most academic courses don’t
    go through details of strategies to test models, assess their qualities, and monitor
    their performance pre-deployment and in production. There are important concepts
    and techniques in testing and debugging models for production that we will review
    in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure testing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration testing of machine learning pipelines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and validating live performance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model assertion
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about the importance of infrastructure
    and integration testing, as well as model monitoring and assertion. You will have
    also learned how to use Python libraries so that you can benefit from them in
    your projects.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Python library requirements:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` >= 1.2.2'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` >= 1.22.4'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytest` >= 7.2.2'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You must also have basic knowledge of the machine learning life cycle
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter09](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter09).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure testing
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Infrastructure testing refers to the process of verifying and validating the
    various components and systems involved in deploying, managing, and scaling machine
    learning models. This includes testing software, hardware, and other resources
    that make up the infrastructure that supports machine learning workflows. Infrastructure
    testing in machine learning helps you ensure that models are trained, deployed,
    and maintained effectively. It provides you with reliable models in a production
    environment. Regular infrastructure testing can help you detect and fix issues
    early and reduce the risk of failures during deployment and in the production
    stage.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the important aspects of infrastructure testing in machine
    learning:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '**Data pipeline testing**: This ensures that the data pipelines responsible
    for data collection, selection, and wrangling are working correctly and efficiently.
    This helps maintain data quality and consistency for training, testing, and deploying
    your machine learning models.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training and evaluation**: This validates the functionality of the
    model training process, such as hyperparameter tuning and model evaluation. This
    process eliminates unexpected issues in training and evaluation to achieve a reliable
    and responsible model.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment and serving**: This tests the process of deploying the trained
    model in a production environment, ensuring that the serving infrastructure, such
    as API endpoints, is working correctly and can handle the expected request load.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and observability**: This tests the monitoring and logging systems
    that provide insights into the performance and behavior of the machine learning
    infrastructure.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration testing**: This verifies that all components of the machine learning
    infrastructure, such as data pipelines, model training systems, and deployment
    platforms, are working together seamlessly and without conflicts.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability testing**: This evaluates the ability of the infrastructure to
    scale up or down in response to changing requirements, such as increased data
    volume, higher user traffic, or more complex models.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and compliance testing**: This ensures that the machine learning
    infrastructure meets necessary security requirements, data protection regulations,
    and privacy standards.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you understand the importance and benefits of infrastructure testing,
    you are ready to learn about the related tools that can help you in model deployment
    and infrastructure management.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as Code tools
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Infrastructure as Code** (**IaC**) and configuration management tools such
    as **Chef**, **Puppet**, and **Ansible** can be used to automate the deployment,
    configuration, and management of software and hardware infrastructures. These
    tools could help us ensure consistency and reliability across different environments.
    Let’s understand how Chef, Puppet, and Ansible work, and how they can help you
    in your projects:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**Chef** ([https://www.chef.io/products/chef-infrastructure-management](https://www.chef.io/products/chef-infrastructure-management)):
    Chef is an open source configuration management tool that relies on a client-server
    model, where the Chef server stores the desired configuration, and the Chef client
    applies it to the nodes.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Puppet** ([https://www.puppet.com/](https://www.puppet.com/)): Puppet is
    another open source configuration management tool that works in a client-server
    model or as a standalone application. Puppet enforces desired configurations across
    nodes by periodically pulling them from the Puppet master server.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ansible** ([https://www.ansible.com/](https://www.ansible.com/)): Ansible
    is an open source and easy-to-use configuration management, orchestration, and
    automation tool that communicates and applies configurations to nodes.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools primarily focus on infrastructure management and automation, but
    they also have modules or plugins to perform basic testing and validation of the
    infrastructure.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure testing tools
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Test Kitchen, ServerSpec, and InSpec are infrastructure testing tools we can
    use to verify and validate the desired configuration and behavior of our infrastructures:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '**Test Kitchen** ([https://github.com/test-kitchen/test-kitchen](https://github.com/test-kitchen/test-kitchen)):
    Test Kitchen is an integration testing framework mainly used with Chef but can
    also work with other IaC tools such as Ansible and Puppet. It allows you to test
    your infrastructure code on different platforms and configurations. Test Kitchen
    creates temporary instances on various platforms (using drivers such as Docker
    or cloud providers), converges your infrastructure code, and runs tests against
    the configured instances. You can use Test Kitchen with different testing frameworks,
    such as ServerSpec or InSpec, to define your tests.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ServerSpec** ([https://serverspec.org/](https://serverspec.org/)): ServerSpec
    is a **behavior-driven development** (**BDD**) testing framework for infrastructure.
    It allows you to write tests in a human-readable language. ServerSpec tests the
    desired state of your infrastructure by executing commands on the target system
    and checking the output against the expected results. You can use ServerSpec with
    Test Kitchen or other IaC tools to ensure that your infrastructure is configured
    correctly.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**InSpec** ([https://github.com/inspec/inspec](https://github.com/inspec/inspec)):
    InSpec, developed by Chef, is an open source infrastructure testing framework.
    It defines tests and compliance rules in a human-readable language. You can run
    InSpec tests independently or in conjunction with tools such as Test Kitchen,
    Chef, or other IaC platforms.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools ensure that our IaC and configuration management setups work as
    expected before deployment to achieve consistency and reliability across different
    environments.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure testing using Pytest
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also use Pytest, which we used for unit testing in the previous chapter
    for infrastructure testing. Let’s assume we write test functions that should start
    with the `test_` prefix in a Python file called `test_infrastructure.py`. We can
    use Python libraries such as `paramiko`, `requests`, or `socket` to interact with
    our infrastructure (for example, making API calls, connecting to servers, and
    so on). For example, we can test whether a web server is responding with status
    code 200:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, we can run the tests that we explained in the previous chapter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Other techniques besides infrastructure testing can help you in preparing your
    models for a successful deployment, such as integration testing, which we will
    cover next.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing of machine learning pipelines
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we train a machine learning model, we need to evaluate how well it interacts
    with the other components of a larger system it belongs to. Integration testing
    helps us in validating that the model works correctly within the overall application
    or infrastructure and meets the desired performance criteria. Some of the important
    components of integration testing to rely on in our machine learning projects
    are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing data pipelines**: We need to evaluate that the data preprocessing
    components before model training, such as data wrangling, are consistent between
    the training and deployment stages.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing APIs**: If our machine learning model is exposed through an API,
    we can test the API endpoints to ensure it handles requests and responses correctly.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing model deployment**: We can use integration testing to assess the
    model’s deployment process, whether it’s deployed as a standalone service, within
    a container, or embedded in an application. This process helps us ensure that
    the deployment environment provides the necessary resources, such as CPU, memory,
    and storage, and that the model can be updated if needed.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing interactions with other components**: We need to verify that our
    machine learning model works seamlessly with databases, user interfaces, or third-party
    services. This may include testing how the model’s predictions are stored, displayed,
    or used within the application.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing end-to-end functionality**: We can use end-to-end tests that simulate
    real-world scenarios and user interactions to validate that the model’s predictions
    are accurate, reliable, and useful in the context of the overall application.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can benefit from integration testing to ensure a smooth deployment and reliable
    operation in real-world applications. There are several tools and libraries we
    can use to create robust integration tests for our machine learning models in
    Python. *Table 9.1* shows some of the popular tools for integration testing:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tool** | **Brief Description** | **URL** |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| Pytest | A framework widely used for unit and integration testing in Python
    | [https://docs.pytest.org/en/7.2.x/](https://docs.pytest.org/en/7.2.x/) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| Postman | An API testing tool that’s used for testing the interaction between
    machine learning models and RESTful APIs | [https://www.postman.com/](https://www.postman.com/)
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| Requests | A Python library that tests APIs and services by sending HTTP
    requests | [https://requests.readthedocs.io/en/latest/](https://requests.readthedocs.io/en/latest/)
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| Locust | A load testing tool that allows you to simulate user behavior and
    test the performance and scalability of your machine learning models under various
    load conditions | [https://locust.io/](https://locust.io/) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| Selenium | A browser automation tool you can use to test the end-to-end functionality
    of web applications that utilize machine learning models | [https://www.selenium.dev/](https://www.selenium.dev/)
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: Table 9.1 – Popular tools for integration testing
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing using pytest
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we want to practice integration testing using `pytest` for a simple Python
    application with two components: a database and a service, both of which retrieve
    data from the database. Let’s assume we have the `database.py` and `service.py`
    script files:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'database.py:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'service.py:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we will write an integration test using `pytest` to ensure that the `UserService`
    component works correctly with the `Database` component. First, we need to write
    our tests in a test script file, called `test_integration.py`, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `pytest` 编写一个集成测试，以确保 `UserService` 组件与 `Database` 组件正确工作。首先，我们需要在名为
    `test_integration.py` 的测试脚本文件中编写我们的测试，如下所示：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The defined `test_get_user_name` function tests the interaction between the
    `UserService` and `Database` components by checking whether the `get_user_name`
    method returns the correct usernames for different user IDs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好的 `test_get_user_name` 函数通过检查 `get_user_name` 方法是否为不同的用户 ID 返回正确的用户名来测试 `UserService`
    和 `Database` 组件之间的交互。
- en: 'To run the test, we can execute the following command in the Terminal:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行测试，我们可以在终端中执行以下命令：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Integration testing using pytest and requests
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 pytest 和 requests 进行集成测试
- en: 'We can combine the `requests` and `pytest` Python libraries to perform integration
    testing on our machine learning APIs. We can use the `requests` library to send
    HTTP requests and the `pytest` library to write test cases. Let’s suppose we have
    a machine learning API with the following endpoint:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 `requests` 和 `pytest` Python 库结合起来，对我们的机器学习 API 进行集成测试。我们可以使用 `requests`
    库发送 HTTP 请求，并使用 `pytest` 库编写测试用例。假设我们有一个机器学习 API，其端点如下：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, the API accepts a JSON payload with input data:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，API 接受一个包含输入数据的 JSON 有效负载：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This returns a JSON response with the predicted price:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个包含预测价格的 JSON 响应：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we need to create a test script file called `test_integration.py`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个名为 `test_integration.py` 的测试脚本文件：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To run the test, we can execute the following command in the Terminal:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行测试，我们可以在终端中执行以下命令：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this example, we defined a test function called `test_predict_house_price`
    that sends a POST request (that is, an HTTP method used to submit data to a server
    to create or update a resource) to the API with the input data as a JSON payload.
    The test function then checks the API response’s status code, content type, and
    predicted price value. If you want to try this with a real API you have, replace
    the example URL with the actual API endpoint.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们定义了一个名为 `test_predict_house_price` 的测试函数，该函数向 API 发送 POST 请求（即用于将数据提交到服务器以创建或更新资源的
    HTTP 方法），并将输入数据作为 JSON 有效负载。然后，测试函数检查 API 响应的状态码、内容类型和预测价格值。如果您想尝试使用您拥有的真实 API，请将示例
    URL 替换为实际的 API 端点。
- en: In addition to the testing strategies we covered in this chapter, you can benefit
    from model monitoring and assertion to have successful deployments and reliable
    models in production environments.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 除了本章中提到的测试策略外，您还可以通过模型监控和断言来确保在生产环境中成功部署和可靠的模型。
- en: Monitoring and validating live performance
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控和验证实时性能
- en: 'We can use monitoring and logging mechanisms during deployment to track the
    model’s performance and detect potential issues. We can regularly evaluate the
    deployed model to ensure it continues to meet performance criteria, or other criteria,
    such as being unbiased, that we defined for it. We can also benefit from the information
    coming from model monitoring to update or retrain the model as needed. Here are
    three important concepts in this subject regarding differences between modeling
    before deployment and in production:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署期间，我们可以使用监控和日志记录机制来跟踪模型的性能并检测潜在问题。我们可以定期评估已部署的模型，以确保它继续满足性能标准或其他标准，例如无偏见，这是我们为其定义的。我们还可以利用模型监控的信息，根据需要更新或重新训练模型。以下是关于部署前和在生产中建模之间差异的三个重要概念：
- en: '**Data variance**: The data that is used in model training and testing goes
    through the steps of data wrangling and all the cleaning and reformatting needed.
    However, the data that is given to the deployed model – that is, the data coming
    from the user to the model – might not go through the same data processes, which
    then causes variations in the model results in production.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据方差**：用于模型训练和测试的数据会经过数据整理和所有必要的清理和重新格式化步骤。然而，提供给已部署模型的（即从用户到模型的数据）可能不会经过相同的数据处理过程，这会导致生产中模型结果出现差异。'
- en: '**Data drift**: Data drift happens if the characteristics and meaning of features
    or independent variables in production differ from those in the modeling stage.
    Imagine you used a third-party tool to generate a score for the health or financial
    situation of people. The algorithm behind that tool could change over time, and
    its range and meaning will not be the same when your model gets used in production.
    If you have not updated your model accordingly, then your model will not work
    as expected as the meaning of the value of the features will not be the same between
    the data used for training and the user data after deployment.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concept drift**: Concept drift is about any change in the definition of output
    variables. For example, real decision boundaries between training data and production
    could be different because of concept drift, meaning the effort made in training
    might result in a decision boundary far from reality in production.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to MLflow, which was introduced in the previous chapter, there
    are Python and libraries tools (as listed in *Table 9.2*) that you can use to
    monitor the performance, I/O data, and infrastructure of machine learning models,
    helping you maintain model quality and reliability in production environments:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tool** | **Brief Description** | **URL** |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| Alibi Detect | An open source Python library that focuses on outlier, adversarial,
    and drift detection | [https://github.com/SeldonIO/alibi-detect](https://github.com/SeldonIO/alibi-detect)
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '| Evidently | An open source Python library for analyzing and monitoring machine
    learning models that offers various model evaluation techniques, such as data
    drift detection and model performance monitoring | [https://github.com/evidentlyai/evidently](https://github.com/evidentlyai/evidently)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: '| ELK Stack | **Elasticsearch**, **Logstash**, **and Kibana** (**ELK**) is
    a popular stack for collecting, processing, and visualizing logs and metrics from
    various sources, including machine learning models | [https://www.elastic.co/elk-stack](https://www.elastic.co/elk-stack)
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '| WhyLabs | A platform that provides observability and monitoring for machine
    learning models | [https://whylabs.ai/](https://whylabs.ai/) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: Table 9.2 – Popular tools for machine learning model monitoring and drift detection
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also benefit from some statistical and visualization techniques for
    detecting and addressing data and concept drifts. Here are some examples of such
    methods for data drift evaluation:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical tests**: We can use hypothesis tests, such as the *Kolmogorov-Smirnov
    test*, *Chi-squared test*, or *Mann-Whitney U test*, to determine whether the
    distribution of input data has changed significantly over time.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distribution metrics**: We can use distribution metrics, such as mean, standard
    deviation, quantiles, and other summary statistics, to compare training data and
    the new data in production. Significant differences in these metrics may indicate
    data drift.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualization**: We can use visualization techniques such as histograms,
    boxplots, or scatter plots for the input features of the training data and the
    new data in production to help identify changes in the data distributions.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化**：我们可以使用直方图、箱线图或散点图等可视化技术来展示训练数据和生产中新数据的输入特征，以帮助识别数据分布的变化。'
- en: '**Feature importance**: We can monitor changes in feature importance values.
    If feature importance in the new data differs significantly from those in the
    training data, it may indicate data drift.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征重要性**：我们可以监控特征重要性值的变化。如果新数据中的特征重要性与训练数据中的特征重要性有显著差异，这可能表明数据漂移。'
- en: '**Distance metrics**: We can measure the difference between the training data
    and the new data distributions using distance metrics such as *Kullback-Leibler
    divergence* or *Jensen-Shannon divergence*.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**距离度量**：我们可以使用诸如*Kullback-Leibler散度*或*Jensen-Shannon散度*等距离度量来衡量训练数据与新数据分布之间的差异。'
- en: Model assertion is another technique, as you will learn next, that helps you
    in building and deploying reliable machine learning models.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 模型断言是另一种技术，正如你接下来将要学习的，它可以帮助你构建和部署可靠的机器学习模型。
- en: Model assertion
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型断言
- en: 'We can use traditional programming assertion in machine learning modeling to
    ensure that the model is behaving as expected. Model assertions can help us detect
    issues early on, such as input data drift or other unexpected behaviors that might
    affect the model’s performance. We can consider model assertions as a set of rules
    that get checked during the model’s training, validation, or even during deployment
    to ensure that the model’s predictions meet the predefined conditions. Model assertions
    can help us in many ways, such as detecting issues with the model or input data,
    allowing us to address them before they impact the model’s performance. They can
    also help us maintain the model’s performance. Here are two examples of model
    assertions:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在机器学习建模中使用传统的编程断言来确保模型按预期行为。模型断言可以帮助我们早期发现问题，例如输入数据漂移或其他可能影响模型性能的意外行为。我们可以将模型断言视为一组在模型训练、验证甚至部署期间进行检查的规则，以确保模型的预测满足预定义的条件。模型断言可以从许多方面帮助我们，例如检测模型或输入数据的问题，使我们能够在它们影响模型性能之前解决它们。它们还可以帮助我们保持模型性能。以下是一些模型断言的示例：
- en: '**Input data assertions**: These can check that the input features fall within
    an expected range or have the correct data type. For example, if a model predicts
    house prices based on the number of rooms, you might assert that the number of
    rooms is always a positive integer.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入数据断言**：这些可以检查输入特征是否在预期的范围内或具有正确的数据类型。例如，如果一个模型根据房间数量预测房价，你可能会断言房间数量始终是正整数。'
- en: '**Output data assertions**: These can check that the model’s predictions meet
    certain conditions or constraints. For example, in a binary classification problem,
    you might assert that the predicted probability is between 0 and 1.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出数据断言**：这些可以检查模型的预测是否满足某些条件或约束。例如，在二元分类问题中，你可能会断言预测的概率在0到1之间。'
- en: 'Let’s go through a simple example of model assertion in Python. In this example,
    we will use a simple linear regression model from `scikit-learn` to predict house
    prices based on the number of rooms, using a toy dataset. First, let’s create
    a toy dataset and train the linear regression model:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的Python中模型断言的例子来了解。在这个例子中，我们将使用`scikit-learn`中的简单线性回归模型，使用玩具数据集根据房间数量预测房价。首先，让我们创建一个玩具数据集并训练线性回归模型：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let’s define our model assertions so that they do the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义我们的模型断言，以便它们执行以下操作：
- en: Check that the input (number of rooms) is a positive integer.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查输入（房间数量）是否为正整数。
- en: Check that the predicted house price is within the expected range.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查预测的房价是否在预期范围内。
- en: 'Here’s the code that does these things:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是执行这些操作的代码：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we can use the defined model assertion functions, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用定义好的模型断言函数，如下所示：
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `assert_input` function checks whether the input data (that is, the number
    of rooms) is an integer and is positive. The `assert_output` function checks whether
    the predicted house price is within a specified range (for example, between 50,000
    and 350,000 in this example). The previous code doesn’t give any `AssertionError`
    assertion as it meets the criteria defined in the model assertion functions. Let’s
    say that, instead of `3`, which is an integer, we use a string, as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert_input`函数检查输入数据（即房间数量）是否为整数且为正数。`assert_output`函数检查预测的房价是否在指定的范围内（例如，在本例中为50,000至350,000）。前面的代码没有给出任何`AssertionError`断言，因为它符合模型断言函数中定义的标准。假设我们不是使用整数`3`，而是使用一个字符串，如下所示：'
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here, we get the following `AssertionError`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们得到以下`AssertionError`：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s say we define the output range for `assert_output` so that it’s between
    `50000` and `150000` and use the model predictions for a house with `3` bedrooms,
    as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们定义了`assert_output`的输出范围，使其在`50000`和`150000`之间，并使用具有`3`个卧室的房屋模型预测，如下所示：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will get the following `AssertionError`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下`AssertionError`：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Model assertion is another technique, side by side with model monitoring, that
    helps ensure the reliability of our models.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 模型断言是另一种技术，与模型监控并列，有助于确保我们模型的可靠性。
- en: With this, we have come to the end of this chapter.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就结束了这一章。
- en: Summary
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about important concepts for test-driven development,
    including infrastructure and integration testing. You learned about the available
    tools and libraries to implement these two types of testing. We also went through
    examples in which you learned how to use the `pytest` library for both infrastructure
    and integration testing. You also learned about model monitoring and model assertion
    as two other important topics for assessing the behavior of our models before
    and in production. These techniques and tools help you in designing strategies
    so that you have a successful deployment and reliable models in production environments.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了测试驱动开发的重要概念，包括基础设施和集成测试。你学习了实现这两种类型测试的可用工具和库。我们还通过示例学习了如何使用`pytest`库进行基础设施和集成测试。你还学习了模型监控和模型断言作为评估我们模型在生产和生产环境中的行为之前和之后的两个其他重要主题。这些技术和工具帮助你设计策略，以便你在生产环境中成功部署并拥有可靠的模型。
- en: In the next chapter, you will learn about reproducibility, an important concept
    in proper machine learning modeling, and how you can use data and model versioning
    to achieve reproducibility.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将了解可重复性，这是正确机器学习建模中的一个重要概念，以及你如何可以使用数据和模型版本控制来实现可重复性。
- en: Questions
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Can you explain the difference between data and concept drifts?
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能解释数据漂移和概念漂移之间的区别吗？
- en: How can model assertions help you in developing reliable machine learning models?
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型断言如何帮助你开发可靠的机器学习模型？
- en: What are some examples of components of integration testing?
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集成测试的组件有哪些例子？
- en: How can we use Chef, Puppet, and Ansible?
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何使用Chef、Puppet和Ansible？
- en: References
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Kang, Daniel, et al. *Model assertions for monitoring and improving ML models*.
    Proceedings of Machine Learning and Systems 2 (2020): 481-496.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kang, Daniel, et al. *模型监控与改进的模型断言*. 机器学习与系统会议论文集 2 (2020): 481-496.'
