["```py\n    inputs = [5, 4, 2, 1, 6]\n    weights = [0.1, 0.3, 0.05, 0.4, 0.9]\n    bias = 4\n    ```", "```py\n    output = (inputs[0] * weights[0] + \n              inputs[1] * weights[1] + \n              inputs[2] * weights[2] +\n              inputs[3] * weights[3] +\n              inputs[4] * weights[4] + \n              bias)\n    output\n    ```", "```py\n    output = 0\n    for x, w in zip(inputs, weights):\n        output += x * w\n    output += bias\n    output\n    ```", "```py\n    import numpy as np\n    output = np.dot(inputs, weights) + bias\n    output\n    ```", "```py\ninputs = [5, 4, 2, 1, 6]\nweights = [\n    [0.1, 0.3, 0.05, 0.4, 0.9],\n    [0.3, 0.15, 0.4, 0.7, 0.2]\n]\nbiases = [4, 2]\n```", "```py\n    layer = []\n    for n_w, n_b in zip(weights, biases):\n        output = 0\n        for x, w in zip(inputs, n_w):\n            output += x * w\n        output += n_b\n        layer.append(output)\n\n    layer\n    ```", "```py\n    import numpy as np\n    layer = []\n    for n_w, n_b in zip(weights, biases):\n        layer.append(np.dot(inputs, n_w) + n_b)\n    layer\n    ```", "```py\n    layer = np.dot(inputs, np.transpose(weights)) + biases\n    layer\n    ```", "```py\n    def step_function(x):\n        return 1 if x > 0 else 0\n    ```", "```py\n    xs = np.arange(-10, 10, step=0.1)\n    ys = [step_function(x) for x in xs]\n    ```", "```py\n    plt.plot(xs, ys, color='#000000', lw=3)\n    plt.title('Step activation function', fontsize=20)\n    ```", "```py\n    def sigmoid_function(x):\n         return 1 / (1 + np.exp(-x))\n    ```", "```py\n    xs = np.arange(-10, 10, step=0.1)\n    ys = [step_function(x) for x in xs]\n    ```", "```py\n    plt.plot(xs, ys, color='#000000', lw=3)\n    plt.title(Sigmoid activation function', fontsize=20)\n    ```", "```py\n    def tanh_function(x):\n        return np.tanh(x)\n    ```", "```py\n    xs = np.arange(-10, 10, step=0.1)\n    ys = [step_function(x) for x in xs]\n    ```", "```py\n    plt.plot(xs, ys, color='#000000', lw=3)\n    plt.title(Tanh activation function', fontsize=20)\n    ```", "```py\n    def relu_function(x):\n        return np.maximum(0, x)\n    ```", "```py\n    xs = np.arange(-10, 10, step=0.1)\n    ys = [step_function(x) for x in xs]\n    ```", "```py\n    plt.plot(xs, ys, color='#000000', lw=3)\n    plt.title(ReLU activation function', fontsize=20)\n    ```", "```py\n    import tensorflow as tf\n    from tensorflow.keras import datasets, layers, models\n    ```", "```py\n    (train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n    train_images, test_images = train_images / 255.0, test_images / 255.0\n    ```", "```py\n    print('\\n'.join([''.join(['{:4}'.format(round(item, 1)) for item in row]) for row in train_images[0]]))\n    ```", "```py\n    model = models.Sequential([\n      layers.Flatten(input_shape=(28, 28)),\n      layers.Dense(128, activation='relu'),\n      layers.Dense(10)\n    ])\n    ```", "```py\n    model.summary()\n    ```", "```py\n        model.compile(\n            optimizer='adam',\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            metrics=['accuracy']\n        )\n        ```", "```py\n    history = model.fit(\n        train_images, \n        train_labels, \n        epochs=10, \n        validation_data=(test_images, test_labels)\n    )\n    ```", "```py\n    import numpy as np\n    prediction = model.predict(test_images[0].reshape(-1, 784))\n    print(f'True digit = {test_labels[0]}')\n    print(f'Predicted digit = {np.argmax(prediction)}')\n    ```"]