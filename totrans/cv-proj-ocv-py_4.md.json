["```py\n!pip install pyyaml easydict munkres\n```", "```py\n%pylab notebook\nimport os\nimport sys\nimport cv2\nfrom imageio import imread\nfrom random import randint\nimport numpy as np\nimport tensorflow as tf\nfrom config import load_config\nfrom nnet.net factory import pose_net\n```", "```py\ndef setup_pose_prediction(cfg):\n    inputs = tf.placeholder(tf.float32, shape=[cfg.batch_size, None, None, 3])\n\n    outputs = pose_net(cfg).test(inputs)\n\n    restorer = tf.train.Saver()\n\n    sess = tf.Session()\n\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n\n    # Restore variables from disk.\n    restorer.restore(sess, cfg.init_weights)\n\n    return sess, inputs, outputs\n```", "```py\ndef extract_cnn_output(outputs_np, cfg, pairwise_stats = None):\n    scmap = outputs_np['part_prob']\n    scmap = np.squeeze(scmap)\n    locref = None\n    pairwise_diff = None\n    if cfg.location_refinement:\n        locref = np.squeeze(outputs_np['locref'])\n        shape = locref.shape\n        locref = np.reshape(locref, (shape[0], shape[1], -1, 2))\n        locref *= cfg.locref_stdev\n    if cfg.pairwise_predict:\n        pairwise_diff = np.squeeze(outputs_np['pairwise_pred'])\n        shape = pairwise_diff.shape\n        pairwise_diff = np.reshape(pairwise_diff, (shape[0], shape[1], -1, 2))\n        num_joints = cfg.num_joints\n        for pair in pairwise_stats:\n            pair_id = (num_joints - 1) * pair[0] + pair[1] - int(pair[0] < pair[1])\n            pairwise_diff[:, :, pair_id, 0] *= pairwise_stats[pair][\"std\"][0]\n            pairwise_diff[:, :, pair_id, 0] += pairwise_stats[pair][\"mean\"][0]\n            pairwise_diff[:, :, pair_id, 1] *= pairwise_stats[pair][\"std\"][1]\n            pairwise_diff[:, :, pair_id, 1] += pairwise_stats[pair][\"mean\"][1]\n    return scmap, locref, pairwise_diff\n```", "```py\ndef argmax_pose_predict(scmap, offmat, stride):\n    \"\"\"Combine scoremat and offsets to the final pose.\"\"\"\n    num_joints = scmap.shape[2]\n    pose = []\n    for joint_idx in range(num_joints):\n        maxloc = np.unravel_index(np.argmax(scmap[:, :, joint_idx]),\n                                  scmap[:, :, joint_idx].shape)\n        offset = np.array(offmat[maxloc][joint_idx])[::-1] if offmat is not None else 0\n        pos_f8 = (np.array(maxloc).astype('float') * stride + 0.5 * stride +\n                  offset)\n        pose.append(np.hstack((pos_f8[::-1],\n                               [scmap[maxloc][joint_idx]])))\n    return np.array(pose)\n```", "```py\ncfg = load_config(\"demo/pose_cfg.yaml\")\nsess, inputs, outputs = setup_pose_prediction(cfg)\n```", "```py\nINFO:tensorflow:restoring parameters from models/mpii/mpii-single-resnet-101\n```", "```py\nfile_name = \"testcases/standing-lef-lift.jpg\"\nimage = np.array(imread(file_name))\nimage_batch = np.expand_dims(image, axis=0).astype(float)\noutputs_np = sess.run(outputs, feed_dict={inputs: image_batch})\nscmap, locref, pairwise_diff = extract_cnn_output(outputs_np, cfg)\npose = argmax_pose_predict(scmap, locref, cfg.stride)\n```", "```py\npose2D = pose[:, :2]\nimage_annot = image.copy()\n\nfor index in range(5):\n    randcolor = tuple([randint(0, 255) for i in range(3)])\n    thickness = int(min(image_annot[:,:,0].shape)/250) + 1\n    start_pt = tuple(pose2D[index].astype('int'))\n    end_pt = tuple(pose2D[index+1].astype('int'))\n    image_annot = cv2.line(image_annot, start_pt, end_pt, randcolor, thickness)\nfor index in range(6,11): #next bunch are arms/shoulders (from one hand to other)\n    randcolor = tuple([randint(0,255) for i in range(3)])\n    thickness = int(min(image_annot[:,:,0].shape)/250) + 1\n    start_pt = tuple(pose2D[index].astype('int'))\n    end_pt = tuple(pose2D[index+1].astype('int'))\n    image_annot = cv2.line(image_annot, start_pt, end_pt, randcolor, thickness)\n#connect Line from chin to top of head\nimage_annot = cv2.line(image_annot,\n                       tuple(pose2D[12].astype('int')), tuple(pose2D[13].astype('int'))\n                       tuple([randint(0,255) for i in range(3)]), thickness)\n```", "```py\nfigure()\nimshow(image)\n```", "```py\n# There no actual joints on waist or coLLar,\n# but we can estimate them from hip/shoulder midpoints\nwaist = tuple(((pose2D[2]+pose2D[3])/2).astype('int'))\ncollar = tuple(((pose2D[8]+pose2D[9])/2).astype('int'))\n# draw the \"spine\"\nimage_annot = cv2.line(image_annot, waist, collar,\n                       tuple([randint(0,255) for i in range(3)]), thickness)\nimage_annot = cv2.line(image_annot, tuple(pose2D[12].astype('int')), collar,\n                       tuple([randint(0,255) for i in range(3)]), thickness)\n# now Label the joints with numbers\nfont = cv2.FONT_HERSHEY_SIMPLEX\nfontsize = min(image_annot[:,:,0].shape)/750 #scale the font size to the image size\nfor idx, pt in enumerate(pose2D):\n    randcolor = tuple([randint(0,255) for i in range(3)])\nimage_annot = cv2.putText(image_annot, str(idx+1),\n                          tup1e(pt.astype('int')),font, fontsize,\n                          randcolor,2,cv2.LINE_AA)\nfigure()\nimshow(image_annot)\n```", "```py\nfile_name = \"testcases/mountain_pose.jpg\"\nimage = np.array(imread(file_name))\nimage_batch = np.expand_dims(image, axis=0).astype(float)\noutputs_np = sess.run(outputs, feed_dict={inputs: image_batch})\nscmap, locref, pairwise_diff = extract_cnn_output(outputs_np, cfg)\npose = argmax_pose_predict(scmap, locref, cfg.stride)\n```", "```py\nimport os\nimport sys\nimport numpy as np\nimport cv2 I\nfrom imageio import imread, imsave\nfrom config import load_config\nfrom dataset.factory import create as create_dataset\nfrom nnet import predict\nfrom dataset.pose_dataset import data_to_input\nfrom multiperson.detections import extract_detections\nfrom multiperson.predict import SpatialModel, eval_graph, get_person_conf_mu1ticut\n# from muLtiperson.visuaLize import PersonDraw, visuaLize_detections\n```", "```py\ncf = load_config(\"demo/pose_cfg_multi.yaml) \n```", "```py\ndataset = create_dataset(cfg)\nsm = SpatialModel(cfg)\nsm.load()\nsess, inputs, outputs = predict.setup_pose_prediction(cfg)\n```", "```py\nfile_name = \"testcases/bus_people.jpg\"\nimage = np.array(imread(file_name))\nimage_batch = data_to_input(image)\n# Compute prediction with the CNN\noutputs_np = sess.run(outputs, feed_dict={inputs: image_batch})\nscmap, locref, pairwise_diff = predict.extract_cnn_output(outputs_np, cfg, dataset\ndetections = extract_detections(cfg, scmap, locref, pairwise_diff)\nunLab, pos_array, unary_array, pwidx_array, pw_array = eval_graph(sm, detections)\nperson_conf_multi = get_person_conf_multicut(sm, unLab, unary_array, pos_array)\nimage_annot = image.copy()\nfor pose2D in person_conf_mu1ti:\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    fontsize = min(image_annot[:,:,0].shape)/1000\n```", "```py\npredictor_path = \"./shape_predictor_68_face_landmarks.dat\"\ndetector = dlib.get_fronta1_face_detector()\npredictor = dlib.shape_predictor(predictor_path)\n\n#Uncomment Line below if you want to use your webcam\n#cap = cv2.VideoCapture(0) #0 is the first camera on your computer, change if you\n#more than one camera\n\n#Comment out the Line below if using webcam\ncap = cv2.VideoCapture('./rollerc.mp4')\nfigure(100)\nfont = cv2.FONT_HERSHEY_SIMPLEX\n```", "```py\nfont = cv2.FONT_HERSHEY_SIMPLEX\nwhile(True):\n    #Capture frame-by-frame\n    ret, img = cap.read()\n    img.flags['WRITEABLE']=True #just in case\n\n    try:\n        dets = detector(img, 1)\n        shape = predictor(img, dets[0])\n    except:\n        print('no face detected', end='\\r')\n        cap.release()\n        break\n#similar to previous example, except frame-by-frame here\n    annotated=img.copy()\n    head_width = shape.part(16).x-shape.part(6).x\n    fontsize = head_width/650\n    for pt in range(68):\n        x,y = shape.part(pt).x, shape.part(pt).y\n        annotated=cv2.putText(annotated, str(pt), (x,y), font, fontsize, (255,255,255), 2, cv2.LINE_AA)\n\n#Let's see our results\n    fig=imshow(cv2.cvtColor(annotated,cv2.COLOR_BGR2RGB)) #OpenCV uses BGR format\n\n    display.c1ear_output(wait=True)\n    display.display(gcf())\n\n#When everything is done, release the capture\ncap.release()\n```"]