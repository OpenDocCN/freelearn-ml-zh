<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 9. NLP in Scala"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. NLP in Scala</h1></div></div></div><p>This chapter describes a few common techniques of <span class="strong"><strong>Natural Language Processing</strong></span> (<span class="strong"><strong>NLP</strong></span>), specifically, the ones that can benefit from Scala. There are some NLP packages in the open <a id="id659" class="indexterm"/>source out there. The most famous of them is probably <a id="id660" class="indexterm"/>NLTK (<a class="ulink" href="http://www.nltk.org">http://www.nltk.org</a>), which is written in Python, and ostensibly even a larger number of proprietary software solutions emphasizing<a id="id661" class="indexterm"/> different aspects of NLP. It is worth mentioning Wolf (<a class="ulink" href="https://github.com/wolfe-pack">https://github.com/wolfe-pack</a>), FACTORIE (<a class="ulink" href="http://factorie.cs.umass.edu">http://factorie.cs.umass.edu</a>), and ScalaNLP (<a class="ulink" href="http://www.scalanlp.org">http://www.scalanlp.org</a>), and skymind (<a class="ulink" href="http://www.skymind.io">http://www.skymind.io</a>), which is partly proprietary. However, few open source <a id="id662" class="indexterm"/>projects<a id="id663" class="indexterm"/> in this area remain<a id="id664" class="indexterm"/> active for a long period of time for one or another reason. Most projects are being eclipsed by Spark and MLlib capabilities, particularly, in the scalability aspect.</p><p>Instead of giving a detailed description of each of the NLP projects, which also might include speech-to-text, text-to-speech, and language translators, I will provide a few basic techniques focused on leveraging Spark MLlib in this chapter. The chapter comes very naturally as the last analytics chapter in this book. Scala is a very natural-language looking computer language and this chapter will leverage the techniques I developed earlier.</p><p>NLP arguably is the core of AI. Originally, the AI was created to mimic the humans, and natural language parsing and understanding is an indispensable part of it. Big data techniques has started to penetrate NLP, even though traditionally, NLP is very computationally intensive and is regarded as a small data problem. NLP often requires extensive deep learning techniques, and the volume of data of all written texts appears to be not so large compared to the logs volumes generated by all the machines today and analyzed by the big data machinery.</p><p>Even though the Library of Congress counts millions of documents, most of them can be digitized in PBs of actual digital data, a volume that any social websites is able to collect, store, and analyze within a few seconds. Complete works of most prolific authors can be stored within a few MBs of files (refer to <span class="emphasis"><em>Table 09-1</em></span>). Nonetheless, the social network and ADTECH companies parse text from millions of users and in hundreds of contexts every day.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>The complete works of</p>
</th><th style="text-align: left" valign="bottom">
<p>When lived</p>
</th><th style="text-align: left" valign="bottom">
<p>Size</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>Plato</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>428/427 (or 424/423) - 348/347 BC</p>
</td><td style="text-align: left" valign="top">
<p>2.1 MB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>William Shakespeare</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>26 April 1564 (baptized) - 23 April 1616</p>
</td><td style="text-align: left" valign="top">
<p>3.8 MB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>Fyodor Dostoevsky</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>11 November 1821 - 9 February 1881</p>
</td><td style="text-align: left" valign="top">
<p>5.9 MB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>Leo Tolstoy</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>9 September 1828 - 20 November 1910</p>
</td><td style="text-align: left" valign="top">
<p>6.9 MB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>Mark Twain</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>November 30, 1835 - April 21, 1910</p>
</td><td style="text-align: left" valign="top">
<p>13 MB</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p>Table 09-1. Complete Works collections of some famous writers (most can be acquired on Amazon.com today for a few dollars, later authors, although readily digitized, are more expensive)</p></blockquote></div><p>The natural language is a dynamic concept that changes over time, technology, and generations. We saw the appearance of emoticons, three-letter abbreviations, and so on. Foreign languages tend to borrow from each other; describing this dynamic ecosystem is a challenge on itself.</p><p>As in the previous chapters, I will focus on how to use Scala as a tool to orchestrate the language analysis rather than rewriting the tools in Scala. As the topic is so large, I will not claim to cover all aspects of NLP here.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Discussing NLP with the example of text processing pipeline and stages</li><li class="listitem" style="list-style-type: disc">Learning techniques for simple text analysis in terms of bags</li><li class="listitem" style="list-style-type: disc">Learning about <span class="strong"><strong>Term Frequency Inverse Document Frequency</strong></span> (<span class="strong"><strong>TF-IDF</strong></span>) technique<a id="id665" class="indexterm"/> that goes beyond simple bag analysis and de facto the standard in <a id="id666" class="indexterm"/><span class="strong"><strong>Information Retrieval</strong></span> (<span class="strong"><strong>IR</strong></span>)</li><li class="listitem" style="list-style-type: disc">Learning<a id="id667" class="indexterm"/> about document clustering with the example of the <span class="strong"><strong>Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>) approach</li><li class="listitem" style="list-style-type: disc">Performing semantic analysis using word2vec n-gram-based algorithms</li></ul></div><div class="section" title="Text analysis pipeline"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec64"/>Text analysis pipeline</h1></div></div></div><p>Before we <a id="id668" class="indexterm"/>proceed to detailed algorithms, let's look at a generic text-processing pipeline depicted in <span class="emphasis"><em>Figure 9-1</em></span>. In text analysis, the input is usually presented as a stream of characters (depending on the specific language).</p><p>Lexical analysis has to do with breaking this stream into a sequence of words (or lexemes in linguistic analysis). Often <a id="id669" class="indexterm"/>it is also called tokenization (and the words called the tokens). <span class="strong"><strong>ANother Tool for Language Recognition</strong></span> (<span class="strong"><strong>ANTLR</strong></span>) (<a class="ulink" href="http://www.antlr.org/">http://www.antlr.org/</a>) and Flex (<a class="ulink" href="http://flex.sourceforge.net">http://flex.sourceforge.net</a>) are probably the most<a id="id670" class="indexterm"/> famous in the open source community. One of the <a id="id671" class="indexterm"/>classical examples of ambiguity is lexical ambiguity. For example, in the phrase <span class="emphasis"><em>I saw a bat.</em></span> <span class="emphasis"><em>bat</em></span> can mean either an animal or a <a id="id672" class="indexterm"/>baseball bat. We usually need context to figure this out, which we will discuss next:</p><div class="mediaobject"><img src="Images/B04935_09_01.jpg" alt="Text analysis pipeline" width="559" height="733"/><div class="caption"><p>Figure 9-1. Typical stages of an NLP process.</p></div></div><p>Syntactic analysis, or parsing, traditionally deals with matching the structure of the text with grammar rules. This is relatively more important for computer languages that do not allow any ambiguity. In natural languages, this process is usually called chunking and tagging. In many cases, the meaning of the word in human language can be subject to context, intonation, or even body language or facial expression. The value of such analysis, as opposed to the big data approach, where the volume of data trumps complexity is still a contentious topic—one example of the latter is the word2vec approach, which will be described later.</p><p>Semantic<a id="id673" class="indexterm"/> analysis is the process of extracting language-independent meaning from the syntactic structures. As much as possible, it also involves removing features specific to particular cultural and linguistic contexts, to the extent that such a project is possible. The sources of ambiguity at this stage are: phrase attachment, conjunction, noun group structure, semantic ambiguity, anaphoric non-literal speech, and so on. Again, word2vec partially deals with these issues.</p><p>Disclosure integration partially deals with the issue of the context: the meaning of a sentence or an idiom can depend on the sentences or paragraphs before that. Syntactic analysis and cultural background play an important role here.</p><p>Finally, pragmatic analysis is yet another layer of complexity trying to reinterpret what is said in terms of what the intention was. How does this change the state of the world? Is it actionable?</p><div class="section" title="Simple text analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec51"/>Simple text analysis</h2></div></div></div><p>The straightforward representation of the document is a bag of words. Scala, and Spark, provides an <a id="id674" class="indexterm"/>excellent paradigm to perform analysis on the word distributions. First, we read the whole collection of texts, and then count the unique words:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ bin/spark-shell </strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  ''_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.</strong></span>
<span class="strong"><strong>Spark context available as sc.</strong></span>
<span class="strong"><strong>SQL context available as sqlContext.</strong></span>

<span class="strong"><strong>scala&gt; val leotolstoy = sc.textFile("leotolstoy").cache</strong></span>
<span class="strong"><strong>leotolstoy: org.apache.spark.rdd.RDD[String] = leotolstoy MapPartitionsRDD[1] at textFile at &lt;console&gt;:27</strong></span>

<span class="strong"><strong>scala&gt; leotolstoy.flatMap(_.split("\\W+")).count</strong></span>
<span class="strong"><strong>res1: Long = 1318234 </strong></span>

<span class="strong"><strong>scala&gt; val shakespeare = sc.textFile("shakespeare").cache</strong></span>
<span class="strong"><strong>shakespeare: org.apache.spark.rdd.RDD[String] = shakespeare MapPartitionsRDD[7] at textFile at &lt;console&gt;:27</strong></span>

<span class="strong"><strong>scala&gt; shakespeare.flatMap(_.split("\\W+")).count</strong></span>
<span class="strong"><strong>res2: Long = 1051958</strong></span>
</pre></div><p>This gives <a id="id675" class="indexterm"/>us just an estimate of the number of distinct words in the repertoire of quite different authors. The simplest way to find intersection between the two corpuses is to find the common vocabulary (which will be quite different as <span class="emphasis"><em>Leo Tolstoy</em></span> wrote in Russian and French, while <span class="emphasis"><em>Shakespeare</em></span> was an English-writing author):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; :silent</strong></span>

<span class="strong"><strong>scala&gt; val shakespeareBag = shakespeare.flatMap(_.split("\\W+")).map(_.toLowerCase).distinct</strong></span>

<span class="strong"><strong>scala&gt; val leotolstoyBag = leotolstoy.flatMap(_.split("\\W+")).map(_.toLowerCase).distinct</strong></span>
<span class="strong"><strong>leotolstoyBag: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at map at &lt;console&gt;:29</strong></span>

<span class="strong"><strong>scala&gt; println("The bags intersection is " + leotolstoyBag.intersection(shakespeareBag).count)</strong></span>
<span class="strong"><strong>The bags intersection is 11552</strong></span>
</pre></div><p>A few thousands word indices are manageable with the current implementations. For any new story, we can determine whether it is more likely to be written by Leo Tolstoy or <span class="emphasis"><em>William Shakespeare</em></span>. Let's take a look at <span class="emphasis"><em>The King James Version of the Bible</em></span>, which also can be <a id="id676" class="indexterm"/>downloaded from Project Gutenberg (<a class="ulink" href="https://www.gutenberg.org/files/10/10-h/10-h.htm">https://www.gutenberg.org/files/10/10-h/10-h.htm</a>):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ (mkdir bible; cd bible; wget http://www.gutenberg.org/cache/epub/10/pg10.txt)</strong></span>

<span class="strong"><strong>scala&gt; val bible = sc.textFile("bible").cache</strong></span>

<span class="strong"><strong>scala&gt; val bibleBag = bible.flatMap(_.split("\\W+")).map(_.toLowerCase).distinct</strong></span>

<span class="strong"><strong>scala&gt;:silent</strong></span>

<span class="strong"><strong>scala&gt; bibleBag.intersection(shakespeareBag).count</strong></span>
<span class="strong"><strong>res5: Long = 7250</strong></span>

<span class="strong"><strong>scala&gt; bibleBag.intersection(leotolstoyBag).count</strong></span>
<span class="strong"><strong>res24: Long = 6611</strong></span>
</pre></div><p>This seems<a id="id677" class="indexterm"/> reasonable as the religious language was popular during the Shakespearean time. On the other hand, plays by <span class="emphasis"><em>Anton Chekhov</em></span> have a larger intersection with the <span class="emphasis"><em>Leo Tolstoy</em></span> vocabulary:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ (mkdir chekhov; cd chekhov;</strong></span>
<span class="strong"><strong> wget http://www.gutenberg.org/cache/epub/7986/pg7986.txt</strong></span>
<span class="strong"><strong> wget http://www.gutenberg.org/cache/epub/1756/pg1756.txt</strong></span>
<span class="strong"><strong> wget http://www.gutenberg.org/cache/epub/1754/1754.txt</strong></span>
<span class="strong"><strong> wget http://www.gutenberg.org/cache/epub/13415/pg13415.txt)</strong></span>

<span class="strong"><strong>scala&gt; val chekhov = sc.textFile("chekhov").cache</strong></span>
<span class="strong"><strong>chekhov: org.apache.spark.rdd.RDD[String] = chekhov MapPartitionsRDD[61] at textFile at &lt;console&gt;:27</strong></span>

<span class="strong"><strong>scala&gt; val chekhovBag = chekhov.flatMap(_.split("\\W+")).map(_.toLowerCase).distinct</strong></span>
<span class="strong"><strong>chekhovBag: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[66] at distinct at &lt;console&gt;:29</strong></span>

<span class="strong"><strong>scala&gt; chekhovBag.intersection(leotolstoyBag).count</strong></span>
<span class="strong"><strong>res8: Long = 8263</strong></span>

<span class="strong"><strong>scala&gt; chekhovBag.intersection(shakespeareBag).count</strong></span>
<span class="strong"><strong>res9: Long = 6457 </strong></span>
</pre></div><p>This is a very simple approach that works, but there are a number of commonly known improvements we can make. First, a common technique is to stem the words. In many languages, words have a common part, often called root, and a changeable prefix or suffix, which may depend on the context, gender, time, and so on. Stemming is the process of improving the distinct count and intersection by approximating this flexible word form to the root, base, or a stem form in general. The stem form does not need to be identical to the morphological root of the word, it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid grammatical root. Secondly, we probably should account for the frequency of the words—while we will describe more elaborate methods in the next section, for the purpose of this exercise, we'll exclude the words with very high count, that usually are present in any document such as articles and possessive pronouns, which are usually called stop words, and the words with very low count. Specifically, I'll use the<a id="id678" class="indexterm"/> optimized <span class="strong"><strong>Porter Stemmer</strong></span> implementation that I described in more detail at the end of the chapter.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>The <a class="ulink" href="http://tartarus.org/martin/PorterStemmer/">http://tartarus.org/martin/PorterStemmer/</a> site contains some of the <a id="id679" class="indexterm"/>Porter Stemmer implementations in Scala and other languages, including a highly optimized ANSI C, which may be more efficient, but here I will provide another optimized Scala version that can be used immediately with Spark.</p></div></div><p>The<a id="id680" class="indexterm"/> Stemmer example will stem the words and count the relative intersections between them, removing the stop words:</p><div class="informalexample"><pre class="programlisting">def main(args: Array[String]) {

    val stemmer = new Stemmer

    val conf = new SparkConf().
      setAppName("Stemmer").
      setMaster(args(0))

    val sc = new SparkContext(conf)

    val stopwords = scala.collection.immutable.TreeSet(
      "", "i", "a", "an", "and", "are", "as", "at", "be", "but", "by", "for", "from", "had", "has", "he", "her", "him", "his", "in", "is", "it", "its", "my", "not", "of", "on", "she", "that", "the", "to", "was", "were", "will", "with", "you"
    ) map { stemmer.stem(_) }

    val bags = for (name &lt;- args.slice(1, args.length)) yield {
      val rdd = sc.textFile(name).map(_.toLowerCase)
      if (name == "nytimes" || name == "nips" || name == "enron")
        rdd.filter(!_.startsWith("zzz_")).flatMap(_.split("_")).map(stemmer.stem(_)).distinct.filter(!stopwords.contains(_)).cache
      else {
        val withCounts = rdd.flatMap(_.split("\\W+")).map(stemmer.stem(_)).filter(!stopwords.contains(_)).map((_, 1)).reduceByKey(_+_)
        val minCount = scala.math.max(1L, 0.0001 * withCounts.count.toLong)
        withCounts.filter(_._2 &gt; minCount).map(_._1).cache
      }
    }

    val cntRoots = (0 until { args.length - 1 }).map(i =&gt; Math.sqrt(bags(i).count.toDouble))

    for(l &lt;- 0 until { args.length - 1 }; r &lt;- l until { args.length - 1 }) {
      val cnt = bags(l).intersection(bags(r)).count
      println("The intersect " + args(l+1) + " x " + args(r+1) + " is: " + cnt + " (" + (cnt.toDouble / cntRoots(l) / cntRoots(r)) + ")")
    }

    sc.stop
    }
}</pre></div><p>When<a id="id681" class="indexterm"/> one runs the main class example from the command line, it outputs the stemmed bag sizes and intersection for datasets specified as parameters (these are directories in the home filesystem with documents):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ sbt "run-main org.akozlov.examples.Stemmer local[2] shakespeare leotolstoy chekhov nytimes nips enron bible"</strong></span>
<span class="strong"><strong>[info] Loading project definition from /Users/akozlov/Src/Book/ml-in-scala/chapter09/project</strong></span>
<span class="strong"><strong>[info] Set current project to NLP in Scala (in build file:/Users/akozlov/Src/Book/ml-in-scala/chapter09/)</strong></span>
<span class="strong"><strong>[info] Running org.akozlov.examples.Stemmer local[2] shakespeare leotolstoy chekhov nytimes nips enron bible</strong></span>
<span class="strong"><strong>The intersect shakespeare x shakespeare is: 10533 (1.0)</strong></span>
<span class="strong"><strong>The intersect shakespeare x leotolstoy is: 5834 (0.5293670391596142)</strong></span>
<span class="strong"><strong>The intersect shakespeare x chekhov is: 3295 (0.4715281914492153)</strong></span>
<span class="strong"><strong>The intersect shakespeare x nytimes is: 7207 (0.4163369701270161)</strong></span>
<span class="strong"><strong>The intersect shakespeare x nips is: 2726 (0.27457329089479504)</strong></span>
<span class="strong"><strong>The intersect shakespeare x enron is: 5217 (0.34431535832271265)</strong></span>
<span class="strong"><strong>The intersect shakespeare x bible is: 3826 (0.45171392986714726)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x leotolstoy is: 11531 (0.9999999999999999)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x chekhov is: 4099 (0.5606253333241973)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x nytimes is: 8657 (0.47796976891152176)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x nips is: 3231 (0.3110369262979765)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x enron is: 6076 (0.38326210407266764)</strong></span>
<span class="strong"><strong>The intersect leotolstoy x bible is: 3455 (0.3898604013063757)</strong></span>
<span class="strong"><strong>The intersect chekhov x chekhov is: 4636 (1.0)</strong></span>
<span class="strong"><strong>The intersect chekhov x nytimes is: 3843 (0.33463022711780555)</strong></span>
<span class="strong"><strong>The intersect chekhov x nips is: 1889 (0.28679311682962116)</strong></span>
<span class="strong"><strong>The intersect chekhov x enron is: 3213 (0.31963226496874225)</strong></span>
<span class="strong"><strong>The intersect chekhov x bible is: 2282 (0.40610513998395287)</strong></span>
<span class="strong"><strong>The intersect nytimes x nytimes is: 28449 (1.0)</strong></span>
<span class="strong"><strong>The intersect nytimes x nips is: 4954 (0.30362042173997206)</strong></span>
<span class="strong"><strong>The intersect nytimes x enron is: 11273 (0.45270741164576034)</strong></span>
<span class="strong"><strong>The intersect nytimes x bible is: 3655 (0.2625720159205085)</strong></span>
<span class="strong"><strong>The intersect nips x nips is: 9358 (1.0000000000000002)</strong></span>
<span class="strong"><strong>The intersect nips x enron is: 4888 (0.3422561629856124)</strong></span>
<span class="strong"><strong>The intersect nips x bible is: 1615 (0.20229053645165143)</strong></span>
<span class="strong"><strong>The intersect enron x enron is: 21796 (1.0)</strong></span>
<span class="strong"><strong>The intersect enron x bible is: 2895 (0.23760453654690084)</strong></span>
<span class="strong"><strong>The intersect bible x bible is: 6811 (1.0)</strong></span>
<span class="strong"><strong>[success] Total time: 12 s, completed May 17, 2016 11:00:38 PM</strong></span>
</pre></div><p>This, in <a id="id682" class="indexterm"/>this case, just confirms the hypothesis that Bible's vocabulary is closer to <span class="emphasis"><em>William Shakespeare</em></span> than to Leo Tolstoy and other sources. Interestingly, modern vocabularies of <span class="emphasis"><em>NY Times</em></span> articles and Enron's e-mails from the previous chapters are much closer to <span class="emphasis"><em>Leo Tolstoy's</em></span>, which is probably more an indication of the translation quality.</p><p>Another thing to notice is that the pretty complex analysis took about <span class="emphasis"><em>40</em></span> lines of Scala code (not counting the libraries, specifically the Porter Stemmer, which is about ~ <span class="emphasis"><em>100</em></span> lines) and about 12 seconds. The power of Scala is that it can leverage other libraries very efficiently to write concise code.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>
<span class="strong"><strong>Serialization</strong></span>
</p><p>We already<a id="id683" class="indexterm"/> talked about serialization in <a class="link" href="ch06.xhtml" title="Chapter 6. Working with Unstructured Data">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span>. As Spark's tasks are executed in different threads and potentially JVMs, Spark does a lot of serialization/deserialization when passing the objects. Potentially, I could use <code class="literal">map { val stemmer = new Stemmer; stemmer.stem(_) }</code> instead of <code class="literal">map { stemmer.stem(_) }</code>, but the latter reuses the object for multiple iterations and seems to be linguistically more appealing. One suggested performance optimization is to use <span class="emphasis"><em>Kryo serializer</em></span>, which is less flexible than the Java serializer, but more performant. However, for integrative purpose, it is much easier to just make every object in the pipeline serializable and use default Java serialization.</p></div></div><p>As another example, let's compute the distribution of word frequencies, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val bags = for (name &lt;- List("shakespeare", "leotolstoy", "chekhov", "nytimes", "enron", "bible")) yield {</strong></span>
<span class="strong"><strong>     |     sc textFile(name) flatMap { _.split("\\W+") } map { _.toLowerCase } map { stemmer.stem(_) } filter { ! stopwords.contains(_) } cache()</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>bags: List[org.apache.spark.rdd.RDD[String]] = List(MapPartitionsRDD[93] at filter at &lt;console&gt;:36, MapPartitionsRDD[98] at filter at &lt;console&gt;:36, MapPartitionsRDD[103] at filter at &lt;console&gt;:36, MapPartitionsRDD[108] at filter at &lt;console&gt;:36, MapPartitionsRDD[113] at filter at &lt;console&gt;:36, MapPartitionsRDD[118] at filter at &lt;console&gt;:36)</strong></span>

<span class="strong"><strong>scala&gt; bags reduceLeft { (a, b) =&gt; a.union(b) } map { (_, 1) } reduceByKey { _+_ } collect() sortBy(- _._2) map { x =&gt; scala.math.log(x._2) }</strong></span>
<span class="strong"><strong>res18: Array[Double] = Array(10.27759958298627, 10.1152465449837, 10.058652004037477, 10.046635061754612, 9.999615579630348, 9.855399641729074, 9.834405391348684, 9.801233318497372, 9.792667717430884, 9.76347807952779, 9.742496866444002, 9.655474810542554, 9.630365631415676, 9.623244409181346, 9.593355351246755, 9.517604459155686, 9.515837804297965, 9.47231994707559, 9.45930760329985, 9.441531454869693, 9.435561763085358, 9.426257878198653, 9.378985497953893, 9.355997944398545, 9.34862295977619, 9.300820725104558, 9.25569607369698, 9.25320827220336, 9.229162126216771, 9.20391980417326, 9.19917830726999, 9.167224080902555, 9.153875834995056, 9.137877200242468, 9.129889247578555, 9.090430075303626, 9.090091799380007, 9.083075020930307, 9.077722847361343, 9.070273383079064, 9.0542711863262...</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>The<a id="id684" class="indexterm"/> distribution of relative frequencies on the log-log scale is presented in the following diagram. With the exception of the first few tokens, the dependency of frequency on rank is almost linear:</p><div class="mediaobject"><img src="Images/B04935_09_02.jpg" alt="Simple text analysis" width="900" height="537"/><div class="caption"><p>Figure 9-2. A typical distribution of word relative frequencies on log-log scale (Zipf's Law)</p></div></div></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="MLlib algorithms in Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec65"/>MLlib algorithms in Spark</h1></div></div></div><p>Let's halt at <a id="id685" class="indexterm"/>MLlib that complements other NLP libraries written in Scala. MLlib is primarily important because of scalability, and thus supports a few of the data preparation and text processing algorithms, particularly in the area of <a id="id686" class="indexterm"/>feature construction (<a class="ulink" href="http://spark.apache.org/docs/latest/ml-features.html">http://spark.apache.org/docs/latest/ml-features.html</a>).</p><div class="section" title="TF-IDF"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec52"/>TF-IDF</h2></div></div></div><p>Although <a id="id687" class="indexterm"/>the preceding analysis can already give a powerful insight, the piece of information<a id="id688" class="indexterm"/> that is missing from the analysis is term frequency information. The term frequencies are relatively more important in information retrieval, where the collection of documents need to be searched and ranked in relation to a few terms. The top documents are usually returned to the user.</p><p>TF-IDF is a standard technique where term frequencies are offset by the frequencies of the terms in the corpus. Spark has an implementation of the TF-IDF. Spark uses a hash function to identify the terms. This approach avoids the need to compute a global term-to-index map, but can be subject to potential hash collisions, the probability of which is determined by the number of buckets of the hash table. The default feature dimension is <span class="emphasis"><em>2^20=1,048,576</em></span>.</p><p>In the Spark implementation, each document is a line in the dataset. We can convert it into to an RDD of iterables and compute the hashing by the following code:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; import org.apache.spark.mllib.feature.HashingTF</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.feature.HashingTF</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vector</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vector</strong></span>

<span class="strong"><strong>scala&gt; val hashingTF = new HashingTF</strong></span>
<span class="strong"><strong>hashingTF: org.apache.spark.mllib.feature.HashingTF = org.apache.spark.mllib.feature.HashingTF@61b975f7</strong></span>

<span class="strong"><strong>scala&gt; val documents: RDD[Seq[String]] = sc.textFile("shakepeare").map(_.split("\\W+").toSeq)</strong></span>
<span class="strong"><strong>documents: org.apache.spark.rdd.RDD[Seq[String]] = MapPartitionsRDD[263] at map at &lt;console&gt;:34</strong></span>

<span class="strong"><strong>scala&gt; val tf = hashingTF transform documents</strong></span>
<span class="strong"><strong>tf: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[264] at map at HashingTF.scala:76</strong></span>
</pre></div><p>When computing <code class="literal">hashingTF</code>, we only need a single pass over the data, applying IDF needs two passes: first to compute the IDF vector and second to scale the term frequencies <a id="id689" class="indexterm"/>by<a id="id690" class="indexterm"/> IDF:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; tf.cache</strong></span>
<span class="strong"><strong>res26: tf.type = MapPartitionsRDD[268] at map at HashingTF.scala:76</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.feature.IDF</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.feature.IDF</strong></span>

<span class="strong"><strong>scala&gt; val idf = new IDF(minDocFreq = 2) fit tf</strong></span>
<span class="strong"><strong>idf: org.apache.spark.mllib.feature.IDFModel = org.apache.spark.mllib.feature.IDFModel@514bda2d</strong></span>

<span class="strong"><strong>scala&gt; val tfidf = idf transform tf</strong></span>
<span class="strong"><strong>tfidf: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[272] at mapPartitions at IDF.scala:178</strong></span>

<span class="strong"><strong>scala&gt; tfidf take(10) foreach println</strong></span>
<span class="strong"><strong>(1048576,[3159,3543,84049,582393,787662,838279,928610,961626,1021219,1021273],[3.9626355004005083,4.556357737874695,8.380602528651274,8.157736974683708,11.513471982269106,9.316247404932888,10.666174121881904,11.513471982269106,8.07948477778396,11.002646358503116])</strong></span>
<span class="strong"><strong>(1048576,[267794,1021219],[8.783442874448122,8.07948477778396])</strong></span>
<span class="strong"><strong>(1048576,[0],[0.5688129477150906])</strong></span>
<span class="strong"><strong>(1048576,[3123,3370,3521,3543,96727,101577,114801,116103,497275,504006,508606,843002,962509,980206],[4.207164322003765,2.9674322162952897,4.125144122691999,2.2781788689373474,2.132236195047438,3.2951341639027754,1.9204575904855747,6.318664992090735,11.002646358503116,3.1043838099579815,5.451238364272918,11.002646358503116,8.43769700104158,10.30949917794317])</strong></span>
<span class="strong"><strong>(1048576,[0,3371,3521,3555,27409,89087,104545,107877,552624,735790,910062,943655,962421],[0.5688129477150906,3.442878442319589,4.125144122691999,4.462482535201062,5.023254392629403,5.160262034409286,5.646060083831103,4.712188947797486,11.002646358503116,7.006282204641219,6.216822672821767,11.513471982269106,8.898512204232908])</strong></span>
<span class="strong"><strong>(1048576,[3371,3543,82108,114801,149895,279256,582393,597025,838279,915181],[3.442878442319589,2.2781788689373474,6.017670811187438,3.8409151809711495,7.893585399642122,6.625632265652778,8.157736974683708,10.414859693600997,9.316247404932888,11.513471982269106])</strong></span>
<span class="strong"><strong>(1048576,[3123,3555,413342,504006,690950,702035,980206],[4.207164322003765,4.462482535201062,3.4399651117812313,3.1043838099579815,11.513471982269106,11.002646358503116,10.30949917794317])</strong></span>
<span class="strong"><strong>(1048576,[0],[0.5688129477150906])</strong></span>
<span class="strong"><strong>(1048576,[97,1344,3370,100898,105489,508606,582393,736902,838279,1026302],[2.533299776544098,23.026943964538212,2.9674322162952897,0.0,11.225789909817326,5.451238364272918,8.157736974683708,10.30949917794317,9.316247404932888,11.513471982269106])</strong></span>
<span class="strong"><strong>(1048576,[0,1344,3365,114801,327690,357319,413342,692611,867249,965170],[4.550503581720725,23.026943964538212,2.7455719545259836,1.9204575904855747,8.268278849083533,9.521041817578901,3.4399651117812313,0.0,6.661441718349489,0.0])</strong></span>
</pre></div><p>Here<a id="id691" class="indexterm"/> we see<a id="id692" class="indexterm"/> each document represented by a set of terms and their scores.</p></div><div class="section" title="LDA"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec53"/>LDA</h2></div></div></div><p>LDA in <a id="id693" class="indexterm"/>Spark MLlib is a clustering <a id="id694" class="indexterm"/>mechanism, where the feature vectors represent the counts of words in a document. The model maximizes the probability of observing the word counts, given the assumption that each document is a mixture of topics<a id="id695" class="indexterm"/> and the words in the documents are generated based on <span class="strong"><strong>Dirichlet distribution</strong></span> (a generalization of beta distribution on multinomial case) for each of the topic independently. The goal is to derive the (latent) distribution of the topics and the parameters of the words generation statistical model.</p><p>The MLlib<a id="id696" class="indexterm"/> implementation is based on 2009 LDA paper (<a class="ulink" href="http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf">http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf</a>) and uses GraphX to implement a distributed <span class="strong"><strong>Expectation Maximization</strong></span> (<span class="strong"><strong>EM</strong></span>) algorithm for assigning<a id="id697" class="indexterm"/> topics to the documents.</p><p>Let's take the Enron e-mail corpus discussed in <a class="link" href="ch07.xhtml" title="Chapter 7. Working with Graph Algorithms">Chapter 7</a>, <span class="emphasis"><em>Working with Graph Algorithms</em></span>, where we tried to figure out communications graph. For e-mail clustering, we need to extract the body of the e-mail and place is as a single line in the training file:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ mkdir enron</strong></span>
<span class="strong"><strong>$ cat /dev/null &gt; enron/all.txt</strong></span>
<span class="strong"><strong>$ for f in $(find maildir -name \*\. -print); do cat $f | sed '1,/^$/d;/^$/d' | tr "\n\r" "  " &gt;&gt; enron/all.txt; echo "" &gt;&gt; enron/all.txt; done</strong></span>
<span class="strong"><strong>$</strong></span>
</pre></div><p>Now, let's use Scala/Spark to construct a corpus dataset containing the document ID, followed by a dense array of word counts in the bag:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ spark-shell --driver-memory 8g --executor-memory 8g --packages com.github.fommil.netlib:all:1.1.2</strong></span>
<span class="strong"><strong>Ivy Default Cache set to: /home/alex/.ivy2/cache</strong></span>
<span class="strong"><strong>The jars for the packages stored in: /home/alex/.ivy2/jars</strong></span>
<span class="strong"><strong>:: loading settings :: url = jar:file:/opt/cloudera/parcels/CDH-5.5.2-1.cdh5.5.2.p0.4/jars/spark-assembly-1.5.0-cdh5.5.2-hadoop2.6.0-cdh5.5.2.jar!/org/apache/ivy/core/settings/ivysettings.xml</strong></span>
<span class="strong"><strong>com.github.fommil.netlib#all added as a dependency</strong></span>
<span class="strong"><strong>:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0</strong></span>
<span class="strong"><strong>  confs: [default]</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#all;1.1.2 in central</strong></span>
<span class="strong"><strong>  found net.sourceforge.f2j#arpack_combined_all;0.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#core;1.1.2 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#native_ref-java;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil#jniloader;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-win-i686;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#native_system-java;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-linux-i686;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1 in central</strong></span>
<span class="strong"><strong>  found com.github.fommil.netlib#netlib-native_system-win-i686;1.1 in central</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1-javadoc.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] net.sourceforge.f2j#arpack_combined_all;0.1!arpack_combined_all.jar (513ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#core;1.1.2!core.jar (18ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-osx-x86_64/1.1/netlib-native_ref-osx-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1!netlib-native_ref-osx-x86_64.jar (167ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-linux-x86_64/1.1/netlib-native_ref-linux-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1!netlib-native_ref-linux-x86_64.jar (159ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-linux-i686/1.1/netlib-native_ref-linux-i686-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1!netlib-native_ref-linux-i686.jar (131ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-win-x86_64/1.1/netlib-native_ref-win-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1!netlib-native_ref-win-x86_64.jar (210ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-win-i686/1.1/netlib-native_ref-win-i686-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-win-i686;1.1!netlib-native_ref-win-i686.jar (167ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-linux-armhf/1.1/netlib-native_ref-linux-armhf-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1!netlib-native_ref-linux-armhf.jar (110ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-osx-x86_64/1.1/netlib-native_system-osx-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1!netlib-native_system-osx-x86_64.jar (54ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-linux-x86_64/1.1/netlib-native_system-linux-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1!netlib-native_system-linux-x86_64.jar (47ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-linux-i686/1.1/netlib-native_system-linux-i686-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-linux-i686;1.1!netlib-native_system-linux-i686.jar (44ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-linux-armhf/1.1/netlib-native_system-linux-armhf-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>[SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1!netlib-native_system-linux-armhf.jar (35ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-win-x86_64/1.1/netlib-native_system-win-x86_64-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1!netlib-native_system-win-x86_64.jar (62ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-win-i686/1.1/netlib-native_system-win-i686-1.1-natives.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-win-i686;1.1!netlib-native_system-win-i686.jar (55ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/native_ref-java/1.1/native_ref-java-1.1.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#native_ref-java;1.1!native_ref-java.jar (24ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/jniloader/1.1/jniloader-1.1.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil#jniloader;1.1!jniloader.jar (3ms)</strong></span>
<span class="strong"><strong>downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/native_system-java/1.1/native_system-java-1.1.jar ...</strong></span>
<span class="strong"><strong>  [SUCCESSFUL ] com.github.fommil.netlib#native_system-java;1.1!native_system-java.jar (7ms)</strong></span>
<span class="strong"><strong>:: resolution report :: resolve 3366ms :: artifacts dl 1821ms</strong></span>
<span class="strong"><strong>  :: modules in use:</strong></span>
<span class="strong"><strong>  com.github.fommil#jniloader;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#all;1.1.2 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#core;1.1.2 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#native_ref-java;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#native_system-java;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-win-i686;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-linux-i686;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-win-i686;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1 from central in [default]</strong></span>
<span class="strong"><strong>  net.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]</strong></span>
<span class="strong"><strong>  :: evicted modules:</strong></span>
<span class="strong"><strong>  com.github.fommil.netlib#core;1.1 by [com.github.fommil.netlib#core;1.1.2] in [default]</strong></span>
<span class="strong"><strong>  --------------------------------------------------------------------</strong></span>
<span class="strong"><strong>  |                  |            modules            ||   artifacts   |</strong></span>
<span class="strong"><strong>  |       conf       | number| search|dwnlded|evicted|| number|dwnlded|</strong></span>
<span class="strong"><strong>  ---------------------------------------------------------------------</strong></span>
<span class="strong"><strong>  |      default     |   19  |   18  |   18  |   1   ||   17  |   17  |</strong></span>
<span class="strong"><strong>  ---------------------------------------------------------------------</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>scala&gt; val enron = sc textFile("enron")</strong></span>
<span class="strong"><strong>enron: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at &lt;console&gt;:21</strong></span>

<span class="strong"><strong>scala&gt; enron.flatMap(_.split("\\W+")).map(_.toLowerCase).distinct.count</strong></span>
<span class="strong"><strong>res0: Long = 529199                                                             </strong></span>

<span class="strong"><strong>scala&gt; val stopwords = scala.collection.immutable.TreeSet ("", "i", "a", "an", "and", "are", "as", "at", "be", "but", "by", "for", "from", "had", "has", "he", "her", "him", "his", "in", "is", "it", "its", "not", "of", "on", "she", "that", "the", "to", "was", "were", "will", "with", "you")</strong></span>
<span class="strong"><strong>stopwords: scala.collection.immutable.TreeSet[String] = TreeSet(, a, an, and, are, as, at, be, but, by, for, from, had, has, he, her, him, his, i, in, is, it, its, not, of, on, she, that, the, to, was, were, will, with, you)</strong></span>
<span class="strong"><strong>scala&gt; </strong></span>

<span class="strong"><strong>scala&gt; val terms = enron.flatMap(x =&gt; if (x.length &lt; 8192) x.toLowerCase.split("\\W+") else Nil).filterNot(stopwords).map(_,1).reduceByKey(_+_).collect.sortBy(- _._2).slice(0, 1000).map(_._1)</strong></span>
<span class="strong"><strong>terms: Array[String] = Array(enron, ect, com, this, hou, we, s, have, subject, or, 2001, if, your, pm, am, please, cc, 2000, e, any, me, 00, message, 1, corp, would, can, 10, our, all, sent, 2, mail, 11, re, thanks, original, know, 12, 713, http, may, t, do, 3, time, 01, ees, m, new, my, they, no, up, information, energy, us, gas, so, get, 5, about, there, need, what, call, out, 4, let, power, should, na, which, one, 02, also, been, www, other, 30, email, more, john, like, these, 03, mark, 04, attached, d, enron_development, their, see, 05, j, forwarded, market, some, agreement, 09, day, questions, meeting, 08, when, houston, doc, contact, company, 6, just, jeff, only, who, 8, fax, how, deal, could, 20, business, use, them, date, price, 06, week, here, net, 15, 9, 07, group, california,...</strong></span>
<span class="strong"><strong>scala&gt; def getBagCounts(bag: Seq[String]) = { for(term &lt;- terms) yield { bag.count(_==term) } }</strong></span>
<span class="strong"><strong>getBagCounts: (bag: Seq[String])Array[Int]</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>

<span class="strong"><strong>scala&gt; val corpus = enron.map(x =&gt; { if (x.length &lt; 8192) Some(x.toLowerCase.split("\\W+").toSeq) else None } ).map(x =&gt; { Vectors.dense(getBagCounts(x.getOrElse(Nil)).map(_.toDouble).toArray) }).zipWithIndex.map(_.swap).cache</strong></span>
<span class="strong"><strong>corpus: org.apache.spark.rdd.RDD[(Long, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[14] at map at &lt;console&gt;:30</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.clustering.{LDA, DistributedLDAModel}</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.clustering.{LDA, DistributedLDAModel}</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.linalg.Vectors</strong></span>

<span class="strong"><strong>scala&gt; val ldaModel = new LDA().setK(10).run(corpus)</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>scala&gt; ldaModel.topicsMatrix.transpose</strong></span>
<span class="strong"><strong>res2: org.apache.spark.mllib.linalg.Matrix = </strong></span>
<span class="strong"><strong>207683.78495933366  79745.88417942637   92118.63972404732   ... (1000 total)</strong></span>
<span class="strong"><strong>35853.48027575886   4725.178508682296   111214.8860582083   ...</strong></span>
<span class="strong"><strong>135755.75666585402  54736.471356209106  93289.65563593085   ...</strong></span>
<span class="strong"><strong>39445.796099155996  6272.534431534215   34764.02707696523   ...</strong></span>
<span class="strong"><strong>329786.21570967307  602782.9591026317   42212.22143362559   ...</strong></span>
<span class="strong"><strong>62235.09960154089   12191.826543794878  59343.24100019015   ...</strong></span>
<span class="strong"><strong>210049.59592560542  160538.9650732507   40034.69756641789   ...</strong></span>
<span class="strong"><strong>53818.14660186875   6351.853448001488   125354.26708575874  ...</strong></span>
<span class="strong"><strong>44133.150537842856  4342.697652158682   154382.95646078113  ...</strong></span>
<span class="strong"><strong>90072.97362336674   21132.629704311104  93683.40795807641   ...</strong></span>
</pre></div><p>We can<a id="id698" class="indexterm"/> also list the words and their<a id="id699" class="indexterm"/> relative importance for the topic in the descending order:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; ldaModel.describeTopics foreach { x : (Array[Int], Array[Double]) =&gt; { print(x._1.slice(0,10).map(terms(_)).mkString(":")); print("-&gt; "); print(x._2.slice(0,10).map(_.toFloat).mkString(":")); println } }</strong></span>
<span class="strong"><strong>com:this:ect:or:if:s:hou:2001:00:we-&gt;0.054606363:0.024220783:0.02096761:0.013669214:0.0132700335:0.012969772:0.012623918:0.011363528:0.010114557:0.009587474</strong></span>
<span class="strong"><strong>s:this:hou:your:2001:or:please:am:com:new-&gt;0.029883621:0.027119286:0.013396418:0.012856948:0.01218803:0.01124849:0.010425644:0.009812181:0.008742722:0.0070441025</strong></span>
<span class="strong"><strong>com:this:s:ect:hou:or:2001:if:your:am-&gt;0.035424445:0.024343235:0.015182628:0.014283071:0.013619815:0.012251413:0.012221165:0.011411696:0.010284024:0.009559739</strong></span>
<span class="strong"><strong>would:pm:cc:3:thanks:e:my:all:there:11-&gt;0.047611523:0.034175437:0.022914853:0.019933242:0.017208714:0.015393614:0.015366959:0.01393391:0.012577525:0.011743208</strong></span>
<span class="strong"><strong>ect:com:we:can:they:03:if:also:00:this-&gt;0.13815293:0.0755843:0.065043546:0.015290086:0.0121941045:0.011561104:0.011326733:0.010967959:0.010653805:0.009674695</strong></span>
<span class="strong"><strong>com:this:s:hou:or:2001:pm:your:if:cc-&gt;0.016605735:0.015834121:0.01289918:0.012708308:0.0125788655:0.011726159:0.011477625:0.010578845:0.010555539:0.009609056</strong></span>
<span class="strong"><strong>com:ect:we:if:they:hou:s:00:2001:or-&gt;0.05537054:0.04231919:0.023271963:0.012856676:0.012689817:0.012186356:0.011350313:0.010887237:0.010778923:0.010662295</strong></span>
<span class="strong"><strong>this:s:hou:com:your:2001:or:please:am:if-&gt;0.030830953:0.016557815:0.014236835:0.013236604:0.013107091:0.0126846135:0.012257128:0.010862533:0.01027849:0.008893094</strong></span>
<span class="strong"><strong>this:s:or:pm:com:your:please:new:hou:2001-&gt;0.03981197:0.013273305:0.012872894:0.011672661:0.011380969:0.010689667:0.009650983:0.009605533:0.009535899:0.009165275</strong></span>
<span class="strong"><strong>this:com:hou:s:or:2001:if:your:am:please-&gt;0.024562683:0.02361607:0.013770585:0.013601272:0.01269994:0.012360005:0.011348433:0.010228578:0.009619628:0.009347991</strong></span>
</pre></div><p>To find out the top documents per topic or top topics per document, we need to convert this model to <code class="literal">DistributedLDA</code> or <code class="literal">LocalLDAModel</code>, which extend <code class="literal">LDAModel</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; ldaModel.save(sc, "ldamodel")</strong></span>

<span class="strong"><strong>scala&gt; val sameModel = DistributedLDAModel.load(sc, "ldamode2l")</strong></span>

<span class="strong"><strong>scala&gt; sameModel.topDocumentsPerTopic(10) foreach { x : (Array[Long], Array[Double]) =&gt; { print(x._1.mkString(":")); print("-&gt; "); print(x._2.map(_.toFloat).mkString(":")); println } }</strong></span>
<span class="strong"><strong>59784:50745:52479:60441:58399:49202:64836:52490:67936:67938-&gt; 0.97146696:0.9713364:0.9661418:0.9661132:0.95249915:0.9519995:0.94945914:0.94944507:0.8977366:0.8791358</strong></span>
<span class="strong"><strong>233009:233844:233007:235307:233842:235306:235302:235293:233020:233857-&gt; 0.9962034:0.9962034:0.9962034:0.9962034:0.9962034:0.99620336:0.9954057:0.9954057:0.9954057:0.9954057</strong></span>
<span class="strong"><strong>14909:115602:14776:39025:115522:288507:4499:38955:15754:200876-&gt; 0.83963907:0.83415157:0.8319566:0.8303818:0.8291597:0.8281472:0.82739806:0.8272517:0.82579833:0.8243338</strong></span>
<span class="strong"><strong>237004:71818:124587:278308:278764:278950:233672:234490:126637:123664-&gt; 0.99929106:0.9968135:0.9964454:0.99644524:0.996445:0.99644494:0.99644476:0.9964447:0.99644464:0.99644417</strong></span>
<span class="strong"><strong>156466:82237:82252:82242:341376:82501:341367:340197:82212:82243-&gt; 0.99716955:0.94635135:0.9431836:0.94241136:0.9421047:0.9410431:0.94075173:0.9406304:0.9402021:0.94014835</strong></span>
<span class="strong"><strong>335708:336413:334075:419613:417327:418484:334157:335795:337573:334160-&gt; 0.987011:0.98687994:0.9865438:0.96953565:0.96953565:0.96953565:0.9588571:0.95852506:0.95832515:0.9581657</strong></span>
<span class="strong"><strong>243971:244119:228538:226696:224833:207609:144009:209548:143066:195299-&gt; 0.7546907:0.7546907:0.59146744:0.59095955:0.59090924:0.45532238:0.45064417:0.44945204:0.4487876:0.44833568</strong></span>
<span class="strong"><strong>242260:214359:126325:234126:123362:233304:235006:124195:107996:334829-&gt; 0.89615464:0.8961442:0.8106028:0.8106027:0.8106023:0.8106023:0.8106021:0.8106019:0.76834095:0.7570231</strong></span>
<span class="strong"><strong>209751:195546:201477:191758:211002:202325:197542:193691:199705:329052-&gt; 0.913124:0.9130985:0.9130918:0.9130672:0.5525752:0.5524637:0.5524494:0.552405:0.55240136:0.5026157</strong></span>
<span class="strong"><strong>153326:407544:407682:408098:157881:351230:343651:127848:98884:129351-&gt; 0.97206575:0.97206575:0.97206575:0.97206575:0.97206575:0.9689198:0.968068:0.9659192:0.9657442:0.96553063</strong></span>
</pre></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Segmentation, annotation, and chunking"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec66"/>Segmentation, annotation, and chunking</h1></div></div></div><p>When the<a id="id700" class="indexterm"/> text is presented in digital form, it is relatively easy to find words as<a id="id701" class="indexterm"/> we can split the stream on non-word characters. This <a id="id702" class="indexterm"/>becomes more complex in spoken language analysis. In this case, segmenters try to optimize a metric, for example, to minimize the number of distinct words in the lexicon and the length or complexity of the phrase (<span class="emphasis"><em>Natural Language Processing with Python</em></span> by <span class="emphasis"><em>Steven Bird et al</em></span>, <span class="emphasis"><em>O'Reilly Media Inc</em></span>, 2009).</p><p>Annotation usually refers to parts-of-speech tagging. In English, these are nouns, pronouns, verbs, adjectives, adverbs, articles, prepositions, conjunctions, and interjections. For example, in the phrase <span class="emphasis"><em>we saw the yellow dog</em></span>, <span class="emphasis"><em>we</em></span> is a pronoun, <span class="emphasis"><em>saw</em></span> is a verb, <span class="emphasis"><em>the</em></span> is an article, <span class="emphasis"><em>yellow</em></span> is an adjective, and <span class="emphasis"><em>dog</em></span> is a noun.</p><p>In some languages, the chunking and annotation depends on context. For example, in Chinese, <span class="emphasis"><em>爱江山人</em></span> literally translates to <span class="emphasis"><em>love country person</em></span> and can mean either <span class="emphasis"><em>country-loving person</em></span> or <span class="emphasis"><em>love country-person</em></span>. In Russian, <span class="emphasis"><em>казнить нельзя помиловать</em></span>, literally translating to <span class="emphasis"><em>execute not pardon</em></span>, can mean <span class="emphasis"><em>execute, don't pardon</em></span>, or <span class="emphasis"><em>don't execute, pardon</em></span>. While in written language, this can be disambiguated using commas, in a spoken language this is usually it is very hard to recognize the difference, even though sometimes the intonation can help to segment the phrase properly.</p><p>For techniques based on word frequencies in the bags, some extremely common words, which are of little value in helping select documents, are explicitly excluded from the vocabulary. These words are called stop words. There is no good general strategy for determining a stop list, but in many cases, this is to exclude very frequent words that appear in almost every document and do not help to differentiate between them for classification or information retrieval purposes.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="POS tagging"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec67"/>POS tagging</h1></div></div></div><p>POS tagging probabilistically annotates each word with it's grammatical function—noun, verb, adjective, and so on. Usually, POS tagging serves as an input to syntactic and <a id="id703" class="indexterm"/>semantic analysis. Let's demonstrate POS tagging<a id="id704" class="indexterm"/> on the FACTORIE toolkit example, a software library written in Scala (<a class="ulink" href="http://factorie.cs.umass.edu">http://factorie.cs.umass.edu</a>). To start, you need to <a id="id705" class="indexterm"/>download the binary image or source files from <a class="ulink" href="https://github.com/factorie/factorie.git">https://github.com/factorie/factorie.git</a> and build it:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ git clone https://github.com/factorie/factorie.git</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ cd factorie</strong></span>
<span class="strong"><strong>$ git checkout factorie_2.11-1.2</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>$ mvn package -Pnlp-jar-with-dependencies</strong></span>
</pre></div><p>After the build, which also includes model training, the following command will start a network server on <code class="literal">port 3228</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ $ bin/fac nlp --wsj-forward-pos --conll-chain-ner</strong></span>
<span class="strong"><strong>java -Xmx6g -ea -Djava.awt.headless=true -Dfile.encoding=UTF-8 -server -classpath ./src/main/resources:./target/classes:./target/factorie_2.11-1.2-nlp-jar-with-dependencies.jar</strong></span>
<span class="strong"><strong>found model</strong></span>
<span class="strong"><strong>18232</strong></span>
<span class="strong"><strong>Listening on port 3228</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Now, all<a id="id706" class="indexterm"/> traffic to <code class="literal">port 3228</code> will be interpreted (as text), and the output will be tokenized and annotated:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ telnet localhost 3228</strong></span>
<span class="strong"><strong>Trying ::1...</strong></span>
<span class="strong"><strong>Connected to localhost.</strong></span>
<span class="strong"><strong>Escape character is '^]'.</strong></span>
<span class="strong"><strong>But I warn you, if you don't tell me that this means war, if you still try to defend the infamies and horrors perpetrated by that Antichrist--I really believe he is Antichrist--I will have nothing more to do with you and you are no longer my friend, no longer my 'faithful slave,' as you call yourself! But how do you do? I see I have frightened you--sit down and tell me all the news.</strong></span>

<span class="strong"><strong>1  1  But  CC  O</strong></span>
<span class="strong"><strong>2  2  I    PRP  O</strong></span>
<span class="strong"><strong>3  3  warn    VBP  O</strong></span>
<span class="strong"><strong>4  4  you    PRP  O</strong></span>
<span class="strong"><strong>5  5  ,      O</strong></span>
<span class="strong"><strong>6  6  if    IN  O</strong></span>
<span class="strong"><strong>7  7  you    PRP  O</strong></span>
<span class="strong"><strong>8  8  do    VBP  O</strong></span>
<span class="strong"><strong>9  9  n't    RB  O</strong></span>
<span class="strong"><strong>10  10  tell    VB  O</strong></span>
<span class="strong"><strong>11  11  me    PRP  O</strong></span>
<span class="strong"><strong>12  12  that    IN  O</strong></span>
<span class="strong"><strong>13  13  this    DT  O</strong></span>
<span class="strong"><strong>14  14  means    VBZ  O</strong></span>
<span class="strong"><strong>15  15  war    NN  O</strong></span>
<span class="strong"><strong>16  16  ,    ,  O</strong></span>
<span class="strong"><strong>17  17  if    IN  O</strong></span>
<span class="strong"><strong>18  18  you  PRP  O</strong></span>
<span class="strong"><strong>19  19  still    RB  O</strong></span>
<span class="strong"><strong>20  20  try    VBP  O</strong></span>
<span class="strong"><strong>21  21  to    TO  O</strong></span>
<span class="strong"><strong>22  22  defend    VB  O</strong></span>
<span class="strong"><strong>23  23  the    DT  O</strong></span>
<span class="strong"><strong>24  24  infamies    NNS  O</strong></span>
<span class="strong"><strong>25  25  and    CC  O</strong></span>
<span class="strong"><strong>26  26  horrors    NNS  O</strong></span>
<span class="strong"><strong>27  27  perpetrated    VBN  O</strong></span>
<span class="strong"><strong>28  28  by    IN  O</strong></span>
<span class="strong"><strong>29  29  that    DT  O</strong></span>
<span class="strong"><strong>30  30  Antichrist    NNP  O</strong></span>
<span class="strong"><strong>31  31  --    :  O</strong></span>
<span class="strong"><strong>32  1  I  PRP  O</strong></span>
<span class="strong"><strong>33  2  really    RB  O</strong></span>
<span class="strong"><strong>34  3  believe    VBP  O</strong></span>
<span class="strong"><strong>35  4  he    PRP  O</strong></span>
<span class="strong"><strong>36  5  is    VBZ  O</strong></span>
<span class="strong"><strong>37  6  Antichrist    NNP  U-MISC</strong></span>
<span class="strong"><strong>38  7  --    :  O</strong></span>
<span class="strong"><strong>39  1  I    PRP  O</strong></span>
<span class="strong"><strong>40  2  will    MD  O</strong></span>
<span class="strong"><strong>41  3  have    VB  O</strong></span>
<span class="strong"><strong>42  4  nothing    NN  O</strong></span>
<span class="strong"><strong>43  5  more    JJR  O</strong></span>
<span class="strong"><strong>44  6  to    TO  O</strong></span>
<span class="strong"><strong>45  7  do    VB  O</strong></span>
<span class="strong"><strong>46  8  with    IN  O</strong></span>
<span class="strong"><strong>47  9  you    PRP  O</strong></span>
<span class="strong"><strong>48  10  and    CC  O</strong></span>
<span class="strong"><strong>49  11  you    PRP  O</strong></span>
<span class="strong"><strong>50  12  are    VBP  O</strong></span>
<span class="strong"><strong>51  13  no    RB  O</strong></span>
<span class="strong"><strong>52  14  longer    RBR  O</strong></span>
<span class="strong"><strong>53  15  my    PRP$  O</strong></span>
<span class="strong"><strong>54  16  friend    NN  O</strong></span>
<span class="strong"><strong>55  17  ,    ,  O</strong></span>
<span class="strong"><strong>56  18  no    RB  O</strong></span>
<span class="strong"><strong>57  19  longer    RB  O</strong></span>
<span class="strong"><strong>58  20  my  PRP$  O</strong></span>
<span class="strong"><strong>59  21  '    POS  O</strong></span>
<span class="strong"><strong>60  22  faithful    NN  O</strong></span>
<span class="strong"><strong>61  23  slave    NN  O</strong></span>
<span class="strong"><strong>62  24  ,    ,  O</strong></span>
<span class="strong"><strong>63  25  '    ''  O</strong></span>
<span class="strong"><strong>64  26  as    IN  O</strong></span>
<span class="strong"><strong>65  27  you    PRP  O</strong></span>
<span class="strong"><strong>66  28  call    VBP  O</strong></span>
<span class="strong"><strong>67  29  yourself    PRP  O</strong></span>
<span class="strong"><strong>68  30  !    .  O</strong></span>
<span class="strong"><strong>69  1  But    CC  O</strong></span>
<span class="strong"><strong>70  2  how    WRB  O</strong></span>
<span class="strong"><strong>71  3  do    VBP  O</strong></span>
<span class="strong"><strong>72  4  you    PRP  O</strong></span>
<span class="strong"><strong>73  5  do    VB  O</strong></span>
<span class="strong"><strong>74  6  ?    .  O</strong></span>
<span class="strong"><strong>75  1  I    PRP  O</strong></span>
<span class="strong"><strong>76  2  see    VBP  O</strong></span>
<span class="strong"><strong>77  3  I    PRP  O</strong></span>
<span class="strong"><strong>78  4  have    VBP  O</strong></span>
<span class="strong"><strong>79  5  frightened    VBN  O</strong></span>
<span class="strong"><strong>80  6  you    PRP  O</strong></span>
<span class="strong"><strong>81  7  --    :  O</strong></span>
<span class="strong"><strong>82  8  sit    VB  O</strong></span>
<span class="strong"><strong>83  9  down    RB  O</strong></span>
<span class="strong"><strong>84  10  and    CC  O</strong></span>
<span class="strong"><strong>85  11  tell    VB  O</strong></span>
<span class="strong"><strong>86  12  me    PRP  O</strong></span>
<span class="strong"><strong>87  13  all    DT  O</strong></span>
<span class="strong"><strong>88  14  the    DT  O</strong></span>
<span class="strong"><strong>89  15  news    NN  O</strong></span>
<span class="strong"><strong>90  16  .    .  O</strong></span>
</pre></div><p>This POS is<a id="id707" class="indexterm"/> a single-path left-right tagger that can process the text as a stream. Internally, the algorithm uses probabilistic techniques to find the most probable assignment. Let's also look at other techniques that do not use grammatical analysis and yet proved to be very useful for language understanding and interpretation.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Using word2vec to find word relationships"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec68"/>Using word2vec to find word relationships</h1></div></div></div><p>Word2vec has <a id="id708" class="indexterm"/>been developed by Tomas Mikolov at Google, around 2012. The original idea behind word2vec was to demonstrate that one might improve efficiency by trading the model's complexity for efficiency. Instead of representing a document as bags of words, word2vec takes each word context into account by trying to analyze n-grams or skip-grams (a set of surrounding tokens with potential the token in question skipped). The words and word contexts themselves are represented by an array of floats/doubles <span class="inlinemediaobject"><img src="Images/B04935_09_01F.jpg" alt="Using word2vec to find word relationships" width="20" height="30"/></span>. The objective function is to maximize log likelihood:</p><div class="mediaobject"><img src="Images/B04935_09_02F.jpg" alt="Using word2vec to find word relationships" width="198" height="58"/></div><p>Where:</p><div class="mediaobject"><img src="Images/B04935_09_03F.jpg" alt="Using word2vec to find word relationships" width="217" height="70"/></div><p>By choosing the optimal <span class="inlinemediaobject"><img src="Images/B04935_09_01F.jpg" alt="Using word2vec to find word relationships" width="20" height="30"/></span> and <a id="id709" class="indexterm"/>to get a comprehensive word representation (also called <span class="strong"><strong>map optimization</strong></span>). Similar words are found based on cosine similarity<a id="id710" class="indexterm"/> metric (dot product) of <span class="inlinemediaobject"><img src="Images/B04935_09_01F.jpg" alt="Using word2vec to find word relationships" width="20" height="30"/></span>. Spark implementation uses hierarchical softmax, which reduces the complexity of computing the conditional probability to <span class="inlinemediaobject"><img src="Images/B04935_09_04F.jpg" alt="Using word2vec to find word relationships" width="95" height="33"/></span>, or log of the vocabulary size <span class="emphasis"><em>V</em></span>, as opposed to <span class="inlinemediaobject"><img src="Images/B04935_09_05F.jpg" alt="Using word2vec to find word relationships" width="52" height="33"/></span>, or proportional to <span class="emphasis"><em>V</em></span>. The training is still linear in the dataset size, but is amenable to big data parallelization techniques.</p><p>
<code class="literal">Word2vec</code> is traditionally used to predict the most likely word given context or find similar words with a similar meaning (synonyms). The following code trains in <code class="literal">word2vec</code> model on <span class="emphasis"><em>Leo Tolstoy's Wars and Peace</em></span>, and finds synonyms for the word <span class="emphasis"><em>circle</em></span>. I had to convert the Gutenberg's representation of <span class="emphasis"><em>War and Peace</em></span> to a single-line format by running the <code class="literal">cat 2600.txt | tr "\n\r" "  " &gt; warandpeace.txt</code> command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val word2vec = new Word2Vec</strong></span>
<span class="strong"><strong>word2vec: org.apache.spark.mllib.feature.Word2Vec = org.apache.spark.mllib.feature.Word2Vec@58bb4dd</strong></span>

<span class="strong"><strong>scala&gt; val model = word2vec.fit(sc.textFile("warandpeace").map(_.split("\\W+").toSeq)</strong></span>
<span class="strong"><strong>model: org.apache.spark.mllib.feature.Word2VecModel = org.apache.spark.mllib.feature.Word2VecModel@6f61b9d7</strong></span>

<span class="strong"><strong>scala&gt; val synonyms = model.findSynonyms("life", 10)</strong></span>
<span class="strong"><strong>synonyms: Array[(String, Double)] = Array((freedom,1.704344822168997), (universal,1.682276637692245), (conception,1.6776193389148586), (relation,1.6760497906519414), (humanity,1.67601036253831), (consists,1.6637604144872544), (recognition,1.6526169382380496), (subjection,1.6496559771230317), (activity,1.646671198014248), (astronomy,1.6444424059160712))</strong></span>

<span class="strong"><strong>scala&gt; synonyms foreach println</strong></span>
<span class="strong"><strong>(freedom,1.704344822168997)</strong></span>
<span class="strong"><strong>(universal,1.682276637692245)</strong></span>
<span class="strong"><strong>(conception,1.6776193389148586)</strong></span>
<span class="strong"><strong>(relation,1.6760497906519414)</strong></span>
<span class="strong"><strong>(humanity,1.67601036253831)</strong></span>
<span class="strong"><strong>(consists,1.6637604144872544)</strong></span>
<span class="strong"><strong>(recognition,1.6526169382380496)</strong></span>
<span class="strong"><strong>(subjection,1.6496559771230317)</strong></span>
<span class="strong"><strong>(activity,1.646671198014248)</strong></span>
<span class="strong"><strong>(astronomy,1.6444424059160712)</strong></span>
</pre></div><p>While in <a id="id711" class="indexterm"/>general, it is hard to some with an objective function, and <code class="literal">freedom</code> is not listed as a synonym to <code class="literal">life</code> in the English Thesaurus, the results do make sense.</p><p>Each word in the word2vec model is represented as an array of doubles. Another interesting application is to find associations <span class="emphasis"><em>a to b is the same as c to ?</em></span> by performing subtraction <span class="emphasis"><em>vector(a) - vector(b) + vector(c)</em></span>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>scala&gt; val a = model.getVectors.filter(_._1 == "monarchs").map(_._2).head</strong></span>
<span class="strong"><strong>a: Array[Float] = Array(-0.0044642715, -0.0013227836, -0.011506443, 0.03691717, 0.020431392, 0.013427449, -0.0036369907, -0.013460356, -3.8938568E-4, 0.02432113, 0.014533845, 0.004130258, 0.00671316, -0.009344602, 0.006229065, -0.005442078, -0.0045390734, -0.0038824948, -6.5973646E-4, 0.021729799, -0.011289608, -0.0030690092, -0.011423801, 0.009100784, 0.011765533, 0.0069619063, 0.017540144, 0.011198071, 0.026103685, -0.017285397, 0.0045515243, -0.0044477824, -0.0074411617, -0.023975836, 0.011371289, -0.022625357, -2.6478301E-5, -0.010510282, 0.010622139, -0.009597833, 0.014937023, -0.01298345, 0.0016747514, 0.01172987, -0.001512275, 0.022340108, -0.009758578, -0.014942565, 0.0040697413, 0.0015349758, 0.010246878, 0.0021413323, 0.008739062, 0.007845526, 0.006857361, 0.01160148, 0.008595...</strong></span>
<span class="strong"><strong>scala&gt; val b = model.getVectors.filter(_._1 == "princess").map(_._2).head</strong></span>
<span class="strong"><strong>b: Array[Float] = Array(0.13265875, -0.04882792, -0.08409957, -0.04067986, 0.009084379, 0.121674284, -0.11963971, 0.06699862, -0.20277102, 0.26296946, -0.058114383, 0.076021515, 0.06751665, -0.17419271, -0.089830205, 0.2463593, 0.062816426, -0.10538805, 0.062085453, -0.2483566, 0.03468293, 0.20642486, 0.3129267, -0.12418643, -0.12557726, 0.06725172, -0.03703333, -0.10810595, 0.06692443, -0.046484336, 0.2433963, -0.12762263, -0.18473054, -0.084376186, 0.0037174677, -0.0040220995, -0.3419341, -0.25928706, -0.054454487, 0.09521076, -0.041567303, -0.13727514, -0.04826158, 0.13326299, 0.16228828, 0.08495835, -0.18073058, -0.018380836, -0.15691829, 0.056539804, 0.13673553, -0.027935665, 0.081865616, 0.07029694, -0.041142456, 0.041359138, -0.2304657, -0.17088272, -0.14424285, -0.0030700471, -0...</strong></span>
<span class="strong"><strong>scala&gt; val c = model.getVectors.filter(_._1 == "individual").map(_._2).head</strong></span>
<span class="strong"><strong>c: Array[Float] = Array(-0.0013353615, -0.01820516, 0.007949033, 0.05430816, -0.029520465, -0.030641818, -6.607431E-4, 0.026548808, 0.04784935, -0.006470232, 0.041406438, 0.06599842, 0.0074243015, 0.041538745, 0.0030222891, -0.003932073, -0.03154199, -0.028486902, 0.022139633, 0.05738223, -0.03890591, -0.06761177, 0.0055152955, -0.02480924, -0.053222697, -0.028698998, -0.005315235, 0.0582403, -0.0024816995, 0.031634405, -0.027884213, 6.0290704E-4, 1.9750209E-4, -0.05563172, 0.023785716, -0.037577976, 0.04134448, 0.0026664822, -0.019832063, -0.0011898747, 0.03160933, 0.031184288, 0.0025268437, -0.02718441, -0.07729341, -0.009460656, 0.005344515, -0.05110715, 0.018468754, 0.008984449, -0.0053139487, 0.0053904117, -0.01322933, -0.015247412, 0.009819351, 0.038043085, 0.044905875, 0.00402788...</strong></span>
<span class="strong"><strong>scala&gt; model.findSynonyms(new DenseVector((for(i &lt;- 0 until 100) yield (a(i) - b(i) + c(i)).toDouble).toArray), 10) foreach println</strong></span>
<span class="strong"><strong>(achievement,0.9432423663884002)</strong></span>
<span class="strong"><strong>(uncertainty,0.9187759184842362)</strong></span>
<span class="strong"><strong>(leader,0.9163721499105207)</strong></span>
<span class="strong"><strong>(individual,0.9048367510621271)</strong></span>
<span class="strong"><strong>(instead,0.8992079672038455)</strong></span>
<span class="strong"><strong>(cannon,0.8947818781378154)</strong></span>
<span class="strong"><strong>(arguments,0.8883634101905679)</strong></span>
<span class="strong"><strong>(aims,0.8725107984356915)</strong></span>
<span class="strong"><strong>(ants,0.8593842583047755)</strong></span>
<span class="strong"><strong>(War,0.8530727227924755)</strong></span>
</pre></div><p>This<a id="id712" class="indexterm"/> can be used to find relationships in the language.</p><div class="section" title="A Porter Stemmer implementation of the code"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec54"/>A Porter Stemmer implementation of the code</h2></div></div></div><p>Porter Stemmer <a id="id713" class="indexterm"/>was first developed around the 1980s and there<a id="id714" class="indexterm"/> are many implementations. The detailed steps and original reference are provided<a id="id715" class="indexterm"/> at <a class="ulink" href="http://tartarus.org/martin/PorterStemmer/def.txt">http://tartarus.org/martin/PorterStemmer/def.txt</a>. It consists of roughly 6-9 steps of suffix/endings replacements, some of which are conditional on prefix or stem. I will provide a Scala-optimized version with the book code repository. For example, step 1 covers the majority of stemming cases and consists of 12 substitutions: the last 8 of which are <a id="id716" class="indexterm"/>conditional on the number of syllables<a id="id717" class="indexterm"/> and the presence of vowels in the stem:</p><div class="informalexample"><pre class="programlisting">  def step1(s: String) = {
    b = s
    // step 1a
    processSubList(List(("sses", "ss"), ("ies","i"),("ss","ss"), ("s", "")), _&gt;=0)
    // step 1b
    if (!(replacer("eed", "ee", _&gt;0)))
    {
      if ((vowelInStem("ed") &amp;&amp; replacer("ed", "", _&gt;=0)) || (vowelInStem("ing") &amp;&amp; replacer("ing", "", _&gt;=0)))
      {
        if (!processSubList(List(("at", "ate"), ("bl","ble"), ("iz","ize")), _&gt;=0 ) )
        {
          // if this isn't done, then it gets more confusing.
          if (doublec() &amp;&amp; b.last != 'l' &amp;&amp; b.last != 's' &amp;&amp; b.last != 'z') { b = b.substring(0, b.length - 1) }
          else
            if (calcM(b.length) == 1 &amp;&amp; cvc("")) { b = b + "e" }
        }
      }
    }
    // step 1c
    (vowelInStem("y") &amp;&amp; replacer("y", "i", _&gt;=0))
    this
  }</pre></div><p>The complete<a id="id718" class="indexterm"/> code is available at <a class="ulink" href="https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala">https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala</a>.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec69"/>Summary</h1></div></div></div><p>In this chapter, I described basic NLP concepts and demonstrated a few basic techniques. I hoped to demonstrate that pretty complex NLP concepts could be expressed and tested in a few lines of Scala code. This is definitely just the tip of the iceberg as a lot of NLP techniques are being developed now, including the ones based on in-CPU parallelization as part of GPUs. (refer to, for example, <span class="strong"><strong>Puck</strong></span> at <a class="ulink" href="https://github.com/dlwh/puck">https://github.com/dlwh/puck</a>). I also gave a flavor of major Spark MLlib NLP implementations.</p><p>In the next chapter, which will be the final chapter of this book, I'll cover systems and model monitoring.</p></div></div>



  </body></html>