- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Out Our Bronze Layer
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Data is a precious thing and will last longer than the systems themselves.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: – Tim Berners-Lee, generally credited as the inventor of the World Wide Web
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll embark on the beginning of your data journey in the
    Databricks platform, exploring the fundamentals of the Bronze layer. We recommend
    employing the Medallion design pattern within the lake house architecture (as
    described in [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073)) to organize your
    data. We’ll start with **Auto Loader**, which you can implement with or without
    **Delta Live Tables** (**DLT**) to insert and transform data in your architecture.
    The benefits of using Auto Loader include quickly transforming new data into the
    Delta format and enforcing or evolving schemas, which are essential for maintaining
    consistent data delivery to the business and customers. As a data scientist, strive
    for efficiency in building your data pipelines and ensuring your data is ready
    for the steps in the machine learning development cycle. You will best learn these
    topics through the example projects, so the *Applying our learning* section is
    the main focus of this chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what topics you will cover in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the Medallion architecture pattern
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming data to Delta with Auto Loader
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DLT, starting with Bronze
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintaining and optimizing Delta Tables
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying our learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting the Medallion architecture pattern
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced the Medallion architecture in [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073).
    As a reminder, this refers to the data design pattern used to organize data logically.
    It has three layers – Bronze, Silver, and Gold. There are also cases where additional
    levels of refinement are required, so your Medallion architecture could be extended
    to Diamond and Platinum levels if needed. The Bronze layer contains raw data,
    the Silver layer contains cleaned and transformed data, and the Gold layer contains
    aggregated and curated data. Curated data refers to the datasets selected, cleaned,
    and organized for a specific business or modeling purpose. This architecture is
    a good fit for data science projects. Maintaining the original data as a source
    of truth is important, while curated data is valuable for research, analytics,
    and machine learning applications. By selecting, cleaning, and organizing data
    for a specific purpose, curated data can help improve its accuracy, relevance,
    and usability.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: This book will introduce quite a few project tasks, such as *building curated
    datasets*, that often fall under the domain of a data engineer. There will also
    be tasks that machine learning engineers, data analysts, and so on commonly perform.
    We include all of this work in our examples because roles are blurred in today’s
    fast-moving world and will vary by company. Titles and expectations quickly evolve.
    Therefore, it’s imperative to have a handle on the entire end-to-end workflow.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining a Bronze layer allows us to go back to the original data when we
    want to create a different feature, solve a new problem that requires us to look
    at historical data from another point of view, or simply maintain the raw level
    of our data for governance purposes. As the world of technology evolves, it’s
    crucial to stay current and follow trends, but the core principles will remain
    for years. For the remainder of this chapter, we will cover DI platform features
    that facilitate building the Bronze layer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 维护一个Bronze层允许我们在需要创建不同的特征、解决需要从另一个角度查看历史数据的新问题，或者仅仅为了治理目的而保持数据的原始级别时，回到原始数据。随着技术世界的不断发展，保持最新并跟随趋势至关重要，但核心原则将保持数年。在本章的剩余部分，我们将介绍便于构建Bronze层的DI平台功能。
- en: Transforming data to Delta with Auto Loader
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Auto Loader将数据转换为Delta
- en: Harness the power of Auto Loader to automate your data ingestion process, significantly
    enhancing your data product’s workflow efficiency. It can ingest data from cloud
    storage and streaming data sources. You can configure Auto Loader to run on a
    schedule or be triggered manually.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 利用Auto Loader的力量来自动化您的数据摄取过程，显著提高您数据产品的工作流程效率。它可以从云存储和流数据源摄取数据。您可以配置Auto Loader按计划运行或手动触发。
- en: 'Here are some benefits of using Databricks’ Auto Loader:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Databricks的Auto Loader的一些好处：
- en: '**It keeps data up to date**: Auto Loader maintains checkpoints, removing the
    need to know which data is new. Auto Loader handles all that on its own.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它保持数据最新**：Auto Loader维护检查点，无需知道哪些数据是新的。Auto Loader会自行处理所有这些。'
- en: '**It improves data quality**: Auto Loader can automatically detect schema changes
    and rescue any new data columns, so you can be confident that your data is accurate.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它提高数据质量**：Auto Loader可以自动检测架构更改并恢复任何新的数据列，因此您可以确信您的数据是准确的。'
- en: '**It increases data agility**: Auto Loader can help you quickly and easily
    ingest new data sources so that you can be more agile in responding to changes
    in your business.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它增加数据敏捷性**：Auto Loader可以帮助您快速轻松地摄取新的数据源，从而使您在应对业务变化时更加敏捷。'
- en: '**Flexible ingestion**: Auto Loader can stream files in batches or continuously.
    This means it can consume batch data as a stream to reduce the overhead of a more
    manual batch pipeline.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的摄取**：Auto Loader可以批量或连续地流式传输文件。这意味着它可以以流的形式消耗批量数据，以减少更手动批量管道的开销。'
- en: Auto Loader is a powerful tool. It can be used standalone, as the underlying
    technology for DLT, or with **Spark Structured Streaming**. Spark Structured Streaming
    is a near-real-time processing engine; we will cover how to create a real-time
    feature in [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244). This chapter also
    covers an example of streaming ingestion in the streaming transactions project.
    Let’s discuss Auto Loader’s ability to evolve a schema over time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Auto Loader是一个强大的工具。它可以独立使用，作为DLT的底层技术，或者与**Spark Structured Streaming**一起使用。Spark
    Structured Streaming是一个接近实时处理引擎；我们将在[*第五章*](B16865_05.xhtml#_idTextAnchor244)中介绍如何创建实时功能。本章还涵盖了一个流式摄取的示例，在流交易项目中。让我们讨论Auto
    Loader随时间演进架构的能力。
- en: Schema evolution
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构演进
- en: Auto Loader’s schema evolution allows you to add or modify fields in streaming
    data seamlessly. Databricks automatically adjusts relevant data to fit the new
    schema while preserving existing data integrity. This automated schema handling
    makes it easy to evolve your data schema over time as your business needs and
    data sources change, without having to worry about data loss or downtime. Furthermore,
    schema handling reduces the amount of work and headache associated with managing
    data pipelines that could change unexpectedly.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Auto Loader的架构演进功能允许您无缝地在流数据中添加或修改字段。Databricks会自动调整相关数据以适应新架构，同时保持现有数据完整性。这种自动化的架构处理使得随着业务需求和数据源的变化，您可以在不担心数据丢失或停机的情况下轻松地随时间演进您的数据架构。
- en: The new schema includes an additional column in the *Applying our learning*
    section. We will show how no data is lost despite a schema changing without notice.
    In our case, the default schema evolution mode for Auto Loader, `.option("cloudFiles.schemaEvolutionMode",
    "addNewColumns")`, in combination with `.option("mergeSchema", "true")`, perform
    schema evolution. Auto Loader will handle changes in the schema for us. Auto Loader
    tracking the changes is beneficial when new data fields become available with
    or without prior notice; code changes are unnecessary.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 新的模式在 *应用我们的学习* 部分中增加了一个额外的列。我们将展示即使在模式未通知的情况下更改，也不会丢失任何数据。在我们的案例中，Auto Loader
    的默认模式演化模式 `.option("cloudFiles.schemaEvolutionMode", "addNewColumns")` 与 `.option("mergeSchema",
    "true")` 结合执行模式演化。Auto Loader 将为我们处理模式变化。当新数据字段可用时，无论是事先通知还是未通知，Auto Loader 跟踪变化是有益的；无需进行代码更改。
- en: There’s documentation on all schema options. The method we use for our project
    is the only option for automatic schema evolution. Every other option will require
    manual intervention. For example, you can use “rescue” mode to rescue data from
    being lost. Alternatively, you can use “failOnNewColumns” mode to cause the pipeline
    to fail and keep the schema unchanged until the production code is updated. There
    are numerous options and patterns for Auto Loader. Check out the *Common loading
    patterns with Auto Loader* link in the *Further reading* section for more information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模式选项都有文档说明。我们项目所采用的方法是唯一适用于自动模式演化的选项。其他任何选项都需要手动干预。例如，您可以使用“rescue”模式来挽救数据以避免丢失。或者，您可以使用“failOnNewColumns”模式来使管道失败并保持模式不变，直到生产代码更新。Auto
    Loader 有许多选项和模式。在“进一步阅读”部分的 *Common loading patterns with Auto Loader* 链接中查看更多信息。
- en: DLT, starting with Bronze
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从青铜开始使用 DLT
- en: As we touched upon in a previous chapter, remember that DLT actively simplifies
    your pipeline operations, empowering you to focus on setting clear objectives
    for your pipeline rather than getting bogged down in operational details. Building
    on this foundation, we will now delve into DLT’s capabilities. Auto Loader’s schema
    evolution is integrated with DLT, underscoring its utility in handling dynamic
    data schemas with minimal manual intervention.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前一章中提到的，请记住 DLT 主动简化了您的管道操作，使您能够专注于为您的管道设定明确的目标，而不是陷入操作细节。在此基础上，我们现在将深入研究
    DLT 的功能。Auto Loader 的模式演化与 DLT 集成，强调了它在处理需要最少手动干预的动态数据模式中的实用性。
- en: DLT benefits and features
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DLT 的优势和功能
- en: 'DLT is a sophisticated framework designed to construct reliable data pipelines.
    DLT automates and streamlines complex operations such as orchestration and cluster
    management, significantly boosting the efficiency of data workflows. All you need
    to do is specify your transformation logic to be up and running. We will focus
    on just a few of the benefits of DLT as they pertain to creating your Bronze data
    layer, but we’ve included a link to DLT documentation in the *Further reading*
    section at the end of this chapter as well. DLT is another great tool in your
    toolbox for ingesting data. It provides several benefits over traditional ETL
    pipelines, including the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: DLT 是一个复杂的框架，旨在构建可靠的数据管道。DLT 自动化和简化了复杂的操作，如编排和集群管理，显著提高了数据工作流的效率。您只需指定您的转换逻辑即可启动运行。我们将重点关注与创建青铜数据层相关的
    DLT 的几个优势，但我们也在本章末尾的“进一步阅读”部分中包含了一个 DLT 文档的链接。DLT 是您工具箱中用于摄取数据的另一个优秀工具。它提供了比传统
    ETL 管道更多的优势，包括以下内容：
- en: '**Declarative pipeline development**: DLT allows you to define your data pipelines
    using SQL or Python, which makes them easier to understand and maintain'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声明式管道开发**：DLT 允许您使用 SQL 或 Python 定义您的数据管道，这使得它们更容易理解和维护。'
- en: '**Automatic data quality testing**: DLT can automatically apply your tests
    to your data, preventing quality issues, which helps to ensure that your pipelines
    are producing accurate results'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动数据质量测试**：DLT 可以自动将您的测试应用到您的数据上，防止质量问题，这有助于确保您的管道产生准确的结果。'
- en: '**Deep visibility for monitoring and recovery**: DLT provides detailed tracking
    and logging information, making troubleshooting problems and recovering from failures
    easier'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度可见性用于监控和恢复**：DLT 提供详细的跟踪和日志信息，使解决问题和从故障中恢复变得更加容易。'
- en: '**Cost-effective streaming through efficient compute autoscaling**: DLT can
    automatically scale your compute resources up or down based on demand, which helps
    to reduce costs'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过高效的计算自动扩展实现成本效益的流式处理**：DLT可以根据需求自动扩展或缩减计算资源，这有助于降低成本'
- en: DLT is a powerful tool for building reliable and scalable data pipelines in
    batch or streaming. It can help you improve the quality, efficiency, and visibility
    of your data processing workflows.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DLT是构建可靠和可扩展的批量或流式数据管道的强大工具。它可以帮助您提高数据处理工作流程的质量、效率和可见性。
- en: 'Here are some of the specific features of DLT that provide these benefits:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DLT的一些具体特性，它们提供了这些优势：
- en: '**Streaming tables**: DLT uses streaming and live tables to process data in
    real time and to keep your pipelines up to date with the latest data.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式表**：DLT使用流式和实时表来实时处理数据，并确保您的管道与最新数据保持同步。'
- en: '**Materialized views**: DLT uses materialized views to create snapshots of
    your data. You can query your data in real time and use it for downstream processing.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物化视图**：DLT使用物化视图来创建数据的快照。您可以在实时查询数据的同时，将其用于下游处理。'
- en: '**Expectations**: DLT uses expectations to test your data for quality issues
    automatically and to take action if the data does not meet your expectations.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**期望**：DLT使用期望来自动测试数据的质量问题，并在数据不符合期望时采取行动。'
- en: '**Autoscaling**: DLT can automatically scale your compute resources up or down
    based on demand, reducing costs and improving the performance of your pipelines.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动扩展**：DLT可以根据需求自动扩展或缩减计算资源，从而降低成本并提高管道的性能。'
- en: DLT is a good way to build reliable, scalable, and testable data pipelines.
    Next, we will focus on DLT in the context of the Bronze layer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: DLT是构建可靠、可扩展和可测试的数据管道的好方法。接下来，我们将专注于青铜层背景下的DLT。
- en: Bronze data with DLT
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DLT中的青铜数据
- en: DLT uses Auto Loader to support schema evolution, offering significant benefits
    in terms of data quality, a key aspect of the Bronze layer in a Medallion architecture.
    In this architecture, the Bronze layer serves as the foundational stage where
    raw data is initially ingested and stored. DLT contributes to this layer by ensuring
    that each transformation applied to the raw data is precisely captured and managed.
    As a data scientist, understanding these transformations is crucial for maintaining
    the integrity of the data processing workflow. A powerful feature of DLT is its
    ability to automatically generate an accurate workflow Directed Acyclic Graph
    (**DAG**). This DAG not only visualizes the sequence and relationships of these
    data transformations but also enhances the reliability of the entire data pipeline.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: DLT使用Auto Loader来支持模式演变，在数据质量方面提供了显著的好处，这是Medallion架构中青铜层的关键方面。在这个架构中，青铜层作为原始数据最初摄入和存储的基础阶段。DLT通过确保对原始数据应用的每个转换都精确捕获和管理来贡献这个层。作为数据科学家，理解这些转换对于维护数据处理工作流程的完整性至关重要。DLT的一个强大功能是能够自动生成准确的流程有向无环图（**DAG**）。这个DAG不仅可视化这些数据转换的顺序和关系，还增强了整个数据管道的可靠性。
- en: 'The following screenshot shows the DLT pipeline workflow:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图展示了DLT管道工作流程：
- en: '![Figure 3.1 – The DLT pipeline workflow for the transactional dataset in this
    chapter](img/B16865_03_01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 本章交易数据集的DLT管道工作流程](img/B16865_03_01.jpg)'
- en: Figure 3.1 – The DLT pipeline workflow for the transactional dataset in this
    chapter
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 本章交易数据集的DLT管道工作流程
- en: In *Figure 3**.1*, there is one task in the DLT workflow, but it can accommodate
    a complex workflow with multiple dependencies.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3*.1中，DLT工作流程中有一个任务，但它可以容纳具有多个依赖关系的复杂工作流程。
- en: Once we get data landed in Delta, there are actions we can take to ensure we
    take full advantage of Delta’s benefits.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据成功导入Delta，我们可以采取一些措施来确保充分利用Delta的优势。
- en: Maintaining and optimizing Delta tables
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维护和优化Delta表
- en: While the primary focus of this book is not on the intricate details of Delta
    table optimization, understanding these techniques is crucial for developing a
    data-centric machine learning solution. Efficient management of Delta tables directly
    impacts the performance and reliability of ML models, as these models heavily
    rely on the quality and accessibility of the underlying data. Employ techniques
    such as `VACUUM`, liquid clustering, `OPTIMIZE`, and bucketing to store, access,
    and manage your data with unparalleled efficiency. Optimized tables ensure that
    the data feeding into ML algorithms is processed efficiently. We’ll cover these
    briefly here, but we also suggest that you refer to the Delta Lake documentation
    for a comprehensive understanding of each technique.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本书的主要焦点不是Delta表的优化细节，但理解这些技术对于开发以数据为中心的机器学习解决方案至关重要。Delta表的效率管理直接影响ML模型的表现和可靠性，因为这些模型高度依赖于底层数据的品质和可访问性。采用诸如`VACUUM`、液态聚类、`OPTIMIZE`和分桶等技术，以无与伦比的效率存储、访问和管理您的数据。优化后的表确保输入到ML算法中的数据得到有效处理。我们在这里简要介绍这些技术，但也建议您参考Delta
    Lake文档以全面了解每种技术。
- en: VACUUM
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VACUUM
- en: The `VACUUM` command is crucial in managing resources within Delta tables. It
    works by cleaning up invalidated files and optimizing the metadata layout. If
    your Delta table undergoes frequent `update`, `insert`, `delete`, and `merge`),
    we recommend running the `VACUUM` operation periodically. DML operations can generate
    numerous small files over time. Failure to run `VACUUM` may result in several
    small files with minimal data remaining online, leading to poor performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`VACUUM`命令在管理Delta表内的资源方面至关重要。它通过清理无效文件和优化元数据布局来实现。如果您的Delta表经常进行`update`、`insert`、`delete`和`merge`操作，我们建议定期运行`VACUUM`操作。DML操作可能会随着时间的推移生成大量的小文件。未能运行`VACUUM`可能会导致在线保留大量数据量极小的小文件，从而导致性能下降。'
- en: Note that `VACUUM` doesn’t run automatically; you must explicitly schedule it.
    Consider scheduling `VACUUM` at regular intervals, such as weekly or monthly,
    depending on your data ingestion frequency and how often you update the data.
    Additionally, you have the option to configure the retention period to optimize
    storage and query performance. The `delta.logRetentionDuration` Delta configuration
    command allows you to control the retention period by specifying the number of
    days you want to keep the data files. This means you will delete all transaction
    log data for a table that goes back beyond the retention setting (e.g., only the
    last seven days of metadata remain). This way, you can control the duration the
    Delta table retains data files before the `VACUUM` operation removes them. The
    retention duration affects your ability to look back in time to previous versions
    of a table. Keep that in mind when deciding how long you want to retain the metadata
    and transaction log. Additionally, the transaction log is not very big.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`VACUUM`不会自动运行；您必须明确安排它。根据您的数据摄取频率和更新数据的频率，考虑定期安排`VACUUM`，例如每周或每月。此外，您可以选择配置保留期以优化存储和查询性能。`delta.logRetentionDuration`
    Delta配置命令允许您通过指定希望保留数据文件的天数来控制保留期。这意味着您将删除超出保留设置（例如，只保留最后七天的元数据）的表的交易日志数据。这样，您就可以控制`VACUUM`操作在删除之前保留数据文件的时间长度。保留期影响您回顾表之前版本的能力。在决定保留元数据和交易日志多长时间时，请记住这一点。此外，交易日志并不大。
- en: Liquid clustering
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 液态聚类
- en: Liquid clustering is a great alternative to partitioning; see how to implement
    it in *Figure 3**.2*. Frequently, data gets over-partitioned, leaving too few
    files in a partition or unbalanced partitions. You do not need partitioning unless
    a table is a terabyte or larger. In addition to replacing partitioning, liquid
    clustering also replaces *Z*-ordering on Delta tables. *Z*-ordering is not compatible
    with clustered tables. When choosing columns to cluster on, include high cardinality
    columns that you use to query filters. Another commonly given example is a timestamp
    column. Instead of creating a derived column date to minimize the cardinality,
    simply cluster on the timestamp. Liquid clustering benefits Delta tables with
    skewed data distribution or changing access patterns. It allows a table to adapt
    to analytical needs by redefining clustering keys without rewriting data, resulting
    in optimized query performance and a flexible, maintenance-efficient structure.
    With liquid clustering enabled, you must use DBR 13.3+ to create, write, or optimize
    Delta tables.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 液态聚类是分区的一个很好的替代方案；请参阅如何在 *图 3**.2* 中实现它。通常，数据会被过度分区，导致分区中文件过少或不平衡。除非表的大小为兆字节或更大，否则您不需要分区。除了替换分区外，液态聚类还替换了
    Delta 表上的 *Z*-排序。*Z*-排序与聚类表不兼容。在选择用于聚类的列时，包括您用于查询过滤的高基数列。另一个常见的例子是时间戳列。与其创建一个派生列日期以最小化基数，不如直接在时间戳上聚类。液态聚类为具有倾斜数据分布或变化访问模式的
    Delta 表带来好处。它允许表通过重新定义聚类键而不重写数据来适应分析需求，从而实现优化的查询性能和灵活、维护高效的架构。启用液态聚类后，您必须使用 DBR
    13.3+ 来创建、写入或优化 Delta 表。
- en: "![Figure 3.2 – Example code to create a table optim\uFEFFized with liquid clustering](img/B16865_03_02.jpg)"
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 使用液态聚类优化的表创建示例代码](img/B16865_03_02.jpg)'
- en: Figure 3.2 – Example code to create a table optimized with liquid clustering
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 使用液态聚类优化的表创建示例代码
- en: OPTIMIZE
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化
- en: The `OPTIMIZE` command will trigger clustering. This is extra important when
    streaming data, as tables are not clustered on write. `OPTIMIZE` also compacts
    data files, which is vital for Delta tables with numerous small files. It merges
    these files into larger ones, enhancing read query speed and storage efficiency,
    which is especially beneficial for large datasets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`OPTIMIZE` 命令将触发聚类。在流式传输数据时，这尤为重要，因为表在写入时不会进行聚类。`OPTIMIZE` 还会压缩数据文件，这对于具有许多小文件的
    Delta 表至关重要。它将这些文件合并成更大的文件，提高了读取查询速度和存储效率，这对于大型数据集特别有益。'
- en: Predictive optimization
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测优化
- en: Predictive optimization is a feature that runs `VACUUM` and `OPTIMIZE` for you.
    Your admin can enable it in settings if you have the premium version, serverless
    enabled, and Unity Catalog set up.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 预测优化是一个为您自动运行 `VACUUM` 和 `OPTIMIZE` 的功能。如果您有高级版本、已启用无服务器和已设置 Unity 目录，您的管理员可以在设置中启用它。
- en: Now that we’ve covered the basics of the Medallion architecture, Auto Loader,
    DLT, and some techniques to optimize your Delta tables, get ready to follow along
    in your own Databricks workspace as we work through the [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)
    code by project.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了 Medallion 架构、Auto Loader、DLT 以及一些优化 Delta 表的技术，准备好在自己的 Databricks
    工作区中跟随我们，通过项目来逐步分析 [*第 3 章*](B16865_03.xhtml#_idTextAnchor123) 的代码。
- en: Applying our learning
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用我们的学习方法
- en: You will ingest the data before diving into each project’s core “data science”
    aspects. We’ve discussed how Auto Loader, schema evolution, and DLT can format
    your data in storage. You’ll notice that the following projects use different
    patterns to load data into the Bronze layer. The streaming transactions project
    uses Auto Loader to ingest incoming JSON files. You will transform the data with
    DLT and Structure Streaming independently, allowing you to get experience with
    both methods.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入每个项目的核心“数据科学”方面之前，您将摄取数据。我们已经讨论了 Auto Loader、模式演变和 DLT 如何在存储中格式化数据。您会注意到以下项目使用不同的模式将数据加载到青铜层。流式事务项目使用
    Auto Loader 摄取传入的 JSON 文件。您将独立使用 DLT 和 Structure Streaming 转换数据，这样您就可以获得两种方法的经验。
- en: Technical requirements
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: Before we begin, review the technical requirements needed to complete the hands-on
    work in this chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，请回顾完成本章动手实践所需的技术要求。
- en: Databricks ML Runtime includes several pre-installed libraries useful for ML
    and data science projects. For this reason, we will use clusters with an ML runtime.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks ML 运行时包括几个预安装的库，这些库对 ML 和数据科学项目很有用。因此，我们将使用具有 ML 运行时的集群。
- en: Project – streaming transactions
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – 流式事务
- en: 'The next step in our streaming transactions project is to build out the Bronze
    layer, or the raw data we’ll eventually use in our classification model. We specifically
    created this streaming project to practice using Auto Loader, schema evolution,
    Spark Structured Streaming, and DLT features, so we’ll use those throughout this
    part of the project. To follow along in your workspace, open the following notebooks:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们流式事务项目的下一步是构建青铜层，也就是我们最终将在分类模型中使用的原始数据。我们特别创建了这个流式项目来练习使用 Auto Loader、模式演变、Spark
    Structured Streaming 和 DLT 功能，因此我们将在这部分项目中使用这些功能。要在你的工作区中跟进，请打开以下笔记本：
- en: '`CH3-01-Auto_Loader_and_Schema_Evolution`'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH3-01-Auto_Loader_and_Schema_Evolution`'
- en: '`CH3-02-Generating_Records_and_Schema_Change`'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH3-02-Generating_Records_and_Schema_Change`'
- en: '`delta_live_tables/CH3-03-Formatting_to_Delta_with_DLT`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delta_live_tables/CH3-03-Formatting_to_Delta_with_DLT`'
- en: 'Here’s where we are in the project flow:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们项目流程中的当前状态：
- en: '![Figure 3.3 – The project plan for the synthetic streaming transactions project](img/B16865_03_03.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 合成流式事务项目的项目计划](img/B16865_03_03.jpg)'
- en: Figure 3.3 – The project plan for the synthetic streaming transactions project
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 合成流式事务项目的项目计划
- en: In [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073), we generated and stored
    our synthetic transaction data in JSON files. Now, we will read that data into
    a Delta table. We’ll also update the data generation notebook to add a product
    string column, which will demonstrate schema evolution, as mentioned previously
    in the *Schema evolution* section. Let’s walk through two options for reading
    and writing the data stream to a Delta table. Both options use Auto Loader to
    ingest. The files are then handled and written out by either Spark Structured
    Streaming or DLT. There are two notebooks for this section. We will start with
    `CH3-01-Auto_Loader_and_Schema_Evolution`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 2 章*](B16865_02.xhtml#_idTextAnchor073) 中，我们生成并存储了我们的合成事务数据在 JSON 文件中。现在，我们将读取这些数据到
    Delta 表中。我们还将更新数据生成笔记本，添加一个产品字符串列，这将演示模式演变，如之前在 *模式演变* 部分中提到的。让我们来探讨两种将数据流读取和写入
    Delta 表的选项。两种选项都使用 Auto Loader 进行摄取。然后，文件由 Spark Structured Streaming 或 DLT 处理和写入。本节有两个笔记本。我们将从
    `CH3-01-Auto_Loader_and_Schema_Evolution` 开始。
- en: Continuous data ingestion using Auto Loader with Structured Streaming
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Structured Streaming 的 Auto Loader 进行连续数据摄取
- en: The code in this section uses Auto Loader to format the incoming JSON files
    as `Delta`. The table name we are using is `synthetic_transactions`. Before processing
    data, we create a widget (see *Figure 3**.4*) to determine whether we want to
    reset the schema and checkpoint history. Resetting the history can be helpful
    when altering or debugging pipelines. If you reset (remove) your checkpoint history,
    you will reprocess all historical data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码使用 Auto Loader 将传入的 JSON 文件格式化为 `Delta`。我们使用的表名是 `synthetic_transactions`。在处理数据之前，我们创建了一个小部件（见
    *图 3.4*）来决定我们是否想要重置模式和检查点历史。重置历史记录在修改或调试管道时可能很有帮助。如果你重置（删除）你的检查点历史，你将重新处理所有历史数据。
- en: '![Figure 3.4 – Creating a widget to reset the checkpoint and schema](img/B16865_03_04.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 创建小部件以重置检查点和模式](img/B16865_03_04.jpg)'
- en: Figure 3.4 – Creating a widget to reset the checkpoint and schema
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 创建小部件以重置检查点和模式
- en: 'Next, we set our variables for the script, which are mainly paths:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为脚本设置变量，这些变量主要是路径：
- en: '![Figure 3.5 – Setting the path variables](img/B16865_03_05.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 设置路径变量](img/B16865_03_05.jpg)'
- en: Figure 3.5 – Setting the path variables
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 设置路径变量
- en: The setup file provides us with `volume_file_path`, where we store the synthetic
    data, schema, and checkpoint folders.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 设置文件为我们提供了 `volume_file_path`，其中我们存储合成数据、模式和检查点文件夹。
- en: 'As shown in *Figure 3**.6*, we add spark configurations to optimize and reduce
    the sample size for inference:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 3.6* 所示，我们添加 spark 配置以优化和减少推理的样本大小：
- en: '![Figure 3.6 – Setting configurations to optimize and reduce the sample size](img/B16865_03_06.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 设置配置以优化和减少样本大小](img/B16865_03_06.jpg)'
- en: Figure 3.6 – Setting configurations to optimize and reduce the sample size
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 设置配置以优化和减少样本大小
- en: You can set these Spark configurations in the cluster’s advanced options. These
    configurations will automatically compact sets of small files into larger files
    for optimal read performance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在集群的高级选项中设置这些 Spark 配置。这些配置将自动将小文件集压缩成大文件，以实现最佳读取性能。
- en: Configuring stream for data ingestion
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置数据摄取的流
- en: 'The stream command is relatively long, so let’s walk through each chunk of
    the code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 流命令相对较长，所以让我们逐块分析代码：
- en: We are creating a stream to read from the synthetic dataset. The format option
    references the stream format. The `cloudFiles` refers to the files located in
    cloud storage. In this case, we generate data and write it to cloud storage. However,
    creating a stream with `.format("kafka")` is possible, which ingests directly
    from the stream without writing to cloud storage first. We also designate the
    file format as JSON.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们正在创建一个流来从合成数据集读取。格式选项引用流格式。`cloudFiles` 指的是位于云存储中的文件。在这种情况下，我们生成数据并将其写入云存储。然而，使用
    `.format("kafka")` 创建流是可能的，它可以直接从流中摄取而不先写入云存储。我们还指定文件格式为 JSON。
- en: '![Figure 3.7 – Creating the stream using CloudFiles and the JSON file format](img/B16865_03_07.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – 使用 CloudFiles 和 JSON 文件格式创建流](img/B16865_03_07.jpg)'
- en: Figure 3.7 – Creating the stream using CloudFiles and the JSON file format
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 使用 CloudFiles 和 JSON 文件格式创建流
- en: The default is to set column types to `string`. However, we can provide schema
    hints so that the columns we are sure about get typed appropriately. While inferring
    the schema, we also want to infer the column types. The new column is a `string`,
    so we do not see this option in action, as new columns default to a `string`.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，将列类型设置为 `string`。然而，我们可以提供模式提示，以便我们确信的列能够得到适当的类型。在推断模式的同时，我们还想推断列类型。新列是
    `string` 类型，因此我们看不到这个选项的实际操作，因为新列默认为 `string`。
- en: '![Figure 3.8 – Setting schema hints to reduce possible type mismatches. We
    want to infer the data type for columns not in the schema hint](img/B16865_03_08.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8 – 设置模式提示以减少可能的数据类型不匹配。我们希望推断不在模式提示中的列的数据类型](img/B16865_03_08.jpg)'
- en: Figure 3.8 – Setting schema hints to reduce possible type mismatches. We want
    to infer the data type for columns not in the schema hint
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 设置模式提示以减少可能的数据类型不匹配。我们希望推断不在模式提示中的列的数据类型
- en: Auto Loader uses the `rescue` column to catch the change and quickly puts the
    new column into play without data loss! Note that the stream will fail and need
    to be restarted. The schema location is needed if we want Auto Loader to keep
    track of the schema and evolve it over time.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Auto Loader 使用 `rescue` 列来捕捉变化，并迅速将新列投入使用而不会丢失数据！请注意，流将失败并需要重启。如果我们希望 Auto Loader
    能够跟踪模式并在时间上进化它，则需要模式位置。
- en: '![Figure 3.9 – Setting schema evolution to add new columns and designating
    the schema location so that Auto Loader keeps track of schema changes over time](img/B16865_03_09.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9 – 设置模式进化以添加新列并指定模式位置，以便 Auto Loader 能够跟踪模式随时间的变化](img/B16865_03_09.jpg)'
- en: Figure 3.9 – Setting schema evolution to add new columns and designating the
    schema location so that Auto Loader keeps track of schema changes over time
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 设置模式进化以添加新列并指定模式位置，以便 Auto Loader 能够跟踪模式随时间的变化
- en: Next, we load the location of the raw data files. We need to select the fields
    we want. We will select all data fields, but you could be selective on the fields
    you pull in.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载原始数据文件的位置。我们需要选择我们想要的字段。我们将选择所有数据字段，但你也可以选择性地拉取字段。
- en: The following lines of code begin the “write” portion of the stream. Here, we
    start `writeStream`. Delta is the default data format, but we prefer to explicitly
    set it. We also designate that this stream is append-only, as we are not performing
    any updates or inserts.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码行开始流的“写入”部分。在这里，我们开始 `writeStream`。Delta 是默认的数据格式，但我们更喜欢明确设置它。我们还指定此流为只追加，因为我们没有执行任何更新或插入操作。
- en: A checkpoint is a mechanism in Spark Structured Streaming that allows you to
    save the state of your stream. If your stream fails, when it restarts, it uses
    the checkpoint to resume processing where it left off. The mergeSchema option
    is essential. Merging the schema adds the new columns as they arrive without intervention.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查点是 Spark Structured Streaming 中的一种机制，允许您保存流的当前状态。如果您的流失败，当它重新启动时，它将使用检查点从上次停止的地方继续处理。合并模式选项是必不可少的。合并模式会将新列添加到到达时，而无需干预。
- en: '![Figure 3.10 – Creating our checkpoint location and setting merge schema](img/B16865_03_10.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 创建我们的检查点位置并设置合并模式](img/B16865_03_10.jpg)'
- en: Figure 3.10 – Creating our checkpoint location and setting merge schema
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 创建我们的检查点位置并设置合并模式
- en: The next step is to set the trigger. The trigger setting refers to the processing
    pace and has two main modes. Per the following code, you can specify the time-based
    trigger interval in seconds. Our data from the generation notebook is continuous.
    Here, we efficiently handle it with micro-batches. We provided a select statement
    in *step 4*. We can now write the result of that statement to Delta files in the
    `destination_location` path.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In *step 7*, we use a trigger with a processing time of 10 seconds. The processing
    time means micro-batches of data will be processed every 10 seconds. Suppose you
    do not require micro-batches. If you need your data processed once an hour or
    once per day, then the `trigger.availableNow` option is best. If you want to process
    whatever new data has arrived in the last hour, use `trigger.AvailableNow` in
    your pipeline, and schedule the pipeline to kick off in workflows using a job
    cluster every hour. At that time, Auto Loader will process all data available
    and then shut down.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, to show schema evolution, we update our data generation notebook from[*Chapter
    2*](B16865_02.xhtml#_idTextAnchor073) to include an additional `data` column.
    You’ll find the new version, `CH3-02-Generating_Records_and_Schema_Change`, in
    the [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123) folder. Note that we provide
    a list of possible products to `writeJsonFile`. The result is an additional field,
    `Product`, with a product string for the record (*Figure 3**.11*).
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.11 – The updated method for generating data with or without a product
    string](img/B16865_03_11.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – The updated method for generating data with or without a product
    string
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: As a result of our changes, you now have a stream of data that adds an additional
    column midstream. You can start the Auto Loader notebook to see the schema evolution
    in action. The stream stops when it detects the extra column, providing an exception
    – `[UNKNOWN_FIELD_EXCEPTION.NEW_FIELDS_IN_RECORD_WITH_FILE_PATH]`. Don’t worry;
    upon restart, the schema evolution takes over. Run the cell with the stream in
    it again to restart.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: You are finished reading and writing with Auto Loader with Spark Structured
    Streaming. Next, we’ll show you how to accomplish the same task with DLT (using
    less code!).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Continuous data ingestion using Auto Loader with DLT
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section uses the Auto Loader code to read the stream of JSON files and
    then DLT to write the files to a table. This notebook is in the `delta_live_tables`
    folder and titled `CH3-03-Formatting_to_Delta_with_DLT`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The only import you need is DLT – `import dlt`. The DLT code is relatively short,
    partly because the pipeline configuration occurs in the pipeline object rather
    than the code. Before we look through our source code, let’s navigate to the **Databricks
    Workflows** pane in the left-hand navigation bar, select **Delta Live Tables**,
    and click **Create Pipeline**, which opens the pipeline settings page, as shown
    in *Figure 3**.12*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.12 – The DL\uFEFFT pipeline setup in the workflow UI](img/B16865_03_12.jpg)"
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – The DLT pipeline setup in the workflow UI
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline configuration for DLT
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The pipeline settings are minimal, given that DLT does the optimization, so
    the setup instructions are minimal. The parameters entered into the pipeline configuration
    are accessible in the pipeline code using `spark.conf.get`, as shown in *Figures
    3.13* and *3.14*:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Enter a pipeline name. We will use `MLIA_Streaming_Transactions`.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For **Product edition**, select **Advanced**. This DLT edition comes with the
    most features, including DLT’s expectation rules.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For **Pipeline mode**, select **Triggered**. This will ensure that the pipeline
    stops processing after a successful run.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For **Paths**, select this notebook from the repository for the source code:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Select **Unity Catalog** as your storage option.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `ml_in_action` for the catalog and `synthetic_transactions_dlt` as the
    target schema.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter `table_name` and `synthetic_transactions_dlt` as configurations in the
    `raw_data_location` as follows (as shown in *Figure 3**.13*):'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: "![Figure 3.13 – Advanced pipel\uFEFFine configuration settings for variables](img/B16865_03_13.jpg)"
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Advanced pipeline configuration settings for variables
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Methods in the DLT pipeline
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have filled in the pipeline UI, let’s focus on the methods used
    in the source code of the pipeline. In this function, we mostly reuse our previous
    code (*Figure 3**.14*). Note that we do not use the setup file in this notebook.
    Instead, we use the variables we set in the pipeline settings’ advanced configuration
    section.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3\uFEFF.\uFEFF14 – The autoloader stream method details](img/B16865_03_14.jpg)"
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – The autoloader stream method details
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Generating the bronze table in a DLT pipeline
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `generate_table()` function feeds the read stream created using Auto Loader
    into DLT. We use `spark.conf.get('variable_name')` to access the variable values
    we defined in the pipeline’s advanced settings (*Figure 3**.13*). In the notebook,
    you will see the final step, a one-line cell called `generate_table()`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Generating the Bronze table in a DLT pipeline](img/B16865_03_15.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Generating the Bronze table in a DLT pipeline
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: DLT is unique. It is not code you run line by line, so you don’t execute the
    pipeline in the notebook. Back in the DLT UI, we save and click the **Start**
    button. After the setup process, you will see a graph that includes our table.
    **Start** not only starts the pipeline but creates it as well. Once it finishes,
    you will have a screen like the one shown in *Figure 3**.1*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Querying streaming tables created with DLT
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can query a streaming table created with DLT just like you query other tables.
    This is handy for populating dashboards, which we will cover in [*Chapter 8*](B16865_08.xhtml#_idTextAnchor384),
    *Monitoring, Evaluating,* *and More*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'If in a notebook you try to query your streaming table, you may get this error:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '`ExecutionException: org.apache.spark.sql.AnalysisException: 403: Your token
    is missing the required scopes for` `this endpoint.`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: This is because to query streaming tables created by a DLT pipeline, you must
    use a shared cluster using Databricks Runtime 13.1 and above, or a SQL warehouse.
    Streaming tables created in a Unity Catalog-enabled pipeline cannot be queried
    from assigned or no-isolation clusters. You can change your cluster or use the
    DBSQL query editor.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline creates our Bronze table, wrapping up this project on streaming
    transaction data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Project – Favorita store sales – time series forecasting
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You downloaded the *Favorita Sales Forecasting* dataset from Kaggle using `opendatasets`
    in the last chapter. We will use that data now to create Delta tables. To follow
    along in your own workspace, open the `CH3-01-Loading_Sales_CSV_Data_as_Delta`
    notebook.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: The downloaded data is in single CSV files. We use pandas to read the datasets
    and Spark to write to a Delta table. We demonstrate this for only the first file
    in the following code block (*Figure 3**.16*).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Reading in the sales holiday events datasets with Pandas](img/B16865_03_16.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Reading in the sales holiday events datasets with Pandas
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: We utilized the data profile capability (*Figure 3**.17*) to check the data
    types before writing to a table. Profiling shows the inferred data type for the
    date field is a string rather than a date or timestamp. Therefore, we alter the
    data type before writing to a Delta table.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Utilizing display(df), we can click + to see the data profile.
    This lets us look at the data types and distributions quickly](img/B16865_03_17.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Utilizing display(df), we can click + to see the data profile.
    This lets us look at the data types and distributions quickly
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Each table for the Favorita project is transformed into Delta tables similarly.
    As a result, we only include the first table in the book’s pages. However, the
    code that transforms each of the tables is, of course, in the repository.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Project – a retrieval augmented generation chatbot
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**RAG** stands for **Retrieval Augmented Generation**. A RAG system often consists
    of your data, a vector database, a search algorithm, and a **generative AI** model
    to generate an answer to a user’s query. *Figure 3**.18* shows the pipeline we
    will build throughout this book.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – A pipeline example that we will try to replicate through the
    book](img/B16865_03_18.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – A pipeline example that we will try to replicate through the book
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see from *Figure 3**.18* that we need to retrieve relevant data according
    to the user query. This is where our search method would access a vector database
    and conduct a semantic or hybrid search. At this stage in the RAG chatbot project,
    we have our PDFs in their raw, unstructured form. We want our users to access
    this knowledge via our chatbot, so we have to bring relevant content with information
    from all the PDFs into our chatbot, running in real time. To do so, we’ll convert
    all our PDFs into a machine-readable format. This chapter shows how to extract
    unstructured data from PDF files, chunk it, and transform your text into embeddings.
    Then, we store the embeddings in a Delta table:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Our first step for data preparation is to extract unstructured information from
    PDF files. To follow along in your workspace, open the `CH3-01-Creating_EmbeddedChunks`
    notebook.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To start building the Bronze data layer, create an empty Delta table that includes
    the table’s schema. Use the `GENERATED BY DEFAULT AS IDENTITY` feature to leverage
    Delta table capabilities to index the newly arriving data automatically. Also,
    add a table property to set the `Change Data Feed` to `true`.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Create an empty Delta table with the name pdf_documentation_text](img/B16865_03_19.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Create an empty Delta table with the name pdf_documentation_text
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: For the next step, read the raw PDFs from the `volume` folder and save them
    to a table named `pdf_raw` (*Figure 3**.20*). We will come back to the `pdf_documentation_text`
    table.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Read the PDF file in the binary format and write it into a
    Bronze layer table](img/B16865_03_20.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – Read the PDF file in the binary format and write it into a Bronze
    layer table
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Store the PDFs in the binary format in the `content` column to extract the
    text later. Let’s see how it looks in a Delta table view. The binary format of
    each PDF is in the `content` column:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Displaying ingested content in the Delta table](img/B16865_03_21.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – Displaying ingested content in the Delta table
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Next, we write a helper function using the `unstructured` library to extract
    the text from the PDF bytes. The function is in the `mlia_utils.rag_funcs` script.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Creating a helper function to extract document text](img/B16865_03_22.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – Creating a helper function to extract document text
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply this function to one of the PDF files we have and check the content
    of our document:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Applying a helper function to extract information from the
    PDF](img/B16865_03_23.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 – Applying a helper function to extract information from the PDF
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Great! *Figure 3**.23* gives us a glimpse into one of the ingested documents.
    You may have many PDFs, which can be very long. Long documents can potentially
    be a problem for our future chatbot because they can easily exceed most LLMs maximum
    context lengths (check out *Further reading* for more information on context lengths).
    Additionally, we likely don’t need an entire document’s worth of text to answer
    a specific question. Instead, we need a section or “chunk” of this text. For this
    project, we create chunks of no more than 500 tokens with a chunk overlap of 50,
    using the `SentenceSplitter` module of the `LlamaIndex` library. You could also
    use the `LangChain` library or any library you choose to split the content into
    chunks. We will use the open source `Llama-tokenizer`, as this will be our main
    family of models across our project. Note that tokenizers may play a crucial role
    in your RAG quality. We have leveraged a Pandas **User Defined Function** (**UDF**)
    to scale across all pages of all documents.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Creating a pandas UDF for our extractor function](img/B16865_03_24.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 – Creating a pandas UDF for our extractor function
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s apply this function to our Delta table:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Applying a helper function to extract information from the
    PDF using PySpark](img/B16865_03_25.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: Figure 3.25 – Applying a helper function to extract information from the PDF
    using PySpark
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Once our chunks are ready, we need to convert them into embeddings. Embeddings
    are the format required to perform a semantic search when ingesting into our Silver
    layer later.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Databricks Model Serving now supports **Foundation Model APIs** (**FMAPIs**),
    which allow you to access and query state-of-the-art open models from a serving
    endpoint. With FMAPIs, you can quickly and easily build applications that leverage
    a high-quality GenAI model without maintaining your own model deployment (for
    more information, see *Deploy provisioned throughput Foundation Model APIs* in
    the *Further* *reading* section).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'FMAPIs are provided in two access modes:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '**Pay-per-token**: This is the easiest way to start accessing foundation models
    on Databricks and is recommended for beginning your journey with them.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provisioned throughput**: This model is recommended for workloads that require
    performance guarantees, fine-tuned models, or have additional security requirements:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Available models for access via the FMAPIs of Databricks](img/B16865_03_26.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 3.26 – Available models for access via the FMAPIs of Databricks
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the FMAPI is available only to US regions. If your workspace
    is not yet in a supported region, you can use any model of your choice (OpenAI,
    BERT, a LlaMA tokenizer, etc.) to convert your content into embeddings.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: You may also need to fine-tune your model embedding to learn from your own content
    for better retrieval results.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Next, we leverage the DI Platform’s pay-per-token capability from the FMAPI
    that provides you with access to the `BGE_large_En` endpoint, through the new
    functionality recently added to the `mlflow >=2.9` - `mlflow` deployments (previously
    known as AI Gateway). This functionality unifies the model serving endpoint management
    on Databricks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27 – Applying the FMAPI with the BGE endpoint to convert “What is
    ChatGPT?” into an embedding](img/B16865_03_27.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 3.27 – Applying the FMAPI with the BGE endpoint to convert “What is ChatGPT?”
    into an embedding
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Now, we apply this embedding conversion across all our chunks, and we again
    make `pandasUDF` for scalability purposes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28 – pandasUDF to apply embedding conversion across all chunks](img/B16865_03_28.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: Figure 3.28 – pandasUDF to apply embedding conversion across all chunks
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying our UDF will append our `raw_table` chunks with the corresponding
    embeddings:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29 – pandasUDF to apply embedding conversion across all chunks](img/B16865_03_29.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: Figure 3.29 – pandasUDF to apply embedding conversion across all chunks
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Once the final step of our data preparation process is completed, we save our
    table in the initially pre-created Delta table `pdf_documentation_text` using
    append mode.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: We’ve ingested our PDFs for this project in one big batch, which works perfectly
    fine as an example. However, it also means that anytime you want to add a new
    PDF to your chatbot’s knowledge base, you must manually rerun all of the preceding
    steps. We recommend a workflow for production-grade solutions to automate the
    preceding steps and incrementally ingest PDFs as they arrive in storage.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: We now have a dataset ready for a vector search index, which we’ll cover in
    [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Project – multilabel image classification
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073), we extracted and stored
    our raw image data in our volume. In this chapter, we prepare our image dataset
    and save training and validation sets into Delta tables. To follow along in your
    workspace, open the `Ch3-01-Loading_Images_2_DeltaTables` notebook:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: We start by creating variables and removing existing data if the `Reset` widget
    value is `True`.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.30 – Cleaning up existing data if Reset = True](img/B16865_03_30.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: Figure 3.30 – Cleaning up existing data if Reset = True
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a function to ingest all our images in one table. Initially,
    each label is in its own `folder_label_name`. We extract `image_name`, `image_id`,
    and `label_id`, as well as create `label_name` using append mode.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.31 – Creating the prep_data2delta function](img/B16865_03_31.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: Figure 3.31 – Creating the prep_data2delta function
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: We use the `prep_data2delta` function to load and prepare our training and validation
    datasets (*Figure 3**.32*). Note that the function will save a Delta table if
    the `write2delta` flag is `True` and will return a DataFrame if the value for
    the `returnDF` flag is `True`. Next, in the notebook, we call `prep_data2delta`
    for the training and validation sets.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Let’s talk about data loaders
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We load data in a fixed-size batch when fine-tuning or training our deep learning
    models. Each framework natively supports specific data types; some expand their
    native formats to other open source formats. Data scientists sometimes prefer
    to keep their data (images, in our case) in blob storage and read it directly
    from storage rather than using Delta tables, as they think this avoids additional
    work. However, we recommend storing images in a Delta table unless you are working
    with large images greater than one GB per image. Storing your images in a Delta
    table allows you to take advantage of Delta and Unity Catalog’s additional benefits,
    such as lineage of data and models, data version control, duplicate data checks,
    and quality assurance.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, you have a few options to read your data while working
    with a PyTorch or PyTorch Lightning framework:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '`DeltaTorchLoader` (recommended)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petastorm
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading images directly from blob/disk/volumes
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our recommendation is to use DeltaTorchLoader. It handles data batching, sampling,
    and multiprocessing while training PyTorch pipelines without requiring a temporary
    copy of files, like with Petastorm. See *Further reading* for more information.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, DeltaTorchLoader, which we are going to use to load
    and transform our data from Delta into the PyTorch/Lightning dataloader framework
    to train our model in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297), requires
    you to have tables in the unmanaged Delta format in volumes when using UC. Don’t
    worry; the lineage is associated with the same path to the volume as your main
    dataset. We’ll talk more about lineage in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
    This requirement is due to UC’s read/write security permissions with blob storage.
    The blob storage maintainers do not support these security settings yet. If you
    are not using UC, you should be able to read Delta tables directly from the managed
    tables.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: There is also an option to read your data using the Petastorm library. We don’t
    recommend Petastorm because it requires a deeper understanding of certain pitfalls.
    The most common are memory usage issues due to data caching and the fact that
    it uses Apache `Parquet` files rather than Delta files, so it consumes all versions
    of your parquet files.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The creators of the DeltaTorchLoader performed a few benchmarks with Petastorm.
    The benchmark was shared at the **Data and AI Summit** (**DAIS**) and is featured
    in *Figure 3**.33*. In this project, we will compare Petastorm to the classic
    Torch loader in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297) to show the performance
    gain. The comparison demonstrates an incredible speed increase when reading the
    batch of data. We’ve also included a great video on *TorchDeltaLoader* in *Further
    reading* if you want to learn more.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.32 – The DeltaTorchLoader benchmark – a performance comparison between
    the DeltaTorch and Petastorm loaders](img/B16865_03_32.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: Figure 3.32 – The DeltaTorchLoader benchmark – a performance comparison between
    the DeltaTorch and Petastorm loaders
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: You can keep filenames in your Delta table instead of the images and collect
    them while passing them to the main PyTorch Loader with the `trainer` function.
    Keeping files in Delta is essential to avoid duplicates and control the list used
    during training and validation, as you can pass the Delta version to MLflow during
    tracking for full replication purposes.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing our data
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once our tables are created and written to the storage, we use a few functions
    to improve read performance on the Delta tables. First, we use `OPTIMIZE` to keep
    an ideal number of files (*Figure 3**.34*). Second, we disable deletion vectors
    because the `DeltaTorchReader` does not support them yet *(**Figure 3**.35*).
    We use the SQL magic command, `%sql`, to perform these operations, using SQL in
    the Python notebook.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.33 – Optimizing the file size and count of the training table](img/B16865_03_33.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: Figure 3.33 – Optimizing the file size and count of the training table
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Note that the variables we saved in Python are inaccessible in SQL, so we hardcode
    them in this example. You could include SQL variables in the `global-setup` notebook
    to avoid this.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.34 – Optimizing the training table](img/B16865_03_34.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 3.34 – Optimizing the training table
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have the training and validation tables loaded and optimized to efficiently
    work with this image data to fine-tune our multi-class computer vision models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on the essentials of building out the Bronze data
    layer within the Databricks Data Intelligence Platform. We emphasized the importance
    of schema evolution, DLT, and the conversion of data into the Delta format and
    applied these principles in our example projects. This chapter highlighted the
    significance of tools such as Auto Loader and DLT in this process. Auto Loader,
    with its proficiency in handling file tracking and automating schema management,
    alongside DLT’s robust capabilities in pipeline development and data quality assurance,
    are pivotal in our data management strategy. These tools facilitate an efficient
    and streamlined approach to data pipeline management, enabling us as data scientists
    to focus more on valuable tasks, such as feature engineering and experimentation.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: With our Bronze layer created, we now move on from this foundational work to
    a more advanced layer of data – the Silver layer. [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180),
    *Transformations toward Our Silver Layer*, will take us deeper into our data and
    demonstrate various Databricks tools that will aid us in the exploration and transformations
    of our data.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following questions solidify key points to remember and tie the content
    back to your experience:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: What are the names of the layers in the Medallion architecture design?
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you wanted to build a managed pipeline with streaming data, which product
    would you use – Structured Streaming or DLT?
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What feature did we use to add the `product` column to our streaming transaction
    data without manual intervention?
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do you have projects from your current position, experience, or on your roadmap
    that would benefit from one or more of the topics covered in this chapter?
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a possible way to lessen the number of partitions when partitioning
    on a high cardinality column?
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After putting thought into the questions, compare your answers to ours:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The layers of the Medallion architecture are Bronze, Silver, and Gold.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We recommend DLT build managed pipelines.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the streaming transactions project example, we used Auto Loader’s schema
    evolution feature to add a column without manual intervention.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We hope so! One example is a managed streaming data pipeline that could benefit
    from the built-in data quality monitoring that comes with DLT.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bucketing is an optimal method specifically designed to provide an additional
    layer of organization in your data. It can reduce the number of output files and
    organize the data better for subsequent reading, and it can be especially useful
    when the partitioning column has high cardinality.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covered different methods of ingesting data into your Bronze layer.
    Take a look at these resources to read more about the areas that interest you
    most:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '*Use liquid clustering for Delta* *tables*:[https://docs.databricks.com/en/delta/clustering.html](https://docs.databricks.com/en/delta/clustering.html)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spark Structured* *Streaming*: [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Delta Live* *Tables*: [https://docs.databricks.com/en/delta-live-tables/index.html](https://docs.databricks.com/en/delta-live-tables/index.html)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DLT Databricks* *Demo*:[https://www.databricks.com/resources/demos/tutorials/lakehouse-platform/full-delta-live-table-pipeline](https://www.databricks.com/resources/demos/tutorials/lakehouse-platform/full-delta-live-table-pipeline)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Auto Loader* *options*: [https://docs.databricks.com/ingestion/auto-loader/options.html](https://docs.databricks.com/ingestion/auto-loader/options.html)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Schema evolution with Auto* *Loader*: [https://docs.databricks.com/ingestion/auto-loader/schema.html#configure-schema-inference-and-evolution-in-auto-loader](https://docs.databricks.com/ingestion/auto-loader/schema.html#configure-schema-inference-and-evolution-in-auto-loader)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Common loading patterns with Auto* *Loader*: [https://docs.databricks.com/ingestion/auto-loader/patterns.html](https://docs.databricks.com/ingestion/auto-loader/patterns.html)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stream processing with Apache Kafka and* *Databricks*:[https://docs.databricks.com/structured-streaming/kafka.html](https://docs.databricks.com/structured-streaming/kafka.html)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How We Performed ETL on One Billion Records For Under $1 With Delta Live*
    *Tables*: [https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html](https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Create tables – Managed vs* *External*: [https://docs.databricks.com/en/data-governance/unity-catalog/create-tables.html#create-tables](https://docs.databricks.com/en/data-governance/unity-catalog/create-tables.html#create-tables)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Take full advantage of the auto-tuning* *available*:[https://docs.databricks.com/delta/tune-file-size.html#configure-delta-lake-to-control-data-file-size](https://docs.databricks.com/delta/tune-file-size.html#configure-delta-lake-to-control-data-file-size)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Import Python modules from Databricks repos: [https://docs.databricks.com/en/delta-live-tables/import-workspace-files.html](https://docs.databricks.com/en/delta-live-tables/import-workspace-files.html)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deletion Vectors: [https://docs.databricks.com/en/delta/deletion-vectors.html](https://docs.databricks.com/en/delta/deletion-vectors.html)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks ML* *Runtime*:[https://docs.databricks.com/runtime/mlruntime.html#introduction-to-databricks-runtime-for-machine-learning](https://docs.databricks.com/runtime/mlruntime.html#introduction-to-databricks-runtime-for-machine-learning)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cluster advanced* *options*:[https://docs.databricks.com/en/clusters/configure.html#spark-configuration](https://docs.databricks.com/en/clusters/configure.html#spark-configuration)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deploy provisioned throughput Foundation Model* *APIs*:[https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html](https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scaling Deep Learning Using Delta Lake Storage Format on* *Databricks*: [https://www.databricks.com/dataaisummit/session/scaling-deep-learning-using-delta-lake-storage-format-databricks/](https://www.databricks.com/dataaisummit/session/scaling-deep-learning-using-delta-lake-storage-format-databricks/)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DeltaTorchLoader*: [https://github.com/delta-incubator/deltatorch](https://github.com/delta-incubator/deltatorch)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2: Heavily Use Case-Focused'
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part introduces you to taking a set of data sources and working with them
    throughout the platform, from one end to another. The goal of this part is simply
    to demonstrate how to thoughtfully use all the bells and whistles of the platform.
    This part provides stories, code, lakehouse features, and best practices.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B16865_04.xhtml#_idTextAnchor180), *Getting to Know Your Data*'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B16865_05.xhtml#_idTextAnchor244), *Feature Engineering on Databricks*'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B16865_06.xhtml#_idTextAnchor297), *Searching for a Signal*'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B16865_07.xhtml#_idTextAnchor325), *Productionizing ML on Databricks*'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B16865_08.xhtml#_idTextAnchor384), *Monitoring, Evaluating, and
    More*'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
