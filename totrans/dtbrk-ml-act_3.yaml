- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Building Out Our Bronze Layer
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建我们的青铜层
- en: “Data is a precious thing and will last longer than the systems themselves.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “数据是宝贵的，比系统本身更持久。”
- en: – Tim Berners-Lee, generally credited as the inventor of the World Wide Web
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 蒂姆·伯纳斯-李，通常被认为是万维网的发明者
- en: In this chapter, you’ll embark on the beginning of your data journey in the
    Databricks platform, exploring the fundamentals of the Bronze layer. We recommend
    employing the Medallion design pattern within the lake house architecture (as
    described in [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073)) to organize your
    data. We’ll start with **Auto Loader**, which you can implement with or without
    **Delta Live Tables** (**DLT**) to insert and transform data in your architecture.
    The benefits of using Auto Loader include quickly transforming new data into the
    Delta format and enforcing or evolving schemas, which are essential for maintaining
    consistent data delivery to the business and customers. As a data scientist, strive
    for efficiency in building your data pipelines and ensuring your data is ready
    for the steps in the machine learning development cycle. You will best learn these
    topics through the example projects, so the *Applying our learning* section is
    the main focus of this chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将开始您的Databricks平台数据之旅，探索青铜层的基础知识。我们建议在湖屋架构（如[*第二章*](B16865_02.xhtml#_idTextAnchor073)中所述）中使用Medallion设计模式来组织您的数据。我们将从**Auto
    Loader**开始，您可以使用或不需要**Delta Live Tables**（**DLT**）来在架构中插入和转换数据。使用Auto Loader的好处包括快速将新数据转换为Delta格式，并强制执行或演进模式，这对于维护对业务和客户的持续数据交付至关重要。作为一名数据科学家，努力提高构建数据管道的效率，并确保您的数据为机器学习开发周期的步骤做好准备。您将通过示例项目最好地学习这些主题，因此本章的重点是*应用我们的学习*部分。
- en: 'Let’s see what topics you will cover in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看本章将涵盖哪些主题：
- en: Revisiting the Medallion architecture pattern
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾Medallion架构模式
- en: Transforming data to Delta with Auto Loader
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Auto Loader将数据转换为Delta格式
- en: DLT, starting with Bronze
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DLT，从青铜开始
- en: Maintaining and optimizing Delta Tables
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护和优化Delta表
- en: Applying our learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: Revisiting the Medallion architecture pattern
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾Medallion架构模式
- en: We introduced the Medallion architecture in [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073).
    As a reminder, this refers to the data design pattern used to organize data logically.
    It has three layers – Bronze, Silver, and Gold. There are also cases where additional
    levels of refinement are required, so your Medallion architecture could be extended
    to Diamond and Platinum levels if needed. The Bronze layer contains raw data,
    the Silver layer contains cleaned and transformed data, and the Gold layer contains
    aggregated and curated data. Curated data refers to the datasets selected, cleaned,
    and organized for a specific business or modeling purpose. This architecture is
    a good fit for data science projects. Maintaining the original data as a source
    of truth is important, while curated data is valuable for research, analytics,
    and machine learning applications. By selecting, cleaning, and organizing data
    for a specific purpose, curated data can help improve its accuracy, relevance,
    and usability.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第二章*](B16865_02.xhtml#_idTextAnchor073)中介绍了Medallion架构。作为提醒，这指的是用于逻辑组织数据的数据设计模式。它包含三个层级——青铜、白银和黄金。在某些情况下，可能需要额外的细化层级，因此如果需要，您的Medallion架构可以扩展到钻石和铂金层级。青铜层包含原始数据，白银层包含清洗和转换后的数据，黄金层包含汇总和精选的数据。精选数据指的是为特定业务或建模目的选择、清洗和整理的数据集。这种架构非常适合数据科学项目。维护原始数据作为事实来源非常重要，而精选数据对于研究、分析和机器学习应用非常有价值。通过为特定目的选择、清洗和组织数据，精选数据可以帮助提高其准确性、相关性和可用性。
- en: Note
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This book will introduce quite a few project tasks, such as *building curated
    datasets*, that often fall under the domain of a data engineer. There will also
    be tasks that machine learning engineers, data analysts, and so on commonly perform.
    We include all of this work in our examples because roles are blurred in today’s
    fast-moving world and will vary by company. Titles and expectations quickly evolve.
    Therefore, it’s imperative to have a handle on the entire end-to-end workflow.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将介绍许多项目任务，例如*构建精选数据集*，这些任务通常属于数据工程师的领域。也会有机器学习工程师、数据分析师等经常执行的任务。我们将所有这些工作包含在我们的示例中，因为在当今快速发展的世界中，角色是模糊的，并且会因公司而异。头衔和期望会迅速演变。因此，掌握整个端到端工作流程至关重要。
- en: Maintaining a Bronze layer allows us to go back to the original data when we
    want to create a different feature, solve a new problem that requires us to look
    at historical data from another point of view, or simply maintain the raw level
    of our data for governance purposes. As the world of technology evolves, it’s
    crucial to stay current and follow trends, but the core principles will remain
    for years. For the remainder of this chapter, we will cover DI platform features
    that facilitate building the Bronze layer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 维护一个Bronze层允许我们在需要创建不同的特征、解决需要从另一个角度查看历史数据的新问题，或者仅仅为了治理目的而保持数据的原始级别时，回到原始数据。随着技术世界的不断发展，保持最新并跟随趋势至关重要，但核心原则将保持数年。在本章的剩余部分，我们将介绍便于构建Bronze层的DI平台功能。
- en: Transforming data to Delta with Auto Loader
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Auto Loader将数据转换为Delta
- en: Harness the power of Auto Loader to automate your data ingestion process, significantly
    enhancing your data product’s workflow efficiency. It can ingest data from cloud
    storage and streaming data sources. You can configure Auto Loader to run on a
    schedule or be triggered manually.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 利用Auto Loader的力量来自动化您的数据摄取过程，显著提高您数据产品的工作流程效率。它可以从云存储和流数据源摄取数据。您可以配置Auto Loader按计划运行或手动触发。
- en: 'Here are some benefits of using Databricks’ Auto Loader:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Databricks的Auto Loader的一些好处：
- en: '**It keeps data up to date**: Auto Loader maintains checkpoints, removing the
    need to know which data is new. Auto Loader handles all that on its own.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它保持数据最新**：Auto Loader维护检查点，无需知道哪些数据是新的。Auto Loader会自行处理所有这些。'
- en: '**It improves data quality**: Auto Loader can automatically detect schema changes
    and rescue any new data columns, so you can be confident that your data is accurate.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它提高数据质量**：Auto Loader可以自动检测架构更改并恢复任何新的数据列，因此您可以确信您的数据是准确的。'
- en: '**It increases data agility**: Auto Loader can help you quickly and easily
    ingest new data sources so that you can be more agile in responding to changes
    in your business.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它增加数据敏捷性**：Auto Loader可以帮助您快速轻松地摄取新的数据源，从而使您在应对业务变化时更加敏捷。'
- en: '**Flexible ingestion**: Auto Loader can stream files in batches or continuously.
    This means it can consume batch data as a stream to reduce the overhead of a more
    manual batch pipeline.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的摄取**：Auto Loader可以批量或连续地流式传输文件。这意味着它可以以流的形式消耗批量数据，以减少更手动批量管道的开销。'
- en: Auto Loader is a powerful tool. It can be used standalone, as the underlying
    technology for DLT, or with **Spark Structured Streaming**. Spark Structured Streaming
    is a near-real-time processing engine; we will cover how to create a real-time
    feature in [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244). This chapter also
    covers an example of streaming ingestion in the streaming transactions project.
    Let’s discuss Auto Loader’s ability to evolve a schema over time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Auto Loader是一个强大的工具。它可以独立使用，作为DLT的底层技术，或者与**Spark Structured Streaming**一起使用。Spark
    Structured Streaming是一个接近实时处理引擎；我们将在[*第五章*](B16865_05.xhtml#_idTextAnchor244)中介绍如何创建实时功能。本章还涵盖了一个流式摄取的示例，在流交易项目中。让我们讨论Auto
    Loader随时间演进架构的能力。
- en: Schema evolution
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构演进
- en: Auto Loader’s schema evolution allows you to add or modify fields in streaming
    data seamlessly. Databricks automatically adjusts relevant data to fit the new
    schema while preserving existing data integrity. This automated schema handling
    makes it easy to evolve your data schema over time as your business needs and
    data sources change, without having to worry about data loss or downtime. Furthermore,
    schema handling reduces the amount of work and headache associated with managing
    data pipelines that could change unexpectedly.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Auto Loader的架构演进功能允许您无缝地在流数据中添加或修改字段。Databricks会自动调整相关数据以适应新架构，同时保持现有数据完整性。这种自动化的架构处理使得随着业务需求和数据源的变化，您可以在不担心数据丢失或停机的情况下轻松地随时间演进您的数据架构。
- en: The new schema includes an additional column in the *Applying our learning*
    section. We will show how no data is lost despite a schema changing without notice.
    In our case, the default schema evolution mode for Auto Loader, `.option("cloudFiles.schemaEvolutionMode",
    "addNewColumns")`, in combination with `.option("mergeSchema", "true")`, perform
    schema evolution. Auto Loader will handle changes in the schema for us. Auto Loader
    tracking the changes is beneficial when new data fields become available with
    or without prior notice; code changes are unnecessary.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 新的模式在 *应用我们的学习* 部分中增加了一个额外的列。我们将展示即使在模式未通知的情况下更改，也不会丢失任何数据。在我们的案例中，Auto Loader
    的默认模式演化模式 `.option("cloudFiles.schemaEvolutionMode", "addNewColumns")` 与 `.option("mergeSchema",
    "true")` 结合执行模式演化。Auto Loader 将为我们处理模式变化。当新数据字段可用时，无论是事先通知还是未通知，Auto Loader 跟踪变化是有益的；无需进行代码更改。
- en: There’s documentation on all schema options. The method we use for our project
    is the only option for automatic schema evolution. Every other option will require
    manual intervention. For example, you can use “rescue” mode to rescue data from
    being lost. Alternatively, you can use “failOnNewColumns” mode to cause the pipeline
    to fail and keep the schema unchanged until the production code is updated. There
    are numerous options and patterns for Auto Loader. Check out the *Common loading
    patterns with Auto Loader* link in the *Further reading* section for more information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模式选项都有文档说明。我们项目所采用的方法是唯一适用于自动模式演化的选项。其他任何选项都需要手动干预。例如，您可以使用“rescue”模式来挽救数据以避免丢失。或者，您可以使用“failOnNewColumns”模式来使管道失败并保持模式不变，直到生产代码更新。Auto
    Loader 有许多选项和模式。在“进一步阅读”部分的 *Common loading patterns with Auto Loader* 链接中查看更多信息。
- en: DLT, starting with Bronze
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从青铜开始使用 DLT
- en: As we touched upon in a previous chapter, remember that DLT actively simplifies
    your pipeline operations, empowering you to focus on setting clear objectives
    for your pipeline rather than getting bogged down in operational details. Building
    on this foundation, we will now delve into DLT’s capabilities. Auto Loader’s schema
    evolution is integrated with DLT, underscoring its utility in handling dynamic
    data schemas with minimal manual intervention.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前一章中提到的，请记住 DLT 主动简化了您的管道操作，使您能够专注于为您的管道设定明确的目标，而不是陷入操作细节。在此基础上，我们现在将深入研究
    DLT 的功能。Auto Loader 的模式演化与 DLT 集成，强调了它在处理需要最少手动干预的动态数据模式中的实用性。
- en: DLT benefits and features
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DLT 的优势和功能
- en: 'DLT is a sophisticated framework designed to construct reliable data pipelines.
    DLT automates and streamlines complex operations such as orchestration and cluster
    management, significantly boosting the efficiency of data workflows. All you need
    to do is specify your transformation logic to be up and running. We will focus
    on just a few of the benefits of DLT as they pertain to creating your Bronze data
    layer, but we’ve included a link to DLT documentation in the *Further reading*
    section at the end of this chapter as well. DLT is another great tool in your
    toolbox for ingesting data. It provides several benefits over traditional ETL
    pipelines, including the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: DLT 是一个复杂的框架，旨在构建可靠的数据管道。DLT 自动化和简化了复杂的操作，如编排和集群管理，显著提高了数据工作流的效率。您只需指定您的转换逻辑即可启动运行。我们将重点关注与创建青铜数据层相关的
    DLT 的几个优势，但我们也在本章末尾的“进一步阅读”部分中包含了一个 DLT 文档的链接。DLT 是您工具箱中用于摄取数据的另一个优秀工具。它提供了比传统
    ETL 管道更多的优势，包括以下内容：
- en: '**Declarative pipeline development**: DLT allows you to define your data pipelines
    using SQL or Python, which makes them easier to understand and maintain'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声明式管道开发**：DLT 允许您使用 SQL 或 Python 定义您的数据管道，这使得它们更容易理解和维护。'
- en: '**Automatic data quality testing**: DLT can automatically apply your tests
    to your data, preventing quality issues, which helps to ensure that your pipelines
    are producing accurate results'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动数据质量测试**：DLT 可以自动将您的测试应用到您的数据上，防止质量问题，这有助于确保您的管道产生准确的结果。'
- en: '**Deep visibility for monitoring and recovery**: DLT provides detailed tracking
    and logging information, making troubleshooting problems and recovering from failures
    easier'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度可见性用于监控和恢复**：DLT 提供详细的跟踪和日志信息，使解决问题和从故障中恢复变得更加容易。'
- en: '**Cost-effective streaming through efficient compute autoscaling**: DLT can
    automatically scale your compute resources up or down based on demand, which helps
    to reduce costs'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过高效的计算自动扩展实现成本效益的流式处理**：DLT可以根据需求自动扩展或缩减计算资源，这有助于降低成本'
- en: DLT is a powerful tool for building reliable and scalable data pipelines in
    batch or streaming. It can help you improve the quality, efficiency, and visibility
    of your data processing workflows.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DLT是构建可靠和可扩展的批量或流式数据管道的强大工具。它可以帮助您提高数据处理工作流程的质量、效率和可见性。
- en: 'Here are some of the specific features of DLT that provide these benefits:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DLT的一些具体特性，它们提供了这些优势：
- en: '**Streaming tables**: DLT uses streaming and live tables to process data in
    real time and to keep your pipelines up to date with the latest data.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式表**：DLT使用流式和实时表来实时处理数据，并确保您的管道与最新数据保持同步。'
- en: '**Materialized views**: DLT uses materialized views to create snapshots of
    your data. You can query your data in real time and use it for downstream processing.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物化视图**：DLT使用物化视图来创建数据的快照。您可以在实时查询数据的同时，将其用于下游处理。'
- en: '**Expectations**: DLT uses expectations to test your data for quality issues
    automatically and to take action if the data does not meet your expectations.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**期望**：DLT使用期望来自动测试数据的质量问题，并在数据不符合期望时采取行动。'
- en: '**Autoscaling**: DLT can automatically scale your compute resources up or down
    based on demand, reducing costs and improving the performance of your pipelines.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动扩展**：DLT可以根据需求自动扩展或缩减计算资源，从而降低成本并提高管道的性能。'
- en: DLT is a good way to build reliable, scalable, and testable data pipelines.
    Next, we will focus on DLT in the context of the Bronze layer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: DLT是构建可靠、可扩展和可测试的数据管道的好方法。接下来，我们将专注于青铜层背景下的DLT。
- en: Bronze data with DLT
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DLT中的青铜数据
- en: DLT uses Auto Loader to support schema evolution, offering significant benefits
    in terms of data quality, a key aspect of the Bronze layer in a Medallion architecture.
    In this architecture, the Bronze layer serves as the foundational stage where
    raw data is initially ingested and stored. DLT contributes to this layer by ensuring
    that each transformation applied to the raw data is precisely captured and managed.
    As a data scientist, understanding these transformations is crucial for maintaining
    the integrity of the data processing workflow. A powerful feature of DLT is its
    ability to automatically generate an accurate workflow Directed Acyclic Graph
    (**DAG**). This DAG not only visualizes the sequence and relationships of these
    data transformations but also enhances the reliability of the entire data pipeline.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: DLT使用Auto Loader来支持模式演变，在数据质量方面提供了显著的好处，这是Medallion架构中青铜层的关键方面。在这个架构中，青铜层作为原始数据最初摄入和存储的基础阶段。DLT通过确保对原始数据应用的每个转换都精确捕获和管理来贡献这个层。作为数据科学家，理解这些转换对于维护数据处理工作流程的完整性至关重要。DLT的一个强大功能是能够自动生成准确的流程有向无环图（**DAG**）。这个DAG不仅可视化这些数据转换的顺序和关系，还增强了整个数据管道的可靠性。
- en: 'The following screenshot shows the DLT pipeline workflow:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图展示了DLT管道工作流程：
- en: '![Figure 3.1 – The DLT pipeline workflow for the transactional dataset in this
    chapter](img/B16865_03_01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 本章交易数据集的DLT管道工作流程](img/B16865_03_01.jpg)'
- en: Figure 3.1 – The DLT pipeline workflow for the transactional dataset in this
    chapter
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 本章交易数据集的DLT管道工作流程
- en: In *Figure 3**.1*, there is one task in the DLT workflow, but it can accommodate
    a complex workflow with multiple dependencies.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3*.1中，DLT工作流程中有一个任务，但它可以容纳具有多个依赖关系的复杂工作流程。
- en: Once we get data landed in Delta, there are actions we can take to ensure we
    take full advantage of Delta’s benefits.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据成功导入Delta，我们可以采取一些措施来确保充分利用Delta的优势。
- en: Maintaining and optimizing Delta tables
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维护和优化Delta表
- en: While the primary focus of this book is not on the intricate details of Delta
    table optimization, understanding these techniques is crucial for developing a
    data-centric machine learning solution. Efficient management of Delta tables directly
    impacts the performance and reliability of ML models, as these models heavily
    rely on the quality and accessibility of the underlying data. Employ techniques
    such as `VACUUM`, liquid clustering, `OPTIMIZE`, and bucketing to store, access,
    and manage your data with unparalleled efficiency. Optimized tables ensure that
    the data feeding into ML algorithms is processed efficiently. We’ll cover these
    briefly here, but we also suggest that you refer to the Delta Lake documentation
    for a comprehensive understanding of each technique.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本书的主要焦点不是Delta表的优化细节，但理解这些技术对于开发以数据为中心的机器学习解决方案至关重要。Delta表的效率管理直接影响ML模型的表现和可靠性，因为这些模型高度依赖于底层数据的品质和可访问性。采用诸如`VACUUM`、液态聚类、`OPTIMIZE`和分桶等技术，以无与伦比的效率存储、访问和管理您的数据。优化后的表确保输入到ML算法中的数据得到有效处理。我们在这里简要介绍这些技术，但也建议您参考Delta
    Lake文档以全面了解每种技术。
- en: VACUUM
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VACUUM
- en: The `VACUUM` command is crucial in managing resources within Delta tables. It
    works by cleaning up invalidated files and optimizing the metadata layout. If
    your Delta table undergoes frequent `update`, `insert`, `delete`, and `merge`),
    we recommend running the `VACUUM` operation periodically. DML operations can generate
    numerous small files over time. Failure to run `VACUUM` may result in several
    small files with minimal data remaining online, leading to poor performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`VACUUM`命令在管理Delta表内的资源方面至关重要。它通过清理无效文件和优化元数据布局来实现。如果您的Delta表经常进行`update`、`insert`、`delete`和`merge`操作，我们建议定期运行`VACUUM`操作。DML操作可能会随着时间的推移生成大量的小文件。未能运行`VACUUM`可能会导致在线保留大量数据量极小的小文件，从而导致性能下降。'
- en: Note that `VACUUM` doesn’t run automatically; you must explicitly schedule it.
    Consider scheduling `VACUUM` at regular intervals, such as weekly or monthly,
    depending on your data ingestion frequency and how often you update the data.
    Additionally, you have the option to configure the retention period to optimize
    storage and query performance. The `delta.logRetentionDuration` Delta configuration
    command allows you to control the retention period by specifying the number of
    days you want to keep the data files. This means you will delete all transaction
    log data for a table that goes back beyond the retention setting (e.g., only the
    last seven days of metadata remain). This way, you can control the duration the
    Delta table retains data files before the `VACUUM` operation removes them. The
    retention duration affects your ability to look back in time to previous versions
    of a table. Keep that in mind when deciding how long you want to retain the metadata
    and transaction log. Additionally, the transaction log is not very big.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`VACUUM`不会自动运行；您必须明确安排它。根据您的数据摄取频率和更新数据的频率，考虑定期安排`VACUUM`，例如每周或每月。此外，您可以选择配置保留期以优化存储和查询性能。`delta.logRetentionDuration`
    Delta配置命令允许您通过指定希望保留数据文件的天数来控制保留期。这意味着您将删除超出保留设置（例如，只保留最后七天的元数据）的表的交易日志数据。这样，您就可以控制`VACUUM`操作在删除之前保留数据文件的时间长度。保留期影响您回顾表之前版本的能力。在决定保留元数据和交易日志多长时间时，请记住这一点。此外，交易日志并不大。
- en: Liquid clustering
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 液态聚类
- en: Liquid clustering is a great alternative to partitioning; see how to implement
    it in *Figure 3**.2*. Frequently, data gets over-partitioned, leaving too few
    files in a partition or unbalanced partitions. You do not need partitioning unless
    a table is a terabyte or larger. In addition to replacing partitioning, liquid
    clustering also replaces *Z*-ordering on Delta tables. *Z*-ordering is not compatible
    with clustered tables. When choosing columns to cluster on, include high cardinality
    columns that you use to query filters. Another commonly given example is a timestamp
    column. Instead of creating a derived column date to minimize the cardinality,
    simply cluster on the timestamp. Liquid clustering benefits Delta tables with
    skewed data distribution or changing access patterns. It allows a table to adapt
    to analytical needs by redefining clustering keys without rewriting data, resulting
    in optimized query performance and a flexible, maintenance-efficient structure.
    With liquid clustering enabled, you must use DBR 13.3+ to create, write, or optimize
    Delta tables.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 液态聚类是分区的一个很好的替代方案；请参阅如何在 *图 3**.2* 中实现它。通常，数据会被过度分区，导致分区中文件过少或不平衡。除非表的大小为兆字节或更大，否则您不需要分区。除了替换分区外，液态聚类还替换了
    Delta 表上的 *Z*-排序。*Z*-排序与聚类表不兼容。在选择用于聚类的列时，包括您用于查询过滤的高基数列。另一个常见的例子是时间戳列。与其创建一个派生列日期以最小化基数，不如直接在时间戳上聚类。液态聚类为具有倾斜数据分布或变化访问模式的
    Delta 表带来好处。它允许表通过重新定义聚类键而不重写数据来适应分析需求，从而实现优化的查询性能和灵活、维护高效的架构。启用液态聚类后，您必须使用 DBR
    13.3+ 来创建、写入或优化 Delta 表。
- en: "![Figure 3.2 – Example code to create a table optim\uFEFFized with liquid clustering](img/B16865_03_02.jpg)"
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 使用液态聚类优化的表创建示例代码](img/B16865_03_02.jpg)'
- en: Figure 3.2 – Example code to create a table optimized with liquid clustering
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 使用液态聚类优化的表创建示例代码
- en: OPTIMIZE
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化
- en: The `OPTIMIZE` command will trigger clustering. This is extra important when
    streaming data, as tables are not clustered on write. `OPTIMIZE` also compacts
    data files, which is vital for Delta tables with numerous small files. It merges
    these files into larger ones, enhancing read query speed and storage efficiency,
    which is especially beneficial for large datasets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`OPTIMIZE` 命令将触发聚类。在流式传输数据时，这尤为重要，因为表在写入时不会进行聚类。`OPTIMIZE` 还会压缩数据文件，这对于具有许多小文件的
    Delta 表至关重要。它将这些文件合并成更大的文件，提高了读取查询速度和存储效率，这对于大型数据集特别有益。'
- en: Predictive optimization
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测优化
- en: Predictive optimization is a feature that runs `VACUUM` and `OPTIMIZE` for you.
    Your admin can enable it in settings if you have the premium version, serverless
    enabled, and Unity Catalog set up.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 预测优化是一个为您自动运行 `VACUUM` 和 `OPTIMIZE` 的功能。如果您有高级版本、已启用无服务器和已设置 Unity 目录，您的管理员可以在设置中启用它。
- en: Now that we’ve covered the basics of the Medallion architecture, Auto Loader,
    DLT, and some techniques to optimize your Delta tables, get ready to follow along
    in your own Databricks workspace as we work through the [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)
    code by project.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了 Medallion 架构、Auto Loader、DLT 以及一些优化 Delta 表的技术，准备好在自己的 Databricks
    工作区中跟随我们，通过项目来逐步分析 [*第 3 章*](B16865_03.xhtml#_idTextAnchor123) 的代码。
- en: Applying our learning
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用我们的学习方法
- en: You will ingest the data before diving into each project’s core “data science”
    aspects. We’ve discussed how Auto Loader, schema evolution, and DLT can format
    your data in storage. You’ll notice that the following projects use different
    patterns to load data into the Bronze layer. The streaming transactions project
    uses Auto Loader to ingest incoming JSON files. You will transform the data with
    DLT and Structure Streaming independently, allowing you to get experience with
    both methods.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入每个项目的核心“数据科学”方面之前，您将摄取数据。我们已经讨论了 Auto Loader、模式演变和 DLT 如何在存储中格式化数据。您会注意到以下项目使用不同的模式将数据加载到青铜层。流式事务项目使用
    Auto Loader 摄取传入的 JSON 文件。您将独立使用 DLT 和 Structure Streaming 转换数据，这样您就可以获得两种方法的经验。
- en: Technical requirements
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: Before we begin, review the technical requirements needed to complete the hands-on
    work in this chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，请回顾完成本章动手实践所需的技术要求。
- en: Databricks ML Runtime includes several pre-installed libraries useful for ML
    and data science projects. For this reason, we will use clusters with an ML runtime.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks ML 运行时包括几个预安装的库，这些库对 ML 和数据科学项目很有用。因此，我们将使用具有 ML 运行时的集群。
- en: Project – streaming transactions
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – 流式事务
- en: 'The next step in our streaming transactions project is to build out the Bronze
    layer, or the raw data we’ll eventually use in our classification model. We specifically
    created this streaming project to practice using Auto Loader, schema evolution,
    Spark Structured Streaming, and DLT features, so we’ll use those throughout this
    part of the project. To follow along in your workspace, open the following notebooks:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们流式事务项目的下一步是构建青铜层，也就是我们最终将在分类模型中使用的原始数据。我们特别创建了这个流式项目来练习使用 Auto Loader、模式演变、Spark
    Structured Streaming 和 DLT 功能，因此我们将在这部分项目中使用这些功能。要在你的工作区中跟进，请打开以下笔记本：
- en: '`CH3-01-Auto_Loader_and_Schema_Evolution`'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH3-01-Auto_Loader_and_Schema_Evolution`'
- en: '`CH3-02-Generating_Records_and_Schema_Change`'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH3-02-Generating_Records_and_Schema_Change`'
- en: '`delta_live_tables/CH3-03-Formatting_to_Delta_with_DLT`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delta_live_tables/CH3-03-Formatting_to_Delta_with_DLT`'
- en: 'Here’s where we are in the project flow:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们项目流程中的当前状态：
- en: '![Figure 3.3 – The project plan for the synthetic streaming transactions project](img/B16865_03_03.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 合成流式事务项目的项目计划](img/B16865_03_03.jpg)'
- en: Figure 3.3 – The project plan for the synthetic streaming transactions project
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 合成流式事务项目的项目计划
- en: In [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073), we generated and stored
    our synthetic transaction data in JSON files. Now, we will read that data into
    a Delta table. We’ll also update the data generation notebook to add a product
    string column, which will demonstrate schema evolution, as mentioned previously
    in the *Schema evolution* section. Let’s walk through two options for reading
    and writing the data stream to a Delta table. Both options use Auto Loader to
    ingest. The files are then handled and written out by either Spark Structured
    Streaming or DLT. There are two notebooks for this section. We will start with
    `CH3-01-Auto_Loader_and_Schema_Evolution`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 2 章*](B16865_02.xhtml#_idTextAnchor073) 中，我们生成并存储了我们的合成事务数据在 JSON 文件中。现在，我们将读取这些数据到
    Delta 表中。我们还将更新数据生成笔记本，添加一个产品字符串列，这将演示模式演变，如之前在 *模式演变* 部分中提到的。让我们来探讨两种将数据流读取和写入
    Delta 表的选项。两种选项都使用 Auto Loader 进行摄取。然后，文件由 Spark Structured Streaming 或 DLT 处理和写入。本节有两个笔记本。我们将从
    `CH3-01-Auto_Loader_and_Schema_Evolution` 开始。
- en: Continuous data ingestion using Auto Loader with Structured Streaming
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Structured Streaming 的 Auto Loader 进行连续数据摄取
- en: The code in this section uses Auto Loader to format the incoming JSON files
    as `Delta`. The table name we are using is `synthetic_transactions`. Before processing
    data, we create a widget (see *Figure 3**.4*) to determine whether we want to
    reset the schema and checkpoint history. Resetting the history can be helpful
    when altering or debugging pipelines. If you reset (remove) your checkpoint history,
    you will reprocess all historical data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码使用 Auto Loader 将传入的 JSON 文件格式化为 `Delta`。我们使用的表名是 `synthetic_transactions`。在处理数据之前，我们创建了一个小部件（见
    *图 3.4*）来决定我们是否想要重置模式和检查点历史。重置历史记录在修改或调试管道时可能很有帮助。如果你重置（删除）你的检查点历史，你将重新处理所有历史数据。
- en: '![Figure 3.4 – Creating a widget to reset the checkpoint and schema](img/B16865_03_04.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 创建小部件以重置检查点和模式](img/B16865_03_04.jpg)'
- en: Figure 3.4 – Creating a widget to reset the checkpoint and schema
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 创建小部件以重置检查点和模式
- en: 'Next, we set our variables for the script, which are mainly paths:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为脚本设置变量，这些变量主要是路径：
- en: '![Figure 3.5 – Setting the path variables](img/B16865_03_05.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 设置路径变量](img/B16865_03_05.jpg)'
- en: Figure 3.5 – Setting the path variables
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 设置路径变量
- en: The setup file provides us with `volume_file_path`, where we store the synthetic
    data, schema, and checkpoint folders.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 设置文件为我们提供了 `volume_file_path`，其中我们存储合成数据、模式和检查点文件夹。
- en: 'As shown in *Figure 3**.6*, we add spark configurations to optimize and reduce
    the sample size for inference:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 3.6* 所示，我们添加 spark 配置以优化和减少推理的样本大小：
- en: '![Figure 3.6 – Setting configurations to optimize and reduce the sample size](img/B16865_03_06.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 设置配置以优化和减少样本大小](img/B16865_03_06.jpg)'
- en: Figure 3.6 – Setting configurations to optimize and reduce the sample size
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 设置配置以优化和减少样本大小
- en: You can set these Spark configurations in the cluster’s advanced options. These
    configurations will automatically compact sets of small files into larger files
    for optimal read performance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在集群的高级选项中设置这些 Spark 配置。这些配置将自动将小文件集压缩成大文件，以实现最佳读取性能。
- en: Configuring stream for data ingestion
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置数据摄取的流
- en: 'The stream command is relatively long, so let’s walk through each chunk of
    the code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 流命令相对较长，所以让我们逐块分析代码：
- en: We are creating a stream to read from the synthetic dataset. The format option
    references the stream format. The `cloudFiles` refers to the files located in
    cloud storage. In this case, we generate data and write it to cloud storage. However,
    creating a stream with `.format("kafka")` is possible, which ingests directly
    from the stream without writing to cloud storage first. We also designate the
    file format as JSON.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们正在创建一个流来从合成数据集读取。格式选项引用流格式。`cloudFiles` 指的是位于云存储中的文件。在这种情况下，我们生成数据并将其写入云存储。然而，使用
    `.format("kafka")` 创建流是可能的，它可以直接从流中摄取而不先写入云存储。我们还指定文件格式为 JSON。
- en: '![Figure 3.7 – Creating the stream using CloudFiles and the JSON file format](img/B16865_03_07.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – 使用 CloudFiles 和 JSON 文件格式创建流](img/B16865_03_07.jpg)'
- en: Figure 3.7 – Creating the stream using CloudFiles and the JSON file format
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 使用 CloudFiles 和 JSON 文件格式创建流
- en: The default is to set column types to `string`. However, we can provide schema
    hints so that the columns we are sure about get typed appropriately. While inferring
    the schema, we also want to infer the column types. The new column is a `string`,
    so we do not see this option in action, as new columns default to a `string`.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，将列类型设置为 `string`。然而，我们可以提供模式提示，以便我们确信的列能够得到适当的类型。在推断模式的同时，我们还想推断列类型。新列是
    `string` 类型，因此我们看不到这个选项的实际操作，因为新列默认为 `string`。
- en: '![Figure 3.8 – Setting schema hints to reduce possible type mismatches. We
    want to infer the data type for columns not in the schema hint](img/B16865_03_08.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.8 – 设置模式提示以减少可能的数据类型不匹配。我们希望推断不在模式提示中的列的数据类型](img/B16865_03_08.jpg)'
- en: Figure 3.8 – Setting schema hints to reduce possible type mismatches. We want
    to infer the data type for columns not in the schema hint
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 设置模式提示以减少可能的数据类型不匹配。我们希望推断不在模式提示中的列的数据类型
- en: Auto Loader uses the `rescue` column to catch the change and quickly puts the
    new column into play without data loss! Note that the stream will fail and need
    to be restarted. The schema location is needed if we want Auto Loader to keep
    track of the schema and evolve it over time.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Auto Loader 使用 `rescue` 列来捕捉变化，并迅速将新列投入使用而不会丢失数据！请注意，流将失败并需要重启。如果我们希望 Auto Loader
    能够跟踪模式并在时间上进化它，则需要模式位置。
- en: '![Figure 3.9 – Setting schema evolution to add new columns and designating
    the schema location so that Auto Loader keeps track of schema changes over time](img/B16865_03_09.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9 – 设置模式进化以添加新列并指定模式位置，以便 Auto Loader 能够跟踪模式随时间的变化](img/B16865_03_09.jpg)'
- en: Figure 3.9 – Setting schema evolution to add new columns and designating the
    schema location so that Auto Loader keeps track of schema changes over time
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 设置模式进化以添加新列并指定模式位置，以便 Auto Loader 能够跟踪模式随时间的变化
- en: Next, we load the location of the raw data files. We need to select the fields
    we want. We will select all data fields, but you could be selective on the fields
    you pull in.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载原始数据文件的位置。我们需要选择我们想要的字段。我们将选择所有数据字段，但你也可以选择性地拉取字段。
- en: The following lines of code begin the “write” portion of the stream. Here, we
    start `writeStream`. Delta is the default data format, but we prefer to explicitly
    set it. We also designate that this stream is append-only, as we are not performing
    any updates or inserts.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码行开始流的“写入”部分。在这里，我们开始 `writeStream`。Delta 是默认的数据格式，但我们更喜欢明确设置它。我们还指定此流为只追加，因为我们没有执行任何更新或插入操作。
- en: A checkpoint is a mechanism in Spark Structured Streaming that allows you to
    save the state of your stream. If your stream fails, when it restarts, it uses
    the checkpoint to resume processing where it left off. The mergeSchema option
    is essential. Merging the schema adds the new columns as they arrive without intervention.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查点是 Spark Structured Streaming 中的一种机制，允许您保存流的当前状态。如果您的流失败，当它重新启动时，它将使用检查点从上次停止的地方继续处理。合并模式选项是必不可少的。合并模式会将新列添加到到达时，而无需干预。
- en: '![Figure 3.10 – Creating our checkpoint location and setting merge schema](img/B16865_03_10.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 创建我们的检查点位置并设置合并模式](img/B16865_03_10.jpg)'
- en: Figure 3.10 – Creating our checkpoint location and setting merge schema
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 创建我们的检查点位置并设置合并模式
- en: The next step is to set the trigger. The trigger setting refers to the processing
    pace and has two main modes. Per the following code, you can specify the time-based
    trigger interval in seconds. Our data from the generation notebook is continuous.
    Here, we efficiently handle it with micro-batches. We provided a select statement
    in *step 4*. We can now write the result of that statement to Delta files in the
    `destination_location` path.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是设置触发器。触发器设置指的是处理速度，有两种主要模式。根据以下代码，你可以指定基于时间的触发器间隔（以秒为单位）。我们的生成笔记本中的数据是连续的。在这里，我们通过微批有效地处理它。我们在*步骤4*中提供了一个选择语句。现在我们可以将那个语句的结果写入`destination_location`路径下的Delta文件。
- en: In *step 7*, we use a trigger with a processing time of 10 seconds. The processing
    time means micro-batches of data will be processed every 10 seconds. Suppose you
    do not require micro-batches. If you need your data processed once an hour or
    once per day, then the `trigger.availableNow` option is best. If you want to process
    whatever new data has arrived in the last hour, use `trigger.AvailableNow` in
    your pipeline, and schedule the pipeline to kick off in workflows using a job
    cluster every hour. At that time, Auto Loader will process all data available
    and then shut down.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*步骤7*中，我们使用了一个10秒的处理时间触发器。处理时间意味着每10秒将处理一次微批数据。假设你不需要微批。如果你需要每小时或每天处理一次数据，那么`trigger.availableNow`选项最佳。如果你想处理在过去一小时到达的任何新数据，请在你的管道中使用`trigger.AvailableNow`，并每小时使用作业集群启动工作流来安排管道。那时，自动加载器将处理所有可用数据然后关闭。
- en: Next, to show schema evolution, we update our data generation notebook from[*Chapter
    2*](B16865_02.xhtml#_idTextAnchor073) to include an additional `data` column.
    You’ll find the new version, `CH3-02-Generating_Records_and_Schema_Change`, in
    the [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123) folder. Note that we provide
    a list of possible products to `writeJsonFile`. The result is an additional field,
    `Product`, with a product string for the record (*Figure 3**.11*).
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，为了展示模式演化，我们将我们的数据生成笔记本从[*第2章*](B16865_02.xhtml#_idTextAnchor073)更新到包含一个额外的`data`列。你将在[*第3章*](B16865_03.xhtml#_idTextAnchor123)文件夹中找到新版本，`CH3-02-Generating_Records_and_Schema_Change`。请注意，我们为`writeJsonFile`提供了一个可能的产品列表。结果是额外的字段，`Product`，包含记录的产品字符串（*图3**.11*）。
- en: '![Figure 3.11 – The updated method for generating data with or without a product
    string](img/B16865_03_11.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11 – 生成带或不带产品字符串的数据的更新方法](img/B16865_03_11.jpg)'
- en: Figure 3.11 – The updated method for generating data with or without a product
    string
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – 生成带或不带产品字符串的数据的更新方法
- en: As a result of our changes, you now have a stream of data that adds an additional
    column midstream. You can start the Auto Loader notebook to see the schema evolution
    in action. The stream stops when it detects the extra column, providing an exception
    – `[UNKNOWN_FIELD_EXCEPTION.NEW_FIELDS_IN_RECORD_WITH_FILE_PATH]`. Don’t worry;
    upon restart, the schema evolution takes over. Run the cell with the stream in
    it again to restart.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的更改，你现在有一个数据流，在中间添加了一个额外的列。你可以启动自动加载器笔记本来查看模式演化的实际操作。当检测到额外的列时，流会停止，提供一个异常
    – `[UNKNOWN_FIELD_EXCEPTION.NEW_FIELDS_IN_RECORD_WITH_FILE_PATH]`。不用担心；重启后，模式演化将接管。再次运行包含流的单元格以重启。
- en: You are finished reading and writing with Auto Loader with Spark Structured
    Streaming. Next, we’ll show you how to accomplish the same task with DLT (using
    less code!).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经使用Spark Structured Streaming完成了读取和写入操作。接下来，我们将向你展示如何使用DLT（使用更少的代码！）完成相同的任务。
- en: Continuous data ingestion using Auto Loader with DLT
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用DLT的自动加载器进行连续数据摄取
- en: This section uses the Auto Loader code to read the stream of JSON files and
    then DLT to write the files to a table. This notebook is in the `delta_live_tables`
    folder and titled `CH3-03-Formatting_to_Delta_with_DLT`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用自动加载器代码读取JSON文件的流，然后使用DLT将文件写入表。这个笔记本位于`delta_live_tables`文件夹中，标题为`CH3-03-Formatting_to_Delta_with_DLT`。
- en: The only import you need is DLT – `import dlt`. The DLT code is relatively short,
    partly because the pipeline configuration occurs in the pipeline object rather
    than the code. Before we look through our source code, let’s navigate to the **Databricks
    Workflows** pane in the left-hand navigation bar, select **Delta Live Tables**,
    and click **Create Pipeline**, which opens the pipeline settings page, as shown
    in *Figure 3**.12*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要导入DLT – `import dlt`。DLT代码相对较短，部分原因是因为管道配置发生在管道对象中而不是代码中。在我们查看源代码之前，让我们导航到左侧导航栏中的**Databricks
    Workflows**面板，选择**Delta Live Tables**，然后点击**创建管道**，这将打开管道设置页面，如图*图3**.12*所示。
- en: "![Figure 3.12 – The DL\uFEFFT pipeline setup in the workflow UI](img/B16865_03_12.jpg)"
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: "![图3.12 – 工作流UI中的DL\uFEFFT管道设置](img/B16865_03_12.jpg)"
- en: Figure 3.12 – The DLT pipeline setup in the workflow UI
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 – 工作流UI中的DLT管道设置
- en: Pipeline configuration for DLT
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DLT的管道配置
- en: 'The pipeline settings are minimal, given that DLT does the optimization, so
    the setup instructions are minimal. The parameters entered into the pipeline configuration
    are accessible in the pipeline code using `spark.conf.get`, as shown in *Figures
    3.13* and *3.14*:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DLT执行优化，因此管道设置是最小的，所以设置说明也是最小的。输入到管道配置中的参数可以通过`spark.conf.get`在管道代码中访问，如图*3.13*和*3.14*所示：
- en: Enter a pipeline name. We will use `MLIA_Streaming_Transactions`.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入管道名称。我们将使用`MLIA_Streaming_Transactions`。
- en: For **Product edition**, select **Advanced**. This DLT edition comes with the
    most features, including DLT’s expectation rules.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于**产品版**，选择**高级**。此DLT版本包含最多功能，包括DLT的期望规则。
- en: For **Pipeline mode**, select **Triggered**. This will ensure that the pipeline
    stops processing after a successful run.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于**管道模式**，选择**触发**。这将确保管道在成功运行后停止处理。
- en: 'For **Paths**, select this notebook from the repository for the source code:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于**路径**，从存储库中选择此笔记本作为源代码：
- en: '[PRE0]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Select **Unity Catalog** as your storage option.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**Unity Catalog**作为您的存储选项。
- en: Select `ml_in_action` for the catalog and `synthetic_transactions_dlt` as the
    target schema.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于目录选择`ml_in_action`，对于目标模式选择`synthetic_transactions_dlt`。
- en: 'Enter `table_name` and `synthetic_transactions_dlt` as configurations in the
    `raw_data_location` as follows (as shown in *Figure 3**.13*):'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`raw_data_location`中将`table_name`和`synthetic_transactions_dlt`作为配置输入，如下所示（如图*3**.13*所示）：
- en: '[PRE1]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: "![Figure 3.13 – Advanced pipel\uFEFFine configuration settings for variables](img/B16865_03_13.jpg)"
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图3.13 – 变量的高级管道配置设置](img/B16865_03_13.jpg)'
- en: Figure 3.13 – Advanced pipeline configuration settings for variables
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 – 变量的高级管道配置设置
- en: Methods in the DLT pipeline
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DLT管道中的方法
- en: Now that we have filled in the pipeline UI, let’s focus on the methods used
    in the source code of the pipeline. In this function, we mostly reuse our previous
    code (*Figure 3**.14*). Note that we do not use the setup file in this notebook.
    Instead, we use the variables we set in the pipeline settings’ advanced configuration
    section.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经填写了管道UI，让我们专注于管道源代码中使用的的方法。在这个函数中，我们主要重用了我们之前的代码（*图3**.14*）。请注意，在这个笔记本中我们不使用设置文件。相反，我们使用在管道设置的高级配置部分设置的变量。
- en: "![Figure 3\uFEFF.\uFEFF14 – The autoloader stream method details](img/B16865_03_14.jpg)"
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: "![图3\uFEFF.\uFEFF14 – 自动加载流方法详情](img/B16865_03_14.jpg)"
- en: Figure 3.14 – The autoloader stream method details
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 – 自动加载流方法详情
- en: Generating the bronze table in a DLT pipeline
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在DLT管道中生成青铜表
- en: The `generate_table()` function feeds the read stream created using Auto Loader
    into DLT. We use `spark.conf.get('variable_name')` to access the variable values
    we defined in the pipeline’s advanced settings (*Figure 3**.13*). In the notebook,
    you will see the final step, a one-line cell called `generate_table()`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate_table()`函数将使用Auto Loader创建的读取流馈送到DLT。我们使用`spark.conf.get(''variable_name'')`来访问我们在管道的高级设置（*图3**.13*）中定义的变量值。在笔记本中，您将看到最终步骤，一个名为`generate_table()`的单行单元格。'
- en: '![Figure 3.15 – Generating the Bronze table in a DLT pipeline](img/B16865_03_15.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图3.15 – 在DLT管道中生成青铜表](img/B16865_03_15.jpg)'
- en: Figure 3.15 – Generating the Bronze table in a DLT pipeline
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15 – 在DLT管道中生成青铜表
- en: DLT is unique. It is not code you run line by line, so you don’t execute the
    pipeline in the notebook. Back in the DLT UI, we save and click the **Start**
    button. After the setup process, you will see a graph that includes our table.
    **Start** not only starts the pipeline but creates it as well. Once it finishes,
    you will have a screen like the one shown in *Figure 3**.1*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: DLT是独特的。它不是逐行运行的代码，因此您不需要在笔记本中执行管道。回到DLT UI，我们保存并点击**开始**按钮。设置过程完成后，您将看到一个包括我们的表的图。**开始**不仅启动了管道，还创建了它。一旦完成，您将看到一个如图*3**.1*所示的屏幕。
- en: Querying streaming tables created with DLT
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询使用DLT创建的流表
- en: You can query a streaming table created with DLT just like you query other tables.
    This is handy for populating dashboards, which we will cover in [*Chapter 8*](B16865_08.xhtml#_idTextAnchor384),
    *Monitoring, Evaluating,* *and More*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像查询其他表一样查询使用DLT创建的流表。这对于填充仪表板很有用，我们将在[*第8章*](B16865_08.xhtml#_idTextAnchor384)中介绍，*监控、评估和更多*。
- en: Important note
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'If in a notebook you try to query your streaming table, you may get this error:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在笔记本中尝试查询您的流式表，您可能会得到这个错误：
- en: '`ExecutionException: org.apache.spark.sql.AnalysisException: 403: Your token
    is missing the required scopes for` `this endpoint.`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`执行异常：org.apache.spark.sql.AnalysisException: 403：您的令牌缺少此端点所需的权限。`'
- en: This is because to query streaming tables created by a DLT pipeline, you must
    use a shared cluster using Databricks Runtime 13.1 and above, or a SQL warehouse.
    Streaming tables created in a Unity Catalog-enabled pipeline cannot be queried
    from assigned or no-isolation clusters. You can change your cluster or use the
    DBSQL query editor.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为要查询由DLT管道创建的流式表，您必须使用Databricks Runtime 13.1及以上版本的共享集群或SQL仓库。在Unity Catalog启用的管道中创建的流式表不能从分配或无隔离集群中查询。您可以更改您的集群或使用DBSQL查询编辑器。
- en: The pipeline creates our Bronze table, wrapping up this project on streaming
    transaction data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 管道创建了我们的大铜表，完成了这个关于流式事务数据的项目的收尾工作。
- en: Project – Favorita store sales – time series forecasting
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – Favorita商店销售 – 时间序列预测
- en: You downloaded the *Favorita Sales Forecasting* dataset from Kaggle using `opendatasets`
    in the last chapter. We will use that data now to create Delta tables. To follow
    along in your own workspace, open the `CH3-01-Loading_Sales_CSV_Data_as_Delta`
    notebook.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您使用`opendatasets`从Kaggle下载了*Favorita销售预测*数据集。我们现在将使用这些数据来创建Delta表。要在自己的工作区中跟随，请打开`CH3-01-Loading_Sales_CSV_Data_as_Delta`笔记本。
- en: The downloaded data is in single CSV files. We use pandas to read the datasets
    and Spark to write to a Delta table. We demonstrate this for only the first file
    in the following code block (*Figure 3**.16*).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 下载的数据是单个CSV文件。我们使用pandas读取数据集，并使用Spark写入Delta表。我们将在下面的代码块（*图3.16*）中演示这一点。
- en: '![Figure 3.16 – Reading in the sales holiday events datasets with Pandas](img/B16865_03_16.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图3.16 – 使用Pandas读取销售假日事件数据集](img/B16865_03_16.jpg)'
- en: Figure 3.16 – Reading in the sales holiday events datasets with Pandas
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – 使用Pandas读取销售假日事件数据集
- en: We utilized the data profile capability (*Figure 3**.17*) to check the data
    types before writing to a table. Profiling shows the inferred data type for the
    date field is a string rather than a date or timestamp. Therefore, we alter the
    data type before writing to a Delta table.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用数据概要功能（*图3.17*）在写入表之前检查数据类型。概要显示日期字段的推断数据类型是字符串，而不是日期或时间戳。因此，我们在写入Delta表之前更改了数据类型。
- en: '![Figure 3.17 – Utilizing display(df), we can click + to see the data profile.
    This lets us look at the data types and distributions quickly](img/B16865_03_17.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图3.17 – 使用display(df)，我们可以点击+来查看数据概要。这让我们可以快速查看数据类型和分布](img/B16865_03_17.jpg)'
- en: Figure 3.17 – Utilizing display(df), we can click + to see the data profile.
    This lets us look at the data types and distributions quickly
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 – 使用display(df)，我们可以点击+来查看数据概要。这让我们可以快速查看数据类型和分布
- en: Each table for the Favorita project is transformed into Delta tables similarly.
    As a result, we only include the first table in the book’s pages. However, the
    code that transforms each of the tables is, of course, in the repository.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Favorita项目的每个表都类似地转换为Delta表。因此，我们只在书页中包含第一个表。然而，转换每个表的代码当然在存储库中。
- en: Project – a retrieval augmented generation chatbot
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – 检索增强生成聊天机器人
- en: '**RAG** stands for **Retrieval Augmented Generation**. A RAG system often consists
    of your data, a vector database, a search algorithm, and a **generative AI** model
    to generate an answer to a user’s query. *Figure 3**.18* shows the pipeline we
    will build throughout this book.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**RAG**代表**检索增强生成**。一个RAG系统通常包括你的数据、一个向量数据库、一个搜索算法和一个**生成式AI**模型来生成对用户查询的答案。*图3.18*展示了我们将在这本书中构建的管道。'
- en: '![Figure 3.18 – A pipeline example that we will try to replicate through the
    book](img/B16865_03_18.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图3.18 – 我们将在本书中尝试复制的管道示例](img/B16865_03_18.jpg)'
- en: Figure 3.18 – A pipeline example that we will try to replicate through the book
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 – 我们将在本书中尝试复制的管道示例
- en: 'We can see from *Figure 3**.18* that we need to retrieve relevant data according
    to the user query. This is where our search method would access a vector database
    and conduct a semantic or hybrid search. At this stage in the RAG chatbot project,
    we have our PDFs in their raw, unstructured form. We want our users to access
    this knowledge via our chatbot, so we have to bring relevant content with information
    from all the PDFs into our chatbot, running in real time. To do so, we’ll convert
    all our PDFs into a machine-readable format. This chapter shows how to extract
    unstructured data from PDF files, chunk it, and transform your text into embeddings.
    Then, we store the embeddings in a Delta table:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 3.18* 我们可以看到，我们需要根据用户查询检索相关数据。这就是我们的搜索方法将访问向量数据库并执行语义或混合搜索的地方。在 RAG 聊天机器人项目的这个阶段，我们的
    PDF 文件以原始的非结构化形式存在。我们希望用户通过我们的聊天机器人访问这些知识，因此我们必须将所有 PDF 中的相关信息内容带入我们的聊天机器人中，该聊天机器人实时运行。为此，我们将所有
    PDF 转换为机器可读格式。本章展示了如何从 PDF 文件中提取非结构化数据，将其分块，并将您的文本转换为嵌入。然后，我们将嵌入存储在 Delta 表中：
- en: Our first step for data preparation is to extract unstructured information from
    PDF files. To follow along in your workspace, open the `CH3-01-Creating_EmbeddedChunks`
    notebook.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们数据准备的第一步是从 PDF 文件中提取非结构化信息。要在您的工作区中跟随，请打开 `CH3-01-Creating_EmbeddedChunks`
    笔记本。
- en: To start building the Bronze data layer, create an empty Delta table that includes
    the table’s schema. Use the `GENERATED BY DEFAULT AS IDENTITY` feature to leverage
    Delta table capabilities to index the newly arriving data automatically. Also,
    add a table property to set the `Change Data Feed` to `true`.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始构建青铜数据层，创建一个包含表模式的空 Delta 表。使用 `GENERATED BY DEFAULT AS IDENTITY` 功能利用 Delta
    表功能自动索引新到达的数据。此外，添加一个表属性以将 `Change Data Feed` 设置为 `true`。
- en: '![Figure 3.19 – Create an empty Delta table with the name pdf_documentation_text](img/B16865_03_19.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.19 – 创建名为 pdf_documentation_text 的空 Delta 表](img/B16865_03_19.jpg)'
- en: Figure 3.19 – Create an empty Delta table with the name pdf_documentation_text
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.19 – 创建名为 pdf_documentation_text 的空 Delta 表
- en: For the next step, read the raw PDFs from the `volume` folder and save them
    to a table named `pdf_raw` (*Figure 3**.20*). We will come back to the `pdf_documentation_text`
    table.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于下一步，从 `volume` 文件夹中读取原始 PDF 文件，并将其保存到名为 `pdf_raw` 的表中（*图 3.20*）。我们将在 `pdf_documentation_text`
    表中返回。
- en: '![Figure 3.20 – Read the PDF file in the binary format and write it into a
    Bronze layer table](img/B16865_03_20.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.20 – 以二进制格式读取 PDF 文件并将其写入青铜层表](img/B16865_03_20.jpg)'
- en: Figure 3.20 – Read the PDF file in the binary format and write it into a Bronze
    layer table
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20 – 以二进制格式读取 PDF 文件并将其写入青铜层表
- en: 'Store the PDFs in the binary format in the `content` column to extract the
    text later. Let’s see how it looks in a Delta table view. The binary format of
    each PDF is in the `content` column:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 PDF 存储在 `content` 列的二进制格式中，以便稍后提取文本。让我们看看它在 Delta 表视图中的样子。每个 PDF 的二进制格式都在
    `content` 列中：
- en: '![Figure 3.21 – Displaying ingested content in the Delta table](img/B16865_03_21.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.21 – 在 Delta 表中显示摄取的内容](img/B16865_03_21.jpg)'
- en: Figure 3.21 – Displaying ingested content in the Delta table
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.21 – 在 Delta 表中显示摄取的内容
- en: Next, we write a helper function using the `unstructured` library to extract
    the text from the PDF bytes. The function is in the `mlia_utils.rag_funcs` script.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `unstructured` 库编写一个辅助函数，从 PDF 字节中提取文本。该函数位于 `mlia_utils.rag_funcs`
    脚本中。
- en: '![Figure 3.22 – Creating a helper function to extract document text](img/B16865_03_22.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.22 – 创建辅助函数以提取文档文本](img/B16865_03_22.jpg)'
- en: Figure 3.22 – Creating a helper function to extract document text
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.22 – 创建辅助函数以提取文档文本
- en: 'Let’s apply this function to one of the PDF files we have and check the content
    of our document:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将此函数应用于我们拥有的其中一个 PDF 文件，并检查文档的内容：
- en: '![Figure 3.23 – Applying a helper function to extract information from the
    PDF](img/B16865_03_23.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.23 – 将辅助函数应用于从 PDF 中提取信息](img/B16865_03_23.jpg)'
- en: Figure 3.23 – Applying a helper function to extract information from the PDF
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.23 – 将辅助函数应用于从 PDF 中提取信息
- en: Great! *Figure 3**.23* gives us a glimpse into one of the ingested documents.
    You may have many PDFs, which can be very long. Long documents can potentially
    be a problem for our future chatbot because they can easily exceed most LLMs maximum
    context lengths (check out *Further reading* for more information on context lengths).
    Additionally, we likely don’t need an entire document’s worth of text to answer
    a specific question. Instead, we need a section or “chunk” of this text. For this
    project, we create chunks of no more than 500 tokens with a chunk overlap of 50,
    using the `SentenceSplitter` module of the `LlamaIndex` library. You could also
    use the `LangChain` library or any library you choose to split the content into
    chunks. We will use the open source `Llama-tokenizer`, as this will be our main
    family of models across our project. Note that tokenizers may play a crucial role
    in your RAG quality. We have leveraged a Pandas **User Defined Function** (**UDF**)
    to scale across all pages of all documents.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！*图 3.23* 让我们窥见了其中一个导入的文档。你可能有很多 PDF 文件，这些文件可能非常长。长文档可能对我们的未来聊天机器人构成潜在问题，因为它们很容易超过大多数
    LLM 的最大上下文长度（更多信息请参阅*进一步阅读*）。此外，我们可能不需要整个文档的文本来回答特定问题。相反，我们需要文本的某个部分或“数据块”。对于这个项目，我们使用
    `LlamaIndex` 库的 `SentenceSplitter` 模块创建不超过 500 个令牌的数据块，数据块重叠为 50。你也可以使用 `LangChain`
    库或任何你选择的库来将内容分割成数据块。我们将使用开源的 `Llama-tokenizer`，因为这将是我们在整个项目中的主要模型系列。请注意，分词器可能在你的
    RAG 质量中扮演关键角色。我们已经利用 Pandas **用户定义函数**（**UDF**）来跨所有文档的所有页面进行扩展。
- en: '![Figure 3.24 – Creating a pandas UDF for our extractor function](img/B16865_03_24.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.24 – 为我们的提取函数创建 pandas UDF](img/B16865_03_24.jpg)'
- en: Figure 3.24 – Creating a pandas UDF for our extractor function
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.24 – 为我们的提取函数创建 pandas UDF
- en: 'Now, let’s apply this function to our Delta table:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将此函数应用于我们的 Delta 表：
- en: '![Figure 3.25 – Applying a helper function to extract information from the
    PDF using PySpark](img/B16865_03_25.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.25 – 使用 PySpark 应用辅助函数从 PDF 中提取信息](img/B16865_03_25.jpg)'
- en: Figure 3.25 – Applying a helper function to extract information from the PDF
    using PySpark
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.25 – 使用 PySpark 应用辅助函数从 PDF 中提取信息
- en: Once our chunks are ready, we need to convert them into embeddings. Embeddings
    are the format required to perform a semantic search when ingesting into our Silver
    layer later.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的数据块准备就绪，我们需要将它们转换为嵌入。嵌入是我们将数据导入 Silver 层进行语义搜索所需的格式。
- en: Databricks Model Serving now supports **Foundation Model APIs** (**FMAPIs**),
    which allow you to access and query state-of-the-art open models from a serving
    endpoint. With FMAPIs, you can quickly and easily build applications that leverage
    a high-quality GenAI model without maintaining your own model deployment (for
    more information, see *Deploy provisioned throughput Foundation Model APIs* in
    the *Further* *reading* section).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 模型托管现在支持 **基础模型 API**（**FMAPIs**），允许你从托管端点访问和查询最先进的开放模型。使用 FMAPIs，你可以快速轻松地构建利用高质量
    GenAI 模型的应用程序，而无需维护自己的模型部署（更多信息请参阅*进一步阅读*部分中的*部署已配置吞吐量基础模型 API*）。
- en: 'FMAPIs are provided in two access modes:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: FMAPIs 提供两种访问模式：
- en: '**Pay-per-token**: This is the easiest way to start accessing foundation models
    on Databricks and is recommended for beginning your journey with them.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按令牌付费**：这是开始访问 Databricks 上的基础模型的最简单方法，并且推荐用于开始使用它们的旅程。'
- en: '**Provisioned throughput**: This model is recommended for workloads that require
    performance guarantees, fine-tuned models, or have additional security requirements:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**已配置吞吐量**：此模型推荐用于需要性能保证、精细调优的模型或具有额外安全要求的工作负载：'
- en: '![Figure 3.26 – Available models for access via the FMAPIs of Databricks](img/B16865_03_26.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.26 – 通过 Databricks 的 FMAPIs 可用的模型](img/B16865_03_26.jpg)'
- en: Figure 3.26 – Available models for access via the FMAPIs of Databricks
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.26 – 通过 Databricks 的 FMAPIs 可用的模型
- en: Important note
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: At the time of writing, the FMAPI is available only to US regions. If your workspace
    is not yet in a supported region, you can use any model of your choice (OpenAI,
    BERT, a LlaMA tokenizer, etc.) to convert your content into embeddings.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，FMAPI 仅适用于美国地区。如果你的工作区尚未在支持的地区，你可以使用任何你选择的模型（OpenAI、BERT、LlaMA 分词器等）将你的内容转换为嵌入。
- en: You may also need to fine-tune your model embedding to learn from your own content
    for better retrieval results.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还需要微调你的模型嵌入，以便从你自己的内容中学习以获得更好的检索结果。
- en: Next, we leverage the DI Platform’s pay-per-token capability from the FMAPI
    that provides you with access to the `BGE_large_En` endpoint, through the new
    functionality recently added to the `mlflow >=2.9` - `mlflow` deployments (previously
    known as AI Gateway). This functionality unifies the model serving endpoint management
    on Databricks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们利用 DI 平台的按令牌付费能力，通过 FMAPI 提供您对 `BGE_large_En` 端点的访问，这是最近添加到 `mlflow >=2.9`
    - `mlflow` 部署（以前称为 AI Gateway）的新功能。此功能统一了 Databricks 上的模型服务端点管理。
- en: '![Figure 3.27 – Applying the FMAPI with the BGE endpoint to convert “What is
    ChatGPT?” into an embedding](img/B16865_03_27.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.27 – 使用 BGE 端点应用 FMAPI 将“什么是 ChatGPT？”转换为嵌入](img/B16865_03_27.jpg)'
- en: Figure 3.27 – Applying the FMAPI with the BGE endpoint to convert “What is ChatGPT?”
    into an embedding
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.27 – 使用 BGE 端点应用 FMAPI 将“什么是 ChatGPT？”转换为嵌入
- en: Now, we apply this embedding conversion across all our chunks, and we again
    make `pandasUDF` for scalability purposes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在所有块上应用这个嵌入转换，并且再次创建 `pandasUDF` 以实现可扩展性。
- en: '![Figure 3.28 – pandasUDF to apply embedding conversion across all chunks](img/B16865_03_28.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.28 – pandasUDF 在所有块上应用嵌入转换](img/B16865_03_28.jpg)'
- en: Figure 3.28 – pandasUDF to apply embedding conversion across all chunks
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.28 – pandasUDF 在所有块上应用嵌入转换
- en: 'Applying our UDF will append our `raw_table` chunks with the corresponding
    embeddings:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 应用我们的 UDF 将会为我们的 `raw_table` 块添加相应的嵌入：
- en: '![Figure 3.29 – pandasUDF to apply embedding conversion across all chunks](img/B16865_03_29.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.29 – pandasUDF 在所有块上应用嵌入转换](img/B16865_03_29.jpg)'
- en: Figure 3.29 – pandasUDF to apply embedding conversion across all chunks
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.29 – pandasUDF 在所有块上应用嵌入转换
- en: Once the final step of our data preparation process is completed, we save our
    table in the initially pre-created Delta table `pdf_documentation_text` using
    append mode.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的数据准备过程的最后一步完成，我们使用追加模式将我们的表保存到最初预创建的 Delta 表 `pdf_documentation_text` 中。
- en: Note
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We’ve ingested our PDFs for this project in one big batch, which works perfectly
    fine as an example. However, it also means that anytime you want to add a new
    PDF to your chatbot’s knowledge base, you must manually rerun all of the preceding
    steps. We recommend a workflow for production-grade solutions to automate the
    preceding steps and incrementally ingest PDFs as they arrive in storage.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将此项目的 PDFs 一次性导入，这作为一个例子来说效果非常好。然而，这也意味着每次你想向你的聊天机器人知识库添加新的 PDF 时，你必须手动重新运行所有前面的步骤。我们建议为生产级解决方案制定工作流程来自动化前面的步骤，并随着存储中到达的
    PDFs 逐步导入。
- en: We now have a dataset ready for a vector search index, which we’ll cover in
    [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个数据集，可以用于向量搜索索引，我们将在 [*第 4 章*](B16865_04.xhtml#_idTextAnchor180) 中介绍。
- en: Project – multilabel image classification
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – 多标签图像分类
- en: 'In [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073), we extracted and stored
    our raw image data in our volume. In this chapter, we prepare our image dataset
    and save training and validation sets into Delta tables. To follow along in your
    workspace, open the `Ch3-01-Loading_Images_2_DeltaTables` notebook:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 2 章*](B16865_02.xhtml#_idTextAnchor073) 中，我们在我们的卷中提取并存储了我们的原始图像数据。在本章中，我们准备我们的图像数据集，并将训练和验证集保存到
    Delta 表中。要在您的空间中跟随，请打开 `Ch3-01-Loading_Images_2_DeltaTables` 笔记本：
- en: We start by creating variables and removing existing data if the `Reset` widget
    value is `True`.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先创建变量，如果 `Reset` 小部件的值为 `True`，则删除现有数据。
- en: '![Figure 3.30 – Cleaning up existing data if Reset = True](img/B16865_03_30.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.30 – 如果 Reset = True，则清理现有数据](img/B16865_03_30.jpg)'
- en: Figure 3.30 – Cleaning up existing data if Reset = True
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.30 – 如果 Reset = True，则清理现有数据
- en: Next, we create a function to ingest all our images in one table. Initially,
    each label is in its own `folder_label_name`. We extract `image_name`, `image_id`,
    and `label_id`, as well as create `label_name` using append mode.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个函数来将所有我们的图像放入一个表中。最初，每个标签都在其自己的 `folder_label_name` 文件夹中。我们提取 `image_name`、`image_id`
    和 `label_id`，并使用追加模式创建 `label_name`。
- en: '![Figure 3.31 – Creating the prep_data2delta function](img/B16865_03_31.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.31 – 创建 prep_data2delta 函数](img/B16865_03_31.jpg)'
- en: Figure 3.31 – Creating the prep_data2delta function
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.31 – 创建 prep_data2delta 函数
- en: We use the `prep_data2delta` function to load and prepare our training and validation
    datasets (*Figure 3**.32*). Note that the function will save a Delta table if
    the `write2delta` flag is `True` and will return a DataFrame if the value for
    the `returnDF` flag is `True`. Next, in the notebook, we call `prep_data2delta`
    for the training and validation sets.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `prep_data2delta` 函数来加载和准备我们的训练和验证数据集（*图 3**.32*）。请注意，如果 `write2delta`
    标志为 `True`，则该函数将保存一个 Delta 表，如果 `returnDF` 标志的值为 `True`，则将返回一个 DataFrame。接下来，在笔记本中，我们为训练和验证集调用
    `prep_data2delta`。
- en: Let’s talk about data loaders
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们谈谈数据加载器
- en: We load data in a fixed-size batch when fine-tuning or training our deep learning
    models. Each framework natively supports specific data types; some expand their
    native formats to other open source formats. Data scientists sometimes prefer
    to keep their data (images, in our case) in blob storage and read it directly
    from storage rather than using Delta tables, as they think this avoids additional
    work. However, we recommend storing images in a Delta table unless you are working
    with large images greater than one GB per image. Storing your images in a Delta
    table allows you to take advantage of Delta and Unity Catalog’s additional benefits,
    such as lineage of data and models, data version control, duplicate data checks,
    and quality assurance.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在微调或训练我们的深度学习模型时，我们以固定大小的批次加载数据。每个框架都原生支持特定的数据类型；一些扩展了它们的原生格式到其他开源格式。数据科学家有时更喜欢将他们的数据（在我们的例子中是图像）保存在
    blob 存储中，并直接从存储中读取，而不是使用 Delta 表，因为他们认为这样可以避免额外的工作。然而，我们建议除非您正在处理每个图像大于 1 GB 的大图像，否则将图像存储在
    Delta 表中。将您的图像存储在 Delta 表中可以让您利用 Delta 和 Unity Catalog 的额外好处，例如数据和方法血缘、数据版本控制、重复数据检查和质量保证。
- en: 'At the time of writing, you have a few options to read your data while working
    with a PyTorch or PyTorch Lightning framework:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，您在使用 PyTorch 或 PyTorch Lightning 框架处理数据时有一些读取数据的选择：
- en: '`DeltaTorchLoader` (recommended)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DeltaTorchLoader`（推荐）'
- en: Petastorm
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petastorm
- en: Reading images directly from blob/disk/volumes
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接从 blob/disk/volumes 读取图像
- en: Our recommendation is to use DeltaTorchLoader. It handles data batching, sampling,
    and multiprocessing while training PyTorch pipelines without requiring a temporary
    copy of files, like with Petastorm. See *Further reading* for more information.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的推荐是使用 DeltaTorchLoader。它在训练 PyTorch 管道时处理数据批处理、采样和多进程，而不需要像 Petastorm 那样复制临时文件。有关更多信息，请参阅*进一步阅读*。
- en: At the time of writing, DeltaTorchLoader, which we are going to use to load
    and transform our data from Delta into the PyTorch/Lightning dataloader framework
    to train our model in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297), requires
    you to have tables in the unmanaged Delta format in volumes when using UC. Don’t
    worry; the lineage is associated with the same path to the volume as your main
    dataset. We’ll talk more about lineage in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
    This requirement is due to UC’s read/write security permissions with blob storage.
    The blob storage maintainers do not support these security settings yet. If you
    are not using UC, you should be able to read Delta tables directly from the managed
    tables.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，我们将使用的 DeltaTorchLoader，用于将数据从 Delta 转换为 PyTorch/Lightning 数据加载器框架以训练我们的模型（见[*第
    6 章*](B16865_06.xhtml#_idTextAnchor297)），要求您在使用 UC 时拥有未管理的 Delta 格式的表。请放心；血缘信息与您的数据集相同的路径关联。我们将在[*第
    6 章*](B16865_06.xhtml#_idTextAnchor297)中更多地讨论血缘信息。这一要求是由于 UC 的 blob 存储的读写安全权限。blob
    存储维护者目前不支持这些安全设置。如果您不使用 UC，您应该能够直接从托管表中读取 Delta 表。
- en: There is also an option to read your data using the Petastorm library. We don’t
    recommend Petastorm because it requires a deeper understanding of certain pitfalls.
    The most common are memory usage issues due to data caching and the fact that
    it uses Apache `Parquet` files rather than Delta files, so it consumes all versions
    of your parquet files.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 Petastorm 库来读取您的数据。我们不推荐使用 Petastorm，因为它需要更深入地理解某些陷阱。最常见的是由于数据缓存导致的内存使用问题，以及它使用
    Apache `Parquet` 文件而不是 Delta 文件，因此它会消耗您所有版本的 parquet 文件。
- en: The creators of the DeltaTorchLoader performed a few benchmarks with Petastorm.
    The benchmark was shared at the **Data and AI Summit** (**DAIS**) and is featured
    in *Figure 3**.33*. In this project, we will compare Petastorm to the classic
    Torch loader in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297) to show the performance
    gain. The comparison demonstrates an incredible speed increase when reading the
    batch of data. We’ve also included a great video on *TorchDeltaLoader* in *Further
    reading* if you want to learn more.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: DeltaTorchLoader 的创建者与 Petastorm 进行了一些基准测试。该基准测试在 **数据和人工智能峰会**（**DAIS**）上分享，并在
    *图 3.33* 中展示。在本项目中，我们将比较 Petastorm 与经典 Torch 加载器在 [*第 6 章*](B16865_06.xhtml#_idTextAnchor297)
    中的性能提升。比较显示，在读取数据批次时速度有显著提升。如果您想了解更多信息，我们还在 *进一步阅读* 部分包含了一个关于 *TorchDeltaLoader*
    的精彩视频。
- en: '![Figure 3.32 – The DeltaTorchLoader benchmark – a performance comparison between
    the DeltaTorch and Petastorm loaders](img/B16865_03_32.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.32 – DeltaTorchLoader 基准测试 – DeltaTorch 和 Petastorm 加载器之间的性能比较](img/B16865_03_32.jpg)'
- en: Figure 3.32 – The DeltaTorchLoader benchmark – a performance comparison between
    the DeltaTorch and Petastorm loaders
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.32 – DeltaTorchLoader 基准测试 – DeltaTorch 和 Petastorm 加载器之间的性能比较
- en: You can keep filenames in your Delta table instead of the images and collect
    them while passing them to the main PyTorch Loader with the `trainer` function.
    Keeping files in Delta is essential to avoid duplicates and control the list used
    during training and validation, as you can pass the Delta version to MLflow during
    tracking for full replication purposes.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 Delta 表中保留文件名而不是图像，并在将它们传递给主 PyTorch 加载器时使用 `trainer` 函数进行收集。在 Delta 中保留文件对于避免重复和控制训练和验证期间使用的列表至关重要，因为您可以将
    Delta 版本传递给 MLflow 以实现完整复制目的。
- en: Optimizing our data
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化我们的数据
- en: Once our tables are created and written to the storage, we use a few functions
    to improve read performance on the Delta tables. First, we use `OPTIMIZE` to keep
    an ideal number of files (*Figure 3**.34*). Second, we disable deletion vectors
    because the `DeltaTorchReader` does not support them yet *(**Figure 3**.35*).
    We use the SQL magic command, `%sql`, to perform these operations, using SQL in
    the Python notebook.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的表创建并写入存储，我们就使用一些函数来提高 Delta 表的读取性能。首先，我们使用 `OPTIMIZE` 保持理想数量的文件（*图 3.34*）。其次，我们禁用删除向量，因为
    `DeltaTorchReader` 目前还不支持它们（*图 3.35*）。我们使用 SQL 魔法命令 `%sql` 在 Python 笔记本中执行这些操作。
- en: '![Figure 3.33 – Optimizing the file size and count of the training table](img/B16865_03_33.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.33 – 优化训练表的大小和数量](img/B16865_03_33.jpg)'
- en: Figure 3.33 – Optimizing the file size and count of the training table
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.33 – 优化训练表的大小和数量
- en: Note that the variables we saved in Python are inaccessible in SQL, so we hardcode
    them in this example. You could include SQL variables in the `global-setup` notebook
    to avoid this.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在 Python 中保存的变量在 SQL 中不可访问，因此在这个例子中我们将其硬编码。您可以在 `global-setup` 笔记本中包含 SQL
    变量以避免这种情况。
- en: '![Figure 3.34 – Optimizing the training table](img/B16865_03_34.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.34 – 优化训练表](img/B16865_03_34.jpg)'
- en: Figure 3.34 – Optimizing the training table
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.34 – 优化训练表
- en: Now, we have the training and validation tables loaded and optimized to efficiently
    work with this image data to fine-tune our multi-class computer vision models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经加载并优化了训练和验证表，以便高效地处理这些图像数据以微调我们的多类计算机视觉模型。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we focused on the essentials of building out the Bronze data
    layer within the Databricks Data Intelligence Platform. We emphasized the importance
    of schema evolution, DLT, and the conversion of data into the Delta format and
    applied these principles in our example projects. This chapter highlighted the
    significance of tools such as Auto Loader and DLT in this process. Auto Loader,
    with its proficiency in handling file tracking and automating schema management,
    alongside DLT’s robust capabilities in pipeline development and data quality assurance,
    are pivotal in our data management strategy. These tools facilitate an efficient
    and streamlined approach to data pipeline management, enabling us as data scientists
    to focus more on valuable tasks, such as feature engineering and experimentation.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于在Databricks数据智能平台内构建青铜数据层的基本要素。我们强调了模式演变、DLT以及将数据转换为Delta格式的重要性，并在我们的示例项目中应用了这些原则。本章强调了Auto
    Loader和DLT等工具在此过程中的重要性。Auto Loader在处理文件跟踪和自动化模式管理方面的熟练程度，以及DLT在管道开发和数据质量保证方面的强大能力，对我们数据管理策略至关重要。这些工具促进了数据管道管理的有效和简化方法，使我们作为数据科学家能够更多地专注于有价值的任务，如特征工程和实验。
- en: With our Bronze layer created, we now move on from this foundational work to
    a more advanced layer of data – the Silver layer. [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180),
    *Transformations toward Our Silver Layer*, will take us deeper into our data and
    demonstrate various Databricks tools that will aid us in the exploration and transformations
    of our data.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了青铜层之后，我们现在从这项基础工作转向更高级的数据层——银层。[*第4章*](B16865_04.xhtml#_idTextAnchor180)，*向银层转变的转换*，将带我们更深入地了解我们的数据，并展示各种Databricks工具，这些工具将帮助我们探索和转换我们的数据。
- en: Questions
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'The following questions solidify key points to remember and tie the content
    back to your experience:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 以下问题有助于巩固需要记住的关键点，并将内容与你的经验联系起来：
- en: What are the names of the layers in the Medallion architecture design?
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Medallion架构设计中的层名称是什么？
- en: If you wanted to build a managed pipeline with streaming data, which product
    would you use – Structured Streaming or DLT?
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想要构建一个使用流数据的托管管道，你会使用哪个产品——结构化流或DLT？
- en: What feature did we use to add the `product` column to our streaming transaction
    data without manual intervention?
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用了什么特性来在不进行人工干预的情况下将`product`列添加到我们的流式事务数据中？
- en: Do you have projects from your current position, experience, or on your roadmap
    that would benefit from one or more of the topics covered in this chapter?
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是否有来自你当前职位、经验或路线图的项目，可以从本章涵盖的一个或多个主题中受益？
- en: What is a possible way to lessen the number of partitions when partitioning
    on a high cardinality column?
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对高基数列进行分区时，减少分区数量的可能方法是什么？
- en: Answers
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: 'After putting thought into the questions, compare your answers to ours:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在思考这些问题后，将你的答案与我们的答案进行比较：
- en: The layers of the Medallion architecture are Bronze, Silver, and Gold.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Medallion架构的层是青铜、银和金。
- en: We recommend DLT build managed pipelines.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们建议使用DLT构建托管管道。
- en: In the streaming transactions project example, we used Auto Loader’s schema
    evolution feature to add a column without manual intervention.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在流式事务项目示例中，我们使用了Auto Loader的模式演变功能来添加列，而不进行人工干预。
- en: We hope so! One example is a managed streaming data pipeline that could benefit
    from the built-in data quality monitoring that comes with DLT.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望如此！一个例子是受益于DLT内置数据质量监控的托管流数据管道。
- en: Bucketing is an optimal method specifically designed to provide an additional
    layer of organization in your data. It can reduce the number of output files and
    organize the data better for subsequent reading, and it can be especially useful
    when the partitioning column has high cardinality.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分桶是一种专门设计来为你的数据提供额外组织层的优化方法。它可以减少输出文件的数量，并更好地组织数据以便后续读取，当分区列具有高基数时，它尤其有用。
- en: Further reading
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'This chapter covered different methods of ingesting data into your Bronze layer.
    Take a look at these resources to read more about the areas that interest you
    most:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了将数据导入青铜层的不同方法。查看这些资源以了解更多关于你最感兴趣领域的相关信息：
- en: '*Use liquid clustering for Delta* *tables*:[https://docs.databricks.com/en/delta/clustering.html](https://docs.databricks.com/en/delta/clustering.html)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用液体聚类对Delta* *表*进行聚类：[https://docs.databricks.com/en/delta/clustering.html](https://docs.databricks.com/en/delta/clustering.html)'
- en: '*Spark Structured* *Streaming*: [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Spark Structured* *Streaming*：[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)'
- en: '*Delta Live* *Tables*: [https://docs.databricks.com/en/delta-live-tables/index.html](https://docs.databricks.com/en/delta-live-tables/index.html)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Delta Live* *Tables*：[https://docs.databricks.com/en/delta-live-tables/index.html](https://docs.databricks.com/en/delta-live-tables/index.html)'
- en: '*DLT Databricks* *Demo*:[https://www.databricks.com/resources/demos/tutorials/lakehouse-platform/full-delta-live-table-pipeline](https://www.databricks.com/resources/demos/tutorials/lakehouse-platform/full-delta-live-table-pipeline)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DLT Databricks* *演示*：[https://www.databricks.com/resources/demos/tutorials/lakehouse-platform/full-delta-live-table-pipeline](https://www.databricks.com/resources/demos/tutorials/lakehouse-platform/full-delta-live-table-pipeline)'
- en: '*Auto Loader* *options*: [https://docs.databricks.com/ingestion/auto-loader/options.html](https://docs.databricks.com/ingestion/auto-loader/options.html)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Auto Loader* *选项*：[https://docs.databricks.com/ingestion/auto-loader/options.html](https://docs.databricks.com/ingestion/auto-loader/options.html)'
- en: '*Schema evolution with Auto* *Loader*: [https://docs.databricks.com/ingestion/auto-loader/schema.html#configure-schema-inference-and-evolution-in-auto-loader](https://docs.databricks.com/ingestion/auto-loader/schema.html#configure-schema-inference-and-evolution-in-auto-loader)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用Auto* *Loader*进行模式演变：[https://docs.databricks.com/ingestion/auto-loader/schema.html#configure-schema-inference-and-evolution-in-auto-loader](https://docs.databricks.com/ingestion/auto-loader/schema.html#configure-schema-inference-and-evolution-in-auto-loader)'
- en: '*Common loading patterns with Auto* *Loader*: [https://docs.databricks.com/ingestion/auto-loader/patterns.html](https://docs.databricks.com/ingestion/auto-loader/patterns.html)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用Auto* *Loader*的常见加载模式：[https://docs.databricks.com/ingestion/auto-loader/patterns.html](https://docs.databricks.com/ingestion/auto-loader/patterns.html)'
- en: '*Stream processing with Apache Kafka and* *Databricks*:[https://docs.databricks.com/structured-streaming/kafka.html](https://docs.databricks.com/structured-streaming/kafka.html)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用Apache Kafka和* *Databricks*进行流处理：[https://docs.databricks.com/structured-streaming/kafka.html](https://docs.databricks.com/structured-streaming/kafka.html)'
- en: '*How We Performed ETL on One Billion Records For Under $1 With Delta Live*
    *Tables*: [https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html](https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何在Delta Live* *Tables*中以低于1美元的成本处理十亿条记录：[https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html](https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html)'
- en: '*Create tables – Managed vs* *External*: [https://docs.databricks.com/en/data-governance/unity-catalog/create-tables.html#create-tables](https://docs.databricks.com/en/data-governance/unity-catalog/create-tables.html#create-tables)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创建表 – 管理与* *外部*：[https://docs.databricks.com/en/data-governance/unity-catalog/create-tables.html#create-tables](https://docs.databricks.com/en/data-governance/unity-catalog/create-tables.html#create-tables)'
- en: '*Take full advantage of the auto-tuning* *available*:[https://docs.databricks.com/delta/tune-file-size.html#configure-delta-lake-to-control-data-file-size](https://docs.databricks.com/delta/tune-file-size.html#configure-delta-lake-to-control-data-file-size)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*充分利用可用的自动调整* *功能*：[https://docs.databricks.com/delta/tune-file-size.html#configure-delta-lake-to-control-data-file-size](https://docs.databricks.com/delta/tune-file-size.html#configure-delta-lake-to-control-data-file-size)'
- en: 'Import Python modules from Databricks repos: [https://docs.databricks.com/en/delta-live-tables/import-workspace-files.html](https://docs.databricks.com/en/delta-live-tables/import-workspace-files.html)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Databricks仓库导入Python模块：[https://docs.databricks.com/en/delta-live-tables/import-workspace-files.html](https://docs.databricks.com/en/delta-live-tables/import-workspace-files.html)
- en: 'Deletion Vectors: [https://docs.databricks.com/en/delta/deletion-vectors.html](https://docs.databricks.com/en/delta/deletion-vectors.html)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除向量：[https://docs.databricks.com/en/delta/deletion-vectors.html](https://docs.databricks.com/en/delta/deletion-vectors.html)
- en: '*Databricks ML* *Runtime*:[https://docs.databricks.com/runtime/mlruntime.html#introduction-to-databricks-runtime-for-machine-learning](https://docs.databricks.com/runtime/mlruntime.html#introduction-to-databricks-runtime-for-machine-learning)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks ML* *运行时*：[https://docs.databricks.com/runtime/mlruntime.html#introduction-to-databricks-runtime-for-machine-learning](https://docs.databricks.com/runtime/mlruntime.html#introduction-to-databricks-runtime-for-machine-learning)'
- en: '*Cluster advanced* *options*:[https://docs.databricks.com/en/clusters/configure.html#spark-configuration](https://docs.databricks.com/en/clusters/configure.html#spark-configuration)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集群高级* *选项*：[https://docs.databricks.com/en/clusters/configure.html#spark-configuration](https://docs.databricks.com/en/clusters/configure.html#spark-configuration)'
- en: '*Deploy provisioned throughput Foundation Model* *APIs*:[https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html](https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*部署配置吞吐量基础模型* *APIs*：[https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html](https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html)'
- en: '*Scaling Deep Learning Using Delta Lake Storage Format on* *Databricks*: [https://www.databricks.com/dataaisummit/session/scaling-deep-learning-using-delta-lake-storage-format-databricks/](https://www.databricks.com/dataaisummit/session/scaling-deep-learning-using-delta-lake-storage-format-databricks/)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在Databricks上使用Delta Lake存储格式扩展深度学习*：[https://www.databricks.com/dataaisummit/session/scaling-deep-learning-using-delta-lake-storage-format-databricks/](https://www.databricks.com/dataaisummit/session/scaling-deep-learning-using-delta-lake-storage-format-databricks/)'
- en: '*DeltaTorchLoader*: [https://github.com/delta-incubator/deltatorch](https://github.com/delta-incubator/deltatorch)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DeltaTorchLoader*：[https://github.com/delta-incubator/deltatorch](https://github.com/delta-incubator/deltatorch)'
- en: 'Part 2: Heavily Use Case-Focused'
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：高度关注用例
- en: This part introduces you to taking a set of data sources and working with them
    throughout the platform, from one end to another. The goal of this part is simply
    to demonstrate how to thoughtfully use all the bells and whistles of the platform.
    This part provides stories, code, lakehouse features, and best practices.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分向您介绍如何从数据源集合开始，并在整个平台上与之协作，从一端到另一端。本部分的目标仅仅是展示如何深思熟虑地使用平台的所有功能和特性。本部分提供了案例故事、代码、湖仓功能以及最佳实践。
- en: 'This part has the following chapters:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 4*](B16865_04.xhtml#_idTextAnchor180), *Getting to Know Your Data*'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B16865_04.xhtml#_idTextAnchor180), *了解您的数据*'
- en: '[*Chapter 5*](B16865_05.xhtml#_idTextAnchor244), *Feature Engineering on Databricks*'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B16865_05.xhtml#_idTextAnchor244), *在Databricks上执行特征工程*'
- en: '[*Chapter 6*](B16865_06.xhtml#_idTextAnchor297), *Searching for a Signal*'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B16865_06.xhtml#_idTextAnchor297), *寻找信号*'
- en: '[*Chapter 7*](B16865_07.xhtml#_idTextAnchor325), *Productionizing ML on Databricks*'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B16865_07.xhtml#_idTextAnchor325), *在Databricks上生产化机器学习*'
- en: '[*Chapter 8*](B16865_08.xhtml#_idTextAnchor384), *Monitoring, Evaluating, and
    More*'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B16865_08.xhtml#_idTextAnchor384), *监控、评估及其他*'
