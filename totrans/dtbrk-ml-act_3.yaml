- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Out Our Bronze Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Data is a precious thing and will last longer than the systems themselves.”
  prefs: []
  type: TYPE_NORMAL
- en: – Tim Berners-Lee, generally credited as the inventor of the World Wide Web
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll embark on the beginning of your data journey in the
    Databricks platform, exploring the fundamentals of the Bronze layer. We recommend
    employing the Medallion design pattern within the lake house architecture (as
    described in [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073)) to organize your
    data. We’ll start with **Auto Loader**, which you can implement with or without
    **Delta Live Tables** (**DLT**) to insert and transform data in your architecture.
    The benefits of using Auto Loader include quickly transforming new data into the
    Delta format and enforcing or evolving schemas, which are essential for maintaining
    consistent data delivery to the business and customers. As a data scientist, strive
    for efficiency in building your data pipelines and ensuring your data is ready
    for the steps in the machine learning development cycle. You will best learn these
    topics through the example projects, so the *Applying our learning* section is
    the main focus of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what topics you will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the Medallion architecture pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming data to Delta with Auto Loader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DLT, starting with Bronze
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintaining and optimizing Delta Tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting the Medallion architecture pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced the Medallion architecture in [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073).
    As a reminder, this refers to the data design pattern used to organize data logically.
    It has three layers – Bronze, Silver, and Gold. There are also cases where additional
    levels of refinement are required, so your Medallion architecture could be extended
    to Diamond and Platinum levels if needed. The Bronze layer contains raw data,
    the Silver layer contains cleaned and transformed data, and the Gold layer contains
    aggregated and curated data. Curated data refers to the datasets selected, cleaned,
    and organized for a specific business or modeling purpose. This architecture is
    a good fit for data science projects. Maintaining the original data as a source
    of truth is important, while curated data is valuable for research, analytics,
    and machine learning applications. By selecting, cleaning, and organizing data
    for a specific purpose, curated data can help improve its accuracy, relevance,
    and usability.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This book will introduce quite a few project tasks, such as *building curated
    datasets*, that often fall under the domain of a data engineer. There will also
    be tasks that machine learning engineers, data analysts, and so on commonly perform.
    We include all of this work in our examples because roles are blurred in today’s
    fast-moving world and will vary by company. Titles and expectations quickly evolve.
    Therefore, it’s imperative to have a handle on the entire end-to-end workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining a Bronze layer allows us to go back to the original data when we
    want to create a different feature, solve a new problem that requires us to look
    at historical data from another point of view, or simply maintain the raw level
    of our data for governance purposes. As the world of technology evolves, it’s
    crucial to stay current and follow trends, but the core principles will remain
    for years. For the remainder of this chapter, we will cover DI platform features
    that facilitate building the Bronze layer.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data to Delta with Auto Loader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Harness the power of Auto Loader to automate your data ingestion process, significantly
    enhancing your data product’s workflow efficiency. It can ingest data from cloud
    storage and streaming data sources. You can configure Auto Loader to run on a
    schedule or be triggered manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some benefits of using Databricks’ Auto Loader:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It keeps data up to date**: Auto Loader maintains checkpoints, removing the
    need to know which data is new. Auto Loader handles all that on its own.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It improves data quality**: Auto Loader can automatically detect schema changes
    and rescue any new data columns, so you can be confident that your data is accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It increases data agility**: Auto Loader can help you quickly and easily
    ingest new data sources so that you can be more agile in responding to changes
    in your business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible ingestion**: Auto Loader can stream files in batches or continuously.
    This means it can consume batch data as a stream to reduce the overhead of a more
    manual batch pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto Loader is a powerful tool. It can be used standalone, as the underlying
    technology for DLT, or with **Spark Structured Streaming**. Spark Structured Streaming
    is a near-real-time processing engine; we will cover how to create a real-time
    feature in [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244). This chapter also
    covers an example of streaming ingestion in the streaming transactions project.
    Let’s discuss Auto Loader’s ability to evolve a schema over time.
  prefs: []
  type: TYPE_NORMAL
- en: Schema evolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Auto Loader’s schema evolution allows you to add or modify fields in streaming
    data seamlessly. Databricks automatically adjusts relevant data to fit the new
    schema while preserving existing data integrity. This automated schema handling
    makes it easy to evolve your data schema over time as your business needs and
    data sources change, without having to worry about data loss or downtime. Furthermore,
    schema handling reduces the amount of work and headache associated with managing
    data pipelines that could change unexpectedly.
  prefs: []
  type: TYPE_NORMAL
- en: The new schema includes an additional column in the *Applying our learning*
    section. We will show how no data is lost despite a schema changing without notice.
    In our case, the default schema evolution mode for Auto Loader, `.option("cloudFiles.schemaEvolutionMode",
    "addNewColumns")`, in combination with `.option("mergeSchema", "true")`, perform
    schema evolution. Auto Loader will handle changes in the schema for us. Auto Loader
    tracking the changes is beneficial when new data fields become available with
    or without prior notice; code changes are unnecessary.
  prefs: []
  type: TYPE_NORMAL
- en: There’s documentation on all schema options. The method we use for our project
    is the only option for automatic schema evolution. Every other option will require
    manual intervention. For example, you can use “rescue” mode to rescue data from
    being lost. Alternatively, you can use “failOnNewColumns” mode to cause the pipeline
    to fail and keep the schema unchanged until the production code is updated. There
    are numerous options and patterns for Auto Loader. Check out the *Common loading
    patterns with Auto Loader* link in the *Further reading* section for more information.
  prefs: []
  type: TYPE_NORMAL
- en: DLT, starting with Bronze
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we touched upon in a previous chapter, remember that DLT actively simplifies
    your pipeline operations, empowering you to focus on setting clear objectives
    for your pipeline rather than getting bogged down in operational details. Building
    on this foundation, we will now delve into DLT’s capabilities. Auto Loader’s schema
    evolution is integrated with DLT, underscoring its utility in handling dynamic
    data schemas with minimal manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: DLT benefits and features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DLT is a sophisticated framework designed to construct reliable data pipelines.
    DLT automates and streamlines complex operations such as orchestration and cluster
    management, significantly boosting the efficiency of data workflows. All you need
    to do is specify your transformation logic to be up and running. We will focus
    on just a few of the benefits of DLT as they pertain to creating your Bronze data
    layer, but we’ve included a link to DLT documentation in the *Further reading*
    section at the end of this chapter as well. DLT is another great tool in your
    toolbox for ingesting data. It provides several benefits over traditional ETL
    pipelines, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Declarative pipeline development**: DLT allows you to define your data pipelines
    using SQL or Python, which makes them easier to understand and maintain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic data quality testing**: DLT can automatically apply your tests
    to your data, preventing quality issues, which helps to ensure that your pipelines
    are producing accurate results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep visibility for monitoring and recovery**: DLT provides detailed tracking
    and logging information, making troubleshooting problems and recovering from failures
    easier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-effective streaming through efficient compute autoscaling**: DLT can
    automatically scale your compute resources up or down based on demand, which helps
    to reduce costs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DLT is a powerful tool for building reliable and scalable data pipelines in
    batch or streaming. It can help you improve the quality, efficiency, and visibility
    of your data processing workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the specific features of DLT that provide these benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Streaming tables**: DLT uses streaming and live tables to process data in
    real time and to keep your pipelines up to date with the latest data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Materialized views**: DLT uses materialized views to create snapshots of
    your data. You can query your data in real time and use it for downstream processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expectations**: DLT uses expectations to test your data for quality issues
    automatically and to take action if the data does not meet your expectations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoscaling**: DLT can automatically scale your compute resources up or down
    based on demand, reducing costs and improving the performance of your pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DLT is a good way to build reliable, scalable, and testable data pipelines.
    Next, we will focus on DLT in the context of the Bronze layer.
  prefs: []
  type: TYPE_NORMAL
- en: Bronze data with DLT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DLT uses Auto Loader to support schema evolution, offering significant benefits
    in terms of data quality, a key aspect of the Bronze layer in a Medallion architecture.
    In this architecture, the Bronze layer serves as the foundational stage where
    raw data is initially ingested and stored. DLT contributes to this layer by ensuring
    that each transformation applied to the raw data is precisely captured and managed.
    As a data scientist, understanding these transformations is crucial for maintaining
    the integrity of the data processing workflow. A powerful feature of DLT is its
    ability to automatically generate an accurate workflow Directed Acyclic Graph
    (**DAG**). This DAG not only visualizes the sequence and relationships of these
    data transformations but also enhances the reliability of the entire data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the DLT pipeline workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The DLT pipeline workflow for the transactional dataset in this
    chapter](img/B16865_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – The DLT pipeline workflow for the transactional dataset in this
    chapter
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.1*, there is one task in the DLT workflow, but it can accommodate
    a complex workflow with multiple dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Once we get data landed in Delta, there are actions we can take to ensure we
    take full advantage of Delta’s benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining and optimizing Delta tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the primary focus of this book is not on the intricate details of Delta
    table optimization, understanding these techniques is crucial for developing a
    data-centric machine learning solution. Efficient management of Delta tables directly
    impacts the performance and reliability of ML models, as these models heavily
    rely on the quality and accessibility of the underlying data. Employ techniques
    such as `VACUUM`, liquid clustering, `OPTIMIZE`, and bucketing to store, access,
    and manage your data with unparalleled efficiency. Optimized tables ensure that
    the data feeding into ML algorithms is processed efficiently. We’ll cover these
    briefly here, but we also suggest that you refer to the Delta Lake documentation
    for a comprehensive understanding of each technique.
  prefs: []
  type: TYPE_NORMAL
- en: VACUUM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `VACUUM` command is crucial in managing resources within Delta tables. It
    works by cleaning up invalidated files and optimizing the metadata layout. If
    your Delta table undergoes frequent `update`, `insert`, `delete`, and `merge`),
    we recommend running the `VACUUM` operation periodically. DML operations can generate
    numerous small files over time. Failure to run `VACUUM` may result in several
    small files with minimal data remaining online, leading to poor performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `VACUUM` doesn’t run automatically; you must explicitly schedule it.
    Consider scheduling `VACUUM` at regular intervals, such as weekly or monthly,
    depending on your data ingestion frequency and how often you update the data.
    Additionally, you have the option to configure the retention period to optimize
    storage and query performance. The `delta.logRetentionDuration` Delta configuration
    command allows you to control the retention period by specifying the number of
    days you want to keep the data files. This means you will delete all transaction
    log data for a table that goes back beyond the retention setting (e.g., only the
    last seven days of metadata remain). This way, you can control the duration the
    Delta table retains data files before the `VACUUM` operation removes them. The
    retention duration affects your ability to look back in time to previous versions
    of a table. Keep that in mind when deciding how long you want to retain the metadata
    and transaction log. Additionally, the transaction log is not very big.
  prefs: []
  type: TYPE_NORMAL
- en: Liquid clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Liquid clustering is a great alternative to partitioning; see how to implement
    it in *Figure 3**.2*. Frequently, data gets over-partitioned, leaving too few
    files in a partition or unbalanced partitions. You do not need partitioning unless
    a table is a terabyte or larger. In addition to replacing partitioning, liquid
    clustering also replaces *Z*-ordering on Delta tables. *Z*-ordering is not compatible
    with clustered tables. When choosing columns to cluster on, include high cardinality
    columns that you use to query filters. Another commonly given example is a timestamp
    column. Instead of creating a derived column date to minimize the cardinality,
    simply cluster on the timestamp. Liquid clustering benefits Delta tables with
    skewed data distribution or changing access patterns. It allows a table to adapt
    to analytical needs by redefining clustering keys without rewriting data, resulting
    in optimized query performance and a flexible, maintenance-efficient structure.
    With liquid clustering enabled, you must use DBR 13.3+ to create, write, or optimize
    Delta tables.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.2 – Example code to create a table optim\uFEFFized with liquid clustering](img/B16865_03_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Example code to create a table optimized with liquid clustering
  prefs: []
  type: TYPE_NORMAL
- en: OPTIMIZE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `OPTIMIZE` command will trigger clustering. This is extra important when
    streaming data, as tables are not clustered on write. `OPTIMIZE` also compacts
    data files, which is vital for Delta tables with numerous small files. It merges
    these files into larger ones, enhancing read query speed and storage efficiency,
    which is especially beneficial for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Predictive optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Predictive optimization is a feature that runs `VACUUM` and `OPTIMIZE` for you.
    Your admin can enable it in settings if you have the premium version, serverless
    enabled, and Unity Catalog set up.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the basics of the Medallion architecture, Auto Loader,
    DLT, and some techniques to optimize your Delta tables, get ready to follow along
    in your own Databricks workspace as we work through the [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)
    code by project.
  prefs: []
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will ingest the data before diving into each project’s core “data science”
    aspects. We’ve discussed how Auto Loader, schema evolution, and DLT can format
    your data in storage. You’ll notice that the following projects use different
    patterns to load data into the Bronze layer. The streaming transactions project
    uses Auto Loader to ingest incoming JSON files. You will transform the data with
    DLT and Structure Streaming independently, allowing you to get experience with
    both methods.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we begin, review the technical requirements needed to complete the hands-on
    work in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks ML Runtime includes several pre-installed libraries useful for ML
    and data science projects. For this reason, we will use clusters with an ML runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Project – streaming transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step in our streaming transactions project is to build out the Bronze
    layer, or the raw data we’ll eventually use in our classification model. We specifically
    created this streaming project to practice using Auto Loader, schema evolution,
    Spark Structured Streaming, and DLT features, so we’ll use those throughout this
    part of the project. To follow along in your workspace, open the following notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH3-01-Auto_Loader_and_Schema_Evolution`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH3-02-Generating_Records_and_Schema_Change`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delta_live_tables/CH3-03-Formatting_to_Delta_with_DLT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s where we are in the project flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – The project plan for the synthetic streaming transactions project](img/B16865_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – The project plan for the synthetic streaming transactions project
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073), we generated and stored
    our synthetic transaction data in JSON files. Now, we will read that data into
    a Delta table. We’ll also update the data generation notebook to add a product
    string column, which will demonstrate schema evolution, as mentioned previously
    in the *Schema evolution* section. Let’s walk through two options for reading
    and writing the data stream to a Delta table. Both options use Auto Loader to
    ingest. The files are then handled and written out by either Spark Structured
    Streaming or DLT. There are two notebooks for this section. We will start with
    `CH3-01-Auto_Loader_and_Schema_Evolution`.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous data ingestion using Auto Loader with Structured Streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code in this section uses Auto Loader to format the incoming JSON files
    as `Delta`. The table name we are using is `synthetic_transactions`. Before processing
    data, we create a widget (see *Figure 3**.4*) to determine whether we want to
    reset the schema and checkpoint history. Resetting the history can be helpful
    when altering or debugging pipelines. If you reset (remove) your checkpoint history,
    you will reprocess all historical data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Creating a widget to reset the checkpoint and schema](img/B16865_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Creating a widget to reset the checkpoint and schema
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we set our variables for the script, which are mainly paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Setting the path variables](img/B16865_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Setting the path variables
  prefs: []
  type: TYPE_NORMAL
- en: The setup file provides us with `volume_file_path`, where we store the synthetic
    data, schema, and checkpoint folders.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 3**.6*, we add spark configurations to optimize and reduce
    the sample size for inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Setting configurations to optimize and reduce the sample size](img/B16865_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Setting configurations to optimize and reduce the sample size
  prefs: []
  type: TYPE_NORMAL
- en: You can set these Spark configurations in the cluster’s advanced options. These
    configurations will automatically compact sets of small files into larger files
    for optimal read performance.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring stream for data ingestion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The stream command is relatively long, so let’s walk through each chunk of
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: We are creating a stream to read from the synthetic dataset. The format option
    references the stream format. The `cloudFiles` refers to the files located in
    cloud storage. In this case, we generate data and write it to cloud storage. However,
    creating a stream with `.format("kafka")` is possible, which ingests directly
    from the stream without writing to cloud storage first. We also designate the
    file format as JSON.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Creating the stream using CloudFiles and the JSON file format](img/B16865_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Creating the stream using CloudFiles and the JSON file format
  prefs: []
  type: TYPE_NORMAL
- en: The default is to set column types to `string`. However, we can provide schema
    hints so that the columns we are sure about get typed appropriately. While inferring
    the schema, we also want to infer the column types. The new column is a `string`,
    so we do not see this option in action, as new columns default to a `string`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Setting schema hints to reduce possible type mismatches. We
    want to infer the data type for columns not in the schema hint](img/B16865_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Setting schema hints to reduce possible type mismatches. We want
    to infer the data type for columns not in the schema hint
  prefs: []
  type: TYPE_NORMAL
- en: Auto Loader uses the `rescue` column to catch the change and quickly puts the
    new column into play without data loss! Note that the stream will fail and need
    to be restarted. The schema location is needed if we want Auto Loader to keep
    track of the schema and evolve it over time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Setting schema evolution to add new columns and designating
    the schema location so that Auto Loader keeps track of schema changes over time](img/B16865_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Setting schema evolution to add new columns and designating the
    schema location so that Auto Loader keeps track of schema changes over time
  prefs: []
  type: TYPE_NORMAL
- en: Next, we load the location of the raw data files. We need to select the fields
    we want. We will select all data fields, but you could be selective on the fields
    you pull in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following lines of code begin the “write” portion of the stream. Here, we
    start `writeStream`. Delta is the default data format, but we prefer to explicitly
    set it. We also designate that this stream is append-only, as we are not performing
    any updates or inserts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A checkpoint is a mechanism in Spark Structured Streaming that allows you to
    save the state of your stream. If your stream fails, when it restarts, it uses
    the checkpoint to resume processing where it left off. The mergeSchema option
    is essential. Merging the schema adds the new columns as they arrive without intervention.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Creating our checkpoint location and setting merge schema](img/B16865_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Creating our checkpoint location and setting merge schema
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to set the trigger. The trigger setting refers to the processing
    pace and has two main modes. Per the following code, you can specify the time-based
    trigger interval in seconds. Our data from the generation notebook is continuous.
    Here, we efficiently handle it with micro-batches. We provided a select statement
    in *step 4*. We can now write the result of that statement to Delta files in the
    `destination_location` path.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In *step 7*, we use a trigger with a processing time of 10 seconds. The processing
    time means micro-batches of data will be processed every 10 seconds. Suppose you
    do not require micro-batches. If you need your data processed once an hour or
    once per day, then the `trigger.availableNow` option is best. If you want to process
    whatever new data has arrived in the last hour, use `trigger.AvailableNow` in
    your pipeline, and schedule the pipeline to kick off in workflows using a job
    cluster every hour. At that time, Auto Loader will process all data available
    and then shut down.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, to show schema evolution, we update our data generation notebook from[*Chapter
    2*](B16865_02.xhtml#_idTextAnchor073) to include an additional `data` column.
    You’ll find the new version, `CH3-02-Generating_Records_and_Schema_Change`, in
    the [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123) folder. Note that we provide
    a list of possible products to `writeJsonFile`. The result is an additional field,
    `Product`, with a product string for the record (*Figure 3**.11*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.11 – The updated method for generating data with or without a product
    string](img/B16865_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – The updated method for generating data with or without a product
    string
  prefs: []
  type: TYPE_NORMAL
- en: As a result of our changes, you now have a stream of data that adds an additional
    column midstream. You can start the Auto Loader notebook to see the schema evolution
    in action. The stream stops when it detects the extra column, providing an exception
    – `[UNKNOWN_FIELD_EXCEPTION.NEW_FIELDS_IN_RECORD_WITH_FILE_PATH]`. Don’t worry;
    upon restart, the schema evolution takes over. Run the cell with the stream in
    it again to restart.
  prefs: []
  type: TYPE_NORMAL
- en: You are finished reading and writing with Auto Loader with Spark Structured
    Streaming. Next, we’ll show you how to accomplish the same task with DLT (using
    less code!).
  prefs: []
  type: TYPE_NORMAL
- en: Continuous data ingestion using Auto Loader with DLT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section uses the Auto Loader code to read the stream of JSON files and
    then DLT to write the files to a table. This notebook is in the `delta_live_tables`
    folder and titled `CH3-03-Formatting_to_Delta_with_DLT`.
  prefs: []
  type: TYPE_NORMAL
- en: The only import you need is DLT – `import dlt`. The DLT code is relatively short,
    partly because the pipeline configuration occurs in the pipeline object rather
    than the code. Before we look through our source code, let’s navigate to the **Databricks
    Workflows** pane in the left-hand navigation bar, select **Delta Live Tables**,
    and click **Create Pipeline**, which opens the pipeline settings page, as shown
    in *Figure 3**.12*.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.12 – The DL\uFEFFT pipeline setup in the workflow UI](img/B16865_03_12.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – The DLT pipeline setup in the workflow UI
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline configuration for DLT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The pipeline settings are minimal, given that DLT does the optimization, so
    the setup instructions are minimal. The parameters entered into the pipeline configuration
    are accessible in the pipeline code using `spark.conf.get`, as shown in *Figures
    3.13* and *3.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: Enter a pipeline name. We will use `MLIA_Streaming_Transactions`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For **Product edition**, select **Advanced**. This DLT edition comes with the
    most features, including DLT’s expectation rules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For **Pipeline mode**, select **Triggered**. This will ensure that the pipeline
    stops processing after a successful run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For **Paths**, select this notebook from the repository for the source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Select **Unity Catalog** as your storage option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `ml_in_action` for the catalog and `synthetic_transactions_dlt` as the
    target schema.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter `table_name` and `synthetic_transactions_dlt` as configurations in the
    `raw_data_location` as follows (as shown in *Figure 3**.13*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: "![Figure 3.13 – Advanced pipel\uFEFFine configuration settings for variables](img/B16865_03_13.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Advanced pipeline configuration settings for variables
  prefs: []
  type: TYPE_NORMAL
- en: Methods in the DLT pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have filled in the pipeline UI, let’s focus on the methods used
    in the source code of the pipeline. In this function, we mostly reuse our previous
    code (*Figure 3**.14*). Note that we do not use the setup file in this notebook.
    Instead, we use the variables we set in the pipeline settings’ advanced configuration
    section.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3\uFEFF.\uFEFF14 – The autoloader stream method details](img/B16865_03_14.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – The autoloader stream method details
  prefs: []
  type: TYPE_NORMAL
- en: Generating the bronze table in a DLT pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `generate_table()` function feeds the read stream created using Auto Loader
    into DLT. We use `spark.conf.get('variable_name')` to access the variable values
    we defined in the pipeline’s advanced settings (*Figure 3**.13*). In the notebook,
    you will see the final step, a one-line cell called `generate_table()`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Generating the Bronze table in a DLT pipeline](img/B16865_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Generating the Bronze table in a DLT pipeline
  prefs: []
  type: TYPE_NORMAL
- en: DLT is unique. It is not code you run line by line, so you don’t execute the
    pipeline in the notebook. Back in the DLT UI, we save and click the **Start**
    button. After the setup process, you will see a graph that includes our table.
    **Start** not only starts the pipeline but creates it as well. Once it finishes,
    you will have a screen like the one shown in *Figure 3**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: Querying streaming tables created with DLT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can query a streaming table created with DLT just like you query other tables.
    This is handy for populating dashboards, which we will cover in [*Chapter 8*](B16865_08.xhtml#_idTextAnchor384),
    *Monitoring, Evaluating,* *and More*.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'If in a notebook you try to query your streaming table, you may get this error:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ExecutionException: org.apache.spark.sql.AnalysisException: 403: Your token
    is missing the required scopes for` `this endpoint.`'
  prefs: []
  type: TYPE_NORMAL
- en: This is because to query streaming tables created by a DLT pipeline, you must
    use a shared cluster using Databricks Runtime 13.1 and above, or a SQL warehouse.
    Streaming tables created in a Unity Catalog-enabled pipeline cannot be queried
    from assigned or no-isolation clusters. You can change your cluster or use the
    DBSQL query editor.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline creates our Bronze table, wrapping up this project on streaming
    transaction data.
  prefs: []
  type: TYPE_NORMAL
- en: Project – Favorita store sales – time series forecasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You downloaded the *Favorita Sales Forecasting* dataset from Kaggle using `opendatasets`
    in the last chapter. We will use that data now to create Delta tables. To follow
    along in your own workspace, open the `CH3-01-Loading_Sales_CSV_Data_as_Delta`
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: The downloaded data is in single CSV files. We use pandas to read the datasets
    and Spark to write to a Delta table. We demonstrate this for only the first file
    in the following code block (*Figure 3**.16*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Reading in the sales holiday events datasets with Pandas](img/B16865_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Reading in the sales holiday events datasets with Pandas
  prefs: []
  type: TYPE_NORMAL
- en: We utilized the data profile capability (*Figure 3**.17*) to check the data
    types before writing to a table. Profiling shows the inferred data type for the
    date field is a string rather than a date or timestamp. Therefore, we alter the
    data type before writing to a Delta table.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Utilizing display(df), we can click + to see the data profile.
    This lets us look at the data types and distributions quickly](img/B16865_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Utilizing display(df), we can click + to see the data profile.
    This lets us look at the data types and distributions quickly
  prefs: []
  type: TYPE_NORMAL
- en: Each table for the Favorita project is transformed into Delta tables similarly.
    As a result, we only include the first table in the book’s pages. However, the
    code that transforms each of the tables is, of course, in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: Project – a retrieval augmented generation chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**RAG** stands for **Retrieval Augmented Generation**. A RAG system often consists
    of your data, a vector database, a search algorithm, and a **generative AI** model
    to generate an answer to a user’s query. *Figure 3**.18* shows the pipeline we
    will build throughout this book.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – A pipeline example that we will try to replicate through the
    book](img/B16865_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – A pipeline example that we will try to replicate through the book
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see from *Figure 3**.18* that we need to retrieve relevant data according
    to the user query. This is where our search method would access a vector database
    and conduct a semantic or hybrid search. At this stage in the RAG chatbot project,
    we have our PDFs in their raw, unstructured form. We want our users to access
    this knowledge via our chatbot, so we have to bring relevant content with information
    from all the PDFs into our chatbot, running in real time. To do so, we’ll convert
    all our PDFs into a machine-readable format. This chapter shows how to extract
    unstructured data from PDF files, chunk it, and transform your text into embeddings.
    Then, we store the embeddings in a Delta table:'
  prefs: []
  type: TYPE_NORMAL
- en: Our first step for data preparation is to extract unstructured information from
    PDF files. To follow along in your workspace, open the `CH3-01-Creating_EmbeddedChunks`
    notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To start building the Bronze data layer, create an empty Delta table that includes
    the table’s schema. Use the `GENERATED BY DEFAULT AS IDENTITY` feature to leverage
    Delta table capabilities to index the newly arriving data automatically. Also,
    add a table property to set the `Change Data Feed` to `true`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Create an empty Delta table with the name pdf_documentation_text](img/B16865_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Create an empty Delta table with the name pdf_documentation_text
  prefs: []
  type: TYPE_NORMAL
- en: For the next step, read the raw PDFs from the `volume` folder and save them
    to a table named `pdf_raw` (*Figure 3**.20*). We will come back to the `pdf_documentation_text`
    table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Read the PDF file in the binary format and write it into a
    Bronze layer table](img/B16865_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – Read the PDF file in the binary format and write it into a Bronze
    layer table
  prefs: []
  type: TYPE_NORMAL
- en: 'Store the PDFs in the binary format in the `content` column to extract the
    text later. Let’s see how it looks in a Delta table view. The binary format of
    each PDF is in the `content` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Displaying ingested content in the Delta table](img/B16865_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – Displaying ingested content in the Delta table
  prefs: []
  type: TYPE_NORMAL
- en: Next, we write a helper function using the `unstructured` library to extract
    the text from the PDF bytes. The function is in the `mlia_utils.rag_funcs` script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Creating a helper function to extract document text](img/B16865_03_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – Creating a helper function to extract document text
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply this function to one of the PDF files we have and check the content
    of our document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Applying a helper function to extract information from the
    PDF](img/B16865_03_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 – Applying a helper function to extract information from the PDF
  prefs: []
  type: TYPE_NORMAL
- en: Great! *Figure 3**.23* gives us a glimpse into one of the ingested documents.
    You may have many PDFs, which can be very long. Long documents can potentially
    be a problem for our future chatbot because they can easily exceed most LLMs maximum
    context lengths (check out *Further reading* for more information on context lengths).
    Additionally, we likely don’t need an entire document’s worth of text to answer
    a specific question. Instead, we need a section or “chunk” of this text. For this
    project, we create chunks of no more than 500 tokens with a chunk overlap of 50,
    using the `SentenceSplitter` module of the `LlamaIndex` library. You could also
    use the `LangChain` library or any library you choose to split the content into
    chunks. We will use the open source `Llama-tokenizer`, as this will be our main
    family of models across our project. Note that tokenizers may play a crucial role
    in your RAG quality. We have leveraged a Pandas **User Defined Function** (**UDF**)
    to scale across all pages of all documents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Creating a pandas UDF for our extractor function](img/B16865_03_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 – Creating a pandas UDF for our extractor function
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s apply this function to our Delta table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Applying a helper function to extract information from the
    PDF using PySpark](img/B16865_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.25 – Applying a helper function to extract information from the PDF
    using PySpark
  prefs: []
  type: TYPE_NORMAL
- en: Once our chunks are ready, we need to convert them into embeddings. Embeddings
    are the format required to perform a semantic search when ingesting into our Silver
    layer later.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks Model Serving now supports **Foundation Model APIs** (**FMAPIs**),
    which allow you to access and query state-of-the-art open models from a serving
    endpoint. With FMAPIs, you can quickly and easily build applications that leverage
    a high-quality GenAI model without maintaining your own model deployment (for
    more information, see *Deploy provisioned throughput Foundation Model APIs* in
    the *Further* *reading* section).
  prefs: []
  type: TYPE_NORMAL
- en: 'FMAPIs are provided in two access modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pay-per-token**: This is the easiest way to start accessing foundation models
    on Databricks and is recommended for beginning your journey with them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provisioned throughput**: This model is recommended for workloads that require
    performance guarantees, fine-tuned models, or have additional security requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Available models for access via the FMAPIs of Databricks](img/B16865_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.26 – Available models for access via the FMAPIs of Databricks
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the FMAPI is available only to US regions. If your workspace
    is not yet in a supported region, you can use any model of your choice (OpenAI,
    BERT, a LlaMA tokenizer, etc.) to convert your content into embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: You may also need to fine-tune your model embedding to learn from your own content
    for better retrieval results.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we leverage the DI Platform’s pay-per-token capability from the FMAPI
    that provides you with access to the `BGE_large_En` endpoint, through the new
    functionality recently added to the `mlflow >=2.9` - `mlflow` deployments (previously
    known as AI Gateway). This functionality unifies the model serving endpoint management
    on Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27 – Applying the FMAPI with the BGE endpoint to convert “What is
    ChatGPT?” into an embedding](img/B16865_03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.27 – Applying the FMAPI with the BGE endpoint to convert “What is ChatGPT?”
    into an embedding
  prefs: []
  type: TYPE_NORMAL
- en: Now, we apply this embedding conversion across all our chunks, and we again
    make `pandasUDF` for scalability purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28 – pandasUDF to apply embedding conversion across all chunks](img/B16865_03_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.28 – pandasUDF to apply embedding conversion across all chunks
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying our UDF will append our `raw_table` chunks with the corresponding
    embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29 – pandasUDF to apply embedding conversion across all chunks](img/B16865_03_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.29 – pandasUDF to apply embedding conversion across all chunks
  prefs: []
  type: TYPE_NORMAL
- en: Once the final step of our data preparation process is completed, we save our
    table in the initially pre-created Delta table `pdf_documentation_text` using
    append mode.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We’ve ingested our PDFs for this project in one big batch, which works perfectly
    fine as an example. However, it also means that anytime you want to add a new
    PDF to your chatbot’s knowledge base, you must manually rerun all of the preceding
    steps. We recommend a workflow for production-grade solutions to automate the
    preceding steps and incrementally ingest PDFs as they arrive in storage.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a dataset ready for a vector search index, which we’ll cover in
    [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180).
  prefs: []
  type: TYPE_NORMAL
- en: Project – multilabel image classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073), we extracted and stored
    our raw image data in our volume. In this chapter, we prepare our image dataset
    and save training and validation sets into Delta tables. To follow along in your
    workspace, open the `Ch3-01-Loading_Images_2_DeltaTables` notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: We start by creating variables and removing existing data if the `Reset` widget
    value is `True`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.30 – Cleaning up existing data if Reset = True](img/B16865_03_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.30 – Cleaning up existing data if Reset = True
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a function to ingest all our images in one table. Initially,
    each label is in its own `folder_label_name`. We extract `image_name`, `image_id`,
    and `label_id`, as well as create `label_name` using append mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.31 – Creating the prep_data2delta function](img/B16865_03_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.31 – Creating the prep_data2delta function
  prefs: []
  type: TYPE_NORMAL
- en: We use the `prep_data2delta` function to load and prepare our training and validation
    datasets (*Figure 3**.32*). Note that the function will save a Delta table if
    the `write2delta` flag is `True` and will return a DataFrame if the value for
    the `returnDF` flag is `True`. Next, in the notebook, we call `prep_data2delta`
    for the training and validation sets.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s talk about data loaders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We load data in a fixed-size batch when fine-tuning or training our deep learning
    models. Each framework natively supports specific data types; some expand their
    native formats to other open source formats. Data scientists sometimes prefer
    to keep their data (images, in our case) in blob storage and read it directly
    from storage rather than using Delta tables, as they think this avoids additional
    work. However, we recommend storing images in a Delta table unless you are working
    with large images greater than one GB per image. Storing your images in a Delta
    table allows you to take advantage of Delta and Unity Catalog’s additional benefits,
    such as lineage of data and models, data version control, duplicate data checks,
    and quality assurance.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, you have a few options to read your data while working
    with a PyTorch or PyTorch Lightning framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DeltaTorchLoader` (recommended)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petastorm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading images directly from blob/disk/volumes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our recommendation is to use DeltaTorchLoader. It handles data batching, sampling,
    and multiprocessing while training PyTorch pipelines without requiring a temporary
    copy of files, like with Petastorm. See *Further reading* for more information.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, DeltaTorchLoader, which we are going to use to load
    and transform our data from Delta into the PyTorch/Lightning dataloader framework
    to train our model in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297), requires
    you to have tables in the unmanaged Delta format in volumes when using UC. Don’t
    worry; the lineage is associated with the same path to the volume as your main
    dataset. We’ll talk more about lineage in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
    This requirement is due to UC’s read/write security permissions with blob storage.
    The blob storage maintainers do not support these security settings yet. If you
    are not using UC, you should be able to read Delta tables directly from the managed
    tables.
  prefs: []
  type: TYPE_NORMAL
- en: There is also an option to read your data using the Petastorm library. We don’t
    recommend Petastorm because it requires a deeper understanding of certain pitfalls.
    The most common are memory usage issues due to data caching and the fact that
    it uses Apache `Parquet` files rather than Delta files, so it consumes all versions
    of your parquet files.
  prefs: []
  type: TYPE_NORMAL
- en: The creators of the DeltaTorchLoader performed a few benchmarks with Petastorm.
    The benchmark was shared at the **Data and AI Summit** (**DAIS**) and is featured
    in *Figure 3**.33*. In this project, we will compare Petastorm to the classic
    Torch loader in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297) to show the performance
    gain. The comparison demonstrates an incredible speed increase when reading the
    batch of data. We’ve also included a great video on *TorchDeltaLoader* in *Further
    reading* if you want to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.32 – The DeltaTorchLoader benchmark – a performance comparison between
    the DeltaTorch and Petastorm loaders](img/B16865_03_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.32 – The DeltaTorchLoader benchmark – a performance comparison between
    the DeltaTorch and Petastorm loaders
  prefs: []
  type: TYPE_NORMAL
- en: You can keep filenames in your Delta table instead of the images and collect
    them while passing them to the main PyTorch Loader with the `trainer` function.
    Keeping files in Delta is essential to avoid duplicates and control the list used
    during training and validation, as you can pass the Delta version to MLflow during
    tracking for full replication purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing our data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once our tables are created and written to the storage, we use a few functions
    to improve read performance on the Delta tables. First, we use `OPTIMIZE` to keep
    an ideal number of files (*Figure 3**.34*). Second, we disable deletion vectors
    because the `DeltaTorchReader` does not support them yet *(**Figure 3**.35*).
    We use the SQL magic command, `%sql`, to perform these operations, using SQL in
    the Python notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.33 – Optimizing the file size and count of the training table](img/B16865_03_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.33 – Optimizing the file size and count of the training table
  prefs: []
  type: TYPE_NORMAL
- en: Note that the variables we saved in Python are inaccessible in SQL, so we hardcode
    them in this example. You could include SQL variables in the `global-setup` notebook
    to avoid this.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.34 – Optimizing the training table](img/B16865_03_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.34 – Optimizing the training table
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have the training and validation tables loaded and optimized to efficiently
    work with this image data to fine-tune our multi-class computer vision models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on the essentials of building out the Bronze data
    layer within the Databricks Data Intelligence Platform. We emphasized the importance
    of schema evolution, DLT, and the conversion of data into the Delta format and
    applied these principles in our example projects. This chapter highlighted the
    significance of tools such as Auto Loader and DLT in this process. Auto Loader,
    with its proficiency in handling file tracking and automating schema management,
    alongside DLT’s robust capabilities in pipeline development and data quality assurance,
    are pivotal in our data management strategy. These tools facilitate an efficient
    and streamlined approach to data pipeline management, enabling us as data scientists
    to focus more on valuable tasks, such as feature engineering and experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: With our Bronze layer created, we now move on from this foundational work to
    a more advanced layer of data – the Silver layer. [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180),
    *Transformations toward Our Silver Layer*, will take us deeper into our data and
    demonstrate various Databricks tools that will aid us in the exploration and transformations
    of our data.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following questions solidify key points to remember and tie the content
    back to your experience:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the names of the layers in the Medallion architecture design?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you wanted to build a managed pipeline with streaming data, which product
    would you use – Structured Streaming or DLT?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What feature did we use to add the `product` column to our streaming transaction
    data without manual intervention?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do you have projects from your current position, experience, or on your roadmap
    that would benefit from one or more of the topics covered in this chapter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a possible way to lessen the number of partitions when partitioning
    on a high cardinality column?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After putting thought into the questions, compare your answers to ours:'
  prefs: []
  type: TYPE_NORMAL
- en: The layers of the Medallion architecture are Bronze, Silver, and Gold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We recommend DLT build managed pipelines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the streaming transactions project example, we used Auto Loader’s schema
    evolution feature to add a column without manual intervention.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We hope so! One example is a managed streaming data pipeline that could benefit
    from the built-in data quality monitoring that comes with DLT.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bucketing is an optimal method specifically designed to provide an additional
    layer of organization in your data. It can reduce the number of output files and
    organize the data better for subsequent reading, and it can be especially useful
    when the partitioning column has high cardinality.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covered different methods of ingesting data into your Bronze layer.
    Take a look at these resources to read more about the areas that interest you
    most:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Use liquid clustering for Delta* *tables*:[https://docs.databricks.com/en/delta/clustering.html](https://docs.databricks.com/en/delta/clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spark Structured* *Streaming*: [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Delta Live* *Tables*: [https://docs.databricks.com/en/delta-live-tables/index.html](https://docs.databricks.com/en/delta-live-tables/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DLT Databricks* *Demo*:[https://www.databricks.com/resources/demos/tutorials/lakehouse-platform/full-delta-live-table-pipeline](https://www.databricks.com/resources/demos/tutorials/lakehouse-platform/full-delta-live-table-pipeline)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Auto Loader* *options*: [https://docs.databricks.com/ingestion/auto-loader/options.html](https://docs.databricks.com/ingestion/auto-loader/options.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Schema evolution with Auto* *Loader*: [https://docs.databricks.com/ingestion/auto-loader/schema.html#configure-schema-inference-and-evolution-in-auto-loader](https://docs.databricks.com/ingestion/auto-loader/schema.html#configure-schema-inference-and-evolution-in-auto-loader)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Common loading patterns with Auto* *Loader*: [https://docs.databricks.com/ingestion/auto-loader/patterns.html](https://docs.databricks.com/ingestion/auto-loader/patterns.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stream processing with Apache Kafka and* *Databricks*:[https://docs.databricks.com/structured-streaming/kafka.html](https://docs.databricks.com/structured-streaming/kafka.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How We Performed ETL on One Billion Records For Under $1 With Delta Live*
    *Tables*: [https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html](https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Create tables – Managed vs* *External*: [https://docs.databricks.com/en/data-governance/unity-catalog/create-tables.html#create-tables](https://docs.databricks.com/en/data-governance/unity-catalog/create-tables.html#create-tables)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Take full advantage of the auto-tuning* *available*:[https://docs.databricks.com/delta/tune-file-size.html#configure-delta-lake-to-control-data-file-size](https://docs.databricks.com/delta/tune-file-size.html#configure-delta-lake-to-control-data-file-size)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Import Python modules from Databricks repos: [https://docs.databricks.com/en/delta-live-tables/import-workspace-files.html](https://docs.databricks.com/en/delta-live-tables/import-workspace-files.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deletion Vectors: [https://docs.databricks.com/en/delta/deletion-vectors.html](https://docs.databricks.com/en/delta/deletion-vectors.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks ML* *Runtime*:[https://docs.databricks.com/runtime/mlruntime.html#introduction-to-databricks-runtime-for-machine-learning](https://docs.databricks.com/runtime/mlruntime.html#introduction-to-databricks-runtime-for-machine-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cluster advanced* *options*:[https://docs.databricks.com/en/clusters/configure.html#spark-configuration](https://docs.databricks.com/en/clusters/configure.html#spark-configuration)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deploy provisioned throughput Foundation Model* *APIs*:[https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html](https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scaling Deep Learning Using Delta Lake Storage Format on* *Databricks*: [https://www.databricks.com/dataaisummit/session/scaling-deep-learning-using-delta-lake-storage-format-databricks/](https://www.databricks.com/dataaisummit/session/scaling-deep-learning-using-delta-lake-storage-format-databricks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DeltaTorchLoader*: [https://github.com/delta-incubator/deltatorch](https://github.com/delta-incubator/deltatorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2: Heavily Use Case-Focused'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part introduces you to taking a set of data sources and working with them
    throughout the platform, from one end to another. The goal of this part is simply
    to demonstrate how to thoughtfully use all the bells and whistles of the platform.
    This part provides stories, code, lakehouse features, and best practices.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B16865_04.xhtml#_idTextAnchor180), *Getting to Know Your Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B16865_05.xhtml#_idTextAnchor244), *Feature Engineering on Databricks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B16865_06.xhtml#_idTextAnchor297), *Searching for a Signal*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B16865_07.xhtml#_idTextAnchor325), *Productionizing ML on Databricks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B16865_08.xhtml#_idTextAnchor384), *Monitoring, Evaluating, and
    More*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
