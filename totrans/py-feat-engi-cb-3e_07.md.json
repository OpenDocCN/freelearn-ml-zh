["```py\n    import pandas as pd\n    from sklearn.datasets import fetch_california_housing\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    ```", "```py\n    X, y = fetch_california_housing(\n        return_X_y=True, as_frame=True)\n    X.drop(labels=[\"Latitude\", \"Longitude\"], axis=1,\n        inplace=True)\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=0)\n    ```", "```py\n    scaler = StandardScaler().set_output(\n        transform=\"pandas\")\n    scaler.fit(X_train)\n    ```", "```py\n    X_train_scaled = scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    ```", "```py\n    scaler.mean_\n    ```", "```py\n    scaler:\n\n    ```", "```py\n\n    Let’s compare the transformed data with the original data to understand the changes.\n    ```", "```py\n    X_test.describe()\n    ```", "```py\n    X_test_scaled.describe()\n    ```", "```py\n    import pandas as pd\n    from sklearn.datasets import fetch_california_housing\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import MinMaxScaler\n    ```", "```py\n    X, y = fetch_california_housing(\n        return_X_y=True, as_frame=True)\n    X.drop(labels=[\"Latitude\", \"Longitude\"], axis=1,\n        inplace=True)\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=0)\n    ```", "```py\n    scaler = MinMaxScaler().set_output(\n        transform=\"pandas\"\")\n    scaler.fit(X_train)\n    ```", "```py\n    X_train_scaled = scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    ```", "```py\nMedInc           0.000000\nHouseAge        0.000000\nAveRooms        0.004705\nAveBedrms      0.004941\nPopulation     0.000140\nAveOccup      -0.000096\nX_test_scaled.max(), we see that the maximum values of the variables are around 1:\n\n```", "```py\n\n Note\nIf you check the maximum values of the variables in the train set after the transformation, you’ll see that they are exactly `1`. Yet, in the test set, we see values greater and smaller than `1`. This occurs because, in the test set, there are observations with larger or smaller magnitudes than those in the train set. In fact, we see the greatest differences in the variables that deviate the most from the normal distribution (the last four variables in the dataset). This behavior is expected because scaling to the minimum and maximum values is sensitive to outliers and very skewed distributions.\nScaling to the minimum and maximum value does not change the shape of the variable’s distribution. You can corroborate that by displaying the histograms before and after the transformation.\nHow it works...\nIn this recipe, we scaled the variables of the California housing dataset to values between `0` and `1`.\n`MinMaxScaler()` from scikit-learn learned the minimum and maximum values and the value range of each variable when we called the `fit()` method and stored these parameters in its `data_max_`, `min_`, and `data_range_` attributes. By using `transform()`, we made the scaler remove the minimum value from each variable in the train and test sets and divide the result by the value range.\nNote\n`MinMaxScaler()` will scale all variables by default. To scale only a subset of the variables in the dataset, you can use `ColumnTransformer()` from scikit-learn or `SklearnTransformerWrapper()` from `Feature-engine`.\n`MinMaxScaler()` will scale the variables between `0` and `1` by default. However, we have the option to scale to a different range by adjusting the tuple passed to the `feature_range` parameter.\nBy default, `MinMaxScaler()` returns NumPy arrays, but we can modify this behavior to return `pandas` DataFrames with the `set_output()` method, as we did in *Step 4*.\nScaling with the median and quantiles\nWhen scaling variables to the median and quantiles, the median value is removed from the observations, and the result is divided by the **Inter-Quartile Range** (**IQR**). The IQR is the difference between the 3rd quartile and the 1st quartile, or, in other words, the difference between the 75th percentile and the 25th percentile:\n![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><mi>x</mi><mo>_</mo><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mn>3</mn><mi>r</mi><mi>d</mi><mi>q</mi><mi>u</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>l</mi><mi>e</mi><mfenced open=\"(\" close=\")\"><mi>x</mi></mfenced><mo>−</mo><mn>1</mn><mi>s</mi><mi>t</mi><mi>q</mi><mi>u</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>l</mi><mi>e</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/27.png)\nThis method is known as **robust scaling** because it produces more robust estimates for the center and value range of the variable. Robust scaling is a suitable alternative to standardization when models require the variables to be centered and the data contains outliers. It is worth noting that robust scaling will not change the overall shape of the variable distribution.\nHow to do it...\nIn this recipe, we will implement scaling with the median and IQR by utilizing scikit-learn:\n\n1.  Let’s start by importing `pandas` and the required scikit-learn classes and functions:\n\n    ```", "```py\n\n     2.  Let’s load the California housing dataset into a `pandas` DataFrame and drop the `Latitude` and `Longitude` variables:\n\n    ```", "```py\n\n     3.  Let’s divide the data into train and test sets:\n\n    ```", "```py\n\n     4.  Let’s set up scikit-learn’s `RobustScaler()`and fit it to the train set so that it learns and stores the median and IQR:\n\n    ```", "```py\n\n     5.  Finally, let’s scale the variables in the train and test sets with the trained scaler:\n\n    ```", "```py\n\n     6.  Let’s print the variable median values learned by `RobustScaler()`:\n\n    ```", "```py\n\n    We see the parameters learned by `RobustScaler()` in the following output:\n\n    ```", "```py\n    scaler.scale_\n    array([2.16550000e+00, 1.90000000e+01, 1.59537022e+00,                 9.41284380e-02, 9.40000000e+02, 8.53176853e-01])\n    ```", "```py\n\nHow it works...\nTo scale the features using the median and IQR, we created an instance of `RobustScaler()`. With `fit()`, the scaler learned the median and IQR for each variable from the train set. With `transform()`, the scaler subtracted the median from each variable in the train and test sets and divided the result by the IQR.\nAfter the transformation, the median values of the variables were centered at `0`, but the overall shape of the distributions did not change. You can corroborate the effect of the transformation by displaying the histograms of the variables before and after the transformation and by printing out the main statistical parameters through `X_test.describe()` and `X_test_scaled.b()`.\nPerforming mean normalization\nIn mean normalization, we center the variable at `0` and rescale the distribution to the value range, so that its values lie between `-1` and `1`. This procedure involves subtracting the mean from each observation and then dividing the result by the difference between the minimum and maximum values, as shown here:\n![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>max</mi><mfenced open=\"(\" close=\")\"><mi>x</mi></mfenced><mo>−</mo><mi mathvariant=\"normal\">m</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/28.png)\nNote\nMean normalization is an alternative to standardization. In both cases, the variables are centered at `0`. In mean normalization, the variance varies, while the values lie between `-1` and `1`. On the other hand, in standardization, the variance is set to `1` and the value range varies.\nMean normalization is a suitable alternative for models that need the variables to be centered at zero. However, it is sensitive to outliers and not a suitable option for sparse data, as it will destroy the sparse nature.\nHow to do it...\nIn this recipe, we will implement mean normalization with `pandas`:\n\n1.  Let’s import `pandas` and the required scikit-learn class and function:\n\n    ```", "```py\n\n     2.  Let’s load the California housing dataset from scikit-learn into a `pandas` DataFrame, dropping the `Latitude` and `Longitude` variables:\n\n    ```", "```py\n\n     3.  Let’s divide the data into train and test sets:\n\n    ```", "```py\n\n     4.  Let’s learn the mean values from the variables in the train set:\n\n    ```", "```py\n\nNote\nWe set `axis=0` to take the mean of the rows – that is, of the observations in each variable. If we set `axis=1` instead, `pandas` will calculate the mean value per observation across all the columns.\nBy executing `print(mean)`, we display the mean values per variable:\n\n```", "```py\n\n1.  Now, let’s determine the difference between the maximum and minimum values per variable:\n\n    ```", "```py\n\n    By executing `print(ranges)`, we display the value ranges per variable:\n\n    ```", "```py\n\nNote\nThe `pandas` `mean()`, `max()`, and `min()` methods return a `pandas` series.\n\n1.  Now, we’ll apply mean normalization to the train and test sets by utilizing the learned parameters:\n\n    ```", "```py\n\nNote\nIn order to transform future data, you will need to store these parameters, for example, in a `.txt` or `.``csv` file.\n*Step 6* returns `pandas` DataFrames with the transformed train and test sets. Go ahead and compare the variables before and after the transformations. You’ll see that the distributions did not change, but the variables are centered at `0`, and their values lie between `-1` and `1`.\nHow it works…\nTo implement mean normalization, we captured the mean values of the numerical variables in the train set using `mean()` from `pandas`. Next, we determined the difference between the maximum and minimum values of the numerical variables in the train set by utilizing `max()` and `min()` from `pandas`. Finally, we used the `pandas` series returned by these functions containing the mean values and the value ranges to normalize the train and test sets. We subtracted the mean from each observation in our train and test sets and divided the result by the value ranges. This returned the normalized variables in a `pandas` DataFrame.\nThere’s more...\nThere is no dedicated scikit-learn transformer to implement mean normalization, but we can combine the use of two transformers to do so.\nTo do this, we need to import `pandas` and load the data, just like we did in *Steps 1* to *3* in the *How to do it...* section of this recipe. Then, follow these steps:\n\n1.  Import the scikit-learn transformers:\n\n    ```", "```py\n\n     2.  Let’s set up `StandardScaler()` to learn and subtract the mean without dividing the result by the standard deviation:\n\n    ```", "```py\n\n     3.  Now, let’s set up `RobustScaler()` so that it does not remove the median from the values but divides them by the value range – that is, the difference between the maximum and minimum values:\n\n    ```", "```py\n\nNote\nTo divide by the difference between the minimum and maximum values, we need to specify `(0, 100)` in the `quantile_range` argument of `RobustScaler()`.\n\n1.  Let’s fit the scalers to the train set so that they learn and store the mean, maximum, and minimum values:\n\n    ```", "```py\n\n     2.  Finally, let’s apply mean normalization to the train and test sets:\n\n    ```", "```py\n\nWe transformed the data with `StandardScaler()` to remove the mean and then transformed the resulting DataFrame with `RobustScaler()` to divide the result by the range between the minimum and maximum values. We described the functionality of `StandardScaler()` in this chapter’s *Standardizing the features* recipe and `RobustScaler()` in the *Scaling with the median and quantiles* recipe of this chapter.\nImplementing maximum absolute scaling\nMaximum absolute scaling scales the data to its maximum value – that is, it divides every observation by the maximum value of the variable:\n![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mi>x</mi><mrow><mi mathvariant=\"normal\">m</mi><mi mathvariant=\"normal\">a</mi><mi mathvariant=\"normal\">x</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/29.png)\nAs a result, the maximum value of each feature will be `1.0`. Note that maximum absolute scaling does not center the data, and hence, it’s suitable for scaling sparse data. In this recipe, we will implement maximum absolute scaling with scikit-learn.\nNote\nScikit-learn recommends using this transformer on data that is centered at `0` or on sparse data.\nGetting ready\nMaximum absolute scaling was specifically designed to scale sparse data. Thus, we will use a bag-of-words dataset that contains sparse variables for the recipe. In this dataset, the variables are words, the observations are documents, and the values are the number of times each word appears in the document. Most entries in the data are `0`.\nWe will use a dataset consisting of a bag of words, which is available in the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Bag+of+Words), which is licensed under CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/legalcode).\nI downloaded and prepared a small bag of words representing a simplified version of one of those datasets. You will find this dataset in the accompanying GitHub repository: [https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/tree/main/ch07-scaling](https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/tree/main/ch07-scaling).\nHow to do it...\nLet’s begin by importing the required packages and loading the dataset:\n\n1.  Let’s import the required libraries and the scaler:\n\n    ```", "```py\n\n     2.  Let’s load the bag-of-words dataset:\n\n    ```", "```py\n\n    If we execute `data.head()`, we will see the DataFrame consisting of the words as columns, the documents as rows, and the number of times each word appeared in a document as values:\n\n![Figure 7.5 – DataFrame with the bag of words](img/B22396_07_5.jpg)\n\nFigure 7.5 – DataFrame with the bag of words\nNote\nAlthough we omit this step in the recipe, remember that the maximum absolute values should be learned from a training dataset only. Split the dataset into train and test sets when carrying out your analysis.\n\n1.  Let’s set up `MaxAbsScaler()` and fit it to the data so that it learns the variables’ maximum values:\n\n    ```", "```py\n\n     2.  Now, let’s scale the variables by utilizing the trained scaler:\n\n    ```", "```py\n\nNote\n`MaxAbsScaler ()` stores the maximum values in its `max_abs_` attribute.\n\n1.  Let’s display the maximum values stored by the scaler:\n\n    ```", "```py\n\n    To follow up, let’s plot the distributions of the original and scaled variables.\n\n     2.  Let’s make a histogram with the bag of words before the scaling:\n\n    ```", "```py\n\n    In the following output, we see histograms with the number of times each word appears in a document:\n\n![Figure 7.6 – Histograms with differen﻿t word counts](img/B22396_07_6.jpg)\n\nFigure 7.6 – Histograms with different word counts\n\n1.  Now, let’s make a histogram with the scaled variables:\n\n    ```", "```py\n\n    In the following output, we can corroborate the change of scale of the variables, but their distribution shape remains the same:\n\n![Figure 7.7 – Histograms of the word counts after the scaling](img/B22396_07_7.jpg)\n\nFigure 7.7 – Histograms of the word counts after the scaling\nWith scaling to the maximum absolute value, we linearly scale down the magnitude of the features.\nHow it works...\nIn this recipe, we scaled the sparse variables of a bag of words to their absolute maximum values by using `MaxAbsScaler()`. With `fit()`, the scaler learned the maximum absolute values for each variable and stored them in its `max_abs_` attribute. With `transform()`, the scaler divided the variables by their absolute maximum values, returning a `pandas` DataFrame.\nNote\nRemember that you can change the output container to a NumPy array or a `polars` DataFrame through the `set_output()` method of the scikit-learn library’s transformers.\nThere’s more...\nIf you want to center the variables’ distribution at `0` and then scale them to their absolute maximum, you can do so by combining the use of two scikit-learn transformers within a pipeline:\n\n1.  Let’s import the required libraries, transformers, and functions:\n\n    ```", "```py\n\n     2.  Let’s load the California housing dataset and split it into train and test sets:\n\n    ```", "```py\n\n     3.  Let’s set up `StandardScaler()` from scikit-learn so that it learns and subtracts the mean but does not divide the result by the standard deviation:\n\n    ```", "```py\n\n     4.  Now, let’s set up `MaxAbsScaler()` with its default parameters:\n\n    ```", "```py\n\n     5.  Let’s include both scalers within a pipeline that returns pandas DataFrames:\n\n    ```", "```py\n\n     6.  Let’s fit the scalers to the train set so that they learn the required parameters:\n\n    ```", "```py\n\n     7.  Finally, let’s transform the train and test sets:\n\n    ```", "```py\n\n    The pipeline applies `StandardScaler()` and `MaxAbsScaler()` in sequence to first remove the mean and then scale the resulting variables to their maximum values.\n\nScaling to vector unit length\nScaling to the vector unit length involves scaling individual observations (not features) to have a unit norm. Each sample (that is, each row of the data) is rescaled independently of other samples so that its norm equals one. Each row constitutes a **feature vector** containing the values of every variable for that row. Hence, with this scaling method, we rescale the feature vector.\nThe norm of a vector is a measure of its magnitude or length in a given space and it can be determined by using the Manhattan (*l1*) or the Euclidean (*l2*) distance. The Manhattan distance is given by the sum of the absolute components of the vector:\n![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><mrow><mi>l</mi><mn>1</mn><mfenced open=\"(\" close=\")\"><mi>x</mi></mfenced><mo>=</mo><mo>|</mo><msub><mi>x</mi><mn>1</mn></msub><mo>|</mo><mo>+</mo><mfenced open=\"|\" close=\"|\"><msub><mi>x</mi><mn>2</mn></msub></mfenced><mo>+</mo><mo>…</mo><mo>..</mo><mo>+</mo><mo>|</mo><msub><mi>x</mi><mi>n</mi></msub><mo>|</mo></mrow></mrow></mrow></math>](img/30.png)\nThe Euclidean distance is given by the square root of the square sum of the component of the vector:\n![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><mi>l</mi><mn>2</mn><mfenced open=\"(\" close=\")\"><mi>x</mi></mfenced><mo>=</mo><msqrt><mrow><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mo>…</mo><mo>+</mo><msubsup><mi>x</mi><mi>n</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mrow></math>](img/31.png)\nHere, ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo></mrow></mrow></math>](img/32.png)and ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>x</mi><mi>n</mi></msub></mrow></math>](img/33.png)are the values of variables *1*, *2*, and *n* for each observation. Scaling to unit norm consists of dividing each feature vector’s value by either *l1* or *l2*, so that after the scaling, the norm of the feature vector is *1*. To be clear, we divide each of ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo></mrow></mrow></math>](img/34.png)and ![<math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>,</mo></mrow></mrow></math>](img/35.png)by *l1* or *l2*.\nThis scaling procedure changes the variables’ distribution, as illustrated in the following figure:\n![Figure 7.8 – Distribution of a normal and skewed variable before and after scaling each observation’s feature vector to its norm](img/B22396_07_8.jpg)\n\nFigure 7.8 – Distribution of a normal and skewed variable before and after scaling each observation’s feature vector to its norm\nNote\nThis scaling technique scales each observation and not each variable. The scaling methods that we discussed so far in this chapter aimed at shifting and resetting the scale of the variables’ distribution. When we scale to the unit length, however, we normalize each observation individually, contemplating their values across all features.\nScaling to the unit norm can be used when utilizing kernels to quantify similarity for text classification and clustering. In this recipe, we will scale each observation’s feature vector to a unit length of `1` using scikit-learn.\nHow to do it...\nTo begin, we’ll import the required packages, load the dataset, and prepare the train and test sets:\n\n1.  Let’s import the required Python packages, classes, and functions:\n\n    ```", "```py\n\n     2.  Let’s load the California housing dataset into a `pandas` DataFrame:\n\n    ```", "```py\n\n     3.  Let’s divide the data into train and test sets:\n\n    ```", "```py\n\n     4.  Let’s set up the scikit-learn library’s `Normalizer()` transformer to scale each observation to the Manhattan distance or `l1`:\n\n    ```", "```py\n\nNote\nTo normalize to the Euclidean distance, you need to set the norm to `l2` using `scaler =` `Normalizer(norm='l2')`.\n\n1.  Let’s transform the train and test sets – that is, we’ll divide each observation’s feature vector by its norm:\n\n    ```", "```py\n\n    We can calculate the length (that is, the Manhattan distance of each observation’s feature vector) using `linalg()` from NumPy.\n\n     2.  Let’s calculate the norm (Manhattan distance) before scaling the variables:\n\n    ```", "```py\n\n    As expected, the norm of each observation varies:\n\n    ```", "```py\n\n     3.  Let’s now calculate the norm after the scaling:\n\n    ```", "```py\n\nNote\nYou need to set `ord=1` for the Manhattan distance and `ord=2` for the Euclidean distance as arguments of NumPy’s `linalg()`function, depending on whether you scaled the features to the `l1` or `l2` norm.\nWe see that the Manhattan distance of each feature vector is `1` after scaling:\n\n```", "```py\n\n Based on the scikit-learn library’s documentation, this scaling method can be useful when using a quadratic form such as the dot-product or any other kernel to quantify the similarity of a pair of samples.\nHow it works...\nIn this recipe, we scaled the observations from the California housing dataset to their feature vector unit norm by utilizing the Manhattan or Euclidean distance. To scale the feature vectors, we created an instance of `Normalizer()` from scikit-learn and set the norm to `l1` for the Manhattan distance. For the Euclidean distance, we set the norm to `l2`. Then, we applied the `fit()` method, although there were no parameters to be learned, as this normalization procedure depends exclusively on the values of the features for each observation. Finally, with the `transform()` method, the scaler divided each observation’s feature vector by its norm. This returned a NumPy array with the scaled dataset. After the scaling, we used NumPy’s `linalg.norm` function to calculate the norm (`l1` and `l2`) of each vector to confirm that after the transformation, it was `1`.\n\n```"]