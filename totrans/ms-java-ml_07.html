<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Deep Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Deep Learning</h1></div></div></div><p>In <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>, we discussed different supervised classification techniques that are general and can be used in a wide range of applications. In the area of supervised non-linear techniques, especially in computer-vision, deep learning and its variants are having a remarkable impact. We find that deep learning and associated methodologies can be applied to image-recognition, image and object annotation, movie descriptions, and even areas such as text classification, language modeling, translations, and so on. (<span class="emphasis"><em>References</em></span> [1, 2, 3, 4, and 5]) </p><p>To set the stage for deep learning, we will start with describing what neurons are and how they can be arranged to build multi-layer neural networks, present the core elements of these networks, and explain how they work. We will then discuss the issues and problems associated with neural networks that gave rise to advances and structural changes in deep learning. We will learn about some building blocks of deep learning such as Restricted Boltzmann Machines and Autoencoders. We will then explore deep learning through different variations in supervised and unsupervised learning. Next, we will take a tour of Convolutional Neural Networks (CNN) and by means of a use case, illustrate how they work by deconstructing an application of CNNs in the area of computer-vision. We will introduce Recurrent Neural Networks (RNN) and its variants and how they are used in the text/sequence mining fields. We will finally present a case study using real-life data of MNIST images and use it to compare/contrast different techniques. We will use DeepLearning4J as our Java toolkit for performing these experiments.</p><div class="section" title="Multi-layer feed-forward neural network"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec60"/>Multi-layer feed-forward neural network</h1></div></div></div><p>Historically, artificial neural networks have been largely identified by multi-layer feed-forward <a id="id1336" class="indexterm"/>perceptrons, and so we will begin with a discussion of the primitive elements of the structure of such networks, how to train them, the problem of overfitting, and techniques to address it.</p><div class="section" title="Inputs, neurons, activation function, and mathematical notation"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec107"/>Inputs, neurons, activation function, and mathematical notation</h2></div></div></div><p>A single <a id="id1337" class="indexterm"/>neuron or perceptron is the same as the unit described <a id="id1338" class="indexterm"/>in the Linear Regression topic in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>. In this chapter, the data <a id="id1339" class="indexterm"/>instance vector will be <a id="id1340" class="indexterm"/>represented by <span class="emphasis"><em>x</em></span> and has <span class="emphasis"><em>d</em></span> dimensions, and each dimension can be represented as <span class="inlinemediaobject"><img src="graphics/B05137_07_003.jpg" alt="Inputs, neurons, activation function, and mathematical notation"/></span>. The weights associated with each dimension are represented as a weight vector <span class="emphasis"><em>w</em></span> that has <span class="emphasis"><em>d</em></span> dimensions, and each dimension can be represented as <span class="inlinemediaobject"><img src="graphics/B05137_07_005.jpg" alt="Inputs, neurons, activation function, and mathematical notation"/></span>. Each neuron has an extra input <span class="emphasis"><em>b</em></span>, known as the bias, associated with it.</p><p>Neuron pre-activation performs the linear transformation of inputs given by:</p><div class="mediaobject"><img src="graphics/B05137_07_007.jpg" alt="Inputs, neurons, activation function, and mathematical notation"/></div><p>The activation function is given by <span class="inlinemediaobject"><img src="graphics/B05137_07_008.jpg" alt="Inputs, neurons, activation function, and mathematical notation"/></span>, which transforms the neuron input <span class="inlinemediaobject"><img src="graphics/B05137_07_009.jpg" alt="Inputs, neurons, activation function, and mathematical notation"/></span> as follows:</p><div class="mediaobject"><img src="graphics/B05137_07_010.jpg" alt="Inputs, neurons, activation function, and mathematical notation"/></div><div class="mediaobject"><img src="graphics/B05137_07_011.jpg" alt="Inputs, neurons, activation function, and mathematical notation"/><div class="caption"><p>Figure 1. Perceptron with inputs, weights, and bias feeding to generate outputs.</p></div></div></div><div class="section" title="Multi-layered neural network"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec108"/>Multi-layered neural network </h2></div></div></div><p>Multi-layered <a id="id1341" class="indexterm"/>neural networks are the first step to understanding deep learning networks as the fundamental concepts and primitives of multi-layered nets form the basis of all deep neural nets.</p><div class="section" title="Structure and mathematical notations"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec119"/>Structure and mathematical notations</h3></div></div></div><p>We <a id="id1342" class="indexterm"/>introduce the generic structure of neural networks in this section.  Most neural nets are variants of the structure outlined here.  We also present the relevant notation that we will use in the rest of the chapter. </p><div class="mediaobject"><img src="graphics/B05137_07_012.jpg" alt="Structure and mathematical notations"/><div class="caption"><p>Figure 2. Multilayer neural network showing an input layer, two hidden layers, and an output layer.</p></div></div><p>The most common supervised learning algorithms pertaining to neural networks use multi-layered perceptrons. The Input Layer consists of several neurons, each connected independently to the input, with its own set of weights and bias. In addition to the Input Layer, there are one or more layers of neurons known as Hidden Layers. The input layer neurons are connected to every neuron in the first hidden layer, that layer is similarly connected to the next hidden layer, and so on, resulting in a fully connected network. The layer of neurons connected to the last hidden layer is called the Output Layer.</p><p>Each hidden <a id="id1343" class="indexterm"/>layer is represented by <span class="inlinemediaobject"><img src="graphics/B05137_07_013.jpg" alt="Structure and mathematical notations"/></span> where <span class="emphasis"><em>k</em></span> is the layer. The pre-activation for layer <span class="emphasis"><em>0 &lt; k </em></span>
<span class="emphasis"><em>&lt; l</em></span> is given by: </p><div class="mediaobject"><img src="graphics/B05137_07_016.jpg" alt="Structure and mathematical notations"/></div><p>The hidden layer activation for <span class="inlinemediaobject"><img src="graphics/B05137_07_017.jpg" alt="Structure and mathematical notations"/></span>:</p><div class="mediaobject"><img src="graphics/B05137_07_018.jpg" alt="Structure and mathematical notations"/></div><p>The final output layer activation is:</p><div class="mediaobject"><img src="graphics/B05137_07_019.jpg" alt="Structure and mathematical notations"/></div><p>The output is generally one class per neuron and it is tuned in such a way that only one neuron activates and all others have 0 as the output. A softmax function with <span class="inlinemediaobject"><img src="graphics/B05137_07_020.jpg" alt="Structure and mathematical notations"/></span> is used for giving the result.</p></div><div class="section" title="Activation functions in NN"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec120"/>Activation functions in NN</h3></div></div></div><p>Some of the <a id="id1344" class="indexterm"/>most well-known activation functions that are used in neural networks are given in the following sections and they are used because the derivatives needed in learning can be expressed in terms of the function itself.</p><div class="section" title="Sigmoid function"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec182"/>Sigmoid function</h4></div></div></div><p>Sigmoid activation <a id="id1345" class="indexterm"/>functions are given by the following equation: </p><div class="mediaobject"><img src="graphics/B05137_07_021.jpg" alt="Sigmoid function"/></div><p>It can be seen as a bounded, strictly increasing and positive transformation function that squashes the <a id="id1346" class="indexterm"/>values between 0 and 1.</p></div><div class="section" title="Hyperbolic tangent (&quot;tanh&quot;) function"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec183"/>Hyperbolic tangent ("tanh") function </h4></div></div></div><p>The <a id="id1347" class="indexterm"/>Tanh function is given by the following equation:</p><div class="mediaobject"><img src="graphics/B05137_07_022.jpg" alt="Hyperbolic tangent (&quot;tanh&quot;) function"/></div><p>It can be seen as bounded, strictly increasing, but as a positive or negative transformation function that squashes the values between -1 and 1.</p></div></div><div class="section" title="Training neural network"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec121"/>Training neural network</h3></div></div></div><p>In this <a id="id1348" class="indexterm"/>section, we will discuss the key elements of training <a id="id1349" class="indexterm"/>neural networks from input training sets, in much the same fashion as we did in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>. The dataset is denoted by <span class="emphasis"><em>D</em></span> and consists of individual data instances. The instances are normally represented as the set <span class="inlinemediaobject"><img src="graphics/B05137_07_024.jpg" alt="Training neural network"/></span>. The labels for each instance are represented as the set <span class="inlinemediaobject"><img src="graphics/B05137_07_025.jpg" alt="Training neural network"/></span>. The entire labeled dataset with numeric or real-valued features is represented as paired elements in a set as given by <span class="inlinemediaobject"><img src="graphics/B05137_07_026.jpg" alt="Training neural network"/></span>. </p><div class="section" title="Empirical risk minimization"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec184"/>Empirical risk minimization</h4></div></div></div><p>Empirical <a id="id1350" class="indexterm"/>risk minimization is a general machine learning concept that is used in many classifications or supervised learning. The main idea behind this technique is to convert a training or learning problem into an optimization problem (<span class="emphasis"><em>References</em></span> [13]).</p><p>Given the parameters for a neural network as <span class="strong"><strong>?</strong></span> = ({<span class="strong"><strong>W</strong></span><sup>1</sup>, <span class="strong"><strong>W</strong></span><sup>2</sup>, … <span class="strong"><strong>W</strong></span>
<span class="emphasis"><em><sup>l</sup></em></span><sup> +1</sup>}, {<span class="strong"><strong>b</strong></span><sup>1</sup>, <span class="strong"><strong>b</strong></span><sup>2</sup>, …<span class="strong"><strong>b</strong></span>
<span class="emphasis"><em><sup>L</sup></em></span> <sup>+1</sup>}) the training problem can be seen as finding the best parameters (<span class="emphasis"><em>?</em></span>)such that</p><div class="mediaobject"><img src="graphics/B05137_07_030.jpg" alt="Empirical risk minimization"/></div><p>Where <span class="inlinemediaobject"><img src="graphics/B05137_07_031.jpg" alt="Empirical risk minimization"/></span> Stochastic gradient descent (SGD) discussed in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span> and <a class="link" href="ch05.html" title="Chapter 5. Real-Time Stream Machine Learning">Chapter 5</a>, <span class="emphasis"><em>Real-time Stream Machine Learning,</em></span> is commonly <a id="id1351" class="indexterm"/>used as the optimization procedure. The SGD applied to training neural networks is: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">initialize <span class="strong"><strong>?</strong></span> = ({<span class="strong"><strong>W</strong></span><sup>1</sup>, <span class="strong"><strong>W</strong></span><sup>2</sup>, … <span class="strong"><strong>W</strong></span><span class="emphasis"><em><sup>l</sup></em></span><sup> +1</sup>}, {<span class="strong"><strong>b</strong></span><sup>1</sup>, <span class="strong"><strong>b</strong></span><sup>2</sup>, …<span class="strong"><strong>b</strong></span><span class="emphasis"><em><sup>L</sup></em></span><sup> +1</sup>})</li><li class="listitem">for i=1 to <span class="emphasis"><em>N</em></span> epochs <div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
for each training sample (<span class="strong"><strong>x</strong></span><sup>t</sup>, <span class="emphasis"><em>y</em></span><sup>t</sup>) 
<span class="inlinemediaobject"><img src="graphics/B05137_07_036.jpg" alt="Empirical risk minimization"/></span>// find the gradient of function
2  ?= ?+ a? //move in direction</li></ol></div></li></ol></div><p>The learning rate used here (a) will impact the algorithm convergence by reducing the oscillation near the optimum; choosing the right value of a is often a hyper parameter search that needs the validation techniques described in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>.</p><p>Thus, to learn the parameters of a neural network, we need to choose a way to do parameter initialization, select a loss function <span class="inlinemediaobject"><img src="graphics/B05137_07_040.jpg" alt="Empirical risk minimization"/></span>, compute the parameter gradients <span class="inlinemediaobject"><img src="graphics/B05137_07_041.jpg" alt="Empirical risk minimization"/></span>, propagate the losses back, select the regularization/penalty function O(<span class="emphasis"><em>?</em></span>), and compute the gradient of regularization <span class="inlinemediaobject"><img src="graphics/B05137_07_043.jpg" alt="Empirical risk minimization"/></span>. In the next few sections, we will describe this step by step.
</p><div class="section" title="Parameter initialization"><div class="titlepage"><div><div><h5 class="title"><a id="ch07lvl5sec75"/>Parameter initialization</h5></div></div></div><p>The parameters of <a id="id1352" class="indexterm"/>neural networks are the weights and biases of each layer from the input layer, through hidden layers, to the output layer. There has been much research in this area as the optimization depends on the start or initialization. Biases are generally set to value 0. The weight initialization depends on the activation functions as some, such as tanh, value 0, cannot be used. Generally, the way to initialize the weights of each layer is by random initialization using a symmetric function with a user-defined boundary.</p></div><div class="section" title="Loss function"><div class="titlepage"><div><div><h5 class="title"><a id="ch07lvl5sec76"/>Loss function</h5></div></div></div><p>The loss function's <a id="id1353" class="indexterm"/>main role is to maximize how well the predicted output label matches the class of the input data vector. </p><p>Thus, maximization <span class="inlinemediaobject"><img src="graphics/B05137_07_044.jpg" alt="Loss function"/></span> is equivalent to minimizing the negative of the log-likelihood or cross-entropy:</p><div class="mediaobject"><img src="graphics/B05137_07_045.jpg" alt="Loss function"/></div></div><div class="section" title="Gradients"><div class="titlepage"><div><div><h5 class="title"><a id="ch07lvl5sec77"/>Gradients</h5></div></div></div><p>We will describe <a id="id1354" class="indexterm"/>gradients at the output layer and the hidden layer without going into the derivation as it is beyond the scope of this book. Interested readers can see the derivation in the text by Rumelhart, Hinton and Williams (<span class="emphasis"><em>References</em></span> [6]).</p><div class="section" title="Gradient at the output layer"><div class="titlepage"><div><div><h6 class="title"><a id="ch07lvl6sec09"/>Gradient at the output layer</h6></div></div></div><p>Gradient at the output layer can be calculated as:</p><div class="mediaobject"><img src="graphics/B05137_07_046.jpg" alt="Gradient at the output layer"/></div><div class="mediaobject"><img src="graphics/B05137_07_047.jpg" alt="Gradient at the output layer"/></div><div class="mediaobject"><img src="graphics/B05137_07_048.jpg" alt="Gradient at the output layer"/></div><p>Where <span class="emphasis"><em>e(y)</em></span> is called the "one hot vector" where only one value in the vector is 1 corresponding to the right class <span class="emphasis"><em>y</em></span> and the rest are 0.</p><p>The gradient at the output layer pre-activation can be calculated similarly: </p><div class="mediaobject"><img src="graphics/B05137_07_051.jpg" alt="Gradient at the output layer"/></div><p>= – (<span class="strong"><strong>e</strong></span>(y) – <span class="strong"><strong>f</strong></span>(<span class="strong"><strong>x</strong></span>))</p></div><div class="section" title="Gradient at the Hidden Layer"><div class="titlepage"><div><div><h6 class="title"><a id="ch07lvl6sec10"/>Gradient at the Hidden Layer</h6></div></div></div><p>A hidden layer gradient is computed using the chain rule of partial differentiation. </p><p>Gradient at the hidden layer <span class="inlinemediaobject"><img src="graphics/B05137_07_053.jpg" alt="Gradient at the Hidden Layer"/></span>
</p><div class="mediaobject"><img src="graphics/B05137_07_054.jpg" alt="Gradient at the Hidden Layer"/></div><p>Gradient at the hidden layer pre-activation can be shown as: </p><div class="mediaobject"><img src="graphics/B05137_07_055.jpg" alt="Gradient at the Hidden Layer"/></div><div class="mediaobject"><img src="graphics/B05137_07_056.jpg" alt="Gradient at the Hidden Layer"/></div><p>Since the hidden layer pre-activation needs partial derivatives of the activation functions as shown previously (<span class="emphasis"><em>g'</em></span>(<span class="emphasis"><em>a</em></span><sup>k</sup><span class="strong"><strong>x</strong></span><sub>j</sub>)), some of the well-known activation functions described previously have partial derivatives in terms of the equation itself, which makes computation very easy. </p><p>For example, the partial derivative of the sigmoid function is <span class="emphasis"><em>g'(a) = g(a)(</em></span>1<span class="emphasis"><em> – g(a))</em></span> and, for the tanh function, it is 1 – <span class="emphasis"><em>g</em></span>(a)<sup>2</sup>.</p></div><div class="section" title="Parameter gradient"><div class="titlepage"><div><div><h6 class="title"><a id="ch07lvl6sec11"/>Parameter gradient </h6></div></div></div><p>The loss gradient of parameters must be computed using gradients of weights and biases. Gradient of weights can be shown as: </p><div class="mediaobject"><img src="graphics/B05137_07_060.jpg" alt="Parameter gradient"/></div><div class="mediaobject"><img src="graphics/B05137_07_061.jpg" alt="Parameter gradient"/></div><p>Gradient of biases can be shown as: </p><div class="mediaobject"><img src="graphics/B05137_07_062.jpg" alt="Parameter gradient"/></div><div class="mediaobject"><img src="graphics/B05137_07_063.jpg" alt="Parameter gradient"/></div></div></div><div class="section" title="Feed forward and backpropagation"><div class="titlepage"><div><div><h5 class="title"><a id="ch07lvl5sec78"/>Feed forward and backpropagation</h5></div></div></div><p>The aim of <a id="id1355" class="indexterm"/>neural network training is to adjust the weights and biases at each layer so that, based on the feedback from the output layer and the loss function that estimates the difference between the predicted output and the actual output, that difference is minimized.</p><p>The neural network algorithm based on initial weights and biases can be seen as forwarding the computations layer by layer as shown in the acyclic flow graph with one hidden layer to demonstrate the flow:</p><div class="mediaobject"><img src="graphics/B05137_07_064.jpg" alt="Feed forward and backpropagation"/><div class="caption"><p>Figure 3: Neural network flow as a graph in feed forward.</p></div></div><p>From the input vector and pre-initialized values of weights and biases, each subsequent element is computed: the pre-activation, hidden layer output, final layer pre-activation, final layer output, and loss function with respect to the actual label. In <a id="id1356" class="indexterm"/>backward propagation, the flow is exactly reversed, from the loss at the output down to the weights and biases of the first layer, as shown in the following figure: </p><div class="mediaobject"><img src="graphics/B05137_07_065.jpg" alt="Feed forward and backpropagation"/><div class="caption"><p>Figure 4: Neural network flow as a graph in back propagation.</p></div></div></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch07lvl5sec79"/>How does it work?</h5></div></div></div><p>The backpropagation algorithm (<span class="emphasis"><em>References</em></span> [6 and 7]) in its entirety can be summarized as follows: </p><p>Compute the output gradient before activation:</p><div class="mediaobject"><img src="graphics/B05137_07_066.jpg" alt="How does it work?"/></div><p>For hidden layers <span class="emphasis"><em>k=l+1 to 1</em></span>:</p><p>Compute the gradient of hidden layer parameters:</p><div class="mediaobject"><img src="graphics/B05137_07_068.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_07_069.jpg" alt="How does it work?"/></div><p>Compute the gradient of the hidden layer below the current:</p><div class="mediaobject"><img src="graphics/B05137_07_070.jpg" alt="How does it work?"/></div><p>Compute the <a id="id1357" class="indexterm"/>gradient of the layer before activation:</p><div class="mediaobject"><img src="graphics/B05137_07_071.jpg" alt="How does it work?"/></div></div><div class="section" title="Regularization"><div class="titlepage"><div><div><h5 class="title"><a id="ch07lvl5sec80"/>Regularization</h5></div></div></div><p>In the empirical risk <a id="id1358" class="indexterm"/>minimization objective defined previously, regularization is used to address the over-fitting problem in machine learning as introduced in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>. The well-known regularization functions are given as follows.</p><div class="section" title="L2 regularization"><div class="titlepage"><div><div><h6 class="title"><a id="ch07lvl6sec12"/>L2 regularization</h6></div></div></div><p>This is applied only to <a id="id1359" class="indexterm"/>the weights and not to the biases and is given for layers connecting (<span class="emphasis"><em>i,j</em></span>) components as:</p><div class="mediaobject"><img src="graphics/B05137_07_073.jpg" alt="L2 regularization"/></div><div class="mediaobject"><img src="graphics/B05137_07_074.jpg" alt="L2 regularization"/></div><p>Also, the gradient of the regularizer can be computed as <span class="inlinemediaobject"><img src="graphics/B05137_07_075.jpg" alt="L2 regularization"/></span>. They are often interpreted as the "Gaussian Prior" over the weight distribution.</p></div><div class="section" title="L1 regularization"><div class="titlepage"><div><div><h6 class="title"><a id="ch07lvl6sec13"/>L1 regularization</h6></div></div></div><p>This is again applied <a id="id1360" class="indexterm"/>only to the weights and not to the biases and is given for layers connecting <span class="emphasis"><em>(i,j)</em></span> components as:</p><div class="mediaobject"><img src="graphics/B05137_07_076.jpg" alt="L1 regularization"/></div><p>And the gradient of this <a id="id1361" class="indexterm"/>regularizer can be computed as <span class="inlinemediaobject"><img src="graphics/B05137_07_077.jpg" alt="L1 regularization"/></span>. It is often interpreted as the "Laplacian Prior" over the weight distribution.</p></div></div></div></div></div></div></div>
<div class="section" title="Limitations of neural networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec61"/>Limitations of neural networks</h1></div></div></div><p>In this section, we <a id="id1362" class="indexterm"/>will discuss in detail the issues faced by neural networks, which will become the stepping stone for building deep learning networks.</p><div class="section" title="Vanishing gradients, local optimum, and slow training"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec109"/>Vanishing gradients, local optimum, and slow training</h2></div></div></div><p>One of the major issues with neural networks is the problem of "vanishing gradient" (<span class="emphasis"><em>References</em></span> [8]). We will try to give a simple explanation of the issue rather than exploring the mathematical derivations in depth. We will choose the sigmoid activation function and a two-layer neural network, as shown in the following figure, to demonstrate the issue:</p><div class="mediaobject"><img src="graphics/B05137_07_078.jpg" alt="Vanishing gradients, local optimum, and slow training"/><div class="caption"><p>Figure 5: Vanishing Gradient issue.</p></div></div><p>As we saw in the activation function description, the sigmoid function squashes the output between the range 0 and 1. The derivative of the sigmoid function <span class="emphasis"><em>g'(a) = g(a)(</em></span>1<span class="emphasis"><em> – g(a))</em></span> has a range between 0 and 0.25. The goal of learning is to minimize the output loss, that is, <span class="inlinemediaobject"><img src="graphics/B05137_07_079.jpg" alt="Vanishing gradients, local optimum, and slow training"/></span>. In general, the output error does not go to 0, so maximum iterations; a user-specified parameter determines the quality of learning and backpropagation of the errors.</p><p>Simplifying to illustrate the effect of output error on the input weight layer:</p><div class="mediaobject"><img src="graphics/B05137_07_080.jpg" alt="Vanishing gradients, local optimum, and slow training"/></div><p>Each of the transformations, for instance, from output to hidden, involves multiplication of two terms, both less than 1:</p><div class="mediaobject"><img src="graphics/B05137_07_081.jpg" alt="Vanishing gradients, local optimum, and slow training"/></div><p>Thus, the value becomes so <a id="id1363" class="indexterm"/>small when it reaches the input layer that the propagation of the gradient has almost vanished. This is known as the vanishing gradient problem. </p><p>A paradoxical situation arises when you need to add more layers to make features more interesting in the hidden layers. But adding more layers also increases the errors. As you add more layers, the input layers become "slow to train," which causes the output layers to be more inaccurate as they are dependent on the input layers; further, and for the same number of iterations, the errors increase with the increase in the number of layers.</p><p>With a fixed number of maximum iterations, more layers and slow propagation of errors can lead to a "local optimum."</p><p>Another issue with basic neural networks is the number of parameters. Finding effective size and weights for each hidden layer and bias becomes more challenging with the increase in the number of layers. If we increase the number of layers, the parameters increase in polynomials. Fitting the parameters for the data requires a large number of data samples. This can result in the problem discussed before, that is, overfitting. </p><p>In the next few sections, we will start learning about the building blocks of deep learning that help overcome these issues. </p></div></div>
<div class="section" title="Deep learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec62"/>Deep learning</h1></div></div></div><p>Deep learning includes <a id="id1364" class="indexterm"/>architectures and techniques for supervised and unsupervised learning with the capacity to internalize the abstract structure of high-dimensional data using networks composed of building blocks to create discriminative or generative models. These techniques have proved enormously successful in recent years and any reader interested in mastering them must become familiar with the basic building blocks of deep learning first and understand the various types of networks in use by practitioners. Hands-on experience building and tuning deep neural networks is invaluable if you intend to get a deeper understanding of the subject. Deep learning, in various domains such as image classification and text learning, incorporates feature generation in its structures thus making the task of mining the features redundant in many applications. The following sections provide a guide to the concepts, building blocks, techniques for composing architectures, and training deep networks.</p><div class="section" title="Building blocks for deep learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec110"/>Building blocks for deep learning</h2></div></div></div><p>In the following <a id="id1365" class="indexterm"/>sections, we introduce the most important components used in deep learning, including Restricted Boltzmann machines, Autoencoders, and Denoising Autoencoders, how they work, and their advantages and limitations. </p><div class="section" title="Rectified linear activation function"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec122"/>Rectified linear activation function</h3></div></div></div><p>The <a id="id1366" class="indexterm"/>Reclin function is given by the equation:</p><p>
<span class="emphasis"><em>g</em></span>(<span class="emphasis"><em>a</em></span>)<span class="emphasis"><em> = reclin </em></span>(<span class="emphasis"><em>a</em></span>)<span class="emphasis"><em> = max </em></span>(0<span class="emphasis"><em>,</em></span>
<span class="emphasis"><em>a</em></span>)</p><p>It can be seen as having a lower bound of 0 and no upper bound, strictly increasing, and a positive transformation function that just does linear transformation of positives.</p><p>It is easier to see that the rectified linear unit or ReLu has a derivative of 1 or identity for values greater than 0. This acts as a significant benefit as the derivatives are not squashed and do not have diminishing values when chained. One of the issues with ReLu is that the value is 0 for negative inputs and the corresponding neurons act as "dead", especially when a large negative value is learned for the bias term. ReLu cannot recover from this as the input and derivative are both 0. This is generally solved by having a "leaky ReLu". These functions have a small value for negative inputs and are given by <span class="inlinemediaobject"><img src="graphics/B05137_07_083.jpg" alt="Rectified linear activation function"/></span> where ? = 0.01, typically.</p></div><div class="section" title="Restricted Boltzmann Machines"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec123"/>Restricted Boltzmann Machines</h3></div></div></div><p>Restricted <a id="id1367" class="indexterm"/>Boltzmann Machines (RBM) is an unsupervised learning neural network (<span class="emphasis"><em>References</em></span> [11]). The idea of RBM is to extract "more <a id="id1368" class="indexterm"/>meaningful features" from labeled or unlabeled data. It is also meant to "learn" from the large quantity of unlabeled data available in many domains when getting access to labeled data is costly or difficult. </p><div class="section" title="Definition and mathematical notation"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec185"/>Definition and mathematical notation</h4></div></div></div><p>In its basic form RBM assumes inputs to be binary values 0 or 1 in each dimension. RBMs are undirected graphical models having two layers, a visible layer represented as <span class="emphasis"><em>x</em></span> and a hidden layer <span class="emphasis"><em>h,</em></span> and connections <span class="emphasis"><em>W</em></span>. </p><p>RBM defines a <a id="id1369" class="indexterm"/>distribution over the visible layer that involves the latent variables from the hidden layer. First an energy function is defined to capture the relationship between the visible and the hidden layers in vector form as: </p><div class="mediaobject"><img src="graphics/B05137_07_088.jpg" alt="Definition and mathematical notation"/></div><p>In scalar form the energy function can be defined as: </p><div class="mediaobject"><img src="graphics/B05137_07_089.jpg" alt="Definition and mathematical notation"/></div><p>The probability of the distribution is given by <span class="inlinemediaobject"><img src="graphics/B05137_07_090.jpg" alt="Definition and mathematical notation"/></span> where <span class="emphasis"><em>Z</em></span> is called the "partitioning function", which is an enumeration over all the values of <span class="emphasis"><em>x and h</em></span>, which are binary, resulting in exponential terms and thus making it intractable!</p><div class="mediaobject"><img src="graphics/B05137_07_093.jpg" alt="Definition and mathematical notation"/><div class="caption"><p>Figure 6: Connection between the visible layer and hidden layer.</p></div></div><p>The Markov network view of the same in scalar form can be represented using all the pairwise factors, as shown in the following figure. This also makes it clear why it is called a "restricted" Boltzmann machine as there is no connection among units within a given <a id="id1370" class="indexterm"/>hidden layer or in the visible layers:</p><div class="mediaobject"><img src="graphics/B05137_07_094.jpg" alt="Definition and mathematical notation"/></div><div class="mediaobject"><img src="graphics/B05137_07_095.jpg" alt="Definition and mathematical notation"/><div class="caption"><p>Figure 7: Input and hidden layers as scalars </p></div></div><p>We have seen that the whole probability distribution function <span class="inlinemediaobject"><img src="graphics/B05137_07_090.jpg" alt="Definition and mathematical notation"/></span> is intractable. We will now derive the basic conditional probability distributions for <span class="emphasis"><em>x, h</em></span>.</p></div><div class="section" title="Conditional distribution"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec186"/>Conditional distribution</h4></div></div></div><p>Although <a id="id1371" class="indexterm"/>computing the whole <span class="emphasis"><em>p(x, h)</em></span> is intractable, the conditional distribution of <span class="emphasis"><em>p(x|h)</em></span> or <span class="emphasis"><em>p(h|x)</em></span> can be easily defined and shown to be a Bernoulli distribution and tractable:</p><div class="mediaobject"><img src="graphics/B05137_07_100.jpg" alt="Conditional distribution"/></div><div class="mediaobject"><img src="graphics/B05137_07_101.jpg" alt="Conditional distribution"/></div><div class="mediaobject"><img src="graphics/B05137_07_102.jpg" alt="Conditional distribution"/></div><p>Similarly, being <a id="id1372" class="indexterm"/>symmetric and undirected:</p><div class="mediaobject"><img src="graphics/B05137_07_103.jpg" alt="Conditional distribution"/></div><div class="mediaobject"><img src="graphics/B05137_07_104.jpg" alt="Conditional distribution"/></div><div class="mediaobject"><img src="graphics/B05137_07_105.jpg" alt="Conditional distribution"/></div></div><div class="section" title="Free energy in RBM"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec187"/>Free energy in RBM</h4></div></div></div><p>The <a id="id1373" class="indexterm"/>distribution of input or the observed variable is:</p><div class="mediaobject"><img src="graphics/B05137_07_106.jpg" alt="Free energy in RBM"/></div><div class="mediaobject"><img src="graphics/B05137_07_107.jpg" alt="Free energy in RBM"/></div><div class="mediaobject"><img src="graphics/B05137_07_108.jpg" alt="Free energy in RBM"/></div><div class="mediaobject"><img src="graphics/B05137_07_109.jpg" alt="Free energy in RBM"/></div><p>The <a id="id1374" class="indexterm"/>function <span class="emphasis"><em>F(x)</em></span> is called free energy.</p></div><div class="section" title="Training the RBM"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec188"/>Training the RBM</h4></div></div></div><p>RBMs are <a id="id1375" class="indexterm"/>trained using the optimization objective of minimizing the average negative log-likelihood over the entire training data. This can be represented as: </p><div class="mediaobject"><img src="graphics/B05137_07_111.jpg" alt="Training the RBM"/></div><p>The optimization is carried out by using stochastic gradient descent:</p><div class="mediaobject"><img src="graphics/B05137_07_112.jpg" alt="Training the RBM"/></div><p>The term <span class="inlinemediaobject"><img src="graphics/B05137_07_113.jpg" alt="Training the RBM"/></span> is called the "positive phase" and the term <span class="inlinemediaobject"><img src="graphics/B05137_07_114.jpg" alt="Training the RBM"/></span> is called the "negative phase" because of how they affect the probability distributions—the positive phase, because it increases the probability of training data by reducing the free energy, and the negative phase, as it decreases the probability of samples generated by the model.</p><p>It has been shown that the overall gradient is difficult to compute analytically because of the "negative phase", as it is computing the expectation over all possible configurations of the input data under the distribution formed by the model and making it intractable!</p><p>To make the computation tractable, estimation is carried out using a fixed number of model samples and they are referred to as "negative particles" denoted by <span class="emphasis"><em>N</em></span>. </p><p>The gradient can be now written as the approximation:</p><div class="mediaobject"><img src="graphics/B05137_07_116.jpg" alt="Training the RBM"/></div><p>Where particles <span class="inlinemediaobject"><img src="graphics/B05137_07_117.jpg" alt="Training the RBM"/></span> are sampled using some sampling techniques such as the Monte Carlo method.</p></div><div class="section" title="Sampling in RBM"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec189"/>Sampling in RBM</h4></div></div></div><p>Gibbs sampling is <a id="id1376" class="indexterm"/>often the technique used to generate samples and learn the probability of <span class="emphasis"><em>p(x,h)</em></span> in terms of <span class="emphasis"><em>p(x|h)</em></span> and <span class="emphasis"><em>p (h|x)</em></span>, which are relatively easy to compute, as shown previously.</p><p>Gibbs sampling for joint sampling of N random variables <span class="inlinemediaobject"><img src="graphics/B05137_07_119.jpg" alt="Sampling in RBM"/></span> is done using N sampling sub-steps of the form <span class="inlinemediaobject"><img src="graphics/B05137_07_120.jpg" alt="Sampling in RBM"/></span> where <span class="emphasis"><em>S</em></span>
<span class="emphasis"><em><sub>-i</sub></em></span> contains samples up to and excluding step <span class="emphasis"><em>S</em></span>
<span class="emphasis"><em><sub>i</sub></em></span>. Graphically, this can be shown as follows:</p><div class="mediaobject"><img src="graphics/B05137_07_123.jpg" alt="Sampling in RBM"/><div class="caption"><p>Figure 8: Graphical representation of sampling done between hidden and input layers.</p></div></div><p>As <span class="inlinemediaobject"><img src="graphics/B05137_07_124.jpg" alt="Sampling in RBM"/></span> it can be shown that the sampling represents the actual distribution <span class="emphasis"><em>p(x,h)</em></span>. </p></div><div class="section" title="Contrastive divergence"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec190"/>Contrastive divergence </h4></div></div></div><p>Contrastive <a id="id1377" class="indexterm"/>divergence (CD) is a trick used to expedite the Gibbs sampling process described previously so it <a id="id1378" class="indexterm"/>stops at step <span class="emphasis"><em>k</em></span> of the process rather than continuing for a long time to guarantee convergence. It has been seen that even <span class="emphasis"><em>k=1</em></span> is reasonable and gives good performance (<span class="emphasis"><em>References</em></span> [10]).</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h5 class="title"><a id="ch07lvl5sec81"/>Inputs and outputs</h5></div></div></div><p>These are the inputs to the algorithm:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Training dataset </li><li class="listitem" style="list-style-type: disc">Number of steps for Gibbs sampling, <span class="emphasis"><em>k</em></span></li><li class="listitem" style="list-style-type: disc">Learning rate a</li><li class="listitem" style="list-style-type: disc">The output is the set of updated parameters</li></ul></div></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch07lvl5sec82"/>How does it work?</h5></div></div></div><p>The complete training <a id="id1379" class="indexterm"/>pseudo-code using CD with the free energy function and partial derivatives can be given as:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">For each instance in training <span class="strong"><strong>x</strong></span><sup>t</sup>:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Generate a negative particle <span class="inlinemediaobject"><img src="graphics/B05137_07_117.jpg" alt="How does it work?"/></span> using <span class="emphasis"><em>k</em></span> steps of Gibbs Sampling.
</li><li class="listitem">Update the parameters:</li></ol></div><div class="mediaobject"><img src="graphics/B05137_07_130.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_07_131.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_07_132.jpg" alt="How does it work?"/></div></li></ol></div></div></div><div class="section" title="Persistent contrastive divergence"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec191"/>Persistent contrastive divergence </h4></div></div></div><p>Persistent <a id="id1380" class="indexterm"/>contrastive divergence is another trick used to compute the joint probability <span class="emphasis"><em>p(x,h)</em></span>. In this method, there is a single chain that does not reinitialize after every observed sample to find the negative particle <span class="inlinemediaobject"><img src="graphics/B05137_07_117.jpg" alt="Persistent contrastive divergence"/></span>. It persists its state and parameters are updated just through running these k states by using the particle from the previous step. </p></div></div><div class="section" title="Autoencoders"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec124"/>Autoencoders</h3></div></div></div><p>An autoencoder is <a id="id1381" class="indexterm"/>another form of unsupervised learning technique in <a id="id1382" class="indexterm"/>neural networks. It is very similar to the feed-forward neural network described at the start with the only difference being it doesn't generate a class at output, but tries to replicate the input at the output layer (<span class="emphasis"><em>References</em></span> [12 and 23]). The goal is to have hidden layer(s) capture the latent or hidden information of the <a id="id1383" class="indexterm"/>input as features that can be useful in unsupervised or supervised learning.</p><div class="section" title="Definition and mathematical notations"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec192"/>Definition and mathematical notations</h4></div></div></div><p>A single hidden <a id="id1384" class="indexterm"/>layer example of an Autoencoder is shown in the following figure: </p><div class="mediaobject"><img src="graphics/B05137_07_133.jpg" alt="Definition and mathematical notations"/><div class="caption"><p>Figure 9: Autoencoder flow between layers</p></div></div><p>The input layer and the output layer have the same number of neurons similar as feed-forward, corresponding to the input vector, <span class="emphasis"><em>x</em></span>. Each hidden layer can have greater, equal, or fewer neurons than the input or output layer and an activation function that does a non-linear transformation of the signal. It can be seen as using the unsupervised or latent hidden structure to "compress" the data effectively.</p><p>The encoder or input transformation of the data by the hidden layer is given by:</p><div class="mediaobject"><img src="graphics/B05137_07_135.jpg" alt="Definition and mathematical notations"/></div><p>And the decoder or output transformation of the data by the output layer is given by:</p><div class="mediaobject"><img src="graphics/B05137_07_136.jpg" alt="Definition and mathematical notations"/></div><p>Generally, a sigmoid function with linear transformation of signals as described in the neural network section is popularly used in the layers:</p><p>
<span class="inlinemediaobject"><img src="graphics/B05137_07_137.jpg" alt="Definition and mathematical notations"/></span> and <span class="inlinemediaobject"><img src="graphics/B05137_07_138.jpg" alt="Definition and mathematical notations"/></span>)</p></div><div class="section" title="Loss function"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec193"/>Loss function </h4></div></div></div><p>The job of the loss <a id="id1385" class="indexterm"/>function is to reduce the training error as before so that an optimization process such as a stochastic gradient function can be used.</p><p>In the case of binary valued input, the loss function is generally the average cross-entropy given by:</p><div class="mediaobject"><img src="graphics/B05137_07_139.jpg" alt="Loss function"/></div><p>It can be easily verified that, when the input signal and output signal match either 0 or 1, the error is 0. Similarly, for real-valued input, a squared error is used:</p><div class="mediaobject"><img src="graphics/B05137_07_140.jpg" alt="Loss function"/></div><p>The gradient of the loss function that is needed for the stochastic gradient procedure is similar to the feed-forward neural network and can be shown through derivation for both real-valued and binary as follows:</p><div class="mediaobject"><img src="graphics/B05137_07_141.jpg" alt="Loss function"/></div><p>Parameter gradients are obtained by back-propagating the <span class="inlinemediaobject"><img src="graphics/B05137_07_142.jpg" alt="Loss function"/></span> exactly as in the neural network. </p></div><div class="section" title="Limitations of Autoencoders"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec194"/>Limitations of Autoencoders</h4></div></div></div><p>Autoencoders <a id="id1386" class="indexterm"/>have some known drawbacks that have been addressed by specialized architectures that we will discuss in the sections to follow.  These limitations are:</p><p>When the size of the Autoencoder is equal to the number of neurons in the input, there is a chance that the weights learned by the Autoencoders are just the identity vectors and that the whole representation simply passes on the inputs exactly as outputs with zero loss. Thus, they emulate "rote learning" or "memorization" without any generalization.</p><p>When the size of the Autoencoder is greater than the number of neurons in the input, the configuration is called an "overcomplete" hidden layer and can have similar problems to the ones mentioned previously. Some of the units can be turned off and others can become identity making it just the copy unit.</p><p>When the size of the <a id="id1387" class="indexterm"/>Autoencoder is less than the number of neurons in the input, known as "undercomplete", the latent structure in the data or important hidden components can be discovered. </p></div><div class="section" title="Denoising Autoencoder"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec195"/>Denoising Autoencoder</h4></div></div></div><p>As mentioned <a id="id1388" class="indexterm"/>previously, when the Autoencoder has a hidden layer size greater than or equal to that of the input, it is not guaranteed to learn the weights and can become simply a unit switch to copy input to output. This issue is addressed by the Denoising Autoencoder. Here there is another layer added between input and the hidden layer. This layer adds some noise to the input using either a well-known distribution <span class="inlinemediaobject"><img src="graphics/B05137_07_143.jpg" alt="Denoising Autoencoder"/></span> or using stochastic noise such as turning a bit to 0 in binary input. This "noisy" input then goes through learning from the hidden layer to the output layer exactly like the Autoencoder. The loss function of the Denoising Autoencoder compares the output with the actual input. Thus, the added noise and the larger hidden layer enable either learning latent structures or adding/removing redundancy to produce the exact signal at the output.  This architecture—where non-zero features at the noisy layer generate features at the hidden layer that are themselves transformed by the activation layer as the signal advances forward—lends a robustness and implicit structure to the learning press (<span class="emphasis"><em>References</em></span> [15]).</p><div class="mediaobject"><img src="graphics/B05137_07_144.jpg" alt="Denoising Autoencoder"/><div class="caption"><p>Figure 10: Denoising Autoencoder</p></div></div></div></div><div class="section" title="Unsupervised pre-training and supervised fine-tuning"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec125"/>Unsupervised pre-training and supervised fine-tuning</h3></div></div></div><p>As we <a id="id1389" class="indexterm"/>discussed in the issues section on neural networks, the issue with over-training arises especially in deep learning as the number of layers, and hence parameters, is large. One way to account for over-fitting is to do data-specific regularization. In this section, we will describe the "unsupervised pre-training" method done in the hidden layers to overcome the issue of over-fitting. Note that this is generally the "initialization process" used in many deep learning algorithms.</p><p>The algorithm of unsupervised pre-training works in a layer-wise greedy fashion. As shown in the following figure, one layer of a visible and hidden structure is considered at a given time. The weights of this layer are learned for a few iterations using unsupervised techniques such as RBM, described previously. The output of the hidden layer is then used as a "visible" or "input" layer and the training proceeds to the next, and so on.</p><p>Each <a id="id1390" class="indexterm"/>learning of layers can be thought of as a "feature extraction or feature generation" process. The real data inputs when transformed form higher-level features at a given layer and then are further combined to form much higher-level features, and so on. </p><div class="mediaobject"><img src="graphics/B05137_07_145.jpg" alt="Unsupervised pre-training and supervised fine-tuning"/><div class="caption"><p>Figure 11: Layer wise incremental learning through unsupervised learning.</p></div></div><p>Once all the hidden layer parameters are learned in pre-training using unsupervised techniques as described previously, a supervised fine-tuning process follows. In the supervised fine-tuning process, a final output layer is added and, just like in a neural network, training is done with forward and backward propagation. The idea is that most weights or parameters are almost fully tuned and only need a small change for producing a discriminative class mapping at the output. </p><div class="mediaobject"><img src="graphics/B05137_07_146.jpg" alt="Unsupervised pre-training and supervised fine-tuning"/><div class="caption"><p>Figure 12: Final tuning or supervised learning.</p></div></div></div><div class="section" title="Deep feed-forward NN"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec126"/>Deep feed-forward NN </h3></div></div></div><p>A deep <a id="id1391" class="indexterm"/>feed-forward neural network involves using the stages pre-training, and fine-tuning.</p><p>Depending on the <a id="id1392" class="indexterm"/>unsupervised learning technique used—RBM, Autoencoders, or Denoising Autoencoders—different algorithms are formed: Stacked RBM, Stacked Autoencoders, and Stacked Denoising Autoencoders, respectively.</p><div class="section" title="Input and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl5sec83"/>Input and outputs</h4></div></div></div><p>Given an <a id="id1393" class="indexterm"/>architecture for the deep feed-forward neural net, these are the inputs for training the network:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Number of layers <span class="emphasis"><em>L</em></span></li><li class="listitem" style="list-style-type: disc">Dataset without labels <span class="emphasis"><em>D</em></span> </li><li class="listitem" style="list-style-type: disc">Dataset with labels <span class="emphasis"><em>D</em></span></li><li class="listitem" style="list-style-type: disc">Number of <a id="id1394" class="indexterm"/>training iterations <span class="emphasis"><em>n</em></span></li></ul></div></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl5sec84"/>How does it work?</h4></div></div></div><p>The generalized <a id="id1395" class="indexterm"/>learning/training algorithm for all three is given as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">For layers <span class="emphasis"><em>l=1 to L</em></span> (Pre-Training):<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Dataset without Labels <span class="inlinemediaobject"><img src="graphics/B05137_07_151.jpg" alt="How does it work?"/></span></li><li class="listitem">Perform Step-wise Layer Unsupervised Learning (RBM, Autoencoders, or Denoising Autoencoders)</li><li class="listitem">Finalize the parameters <span class="strong"><strong>W</strong></span><sup>l</sup>, <span class="strong"><strong>b</strong></span><sup>l</sup> from the preceding step</li></ol></div></li><li class="listitem">For the output layer <span class="emphasis"><em>(L+1)</em></span> perform random initialization of parameters <span class="strong"><strong>W</strong></span><span class="emphasis"><em><sup>L</sup></em></span><sup>+1</sup>, <span class="strong"><strong>b</strong></span><span class="emphasis"><em><sup>L</sup></em></span><sup> +1</sup>.</li><li class="listitem">For layers <span class="emphasis"><em>l=1 to L+1</em></span> (Fine-Tuning):<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Dataset with Labels <span class="inlinemediaobject"><img src="graphics/B05137_07_156.jpg" alt="How does it work?"/></span>.
</li><li class="listitem">Use the <a id="id1396" class="indexterm"/>pre-initialized weights from 1. (<span class="strong"><strong>W</strong></span><sup>l</sup>, <span class="strong"><strong>b</strong></span><sup>l</sup>).</li><li class="listitem">Perform forward-backpropagation for <span class="emphasis"><em>n</em></span> iterations.</li></ol></div></li></ol></div></div></div><div class="section" title="Deep Autoencoders"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec127"/>Deep Autoencoders</h3></div></div></div><p>Deep <a id="id1397" class="indexterm"/>Autoencoders have many layers of hidden units, which <a id="id1398" class="indexterm"/>shrink to a very small dimension and then symmetrically grow to the input size.</p><div class="mediaobject"><img src="graphics/B05137_07_158.jpg" alt="Deep Autoencoders"/><div class="caption"><p>Figure 13: Deep Autoencoders</p></div></div><p>The idea behind Deep Autoencoders is to create features that capture latent complex structures of input using deep networks and at the same time overcome the issue of gradients and underfitting due to the deep structure. It was shown that this methodology generated better features and <a id="id1399" class="indexterm"/>performed better than PCA on many datasets (<span class="emphasis"><em>References</em></span> [13]).</p><p>Deep Autoencoders use the concept of pre-training, encoders/decoders, and fine-tuning to perform unsupervised learning:</p><p>In the pre-training <a id="id1400" class="indexterm"/>phase, the RBM methodology is used to learn greedy stepwise parameters of the encoders, as shown in the following figure, for initialization:</p><div class="mediaobject"><img src="graphics/B05137_07_159.jpg" alt="Deep Autoencoders"/><div class="caption"><p>Figure 14: Stepwise learning in RBM</p></div></div><p>In the unfolding phase the same parameters are symmetrically applied to the decoder network for initialization.</p><p>Finally, fine-tuning <a id="id1401" class="indexterm"/>backpropagation is used to adjust the parameters across the entire network.</p></div><div class="section" title="Deep Belief Networks"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec128"/>Deep Belief Networks</h3></div></div></div><p>Deep Belief <a id="id1402" class="indexterm"/>Networks (DBNs) are the origin of the concept of unsupervised pre-training (<span class="emphasis"><em>References</em></span> [9]). Unsupervised pre-training originated from DBNs and then was found to be equally useful and effective in the feed-forward supervised deep networks.</p><p>Deep belief networks are not supervised feed-forward networks, but a generative model to generate data samples.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl5sec85"/>Inputs and outputs</h4></div></div></div><p>The input layer is the <a id="id1403" class="indexterm"/>instance of data, represented by <a id="id1404" class="indexterm"/>one neuron for each input feature. The output of a DBN is a reconstruction of the input from a hierarchy of learned features of increasingly greater abstraction.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl5sec86"/>How does it work?</h4></div></div></div><p>How a DBN learns the <a id="id1405" class="indexterm"/>joint distribution of the input data is explained here using a three-layer DBN architecture as an example.</p><div class="mediaobject"><img src="graphics/B05137_07_160.jpg" alt="How does it work?"/><div class="caption"><p>Figure 15: Deep belief network</p></div></div><p>The three-hidden-layered DBN as shown has a first layer of undirected RBM connected to a two-layered Bayesian <a id="id1406" class="indexterm"/>network. The Bayesian network with a sigmoid activation function is called a sigmoid Bayesian network (SBN). </p><p>The goal of a generative model is to learn the joint distribution as given by <span class="emphasis"><em>p</em></span>(<span class="strong"><strong>x</strong></span>,<span class="strong"><strong>h</strong></span><sup>(1)</sup>,<span class="strong"><strong>h</strong></span><sup>(2)</sup>,<span class="strong"><strong>h</strong></span><sup>(3)</sup>)</p><p>
<span class="emphasis"><em>p</em></span>(<span class="strong"><strong>x</strong></span>,<span class="strong"><strong>h</strong></span><sup>(1)</sup>,<span class="strong"><strong>h</strong></span><sup>(2)</sup>,<span class="strong"><strong>h</strong></span><sup>(3)</sup>) = <span class="emphasis"><em>p</em></span>(<span class="strong"><strong>h</strong></span><sup>2</sup>),<span class="strong"><strong>h</strong></span><sup>(3)</sup>)<span class="emphasis"><em>p</em></span>(<span class="strong"><strong>h</strong></span><sup>(1)</sup>|<span class="strong"><strong>h</strong></span><sup>(2)</sup>) <span class="emphasis"><em>p</em></span>(<span class="strong"><strong>x</strong></span>|<span class="strong"><strong>h</strong></span><sup>(1)</sup>)</p><p>RBM computation as seen before gives us:</p><div class="mediaobject"><img src="graphics/B05137_07_163.jpg" alt="How does it work?"/></div><p>The Bayesian Network in the next two layers is: </p><div class="mediaobject"><img src="graphics/B05137_07_164.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_07_165.jpg" alt="How does it work?"/></div><p>For binary data:</p><div class="mediaobject"><img src="graphics/B05137_07_166.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_07_166.jpg" alt="How does it work?"/></div></div></div><div class="section" title="Deep learning with dropouts"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec129"/>Deep learning with dropouts</h3></div></div></div><p>Another technique <a id="id1407" class="indexterm"/>used to overcome the "overfitting" issues mentioned in deep <a id="id1408" class="indexterm"/>neural networks is using the dropout technique to learn the parameters. In the next sections, we will define, illustrate, and explain how deep learning with dropouts works.</p><div class="section" title="Definition and mathematical notation"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec198"/>Definition and mathematical notation</h4></div></div></div><p>The idea <a id="id1409" class="indexterm"/>behind dropouts is to "cripple" the deep neural network structure by stochastically removing some of the hidden units as shown in the following figure after the parameters are learned. The units are set to 0 with the dropout probability generally set as <span class="emphasis"><em>p=0.5</em></span>
</p><p>The idea is similar to adding noise to the input, but done in all the hidden layers. When certain features (or a combination of features) are removed stochastically, the neural network has to learn latent features in a more robust way, without the interdependence of some features.</p><div class="mediaobject"><img src="graphics/B05137_07_169.jpg" alt="Definition and mathematical notation"/><div class="caption"><p>Figure 16: Deep learning with dropout indicated by dropping certain units with dark shading.</p></div></div><p>Each hidden layer is represented by <span class="emphasis"><em>h</em></span><sup>k</sup><span class="emphasis"><em>(x)</em></span> where <span class="emphasis"><em>k</em></span> is the layer. The pre-activation for layer <span class="emphasis"><em>0&lt;k&lt;l</em></span> is given by: </p><div class="mediaobject"><img src="graphics/B05137_07_016.jpg" alt="Definition and mathematical notation"/></div><p>The hidden layer <a id="id1410" class="indexterm"/>activation for <span class="emphasis"><em>1&lt; k &lt; l</em></span>. Binary masks are <a id="id1411" class="indexterm"/>represented by <span class="strong"><strong>m</strong></span><sup>k</sup> at each hidden layer:</p><div class="mediaobject"><img src="graphics/B05137_07_172.jpg" alt="Definition and mathematical notation"/></div><p>The final output layer activation is: </p><div class="mediaobject"><img src="graphics/B05137_07_019.jpg" alt="Definition and mathematical notation"/></div></div><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec199"/>Inputs and outputs</h4></div></div></div><p>For training with dropouts, inputs are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Network architecture</li><li class="listitem" style="list-style-type: disc">Training dataset </li><li class="listitem" style="list-style-type: disc">Dropout probability <span class="emphasis"><em>p</em></span> (typically 0.5)</li></ul></div><p>The output is a trained deep neural net that can be applied for predictive use.</p><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch07lvl5sec87"/>How does it work?</h5></div></div></div><p>We will <a id="id1412" class="indexterm"/>now describe the different parts of how deep learning with dropouts works.</p></div></div><div class="section" title="Learning Training and testing with dropouts"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec200"/>Learning Training and testing with dropouts</h4></div></div></div><p>The <a id="id1413" class="indexterm"/>backward propagation learning of weights and biases from the <a id="id1414" class="indexterm"/>output loss function using gradients is very similar to traditional neural network learning. The only difference is that masks are applied appropriately as follows:</p><p>Compute the output gradient before activation:</p><div class="mediaobject"><img src="graphics/B05137_07_066.jpg" alt="Learning Training and testing with dropouts"/></div><p>For hidden layers <span class="emphasis"><em>k=l+1 to 1</em></span>:</p><p>Compute the gradient of hidden layer parameters:</p><div class="mediaobject"><img src="graphics/B05137_07_068.jpg" alt="Learning Training and testing with dropouts"/></div><div class="mediaobject"><img src="graphics/B05137_07_069.jpg" alt="Learning Training and testing with dropouts"/></div><p>
<span class="strong"><strong>h</strong></span><sup>k-1</sup> computation has taken into account the binary mask <span class="strong"><strong>m</strong></span><sup>k-1</sup> applied.</p><p>Compute the gradient of the hidden layer below the current:</p><div class="mediaobject"><img src="graphics/B05137_07_070.jpg" alt="Learning Training and testing with dropouts"/></div><p>Compute the gradient of the layer below before activation:</p><div class="mediaobject"><img src="graphics/B05137_07_177.jpg" alt="Learning Training and testing with dropouts"/></div><p>When testing the model, we cannot use the binary mask as it is stochastic; the "expectation" value of the mask is used. If the dropout probability is <span class="emphasis"><em>p=0.5</em></span>, the same value 0.5 is used as the expectation for the unit at test or model <a id="id1415" class="indexterm"/>application time.</p></div></div><div class="section" title="Sparse coding"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec130"/>Sparse coding</h3></div></div></div><p>Sparse coding is <a id="id1416" class="indexterm"/>another neural network used for unsupervised <a id="id1417" class="indexterm"/>learning and feature generation (<span class="emphasis"><em>References</em></span> [22]). It works on the principle of finding latent structures in high dimensions that capture the patterns, thus performing feature extraction in addition to unsupervised learning.</p><p>Formally, for every input <span class="strong"><strong>x</strong></span><sup>(t)</sup> a latent representation <span class="strong"><strong>h</strong></span><sup>(t)</sup> is learned, which has a sparse representation (most values are 0 in the vector). This is done by optimization using the following objective function:</p><div class="mediaobject"><img src="graphics/B05137_07_180.jpg" alt="Sparse coding"/></div><p>Where the first term <span class="inlinemediaobject"><img src="graphics/B05137_07_181.jpg" alt="Sparse coding"/></span> is to control the reconstruction error and the second term, which uses a regularizer ?, is for sparsity control. The matrix <span class="strong"><strong>D</strong></span> is also known as a Dictionary as it has equivalence to words in a dictionary and <span class="strong"><strong>h</strong></span><sup>(t)</sup> is similar to word frequency; together they capture the impact of words in extracting patterns when performing text mining.</p></div><div class="section" title="Convolutional Neural Network"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec131"/>Convolutional Neural Network</h3></div></div></div><p>Convolutional <a id="id1418" class="indexterm"/>Neural Networks or <a id="id1419" class="indexterm"/>CNNs have become prominent and are widely used in the computer vision domain. Computer vision involves processing images/videos for capturing knowledge and patterns. Annotating images, classifying images/videos, correcting them, story-telling or describing images, and so on, are some of the broad applications in computer visi [16].</p><p>Computer vision problems most generally have to deal with unstructured data that can be described as:</p><p>Inputs that are 2D images with single or multiple color channels or 3D videos that are high-dimensional vectors.</p><p>The features in these 2D or 3D representations have a well-known spatial topology, a hierarchical structure, and some repetitive elements that can be exploited.</p><p>The images/videos have a large number of transformations or variants based on factors such as illumination, noise, and so on. The same person or car can look different based on several factors.</p><p>Next, we will describe some building blocks used in CNNs. We will use simple images such as the letter X of the alphabet to explain the concept and mathematics involved. For example, even though the same character X is represented in different ways in the following <a id="id1420" class="indexterm"/>figure due to translation, scaling, or distortion, the human eye can easily read it as X, but it becomes tricky for the <a id="id1421" class="indexterm"/>computer to see the pattern. The images are shown with the author's permission (<span class="emphasis"><em>References</em></span> [19]):</p><div class="mediaobject"><img src="graphics/B05137_07_184.jpg" alt="Convolutional Neural Network"/><div class="caption"><p>Figure 17: Image of character X represented in different ways.</p></div></div><p>The following figure illustrates how a simple grayscale image of X has common features such as a diagonal from top left, a diagonal from top right, and left and right intersecting diagonals repeated and combined to form a larger X:</p><div class="mediaobject"><img src="graphics/B05137_07_185.jpg" alt="Convolutional Neural Network"/><div class="caption"><p>Figure 18: Common features represented in the image of character X.</p></div></div><div class="section" title="Local connectivity"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec201"/>Local connectivity</h4></div></div></div><p>This is the <a id="id1422" class="indexterm"/>simple concept of dividing the whole image into "patches" or "recipient fields" and giving each patch to the hidden layers. As shown in the figure, instead of 9 X 9 pixels of the complete sample image, a 3 X 3 patch of pixels from the top left goes to the first hidden unit, the overlapping second patch goes to second, and so on. </p><p>Since the fully connected hidden layer would have a huge number of parameters, having smaller patches completely reduces the parameter or high-dimensional space problem!</p><div class="mediaobject"><img src="graphics/B05137_07_186.jpg" alt="Local connectivity"/><div class="caption"><p>Figure 19: Concept of patches on the whole image. </p></div></div></div><div class="section" title="Parameter sharing"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec202"/>Parameter sharing</h4></div></div></div><p>The <a id="id1423" class="indexterm"/>concept of parameter sharing is to construct a weight matrix that can be reused over different patches or recipient fields as constructed in the preceding figure in the local sharing. As shown in the following figure, the Feature map with same parameters <span class="strong"><strong>W</strong></span><sub>1,1</sub> and <span class="strong"><strong>W</strong></span><sub>1,4</sub> creates two different feature maps, Feature Map 1 and 4, both capturing the same features, that is, diagonal edges on either side. Thus, feature maps capture "similar regions" in the images and further reduce the dimensionality of the input space.</p><div class="mediaobject"><img src="graphics/B05137_07_187.jpg" alt="Parameter sharing"/></div></div><div class="section" title="Discrete convolution"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec203"/>Discrete convolution </h4></div></div></div><p>We will <a id="id1424" class="indexterm"/>explain the steps in discrete convolution, taking a simple contrived example with simplified mathematics to illustrate the operation.</p><p>Suppose the kernel representing the diagonal feature is scanned over the entire image as a patch of 3 X 3. If this kernel lands on the self-same feature in the input image and we have to compute the center value through what we call the convolution operator, we get the exact value of 1 because of the matching as shown:</p><div class="mediaobject"><img src="graphics/B05137_07_190.jpg" alt="Discrete convolution"/><div class="caption"><p>Figure 21: Discrete convolution step.</p></div></div><p>The entire <a id="id1425" class="indexterm"/>image when run through this kernel and convolution operator gives a matrix of values as follows:</p><div class="mediaobject"><img src="graphics/B05137_07_191.jpg" alt="Discrete convolution"/><div class="caption"><p>Figure 22: Transformation of the character image after a kernel and convolution operator.</p></div></div><p>We can see how the left diagonal feature gets highlighted by running this scan. Similarly, by running other kernels, as shown in the following figure, we can get a "stack of filtered images":</p><div class="mediaobject"><img src="graphics/B05137_07_192.jpg" alt="Discrete convolution"/><div class="caption"><p>Figure 23: Different features run through the kernel giving a stack of images.</p></div></div><p>Each cell in <a id="id1426" class="indexterm"/>the filtered images can be given as:</p><div class="mediaobject"><img src="graphics/B05137_07_193.jpg" alt="Discrete convolution"/></div><div class="mediaobject"><img src="graphics/B05137_07_194.jpg" alt="Discrete convolution"/></div><div class="mediaobject"><img src="graphics/B05137_07_195.jpg" alt="Discrete convolution"/></div></div><div class="section" title="Pooling or subsampling"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec204"/>Pooling or subsampling</h4></div></div></div><p>Pooling or subsampling works on the stack of filtered images to further shrink the image or compress it, while keeping the pattern as-is. The main steps carried out in pooling are:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Pick a <a id="id1427" class="indexterm"/>window size (for example, 2 X 2) and a stride size (for example, 2).</li><li class="listitem">Move the window over all the filtered images at stride.</li><li class="listitem">At each window, pick the "maximum" value.</li></ol></div><div class="mediaobject"><img src="graphics/B05137_07_196.jpg" alt="Pooling or subsampling"/><div class="caption"><p>Figure 24: Max pooling, done using a window size of 2 X 2 and stride of 2, computes cell values with maximum for first as 1.0, 0.33 for next, and so on.</p></div></div><p>Pooling also plays an important part where the same features if moved or scaled can still be detected due to the use of maximum. The same set of stacked filtered images gets transformed into pooled images as follows:</p><div class="mediaobject"><img src="graphics/B05137_07_197.jpg" alt="Pooling or subsampling"/><div class="caption"><p>Figure 25: Transformation showing how a stack of filtered images is converted to pooled images.</p></div></div></div><div class="section" title="Normalization using ReLU"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec205"/>Normalization using ReLU</h4></div></div></div><p>As we discussed <a id="id1428" class="indexterm"/>in the building blocks of deep learning, ReLUs remove the negative by squashing it to 0 and keep the positives as-is. They also play an important role in gradient computation in the backpropagation, removing the vanishing gradient issue of vanishing gradient.</p><div class="mediaobject"><img src="graphics/B05137_07_198.jpg" alt="Normalization using ReLU"/><div class="caption"><p>Figure 26: Transformation using ReLu.</p></div></div></div></div><div class="section" title="CNN Layers"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec132"/>CNN Layers</h3></div></div></div><p>In this section, we will put together the building blocks discussed earlier to form the complete <a id="id1429" class="indexterm"/>picture of CNNs. Combining the layers of convolution, ReLU, and pooling to form a connected network yielding shrunken images with patterns captured in the final output, we obtain the next composite building block, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B05137_07_199.jpg" alt="CNN Layers"/><div class="caption"><p>Figure 27: Basic Unit of CNN showing a combination of Convolution, ReLu, and Pooling.</p></div></div><p>Thus, these layers can be combined or "deep-stacked", as shown in the following figure, to form a <a id="id1430" class="indexterm"/>complex network that gives a small pool of images as output:</p><div class="mediaobject"><img src="graphics/B05137_07_200.jpg" alt="CNN Layers"/><div class="caption"><p>Figure 28: Deep-stacking the basic units repeatedly to form CNN layers.</p></div></div><p>The output layer is a fully connected network as shown, which uses a voting technique and learns the weights for the desired output. The fully connected output layer can be stacked too.</p><div class="mediaobject"><img src="graphics/B05137_07_201.jpg" alt="CNN Layers"/><div class="caption"><p>Figure 29: Fully connected layer as output of CNN.</p></div></div><p>Thus, the final CNNs can be completely illustrated as follows:</p><div class="mediaobject"><img src="graphics/B05137_07_202.jpg" alt="CNN Layers"/><div class="caption"><p>Figure 30: CNNS with all layers showing inputs and outputs.</p></div></div><p>As <a id="id1431" class="indexterm"/>before, gradient descent is selected as the learning technique using the loss functions to compute the difference and propagate the error backwards.</p><p>CNN's can be used in other domains such as voice pattern recognition, text mining, and so on, if the mapping of the data to the "image" can be successfully done and "local spatial" patterns <a id="id1432" class="indexterm"/>exist. The following figure shows one of the ways of mapping sound and text to images for CNN usage:</p><div class="mediaobject"><img src="graphics/B05137_07_203.jpg" alt="CNN Layers"/><div class="caption"><p>Figure 31: Illustration of mapping between temporal data, such as voice to spatial data, to an image.</p></div></div></div><div class="section" title="Recurrent Neural Networks"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec133"/>Recurrent Neural Networks</h3></div></div></div><p>Normal <a id="id1433" class="indexterm"/>deep networks are used when <a id="id1434" class="indexterm"/>you have finite inputs and there is no interdependence between the input examples or instances. When there are variable length inputs and there are temporal dependencies between them, that is, sequence related data, neural networks must be modified to handle such data. Recurrent Neural Networks (RNN) are examples of neural networks that are used widely to solve such problems, and we will discuss them in the following sections. RNNs are used in many sequence-related problems such as text mining, language modeling, bioinformatics data modeling, and so on, to name a few areas that fit this meta-level description (<span class="emphasis"><em>References</em></span> [18 and 21]).</p><div class="section" title="Structure of Recurrent Neural Networks"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec206"/>Structure of Recurrent Neural Networks</h4></div></div></div><p>We will <a id="id1435" class="indexterm"/>describe the simplest unit of the RNN first and then shown how it is combined to understand it functionally and mathematically and illustrate how different components interact and work. </p><div class="mediaobject"><img src="graphics/B05137_07_204.jpg" alt="Structure of Recurrent Neural Networks"/><div class="caption"><p>Figure 32: Difference between an artificial neuron and a neuron with feedback.</p></div></div><p>Let's consider the <a id="id1436" class="indexterm"/>basic input, a neuron with activation, and its output at a given time <span class="emphasis"><em>t</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_07_206.jpg" alt="Structure of Recurrent Neural Networks"/></div><div class="mediaobject"><img src="graphics/B05137_07_207.jpg" alt="Structure of Recurrent Neural Networks"/></div><p>A neuron with feedback keeps a matrix <span class="strong"><strong>W</strong></span><sub>R</sub> to incorporate previous output at time <span class="emphasis"><em>t-1</em></span> and the equations are: </p><div class="mediaobject"><img src="graphics/B05137_07_210.jpg" alt="Structure of Recurrent Neural Networks"/></div><div class="mediaobject"><img src="graphics/B05137_07_207.jpg" alt="Structure of Recurrent Neural Networks"/></div><div class="mediaobject"><img src="graphics/B05137_07_211.jpg" alt="Structure of Recurrent Neural Networks"/><div class="caption"><p>Figure 33: Chain of neurons with feedbacks connected together.</p></div></div><p>The basic <a id="id1437" class="indexterm"/>RNN stacks the structure of hidden units as shown with feedback connected from the previous layer. At activation at time <span class="emphasis"><em>t</em></span>, it depends not only on <span class="strong"><strong>x</strong></span><sup>(t)</sup> as input, but also on the previous unit given by <span class="strong"><strong>W</strong></span><sub>R</sub><span class="strong"><strong>h</strong></span><sup>(t-1)</sup>. The weights in the feedback connection of RNN are generally the same across all the units, <span class="strong"><strong>W</strong></span><sub>R</sub>. Also, instead of emitting output at the very end of the feed-forward neural network, each unit continuously emits an output that can be used in the loss function calculation.</p></div><div class="section" title="Learning and associated problems in RNNs"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec207"/>Learning and associated problems in RNNs</h4></div></div></div><p>Working with RNNs <a id="id1438" class="indexterm"/>presents some challenges that are <a id="id1439" class="indexterm"/>specific to them but there are common problems that are also encountered in other types of neural net.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The gradient used from the output loss function at any time <span class="emphasis"><em>t</em></span> of the unit has dependency going back to the first unit or <span class="emphasis"><em>t=0</em></span>, as shown in the following figure. This is because the partial derivative at the unit is dependent on the previous unit, since:<div class="mediaobject"><img src="graphics/B05137_07_210.jpg" alt="Learning and associated problems in RNNs"/></div><p>Backpropagation through time (BPTT) is the term used to illustrate the process.</p><div class="mediaobject"><img src="graphics/B05137_07_215.jpg" alt="Learning and associated problems in RNNs"/><div class="caption"><p>Figure 34: Backpropagation through time.</p></div></div></li><li class="listitem">Similar to what we saw in the section on feed-forward neural networks, the cases of exploding and vanishing gradient become more pronounced in RNNs due to the connectivity of units as discussed previously. </li><li class="listitem">Some of the <a id="id1440" class="indexterm"/>solutions for exploding gradients are:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Truncated BPTT is <a id="id1441" class="indexterm"/>a small change to the BPTT process. Instead of propagating the learning back to time <span class="emphasis"><em>t=0</em></span>, it is truncated to a fixed time backward to <span class="emphasis"><em>t=k</em></span>.</li><li class="listitem">Gradient Clipping to cut the gradient above a threshold when it shoots up.</li><li class="listitem">Adaptive Learning Rate. The learning rate adjusts itself based on the feedback and values.</li></ol></div></li><li class="listitem">Some of the solutions for vanishing gradients are: <div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Using ReLU as the activation function; hence the gradient will be 1.</li><li class="listitem">Adaptive Learning Rate. The learning rate adjusts itself based on the feedback and values. </li><li class="listitem">Using extensions such as Long Short Term Memory (LSTM) and Gated Recurrent Units (GRUs), which we will describe next.</li></ol></div></li></ol></div><p>There are many applications of RNNs, for example, in next letter predictions, next word predictions, language translation, and so on.</p><div class="mediaobject"><img src="graphics/B05137_07_218.jpg" alt="Learning and associated problems in RNNs"/><div class="caption"><p>Figure 35: Showing some applications in next letter/word predictions using RNN structures.</p></div></div></div><div class="section" title="Long Short Term Memory"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec208"/>Long Short Term Memory</h4></div></div></div><p>One <a id="id1442" class="indexterm"/>of the neural network architectures or modifications to RNNs that addresses the issue of vanishing gradient is known as long short term memory or LSTM. We will explain some building blocks of LSTM and then put it together for our readers. </p><p>The first modification to RNN is to change the feedback learning matrix to 1, that is, <span class="strong"><strong>W</strong></span><sub>R</sub> = 1, as shown in the following figure: </p><div class="mediaobject"><img src="graphics/B05137_07_220.jpg" alt="Long Short Term Memory"/><div class="caption"><p>Figure 36: Building blocks of LSTM where the feedback matrix is set to 1.</p></div></div><p>This will ensure the inputs from older cell or memory units are passed as-is to the next unit. Hence some modifications are needed.</p><p>The output gate, as shown in the following figure, combines two computations. The first is the <a id="id1443" class="indexterm"/>output from the individual unit, passed through an activation function, and the second is the output of the older unit that has been passed through a sigmoid using scaling.</p><div class="mediaobject"><img src="graphics/B05137_07_221.jpg" alt="Long Short Term Memory"/><div class="caption"><p>Figure 37: Building block Output Gate for LSTM.</p></div></div><p>Mathematically, the output gate at the unit is given by:</p><div class="mediaobject"><img src="graphics/B05137_07_222.jpg" alt="Long Short Term Memory"/></div><div class="mediaobject"><img src="graphics/B05137_07_223.jpg" alt="Long Short Term Memory"/></div><p>The forget gate is between the two memory units. It generates 0 or 1 based on learned weights and transformations. The forget gate is shown in the following figure:</p><div class="mediaobject"><img src="graphics/B05137_07_224.jpg" alt="Long Short Term Memory"/><div class="caption"><p>Figure 38: Building block Forget Gate addition to LSTM. </p></div></div><p>Mathematically, <span class="inlinemediaobject"><img src="graphics/B05137_07_225.jpg" alt="Long Short Term Memory"/></span> can be seen as the representation of the forget gate. Next, the <a id="id1444" class="indexterm"/>input gate and the new gate are combined, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B05137_07_226.jpg" alt="Long Short Term Memory"/><div class="caption"><p>Figure 39: Building blocks New Gate and Input Gate added to complete LSTM.</p></div></div><p>The new memory generation unit uses the current input <span class="emphasis"><em>x</em></span><sub>t</sub> and the old state <span class="emphasis"><em>h</em></span><sub>t-1</sub> through an activation function and generates a new memory <span class="emphasis"><em>C</em></span><sub>t</sub>. The input gate combines the input and the old state and determines whether the new memory or the input should be preserved. </p><p>Thus, the update equation looks like this:</p><div class="mediaobject"><img src="graphics/B05137_07_230.jpg" alt="Long Short Term Memory"/></div></div><div class="section" title="Gated Recurrent Units"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec209"/>Gated Recurrent Units</h4></div></div></div><p>Gated <a id="id1445" class="indexterm"/>Recurrent Units (GRUs) are simplified LSTMs with modifications. Many of the gates are simplified by using one "update" unit as follows:</p><div class="mediaobject"><img src="graphics/B05137_07_231.jpg" alt="Gated Recurrent Units"/><div class="caption"><p>Figure 40: GRUs with Update unit.</p></div></div><p>The <a id="id1446" class="indexterm"/>changes made to the equations are: </p><div class="mediaobject"><img src="graphics/B05137_07_232.jpg" alt="Gated Recurrent Units"/></div><div class="mediaobject"><img src="graphics/B05137_07_233.jpg" alt="Gated Recurrent Units"/></div><div class="mediaobject"><img src="graphics/B05137_07_234.jpg" alt="Gated Recurrent Units"/></div><div class="mediaobject"><img src="graphics/B05137_07_235.jpg" alt="Gated Recurrent Units"/></div></div></div></div></div>
<div class="section" title="Case study"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec63"/>Case study</h1></div></div></div><p>Several benchmarks <a id="id1447" class="indexterm"/>exist for image classification. We will use the MNIST image database for this case study. When we used MNIST in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, Unsupervised Machine Learning Techniques with clustering and outlier detection techniques, each pixel was considered a feature. In addition to learning from the pixel values as in previous experiments, with deep learning techniques we will also be learning new features from the structure of the training dataset. The deep learning algorithms will be trained on 60,000 images and tested on a 10,000-image test dataset. </p><div class="section" title="Tools and software"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec111"/>Tools and software</h2></div></div></div><p>In this chapter, we <a id="id1448" class="indexterm"/>introduce the open-source Java framework for deep learning called DeepLearning4J (DL4J). DL4J has libraries implementing a host of deep learning techniques and they can be used on distributed CPUs and GPUs. </p><p>DeepLearning4J: <a class="ulink" href="https://deeplearning4j.org/index.html">https://deeplearning4j.org/index.html</a>
</p><p>We will illustrate the use of some DL4J libraries in learning from the MNIST training images and apply the learned models to classify the images in the test set.</p></div><div class="section" title="Business problem"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec112"/>Business problem</h2></div></div></div><p>Image <a id="id1449" class="indexterm"/>classification is a particularly attractive test-bed to evaluate deep learning networks. We have previously encountered the MNIST database, which consists of greyscale images of handwritten digits. This time, we will show how both unsupervised and supervised deep learning techniques can be used to learn from the same dataset. The MNIST dataset has 28-by-28 pixel images in a single channel. These images are categorized into 10 labels representing the digits 0 to 9. The goal is to train on 60,000 data points and test our deep learning classification algorithm on the remaining 10,000 images.</p></div><div class="section" title="Machine learning mapping"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec113"/>Machine learning mapping</h2></div></div></div><p>This includes <a id="id1450" class="indexterm"/>supervised and unsupervised methods applied to a classification problem in which there are 10 possible output classes. Some techniques use an initial pre-training stage, which is unsupervised in nature, as we have seen in the preceding sections.</p></div><div class="section" title="Data sampling and transfor"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec114"/>Data sampling and transfor</h2></div></div></div><p>The dataset is available at:</p><p>
<a class="ulink" href="https://yann.lecun.com/exdb/mnist">https://yann.lecun.com/exdb/mnist</a>
</p><p>In the experiments in this case study, the MNIST dataset has been standardized such that pixel values in the range 0 to 255 have been normalized to values from 0.0 to 1.0. The exception is in the experiment using stacked RBMs, where the training and test data have been binarized, that is, set to 1 if the standardized value is greater than or equal to 0.3 and 0 otherwise. Each of the 10 classes is equally represented in both the training set and the test set. In addition, examples are shuffled using a random number generator seed supplied by the user.</p></div><div class="section" title="Feature analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec115"/>Feature analysis</h2></div></div></div><p>The input data <a id="id1451" class="indexterm"/>features are the greyscale values of the pixels in each image. This is the raw data and we will be using the deep learning algorithms to learn higher-level features out of the raw pixel values. The dataset has been prepared such that there are an equal number of examples of each class in both the training and the test sets.</p></div><div class="section" title="Models, results, and evaluation"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec116"/>Models, results, and evaluation</h2></div></div></div><p>We will <a id="id1452" class="indexterm"/>perform different experiments starting with simple MLP, Convolutional Networks, Variational Autoencoders, Stacked RBMS, and DBNs. We will walk through important parts of code that highlight the network structure or specialized tunings, give parameters to help readers, reproduce the experiments, and give the results for each type of network. </p><div class="section" title="Basic data handling"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec134"/>Basic data handling </h3></div></div></div><p>The <a id="id1453" class="indexterm"/>following snippet of code shows: </p><p>How to generically read data from a CSV with a structure enforced by delimiters.</p><p>How to iterate the data and get records.</p><p>How to shuffle data in memory and create training/testing or validation sets:</p><div class="informalexample"><pre class="programlisting">RecordReader recordReader = new  ] CSVRecordReader(numLinesToSkip,delimiter);
recordReader.initialize(new FileSplit(new ClassPathResource(fileName).getFile()));
DataSetIterator iterator = new RecordReaderDataSetIterator(recordReader,batchSize,labelIndex,numClasses);
DataSet allData = iterator.next();
allData.shuffle();
SplitTestAndTrain testAndTrain = allData.splitTestAndTrain(trainPercent); 
DataSet trainingData = testAndTrain.getTrain();
DataSet testData = testAndTrain.getTest();</pre></div><p>DL4J <a id="id1454" class="indexterm"/>has a specific MNIST wrapper for handling the data that we have used, as shown in the following snippet:</p><div class="informalexample"><pre class="programlisting">DataSetIterator mnistTrain = new MnistDataSetIterator(batchSize, true, randomSeed);
DataSetIterator mnistTest = new MnistDataSetIterator(batchSize, false, randomSeed);</pre></div></div><div class="section" title="Multi-layer perceptron"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec135"/>Multi-layer perceptron</h3></div></div></div><p>In the first <a id="id1455" class="indexterm"/>experiment, we will use a basic <a id="id1456" class="indexterm"/>multi-layer perceptron with an input layer, one hidden layer, and an output layer. A detailed list of parameters that are used in the code is given here:</p><div class="section" title="Parameters used for MLP"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec210"/>Parameters used for MLP</h4></div></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Parameter</p>
</th><th style="text-align: left" valign="bottom">
<p>Variable</p>
</th><th style="text-align: left" valign="bottom">
<p>Value</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Number of iterations</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>m</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Learning rate</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>rate</p>
</td><td style="text-align: left" valign="top">
<p>0.0015</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Momentum</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>momentum</p>
</td><td style="text-align: left" valign="top">
<p>0.98</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>L2 regularization</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>regularization</p>
</td><td style="text-align: left" valign="top">
<p>0.005</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Number of rows in input</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>numRows</p>
</td><td style="text-align: left" valign="top">
<p>28</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Number of columns in input</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>numColumns</p>
</td><td style="text-align: left" valign="top">
<p>28</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Layer 0 output size, Layer 1 input size </strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>outputLayer0, inputLayer1</p>
</td><td style="text-align: left" valign="top">
<p>500</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Layer 1 output size, Layer 2 input size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>outputLayer1, inputLayer2</p>
</td><td style="text-align: left" valign="top">
<p>300</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Layer 2 output size, Layer 3 input size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>outputLayer2, inputLayer3</p>
</td><td style="text-align: left" valign="top">
<p>100</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Layer 3 output size,</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>outputNum</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td></tr></tbody></table></div></div><div class="section" title="Code for MLP"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec211"/>Code for MLP</h4></div></div></div><p>In the listing that <a id="id1457" class="indexterm"/>follows, we can see how we first configure the MLP by passing in the hyperarameters using the Builder pattern.  </p><div class="informalexample"><pre class="programlisting">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() .seed(randomSeed) .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // use SGD
.iterations(m)//iterations
.activation(Activation.RELU)//activation function
.weightInit(WeightInit.XAVIER)//weight initialization
.learningRate(rate) //specify the learning rate
.updater(Updater.NESTEROVS).momentum(momentum)//momentum
.regularization(true).l2(rate * regularization) // 
.list()
.layer(0, 
new DenseLayer.Builder() //create the first input layer.
.nIn(numRows * numColumns)
.nOut(firstOutput)
.build())
.layer(1, new DenseLayer.Builder() //create the second input layer
.nIn(secondInput)
.nOut(secondOutput)
.build())
.layer(2, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD) //create hidden layer
.activation(Activation.SOFTMAX)
.nIn(thirdInput)
.nOut(numberOfOutputClasses)
.build())
.pretrain(false).backprop(true) //use backpropagation to adjust weights
.build();</pre></div><p>Training, evaluation, and testing the MLP are shown in the following snippet. Notice the code that <a id="id1458" class="indexterm"/>initializes the visualization backend enabling you to monitor the model training in your browser, particularly the model score (the training error after each iteration) and updates to parameters:</p><div class="informalexample"><pre class="programlisting">MultiLayerNetwork model = new MultiLayerNetwork(conf);
model.init();
model.setListeners(new ScoreIterationListener(5));  //print the score with every iteration
//Initialize the user interface backend
UIServer uiServer = UIServer.getInstance();
//Configure where the network information (gradients, activations, score vs. time etc) is to be stored
//Then add the StatsListener to collect this information from the network, as it trains
StatsStorage statsStorage = new InMemoryStatsStorage();             //Alternative: new FileStatsStorage(File) - see UIStorageExample
int listenerFrequency = 1;
net.setListeners(new StatsListener(statsStorage, listenerFrequency));
//Attach the StatsStorage instance to the UI: this allows the contents of the StatsStorage to be visualized
uiServer.attach(statsStorage);
log.info(""Train model...."");
for( int i=0; i&lt;numEpochs; i++ ){
log.info(""Epoch "" + i);
model.fit(mnistTrain);
        }
log.info(""Evaluate model...."");
Evaluation eval = new Evaluation(numberOfOutputClasses); 
while(mnistTest.hasNext()){
DataSet next = mnistTest.next();
INDArray output = model.output(next.getFeatureMatrix()); //get the networks prediction
eval.eval(next.getLabels(), output); //check the prediction against the true class
        }
log.info(eval.stats());</pre></div><p>The following <a id="id1459" class="indexterm"/>plots show the training error against training iteration for the MLP model. This curve should decrease with iterations: </p><div class="mediaobject"><img src="graphics/B05137_07_236.jpg" alt="Code for MLP"/><div class="caption"><p>Figure 41: Training error as measured with number of iterations of training for the MLP model.</p></div></div><p>In the following <a id="id1460" class="indexterm"/>figure, we see the distribution of parameters in Layer 0 of the MLP as well as the distribution of updates to the parameters. These histograms should have an approximately Gaussian (Normal) shape, which indicates good convergence. For more on how to use charts to tune your model, see the DL4J Visualization page (<a class="ulink" href="https://deeplearning4j.org/visualization">https://deeplearning4j.org/visualization</a>):</p><div class="mediaobject"><img src="graphics/B05137_07_237.jpg" alt="Code for MLP"/><div class="caption"><p>Figure 42: Histograms showing Layer parameters and update distribution.</p></div></div></div></div><div class="section" title="Convolutional Network"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec136"/>Convolutional Network</h3></div></div></div><p>In the second <a id="id1461" class="indexterm"/>experiment, we configured a Convolutional Network (ConvNet) using the built-in MultiLayerConfiguration. The architecture of the network consists of a total of five layers, as can be seen from the following code snippet. Following the input layer, two convolution layers with 5-by-5 filters alternating with Max pooling layers are followed by a fully connected dense layer using the ReLu activation layer, ending with Softmax activation in the final output layer. The optimization algorithm used is Stochastic Gradient Descent, and the loss function is Negative Log Likelihood.</p><p>The various configuration parameters (or hyper-parameters) for the ConvNet are given in the table.</p><div class="section" title="Parameters used for ConvNet"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec212"/>Parameters used for ConvNet</h4></div></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Parameter</p>
</th><th style="text-align: left" valign="bottom">
<p>Variable</p>
</th><th style="text-align: left" valign="bottom">
<p>Value</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Seed</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>seed</p>
</td><td style="text-align: left" valign="top">
<p>123</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Input size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>numRows, numColumns</p>
</td><td style="text-align: left" valign="top">
<p>28, 28</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Number of epochs</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>numEpochs</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Number of iterations</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iterations</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>L2 regularization</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>regularization</p>
</td><td style="text-align: left" valign="top">
<p>0.005</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Learning rate</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>learningRate</p>
</td><td style="text-align: left" valign="top">
<p>0.1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Momentum</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>momentum</p>
</td><td style="text-align: left" valign="top">
<p>0.9</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Convolution filter size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>xsize, ysize</p>
</td><td style="text-align: left" valign="top">
<p>5, 5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Convolution layers stride size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>x, y</p>
</td><td style="text-align: left" valign="top">
<p>1, 1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Number of input channels</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>numChannels</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Subsampling layer stride size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>sx, sy</p>
</td><td style="text-align: left" valign="top">
<p>2, 2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Layer 0 output size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>nOut0</p>
</td><td style="text-align: left" valign="top">
<p>20</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Layer 2 output size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>nOut1</p>
</td><td style="text-align: left" valign="top">
<p>50</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Layer 4 output size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>nOut2</p>
</td><td style="text-align: left" valign="top">
<p>500</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Layer 5 output size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>outputNum</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td></tr></tbody></table></div></div><div class="section" title="Code for CNN"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec213"/>Code for CNN</h4></div></div></div><p>As you can <a id="id1462" class="indexterm"/>see, configuring multi-layer neural networks with the DL4J API is similar whether you are building MLPs or CNNs.  Algorithm-specific configuration is simply done in the definition of each layer.</p><div class="informalexample"><pre class="programlisting">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
.seed(seed)
.iterations(iterations) .regularization(true).l2(regularization)
.learningRate(learningRate)
.weightInit(WeightInit.XAVIER) .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) .updater(Updater.NESTEROVS).momentum(momentum)
.list()
.layer(0, new ConvolutionLayer.Builder(xsize, ysize)
.nIn(nChannels)
.stride(x,y)
.nOut(nOut0)
.activation(Activation.IDENTITY)
.build())
.layer(1, new SubsamplingLayer
.Builder(SubsamplingLayer.PoolingType.MAX)
.kernelSize(width, height)
.stride(sx,sy)
.build())
.layer(2, new ConvolutionLayer.Builder(xsize, ysize)
.stride(x,y)
.nOut(nOut2)
.activation(Activation.IDENTITY)
.build())
.layer(3, new SubsamplingLayer
.Builder(SubsamplingLayer.PoolingType.MAX)
.kernelSize(width, height)
.stride(sx,sy)
.build())
.layer(4, new DenseLayer.Builder()
.activation(Activation.RELU)
.nOut(nOut4).build())
.layer(5, new OutputLayer. Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
.nOut(outputNum)
.activation(Activation.SOFTMAX)
.build())
.setInputType(InputType.convolutionalFlat(numRows,numColumns,1)) 
.backprop(true).pretrain(false).build();</pre></div></div></div><div class="section" title="Variational Autoencoder"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec137"/>Variational Autoencoder</h3></div></div></div><p>In the third <a id="id1463" class="indexterm"/>experiment, we configure a Variational Autoencoder as the classifier.</p><div class="section" title="Parameters used for the Variational Autoencoder"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec214"/>Parameters used for the Variational Autoencoder </h4></div></div></div><p>The parameters used to configure the VAE are shown in the table.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Parameter</p>
</th><th style="text-align: left" valign="bottom">
<p>Variable</p>
</th><th style="text-align: left" valign="bottom">
<p>Values</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Seed for RNG</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>rngSeed</p>
</td><td style="text-align: left" valign="top">
<p>12345</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Number of iterations </strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Iterations</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Learning rate</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>learningRate</p>
</td><td style="text-align: left" valign="top">
<p>0.001</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>RMS decay</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>rmsDecay</p>
</td><td style="text-align: left" valign="top">
<p>0.95</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>L2 regularization</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>regularization</p>
</td><td style="text-align: left" valign="top">
<p>0.0001</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Output layer size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>outputNum</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>VAE encoder layers size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>vaeEncoder1, vaeEncoder2</p>
</td><td style="text-align: left" valign="top">
<p>256, 256</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>VAE decoder layers size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>vaeDecoder1, vaeDecoder2</p>
</td><td style="text-align: left" valign="top">
<p>256, 256</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Size of latent variable space</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>latentVarSpaceSize</p>
</td><td style="text-align: left" valign="top">
<p>128</p>
</td></tr></tbody></table></div></div><div class="section" title="Code for Variational Autoencoder"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec215"/>Code for Variational <a id="id1464" class="indexterm"/>Autoencoder
</h4></div></div></div><p>We have configured two layers each of encoders and decoders and are reconstructing the input using a Bernoulli distribution.</p><div class="informalexample"><pre class="programlisting">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
.seed(rngSeed)
.iterations(iterations)
.optimizationAlgo(
OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
.learningRate(learningRate)
.updater(Updater.RMSPROP).rmsDecay(rmmsDecay)
.weightInit(WeightInit.XAVIER)
.regularization(true).l2(regulaization)
.list()
.layer(0, new VariationalAutoencoder.Builder()
.activation(Activation.LEAKYRELU)
                .encoderLayerSizes(vaeEncoder1, vaeEncoder2)        //2 encoder layers
                .decoderLayerSizes(vaeDecoder1, vaeDecoder2)        //2 decoder layers
.pzxActivationFunction(""identity"")  //p(z|data) activation function
.reconstructionDistribution(new BernoulliReconstructionDistribution(Activation.SIGMOID.getActivationFunction()))     //Bernoulli distribution for p(data|z) (binary or 0 to 1 data only)
.nIn(numRows * numColumns) //Input size                      
.nOut(latentVarSpaceSize) //Size of the latent variable space: p(z|x).
.build())
.layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD).activation(Activation.SOFTMAX)
.nIn(latentVarSpaceSize).nOut(outputNum).build())
.pretrain(true).backprop(true).build();</pre></div></div></div><div class="section" title="DBN"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec138"/>DBN</h3></div></div></div><p>The parameters used in <a id="id1465" class="indexterm"/>DBN are shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Parameter</p>
</th><th style="text-align: left" valign="bottom">
<p>Variable</p>
</th><th style="text-align: left" valign="bottom">
<p>Value</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Input data size</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>numRows, numColumns</p>
</td><td style="text-align: left" valign="top">
<p>28, 28</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Seed for RNG</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>seed</p>
</td><td style="text-align: left" valign="top">
<p>123</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Number of training iterations</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iterations</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Momentum</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>momentum</p>
</td><td style="text-align: left" valign="top">
<p>0.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Layer 0 (input)</strong></span>
</p>
<p>
<span class="strong"><strong>Layer 0 (output)</strong></span>
</p>
<p>
<span class="strong"><strong>Layer 1 (input, output)</strong></span>
</p>
<p>
<span class="strong"><strong>Layer 2 (input, output)</strong></span>
</p>
<p>
<span class="strong"><strong>Layer 3 (input, output)</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>numRows * numColumns</p>
<p>nOut0</p>
<p>nIn1, nOut1</p>
<p>nIn2, nOut2</p>
<p>nIn3, outputNum</p>
</td><td style="text-align: left" valign="top">
<p>28 * 28</p>
<p>500</p>
<p>500, 250</p>
<p>250, 200</p>
<p>200, 10</p>
</td></tr></tbody></table></div><p>Configuring the DBN using the DL4J API is shown in the example used in this case study. The code for the <a id="id1466" class="indexterm"/>configuration of the network is shown here.</p><div class="informalexample"><pre class="programlisting">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
.seed(seed)
.gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)
.gradientNormalizationThreshold(1.0)
.iterations(iterations)
.updater(Updater.NESTEROVS)
.momentum(momentum)
.optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)
.list()
.layer(0, new RBM.Builder().nIn(numRows*numColumns).nOut(nOut0)
.weightInit(WeightInit.XAVIER).lossFunction(LossFunction.KL_DIVERGENCE)
.visibleUnit(RBM.VisibleUnit.BINARY)
.hiddenUnit(RBM.HiddenUnit.BINARY)
.build())
.layer(1, new RBM.Builder().nIn(nIn1).nOut(nOut1)
.weightInit(WeightInit.XAVIER).lossFunction(LossFunction.KL_DIVERGENCE)
.visibleUnit(RBM.VisibleUnit.BINARY)
.hiddenUnit(RBM.HiddenUnit.BINARY)
.build())
.layer(2, new RBM.Builder().nIn(nIn2).nOut(nOut2)
.weightInit(WeightInit.XAVIER).lossFunction(LossFunction.KL_DIVERGENCE)
.visibleUnit(RBM.VisibleUnit.BINARY)
.hiddenUnit(RBM.HiddenUnit.BINARY)
.build())
.layer(3, new OutputLayer.Builder().nIn(nIn3).nOut(outputNum)
.weightInit(WeightInit.XAVIER).activation(Activation.SOFTMAX)
.build())
.pretrain(true).backprop(true)
.build();
MultiLayerNetwork model = new MultiLayerNetwork(conf);
model.init();
model.setListeners(new ScoreIterationListener(listenerFreq));</pre></div></div><div class="section" title="Parameter search using Arbiter"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec139"/>Parameter search using Arbiter</h3></div></div></div><p>DeepLearning4J provides a framework for fine-tuning hyper-parameters by taking the burden of <a id="id1467" class="indexterm"/>hand-tuning away from the modeler; instead, it allows the specification of the parameter space to search. In the following example code snippet, the configuration is specified using a MultiLayerSpace instead of a MutiLayerConfiguration object, in which the ranges for the hyper-parameters are specified by means of ParameterSpace objects in the Arbiter DL4J package for the parameters to be tuned:</p><div class="informalexample"><pre class="programlisting">ParameterSpace&lt;Double&gt; learningRateHyperparam = new ContinuousParameterSpace(0.0001, 0.1);  //Values will be generated uniformly at random between 0.0001 and 0.1 (inclusive)
ParameterSpace&lt;Integer&gt; layerSizeHyperparam = new IntegerParameterSpace(16,256);            //Integer values will be generated uniformly at random between 16 and 256 (inclusive)
MultiLayerSpace hyperparameterSpace = new MultiLayerSpace.Builder()
//These next few options: fixed values for all models
.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
.iterations(1)
.regularization(true)
.l2(0.0001)
//Learning rate: this is something we want to test different values for
.learningRate(learningRateHyperparam)
.addLayer( new DenseLayerSpace.Builder()
//Fixed values for this layer:
.nIn(784)  //Fixed input: 28x28=784 pixels for MNIST
.activation(""relu"")
//One hyperparameter to infer: layer size
.nOut(layerSizeHyperparam)
.build())
.addLayer( new OutputLayerSpace.Builder()
//nIn: set the same hyperparemeter as the nOut for the last layer.
.nIn(layerSizeHyperparam)
//The remaining hyperparameters: fixed for the output layer
.nOut(10)
.activation(""softmax"")
.lossFunction(LossFunctions.LossFunction.MCXENT)
.build())
.pretrain(false).backprop(true).build();</pre></div></div><div class="section" title="Results and analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec140"/>Results and analysis</h3></div></div></div><p>The results <a id="id1468" class="indexterm"/>of evaluating the performance of the four networks on the test data are given in the following table: </p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>MLP</p>
</th><th style="text-align: left" valign="bottom">
<p>ConvNet</p>
</th><th style="text-align: left" valign="bottom">
<p>VAE</p>
</th><th style="text-align: left" valign="bottom">
<p>DBN</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Accuracy</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.9807</p>
</td><td style="text-align: left" valign="top">
<p>0.9893</p>
</td><td style="text-align: left" valign="top">
<p>0.9743</p>
</td><td style="text-align: left" valign="top">
<p>0.7506</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Precision</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.9806</p>
</td><td style="text-align: left" valign="top">
<p>0.9893</p>
</td><td style="text-align: left" valign="top">
<p>0.9742</p>
</td><td style="text-align: left" valign="top">
<p>0.7498</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Recall</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.9805</p>
</td><td style="text-align: left" valign="top">
<p>0.9891</p>
</td><td style="text-align: left" valign="top">
<p>0.9741</p>
</td><td style="text-align: left" valign="top">
<p>0.7454</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>F1 score</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.9806</p>
</td><td style="text-align: left" valign="top">
<p>0.9892</p>
</td><td style="text-align: left" valign="top">
<p>0.9741</p>
</td><td style="text-align: left" valign="top">
<p>0.7476</p>
</td></tr></tbody></table></div><p>The goal of the experiments was not to match benchmark results in each of the neural network structures, but to give a comprehensive architecture implementation in the code with detailed parameters for the readers to explore.</p><p>Tuning the hyper-parameters in deep learning Networks is quite a challenge and though Arbiter and online resources such as gitter ( <a class="ulink" href="https://gitter.im/deeplearning4j/deeplearning4j">https://gitter.im/deeplearning4j/deeplearning4j</a>) help with DL4J, the time and cost of running the hyper-parameter search is quite high as compared to other classification techniques including SVMs. </p><p>The benchmark <a id="id1469" class="indexterm"/>results on the MNIST dataset and corresponding papers are available here: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354">http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354</a></li></ul></div><p>As seen from the benchmark result, Linear 1 Layer NN gets an error rate of 12% and adding more layers reduces it to about 2. This shows the non-linear nature of the data and the need for a complex algorithm to fit the patterns.</p><p>As compared to the benchmark best result on neural networks ranging from a 2.5% to 1.6% error rate, our results are very much comparable with the 2% error rate. </p><p>Most of the benchmark results show Convolutional Network architectures having error rates in the range of 1.1% to 0.5% and our hyper-parameter search has matched the best of those models with an error rate of just under 1.1%. </p><p>Our results for DBN fall far short of the benchmarks at just over 25%. There is no reason to doubt that further tuning can improve performance bringing it to the range of 3-5%.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec64"/>Summary</h1></div></div></div><p>The history of Deep Learning is intimately tied to the limitations of earlier attempts at using neural networks in machine learning and AI, and how these limitations were overcome with newer techniques, technological improvements, and the availability of vast amounts of data.</p><p>The perceptron is the basic neural network. Multi-layer networks are used in supervised learning and are built by connecting several hidden layers of neurons to propagate activations forward and using backpropagation to reduce the training error. Several activation functions are used, most commonly, the sigmoid and tanh functions.</p><p>The problems of neural networks are vanishing or exploding gradients, slow training, and the trap of local minima.</p><p>Deep learning successfully addresses these problems with the help of several effective techniques that can be used for unsupervised as well as supervised learning.</p><p>Among the building blocks of deep learning networks are Restricted Boltzmann Machines (RBM), Autoencoders, and Denoising Autoencoders. RBMs are two-layered undirected networks that are able to extract high-level features from their input. Contrastive divergence is used to speed up the training. Autoencoders are also deep learning networks used in unsupervised learning—they attempt to replicate the input by first encoding learned features in the encoding layer and then reconstructing the input via a set of decoding layers. Denoising Autoencoders address some limitations of Autoencoders, which can sometimes cause them to trivially learn the identity function.</p><p>Deep learning networks are often pretrained in an unsupervised fashion and then their parameters are fine-tuned via supervised fine-tuning. Stacked RBMs or Autoencoders are used in the pretraining phase and the fine-tuning is typically accomplished with a softmax activation in the output layer in the case of classification.</p><p>Deep Autoencoders are good at learning complex latent structures in data and are used in unsupervised learning by employing pre-training and fine-tuning with Autoencoder building blocks. Deep Belief Networks (DBN) are generative models that can be used to create more samples. It is constructed using a directed Bayesian network with an undirected RBM layer on top. Overfitting in deep learning networks can be addressed by learning with dropouts, where some nodes in the network are randomly "turned off".</p><p>Convolutional Neural Networks (CNNs) have a number of applications in computer vision. CNNs can learn patterns in the data translation-invariant and robust to linear scaling in the data. They reduce the dimensionality of the data using convolution filters and pooling layers and can achieve very effective results in classification tasks. A use case involving the classification of digital images is presented.</p><p>When the data arrives as sequences and there are temporal relationships among data, Recurrent Neural Networks (RNN) are used for modeling. RNNs use feedback from previous layers and emit output continually. The problem of vanishing and exploding gradients recurs in RNNs, and are addressed by several modifications to the architecture, such as Long Short Term Memory (LSTM) and Gated Recurrent Networks (GRU).</p><p>In this chapter's case study, we present the experiments done with various deep learning networks to learn from MNIST handwritten digit image datasets. Results using MLP, ConvNet, Variational Autoencoder, and Stacked RBM are presented.</p><p>We think that deep neural networks are able to approximate a significant and representative sub-set of key structures that the underlying data is based on. In addition, the hierarchic structures of the data can be easily captured with the help of different hidden layers. Finally, the invariance against rotation, translation, and the scale of images, for instance, is the last key elements of the performance of deep neural networks. The invariance allows us to reduce the number of possible states to be captured by the neural network (<span class="emphasis"><em>References</em></span> [19]).</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec65"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Behnke, S. (2001). Learning iterative image reconstruction in the neural abstraction pyramid. International Journal of Computational Intelligence and Applications, 1(4), 427–438. </li><li class="listitem">Behnke, S. (2002). Learning face localization using hierarchical recurrent networks. In Proceedings of the 12<sup>th</sup> international conference on artificial neural networks (pp. 1319–1324). </li><li class="listitem">Behnke, S. (2003). Discovering hierarchical speech features using convolutional non-negative matrix factorization. In Proceedings of the international joint conference on neural networks, vol. 4 (pp. 2758–2763).</li><li class="listitem"> Behnke, S. (2003). LNCS, Lecture notes in computer science: Vol. 2766. Hierarchical neural networks for image interpretation. Springer. Behnke, S. (2005). Face localization and tracking in the neural abstraction pyramid. Neural Computing and Applications, 14(2), 97–103.</li><li class="listitem">Casey, M. P. (1996). The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction. Neural Computation, 8(6), 1135–1178.</li><li class="listitem">Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal representations by error propagation. In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing, volume 1, pages 318–362. MIT Press.</li><li class="listitem">Goller, C.; Küchler, A (1996). ""Learning task-dependent distributed representations by backpropagation through structure"". Neural Networks, IEEE. doi:10.1109/ICNN.1996.548916</li><li class="listitem">Hochreiter, Sepp. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02): 107–116, 1998.</li><li class="listitem">G. E. Hinton, S. Osindero, and Y. The (2006). "A fast learning algorithm for deep belief nets," Neural Comput., vol. 18, pp. 1527–1554. </li><li class="listitem">G. E. Hinton (2002). "Training products of experts by minimizing contrastive divergence," Neural Comput., vol. 14, pp. 1771–1800. </li><li class="listitem">G. E. Hinton and R. R. Salakhutdinov (2006). "Reducing the dimensionality of data with neural networks," Science, vol. 313, no. 5786, pp. 504–507.</li><li class="listitem">Hinton, G. E., &amp; Zemel, R. S. (1994). Autoencoders, minimum description length, and Helmholtz free energy. Advances in Neural Information Processing Systems, 6, 3–10.</li><li class="listitem">Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. (2007). "Greedy layer-wise training of deep networks," in Advances in Neural Information Processing Systems 19 (NIPS'06) pp. 153–160. </li><li class="listitem">H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio (2007). "An empirical evaluation of deep architectures on problems with many factors of variation," in Proc. 24<sup>th</sup> Int. Conf. Machine Learning (ICML'07) pp. 473–480.</li><li class="listitem">P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol (2008), "Extracting and composing robust features with denoising autoencoders," in Proc. <sup>25</sup>th Int. Conf. Machine Learning (ICML'08), pp. 1096–1103.</li><li class="listitem">F.-J. Huang and Y. LeCun (2006). "Large-scale learning with SVM and convolutional nets for generic object categorization," in Proc. Computer Vision and Pattern Recognition Conf. (CVPR'06). </li><li class="listitem">F. A. Gers, N. N. Schraudolph, and J. Schmidhuber (2003). Learning precise timing with LSTM recurrent networks. The Journal of Machine Learning Research.</li><li class="listitem">Kyunghyun Cho et. al (2014). Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. <a class="ulink" href="https://arxiv.org/pdf/1406.1078.pdf">https://arxiv.org/pdf/1406.1078.pdf</a>.</li><li class="listitem"><a class="ulink" href="https://brohrer.github.io/how_convolutional_neural_networks_work.html">https://brohrer.github.io/how_convolutional_neural_networks_work.html</a></li><li class="listitem">Henry W. Lin, Max Tegmark, David Rolnick (2016). Why does deep and cheap learning work so well? <a class="ulink" href="https://arxiv.org/abs/1608.08225">https://arxiv.org/abs/1608.08225</a></li><li class="listitem">Mike Schuster and Kuldip K. Paliwal (1997). Bidirectional Recurrent Neural Networks, Trans. on Signal Processing. </li><li class="listitem">H Lee, A Battle, R Raina, AY Ng (2007). Efficient sparse coding algorithms, In Advances in Neural Information Processing Systems</li><li class="listitem">Bengio Y. (2009). Learning deep architectures for AI, Foundations and Trends in Machine Learning 1(2) pages 1-127.</li></ol></div></div></body></html>