["```py\nimport numpy as np\n\ndef loss(v):\n e = 0.0\n for i in range(nb_samples):\n e += np.square(v[0] + v[1]*X[i] - Y[i])\n return 0.5 * e\n```", "```py\ndef gradient(v):\n g = np.zeros(shape=2)\n for i in range(nb_samples):\n g[0] += (v[0] + v[1]*X[i] - Y[i])\n g[1] += ((v[0] + v[1]*X[i] - Y[i]) * X[i])\n return g\n```", "```py\nfrom scipy.optimize import minimize\n\n>>> minimize(fun=loss, x0=[0.0, 0.0], jac=gradient, method='L-BFGS-B')\nfun: 9.7283268345966025\n hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n jac: array([  7.28577538e-06,  -2.35647522e-05])\n message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n nfev: 8\n nit: 7\n status: 0\n success: True\n x: array([ 2.00497209,  1.00822552])\n```", "```py\nfrom sklearn.datasets import load_boston\n\n>>> boston = load_boston()\n\n>>> boston.data.shape\n(506L, 13L)\n>>> boston.target.shape\n(506L,)\n```", "```py\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n>>> X_train, X_test, Y_train, Y_test = train_test_split(boston.data, boston.target, test_size=0.1)\n\n>>> lr = LinearRegression(normalize=True)\n>>> lr.fit(X_train, Y_train)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)\n```", "```py\n>>> lr.score(X_test, Y_test)\n0.77371996006718879\n```", "```py\nfrom sklearn.model_selection import cross_val_score\n\n>>> scores = cross_val_score(lr, boston.data, boston.target, cv=7, scoring='neg_mean_squared_error')\narray([ -11.32601065,  -10.96365388,  -32.12770594,  -33.62294354,\n -10.55957139, -146.42926647,  -12.98538412])\n\n>>> scores.mean()\n-36.859219426420601\n>>> scores.std()\n45.704973900600457\n```", "```py\n>>> cross_val_score(lr, X, Y, cv=10, scoring='r2')\n0.75\n```", "```py\n>>> print('y = ' + str(lr.intercept_) + ' ')\n>>> for i, c in enumerate(lr.coef_):\n print(str(c) + ' * x' + str(i))\n\ny = 38.0974166342 \n-0.105375005552 * x0\n0.0494815380304 * x1\n0.0371643549528 * x2\n3.37092201039 * x3\n-18.9885299511 * x4\n3.73331692311 * x5\n0.00111437695492 * x6\n-1.55681538908 * x7\n0.325992743837 * x8\n-0.01252057277 * x9\n-0.978221746439 * x10\n0.0101679515792 * x11\n-0.550117114635 * x12\n```", "```py\n>>> X = boston.data[0:10] + np.random.normal(0.0, 0.1)\n\n>>> lr.predict(X)\narray([ 29.5588731 ,  24.49601998,  30.0981552 ,  28.01864586,\n 27.28870704,  24.65881135,  22.46335968,  18.79690943,\n 10.53493932,  18.18093544])\n\n>>> boston.target[0:10]\narray([ 24\\. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,  18.9])\n```", "```py\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n>>> diabetes = load_diabetes()\n\n>>> lr = LinearRegression(normalize=True)\n>>> rg = Ridge(0.001, normalize=True)\n\n>>> lr_scores = cross_val_score(lr, diabetes.data, diabetes.target, cv=10)\n>>> lr_scores.mean()\n0.46196236195833718\n\n>>> rg_scores = cross_val_score(rg, diabetes.data, diabetes.target, cv=10)\n>>> rg_scores.mean()\n0.46227174692391299\n```", "```py\nfrom sklearn.linear_model import RidgeCV\n\n>>> rg = RidgeCV(alphas=(1.0, 0.1, 0.01, 0.005, 0.0025, 0.001, 0.00025), normalize=True)\n>>> rg.fit(diabetes.data, diabetes.target)\n\n>>> rg.alpha_\n0.0050000000000000001\n```", "```py\nfrom sklearn.linear_model import Lasso\n\n>>> ls = Lasso(alpha=0.001, normalize=True)\n>>> ls_scores = cross_val_score(ls, diabetes.data, diabetes.target, cv=10)\n>>> ls_scores.mean()\n0.46215747851504058\n```", "```py\nfrom scipy import sparse\n\n>>> ls = Lasso(alpha=0.001, normalize=True)\n>>> ls.fit(sparse.coo_matrix(diabetes.data), diabetes.target)\nLasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n normalize=True, positive=False, precompute=False, random_state=None,\n selection='cyclic', tol=0.0001, warm_start=False)\n```", "```py\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\n\n>>> en = ElasticNet(alpha=0.001, l1_ratio=0.8, normalize=True)\n>>> en_scores = cross_val_score(en, diabetes.data, diabetes.target, cv=10)\n>>> en_scores.mean()\n0.46358858847836454\n\n>>> encv = ElasticNetCV(alphas=(0.1, 0.01, 0.005, 0.0025, 0.001), l1_ratio=(0.1, 0.25, 0.5, 0.75, 0.8), normalize=True)\n>>> encv.fit(dia.data, dia.target)\nElasticNetCV(alphas=(0.1, 0.01, 0.005, 0.0025, 0.001), copy_X=True, cv=None,\n eps=0.001, fit_intercept=True, l1_ratio=(0.1, 0.25, 0.5, 0.75, 0.8),\n max_iter=1000, n_alphas=100, n_jobs=1, normalize=True,\n positive=False, precompute='auto', random_state=None,\n selection='cyclic', tol=0.0001, verbose=0)\n\n>>> encv.alpha_\n0.001\n>>> encv.l1_ratio_\n0.75\n```", "```py\nfrom sklearn.linear_model import LinearRegression\n\n>>> lr = LinearRegression(normalize=True)\n>>> lr.fit(X.reshape((-1, 1)), Y.reshape((-1, 1)))\n>>> lr.intercept_\narray([ 5.500572])\n>>> lr.coef_\narray([[ 2.53688672]])\n```", "```py\nfrom sklearn.linear_model import RANSACRegressor\n\n>>> rs = RANSACRegressor(lr)\n>>> rs.fit(X.reshape((-1, 1)), Y.reshape((-1, 1)))\n>>> rs.estimator_.intercept_\narray([ 2.03602026])\n>>> es.estimator_.coef_\narray([[ 0.99545348]])\n```", "```py\nfrom sklearn.linear_model import LinearRegression\n\n>>> lr = LinearRegression(normalize=True)\n>>> lr.fit(X.reshape((-1, 1)), Y.reshape((-1, 1)))\n>>> lr.score(X.reshape((-1, 1)), Y.reshape((-1, 1)))\n0.10888218817034558\n```", "```py\nfrom sklearn.preprocessing import PolynomialFeatures\n\n>>> pf = PolynomialFeatures(degree=2)\n>>> Xp = pf.fit_transform(X.reshape(-1, 1))\n\n>>> Xp.shape\n(100L, 3L)\n```", "```py\n>>> lr.fit(Xp, Y.reshape((-1, 1)))\n>>> lr.score(Xp, Y.reshape((-1, 1)))\n0.99692778265941961\n```", "```py\nfrom sklearn.feature_selection import SelectFromModel\n\n>>> boston = load_boston()\n\n>>> pf = PolynomialFeatures(degree=2)\n>>> Xp = pf.fit_transform(boston.data)\n>>> Xp.shape\n(506L, 105L)\n\n>>> lr = LinearRegression(normalize=True)\n>>> lr.fit(Xp, boston.target)\n>>> lr.score(Xp, boston.target)\n0.91795268869997404\n\n>>> sm = SelectFromModel(lr, threshold=10)\n>>> Xt = sm.fit_transform(Xp, boston.target)\n>>> sm.estimator_.score(Xp, boston.target)\n0.91795268869997404\n\n>>> Xt.shape\n(506L, 8L)\n```", "```py\n>>> Xo = sm.inverse_transform(Xt)\n>>> Xo.shape\n(506L, 105L)\n```", "```py\n>>> X = np.arange(-5, 5, 0.1)\n>>> Y = X + np.random.uniform(-0.5, 1, size=X.shape)\n```", "```py\nfrom sklearn.isotonic import IsotonicRegression\n\n>>> ir = IsotonicRegression(-6, 10)\n>>> Yi = ir.fit_transform(X, Y)\n```", "```py\n>>> ir.X_min_\n-5.0\n>>> ir.X_max_\n4.8999999999999648\n>>> ir.f_\n<scipy.interpolate.interpolate.interp1d at 0x126edef8>\n```", "```py\n>>> ir.f_(2)\narray(1.7294334618146134)\n```"]