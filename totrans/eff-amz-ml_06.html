<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Predictions and Performances</h1>
            </header>

            <article>
                
<p>It is time to make some predictions! In <a href="08d9b49a-a25c-4706-8846-36be9538b087.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Loading and Preparing the Dataset,</em> we did split the <kbd>Titanic</kbd> dataset into two subsets, the training and held-out subsets, respectively consisting of 70% and 30% of the original dataset randomly shuffled. We have used variations of the training subset extensively in <a href="b71d5007-58a0-4b79-9446-4a36e7476037.xhtml"><span>chapter 5</span></a> <em>Model Creation,</em> to train and select the best classification model. But so far, we have not used the held-out subset at all. In this chapter, we apply our models to this held-out subset to make predictions on unseen data and make a final assessment of the performance and robustness of our models.</p>
<p>Amazon ML offers two types of predictions: batch and streaming. Batch prediction requires a datasource. The samples you want to predict are given to the model all at once in batch mode. Streaming, also known as real-time or online predictions, requires the creation of an API endpoint and consists of submitting sequences of samples, one by one, via HTTP requests. Real-time predictions do not involve the creation of a datasource.</p>
<p>We will start with batch predictions on the Titanic held<span>-</span>out set. We will confirm that our different models perform similarly on the held<span>-</span>out dataset as they did on the validation subsets, assuming that all the subsets have a similar variable distribution. In <a href="b71d5007-58a0-4b79-9446-4a36e7476037.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Model Creation</em>, we concluded that out of our three datasources — suggested recipe with quantile binning (QB), recipe without QB, and the extended dataset — the one with extra variables (<kbd>deck</kbd>, <kbd>title</kbd>, <kbd>log_fare</kbd>, and so on) resulted in the best score on the validation subset. We will verify that this is also the case on the held-out subset. </p>
<p>This chapter is organized in two parts. In the first part, we look at batch predictions on the <kbd>Titanic</kbd> dataset. In the second part, we look at real-time, streaming predictions, with a new text-based quantile binning from the UCI repository. The <kbd>Spam</kbd> dataset is large enough to simulate streaming data. We will create an Amazon ML endpoint and use the Python SDK to send and retrieve classification predictions. </p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Making batch predictions</li>
<li>Making real-time predictions</li>
</ul>
<div class="packt_infobox">In real-world classification problems or regression problems, the previously unseen data you want to make predictions on will not include the target values. In our case, the held-out datasets do contain the solution, and this allows us to assess the model performance on previously unseen data. But with real-world problems, you do not have that luxury and you will have to trust your model.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Making batch predictions</h1>
            </header>

            <article>
                
<p>Making batch predictions on Amazon ML is straightforward and follows this process:</p>
<ol>
<li>From the dashboard, <span class="packt_screen">create a new Batch prediction</span>.</li>
<li>Select the model.</li>
<li>Select the datasource on which to apply the model.</li>
<li>Set the prediction output folder and grant permissions.</li>
<li>Review and launch.</li>
</ol>
<p>We call the <kbd>prediction</kbd> dataset or datasource, the data on which we want to make predictions. In this chapter, we are in a testing context and the <kbd>prediction</kbd> dataset is the <kbd>held-out</kbd> dataset we extracted from the whole original dataset. In a real-world context, the prediction dataset refers to entirely new data and does not include the target variable. </p>
<p>The prediction can only work if the distribution of the prediction dataset is similar to the distribution of the training dataset on which the model has been trained. The prediction datasource and the training datasource must also share the same schema, with one difference the prediction dataset does not <span>need to include</span> the target variable. Amazon ML will verify that the schema defined for your training data is relevant to your prediction data and will issue a warning if the datasets are not similar.</p>
<p>For the sake of convenience, we have recreated the datasets, datasources, and models for this chapter. All datasets and scripts are available in the GitHub repository at <a href="https://github.com/alexperrier/packt-aml/tree/master/ch6" target="_blank">https://github.com/alexperrier/packt-aml/tree/master/ch6</a>. Since we reshuffled the original Titanic data, the evaluation scores will be different from the ones obtained previously for the same dataset. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating the batch prediction job</h1>
            </header>

            <article>
                
<p>To create a batch prediction, go to the Amazon ML dashboard and click on <span class="packt_screen">Create new batch prediction</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="183" src="assets/B05028_06_01.png" width="480"/></div>
<p>Then select the model. We choose the original model related to the <kbd>Titanic</kbd> dataset, the one using the Amazon ML suggested recipe with quantile binning:</p>
<ul>
<li>Quantile binning of all numeric variables</li>
<li>L2 mild regularization</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="228" src="assets/B05028_06_02.png" width="592"/></div>
<p>After the model selection comes the datasource selection. If you have not yet created a datasource for the held-out set, you can do so now. First, upload your prediction dataset to S3 and specify the S3 path for the source of the data:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="225" src="assets/B05028_06_03.png" width="544"/></div>
<p>When you click on <span class="packt_screen">Verify</span>, Amazon ML will check that the prediction dataset follows the same schema as the training dataset on which the model was trained:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="160" src="assets/B05028_06_04.png" width="627"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Interpreting prediction outputs</h1>
            </header>

            <article>
                
<p>The output of the Amazon ML prediction job will consists of two files: the manifest file and the actual prediction results given in a compressed CSV file. Amazon ML will create the files on S3 in an S3 location, <kbd>s3://bucket/folder</kbd>, which you must specify. We use the same path as our data path: <kbd>s3://aml.packt/data/</kbd>. Amazon ML will create a <kbd>/batch_prediction</kbd> folder, where it will write the manifest file as well as an extra subfolder <kbd>/results</kbd>, where the CSV file with the actual predictions will be written. To recap, in our context, the manifest file will be in the  <kbd>s3://aml.packt/data/batch_prediction</kbd><span> folder,</span> and the compressed CSV results file will be in the <kbd>s3://aml.packt/data/batch_prediction/results/</kbd> folder.<em><span> </span></em>The name given to the batch prediction will dictate the naming of the manifest and results files:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="252" src="assets/B05028_06_05.png" width="634"/></div>
<div class="packt_infobox"><strong>Prediction pricing</strong>: If you just created the datasource for the batch prediction, Amazon ML does not yet have access to the data statistics that it needs to calculate the prediction costs. In that case, it will simply inform you of the price, of $0.10 per 1,000 predictions. If the prediction datasource has already been validated and Amazon ML knows the number of records, the estimated price will be the number of rows times the price per prediction rounded up to the nearest cent. You are not billed for the invalid samples Amazon ML fails to predict. More information is available at <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/pricing.html" target="_blank">http://docs.aws.amazon.com/machine-learning/latest/dg/pricing.html</a>.</div>
<p>Review and click on the <span class="packt_screen">Create batch</span> prediction button. The batch prediction job will take a few minutes to complete. When finished, it will have created the manifest and results files in S3 and will show up as completed in the <span class="packt_screen">Batch Prediction</span> section of the Amazon ML dashboard.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Reading the manifest file</h1>
            </header>

            <article>
                
<p>The manifest file contains JSON-formatted data that maps the input file to the prediction results file, as follows:</p>
<pre>
{S3 location of the batch prediction input file.csv : S3 location of the prediction results file}
</pre>
<p>In our context, the manifest file contains the following line:</p>
<pre>
{"s3://aml.packt/data/ch6_titanic_heldout.csv":"s3://aml.packt/batch-prediction/result/bp-yTDNSArMqa6-ch6_titanic_heldout.csv.gz"}
</pre>
<p><strong>Multiple input files</strong>: If your input data is split into several files, all stored in the same S3 location <span><kbd>s3://examplebucket/input/</kbd></span>,<span> </span>all the input files will be considered by the batch prediction job. The manifest file will then contain the mapping from the different input files to the associated results files. For instance, if you have three input files named <kbd>data1.csv</kbd>, <kbd>data2.csv</kbd>, and <kbd>data3.csv</kbd>, and they are all stored in the S3 location <kbd>s3://examplebucket/input/</kbd>, you will see a mapping string that looks like as follows:</p>
<pre>
{"s3://examplebucket/input/data1.csv":"s3://examplebucket/output/batch-prediction/result/bp-example-data1.csv.gz", "s3://examplebucket/input/data2.csv":"<br/> s3://examplebucket/output/batch-prediction/result/bp-example-data2.csv.gz", "s3://examplebucket/input/data3.csv":"<br/> s3://examplebucket/output/batch-prediction/result/bp-example-data3.csv.gz"}
</pre>
<div class="packt_infobox"><strong>Maximum size for predictions</strong>: Amazon ML allows up to 1TB of data for prediction files. If the data on which you want to make predictions is larger, it is possible to split your data into several files, upload them to a specific S3 location, and Amazon ML will handle the different files and generate as many prediction result files as there are input files by running several batches in parallel. The manifest file will contain all the different input/output pairs, <kbd>{input_file.csv : prediction_results.csv.gz}</kbd> for your different batch prediction files.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Reading the results file</h1>
            </header>

            <article>
                
<p>The output results file is compressed with gzip, originates from the UNIX world, and offers better compression than the more common zip compression. A simple click should be sufficient to open and decompress the gzipped results file into a readable CSV file. Alternatively, a call to the gunzip command from the command line should work. Take a look at <a href="http://www.gzip.org/" target="_blank">http://www.gzip.org/</a> for installation on different systems.</p>
<p>For binary classification, the decompressed results file contains two or three columns, depending on whether the initial input file contained the target or not. In our case of binary classification, the result file has the following columns: <kbd>trueLabel</kbd>, <kbd>bestAnswer</kbd>, and <kbd>score</kbd>, where <kbd>trueLabel</kbd> is the initial <kbd>survived</kbd> column. If your initial batch prediction dataset did not include the target values, the results file will only have the <span>bestAnswer</span><span> and</span> <span>score columns:</span></p>
<ul>
<li><kbd>trueLabel</kbd> is the original target value contained in the input file</li>
<li><kbd>bestAnswer</kbd> is the classification result: 0 or 1</li>
<li><kbd>Score</kbd> is the probability for that classification written in scientific notation</li>
</ul>
<p>The classification cutoff threshold for the score probability is 0.5 by default, or set to the threshold value you chose while evaluating the model.</p>
<p>For multiclass classification with <em>N</em> potential target classes, the results file will have <em>N+1</em> or <em>N+2</em> columns. The <kbd>trueLabel</kbd>, <kbd>bestAnswer</kbd>, and <kbd>N</kbd> columns each with the probability scores for each one of the N classes. The chosen class will be the one that bears the highest probability score.</p>
<p>For a regression model, the results file will only contain one/two score columns with the predicted value, and possibly the <kbd>trueLabel</kbd> column.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Assessing our predictions</h1>
            </header>

            <article>
                
<p>Since we know the real class of our held-out samples, we can calculate the <strong>ROC-AUC</strong> score and other metrics to see how close our prediction and validation scores are. Assuming that our data subsets have very similar distributions, both scores should end up very close. The difference only comes from randomness in the samples for the validation and held-out sets.</p>
<p>The following Python script uses the <kbd>scikit-learn</kbd> library (<a href="http://scikit-learn.org/" target="_blank">http://scikit-learn.org/</a>) as well as the pandas library. It takes a few lines of Python to calculate the AUC score of the model on that prediction dataset. First, download the gzipped file from S3 and then, in a Python Notebook or console, run the following:</p>
<pre>
import pandas as pd  <br/>from sklearn import metrics<br/><br/># open file the csv file on your local<br/>df = pd.read_csv(path/location_of_the_unzipped_results_csv_file)<br/><br/># calculate the true and false positive rate<br/>fpr, tpr, threshold = metrics.roc_curve(df.trueLabel, df.score)<br/>roc_auc = metrics.auc(fpr, tpr)
</pre>
<div class="packt_infobox"><strong>Python environment</strong>: All the Python code in this book is for Python 3.5 or above. For more information on the Anaconda library, take a look at <a href="https://www.continuum.io/downloads" target="_blank">https://www.continuum.io/downloads</a>. Anaconda is an amazingly powerful open source data science platform in Python. It contains the most important libraries (<kbd>numpy</kbd>, <kbd>pandas</kbd>, <kbd>scikit-learn</kbd>, <kbd>matplotlib</kbd>, and many others) as well as the Jupyter Notebooks environment. We use the IPython console for its simplicity of use and many magic commands (<a href="http://ipython.readthedocs.io/en/stable/interactive/magics.html" target="_blank">http://ipython.readthedocs.io/en/stable/interactive/magics.html</a>).</div>
<p>Running the previous Python script on the predictions results, we obtain an AUC of 0.84 on our held-out dataset, which is very close to the AUC (0.85) we obtained on the validation set in <a href="b71d5007-58a0-4b79-9446-4a36e7476037.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Model Creation</em>. We can conclude that our model is pretty stable and robust when facing new, previously unforeseen data.</p>
<p>The following plot shows both the ROC curves for the validation (dotted line) and held-out (solid-line) sets for the chosen model. The validation set is slightly better for higher values of the threshold. This difference is a reflection of the different data distributions in the two datasets:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="498" src="assets/B05028_06_06.png" width="498"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Evaluating the held-out dataset</h1>
            </header>

            <article>
                
<p>In <a href="b71d5007-58a0-4b79-9446-4a36e7476037.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Model Creation</em>, we evaluated the performance of our different models on a slice of the training datasource. We obtained for each model an AUC score, and selected the AUC with the best AUC score. We relied on Amazon ML to create the validation set, by splitting the training dataset into two, with 70% for training and 30% of the data for validation. We could have done that split ourselves, created the validation datasource, and specified which datasource to use for the evaluation of the model.</p>
<p>In fact, nothing prevents us from running a model evaluation on the held-out dataset. If you go to the model summary page, you will notice a <span class="packt_screen">Perform another Evaluation</span> button in the Evaluation section:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="128" src="assets/B05028_06_15.png" width="436"/></div>
<p>Click on it. You are asked to select the datasource for the evaluation. Select the held-out dataset; Amazon ML will verify that the data follows the same schema and is similar to the training data. You end up with two evaluations on the model:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="205" src="assets/B05028_06_16.png" width="212"/></div>
<p>And as expected, the evaluation AUC for the held-out dataset is equal to the AUC we obtained by downloading the results and calculating the AUC in Python:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="444" src="assets/B05028_06_17.png" width="598"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Finding out who will survive</h1>
            </header>

            <article>
                
<p>The true value of predictions, however, is not about validating the robustness of our model; it's about making predictions on our prediction dataset, in our context, getting survival predictions on this <kbd>new</kbd> list of passengers contained in the held-out dataset.<br/>
The rows in the results file follow the exact same order as the rows in the prediction file. We can put side by side the first rows of the held-out file and the first rows of the results file, and see that the <kbd>survived</kbd> and the <kbd>trueLabel</kbd> columns are identical:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class=" image-border" height="193" src="assets/B05028_06_13.png" width="640"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Multiplying trials</h1>
            </header>

            <article>
                
<p>The evaluation scores on the various models and dataset version are, to a certain extent, dependent on the samples contained in the evaluation sets. If we run the following experiment several times on the three datasets, we see certain variations in the scores:</p>
<ul>
<li>Shuffle and split the dataset into three -- training, validation, and held-out and create the respective datasources</li>
<li>Train a model on the training dataset, keeping the default Amazon ML settings (mild L2 regularization)</li>
<li>Evaluate the model on the evaluation and held-out datasets</li>
</ul>
<p>The following plot shows the respective performances of the three models for several trials. The average AUC is written on the graph. We see that on average, the extended dataset performs better (<em>AUC = 0.87</em>) than the original dataset with the default recipe (<em>AUC = 0.84</em>) and the original dataset without quantile binning (<em>AUC = 0.83</em>). We also notice that in some trials, the extended dataset performs worse than the original one. For trial 3, the default recipe is even slightly less performant than the no quantile binning one: </p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="481" src="assets/B05028_06_14.png" width="481"/></div>
<p>This shows the importance of inner data distribution variability. When trying out several variants of your datasets with different features and processing, it's important to base your conclusions on several runs of your models. A single evaluation may lead to missing the best model.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Making real-time predictions</h1>
            </header>

            <article>
                
<p>With batch predictions, you submit all the samples you want the model to predict at once to Amazon ML by creating a datasource. With real-time predictions, also called streaming or online predictions, the idea is to send one sample at a time to an API endpoint, a URL, via HTTP queries, and receive back predictions and information for each one of the samples.</p>
<p>Setting up real-time predictions on a model consists of knowing the prediction API endpoint URL and writing a script that can read your data, send each new sample to that API URL, and retrieve the predicted class or value. We will present a Python-based example in the following section.</p>
<p>Amazon ML also offers a way to make predictions on data you create on the fly on the prediction page. We can input the profile of a would-be passenger on the <kbd>Titanic</kbd> and see whether that profile would have survived or not. It is a great way to explore the influence of the dataset variables on the outcome.</p>
<p>Before setting up API for streaming, let’s see what we can gather from submitting several single passenger profiles. We can even try to answer the question – <em>W</em><em>ould you have survived on the Titanic?</em></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Manually exploring variable influence</h1>
            </header>

            <article>
                
<p>Go to the model summary page and click on the <span class="packt_screen">Try real-time predictions</span> link on the left side of the page. The following page shows a form where you can fill out values for the variables in our dataset except for the target. </p>
<p>Let's see if Alex Mr. Perrier, a first-class passenger who embarked at Southhampton with his family (3 sibsp and 2 parch) and who paid a fare of 100 pounds, would have survived. Well, in that case, the model gives a very low probability of survival (0.001), meaning that the model predicts with confidence that this passenger would not have survived. His 12 year old daughter would have had better chances of surviving (<em>probability 0.56</em>), though the model is less sure of it. However, if that same girl was traveling alone (<em>sibsp = parch = 0</em>), her chance of survival would surge to 0.98 under the condition that she traveled in 1st class. In 3rd class, she would have been less fortunate (<em>probability: 0.28</em>):</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="363" src="assets/B05028_06_18.png" width="619"/></div>
<p>So, by changing one variable at a time in the data, we can have a better understanding of the impact of each variable on the outcome. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up real-time predictions</h1>
            </header>

            <article>
                
<p>To demonstrate real-time predictions, we will use the <kbd>Spam</kbd> dataset from the UCI repository. This dataset is composed of 5,574 SMS messages annotated spam or ham (non-spam). There are no missing values and only two variables: the nature of the SMS (ham or spam) and the text message of the SMS, nothing else.  The <kbd>Spam</kbd> dataset is available at <a href="https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection" target="_blank">https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection</a> in its raw form, and in the book's GitHub repository at <a href="https://github.com/alexperrier/packt-aml/tree/master/ch6" target="_blank">https://github.com/alexperrier/packt-aml/tree/master/ch6</a>. We have simply transformed the target from categorical: <kbd>spam</kbd> and <kbd>ham</kbd> values to binary: 1 (for spam) and 0 (for ham) so that Amazon ML understands the prediction to be of the binary-classification type.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">AWS SDK</h1>
            </header>

            <article>
                
<p>AWS offers several APIs and <strong>Software Development Kits (SDKs)</strong> to its many services. You can programmatically manage your files on S3, set up EC2 instances, and create datasources, models, and evaluations on Amazon ML without using the web-based user interface. The AWS APIs are low-level endpoints. In general, it is simpler and more efficient to use the SDKs, which are wrappers around the APIs and are available for several languages (Python, Ruby, Java, C++, and so on). In this book, we will use the Python SDK based on the <kbd>Boto3</kbd> library. We explore in detail the use of the Python SDK in <span class="ChapterrefPACKT"><a href="efe6f699-a4eb-4c88-8e81-1408d6c3c5c4.xhtml" target="_blank">Chapter 7</a><span>, <em>Command Line and SDK.</em> For now, we will only use the <kbd>predict()</kbd> method necessary for real-time predictions. But first, we need to enable access to AWS by setting up AWS credentials on our local machine.</span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up AWS credentials</h1>
            </header>

            <article>
                
<p>In order to access AWS programmatically, we first need to access AWS via the command line. This requires the following:</p>
<ul>
<li>Creating access keys on AWS IAM for your user</li>
<li>Installing the <kbd>AWS-CLI</kbd> command-line interface on local</li>
<li>Configuring <kbd>AWS-CLI</kbd> with the AWS access keys</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">AWS access keys</h1>
            </header>

            <article>
                
<p>If you recall from <a href="5938ed0c-9243-49cc-bf67-314ffb5f9386.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Overview of an Amazon Machine Learning Workflow</em>, we had created access keys for our <kbd>AML@Packt</kbd> user. Access keys are user-based and composed of two parts: the <strong>Access Key ID</strong>, which is always available in the user security credentials tab in IAM, and the <strong>Secret Access Key</strong>, which is only shown at creation time. When creating these access keys, you are given the possibility of downloading them. If you did not do so at that time, you can recreate access keys for your user now. Go to the IAM console at  <a href="https://console.aws.amazon.com/iam" target="_blank">https://console.aws.amazon.com/iam</a>, click on your user profile, select the <span class="packt_screen">Security Credentials</span> tab, and click on the <span class="packt_screen">Create Access Key</span> button. This time make sure you download the keys on your local machine or copy them somewhere. Note that there’s a limit of two sets of access keys per user. You will have to delete existing keys before creating new ones if you already have two keys associated to your user:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="424" src="assets/B05028_03_03.png" width="623"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up AWS CLI</h1>
            </header>

            <article>
                
<p>So far, we have only worked with the AWS web interface, clicking from page to page on the AWS website. Another way to interact with AWS services is via the command line in a terminal window, using the <kbd>aws cli</kbd> library. CLI stands for Command Line Interface. </p>
<p>To install the <kbd>aws cli</kbd> library, open a terminal window. For a Python-based environment (Python 2 version 2.6.5+ or Python 3 version 3.3+), installing <kbd>aws cli</kbd> consists of running the following command in a terminal:</p>
<pre>
pip install awscli
</pre>
<p>Full instructions for installation in other environments are available at <a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html" target="_blank">http://docs.aws.amazon.com/cli/latest/userguide/installing.html</a>.  Once AWS-CLI is installed, run the following command to configure it:</p>
<pre>
<strong>aws configure</strong>
</pre>
<p>You will be asked for your access keys, the default region, and format. See <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration" target="_blank">http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration</a> for more in-depth explanations.</p>
<p>In short, AWS-CLI commands follow this syntax:</p>
<pre>
<strong>aws {service name} {command} {parameters}</strong>
</pre>
<p>Test your setup by running the following:</p>
<pre>
<strong>aws s3 ls aml.packt</strong>
</pre>
<p>You should see a list of your buckets, folders, and files in your s3 account. This is what my output looks like when I list the file and folders in the <kbd>aml.packt</kbd> bucket:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="136" src="assets/B05028_06_07.png" width="401"/></div>
<p>We will explore in detail how to run your Amazon ML projects using CLI in <span class="ChapterrefPACKT"><a href="efe6f699-a4eb-4c88-8e81-1408d6c3c5c4.xhtml" target="_blank">Chapter 7</a><span>, C<em>ommand Line and SDK</em></span></span>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Python SDK</h1>
            </header>

            <article>
                
<p>We will not use the AWS-CLI any further in this chapter, but instead switch to the Python SDK. We needed to setup the credentials for the AWS CLI in order for our SDKs scripts to be able to access our AWS account. To use the Python SDK, we need to install the <kbd>Boto3</kbd> library, which comes bundled in the Anaconda distribution. If you use Anaconda as your Python environment, you should already have the <kbd>boto3</kbd> package installed. If not, you can install it using <kbd>pip</kbd> with the following:</p>
<pre>
<strong>pip install boto3</strong>
</pre>
<p><strong>Boto3</strong> will use the credentials we configured for the AWS CLI. There’s no need for a specific setup. Boto3 is available for most AWS services. The full documentation is available at <a href="https://boto3.readthedocs.io/" target="_blank">https://boto3.readthedocs.io/</a>. Our minimal use of <kbd>Boto3</kbd> only requires to specify the service we want, Machine Learning, and then use the <kbd>predict()</kbd> method to send the proper data to the model. In return, we obtain the predictions we wanted. The following Python code initiates a client to access the machine learning service.</p>
<pre>
<strong>import boto3</strong><br/><strong>client = boto3.client('machinelearning')</strong>
</pre>
<p>The <kbd>predict()</kbd> method requires the following parameters:</p>
<ul>
<li><kbd>MLModelId</kbd>: The ID of the model your want to use to predict</li>
<li><kbd>PredictEndpoint</kbd><em>:</em> The URL of the Amazon ML endpoint for your model</li>
<li><kbd>Record</kbd><em>:</em> A JSON-formatted version of the sample</li>
</ul>
<p>The <kbd>MLModelId</kbd> and <kbd>PredictEndpoint</kbd> URL can be obtained from the model summary page. The <kbd>Record</kbd> is a JSON-formatted string. We will simulate a streaming application by opening a held-out set of samples, looping through each sample and sending it via the <kbd>predict()</kbd> method.</p>
<p>We have split the initial dataset into a training set of 4,400 samples and a held-out set of 1,174 samples. These subsets are available at the GitHub repository. We create a datasource for the training subset, and create a model and its evaluation with default settings (mild L2 regularization). We keep the inferred schema (binary and text), the suggested recipe (no transformation besides tokenization of the text variable), and use the default model parameters (10 passes and mild L2 regularization). The training dataset is further split by Amazon ML into a smaller training dataset and a validation dataset, respectively 70% and 30% of the initial <em>4,400</em> samples. The AUC score obtained on the validation set is very high at <em>0.98</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="114" src="assets/B05028_06_08.png" width="648"/></div>
<p>To get the <kbd>ModelID</kbd> and the <kbd>endpoint</kbd> URL, go to your summary page for the model. Copy the <kbd>ModelID</kbd> from the top of the page. Then scroll down to get to the prediction section and click on the <span class="packt_screen">Create endpoint</span> button:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="301" src="assets/B05028_06_09.png" width="514"/></div>
<p>At that point, you will be given an estimate of the real-time prediction pricing for your model and asked to confirm the creation of the endpoint.</p>
<p>The size of your model is 502.1 KB. You will incur the reserved capacity charge of $0.001 for every hour your endpoint is active. The prediction charge for real-time predictions is $0.0001 per prediction, rounded up to the nearest penny<strong>.</strong></p>
<p>After a few minutes, the endpoint will be created and you will have the endpoint URL:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="135" src="assets/B05028_06_10.png" width="390"/></div>
<p>Now that we know the endpoint URL, we can write a simple Python code that sends an SMS message, a simple text, to our prediction model and see whether this message is predicted to be spam or ham. We send the text <kbd>Hello world, my name is Alex</kbd> to be classified as ham, while the text <kbd>Call now to get dating contacts for free, no cash no credit card</kbd> should probably be detected as spam due to the presence of the words <em>free</em>, <em>cash</em>, <em>dating</em>, and so forth.</p>
<p>The initialization/declaration part of the code is as follows:</p>
<pre>
import boto3<br/>import json  # for parsing the returned predictions<br/><br/># Initialize the client<br/>client = boto3.client('machinelearning')<br/><br/># The endpoint url is obtained from the model summary<br/>endpoint_url = "https://realtime.machinelearning.us-east-1.amazonaws.com/"<br/><br/># replace with your own model ID<br/>model_id = "ml-kJmiRxxxxxx" <br/><br/># The actual sample to be predicted. JSON formatted<br/>record = { "nature": "Hello world, my name is Alex" }
</pre>
<p>We now use the <kbd>predict()</kbd> function of the machine learning service SDK:</p>
<pre>
response = client.predict(    <br/>   MLModelId         = model_id,<br/>   Record               = record,<br/>   PredictEndpoint = endpoint_url<br/>)
</pre>
<p>Finally, pretty print the response:</p>
<pre>
print(json.dumps(response, indent=4))
</pre>
<p>This returns the following JSON-formatted string:</p>
<pre>
{<br/>   "ResponseMetadata": {<br/>       "RetryAttempts": 0,<br/>       "HTTPHeaders": {<br/>           "content-type": "application/x-amz-json-1.1",<br/>           "content-length": "143",<br/>           "date": "Tue, 10 Jan 2017 16:20:49 GMT",<br/>           "x-amzn-requestid": "bfab2af0-d750-11e6-b8c2-45ac3ab2f186"<br/>       },<br/>       "HTTPStatusCode": 200,<br/>       "RequestId": "bfab2af0-d750-11e6-b8c2-45ac3ab2f186"<br/>   },<br/>   "Prediction": {<br/>       "predictedScores": {<br/>           "0": 0.001197131467051804<br/>       },<br/>       "predictedLabel": "0",<br/>       "details": {<br/>           "PredictiveModelType": "BINARY",<br/>           "Algorithm": "SGD"<br/>       }<br/>   }<br/>}
</pre>
<p>The JSON response is composed of two parts: the first part is related to the request itself, <kbd>ResponseMetadata</kbd>, and the second is related to the <kbd>Prediction</kbd>. The <span><kbd>HTTPStatusCode</kbd> in the <kbd><em>ResponseMetadata</em></kbd></span> part tells us that our query was successful <kbd>("HTTPStatusCode": 200)</kbd>.</p>
<p>The interpretation of the Prediction part is straightforward. The SMS was predicted to be spam with a very low probability of 0.12%, hence it was classified as ham, which is what we expected for the text <kbd>Hello world, my name is Alex</kbd>.</p>
<p>We expect the text <kbd>Call now to get dating contacts for free, no cash no credit card</kbd> to be classified as spam, the words <kbd>free</kbd>, <kbd>call</kbd>, and <kbd>dating</kbd> usually being strong indicators of spam messages. We get the following in return:</p>
<pre>
{    <br/>    "predictedScores": { "1": 0.810875654220581 },<br/>    "predictedLabel": "1",<br/>}
</pre>
<p>The text is classified as spam, which is what we expected. As far as we can tell from these two simple examples, our model seems to be working fine. Once we can obtain prediction via API calls on a sample-by-sample basis, it becomes feasible to hook an incoming stream of data into the endpoint and obtain real-time predictions.</p>
<p>To simulate that pipeline, we can use Python to read a whole file of new samples, send each one to the model, and capture the results. Let’s do that with the held-out set of Spam samples.</p>
<p>The following Python code reads the file, loads it into a panda dataframe, and loops over each row of the dataframe. We use <kbd>iterrows()</kbd> to loop over each row of the dataframe. This method is slower than <kbd>itertuples()</kbd>, but has better code readability. The following code is not optimized:</p>
<pre>
import boto3<br/>import json<br/>import pandas as pd<br/><br/># Initialize the Service, the Model ID and the endpoint url<br/>client = boto3.client('machinelearning')<br/># replace with your own endpoint url and model ID<br/>endpoint_url = "https://realtime.machinelearning.us-east-1.amazonaws.com"<br/>model_id = "ml-kJmiRHyn1UM"<br/><br/># Memorize which class is spam and which is ham<br/>spam_label = {'0': 'ham', '1':'spam'}<br/><br/># Load the held out dataset into a panda DataFrame<br/>df = pd.read_csv('held-out.csv')<br/><br/># Loop over each DataFrame rows    <br/>for index, row in df.iterrows():<br/>   # The record<br/>   record = { "body": row['sms'] }<br/>   response = client.predict(    <br/>       MLModelId       = model_id,<br/>       Record          = record,<br/>       PredictEndpoint = endpoint_url<br/>   )<br/><br/>   # get the label and score from the response<br/>   predicted_label = response['Prediction']['predictedLabel']<br/>   predicted_score = response['Prediction']['predictedScores'][predicted_label]<br/>   print("[%s] %s (%0.2f):t %s "% (spam_label[str(row['nature'])],<br/>                               spam_label[predicted_label],<br/>                               predicted_score,<br/>                               row['sms'] )<br/>   )
</pre>
<p>The responses from Amazon ML are blazingly fast. Thousands of samples are processed in a few seconds. This is an extract of the results we get:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="87" src="assets/B05028_06_11.png" width="506"/></div>
<p>Here, each line is formatted as follows:</p>
<pre>
[Predicted class] trueLabel (spam probability):      SMS message  
</pre>
<p>Notice that, out of the three SMS detected as spam, only two were actually spam SMS. The text "<em>Money i have won wining number 946 wot do i do next</em>" was probably detected as spam due to the presence of the words "Money" or "wining" but was in fact a ham message.</p>
<p>Overall, across the whole predictions, the probabilities are very close to either 0 or 1, indicating that the model is very decisive in its classification. No hesitation. The ROC curve for the held-out dataset shows that high level of accuracy:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="345" src="assets/B05028_06_12.png" width="345"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we explored the final step in the Amazon ML workflow, the predictions. Amazon ML offers several ways to apply your models to new datasets in order to make predictions. Batch mode involves submitting all the new data at once to the model and returning the actual predictions in a csv file on S3. Real-time predictions, on the other hand, are based on sending samples one by one to an API and getting prediction results in return. We looked at how to create an API on the Amazon ML platform. We also started using the command line and the Python SDK to interact with the Amazon ML service -- something we will explore in more depth in <span class="ChapterrefPACKT"><a href="efe6f699-a4eb-4c88-8e81-1408d6c3c5c4.xhtml" target="_blank">Chapter 7</a><span>, <em>Command Line and SDK</em>.<br/></span></span></p>
<p>As explained in the previous chapters, the Amazon ML service is built around the Stochastic Gradient Descent (SGD) algorithm. This algorithm has been around for many years and is used in many different domains and applications, from signal processing and adaptive filtering to predictive analysis or deep learning.</p>
<p>In the next chapter, we will present the algorithm and some if its versions, and bring to light its behavior when dealing with different types of data and prediction problems. We will explain why quantile binning of numeric values, which is often frowned upon, is such a performance and stability booster in our case.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>