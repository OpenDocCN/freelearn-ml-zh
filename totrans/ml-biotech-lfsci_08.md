# 第 6 章：无监督机器学习

通常，你将在课程和培训中遇到的许多数据科学教程都围绕**监督机器学习**（SML）领域展开，在该领域中，数据和相应的标签被用来开发预测模型以自动化任务。然而，在现实世界的数据中，预先标记或分类数据的可用性很少，你将遇到的多数数据集将以原始和无标签的形式存在。对于这些情况，或者主要目标是更探索性的或不是必然可自动化的，无监督机器学习的领域将非常有价值。

在本章的整个过程中，我们将探讨与聚类和**降维**（DR）领域相关的许多方法。我们将探讨的主要主题如下：

+   **无监督学习**（UL）简介

+   理解聚类算法

+   教程 – 通过聚类进行乳腺癌预测

+   理解 DR

+   教程 – 探索 DR 模型

在心中牢记这些主题后，我们现在就可以开始着手了！

# UL 简介

我们将 UL 定义为机器学习的一个子集，其中模型是在没有类别或标签存在的情况下进行训练的。与它的监督学习对应物不同，UL 依赖于模型的发展来捕捉以特征形式存在的模式，从而从数据中提取洞察。现在让我们更详细地看看 UL 的两个主要类别。

在 UL 的范围内存在许多不同的方法和技术。我们可以将这些方法分为两大类：具有**离散**数据（聚类）的方法和具有**连续**数据（DR）的方法。我们在这里可以看到这种图形表示：

![图 6.1 – UL 的两种类型](img/B17761_06_001.jpg)

图 6.1 – UL 的两种类型

在这些技术中，数据要么被分组，要么被转换，以便在不知道数据事先的标签或类别的情况下确定标签或提取洞察和表示。以我们在 [*第 5 章*](B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082) 中使用的乳腺癌数据集为例，*理解机器学习*，我们开发了一个分类模型。我们通过明确告诉模型数据中哪些观察结果是恶性的，哪些是良性的来训练模型，从而允许它通过特征学习差异。与我们的监督模型类似，我们可以训练一个无监督的**聚类**模型，通过将我们的数据聚类成组（恶性和良性）来做出类似的预测，而不需要事先知道标签或类别。我们可以使用许多不同类型的聚类模型，我们将在下一节中探索其中的一些，并在本章的后面部分探索其他模型。

除了聚类我们的数据外，我们还可以通过一种称为**DR**的方法来探索和转换我们的数据，我们将将其定义为将高维数据转换到低维空间的过程，在这个空间中保留了特征的有意义属性。数据转换可以用来将特征数量减少到几个，或者为给定数据集设计新的和有用的特征。属于这一类最受欢迎的方法之一是称为**主成分分析**（**PCA**）的过程——我们将在本章后面详细探讨这个特定模型。

在这两个范畴的范围内，有一个应用广泛的领域，它还不完全是一个第三类，因为它具有广泛的应用——这被称为**异常检测**。在UL的范畴内，异常检测正如其名，是一种在未标记数据集中检测异常的方法。请注意，与聚类方法不同，其中数据集的不同标签之间通常有一个平衡（例如，50:50），但异常通常很少见，因为观察的数量通常不是平衡的。从无监督的角度来看，目前最流行的异常检测方法不仅包括**聚类**和**DR**，还包括**神经网络**和**隔离森林**。

现在我们已经对UL的一些高级概念有了感觉，并且知道了我们的目标，那么现在让我们开始一些细节和每个例子。

# 理解聚类算法

在UL的范畴内，最常见的属于该类的方法是**聚类分析**。聚类分析背后的主要思想是将数据分组到两个或更多具有相似性质的类别中，形成组或**簇**。在本节中，我们将探讨这些不同的聚类模型，并随后将我们的知识应用于一个现实世界的场景，即开发用于检测乳腺癌的预测模型。让我们继续探索一些最常见的聚类算法。

## 探索不同的聚类算法

存在的不仅仅是单一的一种，而是一系列广泛的聚类算法，每种算法都有其独特的处理数据聚类的最佳方法，这取决于手头的数据集。我们可以将这些聚类算法分为两大类：**层次聚类**和**划分聚类**。我们可以在下面看到这种分类的图形表示：

![图6.2 – 两种聚类算法](img/B17761_06_002.jpg)

图6.2 – 两种聚类算法

在考虑了这些不同的聚类领域之后，现在让我们更详细地探讨这些内容，从层次聚类开始。

### 层次聚类

**层次聚类**，正如其名所示，是一种尝试根据给定的层次结构使用两种类型的方法（**聚合**或**分解**）对数据进行聚类的算法。聚合聚类是一种**自下而上**的方法，其中数据集中的每个观测值都被分配给其自己的聚类，随后与其他聚类合并以形成一个层次结构。另一方面，**分解聚类**是一种**自上而下**的方法，其中给定数据集的所有观测值最初在一个单独的聚类中，然后被分割。我们可以在这里看到这种图形表示：

![图6.3 – 聚类和分解聚类的区别](img/B17761_06_003.jpg)

图6.3 – 聚类和分解聚类的区别

考虑到层次聚类的概念，我们可以想象出许多有用的应用，这可以帮助我们在系统发育树和其他生物学领域。另一方面，也存在其他聚类方法，其中没有考虑层次结构，例如使用**欧几里得**距离时。

### 欧几里得距离

除了层次聚类之外，我们还有一套属于**基于划分的聚类**思想的模型。这里的核心理念是使用给定方法将数据集分割或划分成聚类。两种最常见的基于划分的聚类类型是**基于距离的聚类**和**基于概率的聚类**。在基于距离的聚类中，这里的核心理念是仅根据距离（如**欧几里得距离**）来确定给定数据点是否属于一个聚类。一个例子是**K-Means**聚类算法——这是一种最常用的聚类算法，鉴于其简单性。

注意，从数学角度来看，**欧几里得**距离，有时也称为**毕达哥拉斯**距离，定义为笛卡尔坐标系上两点之间的距离。例如，对于两个点，*p* (*p1*, *p2*) 和 *q* (*q1*, *q2*)，欧几里得距离可以计算如下：

![](img/Formula_B17761_06_001.jpg)

在二维的上下文中，这个模型相当简单且易于计算。然而，当给定更多维度时，该模型的复杂性会增加，简单表示如下：

![](img/Formula_B17761_06_002.jpg)

现在我们已经更好地理解了欧几里得距离的概念，让我们现在看看一个实际的应用，即K-Means。

### K-Means聚类

考虑到欧几里得距离的概念，我们现在来仔细看看它如何在K-Means的上下文中应用。K-Means算法试图通过将样本分离成具有相等方差的*k*组来聚类数据，并最小化一个**标准**（惯性）。该算法的目标是选择*k*个**质心**以最小化惯性。

从某种意义上说，K-Means模型非常简单，因为它只通过三个简单的步骤进行操作，如下面的图中用星号表示：

![图6.4 – K-Means聚类步骤](img/B17761_06_004.jpg)

图6.4 – K-Means聚类步骤

首先，随机初始化指定数量的*k*个**质心**。其次，每个观测值（用圆圈表示）根据距离进行聚类。然后计算给定簇中所有观测值的平均值，并将质心移动到该平均值。这个过程会不断重复，直到达到预定的阈值为止，从而实现收敛。

**K-Means** 是最常用的聚类算法之一，鉴于其简单性和相对可接受的计算能力。它适用于高维数据，并且相对容易实现。然而，它确实有其局限性，即它假设簇是球形的，这往往会导致非球形簇的数据被错误分组。以另一个簇不是球形而是更像椭圆形的数据集为例。基于**距离**概念的**K-Means**模型的应用不会产生最准确的结果，如下面的截图所示：

![图6.5 – 非球形簇的K-Means聚类](img/B17761_06_005.jpg)

图6.5 – 非球形簇的K-Means聚类

当与非球形簇一起操作时，一个基于**距离**的模型的好替代方案是统计方法，如**高斯混合模型**（**GMM**）。

### GMMs

在聚类的背景下，GMMs是一组由特定数量的**高斯分布**组成的算法。每个分布代表一个特定的簇。到目前为止，在这本书的范围内，我们还没有讨论高斯分布——这是一个你作为数据科学家在整个职业生涯中经常会听到并遇到的概念。让我们继续定义这个概念。

**高斯分布**可以被视为一个统计方程，表示围绕其均值对称分布的数据点。你经常会听到这种分布被称为钟形曲线。我们可以将高斯分布的**概率密度函数**表示如下：

![](img/Formula_B17761_06_003.jpg)

在这里，![公式B17761_06_004](img/Formula_B17761_06_004.png)代表均值，![公式B17761_06_005](img/Formula_B17761_06_005.png)代表方差。请注意，此函数代表一个单一变量。当添加其他变量时，我们将开始进入多元高斯模型的空间，其中*x*和![公式B17761_06_006](img/Formula_B17761_06_006.png)代表长度为![公式B17761_06_007](img/Formula_B17761_06_007.png)的向量。在一个包含*k*个聚类的数据集中，我们需要*k*个高斯分布的混合，其中每个分布都有一个均值和方差。这两个值通过称为**期望最大化**（**EM**）的技术来确定。

我们将**EM**定义为一种算法，当某些数据被认为缺失或不完整时，它确定给定模型的适当参数。这些缺失或不完整的项被称为**潜在变量**，在UL的范围内，我们可以认为实际的聚类是未知的。请注意，如果聚类是已知的，我们将能够确定均值和方差；然而，我们需要知道均值和方差来确定聚类（想想经典的先有鸡还是先有蛋的情况）。我们可以在数据的范围内使用EM来确定这两个变量的适当值，以最佳地拟合模型参数。考虑到所有这些，我们现在可以更智能地讨论GMMs。

我们之前将GMM定义为由多个高斯分布组成的模型。现在，我们将通过包括它是一个由多个高斯分布组成的概率模型，并利用**软聚类**方法来详细阐述这个定义，该方法通过基于概率而不是距离来确定数据点到给定聚类的隶属关系。请注意，这与K-Means形成对比，K-Means使用**硬聚类**方法。使用上一节中*图6.5*所示的前一个示例数据集，应用GMM可能会带来改进的结果，如图所示：

![图6.6 – K-Means聚类与GMMs](img/B17761_06_006.jpg)

图6.6 – K-Means聚类与GMMs

在本节中，我们讨论了在生物技术领域许多应用中常用的几种最常见的聚类算法。我们看到聚类被应用于生物分子数据、科学文献、制造甚至肿瘤学等领域，正如我们将在下面的教程中所体验到的。

## 教程 – 通过聚类预测乳腺癌

在本教程的整个过程中，我们将探讨使用`Wisconsin乳腺癌`数据集来分析和预测癌症的常用聚类算法的应用，该数据集我们在[*第五章*](B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082)《理解机器学习》中应用过。当我们上次访问这个数据集时，我们从监督分类器的角度来开发模型，我们事先知道观察的标签。然而，在大多数实际场景中，事先知道标签的情况很少见。正如我们很快就会看到的，**聚类分析**在这些情况下可以非常有价值，甚至可以用来为数据打标签，以便在分类器的上下文中使用。在本教程的整个过程中，我们将使用数据来开发我们的模型，但假装我们不知道标签。我们只会使用已知的标签来比较我们模型的结果。考虑到这一点，让我们开始吧！

我们将首先导入我们的数据集，就像我们之前做的那样，并检查其形状，如下所示：

[PRE0]

我们注意到这个数据集中有569行数据。在我们之前的应用中，我们已经清理了数据以解决缺失和损坏的值。让我们继续清理这些数据，如下所示：

[PRE1]

当前数据的形状由569行和32列组成，这与我们之前的数据集相匹配，我们现在可以继续进行了。

虽然我们不会使用这些标签来开发任何模型，但让我们快速看一下它们，如下所示：

[PRE2]

在下面的屏幕截图中，我们可以看到有两个类别——`M`代表恶性，`B`代表良性。这两个类别并不完全平衡，但对我们聚类模型来说足够了：

![图6.7 – 两个类别的分布](img/B17761_06_007.jpg)

图6.7 – 两个类别的分布

为了在接下来的聚类分析步骤中更容易地比较这些标签，让我们继续将这些标签编码为数值，其中我们将`M`转换为`1`，将`B`转换为`0`，如下所示：

[PRE3]

我们可以使用`df.head()`函数查看数据集的前几行，并确认`diagnosis`列确实被正确编码。接下来，我们将准备一个快速的对数图来展示几个选定的特征，如下所示：

[PRE4]

我们可以使用`markers`参数来指定两个不同的形状来绘制两个类别，得到以下配对图，显示了我们的特征的散点图：

![图6.8 – 选择特征的配对图](img/B17761_06_008.png.jpg)

图6.8 – 选择特征的配对图

我们的首要目标是浏览许多特征，并了解哪两个特征重叠最少或分离度最高。我们可以看到 `smoothness_mean` 和 `texture_mean` 列有很高的重叠度；然而，`radius_mean` 和 `texture_mean` 的重叠似乎较少。我们可以通过使用 `seaborn` 库绘制散点图来更仔细地观察这些特征，如下所示：

[PRE5]

注意，我们再次可以使用 `style` 和 `markers` 参数来塑造数据点，从而得到以下图表作为输出：

![图6.9 – 两个表现出良好分离的特征的散点图](img/B17761_06_009.png.jpg)

图6.9 – 两个表现出良好分离的特征的散点图

接下来，我们将对数据进行归一化。在统计学中，归一化或标准化可能有多种含义，有时可以互换使用。我们将定义归一化是指将值重新缩放到 [*0*，*1*] 的范围内。另一方面，我们将定义标准化是指将数据重新缩放到平均值和标准差均为1。为了实现我们的当前目标，我们希望使用 `StandardScaler` 类对数据进行标准化，就像我们之前做的那样。回想一下，这个类通过移除均值并缩放到方差来在数据集内标准化特征，如下所示：

![](img/Formula_B17761_06_008.jpg)

在这里，![](img/Formula_B17761_06_009.png) 是样本的标准分数，![](img/Formula_B17761_06_010.png) 是平均值，而 ![](img/Formula_B17761_06_011.png) 是标准差。我们可以在Python中使用以下代码应用此方法：

[PRE6]

在我们的数据集缩放后，我们现在可以开始应用一些模型了。我们将从 `sklearn` 库中的凝聚聚类模型开始。

### 凝聚聚类

回想一下，**凝聚聚类**是一种通过递归合并聚类来形成聚类的算法。让我们继续使用我们的数据集实现凝聚聚类算法，如下所示：

1.  首先，我们将从 `sklearn` 库中导入感兴趣的特定类，然后通过指定我们想要的类别数量并将连接设置为 `ward`（这是最常用的凝聚聚类方法之一）来创建我们模型的实例。代码如下所示：

    [PRE7]

1.  接下来，我们将我们的模型拟合到数据集上，并预测它们所属的聚类。注意在下面的代码片段中，我们使用了 `fit_predict()` 函数，使用前两个特征 `radius_mean` 和 `texture_mean`，而不是整个数据集：

    [PRE8]

1.  然后，我们可以使用 `matplotlib` 和 `seaborn` 生成一个图表，显示左侧的实际（真实）结果和右侧预测的凝聚聚类结果，如下所示：

    [PRE9]

    注意在前面的代码片段中使用了`subplot()`功能，其中`122`的值被用来表示`1`为总行数，`2`为总列数，`2`为特定索引位置的绘图。您可以在以下位置查看输出：

    ![图6.10 – 聚类模型相对于实际结果的比较结果](img/B17761_06_010.png.jpg)

    图6.10 – 聚类模型相对于实际结果的比较结果

1.  从初始估计来看，模型在区分两个聚类方面做得相当合理，尽管对实际真实结果了解甚少。我们可以使用`sklearn`中的`accuracy_score`方法来快速衡量其性能。虽然了解召回率和f-1分数也很重要，但为了简单起见，我们现在将坚持使用准确率。以下代码片段展示了这一过程：

    [PRE10]

总结来说，仅使用数据集的前两个特征进行的层次聚类模型得到了大约83%的准确率——这不是一个糟糕的初次尝试！如果您正在使用提供的代码进行跟随，我鼓励您尝试添加另一个特征，并用三个、四个或五个特征而不是两个特征来拟合模型，看看您是否能够提高性能。更好的是，探索这个数据集中提供的其他特征，看看您是否可以找到其他提供更好分离的特征，并击败我们的83%指标。现在，让我们研究K-Means的性能。

### K-Means聚类

现在，让我们研究使用数据集应用**K-Means**聚类的应用。回想一下，K-Means算法试图通过根据数据点的质心位置将数据划分为*k*个聚类来聚类数据。我们可以按照以下步骤应用K-Means算法：

1.  我们将首先从`sklearn`库中导入`KMeans`类，如下所示：

    [PRE11]

1.  接下来，我们可以初始化K-Means模型的实例，并指定聚类数量为`2`，迭代次数为`10`，初始化方法为`k-means++`。这种初始化设置简单地使用算法选择初始聚类中心，目的是加快收敛速度。我们可以通过一个称为调整的过程来调整参数，以最大化模型性能。以下代码片段展示了这一过程：

    [PRE12]

1.  然后，我们可以使用`fit_predict()`方法来拟合我们的数据，并预测每个观察值的聚类。注意在以下代码片段中，模型仅基于前两个特征进行拟合和预测结果：

    [PRE13]

1.  最后，我们可以使用`seaborn`库来绘制预测结果与已知类别的真实值之间的比较结果，如下所示：

    [PRE14]

    执行此代码后，我们得到一个散点图，显示了我们的结果，如下所示：

    ![图6.11 – K-Means聚类模型相对于实际结果的结果](img/B17761_06_011.png.jpg)

    [PRE15]

1.  接下来，使用`subplot()`方法，我们可以生成四个图表来展示变化，其中每个子图代表一个展示的图表。以下是所需的代码：

    [PRE16]

    代码执行后，我们得到以下展示结果的图：

![图6.12 – 随着特征增加的K-Means聚类模型的结果](img/B17761_06_012.jpg)

图6.12 – 随着特征增加的K-Means聚类模型的结果

我们可以使用仅两个特征来计算准确率，大约为86%，而三个特征产生了89%。然而，我们会注意到，随着更多特征的加入，准确率不仅开始趋于平稳，而且在所有特征都包含时还会下降，产生了较低的准确率82%。请注意，当我们开始向模型添加更多特征时，我们实际上是在增加更多的维度。例如，有三个特征时，我们现在使用的是一个**三维**（**3D**）模型，正如两个数据集之间的混合边界所示。在某些情况下，我们拥有的特征越多，对给定模型的压力就越大。这涉及到一个被称为**维度诅咒**（**COD**）的概念，即在更多维度的情况下，空间的体积开始以惊人的速度增加，这可能会影响模型的性能。我们将在未来的教程中讨论一些可以补救的方法，特别是当我们开始讨论**DR**时。

总结来说，我们能够在我们的数据集上应用K-Means模型，并且使用前三个特征实现了相当高的准确率，达到了89%。现在让我们继续探索统计方法如GMM的应用。

### GMMs

现在，让我们探索GMM**s**在我们数据集上的应用。回想一下，这些模型代表概率分布的混合，一个观察值属于哪个聚类的计算是基于该概率而不是欧几里得距离。考虑到这一点，让我们继续开始，如下所示：

1.  我们可以通过从`sklearn`库中导入`GaussianMixture`类来开始，如下所示：

    [PRE17]

1.  接下来，我们将创建模型的一个实例，并将组件数量指定为`2`，将协方差类型设置为`full`，这样每个组件都有自己的协方差矩阵，如下所示：

    [PRE18]

1.  然后，我们将使用前两个特征将数据模型拟合到我们的数据上，并预测每个观察值的聚类，如下所示：

    [PRE19]

1.  最后，我们可以使用`seaborn`库来绘制结果，如下所示：

    [PRE20]

    执行我们的代码后，我们得到以下输出，显示了数据集的实际结果与我们的预测结果相对比：

![图6.13 – GMM相对于实际结果的结果](img/B17761_06_013.png.jpg)

图6.13 – GMM相对于实际结果的结果

再次，我们可以看到在高斯模型中，两个类别的边界非常明确，几乎没有混合，正如实际结果所示，从而实现了大约85%的准确率。然而，请注意，与K-Means模型相比，GMM预测了一个密集的蓝色圆形分布，橙色类别的某些成员以一种非常非圆形的方式围绕它：

与之前的模型类似，我们再次尝试向这个模型添加一些更多特性，以期进一步提高其性能。然而，从下面的截图我们可以看到，尽管从左到右添加了更多特性，但模型并没有得到改善，其预测能力开始下降：

![图6.14 – 增加特征后的GMM结果](img/B17761_06_014.png.jpg)

图6.14 – 增加特征后的GMM结果

总结来说，在本教程中，我们探讨了在假设没有标签的情况下，使用聚类分析来开发各种预测模型的方法。在整个教程中，我们研究了三种最常见的聚类模型：`Wisconsin乳腺癌`数据集。我们确定，相对于其他模型，使用三个特征的K-Means模型在性能上表现最佳，其中一些模型使用了整个数据集。我们可以推测，所有特征在预测能力方面都具有一定的意义；然而，在模型中包含所有特征会导致性能下降。我们将在下一节中探讨一些减轻这种情况的方法，这涉及到**DR**。

# 理解DR

我们接下来要讨论的**UL**的第二类被称为**DR**。正如全名所表明的，这些方法只是用来减少给定数据集中维度的数量。以一个具有大约100个列的高特征数据集为例——DR算法可以用来帮助将列数减少到大约5个，同时保留每个原始100个列所包含的价值。你可以把DR看作是按水平方式压缩数据集的过程。结果列通常可以分为两种类型：新特征，即在称为**特征工程**（**FE**）的过程中生成了具有新数值的新列，或者旧特征，即在称为**特征选择**的过程中只保留了最有用的列。在接下来的章节中，在UL的范围内，我们将更多地关注**FE**的方面，因为我们创建了许多其他数据集的简化版本的新特征。我们可以在这里看到这个概念的图形说明：

![图6.15 – DR的图形表示](img/B17761_06_015.jpg)

图6.15 – DR的图形表示

我们可以使用许多不同的方法来实现数据降维（DR），每种方法都有自己的流程和理论基础；然而，在我们开始实施这些方法之前，有一个非常重要的概念我们需要解决。你现在可能想知道为什么DR很重要。为什么任何数据科学家会在另一个数据科学家或数据工程师费尽周折地构建一个全面且丰富的数据集之后，还要删除特征呢？对此问题有三个答案，如下所述：

+   我们并不一定从我们的数据集中删除任何数据，而是在不同的视角下探索我们的数据，这可能会提供一些我们使用原始数据集看不到的新见解。

+   开发具有许多特征的模型是一个计算成本高昂的过程，因此使用较少特征来训练我们的模型将始终更快、计算量更少、更有利。

+   使用DR可以帮助减少数据集中的噪声，从而进一步提高聚类模型和数据可视化。

    考虑到这些答案，现在让我们来谈谈你将在许多会议、讨论和面试中听到的概念——COD。

## 避免COD

COD被视为在处理高维数据集时出现的一种普遍现象——这个术语最初是由理查德·E·贝尔曼提出的。本质上，COD指的是在高维数据集中出现的问题，而在类似大小但低维的数据集中不会出现。随着给定数据集中特征数量的增加，样本总数也会成比例增加。以一个一维数据集为例。在这个数据集中，假设我们需要检查总共10个区域。如果我们添加第二个维度，我们现在需要检查总共100个区域。最后，如果我们添加第三个维度，我们现在需要检查总共1,000个区域。回想一下我们迄今为止一直在处理的一些数据集，这些数据集的行数远远超过1,000，至少有10列——这些数据集的复杂性可以迅速增长。这里的主要启示是特征增长对模型的发展有重大影响。我们可以在这里看到这一点的图形说明：

![图6.16 – COD的图形表示](img/B17761_06_016.jpg)

图6.16 – COD的图形表示

随着特征数量的增加，机器学习（ML）模型的整体复杂性也会增加，这可能会产生许多负面影响，如过拟合，从而导致性能不佳。减少数据集维度的主要动机之一是确保避免过拟合，从而产生更稳健的模型。我们可以在这里看到这一点的图形说明：

![图6.17 – 高维对模型性能的影响](img/B17761_06_017.jpg)

图 6.17 – 高维对模型性能的影响

将数据集从高度维数形式降低到低维形式的需求在生命科学和生物技术领域尤为明显。在这个领域中，科学家和工程师面临的各种过程中，通常与任何给定过程相关的特征有数百个。无论我们是在寻找与蛋白质结构、单克隆抗体滴度、小分子对接位点选择、**双特异性T细胞连接器**（**BiTE**）药物设计，甚至是与**自然语言处理**（**NLP**）相关的数据集，特征的减少总是有用的，并且在许多情况下对于开发一个好的机器学习模型是必要的。

现在我们已经对 DR 及其与 COD 概念的关系以及这些方法可能带来的众多好处有了更好的理解，让我们继续看看在这个领域中我们应该了解的一些最常见模型。

## 教程 – 探索 DR 模型

我们可以根据类型、功能、结果等对众多维度算法进行多种不同的分类。然而，为了在本章的几页内对 DR 有一个强有力的概述，我们将把我们的模型分类为线性或非线性。线性模型和非线性模型是两种不同的数据转换类型。我们可以将数据转换视为数据以某种方式改变或重塑的方法。我们可以粗略地将线性方法定义为模型输出与其输入成比例的转换。以 *p* 和 *q* 作为两个数学向量为例。

当以下条件成立时，我们可以认为一个转换是线性的：

+   *p* 的转换乘以一个标量，其结果与将 *p* 乘以标量然后应用转换相同。

+   *p* + *q* 的转换与 *p* + *q* 的转换相同。

如果一个模型不满足这两个特性，则被视为非线性模型。许多不同的模型都包含在这两个类别中；然而，为了本章的目的，我们将探讨近年来在数据科学社区中相当受欢迎的四个主要模型。这里我们可以看到这一点的图形说明：

![图 6.18 – DR 各个领域的模型示例](img/B17761_06_018.jpg)

图 6.18 – DR 各个领域的模型示例

在线性方法的范围内，我们将仔细研究**PCA**和**奇异值分解**（**SVD**）。此外，在非线性方法的范围内，我们将仔细研究**t分布随机邻域嵌入**（**t-SNE**）和**均匀流形近似与投影**（**UMAP**）。考虑到这四种模型以及它们在DR宏大方案中的位置，让我们开始吧。

### PCA

最常见且广泛讨论的UL形式之一是**PCA**。PCA是一种线性形式的DR，允许用户将大量相关特征的大数据集转换成更少的无关特征，这些特征被称为主成分。这些**主成分**，尽管在数量上少于原始特征，但仍能保留与原始数据集一样多的变化或**丰富性**。我们可以在下面看到这一点的图形说明：

![图6.19 – PCA及其主成分的图形表示](img/B17761_06_019.jpg)

图6.19 – PCA及其主成分的图形表示

为了有效地在任何给定数据集上实施PCA，需要发生几件事情。让我们从高层次概述这些步骤以及它们如何影响最终结果。我们首先必须对数据进行归一化或标准化，以确保均值为0，标准差为1。接下来，我们计算所谓的**协方差矩阵**，它是一个包含每对元素之间协方差的方阵。在一个**二维**（**2D**）数据集中，我们可以将协方差矩阵表示如下：

![](img/Formula_B17761_06_012.jpg)

接下来，我们可以计算协方差矩阵的**特征值**和**特征向量**，如下所示：

![](img/Formula_B17761_06_013.jpg)

在这里，*![](img/Formula_Symbol.png)*是给定矩阵*A*的特征值，而*I*是单位矩阵。使用特征向量，我们可以使用以下方程确定特征值*v*：

![](img/Formula_B17761_06_014.jpg)

接下来，将特征值从大到小排序，这代表了按重要性顺序的成分。具有*n*个变量或特征的数据集将具有*n*个特征值和特征向量。然后我们可以将特征值或向量的数量限制在预定的数量，从而降低数据集的维度。然后我们可以使用感兴趣的特征向量形成我们所说的特征向量。

最后，我们可以通过特征向量的**转置**以及原始数据集缩放数据的**转置**，将它们相乘，从而形成**主成分**，如下所示：

![](img/Formula_B17761_06_015.jpg)

在这里，*PrincipalComponents*以矩阵的形式返回。简单，对吧？

现在让我们使用Python实现PCA，如下所示：

1.  首先，我们从 `sklearn` 库中导入 PCA 并实例化一个新的 PCA 模型。我们可以将组件数量设置为 `2`，表示我们只想返回两个组件，并将 `svd_solver` 设置为 `full`。然后，我们可以将数据拟合到我们的缩放数据集上，如下所示：

    [PRE21]

    [PRE22]

1.  接下来，我们可以转换我们的数据并将输出矩阵分配给 `data_pca_2d` 变量，如下所示：

    [PRE23]

1.  最后，我们可以使用 `seaborn` 绘制结果，如下所示：

    [PRE24]

    执行此代码后，这将生成一个散点图，显示我们的主成分，我们的点使用 `y` 着色，如图所示：

![图 6.20 – PCA 结果的散点图](img/B17761_06_020.png.jpg)

图 6.20 – PCA 结果的散点图

PCA 是一种快速且高效的方法，最好在维度变得过于复杂时作为开发 ML 模型的先导使用。回想一下我们在乳腺癌预测相关的聚类分析中使用的数据集。我们可以在原始或缩放数据上运行我们的模型之前，实现一个 DR 算法，如 PCA，将维度减少到仅两个主成分，然后再应用后续的聚类模型。记住，PCA 只是许多线性模型之一。现在，让我们继续探索另一个流行的线性模型，称为 SVD。

### SVD

**SVD** 是一种流行的 **矩阵分解** 方法，通常用于将数据集简化为更简单的形式。在本节中，我们将专门讨论截断 SVD 的应用。此模型与 PCA 非常相似；然而，主要区别在于估计量在计算之前不进行中心化。本质上，这种差异使得模型能够非常有效地用于稀疏矩阵。

现在，让我们介绍并查看一个我们可以用来应用 SVD 的新数据集：*单细胞 RNA*（其中 **RNA** 代表 **核糖核酸**）。该数据集可在 [http://blood.stemcells.cam.ac.uk/data/nestorowa_corrected_log2_transformed_counts.txt](http://blood.stemcells.cam.ac.uk/data/nestorowa_corrected_log2_transformed_counts.txt) 找到。此类数据集涉及单细胞测序的主题——这是一个检查单个细胞序列的过程，以更好地了解它们的属性和功能。此类数据集通常具有许多数据列，使它们成为 DR 模型的理想候选。让我们继续导入此数据集，如下所示：

[PRE25]

观察形状，我们可以看到有 3,991 行数据和 1,645 列。相对于我们使用的许多其他数据集，这个数字相当大。在生物技术领域，DR 非常常用以帮助将此类数据集减少到更易于管理的实体。注意，索引包含有关我们正在查看的细胞类型的一些信息。为了使我们的视觉效果更有趣，让我们通过执行以下代码来捕获此注释数据：

[PRE26]

数据都准备好了，让我们继续在这个数据集上实现截断SVD。我们可以再次通过实例化一个截断SVD模型并将组件设置为`2`，迭代次数为`7`来开始，如下所示：

[PRE27]

接下来，我们可以继续使用`fit_transform()`方法来拟合我们的数据和将DataFrame转换为一个两列的数据集，如下所示：

[PRE28]

最后，我们可以通过散点图和颜色标注来绘制我们的数据集。代码在以下片段中展示：

[PRE29]

我们可以在以下屏幕截图中看到执行此代码的结果：

![图6.21 – SVD模型结果的散点图](img/B17761_06_021.png.jpg)

图6.21 – SVD模型结果的散点图

在前面的屏幕截图中，我们可以看到近1400列的数据被简化为简单的2D表示——是不是很令人着迷？能够以这种方式减少数据的一个最大的优点是它有助于模型开发。让我们假设，为了举例，我们希望将我们之前提到的任何聚类算法应用于这个庞大的数据集。在近1400列的数据集上训练任何给定模型所需的时间将比在2列的数据集上长得多。实际上，如果我们在这个数据集上实现GMM，使用原始数据集的总训练时间将是**12.4秒 ± 158毫秒**，而使用减少后的数据集则是**4.06毫秒 ± 26.6毫秒**。虽然线性模型在处理DR时非常有用，但非线性模型也可以有类似惊人的效果。接下来，让我们看看一个流行的模型，称为t-SNE。

### t-SNE

在非线性DR方面，最常见的一个流行模型是**t-SNE**。与我们所讨论的其他维度模型相比，t-SNE模型的一个独特特征是它使用概率分布来表示邻居之间的相似性。简单来说，t-SNE是一种统计方法，允许对高维数据进行DR和可视化，其中相似点靠近，不相似点远离。

t-SNE是一种**流形**模型，从数学角度来看，它是一个类似于**欧几里得**空间的拓扑空间。流形的概念复杂、广泛，超出了本书的范围。为了简化，我们将说明流形描述了大量的几何表面，如球体、环面或十字形表面。在t-SNE模型的范围内，主要目标是使用几何形状来使用户对高维数据的排列或组织有一个感觉或直觉。

现在，让我们仔细看看 t-SNE 在 Python 中的应用。再次，我们可以在我们的单细胞 RNA 数据集上应用这个模型，并从几何角度了解这个数据的高维组织结构。t-SNE 中的许多参数都可以更改和调整以适应特定目的；然而，有一个参数特别值得简要提及——困惑度。`scikit-learn` 库建议考虑介于 5 和 50 之间的值。让我们继续看看几个示例。

由于 `scikit-learn` 的高层支持，实现这个模型相当简单。我们可以从导入 `scikit-learn` 中的 `TSNE` 类开始，将组件数设置为 `2`，将困惑度设置为 `10`。然后我们可以使用我们的数据集链式调用 `fit_transform()` 方法，如下面的代码片段所示：

[PRE30]

然后，我们可以继续使用 `seaborn` 来绘制我们的数据，以可视化结果，如下所示：

[PRE31]

我们可以在下面的屏幕截图中看到这个结果：

![图 6.22 – t-SNE 模型的结果散点图](img/B17761_06_022.png.jpg)

图 6.22 – t-SNE 模型的结果散点图

真是令人印象深刻！从前面的屏幕截图中我们可以看到，该模型在没有标签知识的情况下，使用它所给出的巨大数据集，对数据点之间的关系进行了二维投影。产生的几何形状给我们一种关于数据的*外观*和*感觉*。我们可以从这个描述中看出，一些点似乎被认为是异常值，因为它们被描绘得离得很远，就像岛屿相对于大陆一样。回想一下，我们为这个特定的图表使用了`10`的困惑度值。让我们继续探索这个参数，使用几个不同的值，如下所示：

[PRE32]

使用这些计算出的值，我们可以使用 `seaborn` 库将它们并排放置，如下所示：

![图 6.23 – 随着困惑度增加的 t-SNE 模型的散点图](img/B17761_06_023.png.jpg)

图 6.23 – 随着困惑度增加的 t-SNE 模型的散点图

当涉及到高维数据时，t-SNE 是最常用的模型之一，它不仅能降低你的维度，还能通过获取其特征及其关系的独特感觉来探索你的数据。尽管 t-SNE 可能很有用且有效，但它确实有几个负面方面。首先，它对于大样本量（如你在某些 RNA 测序数据案例中看到的那样）的扩展性不好。其次，它也没有保留全局数据结构，也就是说，不同簇之间的相似性没有得到很好的维护。另一个试图解决这些担忧并采用与 t-SNE 类似方法的流行模型被称为 UMAP。让我们在下一节中探索这个模型。

### UMAP

**UMAP**模型是一个流行的算法，用于基于流形学习技术的降维和数据可视化，类似于t-SNE。该算法基于三个主要假设，如他们在主要网站([https://umap-learn.readthedocs.io/en/latest](https://umap-learn.readthedocs.io/en/latest))上所述，并在以下内容中概述：

+   数据集在黎曼流形上均匀分布。

+   黎曼度量在局部是常数。

+   流形在局部是连通的。

尽管UMAP和t-SNE非常相似，但它们之间有一些关键的区别。其中最重要的区别之一与相似性保留的概念有关。UMAP模型声称在局部和全局数据中保持相似性，即局部和全局信息，或簇间和簇内信息得到维护。我们可以在这里看到这个概念的图形表示：

![图6.24 – 局部和全局相似性的图形表示](img/B17761_06_024.jpg)

图6.24 – 局部和全局相似性的图形表示

现在我们来应用UMAP到我们的单细胞RNA数据集。我们可以从导入`umap`库并实例化一个新的UMAP模型开始，我们指定组件数量为`2`，邻居数量为`5`。这个第二个参数表示用于流形近似的局部**邻域**的大小。代码如下所示：

[PRE33]

然后，我们可以使用`seaborn`来绘制数据，如下所示：

[PRE34]

执行代码后，我们得到以下输出：

![图6.25 – UMAP结果的散点图](img/B17761_06_025.png.jpg)

图6.25 – UMAP结果的散点图

再次，非常直观的视觉展示！我们可以看到，与t-SNE相比，一些簇已经移动了位置。如果你还记得在t-SNE中，大部分数据被拉在一起，而不管簇之间的相似性如何。使用UMAP，这个信息被保留下来，我们能够更好地了解这些簇之间的关系。注意数据相对于其在t-SNE中的表示的分布。类似于t-SNE，我们可以看到一些*组*的点在不同的邻域中聚集在一起。

总结来说，UMAP是一个功能强大的模型，类似于t-SNE，在处理邻域或簇时，它保留了局部和全局信息。这个模型最常用于可视化，只需几行代码就能很好地了解任何高维数据集的“外观”和“感觉”。

# 摘要

在本章的整个过程中，我们对该领域（UL）有了强烈和高级的理解，包括其用途和应用。然后，我们探讨了与聚类和DR相关的几种最流行的ML方法。在聚类的领域中，我们回顾了一些最常用的模型，如层次聚类、K-Means聚类和GMMs。我们学习了欧几里得距离和概率之间的差异以及它们如何与模型预测相关。此外，我们还将这些模型应用于`Wisconsin乳腺癌`数据集，并在其中几个模型中实现了相对较高的准确率。在DR领域中，我们对该领域与COD相关的意义有了深入的理解。然后，我们使用*单细胞RNA*数据集实现了PCA、SVD、t-SNE和UMAP等模型，将超过1400列减少到2列。然后，我们使用`seaborn`可视化我们的结果，并检查了模型之间的差异。在本章的整个过程中，我们设法在不使用标签的情况下开发我们的模型，我们只在开发过程之后使用标签进行比较。

在下一章中，我们将探讨SML领域，在该领域中，我们除了使用数据标签外，还使用数据本身来开发强大的预测模型。
