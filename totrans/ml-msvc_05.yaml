- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning System Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we delved into the different machine learning concepts
    and the packages and libraries used to create these models. Using that information,
    we will begin to discuss the design process when building a machine learning pipeline
    and the different components found in most machine learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following areas in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning system components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit and transform interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train and serve interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orchestration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning system components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many moving parts required in order to build a robust machine learning
    system. Starting from gathering data to deploying your model to the user, each
    plays a vital role in keeping the system dynamic and scalable. Here, we will briefly
    discuss the different stages in the machine learning system life cycle and the
    role they play. These stages can be edited in order to suit the model or application
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The majority of machine learning systems include the following stages, with
    some other stages depending on business needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data collection**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Date preprocessing**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model testing**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model serving**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Realistically, the majority of the time spent building machine learning systems
    is spent on the data. This is a key element in the process that can decide the
    effectiveness of your system since the model is dependent on the data it uses
    during training. Just like the human body, if you feed the model poor data or
    not enough data, it will output poor results.
  prefs: []
  type: TYPE_NORMAL
- en: The first part when it comes to data is the collection process. Understanding
    the application and the goal of the task can assist in the process of deciding
    how to collect data and what data to collect. We then determine the target value
    that we want to predict, such as the price of a home or the presence of a certain
    disease. These target values can be collected explicitly or implicitly. A target
    variable is explicit when we can directly determine the value of the variable
    we are trying to capture, while an implicit target value is found by using contextual
    data to determine the target value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the task, we usually store the data in a database (for either
    metadata or tabular data) such as MySQL or cloud storage (for images, video, or
    audio) such as Amazon S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: Data collection](img/B18934_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Data collection'
  prefs: []
  type: TYPE_NORMAL
- en: Once we set up continuous data collection, we must devise a procedure for cleaning
    and processing the data. Not everything we collect will be perfect. You will always
    find missing data and certain outliers, which can negatively impact our model.
    No matter how intuitive your model is, it will always perform poorly with garbage
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Some practices to deal with unclean data include removing outliers, normalizing
    certain features, or imputing missing data depending on the amount of data you
    have collected. Once the data has gone through the cleaning process, the next
    step is the feature selection/engineering process.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the different features your data contains plays an important role
    when your model tries to find the relationship in its data. **Exploratory Data
    Analysis** (**EDA**) is the common process used when it comes to understanding
    the data you have collected and how the data is structured. This helps when it
    comes to determining which features to use in your model. As we previously mentioned
    in [*Chapter 4*](B18934_04.xhtml#_idTextAnchor051), when we include more features
    in our models, it allows them to map to more complex problems. However, adding
    too many features can lead to overfitting, so it is important to research the
    most important features for your model.
  prefs: []
  type: TYPE_NORMAL
- en: While most machine learning models can find patterns and relationships in data,
    the best way of understanding the data you collect is via the experts in the field
    of the task you are trying to solve. Subject matter experts can provide the best
    insight into what features to focus on when creating your model. Some unsupervised
    machine learning models, such as PCA and t-SNE, can group and find features that
    can provide the most valuable information for your model.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Having domain knowledge of the problem you are trying to solve is the most effective
    way of understanding your data and determining which features to use for training
    your machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have set up the processes to collect and clean the data, the next step
    is creating and training your model. Thanks to most machine learning libraries,
    you can import prebuilt models and even use weights from already trained models
    to use on your own model. Here, it is common practice to use different models
    and techniques to see which produces the best result, and from there, you can
    choose the best model and begin to fine-tune it by updating the hyperparameters.
    This process can take time depending on the amount of data you use.
  prefs: []
  type: TYPE_NORMAL
- en: Testing your model is a critical element in your system’s pipeline. Depending
    on the application, a poor model can negatively impact your business and give
    your users a bad experience. To prevent that, you need to determine the different
    metrics and thresholds that need to be met for the model to be production-ready.
    If the model can’t meet these expectations, then you need to go back and understand
    the weaknesses of the model and address them before training again.
  prefs: []
  type: TYPE_NORMAL
- en: 'After performing tests and getting solid results from your model, you can now
    deploy your model to the user application. This varies from application to application.
    From then, the whole process can start from the beginning, where new data is inserted
    and follows the machine learning pipeline so it can dynamically grow based on
    user actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2: The machine learning pipeline](img/B18934_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: The machine learning pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will look into the details of the different interfaces
    that constitute our machine learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Fit and transform interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have looked at the entire pipeline process, we will look in detail
    at the different interfaces that make up the machine learning system. The majority
    of the systems include the following interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fit**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transform**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Train**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serve**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it comes to the data and creating the model, we come across the fit and
    transform interfaces. We will start by looking at the transform interface.
  prefs: []
  type: TYPE_NORMAL
- en: Transform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transform interface is the process of taking in the collected data and preprocessing
    the data so that the model can train properly and extract meaningful information.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is common for the data we collect to have missing values or outliers, which
    can cause bias in our model. To remove this bias, we can apply certain techniques
    that help remove the skew in the data and produce meaningful machine learning
    models. Some of the following techniques we will learn about fall into the following
    three types of transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clipping**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log transformation is the most common and simple transformation technique we
    can apply to our data. A lot of the time, our data is skewed in one direction,
    which can introduce bias. To help mitigate the skewed distribution, we can simply
    apply the log function to our data, and this shifts our data into more of a normal
    distribution, which allows the data to be more balanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can perform this transformation by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.3: Performing log transformation on skewed data](img/B18934_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Performing log transformation on skewed data'
  prefs: []
  type: TYPE_NORMAL
- en: Once we apply the log transformation, we can start looking at the other transformations.
    The second transformation we can use is the clipping transformation. The more
    we make our data follow a normal distribution, the better, but we may encounter
    outliers that can skew our data. To help reduce the impact that outliers have
    on our data, we can apply a quantile function. The most common quantile range
    that people use is the 0.05 and 0.95 percentile. This means that any data below
    the 0.05 percentile will be rounded up to the lower bound while any data above
    the 0.95 percentile will be rounded down to the upper bound. This allows us to
    retain the majority of the data while reducing the impact that outliers have on
    the model. The upper and lower ranges can also be modified based on what makes
    sense for the distribution of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This transformation can be performed using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.4: Clipping transformation on data](img/B18934_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Clipping transformation on data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last major transformation technique is scaling transformations. A lot of
    the time, the data we collect have different types of metrics and values, which
    can skew our data and confuse our model. For example, one feature measures the
    revenue of companies in the millions while another feature measures the employee
    count in the thousands, and when using these features to train the model, the
    discrepancy may put more emphasis on one feature over another. To prevent these
    kinds of problems, we can apply scaling transformations, which can be of the following
    types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MinMax**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max Abs**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robust**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit Vector**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The MinMax scaler is the simplest scaling transformation. It works best when
    the data is not distorted. This scales the data between 0 and 1\. It can be calculated
    using this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can perform this scaling transformation using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The MaxAbs scaler is similar to MinMax but rather than scaling the data between
    0 to 1, it scales the data from -1 to 1\. This can be calculated using the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can perform this scaling transformation using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The Standard scaler is another popular scaling transformation. Rather than
    using the min and max like the MinMax scaler, this scales the data so that the
    mean is 0 and the standard deviation is 1\. This scaler works on the assumption
    that the data is normally distributed. This can be calculated using the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can perform this scaling transformation using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The MinMax, MaxAbs, and Standard scalers, while powerful, can suffer from outliers
    and skewed distribution. To remedy this issue, we can use the Robust scaler. Rather
    than using the mean or max, this scaler works by removing the median from the
    data and then scaling the data using the interquartile range. This can be calculated
    using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_05_004.jpg)![](img/Formula_05_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can perform this scaling transformation using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have the Unit Vector scaler, also known as a normalizer. While
    the other scaler functions work based on columns, this scaler normalizes based
    on rows. It uses the MinMax scaler formula and converts positive values between
    0 and 1 and negative values between -1 and 1\. There are two ways of performing
    this scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: L1 norm – values in the column are converted so that the sum of their absolute
    value in the row equals 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 norm – values in the column are squared and added so that the sum of their
    absolute value in the row is equal to 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can perform this scaling transformation using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There are many more scaling and transforming techniques, but these are the most
    commonly used, as they provide stable and consistent results.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Much of the development process takes place in the transformation stage. Understanding
    how the data is structured and distributed helps dictate which transformation
    methods you will perform on your data. No matter how advanced your model is, poorly
    structured data will produce weak models.
  prefs: []
  type: TYPE_NORMAL
- en: Fit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we will look at the fit interface. This interface refers to the process
    of creating the machine learning model that will be used in training. With today’s
    technology, not much work or effort is needed to create the model used for training
    in the machine learning pipeline. There are already prebuilt models ready to be
    imported and used for any type of application.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a small example of creating a KNN classification model using the scikit-learn
    library.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import all the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We then import the data, split the data into training and testing batches,
    and apply a standard scaler transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We then initialize a KNN model with k = `3` and then perform training on the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The main effort when using the fit interface is setting up the models that will
    be used for the training phase of the machine learning pipeline. Due to the simplicity
    of importing multiple prebuilt models, it is common practice to import multiple
    types of machine learning models and train all of them at once. This way, we are
    able to test different types of models and determine which one of them performs
    the best. Once we decide which model to use, we can then start to experiment with
    different hyperparameters to further fine-tune our model.
  prefs: []
  type: TYPE_NORMAL
- en: Train and serve interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The transform and fit interfaces are responsible for preparing the data and
    setting up our machine learning models for our pipeline. Now that we have preprocessed
    the data, we need to begin looking at how we can begin the actual training process
    and take our trained models and deploy them for our clients to use.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have preprocessed the data and created our models, we can begin
    the training process. This stage can vary from time to time depending on the quality
    of data being trained on or the type of model being used during training.
  prefs: []
  type: TYPE_NORMAL
- en: Once we preprocess the data, we need to split the dataset into training and
    testing sets. This is done to prevent overfitting. We need the model to be able
    to generalize the data, and using all the data for training would defeat the purpose.
  prefs: []
  type: TYPE_NORMAL
- en: A common practice is to split your data into 70% training and 30% testing. This
    way, the model has enough data to learn the relationships and uses the testing
    data to self-correct its training process.
  prefs: []
  type: TYPE_NORMAL
- en: There is a more robust approach to splitting the data, which is called **K-Fold
    Cross-Validation**. This process works best in cases where there may not be enough
    training data. To perform this, we split the data into *k* number of subsets and
    then we train on all subsets except for one. We then iterate through this process
    where a new subset is selected to be the test data. Finally, we measure the performance
    of the model by averaging the metrics for each iteration. This way, we can train
    and test using all the data without leaving any important features that may be
    useful when it comes to learning the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5: K-Cross Validation](img/B18934_05_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: K-Cross Validation'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have split the data, now comes the actual training part. This part is
    as simple as setting up the function used to train the model. This part depends
    on the type of library you use and the different APIs it offers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a simple example using the scikit-learn library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: After training your model, you must measure its performance. To prevent poor
    models from being deployed to users, it is a common practice to measure certain
    metrics and set certain thresholds that need to be met before a model is considered
    ready for production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the type of model you create, certain metrics need to be evaluated.
    For example, a regression model will typically look at the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Absolute** **Error** (**MAE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean Squared** **Error** (**MSE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Root Mean Squared** **Error** (**RMSE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**R-Squared** (**R2**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For classification models, you will monitor the following metrics to determine
    the model’s strength:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision and recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The F1-score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Area Under the Receiver Operating Characteristics** **Curve** (**AUROC**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having domain knowledge helps immensely when determining what thresholds are
    applicable to the model you are training. In some cases, such as with cancer detection
    models, it is important to avoid false negatives, so it is important to set stricter
    thresholds for what models can be used confidently.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Before serving your model, you need to make sure the model is viable for production.
    Setting up the metric thresholds that the model needs to pass is a fundamental
    way of validating your models before deploying them. If your model fails to pass
    these criteria, then there should be a process to redo the data transformation
    and model training phases until it can pass the thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Serving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to serving our model, this is open and flexible depending on
    the user’s needs. In most cases, we are deploying our model into one of two types
    of systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model serving**, where we deploy our model as an API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model embedding**, where we deploy our model straight into an application
    or device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model embedding is the simplest way of deploying your model. You create a binary
    file containing your model and you embed the file into your application code.
    This simplicity provides the best performance when making predictions, but this
    comes at a cost. Because you directly embed the file into your application, it
    is difficult to scale your model since you will have to recreate and reupload
    the file every time you make an update to your model. As such, this is not considered
    a recommended practice.
  prefs: []
  type: TYPE_NORMAL
- en: Model serving is the most commonly used method on today’s market. This separation
    between the application and the model makes it easy for a developer to maintain
    and update the model without having to change the application itself. You simply
    create an API service that a user can access to make calls and predictions. Due
    to the separation, you can continuously update the model without having to redeploy
    the whole application.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative to model embedding that includes model serving is creating a
    microservice that includes the binary file of the model, which could be accessed
    by other applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6: Serving machine learning models](img/B18934_05_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Serving machine learning models'
  prefs: []
  type: TYPE_NORMAL
- en: One of the more intuitive approaches is creating your own package or library
    that includes all the models that you have trained. That way, you can scale efficiently
    by allowing multiple applications to access the different models you have created.
  prefs: []
  type: TYPE_NORMAL
- en: Everything we’ve seen so far is what it takes to build a simple machine learning
    pipeline. While this is doable for most applications, to be dynamic and robust,
    we need to look at orchestration and what it can offer us to support more advanced
    applications and problems.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the different interfaces and the roles they play in the
    machine learning pipeline, the next step is understanding how to wrap everything
    together into one seamless system. To understand the holistic system, we must
    first understand automation and orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: '**Automation** refers to the process of automating small or simple tasks, such
    as uploading files to a server or deploying an application, without human intervention.
    Rather than having a person perform these repetitive tasks, we can program our
    system to handle these simple tasks, thus reducing wasted time and resources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is useful for most systems due to the linear nature of the pipeline. This
    highlights a common limitation of automation though – the lack of flexibility.
    Most systems today require a more dynamic process to be able to adapt to certain
    applications and processes, and automation alone isn’t enough:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7: A linear system pipeline](img/B18934_05_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: A linear system pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: This is where orchestration comes into action. **Orchestration** is the configuration
    and coordination of automated tasks to create a whole workflow. We can create
    a system to perform certain jobs or tasks based on a certain set of rules. It
    takes some planning and understanding to create a comprehensive orchestration
    workflow since the user determines what actions the system needs to take for certain
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple example would be deploying an application to users. There can be many
    moving parts in the system, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to a server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uploading certain files to certain servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling user requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data or logs in a database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s say that after the recent changes have been deployed, the app has suffered
    critical errors, which may bring down the application. The system admin could
    set up rules for recovering and restoring the system, such as rolling back to
    a stable version. With the system able to self-recover, the developers can spend
    more time in development rather than dealing with overhead when it comes to recovery.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on certain outcomes, not all tasks may need to be performed. There
    may be backup actions that need to take place, or different paths that the system
    needs to go through to maintain a stable workflow. This way, the system can adapt
    to its environment and self-sustain without much human intervention:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8: A dynamic system pipeline (orchestration)](img/B18934_05_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: A dynamic system pipeline (orchestration)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The different tasks in the machine learning system that can be automated are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Gathering and preprocessing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the machine learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running tests and diagnostics on the trained model to evaluate its performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serving the machine learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring the model in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these automated tasks, the system admin needs to orchestrate the stages
    of the pipeline to be dynamic and sustainable. The following components help create
    a robust system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scheduling**: The system must be able to schedule and run different automated
    tasks in the pipeline individually while maintaining system dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CI/CD Testing**: After model training is complete, it is imperative to do
    automated testing on your model to measure its performance. If it fails to pass
    certain metrics, you must repeat the training process from the beginning to address
    the weaknesses of the model; otherwise, it cannot be deployed to production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment**: Depending on where you will deploy your model to production,
    setting up an automated process can help reduce the time spent on deployment and
    still maintain an updated version of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring**: After deploying your model, continuously monitoring the model’s
    performance in production is needed to maintain the model’s health without it
    decaying. This will give us an indication of when we need to update our pipeline
    or our model in order to stay efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what your business needs are and how your model functions gives
    you a good picture of how you want to orchestrate your machine learning pipeline.
    Setting up backup phases to address certain pitfalls in your system allows it
    to be more dynamic and adaptable to industry demands.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the different key components that make up a machine
    learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: From there, we looked in detail at the interfaces that make up the components.
    We started with the transform interface, which is responsible for the data aspect
    of the pipeline. It takes the data and applies different types of data transformation
    that allow us to maintain clean and stable data, which we can later use in our
    machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: After our transformation stage, we start creating our model in the fit interface.
    Here, we can use the prebuilt models that the libraries and packages offer to
    initialize our models. Due to the ease of creating models, it is a good practice
    to test different types of models to see which model performs the best based on
    our data.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have created our model, we can begin the actual training of our model.
    We need to split our data into training and test sets to allow our model to understand
    the relationship in our data. From there, we can measure the different metrics
    in our model to validate the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Once we feel comfortable with our model’s performance, we can start to deploy
    our application to production. There are two major ways of deploying our model,
    whether it be embedded into our application or deployed as a service for our clients
    to use.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, wrapping everything together, we learned what orchestration consists
    of when it comes to machine learning. We learned what concepts need to be considered
    when orchestrating your machine learning pipeline and how to keep your system
    dynamic and robust to keep up with everyday demands.
  prefs: []
  type: TYPE_NORMAL
- en: As time passes and data changes, it is important that we adjust and maintain
    our models to handle certain situations that may arise in the real world. In the
    next chapter, we will look at how we can maintain our machine learning models
    when our data starts to shift and change.
  prefs: []
  type: TYPE_NORMAL
