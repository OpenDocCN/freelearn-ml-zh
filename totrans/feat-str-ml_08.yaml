- en: 'Chapter 6: Model to Production and Beyond'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章：模型到生产及之后
- en: In the last chapter, we discussed model training and prediction for online and
    batch models with **Feast**. For the exercise, we used the Feast infrastructure
    that was deployed to the AWS cloud during the exercises in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Stores to ML Models*. During these exercises, we looked at how
    Feast decouples feature engineering from model training and model prediction.
    We also learned how to use offline and online stores during batch and online prediction.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了使用 **Feast** 进行在线和批量模型的模型训练和预测。对于练习，我们使用了在 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065)
    *添加特征存储到机器学习模型* 练习期间部署到 AWS 云的 Feast 基础设施。在这些练习中，我们探讨了 Feast 如何将特征工程与模型训练和模型预测解耦。我们还学习了如何在批量预测和在线预测期间使用离线和在线存储。
- en: In this chapter, we will reuse the feature engineering pipeline and the model
    built in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065), *Adding Feature
    Stores to ML Models*, and [*Chapter 5*](B18024_05_ePub.xhtml#_idTextAnchor078),
    *Model Training and Inference*, to productionize the **machine learning** (**ML**)
    pipeline. The goal of this chapter is to reuse everything that we have built in
    the previous chapters, such as Feast infrastructure on AWS, feature engineering,
    model training, and model-scoring notebooks, to productionize the ML model. As
    we go through the exercises, it will give us an opportunity to look at how early
    adoption of Feast not only decoupled the ML pipeline stages but also accelerated
    the production readiness of the ML model. Once we productionize the batch and
    online ML pipelines, we will look at how the adoption of Feast opens up opportunities
    for other aspects of the ML life cycle, such as feature monitoring, automated
    model retraining, and also how it can accelerate the development of a future ML
    model. This chapter will help you understand how to productionize the batch and
    online models that use Feast, and how to use Feast for feature drift monitoring
    and model retraining.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重用 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065) *添加特征存储到机器学习模型*
    和 [*第 5 章*](B18024_05_ePub.xhtml#_idTextAnchor078) *模型训练和推理* 中构建的特征工程管道和模型，以投入生产机器学习（ML）管道。本章的目标是重用我们在前几章中构建的一切，例如
    AWS 上的 Feast 基础设施、特征工程、模型训练和模型评分笔记本，以投入生产 ML 模型。随着我们进行练习，这将给我们一个机会来了解 Feast 的早期采用不仅解耦了
    ML 管道阶段，还加速了 ML 模型的生产准备。一旦我们将批量和在线 ML 管道投入生产，我们将探讨 Feast 的采用如何为 ML 生命周期的其他方面开辟机会，例如特征监控、自动模型重新训练，以及它如何加速未来
    ML 模型的开发。本章将帮助您了解如何投入生产使用 Feast 的批量和在线模型，以及如何使用 Feast 进行特征漂移监控和模型重新训练。
- en: 'We will discuss the following topics in order:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按以下顺序讨论以下主题：
- en: Setting up Airflow for orchestration
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 Airflow 进行编排
- en: Productionizing a batch model pipeline
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将批量模型管道投入生产
- en: Productionizing an online model pipeline
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将在线模型管道投入生产
- en: Beyond model production
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越模型生产
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To follow the code examples in the chapter, the resources created in [*Chapter
    4*](B18024_04_ePub.xhtml#_idTextAnchor065), *Adding Feature Store to ML Models*,
    and [*Chapter 5*](B18024_05_ePub.xhtml#_idTextAnchor078), *Model Training and
    Inference*, are required. You will need familiarity with Docker and any notebook
    environment, which could be a local setup, such as Jupyter, or an online notebook
    environment, such as Google Colab, Kaggle, or SageMaker. You will also need an
    AWS account with full access to some of the resources, such as Redshift, S3, Glue,
    DynamoDB, and the IAM console. You can create a new account and use all the services
    for free during the trial period. You can find the code examples of the book and
    feature repository in the following GitHub links:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随章节中的代码示例，需要使用在 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065) *添加特征存储到机器学习模型*
    和 [*第 5 章*](B18024_05_ePub.xhtml#_idTextAnchor078) *模型训练和推理* 中创建的资源。您需要熟悉 Docker
    和任何笔记本环境，这可能是一个本地设置，例如 Jupyter，或者一个在线笔记本环境，例如 Google Colab、Kaggle 或 SageMaker。您还需要一个
    AWS 账户，可以完全访问一些资源，例如 Redshift、S3、Glue、DynamoDB 和 IAM 控制台。您可以在试用期间创建一个新账户并免费使用所有服务。您可以在以下
    GitHub 链接中找到本书的代码示例和特征存储库：
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter06](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter06)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter06](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter06)'
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation)'
- en: Setting up Airflow for orchestration
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Airflow 以进行编排
- en: To productionize the online and batch model, we need a workflow orchestration
    tool that can run the ML pipelines for us on schedule. There are a bunch of tools
    available, such as Apache Airflow, AWS Step Functions, and SageMaker Pipelines.
    You can also run it as GitHub workflows if you prefer. Depending on the tools
    you are familiar with or offered at your organization, orchestration may differ.
    For this exercise, we will use Amazon **Managed Workflows for Apache Airflow**
    (**MWAA**). As the name suggests, it is an Apache Airflow-managed service by AWS.
    Let's create an Amazon MWAA environment in AWS.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将在线和批量模型投入生产，我们需要一个工作流程编排工具，它可以按计划为我们运行 ML 管道。有多个工具可供选择，例如 Apache Airflow、AWS
    Step Functions 和 SageMaker Pipelines。如果您更喜欢，也可以将其作为 GitHub 工作流程运行。根据您熟悉或组织提供的工具，编排可能会有所不同。对于这个练习，我们将使用
    Amazon **Managed Workflows for Apache Airflow**（**MWAA**）。正如其名所示，这是 AWS 提供的由 Apache
    Airflow 管理的服务。让我们在 AWS 中创建一个 Amazon MWAA 环境。
- en: Important Note
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Amazon MWAA doesn''t have a free trial. You can view the pricing for the usage
    at this URL: [https://aws.amazon.com/managed-workflows-for-apache-airflow/pricing/](https://aws.amazon.com/managed-workflows-for-apache-airflow/pricing/).
    Alternatively, you can choose to run Airflow locally or on EC2 instances (EC2
    has free tier resources). You can find the setup instructions to run Airflow locally
    or on EC2 here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon MWAA 没有免费试用。您可以通过此网址查看使用价格：[https://aws.amazon.com/managed-workflows-for-apache-airflow/pricing/](https://aws.amazon.com/managed-workflows-for-apache-airflow/pricing/).
    或者，您可以选择在本地或 EC2 实例上运行 Airflow（EC2 提供免费层资源）。您可以在以下位置找到运行 Airflow 本地或 EC2 的设置说明：
- en: 'Airflow local setup: [https://towardsdatascience.com/getting-started-with-airflow-locally-and-remotely-d068df7fcb4](https://towardsdatascience.com/getting-started-with-airflow-locally-and-remotely-d068df7fcb4)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 本地设置：[https://towardsdatascience.com/getting-started-with-airflow-locally-and-remotely-d068df7fcb4](https://towardsdatascience.com/getting-started-with-airflow-locally-and-remotely-d068df7fcb4)
- en: 'Airflow on EC2: [https://christo-lagali.medium.com/getting-airflow-up-and-running-on-an-ec2-instance-ae4f3a69441](https://christo-lagali.medium.com/getting-airflow-up-and-running-on-an-ec2-instance-ae4f3a69441)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 在 EC2 上：[https://christo-lagali.medium.com/getting-airflow-up-and-running-on-an-ec2-instance-ae4f3a69441](https://christo-lagali.medium.com/getting-airflow-up-and-running-on-an-ec2-instance-ae4f3a69441)
- en: S3 bucket for Airflow metadata
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: S3 存储桶用于 Airflow 元数据
- en: Before we create an environment, we need an S3 bucket to store the Airflow dependencies,
    `airflow-for-ml-mar-2022`. In the S3 bucket, create a folder named `dags`. We
    will be using this folder to store all the Airflow DAGs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建环境之前，我们需要一个 S3 存储桶来存储 Airflow 依赖项，`airflow-for-ml-mar-2022`。在 S3 存储桶中，创建一个名为
    `dags` 的文件夹。我们将使用此文件夹来存储所有 Airflow DAG。
- en: 'The Amazon MWAA provides multiple different ways to configure additional plugins
    and Python dependencies to be installed in the Airflow environment. Since we need
    to install a few Python dependencies to run our project, we need to tell Airflow
    to install these required dependencies. One way of doing it is by using the `requirements.txt`
    file. The following code block shows the contents of the file:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon MWAA 提供了多种不同的方式来配置要安装到 Airflow 环境中的附加插件和 Python 依赖项。由于我们需要安装一些 Python
    依赖项来运行我们的项目，我们需要告诉 Airflow 安装这些必需的依赖项。一种方法是通过使用 `requirements.txt` 文件。以下代码块显示了文件的内容：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Save the contents of the preceding code block in a `requirements.txt` file.
    We will be using `papermill` ([https://papermill.readthedocs.io/en/latest/](https://papermill.readthedocs.io/en/latest/))
    to run the Python notebooks. You can also extract code and run the Python script
    using the `bash` or `python` operator available in Airflow.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将前面代码块的内容保存到 `requirements.txt` 文件中。我们将使用 `papermill` ([https://papermill.readthedocs.io/en/latest/](https://papermill.readthedocs.io/en/latest/))
    来运行 Python 笔记本。您还可以使用 Airflow 中可用的 `bash` 或 `python` 操作提取代码并运行 Python 脚本。
- en: Important Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you are running Airflow locally, make sure that the library versions are
    compatible with the Airflow version. The Amazon MWAA Airflow version at the time
    of writing is 2.2.2.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在本地运行 Airflow，请确保库版本与 Airflow 版本兼容。撰写本文时，Amazon MWAA 的 Airflow 版本是 2.2.2。
- en: Once you have the `requirement.txt` file created, upload it into the S3 bucket
    we have created. We will be using it in the next section during the environment
    creation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了`requirement.txt`文件，将其上传到我们创建的S3存储桶中。我们将在创建环境时下一节使用它。
- en: Amazon MWAA environment for orchestration
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon MWAA环境用于编排
- en: 'Now that we have the required resources for creating the Amazon MWAA environment,
    let''s follow the following steps to create the environment:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了创建Amazon MWAA环境所需的资源，让我们按照以下步骤创建环境：
- en: 'To create a new environment, log in to your AWS account and navigate to the
    Amazon MWAA console using the search bar in the AWS console. Alternatively, visit
    [https://us-east-1.console.aws.amazon.com/mwaa/home?region=us-east-1#environments](https://us-east-1.console.aws.amazon.com/mwaa/home?region=us-east-1#environments).
    The following web page will be displayed:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建一个新环境，登录到您的AWS账户，并使用AWS控制台中的搜索栏导航到Amazon MWAA控制台。或者，访问[https://us-east-1.console.aws.amazon.com/mwaa/home?region=us-east-1#environments](https://us-east-1.console.aws.amazon.com/mwaa/home?region=us-east-1#environments)。以下网页将显示：
- en: '![Figure 6.1 – The Amazon MWAA environments console'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 – Amazon MWAA环境控制台'
- en: '](img/B18024_06_001.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18024_06_001.jpg]'
- en: Figure 6.1 – The Amazon MWAA environments console
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – Amazon MWAA环境控制台
- en: 'On the page displayed in *Figure 6.1*, click on the **Create environment**
    button, and the following page will be displayed:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*图6.1*显示的页面上，点击**创建环境**按钮，随后将显示以下页面：
- en: '![Figure 6.2 – Amazon MWAA environment details'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2 – Amazon MWAA环境详情'
- en: '](img/B18024_06_002.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18024_06_002.jpg]'
- en: Figure 6.2 – Amazon MWAA environment details
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – Amazon MWAA环境详情
- en: 'Provide a name for the Amazon MWAA environment on the page displayed in *Figure
    6.2*. Scroll down to the **DAG code in Amazon S3** section; you should see the
    following parameters on the screen:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*图6.2*显示的页面上为Amazon MWAA环境提供一个名称。向下滚动到**Amazon S3中的DAG代码**部分；你应该在屏幕上看到以下参数：
- en: '![Figure 6.3 – Amazon MWAA – the DAG code in S3 section'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3 – Amazon MWAA – S3中的DAG代码部分'
- en: '](img/B18024_06_003.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18024_06_003.jpg]'
- en: Figure 6.3 – Amazon MWAA – the DAG code in S3 section
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – Amazon MWAA – S3中的DAG代码部分
- en: 'On the screen displayed in *Figure 6.3*, enter the S3 bucket in the textbox
    or use the `requirements.txt` file that we uploaded or enter the path to the file.
    As we don''t need any plugins to run the project, we can leave the optional **Plugins
    file** field blank. Click on the **Next** button:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*图6.3*显示的屏幕上，在文本框中输入S3存储桶或使用我们上传的`requirements.txt`文件，或输入文件的路径。由于我们不需要任何插件来运行项目，我们可以将可选的**插件文件**字段留空。点击**下一步**按钮：
- en: '![Figure 6.4 – Amazon MWAA advanced settings'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.4 – Amazon MWAA高级设置'
- en: '](img/B18024_06_004.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18024_06_004.jpg]'
- en: Figure 6.4 – Amazon MWAA advanced settings
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – Amazon MWAA高级设置
- en: The next page displayed is shown in *Figure 6.4*. For **Virtual private cloud
    (VPC)**, select the available default VPC from the dropdown. One caveat here is
    that the selected VPC should have at least two private subnets. If it doesn't
    have the private subnets, when you try to select **Subnet 1** and **Subnet 2**,
    you will notice that all the options are grayed out. If you run into this scenario,
    click on **Create MWAA VPC**. It will take you to the CloudFormation console;
    once you have filled in the form with all the parameters, follow through and click
    on **Create stack**. It will create a VPC that can be used by Amazon MWAA. Once
    the VPC is created, come back to this window and select the new VPC and subnets,
    and continue.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个显示的页面如图6.4所示。对于**虚拟专用云(VPC**)，从下拉列表中选择可用的默认VPC。这里有一个注意事项，所选VPC应至少有两个私有子网。如果没有私有子网，当你尝试选择**子网1**和**子网2**时，你会注意到所有选项都变灰了。如果你遇到这种情况，点击**创建MWAA
    VPC**。它将带你进入CloudFormation控制台；一旦你填写了所有参数的表格，继续操作并点击**创建堆栈**。它将创建一个Amazon MWAA可以使用的VPC。一旦VPC创建完成，返回此窗口并选择新的VPC和子网，然后继续。
- en: After selecting the VPC, for **Web server access**, select **Public network**;
    leave everything else to default, and scroll all the way down. In the **Permissions**
    section, you will notice that it says it will create a new role for Amazon MWAA.
    Make a note of the role name. We will have to add permissions to this role later.
    After that, click on **Next**.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择VPC后，对于**Web服务器访问**，选择**公共网络**；将其他所有选项保留为默认设置，并将滚动条拉至最底部。在**权限**部分，你会注意到它将创建一个新的角色用于Amazon
    MWAA。记下角色名称。我们稍后需要向此角色添加权限。之后，点击**下一步**。
- en: On the next page, review all the input provided, scroll all the way down, and
    click on **Create environment**. It will take a few minutes to create the environment.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一页，查看所有提供的输入，滚动到最底部，然后点击**创建环境**。创建环境可能需要几分钟。
- en: Once the environment is created, you should be able to see the environment in
    the **Available** state on the Amazon MWAA environments page. Pick the environment
    that we just created and click on the **Open Airflow UI** link. An Airflow home
    page will be displayed, similar to the one in the following figure:![Figure 6.5
    – The Airflow UI
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境创建完成后，你应该能够在Amazon MWAA环境页面上看到处于**可用**状态的环境。选择我们刚刚创建的环境，然后点击**打开Airflow UI**链接。将显示一个Airflow主页，类似于以下图所示：![图6.5
    – Airflow UI
- en: '](img/B18024_06_005.jpg)'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B18024_06_005.jpg)'
- en: Figure 6.5 – The Airflow UI
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – Airflow UI
- en: 'To test whether everything is working fine, let''s quickly create a simple
    DAG and look at how it works. The following code block creates a simple DAG with
    a dummy operator and a Python operator:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试一切是否运行正常，让我们快速创建一个简单的DAG并查看其工作方式。以下代码块创建了一个简单的DAG，包含一个虚拟操作符和一个Python操作符：
- en: '[PRE5]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The DAG defined in the preceding code is pretty simple; it has two tasks – `start`
    and `hello_operator`. The `start` task is a `DummyOperator`, does nothing, and
    is used for making the DAG look pretty on the UI. The `hello_operator` task just
    invokes a function that returns a message. In the last line, we define a dependency
    between the operators.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的DAG相当简单；它有两个任务 – `start` 和 `hello_operator`。`start` 任务是一个 `DummyOperator`，什么都不做，用于使DAG在UI上看起来更美观。`hello_operator`
    任务只是调用一个返回消息的函数。在最后一行，我们定义了操作符之间的依赖关系。
- en: 'Copy the preceding code block, save the file as `example_dag.py`, and upload
    it to the `dags` folder in S3 that we created earlier. (My S3 location is `s3://airflow-for-ml-mar-2022/dags`.)
    Once you upload it, it should appear in the Airflow UI within seconds. The following
    figure displays the Airflow UI with the DAG:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制前面的代码块，将其保存为`example_dag.py`，并将其上传到我们之前创建的S3中的`dags`文件夹。（我的S3位置是`s3://airflow-for-ml-mar-2022/dags`。）上传后，它应该在几秒钟内出现在Airflow
    UI中。以下图显示了带有DAG的Airflow UI：
- en: '![Figure 6.6 – The Airflow UI with the example DAG'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 – 带示例DAG的Airflow UI'
- en: '](img/B18024_06_006.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_006.jpg)'
- en: Figure 6.6 – The Airflow UI with the example DAG
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 带示例DAG的Airflow UI
- en: By default, the DAGs are disabled; hence, when you visit the page, you may not
    see the exact page such as the one displayed in *Figure 6.6*. Enable the DAG by
    clicking on the toggle button in the left-most column. Once enabled, DAG will
    run for the first time and update the run results. You can also trigger the DAG
    using the icon in the **Links** column. Click on the **hello_world** hyperlink
    in the DAG column in the UI. You will see the details page of the DAG with different
    tabs. Feel free to play around and look at the different options available on
    the details page.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，DAG是禁用的；因此，当你访问页面时，你可能看不到像*图6.6*中显示的确切页面。通过点击最左侧列中的切换按钮启用DAG。一旦启用，DAG将首次运行并更新运行结果。你还可以使用**链接**列中的图标触发DAG。在UI的DAG列中点击**hello_world**超链接。你将看到DAG的不同选项卡详情页面。请随意尝试并查看详情页面上的不同选项。
- en: 'The following figure displays the graph view of the DAG:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下图显示了DAG的图形视图：
- en: '![Figure 6.7 – The graph view of the DAG'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7 – DAG的图形视图'
- en: '](img/B18024_06_007.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_007.jpg)'
- en: Figure 6.7 – The graph view of the DAG
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – DAG的图形视图
- en: Now that we have verified that Airflow is set up correctly, let's add the required
    permissions for Airflow to run the ML pipeline.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经验证了Airflow设置正确，让我们为Airflow运行ML管道添加所需的权限。
- en: 'If you recall, during the last step of environment creation (the paragraph
    following *Figure 6.4*), we made note of the role name the Airflow environment
    is using to run the DAGs. Now, we need to add permissions to the role. To do so,
    navigate to the AWS IAM roles console page using the search function or visit
    https://us-east-1.console.aws.amazon.com/iamv2/home?region=us-east-1#/roles. In
    the console, you should see the IAM role that is associated with the Airflow environment.
    Select the IAM role; you should see the following page:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还记得，在环境创建的最后一步（*图6.4*之后的段落），我们记录了Airflow环境运行DAG时使用的角色名称。现在，我们需要向该角色添加权限。为此，使用搜索功能导航到AWS
    IAM角色控制台页面或访问https://us-east-1.console.aws.amazon.com/iamv2/home?region=us-east-1#/roles。在控制台中，你应该看到与Airflow环境关联的IAM角色。选择IAM角色；你应该会看到以下页面：
- en: '![Figure 6.8 – The Amazon MWAA IAM role'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.8 – Amazon MWAA IAM角色'
- en: '](img/B18024_06_008.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_06_008.jpg](img/B18024_06_008.jpg)'
- en: Figure 6.8 – The Amazon MWAA IAM role
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 亚马逊MWAA IAM角色
- en: Important Note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you didn't make a note, you can find the role name in the environment details
    page on the AWS console.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有做笔记，你可以在AWS控制台的“环境详情”页面上找到角色名称。
- en: 'In *Figure 6.8*, click on **Add permissions**; from the dropdown, select **Attach
    policies**, and you will be taken to the following page:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*图6.8*中，点击**添加权限**；从下拉菜单中选择**添加策略**，你将被带到以下页面：
- en: '![Figure 6.9 – IAM – Attach policies'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.9 – IAM – 添加策略'
- en: '](img/B18024_06_009.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_06_009.jpg](img/B18024_06_009.jpg)'
- en: Figure 6.9 – IAM – Attach policies
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – IAM – 添加策略
- en: On the web page, search and select the following policies – **AmazonS3FullAccess**,
    **AWSGlueConsoleFullAccess**, **AmazonRedshiftFullAccess**, and **AmazonDynamoDBFullAccess**.
    Once the policies are selected, scroll down and click on **Attach policies** to
    save the role with the new policies.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网页上，搜索并选择以下策略 – **AmazonS3FullAccess**、**AWSGlueConsoleFullAccess**、**AmazonRedshiftFullAccess**和**AmazonDynamoDBFullAccess**。一旦选择了策略，向下滚动并点击**添加策略**以保存带有新策略的角色。
- en: Important Note
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is never a good idea to assign full access to any of the resources without
    restrictions. When you run an enterprise application, it is recommended to restrict
    access based on the resources, such as read-only access to a specific S3 bucket
    and DynamoDB tables.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 没有任何限制地分配任何资源的完全访问权限从来都不是一个好主意。当你运行企业应用时，建议根据资源限制访问，例如只读访问特定的S3桶和DynamoDB表。
- en: If you are running Airflow locally, you can use the IAM user credential in the
    notebook.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你是在本地运行Airflow，你可以在笔记本中使用IAM用户凭证。
- en: Now that our orchestration system is ready, let's look at how to use it to productionize
    the ML pipeline.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了编排系统，让我们看看如何使用它来将机器学习流水线投入生产。
- en: Productionizing the batch model pipeline
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量模型流水线的投入生产
- en: In [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065), *Adding Feature Store
    to ML Models*, for model training, we used the features ingested by the feature
    engineering notebook. We also created a model-scoring notebook that fetches features
    for a set of customers from Feast and runs predictions for it using the trained
    model. For the sake of the experiment, let's assume that the raw data freshness
    latency is a day. That means the features need to be regenerated once a day, and
    the model needs to score customers against those features once a day and store
    the results in an S3 bucket for consumption. To achieve this, thanks to our early
    organization and decoupling of stages, all we need to do is run the feature engineering
    and model scoring notebook/Python script once a day consecutively. Now that we
    also have a tool to perform this, let's go ahead and schedule this workflow in
    the Airflow environment.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B18024_04_ePub.xhtml#_idTextAnchor065)，“将特征存储添加到机器学习模型”中，为了模型训练，我们使用了特征工程笔记本中摄取的特征。我们还创建了一个模型评分笔记本，它从Feast中获取一组客户的特征，并使用训练好的模型对其进行预测。为了实验的目的，让我们假设原始数据的新鲜度延迟为一天。这意味着特征需要每天重新生成一次，模型需要每天对客户进行评分，并将结果存储在S3桶中以供消费。为了实现这一点，多亏了我们早期的组织和阶段解耦，我们只需要每天连续运行特征工程和模型评分笔记本/Python脚本。现在我们也有了一个执行这个任务的工具，让我们继续在Airflow环境中安排这个工作流程。
- en: 'The following figure displays how we will be operationalizing the batch model:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了我们将如何运营化批量模型：
- en: '![Figure 6.10 – Operationalization of the batch model'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.10 – 批量模型的运营化'
- en: '](img/B18024_06_010.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_06_010.jpg](img/B18024_06_010.jpg)'
- en: Figure 6.10 – Operationalization of the batch model
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 批量模型的运营化
- en: As you can see in the figure, to operationalize the workflow, we will use Airflow
    to orchestrate the feature-engineering and model-scoring notebooks. The raw data
    source for feature engineering, in our case, is the S3 bucket where `online-retail.csv`
    is stored. As we have already designed our scoring notebook to load the production
    model from the model repo (in our case, an S3 bucket) and store the prediction
    results in a S3 bucket, we will reuse the same notebook. One thing you might notice
    here is that we are not using the model-training notebook for every run; the reason
    is obvious – we want to run predictions against a version of the model that has
    been validated, tested, and also met our performance criteria on the test data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在图中所见，为了使工作流程投入运行，我们将使用 Airflow 来编排特征工程和模型评分笔记本。在我们的案例中，特征工程的原始数据源是存储 `online-retail.csv`
    的 S3 存储桶。由于我们已设计好评分笔记本从模型仓库（在我们的案例中是一个 S3 存储桶）加载生产模型并将预测结果存储在 S3 存储桶中，我们将重用相同的笔记本。你可能在这里注意到的一点是我们不是每次运行都使用模型训练笔记本；原因很明显
    – 我们希望针对经过验证、测试并且也在测试数据上满足我们性能标准的模型版本进行预测。
- en: 'Before scheduling this workflow, I have done minor changes to the feature-engineering
    notebook and model-prediction notebooks. The final notebooks can be found at the
    following GitHub URL: [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/(ch6_feature_engineering.ipynb,ch6_model_prediction.ipynb)](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/(ch6_feature_engineering.ipynb,ch6_model_prediction.ipynb)).
    To schedule the workflow, download the final notebooks from GitHub and upload
    them to an S3 bucket that we created earlier, as an Airflow environment will need
    to access these notebooks during runs. I will upload it to the following location:
    `s3://airflow-for-ml-mar-2022/notebooks/`.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在安排此工作流程之前，我对特征工程笔记本和模型预测笔记本进行了少量修改。最终的笔记本可以在以下 GitHub URL 中找到：[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/(ch6_feature_engineering.ipynb,ch6_model_prediction.ipynb)](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/(ch6_feature_engineering.ipynb,ch6_model_prediction.ipynb))。要安排工作流程，请从
    GitHub 下载最终的笔记本并将它们上传到我们之前创建的 S3 存储桶，因为 Airflow 环境在运行期间需要访问这些笔记本。我将将其上传到以下位置：`s3://airflow-for-ml-mar-2022/notebooks/`。
- en: Important Note
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: AWS secret access key and an S3 path – I have commented out AWS credentials
    in both the notebooks, as we are adding permissions to an Amazon MWAA IAM role.
    If you are running it in local Airflow, please uncomment and add secrets. Also,
    update the S3 URLs wherever necessary, as the S3 URLs point to the private buckets
    that I have created during the exercises.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 密钥访问密钥和 S3 路径 – 我在两个笔记本中都注释掉了 AWS 凭据，因为我们正在向 Amazon MWAA IAM 角色添加权限。如果你在本地
    Airflow 中运行它，请取消注释并添加密钥。同时，在必要时更新 S3 URL，因为 S3 URL 指向我在练习期间创建的私有存储桶。
- en: Feature repo – As we have seen before, we must clone the feature repo so that
    the Feast library can read the metadata. You can follow the same `git clone` (provided
    that `git` is installed) approach or set up a GitHub workflow to push the repo
    to S3 and download the same in the notebook. I have left both code blocks in the
    notebook with comments. You can use whichever is convenient.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 功能仓库 – 如我们之前所见，我们必须克隆功能仓库，以便 Feast 库可以读取元数据。你可以遵循相同的 `git clone` 方法（前提是已安装 `git`）或者设置
    GitHub 工作流程将仓库推送到 S3 并在笔记本中下载相同的文件。我已经在笔记本中留下了两个代码块并附有注释。你可以使用任何方便的方法。
- en: 'S3 approach – To use an S3 download approach, clone the repo in your local
    system and run the following commands in the Linux terminal to upload it to a
    specific S3 location:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: S3 方法 – 要使用 S3 下载方法，在你的本地系统中克隆仓库，然后在 Linux 终端中运行以下命令将其上传到特定的 S3 位置：
- en: '**export AWS_ACCESS_KEY_ID=<aws_key>**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**export AWS_ACCESS_KEY_ID=<aws_key>**'
- en: '**export AWS_SECRET_ACCESS_KEY=<aws_secret>**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**export AWS_SECRET_ACCESS_KEY=<aws_secret>**'
- en: '**AWS_DEFAULT_REGION=us-east-1**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS_DEFAULT_REGION=us-east-1**'
- en: '**aws s3 cp customer_segmentation s3://<s3_bucket>/customer_segmentation --recursive**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**aws s3 cp customer_segmentation s3://<s3_bucket>/customer_segmentation --recursive**'
- en: On successful upload, you should be able to see the folder contents in the S3
    bucket.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 上传成功后，你应该能够在 S3 存储桶中看到文件夹内容。
- en: Now that the notebooks are ready, let's write the Airflow DAG for the batch
    model pipeline. The DAG will have the following tasks in order – `start` (`dummy
    operator`), `feature_engineering` (`Papermill operator`), `model_prediction` (`Papermill
    operator`), and `end` (`dummy operator`).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在笔记本已经准备好了，让我们编写批量模型管道的 Airflow DAG。DAG 将按以下顺序包含以下任务 – `start`（虚拟操作符）、`feature_engineering`（Papermill
    操作符）、`model_prediction`（Papermill 操作符）和 `end`（虚拟操作符）。
- en: 'The following code block contains the first part of the Airflow DAG:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含 Airflow DAG 的第一部分：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code block, we have defined the imports and DAG parameters
    such as `name`, `schedule_interval`, and `start_date`. The `schedule_interval='@daily'`
    schedule says that the DAG should run daily.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们定义了导入和 DAG 参数，如 `name`、`schedule_interval` 和 `start_date`。`schedule_interval='@daily'`
    调度表示 DAG 应该每天运行。
- en: 'The following code block defines the rest of the DAG (the second part), which
    contains all the tasks and the dependencies among them:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块定义了 DAG 的其余部分（第二部分），其中包含所有任务及其之间的依赖关系：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you can see in the code block, there are four steps that will execute one
    after the other. The `feature_engineering` and `model_prediction` steps are run
    using `PapermillOperator`. This takes the path to the S3 notebook as input. I
    have also set an output path to another S3 location so that we can check the output
    notebook of each run. The last line defines the dependency between the tasks.
    Save the preceding two code blocks (the first and second parts) as a Python file
    and call it `batch-model-pipeline-dag.py`. After saving the file, navigate to
    the S3 console to upload the file into the `dags` folder that we pointed our Airflow
    environment to in *Figure 6.3*. The uploaded file is processed by the Airflow
    scheduler. When you navigate to the Airflow UI, you should see the new DAG called
    **customer_segmentation_batch_model** on the screen.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在代码块中看到的，这里有四个步骤将依次执行。`feature_engineering` 和 `model_prediction` 步骤使用 `PapermillOperator`
    运行。这需要输入 S3 笔记本路径。我还设置了一个输出路径到另一个 S3 位置，这样我们就可以检查每次运行的输出笔记本。最后一行定义了任务之间的依赖关系。将前两个代码块（第一部分和第二部分）保存为
    Python 文件，并将其命名为 `batch-model-pipeline-dag.py`。保存文件后，导航到 S3 控制台，将文件上传到我们在 *图 6.3*
    中指向的 Airflow 环境的 `dags` 文件夹。上传的文件将由 Airflow 调度器处理。当您导航到 Airflow UI 时，您应该在屏幕上看到名为
    **customer_segmentation_batch_model** 的新 DAG。
- en: 'The following figure displays the Airflow UI with the DAG:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了包含 DAG 的 Airflow UI：
- en: '![Figure 6.11 – The batch model DAG on Airflow'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.11 – Airflow 上的批量模型 DAG'
- en: '](img/B18024_06_011.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_011.jpg)'
- en: Figure 6.11 – The batch model DAG on Airflow
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – Airflow 上的批量模型 DAG
- en: 'As we have not enabled the DAG by default option during the Airflow environment
    creation (which can be set in Airflow configuration variables in Amazon MWAA),
    when the DAG appears on the UI for the first time, it will be disabled. Click
    on the toggle button on the left-most column to enable it. Once enabled, the DAG
    will run for the first time. Click on the **customer_segmentation_batch_model**
    hyperlink to navigate to the details page, and feel free to look around to see
    the different visualization and properties of the DAG. If you navigate to the
    **Graph** tab, the DAG will be displayed, as shown in the following screenshot:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在创建 Airflow 环境时没有默认启用 DAG 选项（这可以在 Amazon MWAA 中的 Airflow 配置变量中设置），当 DAG
    首次出现在 UI 上时，它将处于禁用状态。点击最左侧列的切换按钮来启用它。一旦启用，DAG 将首次运行。点击 **customer_segmentation_batch_model**
    超链接导航到详情页面，并随意查看 DAG 的不同可视化和属性。如果您导航到 **Graph** 选项卡，DAG 将如以下截图所示显示：
- en: '![Figure 6.12 – The batch model DAG graph view'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.12 – 批量模型 DAG 图形视图'
- en: '](img/B18024_06_012.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_012.jpg)'
- en: Figure 6.12 – The batch model DAG graph view
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – 批量模型 DAG 图形视图
- en: In *Figure 6.12*, you can see the graph view of the DAG. If there were any failures
    in the last run, they will appear in red outline. You can also view the logs of
    successful execution or failures for each of the tasks. As all the tasks are green,
    this means everything went well. You can also see the results of the last few
    runs in *Figure 6.11*. Airflow also provides you with the history of all the runs.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 6.12* 中，您可以查看 DAG 的图形视图。如果上次运行有任何失败，它们将以红色轮廓显示。您还可以查看每个任务的执行日志或失败记录。由于所有任务都是绿色的，这意味着一切顺利。您还可以在
    *图 6.11* 中查看最近几次运行的成果。Airflow 还为您提供所有运行的记录。
- en: Now that the task run is complete, we can go and check the output notebook,
    the S3 bucket for the new set of features, or the S3 bucket for the new set of
    predictions. All three should be available after successful runs. Here, we will
    be verifying just the prediction results folder, but feel free to verify the others
    as well.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在任务运行已完成，我们可以去检查输出笔记本、新特征集的S3桶或新预测集的S3桶。所有这三个在成功运行后都应可用。在这里，我们将验证预测结果文件夹，但也请随意验证其他文件夹。
- en: Important Note
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In case of any failures, verify the logs for the failed tasks (click on the
    failed task in graph view to see available information). Check the permissions
    for Amazon MWAA, the S3 paths for input/output, and also whether all the requirements
    are installed in the Amazon MWAA environment.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有任何失败，请验证失败任务的日志（在图形视图中点击失败任务以查看可用信息）。检查Amazon MWAA的权限、输入/输出的S3路径，以及是否在Amazon
    MWAA环境中安装了所有要求。
- en: 'The following screenshot shows the new prediction results in an S3 bucket:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了S3桶中的新预测结果：
- en: '![Figure 6.13 – The prediction results in an S3 bucket'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.13 – The prediction results in an S3 bucket]'
- en: '](img/B18024_06_013.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_06_013.jpg]'
- en: Figure 6.13 – The prediction results in an S3 bucket
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – S3桶中的预测结果
- en: 'In addition, you can also do all kinds of fancy things with Airflow, such as
    sending email notifications for failure, Slack notifications for daily runs, and
    integration with PagerDuty. Feel free to explore the options. Here is a list of
    supported providers in Airflow: [https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html](https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以使用Airflow做各种花哨的事情，例如发送失败时的电子邮件通知、日常运行的Slack通知，以及与PagerDuty的集成。请随意探索选项。以下是Airflow支持的服务提供者列表：[https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html](https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html)。
- en: Now that our batch model is running in production, let's look at how to productionize
    the online model with Feast.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们批处理模型已在生产环境中运行，让我们看看如何使用Feast将在线模型进行生产化。
- en: Productionizing an online model pipeline
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产化在线模型管道
- en: 'In the previous chapter, for the online model, we built REST endpoints to serve
    on-demand predictions for customer segmentation. Though the online model is hosted
    as a REST endpoint, it needs a supporting infrastructure for the following functions:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，对于在线模型，我们构建了REST端点以提供客户细分的需求预测。尽管在线模型以REST端点托管，但它需要以下功能的支持基础设施：
- en: To serve features in real time (we have Feast for that)
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了实时提供特征（我们为此有Feast）
- en: To keep features up to date (we will use the feature-engineering notebook with
    Airflow orchestration for this)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了保持特征更新（我们将使用带有Airflow编排的特征工程笔记本来完成此操作）
- en: In this chapter, we will continue from where we left off and use the feature-engineering
    notebook built in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065), *Adding
    Feature Store to ML Models,* in combination with a notebook to synchronize offline
    data to an online store in Feast.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续上一章的内容，并使用在[*第4章*](B18024_04_ePub.xhtml#_idTextAnchor065)“将特征存储添加到机器学习模型”中构建的特征工程笔记本，结合一个笔记本将离线数据同步到Feast的在线存储。
- en: 'The following figure shows the operationalization of the online model pipeline:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了在线模型管道的运营化：
- en: '![Figure 6.14 – The operationalization of the online model'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.14 – The operationalization of the online model]'
- en: '](img/B18024_06_014.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_06_014.jpg]'
- en: Figure 6.14 – The operationalization of the online model
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 在线模型的运营化
- en: 'As you can see in *Figure 6.14*, we will use Airflow for the orchestration
    of feature engineering; data freshness is still one day here, and scheduling can
    be done for a shorter duration. Feast can also support streaming data if there
    is a need. The following URL has an example you can use: [https://docs.Feast.dev/reference/data-sources/push](https://docs.Feast.dev/reference/data-sources/push).
    The REST endpoints developed in [*Chapter 5*](B18024_05_ePub.xhtml#_idTextAnchor078),
    *Model Training and Inference*, will be Dockerized and deployed as a SageMaker
    endpoint.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*图6.14*]所示，我们将使用Airflow进行特征工程的编排；数据新鲜度仍然是每天一次，并且可以安排更短的时间。如果需要，Feast还可以支持流数据。以下URL有一个可用的示例：[https://docs.Feast.dev/reference/data-sources/push](https://docs.Feast.dev/reference/data-sources/push)。在[*第5章*](B18024_05_ePub.xhtml#_idTextAnchor078)“模型训练和推理”中开发的REST端点将被Docker化并作为SageMaker端点部署。
- en: Important Note
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Once Dockerized, the Docker image can be used to deploy into any containerized
    environment, such as Elastic Container Service, Elastic BeanStalk, and Kubernetes.
    We are using SageMaker, as it takes less time to set up and also has advantages
    such as data capture and IAM authentication that come out of the box.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦容器化，Docker 镜像就可以用于部署到任何容器化环境中，例如 Elastic Container Service，Elastic BeanStalk
    和 Kubernetes。我们使用 SageMaker，因为它设置起来更快，并且还具有开箱即用的优势，如数据捕获和 IAM 认证。
- en: Orchestration of a feature engineering job
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程作业的编排
- en: 'As we already have two notebooks (feature engineering and sync offline to online
    store) and we are familiar with Airflow, let''s schedule the feature engineering
    workflow first. Again, in the notebook, I have done some minor changes. Please
    verify the changes before using it. You can find the notebooks (`ch6_feature_engineering.ipynb`
    a[nd `ch6_sync_offline_online.ipynb`) here: https://github.com/PacktPublishing/Feature-Store-for-Machin](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter06/notebooks)e-Learning/tree/main/Chapter06/notebooks.
    Just the way we did it for the batch model, download the notebooks and upload
    them to a specific S3 location. I will be uploading them to the same location
    as before: `s3://airflow-for-ml-mar-2022/notebooks/`. Now that the notebooks are
    ready, let''s write the Airflow DAG for the online model pipeline. The DAG will
    have the following steps in order – `start` (`dummy operator`), `feature_engineering`
    (`Papermill operator`), `sync_offline_to_online` (`Papermill operator`), and `end`
    (`dummy operator`).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经有两个笔记本（特征工程和同步离线到在线存储）并且我们对 Airflow 很熟悉，让我们首先安排特征工程工作流程。同样，在笔记本中，我进行了一些小的修改。请在使用之前验证这些修改。您可以在以下位置找到笔记本（`ch6_feature_engineering.ipynb`
    和 `ch6_sync_offline_online.ipynb`）：https://github.com/PacktPublishing/Feature-Store-for-Machin[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter06/notebooks)e-Learning/tree/main/Chapter06/notebooks。就像我们对批量模型所做的那样，下载笔记本并将它们上传到特定的
    S3 位置。我将会上传到之前相同的位置：`s3://airflow-for-ml-mar-2022/notebooks/`。现在笔记本已经准备好了，让我们编写在线模型管道的
    Airflow DAG。DAG 将按照以下顺序执行步骤 – `start`（虚拟操作符）, `feature_engineering`（Papermill
    操作符）, `sync_offline_to_online`（Papermill 操作符）, 和 `end`（虚拟操作符）。
- en: 'The following code block contains the first part of the Airflow DAG:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块包含 Airflow DAG 的第一部分：
- en: '[PRE34]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Just like in the case of the batch model pipeline DAG, this contains the DAG
    parameters.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 就像批量模型管道 DAG 的情况一样，这包含 DAG 参数。
- en: 'The following code block defines the rest of the DAG (the second part), which
    contains all the tasks and the dependencies among them:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块定义了 DAG 的其余部分（第二部分），其中包含所有任务及其之间的依赖关系：
- en: '[PRE42]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The structure of the Airflow DAG is similar to the batch model DAG we looked
    at earlier; the only difference is the third task, `sync_offline_to_online`. This
    notebook syncs the latest features from offline data to online data. Save the
    preceding two code blocks (the first and second parts) as a Python file and call
    it `online-model-pipeline-dag.py`. After saving the file, navigate to the S3 console
    to upload the file into the `dags` folder that we pointed our Airflow environment
    to in *Figure 6.3*. As with the batch model, the uploaded file is processed by
    the Airflow scheduler, and when you navigate to the Airflow UI, you should see
    the new DAG called **customer_segmentation_online_model** on the screen.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow DAG 的结构与我们之前看到的批量模型 DAG 类似；唯一的区别是第三个任务，`sync_offline_to_online`。这个笔记本将离线数据中的最新特征同步到在线数据中。将前两个代码块（第一部分和第二部分）保存为
    Python 文件，并将其命名为 `online-model-pipeline-dag.py`。保存文件后，导航到 S3 控制台，将文件上传到我们在 *图
    6.3* 中指向的 Airflow 环境的 `dags` 文件夹。与批量模型一样，上传的文件将由 Airflow 调度器处理，当你导航到 Airflow UI
    时，你应该在屏幕上看到名为 **customer_segmentation_online_model** 的新 DAG。
- en: 'The following screenshot displays the Airflow UI with the DAG:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了带有 DAG 的 Airflow UI：
- en: '![Figure 6.15 – The Airflow UI with both the online and batch models'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.15 – 带有在线和批量模型的 Airflow UI'
- en: '](img/B18024_06_015.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18024_06_015.jpg)'
- en: Figure 6.15 – The Airflow UI with both the online and batch models
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 带有在线和批量模型的 Airflow UI
- en: 'To enable the DAG, click on the toggle button on the left-most column. Once
    enabled, the DAG will run for the first time. Click on the **customer_segmentation_online_model**
    hyperlink to navigate to the details page, and feel free to look around to see
    the different visualization and properties of the DAG. If you navigate to the
    **Graph** tab, the DAG will be displayed, as shown in the following screenshot:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用 DAG，点击最左侧列的切换按钮。一旦启用，DAG 将首次运行。点击**customer_segmentation_online_model**超链接导航到详细信息页面，您可以随意浏览以查看
    DAG 的不同可视化和属性。如果您导航到**图形**选项卡，DAG 将会显示，如下面的屏幕截图所示：
- en: '![Figure 6.16 – The online model pipeline graph view'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.16 – 在线模型管道图形视图'
- en: '](img/B18024_06_016.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_06_016.jpg](img/B18024_06_016.jpg)'
- en: Figure 6.16 – The online model pipeline graph view
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 6.16 – 在线模型管道图形视图
- en: As you can see in *Figure 6.16*, on successful runs, the graph will be green.
    As discussed during the batch model pipeline execution, you can verify the output
    notebook, DynamoDB tables, or S3 bucket to make sure that everything is working
    fine and can also check logs in case of failures.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在*图 6.16*中看到的，在成功运行的情况下，图形将显示为绿色。正如在批量模型管道执行期间所讨论的，您可以验证输出笔记本、DynamoDB 表或
    S3 存储桶，以确保一切正常工作，并在出现故障的情况下检查日志。
- en: Now that the first part of the online model pipeline is ready, let's Dockerize
    the REST endpoints we developed in the previous chapter and deploy them as a SageMaker
    endpoint.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在线模型管道的第一部分已经准备好了，让我们将上一章中开发的 REST 端点 Docker 化，并将它们作为 SageMaker 端点部署。
- en: Deploying the model as a SageMaker endpoint
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型部署为 SageMaker 端点
- en: To deploy the model to SageMaker, we need to first dockerize the REST API we
    built in [*Chapter 5*](B18024_05_ePub.xhtml#_idTextAnchor078), *Model Training
    and Inference*. Before we do that, let's create an **Elastic Container Registry**
    (**ECR**), where we can save the Docker image of the model and use it in SageMaker
    endpoint configurations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 要将模型部署到 SageMaker，我们首先需要将我们在[*第 5 章*](B18024_05_ePub.xhtml#_idTextAnchor078)“模型训练和推理”中构建的
    REST API Docker 化。在我们这样做之前，让我们创建一个**弹性容器注册库**（**ECR**），在那里我们可以保存模型的 Docker 镜像，并在
    SageMaker 端点配置中使用它。
- en: An ECR for the Docker image
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker 镜像的 ECR
- en: 'To create the ECR resource, navigate to the ECR console from the search bar
    or use the following URL: [https://us-east-1.console.aws.amazon.com/ecr/repositories?region=us-east-1](https://us-east-1.console.aws.amazon.com/ecr/repositories?region=us-east-1).
    The following page will be displayed:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建 ECR 资源，从搜索栏导航到 ECR 控制台或使用以下 URL：[https://us-east-1.console.aws.amazon.com/ecr/repositories?region=us-east-1](https://us-east-1.console.aws.amazon.com/ecr/repositories?region=us-east-1)。将显示以下页面：
- en: '![Figure 6.17 – The ECR home page'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.17 – ECR 主页'
- en: '](img/B18024_06_017.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_06_017.jpg](img/B18024_06_017.jpg)'
- en: Figure 6.17 – The ECR home page
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 6.17 – ECR 主页
- en: 'On the page displayed in *Figure 6.17*, you can choose either the **Private**
    or **Public** repository tab. Then, click on the **Create repository** button:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.17*显示的页面上，您可以选择**私有**或**公共**存储库选项卡。然后，点击**创建存储库**按钮：
- en: '![Figure 6.18 – ECR – Create repository'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.18 – ECR – 创建存储库'
- en: '](img/B18024_06_018.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_06_018.jpg](img/B18024_06_018.jpg)'
- en: Figure 6.18 – ECR – Create repository
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 6.18 – ECR – 创建存储库
- en: I have selected **Private** here; depending on whether you choose **Private**
    or **Public**, the options will change, but either way, it's straightforward.
    Fill in the required fields, scroll all the way down, and click on **Create repository**.
    Once the repository is created, go into the repository details page, and you should
    see a page similar to the one shown in *Figure 6.19*.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里选择了**私有**；根据您选择的是**私有**还是**公共**，选项将会有所不同，但无论如何，操作都很简单。填写所需的字段，滚动到页面底部，然后点击**创建存储库**。一旦创建存储库，进入存储库详细信息页面，您应该会看到一个类似于*图
    6.19*所示的页面。
- en: Important Note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Private repositories are secured with IAM, whereas public repositories can
    be accessed by anybody on the internet. Public repositories are mainly used for
    sharing/open sourcing your work with others outside an organization:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 私有存储库通过 IAM 进行保护，而公共存储库则可以被互联网上的任何人访问。公共存储库主要用于与组织外的人共享/开源您的工作：
- en: '![ Figure 6.19 – ECR repository details'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '![ Figure 6.19 – ECR 存储库详细信息'
- en: '](img/B18024_06_019.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_06_019.jpg](img/B18024_06_019.jpg)'
- en: Figure 6.19 – ECR repository details
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 6.19 – ECR 存储库详细信息
- en: 'On the preceding page, click on **View push commands**, and you should see
    a popup, similar to the one shown in *Figure 6.20*:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的页面上，点击**查看推送命令**，您应该会看到一个类似于*图 6.20*所示的弹出窗口：
- en: '![Figure 6.20 – ECR push commands'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.20 – ECR 推送命令'
- en: '](img/B18024_06_020.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_06_020.jpg](img/B18024_06_020.jpg)'
- en: Figure 6.20 – ECR push commands
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20 – ECR 推送命令
- en: Depending on the operating system you are using for building the Docker image,
    save the necessary commands. We will use these commands to build the Docker image.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你用于构建 Docker 镜像的操作系统，保存必要的命令。我们将使用这些命令来构建 Docker 镜像。
- en: Building the Docker image
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建 Docker 镜像
- en: 'As mentioned earlier, we will be using the REST endpoints built in the previous
    chapter in this section. If you recall correctly, we had added two REST endpoints,
    `ping` and `invocations`. These endpoints are not random, though the same can
    be hosted in any container environment. To host a Docker image in the SageMaker
    endpoints, the requirement is that it should have the `ping` (which is the `GET`
    method) and `invocations` (which is the `POST` method) routes. I have added a
    couple of files to the same folder structure, which will be useful for building
    the Docker image. The REST code and folder structure are available at the following
    URL: [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/online-model-rest-api](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/online-model-rest-api).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在本节中我们将使用上一章中构建的 REST 端点。如果你记得正确，我们添加了两个 REST 端点，`ping` 和 `invocations`。这些端点并非随机，尽管它们可以在任何容器环境中托管。要在
    SageMaker 端点中托管 Docker 镜像，要求它应该有 `ping`（这是 `GET` 方法）和 `invocations`（这是 `POST`
    方法）路由。我已经在相同的文件夹结构中添加了一些文件，这些文件将有助于构建 Docker 镜像。REST 代码和文件夹结构可在以下 URL 获取：[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/online-model-rest-api](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/online-model-rest-api)。
- en: Important Note
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The additional files are `Dockerfile`, `requirements.txt`, and `serve`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的文件是 `Dockerfile`、`requirements.txt` 和 `serve`。
- en: Consecutively, clone the REST code to the local system, copy the feature repository
    into the `root` directory of the project, export the credentials, and then run
    the commands in *Figure 6.20*.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 连续地，将 REST 代码克隆到本地系统，将特征仓库复制到项目的 `root` 目录，导出凭据，然后运行 *图 6.20* 中的命令。
- en: Important Note
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can use the same user credential that was created in [*Chapter 4*](B18024_04_ePub.xhtml#_idTextAnchor065),
    *Adding Feature Store to ML Models*. However, we had missed adding ECR permissions
    to the user. Please navigate to the IAM console and add **AmazonEC2ContainerRegistryFullAccess**
    to the user. Otherwise, you will get an access error.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用在 [*第 4 章*](B18024_04_ePub.xhtml#_idTextAnchor065) 中创建的相同用户凭据，*将特征存储添加到机器学习模型*。然而，我们遗漏了向用户添加
    ECR 权限。请导航到 IAM 控制台，并将 **AmazonEC2ContainerRegistryFullAccess** 添加到用户。否则，你将遇到访问错误。
- en: 'The following are the example commands:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例命令：
- en: '[PRE60]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The commands logs in to ECR using the credentials set in the environment, builds
    the Docker image, and tags and pushes the Docker image to the registry. Once the
    image is pushed, if you navigate back to the screen in *Figure 6.19*, you should
    see the new image, as shown in the following screenshot:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 使用环境变量中设置的凭据登录到 ECR，构建 Docker 镜像，并将 Docker 镜像标记和推送到注册表。一旦镜像被推送，如果你导航回 *图 6.19*
    中的屏幕，你应该会看到新的镜像，如下面的截图所示：
- en: '![Figure 6.21 – ECR with the pushed image'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.21 – 推送到 ECR 的镜像'
- en: '](img/B18024_06_021.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_06_021.jpg]'
- en: Figure 6.21 – ECR with the pushed image
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.21 – 推送到 ECR 的镜像
- en: Now that the image is ready, copy the image **Uniform Resource Identifier (URI)**
    by clicking on the icon next to **Copy URI**, as shown in *Figure 6.21*. Let's
    deploy the Docker image as a SageMaker endpoint next.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，镜像已经准备好了，通过点击 *图 6.21* 中 **复制 URI** 旁边的图标来复制镜像的 **统一资源标识符 (URI**)。接下来，让我们将
    Docker 镜像作为 SageMaker 端点进行部署。
- en: Creating a SageMaker endpoint
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 SageMaker 端点
- en: Amazon SageMaker aims at providing managed infrastructure for ML. In this section,
    we will only be using the SageMaker inference components. SageMaker endpoints
    are used for deploying a model as REST endpoints for real-time prediction. It
    supports Docker image models and also supports a few flavors out of the box. We
    will be using the Docker image that we pushed into the ECR in the previous section.
    SageMaker endpoints are built using three building blocks – models, endpoint configs,
    and endpoints. Let's use these building blocks and create an endpoint next.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 致力于为机器学习提供托管基础设施。在本节中，我们只将使用 SageMaker 推理组件。SageMaker 端点用于将模型作为
    REST 端点进行实时预测。它支持 Docker 镜像模型，并且开箱即支持一些变体。我们将使用上一节中推送到 ECR 的 Docker 镜像。SageMaker
    端点是使用三个构建块构建的 - 模型、端点配置和端点。让我们使用这些构建块并创建一个端点。
- en: A SageMaker model
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SageMaker 模型
- en: 'The model is used to define the model parameters such as the name, the location
    of the model, and the IAM role. To define a model, navigate to the SageMaker console
    using the search bar and look for `Models` in the **Inference** section. Alternatively,
    visit [https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/models](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/models).
    The following screen will be displayed:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 模型用于定义模型参数，如名称、模型的存储位置和IAM角色。要定义模型，使用搜索栏导航到SageMaker控制台，并在**推理**部分查找`模型`。或者，访问[https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/models](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/models)。以下屏幕将显示：
- en: '![Figure 6.22 – The SageMaker Models console'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.22 – SageMaker模型控制台'
- en: '](img/B18024_06_022.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_022.jpg)'
- en: Figure 6.22 – The SageMaker Models console
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 – SageMaker模型控制台
- en: 'On the displayed page, click on **Create model** to navigate to the next screen.
    The following page will be displayed:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在显示的页面上，点击**创建模型**以跳转到下一屏幕。以下页面将显示：
- en: '![Figure 6.23 – SageMaker – Create model'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.23 – SageMaker – 创建模型'
- en: '](img/B18024_06_023.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_023.jpg)'
- en: Figure 6.23 – SageMaker – Create model
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23 – SageMaker – 创建模型
- en: 'As shown in *Figure 6.23*, input a model name, and for the IAM role, select
    **Create a new role** from the dropdown. A new popup appears, as displayed in
    the following screenshot:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图6.23*所示，输入模型名称，对于IAM角色，从下拉菜单中选择**创建新角色**。将出现一个新的弹出窗口，如下面的屏幕截图所示：
- en: '![Figure 6.24 – The SageMaker model – Create an IAM role'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.24 – SageMaker模型 – 创建IAM角色'
- en: '](img/B18024_06_024.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_024.jpg)'
- en: Figure 6.24 – The SageMaker model – Create an IAM role
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 – SageMaker模型 – 创建IAM角色
- en: 'In the popup, leave everything as default for the purpose of this exercise
    and click on **Create role**. AWS will create an IAM role, and on the same screen,
    you should see a message in the dialog with a link to the IAM role. The following
    figure shows the displayed message:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在弹出窗口中，为了本次练习的目的，保留所有默认设置，然后点击**创建角色**。AWS将创建一个IAM角色，在同一屏幕上，你应该在对话框中看到一个带有IAM角色链接的消息。以下图显示了显示的消息：
- en: '![Figure 6.25 – The SageMaker model – the new execution role is created'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.25 – SageMaker模型 – 新的执行角色已创建'
- en: '](img/B18024_06_025.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_025.jpg)'
- en: Figure 6.25 – The SageMaker model – the new execution role is created
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25 – SageMaker模型 – 新的执行角色已创建
- en: 'Now, if you recall correctly, we are using DynamoDB as the online store; as
    we are reading data on demand from DynamoDB tables, the IAM role needs access
    to them. Therefore, navigate to the IAM role we just created using the link displayed
    on the page in a new tab, add **AmazonDynamoDBFullAccess**, and come back to this
    tab. Scroll down to the **Container definition 1** section, where you should see
    the following parameters:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你记得正确的话，我们正在使用DynamoDB作为在线存储；因为我们是从DynamoDB表中按需读取数据，所以IAM角色需要访问它们。因此，使用页面显示的链接在新标签页中导航到我们刚刚创建的IAM角色，添加**AmazonDynamoDBFullAccess**，然后返回此标签页。向下滚动到**容器定义1**部分，你应该会看到以下参数：
- en: '![Figure 6.26 – The SageMaker model – the Container definition 1 section'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.26 – SageMaker模型 – 容器定义1部分'
- en: '](img/B18024_06_026.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_026.jpg)'
- en: Figure 6.26 – The SageMaker model – the Container definition 1 section
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26 – SageMaker模型 – 容器定义1部分
- en: 'For the **Location of inference code image** parameter, paste the image URI
    that we copied from the screen, as displayed in *Figure 6.21*. Leave the others
    as their defaults and scroll again to the **Network** section:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**推理代码图像位置**参数，粘贴从*图6.21*中复制的图像URI，然后保留其他设置不变，再次滚动到**网络**部分：
- en: '![Figure 6.27 – The Sagemaker Model – the Network section'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.27 – Sagemaker模型 – 网络部分'
- en: '](img/B18024_06_027.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_027.jpg)'
- en: Figure 6.27 – The Sagemaker Model – the Network section
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.27 – Sagemaker模型 – 网络部分
- en: Here, select the **VPC** to **Default vpc**, select one or two subnets from
    the list, and choose the default security group. Scroll down to the bottom and
    click on **Create model**.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，选择**VPC**为**默认vpc**，从列表中选择一个或两个子网，并选择默认的安全组。向下滚动到页面底部，然后点击**创建模型**。
- en: Important Note
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is never a good idea to select the default security group for production
    deployment, as inbound rules are not restrictive.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产部署中，选择默认安全组从来不是一个好主意，因为入站规则不是限制性的。
- en: Now that the model is ready, let's create the endpoint configuration next.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经准备好了，接下来让我们创建端点配置。
- en: Endpoint configuration
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 端点配置
- en: 'To set up the endpoint configuration, navigate to the SageMaker console using
    the search bar and look for `Endpoint Configurations` in the **Inference** section.
    Alternatively, visit [https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpointConfig](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpointConfig).
    The following page will be displayed:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置端点配置，请使用搜索栏导航到 SageMaker 控制台，并在 **推理** 部分查找 `Endpoint Configurations`。或者，访问
    [https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpointConfig](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpointConfig)。将显示以下页面：
- en: '![Figure 6.28 – The Sagemaker Endpoint configuration console'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.28 – Sagemaker 端点配置控制台'
- en: '](img/B18024_06_028.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_028.jpg)'
- en: Figure 6.28 – The Sagemaker Endpoint configuration console
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.28 – Sagemaker 端点配置控制台
- en: 'On the displayed web page, click on **Create endpoint configuration**. You
    will be navigated to the following page:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在显示的网页上，点击 **创建端点配置**。您将被导航到以下页面：
- en: '![Figure 6.29 – SageMaker – Create endpoint configuration'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.29 – SageMaker – 创建端点配置'
- en: '](img/B18024_06_029.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_029.jpg)'
- en: Figure 6.29 – SageMaker – Create endpoint configuration
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.29 – SageMaker – 创建端点配置
- en: 'On this screen, fill in the `customer-segmentation-config`. Scroll down to
    the **Data capture** section. This is used to define what percent of real-time
    inference data needs to be captured, where (the S3 location), and how it needs
    to be stored (JSON or CSV). You can choose to enable this or leave it disabled.
    I have left it disabled for this exercise. If you enable it, it will ask you for
    additional information. The section following **Data capture** is **Production
    variants**. This is used for setting up multiple model variants, and A/B testing
    of the models. For now, since we only have one variant, let''s add that here.
    To add a variant, click on the **Add model** link in the section; the following
    popup will appear:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在此屏幕上，填写 `customer-segmentation-config`。滚动到 **数据捕获** 部分。这用于定义需要捕获多少百分比的实时推理数据，在哪里（S3
    位置），以及如何存储（JSON 或 CSV）。您可以选择启用或禁用此功能。我在这个练习中将其禁用了。如果您启用它，它将要求您提供更多信息。**数据捕获**
    之后的部分是 **生产变体**。这用于设置多个模型变体，以及模型的 A/B 测试。目前，因为我们只有一个变体，所以让我们在这里添加它。要添加一个变体，请在该部分点击
    **添加模型** 链接；以下弹出窗口将出现：
- en: '![Figure 6.30 – SageMaker – adding a model to the endpoint config'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.30 – SageMaker – 将模型添加到端点配置'
- en: '](img/B18024_06_030.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_030.jpg)'
- en: Figure 6.30 – SageMaker – adding a model to the endpoint config
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.30 – SageMaker – 将模型添加到端点配置
- en: In the popup, select the model that we created earlier, scroll all the way down,
    and click on **Create endpoint configuration**.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在弹出窗口中，选择我们之前创建的模型，滚动到页面底部，然后点击 **创建端点配置**。
- en: SageMaker endpoint creation
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SageMaker 端点创建
- en: 'The last step is to use the endpoint configuration to create an endpoint. To
    create a SageMaker endpoint, navigate to the SageMaker console using the search
    bar and look for `Endpoints` in the **Inference** section. Alternatively, visit
    https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints.
    The following page will be displayed:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用端点配置创建端点。要创建 SageMaker 端点，请使用搜索栏导航到 SageMaker 控制台，并在 **推理** 部分查找 `Endpoints`。或者，访问
    https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints。将显示以下页面：
- en: '![Figure 6.31 – The SageMaker Endpoints console'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.31 – SageMaker 端点控制台'
- en: '](img/B18024_06_031.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_031.jpg)'
- en: Figure 6.31 – The SageMaker Endpoints console
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.31 – SageMaker 端点控制台
- en: 'On the page shown in *Figure 6.31*, click on **Create endpoint** to navigate
    to the following page:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 6.31* 所示的页面上，点击 **创建端点** 以导航到以下页面：
- en: '![Figure 6.31 – SageMaker – creating an endpoint'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.31 – SageMaker – 创建端点'
- en: '](img/B18024_06_032.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_06_032.jpg)'
- en: Figure 6.31 – SageMaker – creating an endpoint
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.31 – SageMaker – 创建端点
- en: On the web page displayed in *Figure 6.31*, provide an endpoint name. I have
    given the name `customer-segmentation-endpoint`. Scroll down to the **Endpoint
    configuration** section, select the endpoint configuration we created earlier,
    and click on the **Select endpoint configuration** button. Once it is selected,
    click on **Create endpoint**. It will take a few minutes to create an endpoint.
    When the endpoint status changes to **Available**, your model is live for serving
    real-time traffic.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.31*显示的网页上，提供一个端点名称。我已给出名称`customer-segmentation-endpoint`。向下滚动到**端点配置**部分，选择我们之前创建的端点配置，然后点击**选择端点配置**按钮。一旦选择，点击**创建端点**。创建端点需要几分钟。当端点状态变为**可用**时，您的模型即可用于实时流量服务。
- en: Testing the SageMaker endpoint
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试SageMaker端点
- en: The next thing we need to know is how to consume the model. There are different
    ways – you can use the SageMaker library, Amazon SDK client (Python, TypeScript,
    or any other available), or a SageMaker endpoint URL. All these methods default
    to AWS IAM authentication. If you have special requirements and want to expose
    the model without authentication or with custom authentication, it can be achieved
    using the API gateway and Lambda authorizer. For the purpose of this exercise,
    we will be using the `boto3` client to invoke the API. Irrespective of how we
    invoke the endpoint, the results should be the same.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来需要了解的是如何消费模型。有不同方式——您可以使用SageMaker库、Amazon SDK客户端（Python、TypeScript或其他可用的语言），或SageMaker端点URL。所有这些方法默认使用AWS
    IAM身份验证。如果您有特殊要求并希望在不进行身份验证或使用自定义身份验证的情况下公开模型，可以使用API网关和Lambda授权器来实现。为了本练习的目的，我们将使用`boto3`客户端来调用API。无论我们如何调用端点，结果都应该是相同的。
- en: 'The following code block invokes the endpoint using the `boto3` client:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块使用`boto3`客户端调用端点：
- en: '[PRE61]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: In the preceding code block, we are invoking the endpoint that we created to
    run predictions for two customers with the `12747.0` and `12841.0` IDs. The endpoint
    will respond within milliseconds with the predictions for the given customer IDs.
    Now, the endpoint can be shared with the model consumers.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们正在调用我们创建的端点，为具有`12747.0`和`12841.0` ID的两个客户运行预测。端点将在毫秒内对给定的客户ID做出预测。现在，端点可以与模型消费者共享。
- en: Now that the model is in production, let's look at a few aspects that come after
    a model moves to production.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已投入生产，让我们看看模型转移到生产后的一些后续方面。
- en: Beyond model production
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越模型生产
- en: In this section, we will discuss the postproduction aspects of ML and how we
    benefit from the adoption of a feature store.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论机器学习的生产后方面以及我们如何从采用特征存储中受益。
- en: Feature drift monitoring and model retraining
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征漂移监控和模型重新训练
- en: Once the model is in production, the next question that will come up frequently
    is how the model is performing in production. There may be different metrics used
    to measure the performance of a model – for instance, for a recommendation model,
    performance may be measured by a conversion rate, which is how often the recommended
    product was purchased. Similarly, predicting the next action of a customer may
    be measured by error rate, and so on. There is no universal way of doing it. But
    if a model's performance is bad, it needs to be retrained or replaced with a new
    one.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 模型投入生产后，接下来经常出现的问题是模型在生产中的表现如何。可能使用不同的指标来衡量模型的表现——例如，对于推荐模型，表现可能通过转化率来衡量，即推荐的产品被购买频率。同样，预测客户的下一步行动可能通过错误率来衡量，等等。没有通用的方法来做这件事。但如果模型的表现不佳，则需要重新训练或用新的模型替换。
- en: One other aspect that defines when a model should be retrained is when the feature
    starts to drift away from the values with which it was trained. For example, let's
    say the mean frequency value of the customer during the initial model training
    was 10, but now, the mean frequency value is 25\. Similarly, the lowest monetary
    value was initially $100.00 and now it is $500.00\. This is called **data drift**.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 定义何时应该重新训练模型的其他方面之一是当特征开始偏离其训练时的值。例如，假设在初始模型训练期间客户的平均频率值为10，但现在平均频率值为25。同样，最低货币值最初为100.00美元，现在为500.00美元。这被称为**数据漂移**。
- en: 'Data drift monitoring measures the change in the statistical distribution of
    the data; in the case of feature monitoring, it is comparing the change in the
    statistical distribution of a feature from *t1* time to *t2* time. The article
    at the following URL discusses different metrics for data drift monitoring: [https://towardsdatascience.com/automating-data-drift-thresholding-in-machine-learning-systems-524e6259f59f](https://towardsdatascience.com/automating-data-drift-thresholding-in-machine-learning-systems-524e6259f59f).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 数据漂移监控衡量的是数据统计分布的变化；在特征监控的情况下，它是比较从时间`t1`到时间`t2`的特征统计分布的变化。以下URL的文章讨论了数据漂移监控的不同指标：[https://towardsdatascience.com/automating-data-drift-thresholding-in-machine-learning-systems-524e6259f59f](https://towardsdatascience.com/automating-data-drift-thresholding-in-machine-learning-systems-524e6259f59f)。
- en: With a feature store, it is easy to retrieve a training dataset from two different
    points in time, namely the dataset used for model training and the latest feature
    values for all the features used in model training. Now, all we need to do is
    run data drift monitoring on schedule to generate a drift report. The standardization
    that Feast brought to the table is, since the data is stored and retrieved using
    standard APIs, a generic feature drift monitoring can be run on schedule for all
    the datasets in the feature store. The feature drift report can be used as one
    of the indicators for model retraining. If feature drift is affecting the model's
    performance, it can be retrained with the latest dataset, and deployed and AB-tested
    with the current production model.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征存储，可以轻松地从两个不同时间点检索训练数据集，即用于模型训练的数据集和用于模型训练的所有特征的最新特征值。现在，我们只需要按计划运行数据漂移监控来生成漂移报告。Feast带来的标准化是，由于数据是使用标准API存储和检索的，因此可以在特征存储中的所有数据集上按计划运行通用的特征漂移监控。特征漂移报告可以用作模型重新训练的指标之一。如果特征漂移影响了模型的性能，可以使用最新的数据集重新训练，并与当前的生产模型部署和A/B测试。
- en: Model reproducibility and prediction issues
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型可复现性和预测问题
- en: If you recall from [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An
    Overview of the Machine Learning Life Cycle*, model reproducibility is one of
    the common problems of ML. We need a way to consistently reproduce the model (or
    training data used for model). Without a feature store, if the underlying raw
    data that is used to generate features changes, it is not possible to reproduce
    the same training dataset. However, with a feature store, as we discussed earlier,
    the features are versioned with a timestamp (one of the columns in the features
    DataFrame is an event timestamp). Hence, we can query the historical data to generate
    the same feature set used for model training. If the algorithm used for training
    the model is not stochastic, the model can also be reproduced. Let's try this
    out.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得[第1章](B18024_01_ePub.xhtml#_idTextAnchor014)中的概述，即“机器学习生命周期概述”，模型可复现性是机器学习的一个常见问题。我们需要一种方法来一致地复现模型（或用于模型的训练数据）。如果没有特征存储，如果用于生成特征的底层原始数据发生变化，就无法复现相同的训练数据集。然而，使用特征存储，正如我们之前讨论的，特征与时间戳版本化（特征DataFrame中的一列是事件时间戳）。因此，我们可以查询历史数据来生成用于模型训练的相同特征集。如果用于训练模型的算法不是随机的，模型也可以复现。让我们试试看。
- en: 'Since we have already done something similar to this in the *Model training
    with a feature store* section of [*Chapter 5*](B18024_05_ePub.xhtml#_idTextAnchor078),
    *Model Training and Inference*, we will reuse the same code to run this experiment.
    Copy and run all the code till you create the entity DataFrame and then replace
    the `event_timestamp` column with an older timestamp (the timestamp of when the
    model was trained), as shown here. In this case, the model was trained at `2022-03-26
    16:24:21`, as shown in *Figure 5.1* of [*Chapter 5*](B18024_05_ePub.xhtml#_idTextAnchor078),
    *Model Training and Inference*:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经在[第5章](B18024_05_ePub.xhtml#_idTextAnchor078)的“使用特征存储进行模型训练”部分中做了类似的事情，即“模型训练和推理”，我们将重用相同的代码来运行这个实验。复制并运行所有代码，直到你创建实体DataFrame，然后将`event_timestamp`列替换为较旧的时间戳（模型训练的时间戳），如下所示。在这种情况下，模型是在`2022-03-26
    16:24:21`训练的，如[第5章](B18024_05_ePub.xhtml#_idTextAnchor078)的“模型训练和推理”中的*图5.1*所示：
- en: '[PRE76]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Once you are done replacing the timestamp, continue running the code from the
    *Dee's model training experiments* section of [*Chapter 5*](B18024_05_ePub.xhtml#_idTextAnchor078),
    *Model Training and Inference*. You should be able to generate the exact same
    dataset that was used in Dee's model training (in this case, the dataset in *Figure
    5.2* of [*Chapter 5*](B18024_05_ePub.xhtml#_idTextAnchor078), *Model Training
    and Inference*). Hence, if the model uses a nonrandom algorithm, then the model
    can also be reproduced using the feature set.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦替换完时间戳，请继续从[*第五章*](B18024_05_ePub.xhtml#_idTextAnchor078)的“Dee的模型训练实验”部分运行代码，*模型训练和推理*。你应该能够生成与Dee模型训练中使用的相同的数据集（在这种情况下，*第五章*](B18024_05_ePub.xhtml#_idTextAnchor078)中的*图5.2*所示的数据集）。因此，如果模型使用非随机算法，则也可以使用特征集重现模型。
- en: One other advantage of a feature store is the debugging prediction issue. Let's
    consider a scenario where you have a website-facing model that is classifying
    a transaction as fraudulent or not. During the peak hour, it flagged a few transactions
    as fraudulent, but the transactions were legitimate. The customer called in and
    complained to the customer service department, and now it's the data scientist
    Subbu's turn to figure out what went wrong. If there was no feature store in the
    project, to reproduce the issue, Subbu would have to go into the raw data, try
    to generate the features, and see whether the behavior still remains the same.
    If not, Subbu would have to go into the application log, process it, look for
    user behavior before the event, try to reproduce it from the user interaction
    perspective, and also capture the features for all these trials, hoping that the
    issue can be reproduced at least once.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储的另一个优点是调试预测问题。让我们考虑一个场景，你有一个面向网站的模型，该模型正在将交易分类为欺诈或非欺诈。在高峰时段，它将几笔交易标记为欺诈，但这些交易实际上是合法的。客户打电话给客服部门投诉，现在轮到数据科学家Subbu来找出问题所在。如果项目中没有特征存储，为了重现问题，Subbu必须进入原始数据，尝试生成特征，并查看行为是否仍然相同。如果不相同，Subbu必须进入应用程序日志，进行处理，查找事件之前的行为，并从用户交互的角度尝试重现，同时捕获所有这些试验的特征，希望至少能重现一次问题。
- en: On the other hand, with the feature store used in the project, Subbu will figure
    out the approximate time when the event happened, what the entities and features
    used in the model are, and what the version of the model that was running in production
    was at the time of the event. With this information, Subbu will connect to the
    feature store and fetch all the features used in the model for all the entities
    for the approximate time range when the issue happened. Let's say that the event
    occurred between 12:00pm to 12:15pm today, features were streaming, and the freshness
    interval was around 30 seconds. This means that, on average, for a given entity,
    there is a chance that features will change in the next 30 seconds from any given
    time.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，使用项目中使用的特征存储，Subbu将找出事件发生的大概时间，模型中使用的实体和特征是什么，以及事件发生时生产中运行的模型版本。有了这些信息，Subbu将连接到特征存储并获取在问题发生的大概时间范围内所有实体使用的所有特征。比如说，事件发生在今天中午12:00到12:15之间，特征是流式的，新鲜度间隔大约是30秒。这意味着，对于给定的实体，从任何给定时间开始，特征在接下来的30秒内可能会发生变化。
- en: To reproduce the issue, Subbu will form an entity DataFrame with the same entity
    ID repeated 30 times in one column and, for the event time column, a timestamp
    from 12:00pm to 12:15pm with 30-second intervals. With this entity DataFrame,
    Subbu will query the historical store using the Feast API and run the prediction
    for the generated features. If the issue is reproduced, Subbu has the feature
    set that caused the issue. If not, using the entity DataFrame, the interval will
    be reduced to less than 30 seconds, maybe to 10 seconds, to figure out if features
    changed at a faster pace than 30 seconds. Subbu can continue doing this till she
    finds the feature set that reproduces the issue.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重现问题，Subbu将创建一个实体DataFrame，其中一个列中重复30次相同的实体ID，对于事件时间列，从中午12:00到12:15的30秒间隔的时间戳。有了这个实体DataFrame，Subbu将使用Feast
    API查询历史存储并运行生成的特征的预测。如果问题重现，Subbu就有导致问题的特征集。如果没有重现，使用实体DataFrame，间隔将减少到小于30秒，可能到10秒，以确定特征是否比30秒更快地发生变化。Subbu可以继续这样做，直到她找到重现问题的特征集。
- en: A headstart for the next model
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为下一个模型赢得先机
- en: Now that the model has productionized, the data scientist Subbu picks up the
    next problem statement. Let's assume that the next ML model has to predict the
    **Next Purchase Day** (**NPD**) of a customer. The use case here could be that
    based on the NPD, we want to run a campaign for a customer. If a customers' purchase
    day is farther in the future, we want to offer a special deal so that we can encourage
    purchasing sooner. Now, before going to a raw dataset, Subbu can look for available
    features based on how the search and discoverability aspect is integrated into
    the feature store. Since Feast moved from service-oriented to SDK/CLI-oriented,
    there is a need for catalog tools, a GitHub repository of all the feature repositories,
    a data mesh portal, and so on. However, in the case of feature stores such as
    SageMaker or Databricks, users can connect to feature store endpoints (with SageMaker
    runtime using a boto3 or Databricks workspace) and browse through the available
    feature definitions using the API or from the UI. I have not used the Tecton feature
    store before, but Tecton also offers a UI for its feature store that can be used
    to browse through the available features. As you can see, this is one of the drawbacks
    of the different versions of Feast between 0.9.X and 0.20.X (0.20 is the version
    at the time of writing).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型已经投入生产，数据科学家Subbu接手下一个问题陈述。假设下一个机器学习模型需要预测客户的**下一次购买日**（**NPD**）。这里的用例可能是基于NPD，我们想要为客户运行一次营销活动。如果客户的购买日较远，我们想要提供特别优惠，以便我们可以鼓励他们尽早购买。现在，在查看原始数据集之前，Subbu可以根据搜索和可发现性方面如何集成到特征存储中查找可用的特征。由于Feast从面向服务的架构迁移到SDK/CLI导向，需要目录工具、所有特征存储库的GitHub仓库、数据网格门户等。然而，在SageMaker或Databricks等特征存储的情况下，用户可以通过API或从UI浏览可用的特征定义来连接到特征存储端点（使用SageMaker运行时通过boto3或Databricks工作区）。我之前没有使用过Tecton特征存储，但Tecton也为其特征存储提供了一个UI，可以用来浏览可用的特征。正如你所见，这是Feast在0.9.X和0.20.X（0.20是撰写时的版本）不同版本之间的一大缺点。
- en: Let's assume, for now, that Subbu has a way to locate all the feature repositories.
    Now, she can connect and browse through them to figure out what the projects and
    feature definitions are that could be useful in the NPD model. So far, we have
    just one feature repository that has the customer RFM features that we have been
    using so far, and these features can be useful in the model. To use these features,
    all Subbu has to do is get read access to the AWS resource, and the latest RFM
    features will be available every day for experimentation and can also be used
    if the model moves to production.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 假设现在Subbu有办法定位所有特征存储库。现在，她可以连接并浏览它们，以找出在NPD模型中可能有用的项目和特征定义。到目前为止，我们只有一个包含我们迄今为止一直在使用的客户RFM特征的存储库，这些特征在模型中可能很有用。要使用这些特征，Subbu只需要获取AWS资源的读取权限，最新的RFM特征将每天可用于实验，如果模型转移到生产环境也可以使用。
- en: To see how beneficial the feature store would be during the development of the
    subsequent model, we should try building the NPD. I will go through the initial
    few steps to get you started on the model. As we followed a blog during the development
    of the first model, we will be following another part in the same blog series,
    which can be found at [https://towardsdatascience.com/predicting-next-purchase-day-15fae5548027](https://towardsdatascience.com/predicting-next-purchase-day-15fae5548027).
    Please read through the blog, as it discusses the approach and why the author
    thinks specific features will be useful. Here, we will be skipping ahead to the
    feature engineering section.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到特征存储在后续模型开发中的好处，我们应该尝试构建NPD。我将通过最初的几个步骤来帮助你开始模型。因为我们遵循了一个博客来开发第一个模型，我们将继续遵循同一博客系列中的另一部分，该部分可以在[https://towardsdatascience.com/predicting-next-purchase-day-15fae5548027](https://towardsdatascience.com/predicting-next-purchase-day-15fae5548027)找到。请阅读该博客，因为它讨论了方法以及作者为什么认为特定的特征将是有用的。在这里，我们将跳过特征工程部分。
- en: 'We will be using the feature set that the blog author uses, which includes
    the following:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用博客作者使用的功能集，包括以下内容：
- en: RFM features and clusters
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RFM特征和聚类
- en: The number of days between the last three purchases
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后三次购买之间的天数
- en: The mean and standard deviation of the differences between the purchases
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购买差异的平均值和标准差
- en: The first feature set already exists in the feature store; we don't need to
    do anything extra for it. But for the other two, we need to do feature engineering
    from the raw data. The notebook at [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/ch6_next_purchase_day_feature_engineering.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/ch6_next_purchase_day_feature_engineering.ipynb)
    has the required feature engineering to generate the features in the preceding
    second and third bullet points. I will leave the ingestion of these features into
    a feature store and using the features (RFM) from the previous model in combination
    with these to train a new model as an exercise. As you develop and productionize
    this model, you will see the benefit of the feature store and how it can accelerate
    model building.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个特征集已经存在于特征存储库中；我们不需要为此做任何额外的工作。但对于其他两个，我们需要从原始数据中进行特征工程。在[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/ch6_next_purchase_day_feature_engineering.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/ch6_next_purchase_day_feature_engineering.ipynb)的笔记本中有生成前述第二和第三要点中所需特征的必要特征工程。我将把这些特征的导入到特征存储库以及使用前一个模型中的特征（RFM）结合这些特征来训练新模型作为一个练习。随着你开发和生产化这个模型，你将看到特征存储库的好处以及它如何可以加速模型构建。
- en: Next, let's discuss how to change a feature definition when the model is in
    production.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论当模型处于生产状态时如何更改特征定义。
- en: Changes to feature definition after production
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产后的特征定义变更
- en: So far, we have discussed feature ingestion, query, and changes to a feature
    set during the development phases. However, we haven't talked about changes to
    feature definitions when the model is in production. Often, it is argued that
    changing feature definition once the model moves to production is difficult. The
    reason for this is that there is a chance that multiple models are using feature
    definitions and any changes to them will have a cascading effect on the models.
    This is one of the reasons why some feature stores don't yet support updates on
    feature definitions. We need a way to handle the change effectively here.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了在开发阶段对特征集的导入、查询以及对其变更。然而，我们还没有谈到当模型处于生产状态时对特征定义的变更。通常，人们认为一旦模型进入生产状态，更改特征定义是困难的。原因在于，可能存在多个模型正在使用特征定义，对它们的任何变更都可能会对模型产生级联效应。这也是为什么一些特征存储库尚未支持特征定义更新功能的原因之一。我们需要一种有效处理变更的方法。
- en: This is still a gray area; there is no right or wrong way of doing it. We can
    adopt any mechanism that we use in other software engineering processes. A simple
    one could be the versioning of the feature views, similar to the way we do our
    REST APIs or Python libraries. Whenever a change is needed for a production feature
    set, assuming that it is being used by others, a new version of the feature view
    (let's call it `customer-segemantion-v2`) will be created and used. However, the
    previous version will still need to be managed until all the models migrate. If,
    for any reason, there are models that need the older version and cannot be migrated
    to the newer version of the feature table/views, it may have to be managed or
    handed over to the team that needs it. There needs to be some discussion on ownership
    of the features and feature engineering jobs.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然是一个灰色地带；没有正确或错误的方法来做这件事。我们可以采用在其他软件工程过程中使用的任何机制。一个简单的例子可以是特征视图的版本控制，类似于我们处理REST
    API或Python库的方式。每当需要对生产特征集进行变更时，假设它被其他人使用，就会创建并使用一个新的特征视图版本（让我们称它为`customer-segmentation-v2`）。然而，在所有模型迁移之前，还需要管理之前的版本。如果由于任何原因，有模型需要旧版本且无法迁移到新版本的特征表/视图，可能需要对其进行管理或转交给需要它的团队。需要对特征和特征工程工作的所有权进行一些讨论。
- en: This is where the concept of data as a product is very meaningful. The missing
    piece here is a framework for producers and consumers to define contracts and
    notify changes. The data producers need a way of publishing their data products;
    here, the data product is feature views. The consumers of the product can subscribe
    to the data product and use it. During the feature set changes, the producers
    can define a new version of the data product and depreciate the older version
    so that consumers will be notified of what the changes are. This is just my opinion
    on a solution, but I'm sure there are better minds out there who may already be
    implementing another solution.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是数据作为产品概念非常有意义的地方。这里缺失的部分是生产者和消费者定义合同和通知变更的框架。数据生产者需要一种发布他们的数据产品的方式；在这里，数据产品是特征视图。产品的消费者可以订阅数据产品并使用它。在特征集变更期间，生产者可以定义数据产品的新版本，并使旧版本过时，以便消费者能够得知变更内容。这只是我对解决方案的看法，但我相信世界上有更聪明的人可能已经在实施另一种解决方案。
- en: With that, let's summarize what we have learned in this chapter and move on
    to the next one.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，让我们总结本章所学的内容，并继续下一章。
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we aimed at using everything we built in the previous chapters
    and productionizing the ML models for batch and online use cases. To do that,
    we created an Amazon MWAA environment and used it for the orchestration of the
    batch model pipeline. For the online model, we used Airflow for the orchestration
    of the feature engineering pipeline and the SageMaker inference components to
    deploy a Docker online model as a SageMaker endpoint. We looked at how a feature
    store facilitates the postproduction aspects of ML, such as feature drift monitoring,
    model reproducibility, debugging prediction issues, and how to change a feature
    set when the model is in production. We also looked at how data scientists get
    a headstart on the new model with the use of a feature store. So far, we have
    used Feast in all our exercises; in the next chapter, we will look at a few of
    the feature stores that are available on the market and how they differ from Feast,
    alongside some examples.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的目标是利用前几章所构建的一切，并将机器学习模型用于批量处理和在线用例的生产化。为此，我们创建了一个Amazon MWAA环境，并使用它来编排批量模型管道。对于在线模型，我们使用Airflow来编排特征工程管道和SageMaker推理组件，以部署一个作为SageMaker端点的Docker在线模型。我们探讨了特征存储如何促进机器学习的后期生产方面，例如特征漂移监控、模型可复现性、调试预测问题，以及当模型在生产中时如何更改特征集。我们还探讨了数据科学家如何通过使用特征存储来在新模型上取得领先。到目前为止，我们在所有练习中都使用了Feast；在下一章中，我们将探讨市场上可用的几个特征存储，以及它们与Feast的不同之处，并附带一些示例。
