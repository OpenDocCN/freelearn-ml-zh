- en: Chapter 6. Credit Risk Detection and Prediction – Predictive Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered a lot of ground in the financial domain
    where we took up the challenge of detecting and predicting bank customers who
    could be potential credit risks. We now have a good idea about our main objective
    regarding credit risk analysis. Besides this, the substantial knowledge gained
    from descriptive analytics of the dataset and its features will be useful for
    predictive analytics, as we had mentioned earlier.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be journeying through the world of predictive analytics,
    which sits at the core of machine learning and data science. Predictive analytics
    encompasses several things which include classification algorithms, regression
    algorithms, domain knowledge, and business logic which are combined to build predictive
    models and derive useful insights from data. We had discussed various machine
    learning algorithms at the end of the previous chapter which would be applicable
    for solving our objective, and we will be exploring several of them in this chapter
    when we build predictive models using the given dataset and these algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: An interesting take on predictive analytics is that it holds a lot of promise
    for organizations who want to strengthen their business and profits in the future.
    With the advent of big data, most organizations now have more data than they can
    analyze! While this is a big challenge, a tougher challenge is to select the right
    data points from this data and build predictive models which would be capable
    of predicting outcomes correctly in the future. However, there are several caveats
    in this approach because each model is basically mathematical functions based
    on formulae, assumptions, and probability. Also, in the real world, conditions
    and scenarios keep changing and evolving and thus one must remember that a predictive
    model built today may be completely redundant tomorrow.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: A lot of skeptics say that it is extremely difficult for computers to mimic
    humans to predict outcomes which even humans can't predict due of the ever changing
    nature of the environment with time, and hence all statistical methods are only
    valuable under ideal assumptions and conditions. While this is true to some extent,
    with the right data, a proper mindset, and by applying the right algorithms and
    techniques, we can build robust predictive models which can definitely try and
    tackle problems which would be otherwise impossible to tackle by conventional
    or brute-force methods.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Predictive modeling is a difficult task and while there might be a lot of challenges
    and results might be difficult to obtain always, one must take these challenges
    with a pinch of salt and remember the quotation from the famous statistician George
    E.P. Box, who claimed that *Essentially all models are wrong but some are useful!*,
    which is quite true based on what we discussed earlier. Always remember that a
    predictive model will never be 100% perfect but, if it is built with the right
    principles, it will be very useful!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Predictive analytics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to predict credit risk
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important concepts in predictive modeling
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling using logistic regression
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling using support vector machines
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling using decision trees
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling using random forests
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling using neural networks
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model comparison and selection
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive analytics
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We had already discussed a fair bit about predictive analytics in the previous
    chapter to give you a general overview of what it means. We will be discussing
    it in more detail in this section. Predictive analytics can be defined as a subset
    of the machine learning universe, which encompasses a wide variety of supervised
    learning algorithms based on data science, statistics, and mathematical formulae
    which enable us to build predictive models using these algorithms and data which
    has already been collected. These models enable us to make predictions of what
    might happen in the future based on past observations. Combining this with domain
    knowledge, expertise, and business logic enables analysts to make data driven
    decisions using these predictions, which is the ultimate outcome of predictive
    analytics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The data we are talking about here is data which has already been observed in
    the past and has been collected over a period of time for analysis. This data
    is often known as historical data or training data which is fed to the model.
    However, most of the time in the predictive modeling methodology, we do not feed
    the raw data directly but use features extracted from the data after suitable
    transformations. The data features along with a supervised learning algorithm
    form a predictive model. The data which is obtained in the present can then be
    fed to this model to predict outcomes which are under observation and also to
    test the performance of the model with regards to various accuracy metrics. This
    data is known as testing data in the machine learning world.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'The analytics pipeline that we will be following for carrying out predictive
    analytics in this chapter is a standard process, which is explained briefly in
    the following steps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '**Getting the data**: Here we get the dataset on which we will be building
    the predictive model. We will perform some basic descriptive analysis of the dataset,
    which we have already covered in the previous chapter. Once we have the data we
    will move on to the next step.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data preprocessing**: In this step, we carry out data transformations, such
    as changing data types, feature scaling, and normalization, if necessary, to prepare
    the data for being trained by models. Usually this step is carried out after the
    dataset preparation step. However, in this case, the end results are the same,
    so we can perform these steps in any order.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dataset preparation**: In this step, we use some ratio like 70:30 or 60:40
    to separate the instances from the data into training and testing datasets. We
    usually use the training dataset to train a model and then check its performance
    and predicting capability with the testing dataset. Often data is divided in proportions
    of 60:20:20 where we also have a validation dataset besides the other two datasets.
    However, we will just keep it to two datasets in this chapter.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature selection**: This process is an iterative one which even occurs in
    a later stage if needed. The main objective in this step is to choose a set of
    attributes or features from the training dataset that enables the predictive model
    to give the best predictions possible, minimizing error rates and maximizing accuracy.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Predictive modeling**: This is the main step where we select a machine learning
    algorithm best suited for solving the problem and build the predictive model using
    the algorithm by feeding it the features extracted from the data in the training
    dataset. The output of this stage is a predictive model which can be used for
    predictions on future data instances.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model evaluation**: In this phase, we use the testing dataset to get predictions
    from the predictive model and use a variety of techniques and metrics to measure
    the performance of the model.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model tuning**: We fine tune the various parameters of the model and perform
    feature selection again if necessary. We then rebuild the model and re-evaluate
    it until we are satisfied with the results.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model deployment**: Once the predictive model gives a satisfactory performance,
    we can deploy this model by using a web service in any application to provide
    predictions in real time or near real time. This step focuses more on software
    and application development around deploying the model, so we won''t be covering
    this step since it is out of scope. However, there are a lot of tutorials out
    there regarding building web services around predictive models to enable *Prediction
    as a service*.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The last three steps are iterative and may be performed several times if
    needed**.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Even though the preceding process might look pretty intensive at first glance,
    it is really a very simple and straight-forward process, which once understood
    would be useful in building any type of predictive modeling. An important thing
    to remember is that predictive modeling is an iterative process where we might
    need to analyze the data and build the model several times by getting feedback
    from the model predictions and evaluating them. It is therefore extremely important
    that you do not get discouraged even if your model doesn't perform well on the
    first go because a model can never be perfect, as we mentioned before, and building
    a good predictive model is an art as well as science!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管乍一看这个过程可能看起来相当复杂，但实际上它是一个非常简单且直接的过程，一旦理解，就可以用于构建任何类型的预测模型。需要记住的一个重要事情是，预测建模是一个迭代的过程，我们可能需要通过从模型预测中获得反馈并评估它们来多次分析数据和构建模型。因此，即使你的模型在第一次尝试时表现不佳，你也绝不能气馁，因为正如我们之前提到的，模型永远不可能完美，构建一个好的预测模型既是艺术也是科学！
- en: In the next section, we will be focusing on how we would apply predictive analytics
    to solve our prediction problem and the kind of machine learning algorithms we
    will be exploring in this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将关注如何应用预测分析来解决我们的预测问题，以及在本章中我们将探索的机器学习算法类型。
- en: How to predict credit risk
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何预测信用风险
- en: If you remember our main objective from the previous chapter, we were dealing
    with customer data from a German bank. We will quickly recap our main problem
    scenario to refresh your memory. These bank customers are potential candidates
    who ask for credit loans from the bank with the stipulation that they make monthly
    payments with some interest on the amount to repay the credit amount. In a perfect
    world there would be credit loans dished out freely and people would pay them
    back without issues. Unfortunately, we are not living in a utopian world, and
    so there will be customers who will default on their credit loans and be unable
    to repay the amount, causing huge losses to the bank. Therefore, credit risk analysis
    is one of the crucial areas which banks focus on where they analyze detailed information
    pertaining to customers and their credit history.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得上一章的主要目标，我们当时处理的是来自德国银行的客户数据。我们将快速回顾我们的主要问题场景以刷新你的记忆。这些银行客户是潜在的候选人，他们向银行申请信用贷款，条件是他们必须每月支付一定的利息来偿还贷款金额。在理想的世界里，信用贷款会被自由发放，人们会毫无问题地偿还它们。不幸的是，我们并不生活在一个乌托邦的世界，因此将会有一些客户会违约，无法偿还贷款金额，这会给银行造成巨大的损失。因此，信用风险分析是银行关注的几个关键领域之一，他们分析与客户及其信用历史相关的详细信息。
- en: Now coming back to the main question, for predicting credit risk, we need to
    analyze the dataset pertaining to customers, build a predictive model around it
    using machine learning algorithms, and predict whether a customer is likely to
    default on paying the credit loan and could be labeled as a potential credit risk.
    The process which we will follow for achieving this is what we discussed in the
    previous section. You already have an idea about the data and features associated
    with it from the previous chapter. We will explore several predictive models,
    understand the concepts behind how the models work, and then build these models
    for predicting credit risk. Once we start predicting the outcomes, we will compare
    the performance of these different models and then talk about the business impact
    and how to derive insights from the model prediction outcomes. Do note that the
    predictions are not the output in the predictive analytics life cycle but the
    valuable insights that we derive from these predictions is the end goal. Businesses
    such as financial institutions get value only from using domain knowledge to translate
    prediction outcomes and raw numbers from machine learning algorithms to data driven
    decisions, which, when executed at the right time, help grow the business.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'For this scenario, if you remember the dataset well, the feature `credit.rating`
    is the response or class variable, which indicates the credit rating of the customers.
    We will be predicting this value for the other customers based on other features
    which are independent variables. For modeling, we will be using machine learning
    algorithms which belong to the supervised learning family of algorithms. These
    algorithms are used for predictions and can be divided into two broad categories:
    classification and regression. However, they have some differences which we will
    talk about now. In the case of regression, the values for the variables to be
    predicted are continuous values, like predicting prices of houses based on different
    features such as the number of rooms, the area of the house, and so on. Regression
    mostly deals with estimating and predicting a response value based on input features.
    In the case of classification, the values for the variables to be predicted have
    discrete and distinct labels, such as predicting the credit rating for customers
    for our bank, where the credit rating can either be good, which is denoted by
    `1` or bad, which is denoted by `0`. Classification mostly deals with categorizing
    and identifying group memberships for each data tuple in the dataset. Algorithms
    such as logistic regression are special cases of regression models which are used
    for classification, where the algorithm estimates the odds that a variable is
    in one of the class labels as a function of the other features. We will be building
    predictive models using the following machine learning algorithms in this chapter:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have chosen these algorithms to give a good flavor of the diverse set of
    supervised machine learning algorithms which are present, so that you gain knowledge
    not only about the concepts behind these models but also learn to implement building
    models using them, and compare model performances using various techniques. Before
    we begin our analysis, we will glance over some basic concepts in predictive modeling
    that are mentioned in this book and talk about some of them in detail so you get
    a good idea of what goes on behind the scenes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Important concepts in predictive modeling
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already looked at several concepts when we talked about the machine learning
    pipeline. In this section, we will look at typical terms which are used in predictive
    modeling, and also discuss about model building and evaluation concepts in detail.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data preparation step, as discussed earlier, involves preparing the datasets
    necessary for feature selection and building the predictive models using the data.
    We frequently use the following terms in this context:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**Datasets**: They are typically a collection of data points or observations.
    Most datasets usually correspond to some form of structured data which involves
    a two dimensional data structure, such as a data matrix or data table (in R this
    is usually represented using a data frame) containing various values. An example
    is our `german_credit_dataset.csv` file from [Chapter 5](part0038_split_000.html#147LC1-973e731d75c2419489ee73e3a0cf4be8
    "Chapter 5. Credit Risk Detection and Prediction – Descriptive Analytics"), *Credit
    Risk Detection and Prediction – Descriptive Analytics*.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data observations**: They are the rows in a dataset where each row consists
    of a set of observations against a set of attributes. These rows are also often
    called tuples. For our dataset, each row containing information about a customer
    is a good example.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data features**: They are the columns in a dataset which describe each row
    in the dataset. These features are often called attributes or variables. Features
    such as `credit.rating`, `account.balance`, and so on form the features of our
    credit risk dataset.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation**: It refers to the act of transforming various data
    features as needed based on observations from descriptive analytics. Data type
    conversions, missing values imputation, and scaling and normalization are some
    of the most used techniques. Also, for categorical variables, if your algorithms
    are not able to detect the different levels in the variable, you need to convert
    it to several dummy variables; this process is known as one-hot encoding.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training data**: It refers to the data which is solely used to train the
    predictive models. The machine learning algorithm picks up the tuples from this
    dataset and tries to find out patterns and learn from the various observation
    instances.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing data**: It refers to the data which is fed to the predictive model
    to get predictions and then we check the accuracy of the model using the class
    labels which are already present in the tuples for this dataset. We never train
    the model with the testing data because it would bias the model and give incorrect
    evaluations.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building predictive models
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We build the actual predictive models using machine learning algorithms and
    data features which finally start giving out predictions as we feed it new data
    tuples. Some concepts associated with building predictive models are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '**Model training**: It is analogous to building the predictive model where
    we use a supervised machine learning algorithm and feed the training data features
    to it and build the predictive model.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictive model**: It is based on some machine learning algorithm, which
    is essentially a mathematical model at heart, with some assumptions, formulae,
    and parameter values.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model selection**: It is a process where the main objective is to select
    a predictive model from several iterations of predictive models. The criteria
    for selecting the best model can vary, depending on the metrics we want to choose,
    such as maximizing the accuracy, minimizing the error rate, or getting the maximum
    AUC, which is something we will discuss later. Cross-validation is a good way
    to run this iterative process.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter optimization**: It is basically trying to choose a set of
    the hyperparameters used by the algorithm in the model such that the performance
    of the model is optimal with regards to its prediction accuracy. This is usually
    done by a grid search algorithm.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross validation**: It is a model validation technique which is used to estimate
    how a model would perform in a generic fashion. It is mainly used in iterative
    processes where the end goal is to optimize the model and make sure it is not
    over fit to the data so that the model can generalize well with new data and make
    good predictions. Usually, several rounds of cross validation are run iteratively.
    Each round of cross validation involves splitting the data into train and test
    sets; using the training data to train the model and then evaluating its performance
    with the test set. At the end of this, we get a model which is the best of the
    lot.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating predictive models
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most important part in predictive modeling is testing whether the models
    created are actually useful. This is done by evaluating the models on the testing
    data and using various metrics to measure the performance of the model. We will
    discuss some popular model evaluation techniques here. To explain the concepts
    clearly, we will consider an example with our data. Let us assume we have 100
    customers and 40 of them have a bad credit rating with class label 0 and the remaining
    60 have a good credit rating with class label 1 in the test data. Let us now assume
    that our model predicts 22 instances out of the 40 bad instances as bad and the
    remaining 18 as good. The model also predicts 40 instances out of the 60 good
    customers as good and the remaining 20 as bad. We will now see how we evaluate
    the model performance with different techniques:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction values**: They are usually discrete values which belong to a specific
    class or category and are often known as class labels. In our case, it is a binary
    classification problem where we deal with two classes where label 1 indicates
    customers with good credit rating and 0 indicates bad credit rating.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confusion matrix**: It is a nice way to see how the model is predicting the
    different classes. It is a contingency table with usually two rows and two columns
    for a binary classification problem like ours. It reports the number of predicted
    instances in each class against the actual class values. For our preceding example,
    the confusion matrix would be a 2x2 matrix where two rows would indicate the predicted
    class labels and two columns would indicate the actual class labels. The total
    number of predictions with the bad (0) class label which are actually having the
    bad label is called **True Negative** (**TN**) and the remaining bad instances
    wrongly predicted as good are called **False Positive** (**FP**). Correspondingly,
    the total number of predictions with the good (1) class label that are actually
    labeled as good are called **True Positive** (**TP**) and the remaining good instances
    wrongly predicted as bad are called **False Negative** (**FN**).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will depict this in the following figure and discuss some important metrics
    derived from the confusion matrix, also depicted in the same figure:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Evaluating predictive models](img/00163.jpeg)'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'In the preceding figure, the values which are highlighted in the 2x2 matrix
    are the ones which were correctly predicted by our model. The ones in white were
    incorrectly predicted by the model. We can therefore infer the following measures
    quite easily: TN is 22, **FP** is **18**, **TP** is **40**, and **FN** is **20**.
    Total **N** is **40** and total P is **60**, which add up to 100 customers in
    our example dataset.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '**Specificity** is also known as **true negative rate**, and can be represented
    by the formula ![Evaluating predictive models](img/00164.jpeg), which gives us
    the proportion of total true negatives correctly predicted by the total number
    of instances which are actually negative. In our case, we have a specificity of
    **55%**.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitivity**, also known as **true positive rate** and **recall**, has the
    formula ![Evaluating predictive models](img/00165.jpeg), which indicates the proportion
    of total true positives correctly predicted by the total number of instances which
    are actually positive. Our example has a sensitivity of **67%**.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**, also known as **positive predictive value**, has the formula
    ![Evaluating predictive models](img/00166.jpeg), which indicates the number of
    actual positive instances out of all the positive predictions. Our example has
    a precision of **69%**.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '**Negative** **predictive value** has the formula ![Evaluating predictive models](img/00167.jpeg),
    which indicates the number of actual negative instances out of all the negative
    predictions. Our example has an NPV of **52%**.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '**False** **positive rate**, also known as **fall-out**, is basically the inverse
    of specificity; where the formula is ![Evaluating predictive models](img/00168.jpeg),
    which indicates the number of false positive predictions out of all the negative
    instances. Our example has an FPR of **45%**.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '**False** **Negative Rate**, also known as **miss rate**, is basically the
    inverse of sensitivity; where the formula is ![Evaluating predictive models](img/00169.jpeg),
    which indicates the number of false negative predictions out of all the positive
    instances. Our example has an FNR of **33%**.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy** is basically the metric which denotes how accurate the model is
    in making predictions, where the formula is ![Evaluating predictive models](img/00170.jpeg).
    Our prediction accuracy is **62%**.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '**F1** score is another metric of measuring a model''s accuracy. It takes into
    account both the precision and recall values by computing the harmonic mean of
    the values, depicted by the formula ![Evaluating predictive models](img/00171.jpeg).
    Our model has an **f1** score of **68%**.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'A **Receiver** **Operator Characteristic** (**ROC**) curve is basically a plot
    which is used to visualize the model performance as we vary its threshold. The
    ROC plot is defined by the FPR and TPR as the *x* and *y* axes respectively, and
    each prediction sample can be fit as a point in the ROC space. Perfect plot would
    involve a TPR of 1 and an FPR of 0 for all the data points. An average model or
    a baseline model would be a diagonal straight line from *(0, 0)* to *(1, 1)* indicating
    both values to be `0.5`. If our model has an ROC curve above the base diagonal
    line, it indicates that it is performing better than the baseline. The following
    figure explains how a typical ROC curve looks like in general:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating predictive models](img/00172.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: '**Area under curve** (**AUC**) is basically the area under the ROC curve obtained
    from the model evaluation. The AUC is a value which indicates the probability
    that the model will rank a randomly chosen positive instance higher than a randomly
    chosen negative one. Therefore, the higher the AUC, the better it is. Do check
    out the file `performance_plot_utils.R` (shared with the code bundle of the chapter),
    which has some utility functions to plot and depict these values that we will
    be using later when we evaluate our model.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: This should give you enough background on important terms and concepts related
    to predictive modeling, and now we will start with our predictive analysis on
    the data!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 5](part0038_split_000.html#147LC1-973e731d75c2419489ee73e3a0cf4be8
    "Chapter 5. Credit Risk Detection and Prediction – Descriptive Analytics"), *Credit
    Risk Detection and Prediction – Descriptive Analytics*, we had analyzed the credit
    dataset from the German bank and performed several transformations already. We
    will be working on that transformed dataset in this chapter. We had saved the
    transformed dataset which you can check out by opening the `credit_dataset_final.csv`
    file. We will be doing all our analysis in R as usual. To load the data in memory,
    run the following code snippet:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This loads the dataset into a data frame which can now be readily accessed using
    the `credit.df` variable. Next, we will focus on data transformation and normalization.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the data preprocessing step, we will be focusing on two things mainly: data
    type transformations and data normalization. Finally we will split the data into
    training and testing datasets for predictive modeling. You can access the code
    for this section in the `data_preparation.R` file. We will be using some utility
    functions, which are mentioned in the following code snippet. Remember to load
    them up in memory by running them in the R console:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding functions operate on the data frame to transform the data. For
    data type transformations, we mainly perform factoring of the categorical variables,
    where we transform the data type of the categorical features from numeric to factor.
    There are several numeric variables, which include `credit.amount`, `age`, and
    `credit.duration.months`, which all have various values and if you remember the
    distributions in the previous chapter, they were all skewed distributions. This
    has multiple adverse effects, such as induced collinearity, gradients being affected,
    and models taking longer times to converge. Hence, we will be using z-score normalization,
    where each value represented by, let''s say, ![Data preprocessing](img/00173.jpeg),
    for a feature named E, can be calculated using the formula ![Data preprocessing](img/00174.jpeg)
    where ![Data preprocessing](img/00175.jpeg) represents the overall mean and ![Data
    preprocessing](img/00176.jpeg) represents the standard deviation of the feature
    `E`. We use the following code snippet to perform these transformations on our
    data:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once the preprocessing is complete, we will split our data into training and
    test datasets in the ratio of 60:40, where 600 tuples will be in the training
    dataset and 400 tuples will be in the testing dataset. They will be selected in
    a random fashion as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have our datasets ready, we will explore feature importance and
    selection in the following section.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of feature selection involves ranking variables or features according
    to their importance by training a predictive model using them and then trying
    to find out which variables were the most relevant features for that model. While
    each model often has its own set of important features, for classification we
    will use a random forest model here to try and figure out which variables might
    be of importance in general for classification-based predictions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'We perform feature selection for several reasons, which include:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Removing redundant or irrelevant features without too much information loss
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preventing overfitting of models by using too many features
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing variance of the model which is contributed from excess features
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing training time and converging time of models
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building simple and easy to interpret models
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be using a recursive feature elimination algorithm for feature selection
    and an evaluation algorithm using a predictive model where we repeatedly construct
    several machine learning models with different features in different iterations.
    At each iteration, we keep eliminating irrelevant or redundant features and check
    the feature subset for which we get maximum accuracy and minimum error. Since
    this is an iterative process and follows the principle of the popular greedy hill
    climbing algorithm, an exhaustive search with a global optima outcome is generally
    not possible and depending on the starting point, we may end up at local optima
    with a subset of features which may be different from the subset of features we
    obtain in a different run. However, most of the features in the obtained subset
    will usually be constant if we run it several times using cross-validation. We
    will use the random forest algorithm, which we will explain in more detail later
    on. For now, just remember it is an ensemble learning algorithm that uses several
    decision trees at each stage in its training process. This tends to reduce variance
    and overfitting with a small increase towards bias of the model since we introduce
    some randomness into this process at each stage in the algorithm.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this section is present in the `feature_selection.R` file. We
    will first load the necessary libraries. Install them in case you do not have
    them installed, as we did in the previous chapters:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: By default, the preceding code uses cross-validation where the data is split
    into training and test sets. For each iteration, recursive feature elimination
    takes place and the model is trained and tested for accuracy and errors on the
    test set. The data partitions keep changing randomly for every iteration to prevent
    overfitting of the model and ultimately give a generalized estimate of how the
    model would perform in a generic fashion. If you observe, our function runs it
    for 20 iterations by default. Remember, in our case, we always train on the training
    data which is internally partitioned for cross-validation by the function. The
    variable `feature.vars` indicate all the independent feature variables that can
    be accessed in the training dataset using the `train.data[,-1]` subsetting, and
    to access the `class.var`,which indicates the class variable to be predicted,
    we subset using `train.data[,1]`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We do not touch the test data at all because we will be using it only for predictions
    and model evaluations. Therefore, we would not want to influence the model by
    using that data since it would lead to incorrect evaluations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'We now run the algorithm using our defined function on the training data using
    the following code. It may take some time to run, so be patient if you see that
    R is taking some time to return the results:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'On viewing the results, we get the following output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature selection](img/00177.jpeg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: From the output, you can see that it has found a total of 10 features that were
    the most important out of the 20 and it has returned the top five features by
    default. You can play around with this result variable even further and see all
    the variables with their importance by using the `varImp(rfe.results)` command
    in the R console. The values and importance values may differ for you because
    the training and test data partitions are done randomly, if you remember, so do
    not panic if you see different values from the screenshot. However, the top five
    features will usually remain consistent based on our observations. We will now
    start building predictive models using the different machine learning algorithms
    for the next stage of our analytics pipeline. However, do remember that since
    the training and test sets are randomly chosen, your sets might give slightly
    different results than what we depict here when we performed these experiments.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Modeling using logistic regression
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is a type of regression model where the dependent or class
    variable is not continuous but categorical, just as in our case, credit rating
    is the dependent variable with two classes. In principle, logistic regression
    is usually perceived as a special case of the family of generalized linear models.
    This model functions by trying to find out the relationship between the class
    variable and the other independent feature variables by estimating probabilities.
    It uses the logistic or sigmoid function for estimating these probabilities. Logistic
    regression does not predict classes directly but the probability of the outcome.
    For our model, since we are dealing with a binary classification problem, we will
    be dealing with binomial logistic regression.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will load the library dependencies as follows and separate the testing
    feature and class variables:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we will train the initial model with all the independent variables as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can view the model details using the `summary(lr.model)` command, which
    shows you the various variables and their importance based on their significance
    values. We show a part of these details in the following snapshot:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using logistic regression](img/00178.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: You can see that the model automatically performs one-hot encoding of categorical
    variables, which is basically having a variable for each category in that variable.
    The variables with stars beside them have p-values `< 0.05` (which we discussed
    in the previous chapter) and are therefore significant.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we perform predictions on the test data and evaluate the results as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: On running this, we get a confusion matrix with associated metrics, which we
    discussed earlier, which are shown in the following figure. It is quite interesting
    to see that we achieved an overall accuracy of **71.75%**, which is quite decent,
    considering this dataset has a majority of good credit rating customers. It is
    predicting bad credit ratings quite well, which is evident from the **specificity**
    of **48%**. **Sensitivity** is **83%**, which is quite good, **NPV** is **58%**,
    and **PPV** is **76%**.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using logistic regression](img/00179.jpeg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: 'We will now try to build another model with some selected features and see
    how it performs. If you remember, we had some generic features that are important
    for classification, which we obtained in the earlier section on feature selection.
    We will still run feature selection specifically for logistic regression to see
    feature importance using the following code snippet:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We get the following plot from which we select the top five variables to build
    the next model. As you can see, reading the plot is pretty simple. The greater
    the importance, the more important the variable is. Feel free to add more variables
    and build different models using them!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using logistic regression](img/00180.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: 'Next, we build the model using a similar approach to before and test the model
    performance on the test data using the following code snippet:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We get the following confusion matrix. However, if you look at the model evaluation
    results, as shown in following output, you will see that now accuracy has slightly
    increased and is **72.25%**. **Sensitivity** has shot up to **94%**, which is
    excellent, but sadly this has happened at the cost of specificity, which has gone
    down to **27%**, and you can clearly see that more bad credit ratings are being
    predicted as good, which is 95 out of the total 130 bad credit rating customers
    in the test data! **NPV** has gone up to **69%** because fewer positive credit
    ratings are being misclassified as false negatives because of higher sensitivity.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using logistic regression](img/00181.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: Now comes the question of which model we want to select for predictions. This
    does not solely depend on the accuracy but on the domain and business requirements
    of the problem. If we predict a customer with a **bad credit rating** (**0**)
    as **good** (**1**), it means we are going to approve the credit loan for the
    customer who will end up not paying it, which will cause losses to the bank. However,
    if we predict a customer with **good credit rating** (**1**) as **bad** (**0**),
    it means we will deny him the loan in which case the bank will neither profit
    nor will incur any losses. This is much better than incurring huge losses by wrongly
    predicting bad credit ratings as good.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we choose our first model as the best one and now we will view some
    metric evaluation plots using the following code snippet:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We get the following plots from the preceding code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using logistic regression](img/00182.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: You can see from the preceding plot that the **AUC** is **0.74**, which is pretty
    good for a start. We will now build the next predictive model using support vector
    machines using a similar process and see how it fares.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Modeling using support vector machines
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Support vector machines belong to the family of supervised machine learning
    algorithms used for both classification and regression. Considering our binary
    classification problem, unlike logistic regression, the SVM algorithm will build
    a model around the training data in such a way that the training data points belonging
    to different classes are separated by a clear gap, which is optimized such that
    the distance of separation is the maximum. The samples on the margins are typically
    called the support vectors. The middle of the margin which separates the two classes
    is called the optimal separating hyperplane.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Data points on the wrong side of the margin are weighed down to reduce their
    influence and this is called the soft margin compared to the hard margins of separation
    we discussed earlier. SVM classifiers can be simple linear classifiers where the
    data points can be linearly separated. However, if we are dealing with data consisting
    of several features such that a linear separation is not possible directly, then
    we make use of several kernels to achieve the same and these form the non-linear
    SVM classifiers. You will be able to visualize how an SVM classifier actually
    looks much better with the following figure from the official documentation for
    the `svm` library in R:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using support vector machines](img/00183.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf](https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, you can clearly see that we can place multiple hyperplanes
    separating the data points. However, the criterion for choosing the separating
    hyperplane is such that the distance of separation from the two classes is the
    maximum and the support vectors are the representative samples of the two classes
    as depicted on the margins. Revisiting the issue of non-linear classifiers, SVM
    has several kernels which can be used to achieve this besides the regular linear
    kernel used for linear classification. These include polynomial, **radial basis**
    **function** (**RBF**), and several others. The main principle behind these non-linear
    kernel functions is that, even if linear separation is not possible in the original
    feature space, they enable the separation to happen in a higher dimensional transformed
    feature space where we can use a hyperplane to separate the classes. An important
    thing to remember is the curse of dimensionality that applies here; since we may
    end up working with higher dimensional feature spaces, the model generalization
    error increases and the predictive power of the model decreases. It we have enough
    data, it still performs reasonably. We will be using the RBF kernel, also known
    as the radial basis function, in our model and for that two important parameters
    are cost and gamma.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the necessary dependencies and preparing the testing
    data features:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once this is done, we build the SVM model using the training data and the RBF
    kernel on all the training set features:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The properties of the model are generated as follows from the `summary` function:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using support vector machines](img/00184.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'Now we use our testing data on this model to make predictions and evaluate
    the results as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This gives us the following confusion matrix like we saw in logistic regression
    and the details are depicted for the model performance. We observe that the **accuracy**
    is **67.5%**, **sensitivity** is **100%**, and **specificity** is **0%**, which
    means that it is a very aggressive model which just predicts every customer rating
    as good. This model clearly suffers from the major class classification problem
    and we need to improve this.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using support vector machines](img/00185.jpeg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: 'To build a better model, we need some feature selection. We already have the
    top five best features which we had obtained in the *Feature selection* section.
    Nevertheless, we will still run a feature selection algorithm specifically for
    SVM to see feature importance, as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This gives us a plot and we see that the top five important variables are similar
    to our top five best features, except this algorithm ranks age as more important
    than `credit.amount`, so you can test this by building several models with different
    features and see which one gives the best results. For us, the features selected
    from random forests gave a better result. The variable importance plot is depicted
    as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using support vector machines](img/00186.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'We now build a new SVM model based on the top five features that gave us the
    best results and evaluate its performance on the test data using the following
    code snippet:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Modeling using support vector machines](img/00187.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: 'We will definitely select this model and move on to model optimization by hyperparameter
    tuning using a grid search algorithm as follows to optimize the cost and gamma
    parameters:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Output**:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using support vector machines](img/00188.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'The grid search plot can be viewed as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Output:**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using support vector machines](img/00189.jpeg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'The darkest region shows the parameter values which gave the best performance.
    We now select the best model and evaluate it once again as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'On observing the confusion matrix results we obtained from the following output
    (we are henceforth depicting only the metrics which we are tracking), we see that
    the overall **accuracy** has increased to **71%**, **sensitivity** to **86%**,
    and **specificity** to **41%**, which is excellent compared to the previous model
    results:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using support vector machines](img/00190.jpeg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'You see how powerful hyperparameter optimizations can be in predictive modeling!
    We also plot some evaluation curves as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can see how the predictions are plotted in the evaluation space, and we
    see that the AUC in this case is 0.69 from the following ROC plot:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using support vector machines](img/00191.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s say we want to optimize the model based on this ROC plot with the
    objective of maximizing the AUC. We will try that now, but first we need to encode
    the values of the categorical variables to include some letters because R causes
    some problems when representing column names of factor variables that have only
    numbers. So basically, if `credit.rating` has values `0`, `1` then it gets transformed
    to **X0** and **X1**; ultimately our categories are still distinct and nothing
    changes. We transform our data first with the following code snippet:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we build an AUC optimized model using grid search again, as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Our next step is to perform predictions on the test data and evaluate the confusion
    matrix:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This gives us the following results:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using support vector machines](img/00192.jpeg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 'We see now that **accuracy** has increased further to **72%** and **specificity**
    has decreased slightly to **40%**, but **sensitivity** has increased to **87%**,
    which is good. We plot the curves once again, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This gives us the following plots, the same as we did in our earlier iterations:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using support vector machines](img/00193.jpeg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: It is quite pleasing to see that the AUC has indeed increased from 0.69 earlier
    to 0.74 now, which means the AUC based optimization algorithm indeed worked, since
    it has given better performance than the previous model in all the aspects we
    have been tracking. Up next, we will look at how to build predictive models using
    decision trees.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Modeling using decision trees
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are algorithms which again belong to the supervised machine learning
    algorithms family. They are also used for both classification and regression,
    often called **CART**, which stands for **classification and regression trees**.
    These are used a lot in decision support systems, business intelligence, and operations
    research.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are mainly used for making decisions that would be most useful
    in reaching some objective and designing a strategy based on these decisions.
    At the core, a decision tree is just a flowchart with several nodes and conditional
    edges. Each non-leaf node represents a conditional test on one of the features
    and each edge represents an outcome of the test. Each leaf node represents a class
    label where predictions are made for the final outcome. Paths from the root to
    all the leaf nodes give us all the classification rules. Decision trees are easy
    to represent, construct, and understand. However, the drawback is that they are
    very prone to overfitting and often these models do not generalize well. We will
    follow a similar analytics pipeline as before, to build some models based on decision
    trees.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with loading the necessary dependencies and test data features:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we will build an initial model with all the features as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We predict and evaluate the model on the test data with the following code:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'From the following output, we see that the model **accuracy** is around **68%**,
    **sensitivity** is **92%**, which is excellent, but **specificity** is only **18%**,
    which we should try and improve:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using decision trees](img/00194.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'We will now try feature selection to improve the model. We use the following
    code to train the model and rank the features by their importance:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This gives us the following plot showing the importance of different features:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using decision trees](img/00195.jpeg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: 'If you observe closely, the decision tree does not use all the features in
    the model construction and the top five features are the same as those we obtained
    earlier when we talked about feature selection. We will now build a model using
    these features as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We now make predictions on the test data and evaluate it, as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This gives us the following confusion matrix with other metrics:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using decision trees](img/00196.jpeg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: You can see now that the overall model **accuracy** has decreased a bit to **62%**.
    However, we have increased our bad credit rating prediction where we predict a
    100 bad credit rating customers out of 130, which is excellent! Consequently,
    **specificity** has jumped up to **77%** and **sensitivity** is down to **55%**,
    but we still classify a substantial number of good credit rating customers as
    good. Though this model is a bit aggressive, it is a reasonable model because
    though we deny credit loans to more customers who could default on the payment,
    we also make sure a reasonable number of good customers get their credit loans
    approved.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: The reason we obtained these results is because we have built the model with
    a parameter called prior, if you check the modeling section earlier. This prior
    basically empowers us to apply weightages to the different classes in the class
    variable. If you remember, we had **700** people with a **good credit rating**
    and **300** people with a **bad credit rating** in our dataset, which was highly
    skewed, so while training the model, we can use prior to specify the importance
    of each of the classes in this variable and thus adjust the importance of misclassification
    of each class. In our model, we give more importance to the bad credit rating
    customers.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'You can reverse the priors and give more importance to the good rating customers
    by using the parameter as `prior = c(0.7, 0.3)`, which would give the following
    confusion matrix:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using decision trees](img/00197.jpeg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: You can clearly see now that, since we gave more importance to good credit ratings,
    the **sensitivity** has jumped up to **92%** and **specificity** has gone down
    to **18%**. You can see that this gives you a lot of flexibility over your modeling
    depending on what you want to achieve.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'To view the model, we can use the following code snippet:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Output**:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using decision trees](img/00198.jpeg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'To visualize the preceding tree, you can use the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This gives us the following tree, and we can see that, using the priors, the
    only feature that is being used now out of the five features is `account.balance`
    and it has ignored all the other features. You can try and optimize the model
    further by using hyperparameter tuning by exploring the `tune.rpart` function
    from the `e1071` package:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using decision trees](img/00199.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'We finish our analysis by plotting some metric evaluation curves as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The **AUC** is around **0.66**, which is not the best but definitely better
    than the baseline denoted by the red line in the following plot:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using decision trees](img/00200.jpeg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: Based on our business requirements, this model is quite fair. We will discuss
    model comparison later on in this chapter. We will now use random forests to build
    our next set of predictive models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Modeling using random forests
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests, also known as random decision forests, are a machine learning
    algorithm that comes from the family of ensemble learning algorithms. It is used
    for both regression and classification tasks. Random forests are nothing but a
    collection or ensemble of decision trees, hence the name.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'The working of the algorithm can be described briefly as follows. At any point
    in time, each tree in the ensemble of decision trees is built from a bootstrap
    sample, which is basically sampling with replacement. This sampling is done on
    the training dataset. During the construction of the decision tree, the split
    which was earlier being chosen as the best split among all the features is not
    done anymore. Now the best split is always chosen from a random subset of the
    features each time. This introduction of randomness into the model increases the
    bias of the model slightly but decreases the variance of the model greatly which
    prevents the overfitting of models, which is a serious concern in the case of
    decision trees. Overall, this yields much better performing generalized models.
    We will now start our analytics pipeline by loading the necessary dependencies:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we will build the initial training model with all the features as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can view the model details by using the following code:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Output**:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using random forests](img/00201.jpeg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: This gives us information about the **out of bag error** (**OOBE**), which is
    around **23%**, and the confusion matrix which is calculated on the training data,
    and also how many variables it is using at each split.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will perform predictions using this model on the test data and evaluate
    them:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We now make predictions with this model on the test data and evaluate its performance
    as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This gives us the following confusion matrix as the output with the other essential
    performance metrics:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using random forests](img/00203.jpeg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: We get a slightly decreased **accuracy** of **71%**, which is obvious because
    we have eliminated many features, but now the **specificity** has increased to
    **42%**, which indicates it is able to classify more bad instances correctly as
    bad. **Sensitivity** has decreased slightly to **84%**. We will now use grid search
    to perform hyperparameter tuning on this model as follows, to see if we can improve
    the performance further. The parameters of interest here include `ntree`, indicating
    the number of trees, `nodesize`, indicating the minimum size of terminal nodes,
    and `mtry`, indicating the number of variables sampled randomly at each split.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Output**:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using random forests](img/00204.jpeg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: 'We now get the best model from the preceding grid search, perform predictions
    on the test data, and evaluate its performance with the following code snippet:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can make several observations from the following output. Performance has
    improved very negligibly as the overall **accuracy** remains the same at **71%**
    and **specificity** at **42%**. **Sensitivity** has increased slightly to **85%**
    from **84%**:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using random forests](img/00205.jpeg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'We now plot some performance curves for this model, as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We observe that the total **AUC** is about **0.7** and is much better than
    the red baseline **AUC** of **0.5** in the following plot:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using random forests](img/00206.jpeg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: The last algorithm we will explore is neural networks and we will build our
    models using them in the following section.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Modeling using neural networks
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks, or to be more specific in this case, artificial neural networks,
    is a family of machine learning models whose concepts are based on the working
    of biological neural networks, just like our nervous system. Neural networks have
    been there for a long time, but recently there has been an upsurge of interest
    in building highly intelligent systems using deep learning and artificial intelligence.
    Deep learning makes use of deep neural networks, which are essentially neural
    networks with a huge number of hidden layers between the input and output layers.
    A typical neural network can be visualized with the following figure:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using neural networks](img/00207.jpeg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: From the figure, you can deduce that this neural network is an interconnected
    network of various nodes, also called neurons. Each node represents a neuron which
    is nothing but a mathematical function. It is impossible to go into every detail
    of how to represent a node mathematically but we will give the gist here. These
    mathematical functions receive one or more inputs with weights, which are represented
    in the preceding figure as edges, and then it performs some computation on these
    inputs to give an output. Various popular functions used in these nodes include
    step function and the sigmoid function, which you have already seen in use in
    the logistic regression algorithm. Once the inputs are weighed and transformed
    by the function, the activation of these functions is sent to further nodes until
    it reaches the output layer. A collection of nodes form a layer, just like in
    the earlier figure, we have three layers.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'So, a neural network depends on several neurons or nodes and the pattern of
    interconnection between them, the learning process that is used to update the
    weights of the connections at each iteration (popularly called as epoch), and
    the activation functions of the nodes that convert the node''s inputs with weights
    to its output activation, which is passed layer by layer till we get the output
    prediction. We will start with loading the necessary dependencies as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We will now have to do some feature value encoding, similar to what we did
    when we did AUC optimization for SVM. To refresh your memory, you can run the
    following code snippet:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once we have our data ready, we will build our initial neural network model
    using all the features as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We now perform predictions on the test data and evaluate the model performance
    as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: You can observe from the following output that our model has an **accuracy**
    of **72%**, which is quite good. It is predicting bad ratings as bad quite well,
    which is evident from the **specificity** which is **48%**, and as usual **sensitivity**
    is good at **84%**.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using neural networks](img/00208.jpeg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: 'We will now use the following code snippet to plot the features of importance
    for neural network based models:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This gives us the following plot ranking variables according to their importance:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using neural networks](img/00209.jpeg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: 'We select some of the most important features from the preceding plot and build
    our next model as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We now perform predictions on the test data and evaluate the model performance:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This gives us the following confusion matrix with various metrics of our interest.
    We observe from the following output that the **accuracy** has increased slightly
    to **73%** and **sensitivity** has now increased to **87%** at the cost of **specificity**,
    which has dropped to **43%**:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using neural networks](img/00210.jpeg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: 'You can check the hyperparameter tuning which it has done internally, as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The following plot shows the accuracy of the various models with different
    numbers of nodes in the hidden layer and the weight decay:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using neural networks](img/00211.jpeg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: 'Based on our requirement that the bank makes minimum losses, we select the
    best model as the initial neural network model that was built, since it has an
    accuracy similar to the new model and its specificity is much higher which is
    extremely important. We now plot some performance curves for the best model as
    follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We observe from the following plot that the **AUC** is **0.74**, which is quite
    good and performs a lot better than the baseline denoted in red:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling using neural networks](img/00212.jpeg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: This concludes our predictive modeling session and we will wrap it up with model
    selection and comparisons.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Model comparison and selection
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have explored various machine learning techniques and built several models
    to predict the credit ratings of customers, so now comes the question of which
    model we should select and how the models compare against each other. Our test
    data has 130 instances of customers with a **bad credit rating** (**0**) and 270
    customers with a **good credit rating** (**1**).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'If you remember, earlier we had talked about using domain knowledge and business
    requirements after doing modeling to interpret results and make decisions. Right
    now, our decision is to choose the best model to maximize profits and minimize
    losses for the German bank. Let us consider the following conditions:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: If we incorrectly predict a customer with bad credit rating as good, the bank
    will end up losing the whole credit amount lent to him since he will default on
    the payment and so loss is 100%, which can be denoted as -1 for our ease of calculation.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we correctly predict a customer with bad credit rating as bad, we correctly
    deny him a credit loan and so there is neither any loss nor any profit.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we correctly predict a customer with good credit rating as good, we correctly
    give him the credit loan. Assuming the bank has an interest rate on the sum of
    money lent, let us assume the profit is 30% from the interest money that is paid
    back monthly by the customer. Therefore, profit is denoted as 30% or +0.3 for
    our ease of calculation.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we incorrectly predict a customer with good credit rating as bad, we incorrectly
    deny him the credit loan but there is neither any profit nor any loss involved
    in this case.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keeping these conditions in mind, we will make a comparison table for the various
    models, including some of the metrics we had calculated earlier for the best model
    for each machine learning algorithm. Remember that considering all the model performance
    metrics and business requirements, there is no one model that is the best among
    them all. Each model has its own set of good performance points, which is evident
    in the following analysis:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![Model comparison and selection](img/00213.jpeg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: The cells highlighted in the preceding table show the best performance for that
    particular metric. As we mentioned earlier, there is no best model and we have
    listed down the models that have performed best against each metric. Considering
    the total overall gain, decision tree seems to be the model of choice. However,
    this is assuming that the credit loan amount requested is constant per customer.
    Remember that if each customer requests loans of different amounts then this notion
    of total gain cannot be compared because then the profit from one loan might be
    different to another and the loss incurred might be different on different loans.
    This analysis is a bit complex and out of the scope of this chapter, but we will
    mention briefly how this can be computed. If you remember, there is a `credit.amount`
    feature, which specifies the credit amount requested by the customer. Since we
    already have the customer numbers in the training data, we can aggregate the rated
    customers with their requested amount and sum up the ones for which losses and
    profits are incurred, and then we will get the total gain of the bank for each
    method!
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explored several important areas in the world of supervised learning in this
    chapter. If you have followed this chapter from the beginning of our journey and
    braved your way till the end, give yourself a pat on the back! You now know what
    constitutes predictive analytics and some of the important concepts associated
    with it. Also, we have seen how predictive modeling works and the full predictive
    analytics pipeline in actual practice. This will enable you to build your own
    predictive models in the future and start deriving valuable insights from model
    predictions. We also saw how to actually use models to make predictions and evaluate
    these predictions to test model performance so that we can optimize the models
    further and then select the best model based on metrics as well and business requirements.
    Before we conclude and you start your own journey into predictive analytics, I
    will like to mention that you should always remember Occam's razor, which states
    that *Among competing hypotheses, the one with the fewest assumptions should be
    selected,* which can be also interpreted as *Sometimes, the simplest solution
    is the best one*. Do not blindly jump into building predictive models with the
    latest packages and techniques, because first you need to understand the problem
    you are solving and then start from the simplest implementation, which will often
    lead to better results than most complex solutions.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
