- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Future Trends and Developments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Intelligence will drive the next generation of technologies, not big data. Big
    data systems have some issues, as discussed in [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017),
    *Challenges in Big Data and Traditional AI*, and the world is gradually transitioning
    from the data-centric era to the intelligence-centric generation. **Federated
    learning** (**FL**) will play a core role in wisdom-driven technologies. Thus,
    the time is now to welcome the world of collective intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will talk about the direction of future AI technologies
    that are driven by the paradigm shift happening with FL. For many AI fields, such
    as privacy-sensitive areas and fields requiring scalability in **machine learning**
    (**ML**), the benefits and potential of FL are already significant, mainly because
    of the privacy-preserving and distributed learning aspects that FL naturally supports
    with its design. You will then learn about the different types of FL as well as
    the latest development efforts in that area, as seen in the split and swarm learning
    techniques, which can be considered as evolutional frameworks enhancing FL.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, FL creates a new concept of an *Internet of Intelligence*, where
    people and computers exchange their wisdom instead of just data themselves. The
    Internet of Intelligence for everyone is further accelerated by blockchain technologies
    as well. This Internet of Intelligence can then form a newly defined concept of
    *collective intelligence* that drives another innovation, from *data-centric*
    approaches to *intelligence-centric* or *model-centric* approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will share a collective vision in which FL plays a key role in collaboratively
    creating intelligence learned by many people and machines around the world.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at future AI trends
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ongoing research and developments in FL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Journeying on to collective intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at future AI trends
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The majority of industry leaders are now aware of the limitations of centralized
    ML as discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The limitation of centralized ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When looking at the future of AI, it is important to first know the fact that
    many companies today are struggling to extract intelligence and obtain insight
    from the data they possess. More than half of the data that organizations and
    companies have collected is usually not used. Traditional approaches to machine
    learning and data science need data to be organized and consolidated into data
    lakes and stores in advance of analyzing and training ML models. You need to duplicate
    and move the data, which will result in delays in realizing and delivering the
    value of the intelligence extracted from the data, together with certain operational
    risks and complexities.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, most of the data generated by enterprise companies will be created
    and processed outside a traditional centralized data center or cloud. It is becoming
    increasingly unrealistic and inefficient to process data for generating insight
    in a centralized manner.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, based on some market reports out there, most of the largest global
    organizations and companies will explore FL at least once to create much more
    accurate, secure, and sustainable models environmentally.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, quite a few industries and markets are gradually becoming aware
    of the importance of a distributed and FL paradigm, because they are facing the
    unavoidable issues and limitations of the current centralized AI training with
    big data, as discussed in [*Chapter 1*](B18369_01.xhtml#_idTextAnchor017), *Challenges
    in Big Data and Traditional AI*. FL brings the model to the data where the training
    process resides instead of bringing the data to the model. Thus, FL is considered
    to be the future of data science and ML.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let’s summarize the points of why FL is beneficial to those
    companies, especially enterprises that have been facing the aforementioned issues.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the benefits of FL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will summarize the benefits of FL that have been introduced
    throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Increased model accuracy and generalizability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FL realizes collaborative and distributed learning that can improve the performance
    of ML models, by training on dispersed datasets locally to continuously incorporate
    the learning into a global model. This way, more accurate and generalized ML models
    can be produced.
  prefs: []
  type: TYPE_NORMAL
- en: Further privacy and security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FL provides privacy and security advantages because it won’t require private
    and raw data by its design and security mechanisms, as we discussed previously
    in [*Chapter 2*](B18369_02.xhtml#_idTextAnchor037), *What Is Federated Learning?*
    and [*Chapter 9*](B18369_09.xhtml#_idTextAnchor224), *Case Studies with Key Use
    Cases of Federated Learning Applications*. Thus, FL reduces the potential risk
    of data misuse, leakage, or exposure to sensitive information. FL is also compliant
    with many privacy regulations, such as **General Data Protection Regulation**
    (**GDPR**), **California Consumer Privacy Act** (**CCPA**), and **Health Insurance
    Portability and Accountability Act** (**HIPAA**).
  prefs: []
  type: TYPE_NORMAL
- en: Improved speed and efficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FL is also known to realize high computation efficiency, which can accelerate
    the deployment and testing of ML models as well as decrease communication and
    computational latency. Due to the decentralized nature of FL, the delay for model
    delivery and update is minimized, which leads to a prediction by the global model
    in near real time. Real-time delivery and updates of intelligence are really valuable
    for time-sensitive ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: FL also helps reduce bandwidth and energy consumption by overcoming system heterogeneity
    and unbalanced data distribution, which leads to minimizing data storage and transfer
    costs that can also significantly contribute to reducing the environmental impact.
  prefs: []
  type: TYPE_NORMAL
- en: Toward distributed learning for further privacy and training efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently, AI is trained on huge computational servers, usually happening on
    big machines in big data companies.
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the era of the supercomputer, which can process a huge amount of
    data and tasks within one machine or one cluster of machines, the evolutional
    process in technology starts from a central location and gradually transitions
    to distributed environments.
  prefs: []
  type: TYPE_NORMAL
- en: The same thing is exactly about to happen in AI. Now, the data lake concept
    is popular to organize and train ML models in one place, but ML already requires
    distributed learning frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: FL is a great way to distribute a training process over multiple nodes. As shown
    in many research reports, most data is not fully used to extract insights into
    ML models.
  prefs: []
  type: TYPE_NORMAL
- en: There are some companies and projects that are trying to use FL as a powerful
    distributed learning technique, such as the platforms provided by Devron ([devron.ai](https://devron.ai)),
    FedML ([fedml.ai](https://fedml.ai)), and STADLE ([stadle.ai](https://stadle.ai)).
    These platforms are already resolving the issues discussed in *The limitation
    of centralized AI* section and have shown a drastic improvement in the ML process
    in various use cases, as stated in the *Revisiting the benefits of FL* section.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the AI trends that we have discussed, let’s look into the ongoing research
    and developments related to FL that cutting-edge companies are conducting now
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Ongoing research and developments in FL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now talk about the ongoing research and development projects that are being
    taken place both in academia and industries around the world. Let’s start with
    the different types and approaches of FL, and move on to ongoing efforts to further
    enhance the FL framework.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring various FL types and approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this book, we have visited the most basic algorithms and design concepts
    of an FL system. In the real world, we need to dig a bit deeper into what types
    of FL frameworks are available to extract the best performance out of those algorithms.
    Depending on the data scenario and use cases, we have several approaches in FL,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal FL and vertical FL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized FL and decentralized FL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-silo FL and cross-device FL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s look at each type of FL in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal FL and vertical FL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Horizontal FL** uses datasets with the same feature space or schema across
    all distributed devices ([https://www.arxiv-vanity.com/papers/1902.04885/](https://www.arxiv-vanity.com/papers/1902.04885/)).
    This actually means that datasets share the same columns with different rows.
    Most existing FL projects are based on horizontal FL. Datasets and training processes
    with horizontal FL are straightforward because the datasets are formed identically,
    with different data distributions and inputs to be learned. Horizontal FL is also
    called homogeneous or sample-based FL.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertical FL** is applied to the cases where different datasets share the
    same sample ID space but differ in feature space. You can check out this paper
    (https://arxiv.org/pdf/2202.04309) for further information about vertical FL.
    Relating these different databases through FL can be challenging, especially if
    the unique ID for the data is different. The key idea of vertical FL is to improve
    an ML model by using distributed datasets with a diverse set of attributes. Therefore,
    vertical FL can handle the partitioned data vertically with different attributes
    in the same sample space. Vertical FL is also called heterogeneous or feature-based
    FL.'
  prefs: []
  type: TYPE_NORMAL
- en: Centralized FL and decentralized FL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Centralized FL** is currently the most common approach and most of the platforms
    employ this framework. It uses a centralized server to collect and aggregate the
    different ML models, with distributed training across all local data sources.
    In this book, we focused on a centralized FL approach, with a scenario where local
    training agents communicate the learning results to a centralized FL server to
    create a global model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decentralized FL**, on the other hand, does not use a centralized server
    to aggregate ML models. It requires individual ML models trained over local data
    sources to be communicated among themselves without a master node. In this case,
    model weights are transferred from each individual dataset to the others for further
    training. It could potentially be susceptible to model poisoning if an untrusted
    party could access the intelligence, and this is a common problem derived from
    peer-to-peer frameworks as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-silo FL and cross-device FL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Cross-silo FL** is the case where ML models are trained on data distributed
    across any functional, organizational, and regulatory barriers. In this case,
    big data is usually stored in a larger size of storage, with training computing
    capabilities such as cloud virtual machines. In the cross-silo FL case, the number
    of silos/training environments is relatively small, so not so many agents are
    needed in the FL process.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross-device FL** is the case where models need to be trained at scale, often
    within edge devices, such as mobile phones, **Internet of Things** (**IoT**) devices,
    Raspberry Pi-type environments, and so on. In this case, a huge number of devices
    are connected for the aggregation of ML models. In the cross-device FL case, the
    limitation basically lies in the low computing power of those edge devices. The
    framework also needs to handle a number of disconnected and inactive devices to
    conduct a consistent and continuous FL process. The training process and its data
    volume should be limited too.'
  prefs: []
  type: TYPE_NORMAL
- en: That concludes the different types of FL that can be applied to a variety of
    scenarios in ML applications. There are new techniques that try to enhance the
    FL framework to evolve into the next generation of AI technologies with FL. Let’s
    look into several advanced approaches in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding enhanced distributed learning frameworks with FL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are ongoing efforts to further enhance FL or distributed learning frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Split learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Split learning**, developed in the MIT Media Lab, is an emerging distributed
    learning technique that enables partitioning ML models into multiple sections,
    trains those partitioned ML models at distributed clients, and aggregates them
    at the end. Split learning does not have to share the data either, so it is considered
    a privacy-preserving AI as well.'
  prefs: []
  type: TYPE_NORMAL
- en: The overall framework is similar to the FL. However, there is a difference in
    that the neural network is partitioned into multiple sections that will be trained
    on distributed clients. The trained weights of the section of the neural network
    are then transferred to the server and clients. The weights of those multiple
    sections are continuously trained in the next training sessions. Therefore, no
    raw and private data is shared among the distributed clients, and only the weights
    of each section are sent to the next client.
  prefs: []
  type: TYPE_NORMAL
- en: Especially, **SplitFed** ([https://arxiv.org/abs/2004.12088](https://arxiv.org/abs/2004.12088))
    is another advanced technique that combines split learning and FL. SplitFed splits
    the deep neural network architecture between the FL clients and servers to realize
    a higher level of privacy than FL. It offers better efficiency than split learning
    based on the parallel learning paradigm of FL.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Swarm learning** is a decentralized ML solution built on blockchain technology,
    particularly designed to enable enterprise industries to take advantage of the
    power of distributed data, which results in protecting data privacy and security.'
  prefs: []
  type: TYPE_NORMAL
- en: This can be achieved by individual nodes sharing parameters of ML models derived
    from the local data.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters shared from the distributed clients are merged into a global model.
    The difference from the normal FL is that the merge process is not performed by
    a central server. The distributed nodes and clients choose a temporary leader
    to perform the merge. That is why swarm learning is truly decentralized, also
    providing greater fault tolerance and resiliency. The distributed agents have
    the collective intelligence of a network without sharing local data into one node.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm learning builds on top of blockchain. Blockchain provides the decentralized
    control, scalability, and fault-tolerance aspects to work beyond the restrictions
    of a single enterprise. At the same time, blockchain introduces a tamperproof
    cryptocurrency framework, and the participants can use the framework to monetize
    their contributions.
  prefs: []
  type: TYPE_NORMAL
- en: BAFFLE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition, there is a framework called **BAFFLE** that stands for **Blockchain
    Based Aggregator Free Federated Learning** (https://arxiv.org/abs/1909.07452).
    BAFFLE is also an aggregator-free, blockchain-driven FL framework that is inherently
    decentralized. BAFFLE utilizes **Smart Contracts** (**SCs**) from the blockchain
    framework to coordinate round management, as well as model aggregation and updating
    tasks of FL. Using BAFFLE boosts computational performance. The global model is
    also decomposed into many sets of chunks, directly handled by the SC.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about the latest research and developments in the FL
    field, in the next section, let’s look at a more visionary aspect of the AI, science,
    and technologies of collective intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Journeying on to collective intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data has been a game changer for the AI movement. While the amount of data
    generated at the edge and by people will increase exponentially, intelligence
    derived from that data benefits society. Therefore, the big data era will gradually
    pass the baton to the collective intelligence era, empowered by FL, in which people
    will collaboratively create a wisdom-driven world.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by defining an intelligence-centric era where the concept of collective
    intelligence is realized based on FL.
  prefs: []
  type: TYPE_NORMAL
- en: Intelligence-centric era with collective intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Collective Intelligence** (**CI**) is the concept of a large group of single
    entities acting together in ways that seem intelligent. CI is an emergent phenomenon
    where groups of people process information to achieve insights that are not understandable
    by just individual members alone.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Thomas Malone, the head of the MIT Center for Collective Intelligence,
    and the person who initially coined the phrase *collective intelligence*, broadened
    the definition of CI: *“CI is something that can emerge from a group that includes
    people and computers. CI is a very general property, and superminds can arise
    in many kinds of systems, although the systems I’ve mostly talked about are those
    that involve people and computers”* (Reference: [https://www2.deloitte.com/xe/en/insights/focus/technology-and-the-future-of-work/human-and-machine-collaboration.html](https://www2.deloitte.com/xe/en/insights/focus/technology-and-the-future-of-work/human-and-machine-collaboration.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: We are now welcoming the new perspective of CI in technologies empowered by
    FL.
  prefs: []
  type: TYPE_NORMAL
- en: Data, in the current world of technology, is a great source to extract intelligence.
    Dispersed datasets around the world can be converted into a collection of intelligence
    represented by AI technologies. The current trend, as mentioned, is big data,
    so big data companies are leading not only the technology industries but also
    the entire economy of the world as well. The future is moving in a CI direction.
    The vision of CI is even clearer with the emergence of sophisticated ML algorithms,
    including deep learning, as the intelligence represented by ML models can extract
    intelligence from people, computers, or any devices that generate meaningful data.
  prefs: []
  type: TYPE_NORMAL
- en: Why does FL promote the idea of CI? The nature of FL is to collect a set of
    distributed intelligence to be enhanced by an aggregating mechanism as discussed
    in this book. This itself enables a data-less platform that does not require collecting
    data from people or devices directly.
  prefs: []
  type: TYPE_NORMAL
- en: With the big data issues discussed throughout the book, we have steered clear
    of focusing on data-centric platform. However, it is also true that learning big
    data is very much critical and inevitable to really create systems and applications
    that are truly valuable and deliver real value in many domains of the world. That
    is why the big data field is still the most prosperous industry, even if it is
    facing significant challenges represented by privacy regulations, security, data
    silos, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Now is the time to further develop and disseminate the technologies such as
    FL that can accelerate the era of CI by fundamentally resolving the issues of
    big data. This way, we can realize a new era of technologies, truly driven by
    CI that has been backed up by an authentic mathematical basis.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, *data-centric* platforms are the current trend. So many data and
    auto ML vendors can support and automate the processes of creating ML-based intelligence
    by organizing data and learning procedures to do so. An *intelligence-centric*
    or *model-centric* platform should be the next wave of technology in which people
    can share and enhance intelligence that they generate on their own. With FL, we
    can even realize crowd-sourced learning, where people can collaboratively and
    continuously enhance the quality and performance of ML models. Thus, FL is a critical
    and essential part of the intelligence-centric platform to truly achieve a wisdom-driven
    world.
  prefs: []
  type: TYPE_NORMAL
- en: Internet of Intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The IoT evolved into the **Internet of Everything**. However, what is the essential
    information that people want? Is it just big data? Or intelligence derived from
    data? With 5G technologies, a lot of data can be transferred over the Internet
    at a much higher speed, partially resolving the latency issues in many AI applications.
    FL can exchange less information than raw data but still needs to transfer ML
    models over the Internet.
  prefs: []
  type: TYPE_NORMAL
- en: While lots of research projects are minimizing communications latency in FL,
    in the future, information related to intelligence will be another entity often
    exchanged over the web. There will be a model repository such as **Model Zoo**
    everywhere, and crowdsourced learning empowered by FL will be more common to create
    better intelligence over the Internet with people worldwide collaboratively.
  prefs: []
  type: TYPE_NORMAL
- en: This paradigm shift is not just in the AI field itself but also in the wide
    range of information technologies. As we’ll discuss in the next sections, this
    **Internet of Intelligence** movement will be the basis of crowdsourced learning
    and CI, and will help make intelligence available to as many people as possible
    in the coming years.
  prefs: []
  type: TYPE_NORMAL
- en: Crowdsourced learning with FL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *collection of intelligence* performed by FL naturally makes it a strong
    fit for moving toward CI. The same thing is applied to a scenario where people
    can collectively contribute a training process to global ML models.
  prefs: []
  type: TYPE_NORMAL
- en: High-performing ML models in areas such as computer vision and natural language
    processing have been trained by certain big data companies, often spending a huge
    amount of money, including hundreds of millions of dollars.
  prefs: []
  type: TYPE_NORMAL
- en: Is there any way to collectively train an ML model that will probably be beneficial
    for a wide range of people in general? With the advanced framework of FL, that
    is possible.
  prefs: []
  type: TYPE_NORMAL
- en: FL provides an authentic way to manage the aggregation of multiple trained models
    from various distributed agents. In this case, the distributed agents themselves
    may be people worldwide, where each individual user and trainer of the ML model
    has their own unique datasets that are not available to anybody else because of
    data privacy, silos, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This way of utilizing CI is often called *crowdsourced learning*. However, traditional
    crowdsourced learning is conducted in a much more limited way, just based on facilitating
    and recruiting data annotators at a large scale.
  prefs: []
  type: TYPE_NORMAL
- en: With this new paradigm with FL, users on the CI platform can access and download
    ML models that they are interested in and retrain them if necessary to absorb
    learning in their own environments. Then, with the framework to share the trained
    ML models by those users, an advanced aggregation framework of FL could pick up
    the appropriate models to be federated and make the global model perform better,
    adopting diverse data that can be only accessible to the users.
  prefs: []
  type: TYPE_NORMAL
- en: This way, intelligence by ML is becoming more available to many individuals
    in general, not just to specific companies that have a significant amount of data
    and budgets to train an authentic ML model. In other words, without an FL framework,
    collaborative learning is difficult and tricky and almost impossible to even automate.
    This openness of the ML models will move the entire technological world to the
    next level, and a lot more applications will become feasible, with truly powerful
    intelligence that is trained by enthusiasts to make the world better.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter of the book, we discussed fascinating future trends and
    developments in which FL is expected to play a crucial role in the coming decade.
    In the future, FL is a *must-to-have* technology from a *nice-to-have* framework
    for most enterprises and application providers, because of the inevitable privacy
    regulations and technology trends requiring scalability with so many users.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed, future technologies will be empowered by the concept of the
    Internet of Intelligence, by which people and computers mainly exchange their
    wisdom altogether to create a more intelligent society and world. Finally, the
    data-centric technologies will gradually evolve into intelligence-centric technologies
    because of the current collaborative learning trend with CI, which makes people
    pay significant attention to FL-related technologies, whose foundations are discussed
    throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: This book was written at the dawn of a new age in advancements made possible
    by AI. There are many uncertainties and many more challenges ahead. We have made
    great strides in utilizing the big data playbook in the last couple of decades,
    and we have now outgrown those methods and must adopt new ways of doing things,
    new technologies, and new ideas to forge ahead. As long as we capture the current
    moment and invest in new technologies such as FL, we will have a bright future
    ahead of us.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some sources if you wish to dive deeper into some concepts
    discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*UNDERSTANDING THE TYPES OF FEDERATED LEARNING*, posted by OpenMinded: [https://blog.openmined.org/federated-learning-types](https://blog.openmined.org/federated-learning-types'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Thapa, Chandra, et al. *SplitFed: When Federated Learning Meets Split Learning*,
    Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36\. No. 8\.
    2022: [https://arxiv.org/pdf/2004.12088.pdf](https://arxiv.org/pdf/2004.12088.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '*SWARM LEARNING: TURN YOUR DISTRIBUTED DATA INTO COMPETITIVE EDGE,* technical
    white paper: [https://www.labs.hpe.com/pdf/Swarm_Learning.pdf](https://www.labs.hpe.com/pdf/Swarm_Learning.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Paritosh Ramanan and Kiyoshi Nakayama. *BAFFLE: Blockchain based aggregator
    free federated learning*, 2020 IEEE International Conference on Blockchain (Blockchain).
    IEEE, 2020: [https://arxiv.org/pdf/1909.07452.pdf](https://arxiv.org/pdf/1909.07452.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix: Exploring Internal Libraries'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B18369_04.xhtml#_idTextAnchor085), *Federated Learning Server
    Implementation with Python*, and [*Chapter 5*](B18369_05.xhtml#_idTextAnchor130),
    *Federated Learning Client-Side Implementation*, both about the implementation
    of `fl_main/lib/util` directory of the provided `simple-fl` GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: In this appendix, we will provide an overview of the internal library and utilization
    classes and functions with code samples to achieve their functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the internal libraries for the FL system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enumeration classes for implementing the FL system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding communication handler functionalities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the data structure handler class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding helper and supporting libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Messengers to generate communication payloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the library code files introduced in this chapter can be found in the `fl_main/lib/util`
    directory of the GitHub repository (https://github.com/tie-set/simple-fl).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You can use the code files for personal or educational purposes. Please note
    that we will not support deployment for commercial use and will not be responsible
    for any errors, issues, or damages caused by using the code.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the internal libraries for the FL system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure A.1* shows the Python code components for the internal libraries found
    in the `lib/util` folder of the `fl_main` directory, which is used in the database,
    aggregator, and agent of the FL system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure A.1 – Python software components for the internal libraries used in
    the database, aggregator, and agent'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_A_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure A.1 – Python software components for the internal libraries used in the
    database, aggregator, and agent
  prefs: []
  type: TYPE_NORMAL
- en: The following are brief descriptions of the Python files for the internal libraries
    found in the `lib/util` folder of the FL system.
  prefs: []
  type: TYPE_NORMAL
- en: states.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `states.py` file in the `lib/util` folder defines a variety of enumeration
    classes to support implementing the FL system. Definitions of the classes include
    FL client states, types of ML models and messages, and locations of the information
    and values of various messages.
  prefs: []
  type: TYPE_NORMAL
- en: communication_handler.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `communication_handler.py` file in the `lib/util` folder can provide communication
    functionalities among the database, FL server, and clients, mainly defining the
    `send` and `receive` functions between them. Also, it provides the functions to
    start the servers for the database, aggregator, and agent.
  prefs: []
  type: TYPE_NORMAL
- en: data_struc.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `data_struc.py` file in the `lib/util` folder defines the class called `LimitedDict`
    to support an aggregation process of the FL cycle. It provides functions to convert
    ML models with a dictionary format into `LimitedDict` and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: helpers.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `helpers.py` file in the `lib/util` folder has a collection of internal
    helper functions, such as reading configuration files, generating unique hash
    IDs, packaging ML models into a dictionary, loading and saving local ML models,
    getting the IP address of the machine, and manipulating the FL client state.
  prefs: []
  type: TYPE_NORMAL
- en: messengers.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `messengers.py` file in the `lib/util` folder is for generating a variety
    of messages as communication payloads among FL systems to facilitate the implementation
    of communication protocols of the simple FL system discussed throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have discussed an overview of the FL system’s internal libraries,
    next, let’s talk about the individual code files in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Enumeration classes for implementing the FL system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enumeration classes are for assisting implemention of the FL system. They are
    defined in the `states.py` file found in the `lib/util` folder of the `fl_main`
    directory. Let us look into what libraries are imported to define the enumeration
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Importing libraries to define the enumeration classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this `states.py` code example, the file imports general libraries such as
    `Enum` and `IntEnum` from `enum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll explain the class that defines the prefixes of three components
    of the FL system.
  prefs: []
  type: TYPE_NORMAL
- en: IDPrefix defining the FL system components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is a list of classes to define the FL system components. `IDPrefix`
    is the prefix to indicate which FL component is referred to in the code, such
    as `agent`, `aggregator`, or `database`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll provide a list of the classes for the client state.
  prefs: []
  type: TYPE_NORMAL
- en: Client state classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is a list of enumeration classes related to the FL client states,
    including the state of waiting for global models (`waiting_gm`), the state of
    ML training (`training`), the state of sending local ML models (`sending`), and
    the state of receiving the global models (`gm_ready`). The client states defined
    in the agent specification are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: List of classes defining the types of ML models and messages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following is a list of classes defining the types of ML models and messages
    related to the FL system implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The ModelType class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The types of ML models, including `local` models and `cluster` models (`global`
    models), are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The DBMsgType class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The message types are defined in the communication protocol between an aggregator
    and database, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The AgentMsgType class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The message types are defined in the communication protocol sent from an agent
    to an aggregator, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The AggMsgType class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The message types are defined in the communication protocol sent from an aggregator
    to an agent, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: List of state classes defining message location
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following is a list of classes defining the message location related to
    communication between the FL systems.
  prefs: []
  type: TYPE_NORMAL
- en: The ParticipateMSGLocation class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The index indicator to read a participation message from an agent to the aggregator
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The ParticipateConfirmationMSGLocation class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The index indicator to read a participation confirmation message sent back
    from the aggregator is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The DBPushMsgLocation class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The index indicator to read a `push` message from an aggregator to the database
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The GMDistributionMsgLocation class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The index indicator to read a global model distribution message from an aggregator
    to agents is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The ModelUpMSGLocation class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The index indicator to a message uploading local ML models from an agent to
    an aggregator is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The PollingMSGLocation class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The index indicator for a `polling` message from an agent to an aggregator
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We have defined the enumeration classes that are utilized throughout the code
    of the FL system. In the next section, we will discuss the communication handler
    functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding communication handler functionalities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The communication handler functionalities are implemented in the `communication_handler.py`
    file, which can be found in the `lib/util` folder of the `fl_main` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Importing libraries for the communication handler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this `communication_handler.py` code example, the handler imports general
    libraries such as `websockets`, `asyncio`, `pickle`, and `logging`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll provide a list of functions of the communication handler.
  prefs: []
  type: TYPE_NORMAL
- en: Functions of the communication handler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following is a list of the functions related to the communication hander.
    Although the **Secure Sockets Layer** (**SSL**) or **Transport Layer Security**
    (**TLS**) framework is not implemented in the communication handler code here
    for simplification, it is recommended to support them to secure communication
    among FL components all the time.
  prefs: []
  type: TYPE_NORMAL
- en: The init_db_server function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `init_db_server` function is for starting the database server on the FL
    server side. It takes a function, database IP address, and socket information
    as inputs and initiates the server functionality based on the WebSocket framework.
    You can use any other communication protocol, such as HTTP, as well. Here is the
    sample code to initiate the database server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The init_fl_server function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `init_fl_server` function is for starting the FL server on the aggregator
    side. As parameters, it takes three functions for agent registration, receiving
    messages from agents, and the model synthesis routine, as well as the aggregator’s
    IP address and registration and receiver sockets info (to receive messages from
    agents) to initiate the server functionality based on the WebSocket framework.
    Here is the sample code for initiating the FL server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The init_client_server function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `init_client_server` function is for starting the FL client-side server
    functionalities. It takes a function, the agent’s IP address, and the socket info
    to receive messages from an aggregator as inputs and initiate the functionality
    based on the WebSocket framework. Here is sample code for initiating the FL client-side
    server functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The send function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `send` function is for sending a message to the destination specified by
    the IP address and socket info taken as parameters together with a message to
    be sent. It returns a response message sent back from the destination node to
    the source node, if there is one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The send_websocket function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `send_websocket` function is for returning a message to the message source
    specified by the WebSocket information, taken as a parameter together with a message
    to be sent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The receive function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `receive` function is used to receive a message with the WebSocket taken
    as a parameter and returns a pickled message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will talk about the data structure class that handles processing ML
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data structure handler class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data structure handler is implemented in the `data_struc.py` file, which
    can be found in the `lib/util` folder of the `fl_main` directory. The data structure
    class has the `LimitedDict` class to handle the aggregation of the ML models in
    a consistent manner.
  prefs: []
  type: TYPE_NORMAL
- en: Importing libraries for the data structure handler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this `data_struc.py` code example, the handler imports general libraries,
    such as `numpy` and `Dict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s move on to the `LimitedDict` class and its functions related to
    the data structure handler.
  prefs: []
  type: TYPE_NORMAL
- en: The LimitedDict class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following is a definition of the `LimitedDict` class and its functions related
    to the data structure handler.
  prefs: []
  type: TYPE_NORMAL
- en: The LimitedDict class and its functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The functions of the `LimitedDict` class are for converting a dictionary format
    into a class with keys and values. `LimitedDict` is used with the buffer in ML
    models to store local and cluster models in the memory space of the state manager
    of the aggregator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The convert_LDict_to_Dict function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `convert_LDict_to_Dict` function is used to convert the `LimitedDict` instance
    defined previously into a normal dictionary format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will talk about the helper and supporting libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding helper and supporting libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The helper and supporting functions are implemented in the `helpers.py` file,
    which can be found in the `lib/util` folder of the `fl_main` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Importing libraries for helper libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this `helpers.py` code example, the file imports general libraries such
    as `json` and `time`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s move on to the list of functions of the helper library.
  prefs: []
  type: TYPE_NORMAL
- en: Functions of the helper library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following is a list of functions related to the helper library.
  prefs: []
  type: TYPE_NORMAL
- en: The set_config_file function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `set_config_file` function takes the type of the config file, such as `db`,
    `aggregator`, or `agent`, as a parameter and returns a string of the path to the
    configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The read_config function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `read_config` function reads a JSON configuration file to set up the database,
    aggregator, or agent. It takes a config path as a parameter and returns config
    info in a dictionary format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The generate_id function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `generate_id` function generates a system-wide unique ID based on the MAC
    address and instantiation time with a hash function (`sha256`) returning the hash
    value as an ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The generate_model_id function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `generate_model_id` function generates a system-wide unique ID for a set
    of models based on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Component ID**: The ID of the FL system entity that created the models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation time**: The time the models were created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ID is generated by a hash function (sha256). It takes the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`component_type`: A string value with a prefix indicating the component type
    of `IDPrefix`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`component_id`: A string value of the ID of the entity that created the models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gene_time`: A float value of the time the models were created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This function returns the hash value as a model ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The create_data_dict_from_models function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `create_data_dict_from_models` function creates the data dictionary for
    ML models by taking the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model_id`: A string value of the model ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models`: The `np.array` about ML models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`component_id`: The ID of the FL system such as aggregator ID and agent ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It returns a data dictionary containing the ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The create_meta_data_dict function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `create_meta_data_dict` function creates the metadata dictionary with the
    metadata of the ML models, taking the performance metrics (`perf_val`) and the
    number of samples (`num_samples`) as parameters, and returns `meta_data_dict`,
    containing the performance value and the number of samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The compatible_data_dict_read function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `compatible_data_dict_read` function takes `data_dict`, which contains
    the information related to ML models, extracts the values if the corresponding
    key exists in the dictionary, and returns the component ID, the generation time
    of the ML models, the ML models themselves, and the model IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The save_model_file function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `save_model_file` function is for saving a given set of models into a local
    file. It takes the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data_dict`: A dictionary containing the model ID and ML models with the `Dict[str,np.array]`
    format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: A string value of the path to the directory of the ML model storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: A string value of the model filename.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`performance_dict`: A dictionary containing performance data with the `Dict[str,float]`
    format. Each entry contains both the model ID and its performance information:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The load_model_file function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`load_model_file` reads a local model file that takes the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`path`: A string value of the path to the directory to store ML models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: A string value of the model filename'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It returns the unpickled ML models and performance data in the `Dict` format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The read_state function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `read_state` function reads a local state file that takes the following
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`path`: A string value of the path to the directory of the client state file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: A string value of the model filename'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This function returns a client state, `ClientState` (for example, *training*
    or *sending*), the state indicated in the file, in an integer format. If the client
    state file is being written at the time of access, it will try to read the file
    again after 0.01 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The write_state function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`write_state` changes the client state on the state file in the agent. It takes
    the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`path`: A string value of the path to the directory of the client state file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: A string value of the model filename'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`state`: The value of `ClientState` (for example, *training* or *sending*)
    to set up a new client state:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The get_ip function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `get_ip` function obtains the IP address of the machine and returns the
    value of the IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The init_loop function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `init_loop` function is used to start a continuous loop function. It takes
    a function for running a loop function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, let’s look at the messenger functions to create communication
    payloads.
  prefs: []
  type: TYPE_NORMAL
- en: Messengers to generate communication payloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The messenger functions are defined in the `messengers.py` file, which can be
    found in the `lib/util` folder of the `fl_main` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Importing libraries for messengers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this `messengers.py` code example, the file imports general libraries, such
    as `time` and `numpy`. It also imports `ModelType`, `DBMsgType`, `AgentMsgType`,
    and `AggMsgType`, which were defined in the *Enumeration classes for implementing
    the FL system* section in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s move on to the list of functions of the `messengers` library.
  prefs: []
  type: TYPE_NORMAL
- en: Functions of messengers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following is a list of functions related to the `messengers` library.
  prefs: []
  type: TYPE_NORMAL
- en: The generate_db_push_message function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `generate_db_push_message` function generates and returns a message for
    pushing the message containing ML models to the database. It takes the following
    parameters to package them as a payload message (in a `List` format with the message
    type defined as `push`) between the aggregator and database:'
  prefs: []
  type: TYPE_NORMAL
- en: '`component_id`: A string value of the component ID, such as the aggregator
    ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`round`: FL round information in an integer format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_type`: The type of ML model, such as `cluster` or `local` models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models`: ML models with the `Dict[str, np.array]` format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_id`: A string value of the unique ID of the ML models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gene_time`: A float value of the time at which the ML models are generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`performance_dict`: Performance data with the `Dict[str, float]` format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code provides the functionality of generating the preceding database
    push message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The generate_lmodel_update_message function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `generate_lmodel_update_message` function generates and returns a message
    for sending the aggregator a message containing the local models created in an
    agent. It takes the following parameters to package them as a payload message
    (in `List` format with the message type defined as `update`) between the agent
    and aggregator:'
  prefs: []
  type: TYPE_NORMAL
- en: '`agent_id`: A string value of the agent ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_id`: A string value of the unique ID of the ML models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_models`: Local ML models with the `Dict[str, np.array]` format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`performance_dict`: Performance data with the `Dict[str, float]` format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code shows the functionality of generating the preceding local
    model update message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The generate_cluster_model_dist_message function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `generate_cluster_model_dist_message` function generates and returns a
    message in `List` format to send a message containing the global models created
    by an aggregator to the connected agents. It takes the following parameters to
    package them as a payload message (in `List` format with the message type defined
    as `update`) between the aggregator and agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '`aggregator_id`: A string value of the aggregator ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_id`: A string value of the unique ID of the ML models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`round`: FL round information in an integer format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models`: ML models with the `Dict[str, np.array]` format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code shows the functionality of generating the preceding cluster
    model distribution message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The generate_agent_participation_message function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `generate_agent_participation_message` function generates and returns a
    message to send a participation request message containing the initial models
    created by an agent to the connected aggregator. It takes the following parameters
    to package them as a payload message (in `List` format with the message type defined
    as `participate`) between the agent and aggregator:'
  prefs: []
  type: TYPE_NORMAL
- en: '`agent_name`: A string value of the agent name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`agent_id`: A string value of the agent ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_id`: A string value of the unique ID of the ML models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models`: ML models with the `Dict[str, np.array]` format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_weights_flag`: A Boolean value to indicate whether the weights are initialized
    or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`simulation_flag`: A Boolean value to indicate whether the run is for a simulation
    or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exch_socket`: Socket information with a string value to send a message from
    an aggregator to this agent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gene_time`: A float value of the time at which the ML models are generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`meta_dict`: Performance data with the `Dict[str, float]` format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`agent_ip`: IP address of the agent itself'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code shows the functionality of generating the preceding agent
    participation message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The generate_agent_participation_confirm_message function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `generate_agent_participation_confirm_message` function generates and returns
    a message to send a participation confirmation message containing the global models
    back to the agent. It takes the following parameters to package them as a payload
    message (in `List` format with the message type defined as `welcome`) between
    the aggregator and agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '`aggregator_id`: A string value of the aggregator ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_id`: A string value of the unique ID of the ML models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models`: ML models with the `Dict[str, np.array]` format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`round`: FL round information in an integer format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`agent_id`: A string value of the agent ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exch_socket`: A port number to reach out to an agent from the aggregator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recv_socket`: A port number to receive messages from the agent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code shows the functionality of generating the preceding agent
    participation confirmation message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The generate_polling_message function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `generate_polling_message` function generates and returns a message to
    send a `polling` message containing the polling signal to the aggregator. It takes
    the following parameters to package them as a payload message (in `List` format
    with the message type defined as `polling`) between the agent and aggregator:'
  prefs: []
  type: TYPE_NORMAL
- en: '`round`: FL round information in an integer format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`agent_id`: A string value of the agent ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code shows the functionality of generating the preceding polling
    message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The generate_ack_message function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `generate_ack_message` function generates and returns a message to send
    an `ack` message containing the acknowledgment signal back to an agent. No parameter
    is required to create a payload message (in `List` format with the message type
    defined as `ack`) between the aggregator and agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explained the internal libraries in detail so that
    you can implement the entire FL system without further investigating what and
    how to code for basic functionalities such as communication and data structure
    conversion frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are mainly five aspects that the internal library covers: *enumeration
    classes*, defining the system states, such as FL client states; the *communication
    handler*, supporting send and receive functionalities; the *data structure*, to
    handle ML models when aggregation happens; *helper and support functions*, which
    cope with basic operations, such as saving data and producing randomized IDs;
    and *messenger functions*, to generate various payloads sent among the database,
    aggregator, and agents.'
  prefs: []
  type: TYPE_NORMAL
- en: With these functions, you will find the implementation of FL systems easy and
    smooth, but these libraries only support achieving some minimal functionality
    of the FL system; hence, it is up to you to further enhance the FL system to create
    a more authentic platform that can be used in real-life use cases and technologies.
  prefs: []
  type: TYPE_NORMAL
