- en: Model Creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have now created several data sources based on the original `Titanic` dataset
    in S3\. We are ready to train and evaluate an Amazon ML prediction model. In Amazon
    ML, creating a model consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the training datasource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a recipe for data transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting the parameters of the learning algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the quality of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will start by exploring the data transformations available
    in Amazon ML, and we will compare different recipes for the `Titanic` dataset.
    Amazon ML defines recipes by default depending on the nature of the data. We will
    investigate and challenge these default transformations.
  prefs: []
  type: TYPE_NORMAL
- en: The model-building step is simple enough, and we will spend some time examining
    the available parameters. The model evaluation is where everything converges.
    The evaluation metrics are dependent on the type of the prediction at hand, regression,
    binary or multi-class classification. We will look at how these different evaluations
    are carried out. We will also download the model training logs to better understand
    what goes on under the Amazon ML hood when training the model. We will conclude
    the chapter by comparing the model evaluation for several data recipes and regularization
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter is organized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Recipes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering, recipes, and regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the end of [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml), *Loading
    and Preparing the Dataset,* we modified the schema to exclude three variables:
    `boat`, `body`, and `home.dest` from the original dataset and created a new datasource
    based on this schema. We will use this datasource to train the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to your Amazon ML datasource dashboard; you should see three datasources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Titanic train set: It is the original raw dataset with 14 variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Titanic train set 11 variables: Has 11 variables; `boat`, `body` and `home.dest`
    have been removed from the schema'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Titanic train set extended: It is the cleaned up and extended dataset we obtained
    through SQL-based feature engineering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will work with the `Titanic train set 11 variables` datasource. Before starting
    with the model creation, let’s first review what types of data transformations
    are available in Amazon ML.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data with recipes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A crucial element of the data science workflow is feature engineering. Amazon
    ML offers certain data transformations via its data recipes. Note that although
    transformations are conceptually part of the ETL or data preparation phase of
    a predictive analytics workflow, in Amazon ML, data recipes are part of the model-building
    step and not of the initial datasource creation step. In this section, we start
    by reviewing the available data transformations in Amazon ML, and then we apply
    some of them to the `Titanic` dataset using the `Titanic train set 11 variables`
    datasource.
  prefs: []
  type: TYPE_NORMAL
- en: Managing variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipes are JSON-structured scripts that contains the following three sections
    in the given order:'
  prefs: []
  type: TYPE_NORMAL
- en: Groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assignments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An empty recipe instructing Amazon ML to take all the dataset variables into
    account for model training will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The recipe does not transform the data in any way.
  prefs: []
  type: TYPE_NORMAL
- en: The complete Amazon ML recipes documentation is available at [http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html](http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html).
  prefs: []
  type: TYPE_NORMAL
- en: Grouping variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Groups enable grouping of multiple variables to facilitate applying the same
    transformations to several variables. The groups section of the recipe has a naming
    function. Group definition follows this syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Amazon ML has defined a set of default groups based on the type of the variables:
    `ALL_TEXT`, `ALL_NUMERIC`, `ALL_CATEGORICAL`, `ALL_BINARY`, and the `ALL_INPUTS`
    group for all the variables at once. Let''s look at a couple of examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example where we want to apply the same transformation
    (normalization) on the `age` and `fare` variables. We can define a group and name
    it `TO_BE_NORMALIZED`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, consider an e-mail spam detection context where for each e-mail,
    we have a header, a subject, and a body. We want to create N-grams of the e-mail
    title and body but not of the header; we can define a group composed of all text
    variables with the exception of specifically excluded ones. Here we create a group
    named `N_GRAM_TEXT` that combines all text variables except the header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Naming variables with assignments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main purpose of assignments is naming facilitation. You can choose to name
    the transformed variable or group of variables in the assignments section or directly
    in the output section. Assignments are only for convenience and readability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assignments follow this syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, you could rename and normalize the numeric variables as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Or rename and process the subject and body of your e-mails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You can also leave the assignments section empty and apply the transformations
    to the variables groups in the output section. In the end, it’s more a question
    of style and readability than anything else.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying outputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The outputs section is where you explicitly list all the variables that will
    be used for the model training. If you have defined a group with some of the variables
    but you still want the original variables to be accounted for, you need to explicitly
    list them. The assignment section declares a list composed of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assignments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformation (variable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, if you wanted the original body and subject of the e-mails as
    well as the `bigrams` you defined in assignments, you would need to declare the
    outputs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following outputs declaration declares all the text variables and adds
    the bigrams assignment defined earlier on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The recipe format reference page has other examples of combining groups, assignments,
    and outputs to create recipes: [http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html](http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html).
    We will now look at the available transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing through seven transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon ML offers the following seven transformations. Four transformations
    for text variables are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Lowercase transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove punctuation transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N-gram transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Orthogonal sparse bigram (OSB)** transformation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two transformations for numeric variables are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Normalization transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantile binning transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And one transformation for coupling text with categorical variables:'
  prefs: []
  type: TYPE_NORMAL
- en: Cartesian product transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These transformations are well explained on the Amazon ML documentation ([http://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html](http://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Using simple transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The lowercase transformation takes a text variable as input and returns the
    text in lowercase: `Amazon ML is great for Predictive Analytics` is returned as
    `amazon ml is great for predictive analytics`. Syntax for lowercase transformation
    is `lowercase(text_variable)` or `lowercase(group_of_text_variables)`.
  prefs: []
  type: TYPE_NORMAL
- en: The remove punctuation transformation also takes a text variable as input and
    removes all punctuation signs, with the exception of hyphens within words (`seat-belts`
    will remain as `seat-belts`). It is not possible to define your own set of punctuation
    signs. Syntax for the remove punctuation transformation is `no_punct(text_variable)`
    or `no_punct(group_of_text_variables)`.
  prefs: []
  type: TYPE_NORMAL
- en: The normalization transformation normalizes numeric variables to have a mean
    of zero and a variance of one. This is a useful transformation when numeric variables
    vary significantly in range. This transformation corresponds to the **z-score**
    normalization also known as  standardization and not to the min-max normalization
    (see [Chapter 2,](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml) *Machine Learning
    Definitions and Concepts*). Syntax for normalization transformation is `normalize(numeric_variable)`
    or `normalize(group_of_numeric_variables)`.
  prefs: []
  type: TYPE_NORMAL
- en: Text mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The N-gram and the **orthogonal sparse bigram** (**OSB**) transformations are
    the main text-mining transformations available in Amazon ML.
  prefs: []
  type: TYPE_NORMAL
- en: In text mining, the classic approach is called the **bag-of-words** approach.
    This approach boils down to discarding the order of the word in a given text and
    only considering the relative frequency of the words in the documents. Although
    it may seem to be overly simplistic, since the order of the words is essential
    to understand a message, this approach has given satisfying results in all types
    of natural language processing problems. A key part of the bag-of-words method,
    is driven by the need to extract the words from a given text. However, instead
    of considering single words as the only elements holding information, we could
    extract sequences of words. These sequences are called N-grams. Sequences of two
    words are called bigrams, for three words trigrams, and so forth. Single words
    are also called unigrams. N-grams are also called tokens and the process of extracting
    words, and N-grams from a text is called tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, consider the sentence: *The brown fox jumps over the dog*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unigrams** are {*The, brown, fox, jumps, over, the, dog*}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bigrams** are {*The brown, brown fox, fox jumps, jumps over, over the, the
    dog*}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trigrams** are {*The brown fox, brown fox jumps, fox jumps over, jumps over
    the, over the dog*}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no rule or heuristic that would let you know if you need N-grams in
    your model or what order of N-grams would be the most beneficial for your model.
    It depends on the type of text you are dealing with. Only experimentation can
    tell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon ML offers two tokenization transformations: N-gram and OSB.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **N-gram** transformation: Takes a text variable and an integer from 2
    to 10 and returns expected N-grams. Note that all text variables are, by default,
    tokenized as unigrams in Amazon ML. There is no need to explicitly specify unigrams
    in the recipe. `ngram(text_variable, n)` will produce bigrams for *n= 2*, trigrams
    for *n=3* and so forth.'
  prefs: []
  type: TYPE_NORMAL
- en: The **OSB** or orthogonal sparse bigram transformation is an extension on the
    bigram transformation (N*-gram* with *n=2*). Given a word in a text, compose pairs
    of words by associating the other words separated by *1,2, …, N* words from the
    initial word. *N* being the size of the OSB window.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in the sentence *this is a limited time offer*, first consider
    the word *offer*. The OSBs for a window of four are: *time_offer*, *limited_<skip>_offer*,
    *a_<skip>_<skip>_offer*, *is_<skip>_<skip>_<skip>_offer*, *this_<skip>_<skip>_<skip>_<skip>_offer*.
    We build word pairs by skipping 1,2,..., N words each time.'
  prefs: []
  type: TYPE_NORMAL
- en: The OSB transformation allows us to extract information about the context surrounding
    each word. For instance, the OSB *is_<skip>_<skip>_offer*, could be used to detect
    strings such as *is a special offer* as well as *is our best offer*. OSB extraction
    has been found to generally improve the performance of spam filtering algorithms.
    Syntax for OSB transformation is `osb(text_variable, N)`, with `N` the size of
    the window ranging from 2 to 10.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that some very standard text transformations are absent from
    Amazon ML recipes. Stemming and **Lemmatization** are used to regroup words with
    different endings to a common base form (for instance, *walking*, *walker* and
    *walked* would all be accounted for as *walk*) and are not offered in Amazon ML.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, removing very common words, known as *stopwords*, such as articles
    or prepositions (the, a, but, in, is, are, and so on) from a text is also a very
    standard text-mining transformation but is not an option in Amazon ML recipes.
    It is nonetheless possible that Amazon ML carries out similar transformations
    in the background without explicitly stating so. However, nothing in the available
    documentation indicates that to be the case.
  prefs: []
  type: TYPE_NORMAL
- en: Coupling variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Cartesian product transformation combines two categorical or text variables
    into one. Consider, for instance, a dataset of books and for each book, their
    title and genre. We could imagine that the title of a book has some correlation
    with its genre, and creating a new `title_genre` variable would bring forth that
    relation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following four books, their titles, and genres. Coupling the words
    in the title with the genre of the book adds extra information to the words in
    the title. Information that the model could use effectively. This is illustrated
    in the `title_genre` column in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Title** | **Genre** | **title_genre** |'
  prefs: []
  type: TYPE_TB
- en: '| *All the Birds in the Sky* | scifi | {`all_scifi`, `birds_scifi`, `sky_scifi`}
    |'
  prefs: []
  type: TYPE_TB
- en: '| *Robots and Empire* | scifi | `{robots_scifi`, `emprire_scifi`} |'
  prefs: []
  type: TYPE_TB
- en: '| *The Real Cool Killers* | crime | {`real_crime`, `cool_crime`, `killers_crime`}
    |'
  prefs: []
  type: TYPE_TB
- en: '| *Bullet in the Sky* | crime | {`bullet_crime`, `sky_crime`} |'
  prefs: []
  type: TYPE_TB
- en: The word sky now takes a different meaning if it's in the title of a crime novel: `sky_crime`
    or in the title of a SciFi novel: `sky_scifi`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the `Titanic` dataset, we could couple the `sibsp` and `*parch*`
    variables (number of siblings and number of parents) by taking their cartesian
    products: `sibsp*parch` and come up with a new variable that distinguishes between
    passengers with (without) parents and few or no siblings from those with (without)
    parents and many siblings. Syntax is `cartesian(var1, var2)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Binning numeric values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final and most important transformation is quantile binning. The goal with
    quantile binning is to transform a numeric variable into a categorical one in
    order to better extract the relation between the variable and the prediction target.
    This is particularly useful in the presence of nonlinearities between a variable
    and the target. By splitting the original numeric variables values into *n* bins
    of equal size, it is possible to substitute each value by a corresponding bin.
    Since the number of bins is finite (from 2 to 1,000), the variable is now categorical.
    Syntax is `quantile_bin(var, N)` with `N` the number of bins.
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of unsupervised binning, equal frequency and equal width
    binning. In equal frequency, each bin has the same number of samples, whereas
    in equal width binning, the variable range is split into N smaller ranges of equal
    width. Quantile binning usually refers to equal frequency binning.
  prefs: []
  type: TYPE_NORMAL
- en: Categorizing continuous data is not always a good approach as you are, by definition,
    throwing away information that could be useful for the model. This page lists
    several other problems associated with binning: [http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous](http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous).
    However, Amazon ML seems to be quite fond of the quantile binning technique. In
    fact, of all the datasets we considered, Amazon ML always applied quantile binning
    to all the numeric variables, in the suggested recipe and often used large, sometimes
    very large, number of bins. For instance, the default transformation for the `fare`
    variable in the `Titanic` dataset was quantile binning with 500 bins although
    the variable only ranged from 0 to 512\. We compare the evaluations obtained by
    keeping the original numeric values versus applying quantile binning at the end
    of this chapter, *Keeping variables as numeric or applying quantile binning?*
    section Now that we’ve explored the available recipes, let’s look at how Amazon
    ML suggests we transform our `Titanic` dataset
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon ML always suggests a recipe based on your datasource when you create
    a model. You can choose to use that recipe or to modify it. We will now create
    our first model and during that process analyze the recipe Amazon ML has generated
    for us.
  prefs: []
  type: TYPE_NORMAL
- en: Go to the model dashboard, and click on the Create new... | ML model button.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will go through three screens:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the datasource, choose the `Titanic train set with 11 variables`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon ML will validate the datasource and present a summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Choose the default or Custom model creation; choose the custom path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B05028_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next screen is split between the attributes, their type and a sample of
    values on the left side, and the suggested recipe on the right side, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Editing the suggested recipe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is where you can edit the recipe and replace it with a recipe of your own
    creation.
  prefs: []
  type: TYPE_NORMAL
- en: You can find all the JSON in this chapter in the book's GitHub repository, properly
    formatted and indented at [https://github.com/alexperrier/packt-aml/blob/master/ch5/recipes.json](https://github.com/alexperrier/packt-aml/blob/master/ch5/recipes.json).
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipe is validated while you type. Anything not respecting the JSON format
    will result in the following error message: `Recipe must be valid JSON`. Some
    common errors include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Indentation is not respected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last element between braces should not be followed by a comma
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All strings must be between double quotes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manually formatting JSON text is not fun. This online JSON editor is very helpful:
    [http://www.cleancss.com/json-editor/](http://www.cleancss.com/json-editor/).'
  prefs: []
  type: TYPE_NORMAL
- en: Applying recipes to the Titanic dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recipe generated by Amazon ML for our dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: All numeric values are quantile binned. No further processing is done on the
    text, binary, or categorical variables. The output section of the recipe shows
    that the numeric variables are replaced by the binned equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Further comments can be made on this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: The `sibsp` and `parch` variables are grouped together. First of all, both `sibsp`
    and `parch` have similar ranges, 0 to 9 and 0 to 8 respectively. It makes sense
    to have the same number of bins for both variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Amazon ML chose 50 bins for `sibsp` and `parch`, 100 bins for `age`, and
    10 bins for `fare` is less clear.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We found that the number of bins was very sensitive to the data in the training
    set. Several versions of the initial datasets produced very different binning
    numbers. One constant in all our trials was that all the numeric values went through
    quantile binning with a rather high number of bins. In one instance, Amazon ML
    suggested 500 bins for the `fare` variable and 200 for the `age` variable. In
    both cases, we would have ended with a very small number of samples per bin since
    our total number of training sample consists of just 1,047 passengers. How Amazon
    ML calculates the optimal number of bins is not clear.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other transformations Amazon ML could decide to apply to our `Titanic`
    dataset such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting bigrams or OSBs from the passengers' titles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coupling `sibsp` and `parch` with cartesian product transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing between recipes and data pre-processing.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have transformed our initial dataset via scripts and Amazon ML recipes.
    The two techniques are complementary. Some transformation and data manipulation
    can only be done by preprocessing the data. We did so in [Chapter 4,](08d9b49a-a25c-4706-8846-36be9538b087.xhtml) *Loading
    and Preparing the Dataset* with Athena and SQL. We could have achieved similar
    data processing with other scripting languages such as Python or R, which are
    most fruitful for creative feature engineering. SQL and scripts can also better
    deal with outliers and missing values — corrections that are not available with
    Amazon ML recipes.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the Amazon ML transformations is to prepare the data for consumption
    by the Amazon ML algorithm, whereas scripted feature engineering is about cleaning
    up the data and creating new variables out of the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although Amazon ML recipes are quite restrained, they offer an easy way to
    fiddle around with the dataset and quickly compare models based on different recipes.
    Creating a new model and associated evaluation from a given datasource and schema only
    takes a few clicks. And by choosing to write different recipes for each model,
    it becomes possible to experiment with a wide range of datasets. Recipes allow
    us to create a fast try-fail loop. The associated workflow becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify the datasource.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with different recipes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create or remove variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and select the model associated to that recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the best recipe is found, then start optimizing the model parameters, regularization,
    passes, and memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can compare how we transformed the data with scripting (Athena and SQL)
    and with recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recipes:**'
  prefs: []
  type: TYPE_NORMAL
- en: Removing features (`boat`, `body`, and `home.dest`). This can also be done via
    the schema or directly by removing the columns from the dataset CSV file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cartesian product for an indication of family by aggregating `parch` and `sibsp`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization of numeric values (a possibility).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization of all text variables, names, destinations, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantile binning of all numeric values; although the number of bins were large
    this transformation produced good results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scripting (SQL):**'
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values for `age`: We replaced all missing values by the mean
    of the *age*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text processing: We extracted `titles` from the `name` variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Created a new feature, the `family_size` as the sum of `parch` and `sibsp`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extraction of the `deck` from the cabin number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both approaches are very complementary.
  prefs: []
  type: TYPE_NORMAL
- en: Parametrizing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that our data has been prepared for the SGD algorithm, we are ready to
    set the parameters of our experiment. In a way similar to scientific experimentation,
    we will want to try out several sets of parameters to test several models and
    pick up the best one. The next screenshot shows where we actually specify our model
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Model memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data passes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuffling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B05028_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting model memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model memory is related to the memory Amazon ML must set aside to build and
    evaluate your model. It is set, by default, to 100Mb. In the case of the `Titanic`
    dataset, the model memory was always below 1Mb as shown by the logs. Model memory
    is also used to set aside memory when dealing with streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the number of data passes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon ML will use the training set of samples several times, each time shuffling
    it and using the new sequence to increase prediction. It's similar to squeezing
    a wet piece of cloth — each time you wring it, more water comes out of it. Set
    by default to 10 passes, it does not hurt to set it to the maximum value of a
    100 at the expense of a longer training time for the model and a higher cost of
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen in [Chapter 2,](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml) *Machine
    Learning Definitions and Concepts*, regularization makes your model more robust
    and allows it to better handle previously unseen data by reducing overfitting.
    The rule of thumb is to lower regularization if your evaluation score is poor
    (underfitting) and increase it if your model shows great performance on the training
    set but poor results on the evaluation set (overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: Creating an evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Evaluations and models are independent in Amazon ML. You can train a model
    and carry out several evaluations by specifying different evaluation datasets.
    The evaluation page, shown in the following screenshot, lets you name and specify
    how the model will be evaluated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: As you know by now, to evaluate a model, you need to split your dataset into
    two parts, the training and the evaluation sets with a 70/30 split. The training
    part is used to train your model, while the evaluation part is used to evaluate
    the model. At this point, you can let Amazon ML split the dataset into training
    and evaluation or specify a different datasource for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the initial `Titanic` file was ordered by class and passenger alphabetical
    order. Using this ordered dataset and splitting it without shuffling, that is,
    taking sequentially the first 70% samples, would give the model a very different
    data for the training and the evaluation sets. The evaluation would not be relevant.
    However, if your data is not already shuffled, you can tell Amazon ML to shuffle
    it. It is a good practice to let Amazon ML reshuffle your data by default just
    in case your own randomizing left some sequential patterns in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ML will make some verifications regarding your training and validation
    sets, checking that there is enough data for the validation, that the two sets
    follow similar distributions, and that the evaluation set has valid samples. Take
    a look at [http://docs.aws.amazon.com/machine-learning/latest/dg/evaluation-alerts.html](http://docs.aws.amazon.com/machine-learning/latest/dg/evaluation-alerts.html) for
    more information on Evaluation Alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you choose to let Amazon ML split the data, it will create two
    new datasources titled in a way that lets you see how the split was performed.
    You can reuse these new datasources if you decide to test another model with different
    recipes or model parameters such as regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Titanic.csv_[percentBegin=0, percentEnd=70, strategy=sequential]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Titanic.csv_[percentBegin=70, percentEnd=100, strategy=random]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click on Review, make sure your model is as expected, and click on the final
    *Create ML model* button. Creating the model usually takes a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, Amazon ML will use the training set to train several models and
    the evaluation sets to select the best one.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ML runs several model training in parallel, each time trying new parameters
    and shuffling the training set at each new pass. Once the number of passes initially
    set has been exhausted or the algorithm has converged, whichever comes first,
    the model is considered trained. For each model it trains, Amazon ML uses it for
    prediction on the validation subset to obtain an evaluation score per model. Once
    all the models have been trained and evaluated this way, Amazon ML simply selects
    the one with the best evaluation score.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation metric depends on the type of prediction at hand. AUC and `F1`
    score for classification (binary and multiclass), and RMSE for regression. How
    the evaluation results are displayed by Amazon ML also depends on the type of
    prediction at hand.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with evaluation for binary classification for our Titanic prediction,
    followed by the regression case with a new dataset related to Air traffic delays,
    and finally perform multiclass classification with the classic `Iris` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating binary classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once your model is ready, click on the model's title from the service dashboard
    to access the model's result page, which contains the summary of the model, its
    settings and the evaluation results.
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot shows that we obtained an *AUC* score of `0.880`, which
    is considered very good for most machine-learning applications. **AUC** stands
    for the **Area under the Curve** and was introduced in [Chapter 2](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml), *Machine
    Learning Definitions and Concepts*. It is the de-facto metric for classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The baseline for Binary classification is an AUC of 0.5, which is the score
    for a model that would randomly predict 0 or 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Amazon ML validates the model by checking the following conditions and raising
    alerts in case the conditions are not met:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In our case, no alerts were raised:'
  prefs: []
  type: TYPE_NORMAL
- en: The training and validation datasets were separate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The validation dataset had a sufficient number of samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The validation and training sets shared the same schema
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All samples of the validation set were valid and used for the evaluation, implying
    that the target was not missing for one or more samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distribution of the target variable was similar in the training and validation
    sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these alerts will not happen if we let Amazon ML handle the training
    validation data split, but they might be more frequent if we provide the validation
    set ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: The AUC score is not the only element Amazon ML gives us to evaluate the quality
    of our model. By clicking on the *Explore performance* link, we can analyze further
    the performance of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the model performances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may recall from [Chapter 2](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml):
    *Machine Learning Definitions and Concepts,* that in a binary classification context,
    a logistic regression model calculates for each sample to be predicted a probability
    — the probability of belonging to one class or the other. The model will not directly
    output the class of the sample to be predicted. The sample is assigned to one
    class or the other depending on whether the probability is below or above a certain
    threshold. By default, this threshold is set to 0.5\. Although the AUC score given
    by the evaluation does not depend on the value of the decision threshold, other
    classification metrics do. We can change the value of the threshold and see how
    that impacts our predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: The **Explore performance** page of the evaluation shows several other classification
    metrics as well as the confusion matrix of the model. The vertical bar in the
    graph below is a cursor that can slide left or right. By sliding the cursor, we
    increase or decrease the decision threshold used to classify a prediction sample
    as belonging to one class or another. As we move that cursor, the following metrics
    vary accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: False positive rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision: proportion of predicted positives that are truly positives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall (the proportion of positives that are correctly identified)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a threshold of *0.5*, we have the following sceenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we lower the threshold to *0.4*, accuracy decreases while recall increases,
    as you can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And if we raise the threshold to *0.7*, accuracy increases slightly while recall
    decreases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_10.png)'
  prefs: []
  type: TYPE_IMG
- en: In our context, the predictions are quite clearly separated between survived
    and did not survive values. Slightly changing the threshold does not have a huge impact
    on the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon ML uses the standard metric RMSE for linear regression. RMSE is defined
    as the sum of the squares of the difference between the real values and the predicted
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_015.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *ŷ* are the predicted values and *y* the real values. The closer the predictions
    are to the real values, the lower the RMSE is; therefore, a lower RMSE is interpreted
    as a better predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the evaluation in the regression context, we will consider a
    simplified version of the **Airlines delay** dataset available on Kaggle at [https://www.kaggle.com/giovamata/airlinedelaycauses](https://www.kaggle.com/giovamata/airlinedelaycauses).
    The full dataset is quite large (*~250Mb*). We extracted roughly *19,000* rows
    from the year 2008, filtering out cancelled flights. We also removed several variables
    that were too correlated with our target, which is the `Airdelay` variable. The
    resulting dataset and schema are available on GitHub at [https://github.com/alexperrier/packt-aml/tree/master/ch5](https://github.com/alexperrier/packt-aml/tree/master/ch5).
  prefs: []
  type: TYPE_NORMAL
- en: 'We upload the dataset to S3, create a datasource, train and evaluate a model
    and finally obtain an RMSE of 7.0557 with a baseline of 31.312. The baseline for
    regression is given by a model that always predicts the average of the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Exploring further, we obtain the following histograms of residuals. As we can
    see in the next screenshot, the errors are roughly bell-shaped and centered around
    *0*, meaning that our errors are half the time overestimating/underestimating
    the real values. All the information available in the dataset has been consumed
    by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_15.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating multiclass classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The classic dataset for multiclass classification is the `Iris` dataset composed
    of three types of Iris flowers. This dataset is quite simple, very popular and
    using it to illustrate the performance of a platform as powerful as Amazon ML
    seems overkill. Luckily, there are another three class datasets composed of seeds.
    The seeds dataset is available at [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)
    and of course on the GitHub repository accompanying this book (as well as the
    schema).
  prefs: []
  type: TYPE_NORMAL
- en: The seed dataset has 210 samples distributed evenly among three different `seedTypes`
    and seven attributes. The dataset has an ID, which must be set to categorical,
    all attributes are NUMERIC, and the target is the `seedType.` We upload the dataset
    to S3, and create a datasource and a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The metric for multiclass classification is the *F1* score defined as the harmonic
    mean of precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The baseline for a multiclass classification problem is the macro average *F1*
    score for a model that would always predict the most common class. In the case
    of the seed dataset, we obtain a *F1* score of 0.870 for baseline of 0.143:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Performance exploration is not as developed as in the binary classification
    case. Amazon ML gives us the confusion matrix which shows, for each class, the
    ratio of correctly predicted samples over the real number of samples in that class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Analyzing the logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For every operation it carries out, Amazon ML gives us access to the related
    logs. We can download and analyze the model training logs and infer a few things
    on how Amazon ML trains and selects the best model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go back to the last Titanic model, and in the summary part, click on the Download
    Log link. The log file is too long to be reproduced here but is available at [https://github.com/alexperrier/packt-aml/blob/master/ch5/titanic_training.log](https://github.com/alexperrier/packt-aml/blob/master/ch5/titanic_training.log):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Amazon ML launches five versions of the SGD algorithm in parallel. Each version
    is called a learner and corresponds to a different value for the learning rate:
    0.01, 0.1,1, 10, and 100\. The following five metrics are calculated at each new
    pass of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F1-score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `negative-log-likelihood` is also calculated to assess whether the last
    iterations have brought significant improvement in reducing the residual error.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the learning rate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you recall from [Chapter 2](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml), *Machine
    Learning Definitions and Concepts*, under the section *Regularization on linear
    models*, the **Stochastic Gradient Descent (SGD)** algorithm has a parameter called
    the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: The SGD is based on the idea of taking each new (block of) data sample to make
    little corrections to the linear regression model coefficients. At each iteration,
    the input data samples are used either on a sample-by-sample basis or on a block-by-block
    basis to estimate the best correction (the so-called gradient) to make to the
    linear regression coefficients to further reduce the estimation error. It has
    been shown that the SGD algorithm converges to an optimal solution for the linear
    regression weights. These corrections are multiplied by a parameter called the
    `learning rate` , which drives the amount of correction brought to the coefficients
    at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: SGD calculations are low in computation costs. It's a fascinating yet simple
    algorithm that is used in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a marble in a bowl. Set the marble on the rim of the bowl and let it
    drop into the bowl with a circular movement. It will circle around the bowl while
    falling to the bottom. At the end of its descent, it will tend to circle around
    the bottom of the bowl and finally come to rest at the lowest point of the bowl.
    The SGD behaves similarly when you consider the marble as the prediction error
    at each iteration and the bottom of the bowl as the ultimate and most optimal
    coefficients that could be estimated. At each iteration, the prediction error
    becomes smaller on average. The error will not follow the most direct path to
    the bottom of the bowl like the marble does, nor will it reach the lowest most
    optimal solution, but on average, the predictions get better and the error decreases
    iteration after iteration. After a certain number of iterations, the error will
    approach its potential optimal minimum. How fast and how close it gets to the
    minimum error and the best coefficients depends directly on the value of the learning
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate controls how much the weights are corrected at each iteration.
    The learning rate drives the convergence of the algorithm. The larger the learning
    rate, the faster the convergence and potentially, the larger the residual error
    once converged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, choosing an optimal learning rate will be a balance between the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A faster convergence and poorer estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A slower convergence and more accurate estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, if the learning rate is too small, the convergence can be too slow
    and take too long to reach an optimal solution. One standard strategy is to decrease
    the learning rate as the algorithm converges, thus ensuring a fast convergence
    at the beginning, which will slow down as the prediction error becomes slower.
    As the learning rate decreases, the coefficient estimation becomes more accurate.
    Small learning rates mean that the algorithm converges slowly, while higher values
    mean each new sample has a bigger impact on the correcting factor. Amazon ML does
    not use that strategy and keeps the learning rate constant. In Amazon ML, the
    learning rate is set for you. You cannot choose a value.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing convergence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By parsing the logs, we can extract the following convergence plots for our
    Titanic model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous screenshot of different plots shows four metrics: Accuracy, AUC,
    F1 score, and Precision for the five different values of the learning rate. The
    model was set to 50 passes with mild (10^-6) L2 regularization on the Titanic
    training dataset. We can see that, for all metrics, the best value for the learning
    rate is either 10 or 100, with a slight advantage for learning rate=100\. These
    values converge faster and reach better scores. The smallest learning rate (0.01)
    converges far slower. In our context, faster convergence and large learning rate
    values beat smaller rate values.'
  prefs: []
  type: TYPE_NORMAL
- en: The default number of passes when creating a model is *10*. We can see that
    10 iterations would not have been sufficient for the score to stabilize and converge.
    At the 10th iteration, the curves are barely out of the chaotic initialization
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the negative log likelihood graph extracted from the logs, we also
    see that the best learner corresponds to a learning rate of 100 shown here by
    the curve with diamond shapes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_18.png)'
  prefs: []
  type: TYPE_IMG
- en: One conclusion that can be made from these graphs is that you should not limit
    your model to the default 10 passes.
  prefs: []
  type: TYPE_NORMAL
- en: These two convergence graphs are entirely dependent on the problem at hand.
    For a different dataset, we would have ended with entirely different graphs in
    terms of convergence rate, learning rate, and score achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following graph compares AUC for three different models:'
  prefs: []
  type: TYPE_NORMAL
- en: No regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mild regularization (10^-6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggressive regularization (10^-2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B05028_05_19.png)'
  prefs: []
  type: TYPE_IMG
- en: We notice that there is no significant difference between having no regularization
    and having mild regularization. Aggressive regularization, however, has a direct
    impact on the model performance. The algorithm converges to a lower AUC, and the
    optimal learning rate is no longer 100 but 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing the performance graph given by Amazon ML for mild and aggressive
    regularization, we see that although the scores (AUC, accuracy, and so on) are
    very similar in both cases, the difference lies with the certainty of the predictions.
    In the mild regularization case (left graph), the predictions are far apart. The
    probabilities or predictions that a sample is zero or one are very distinct. In
    the aggressive regularization case (right graph), this separation is far less
    obvious. The probabilities for samples to belong to one class versus the other
    are much closer. The decision boundary is less clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Mild regularization** | **Aggressive regularization** |'
  prefs: []
  type: TYPE_TB
- en: '| ![](img/B05028_05_20.png) | ![](img/B05028_05_21.png) |'
  prefs: []
  type: TYPE_TB
- en: The goal of regularization being to decouple the performance of the model from
    the training data in order to reduce overfitting, it may well be that, on the
    held-out dataset on previously unseen data, heavy regularization would give better
    results and no regularization would perform worse than mild regularization. Less
    optimal performance in the training-validation phase is sometimes more robust
    during the real prediction phase. It’s important to keep in mind that performance
    in the validation phase does not always translate into performance in the prediction
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing different recipes on the Titanic dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this last section, we would like to compare several recipes and see if our
    SQL, based feature engineering drives a better model performance. In all our experimentation,
    the one thing that stood out with regards to the recipes Amazon ML suggested was
    that all the numeric variables ended up being categorized via quantile binning.
    The large number of bins was also in question. We compare the following scenarios
    on the `Titanic` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Suggested Amazon ML recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numeric values are kept as numeric. No quantile binning is involved in the recipe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The extended Titanic datasource we created in [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml), *Loading
    and Preparing the Dataset* is used with the suggested Amazon ML recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We slightly modified the extended Titanic dataset that was used in [Chapter
    4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml), *Loading and Preparing the Dataset:*
  prefs: []
  type: TYPE_NORMAL
- en: There was no need to have both `fare` and `log_fare`. We removed `fare`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We manually corrected some titles that were not properly extracted from the
    names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new extended dataset is available in the GitHub repository for his chapter
    as `ch5_extended_titanic_training.csv`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all three cases, we apply L2 mild regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping variables as numeric or applying quantile binning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We found that keeping all numeric variables as numeric and avoiding any quantile
    binning had a very direct and negative effect on the model performance. The overall
    score was far lower in the numeric case than in the quantile binning case: `AUC:
    0.81` for all numeric versus  `AUC: 0.88` for QB.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the convergence graph for the *All Numeric* model, it appears that
    the algorithm converged much more slowly than it had for the quantile binning
    model. It obviously had not converged after 50 passes, so we increased the number
    of passes to 100\. We also noticed that in the *All Numeric* case, the best learning
    rate was equal to 0.01, whereas in the quantile binning model, the best learning
    rate was much larger (10 or 100). A smaller learning rate induces a slower convergence
    rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also see on the following performance charts that the quantile binning model
    separates the classes much better than the All Numeric model:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Quantile Binning 50 passes** | **All Numeric 100 Passes** |'
  prefs: []
  type: TYPE_TB
- en: '| ![](img/B05028_05_23.png) | ![](img/B05028_05_24.png) |'
  prefs: []
  type: TYPE_TB
- en: 'So quantile binning is definitely preferable to no quantile binning. What about
    our efforts to extend the initial dataset with new features? Well, somehow, our
    extended model did not produce better results than the initial dataset. Extracting
    the `title` from the `name`, replacing missing values for the `age`, and extracting
    the `deck` from the `cabin` did not generate an obviously better model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Original Titanic dataset: AUC 0.88'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Extended Titanic dataset with feature engineering: AUC 0.82'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convergence and performance charts were similar for both models and are not
    reproduced here. Several factors can be at play here to explain why our improved
    dataset did not produce a better model, and further analysis would be required
    to understand which feature engineering had a positive impact on the model and
    which one did not. However, we will see in the next chapter that this may also
    have been dependent on the actual samples in the evaluation set. On average, the
    extended dataset generates better performances but for this particular trial,
    the associated model performed roughly the same as the one trained on the original
    dataset. The conclusion being that it is worth the effort to run several trials
    to assess the quality and performance of a model, and not rely on a unique trial
    where the particularities of the evaluation set may influence the comparison between
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the model logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The convergence plots were obtained by parsing the Amazon ML model logs to
    extract the data into a CSV file that could be used later on to create plots.
    The process is simple and mostly based on command line scripting using the `grep`
    and the `sed` commands. We want to extract and parse the following lines from
    the log file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And convert them into a CSV format as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| iteration | alpha | learner | accuracy | recall | precision | f1 | auc |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.01 | 1050 | 0.5937 | 0.56 | 0.4828 | 0.5185 | 0.6015 |'
  prefs: []
  type: TYPE_TB
- en: The first step is to extract the right lines from the log file. We notice that
    they all contain the string `model-performance:`. We use grep to extract all the
    lines containing this string into a temporary file that we name `model_performance.tmp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy-paste the log data from the Amazon ML Model page into a log file (`model.log`)
    and in the terminal run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The trick then is to replace the right sub-strings by commas using the `sed`
    command. The `sed` command follows this syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `-i.bak` option makes it possible to replace the string within the file
    itself without the need to create a temporary file.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for instance, replacing the string `INFO: learner-id=` by a comma in the
    `model_performance.tmp` file is obtained by running the following line in a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'With the following commands, most of the original log file will have been transformed
    into a CSV formatted file, which you can use as a base for visualizing the convergence
    of the Amazon ML model. The rest of the file cleaning can be done in a spreadsheet
    editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'A similar pattern can be used to extract the negative log likelihood data from
    the Amazon ML model logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We end up with a CSV file with a row for each iteration and a column for the
    learning rate and each metric.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we created predictive models in Amazon ML--from selecting
    the datasource, applying transformations to the initial data with recipes, and
    analyzing the performance of the trained model. The model performance exploration
    depends on the type of prediction problem at hand: binary, multi-classification,
    or regression. We also looked at the model logs for the Titanic dataset and learned
    how the SGD algorithm trains and selects the best model out of several different
    ones with different learning rates.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we compared several data transformation strategies and their impact
    on the model performance and algorithm convergence in the context of the Titanic
    dataset. We found out that quantile binning of numeric values is a key strategy in
    boosting the convergence speed of the algorithm, which overall generated much
    better models.
  prefs: []
  type: TYPE_NORMAL
- en: So far, these models and performance evaluation are all obtained on training
    data. That is data that is fully available to the model from the start. The raison d'être of
    these models is not to run on subsets of the training data, but to make robust
    predictions on previously unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will apply these models on the held-out datasets we
    created in [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml), *Loading and
    preparing the dataset,* to make real predictions.
  prefs: []
  type: TYPE_NORMAL
