- en: Model Creation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have now created several data sources based on the original `Titanic` dataset
    in S3\. We are ready to train and evaluate an Amazon ML prediction model. In Amazon
    ML, creating a model consists of the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the training datasource
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a recipe for data transformation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting the parameters of the learning algorithm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the quality of the model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will start by exploring the data transformations available
    in Amazon ML, and we will compare different recipes for the `Titanic` dataset.
    Amazon ML defines recipes by default depending on the nature of the data. We will
    investigate and challenge these default transformations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: The model-building step is simple enough, and we will spend some time examining
    the available parameters. The model evaluation is where everything converges.
    The evaluation metrics are dependent on the type of the prediction at hand, regression,
    binary or multi-class classification. We will look at how these different evaluations
    are carried out. We will also download the model training logs to better understand
    what goes on under the Amazon ML hood when training the model. We will conclude
    the chapter by comparing the model evaluation for several data recipes and regularization
    strategies.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter is organized as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Recipes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model parameters
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluations
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log analysis
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering, recipes, and regularization
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the end of [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml), *Loading
    and Preparing the Dataset,* we modified the schema to exclude three variables:
    `boat`, `body`, and `home.dest` from the original dataset and created a new datasource
    based on this schema. We will use this datasource to train the model.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to your Amazon ML datasource dashboard; you should see three datasources:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: 'Titanic train set: It is the original raw dataset with 14 variables'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Titanic train set 11 variables: Has 11 variables; `boat`, `body` and `home.dest`
    have been removed from the schema'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Titanic train set extended: It is the cleaned up and extended dataset we obtained
    through SQL-based feature engineering.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will work with the `Titanic train set 11 variables` datasource. Before starting
    with the model creation, let’s first review what types of data transformations
    are available in Amazon ML.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data with recipes
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A crucial element of the data science workflow is feature engineering. Amazon
    ML offers certain data transformations via its data recipes. Note that although
    transformations are conceptually part of the ETL or data preparation phase of
    a predictive analytics workflow, in Amazon ML, data recipes are part of the model-building
    step and not of the initial datasource creation step. In this section, we start
    by reviewing the available data transformations in Amazon ML, and then we apply
    some of them to the `Titanic` dataset using the `Titanic train set 11 variables`
    datasource.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Managing variables
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipes are JSON-structured scripts that contains the following three sections
    in the given order:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Groups
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assignments
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An empty recipe instructing Amazon ML to take all the dataset variables into
    account for model training will be as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The recipe does not transform the data in any way.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The complete Amazon ML recipes documentation is available at [http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html](http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Grouping variables
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Groups enable grouping of multiple variables to facilitate applying the same
    transformations to several variables. The groups section of the recipe has a naming
    function. Group definition follows this syntax:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Amazon ML has defined a set of default groups based on the type of the variables:
    `ALL_TEXT`, `ALL_NUMERIC`, `ALL_CATEGORICAL`, `ALL_BINARY`, and the `ALL_INPUTS`
    group for all the variables at once. Let''s look at a couple of examples.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example where we want to apply the same transformation
    (normalization) on the `age` and `fare` variables. We can define a group and name
    it `TO_BE_NORMALIZED`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Similarly, consider an e-mail spam detection context where for each e-mail,
    we have a header, a subject, and a body. We want to create N-grams of the e-mail
    title and body but not of the header; we can define a group composed of all text
    variables with the exception of specifically excluded ones. Here we create a group
    named `N_GRAM_TEXT` that combines all text variables except the header:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Naming variables with assignments
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main purpose of assignments is naming facilitation. You can choose to name
    the transformed variable or group of variables in the assignments section or directly
    in the output section. Assignments are only for convenience and readability.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'Assignments follow this syntax:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For instance, you could rename and normalize the numeric variables as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Or rename and process the subject and body of your e-mails:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can also leave the assignments section empty and apply the transformations
    to the variables groups in the output section. In the end, it’s more a question
    of style and readability than anything else.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Specifying outputs
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The outputs section is where you explicitly list all the variables that will
    be used for the model training. If you have defined a group with some of the variables
    but you still want the original variables to be accounted for, you need to explicitly
    list them. The assignment section declares a list composed of the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Groups
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assignments
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variables
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformation (variable)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, if you wanted the original body and subject of the e-mails as
    well as the `bigrams` you defined in assignments, you would need to declare the
    outputs as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following outputs declaration declares all the text variables and adds
    the bigrams assignment defined earlier on:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The recipe format reference page has other examples of combining groups, assignments,
    and outputs to create recipes: [http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html](http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html).
    We will now look at the available transformations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 菜单格式参考页面提供了其他示例，展示了如何结合分组、分配和输出以创建菜单：[http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html](http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html)。现在我们将查看可用的转换。
- en: Data processing through seven transformations
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过七个转换进行数据处理
- en: 'Amazon ML offers the following seven transformations. Four transformations
    for text variables are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon ML提供了以下七个转换。以下是对文本变量的四个转换：
- en: Lowercase transformation
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小写转换
- en: Remove punctuation transformation
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除标点符号转换
- en: N-gram transformation
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N-gram转换
- en: '**Orthogonal sparse bigram (OSB)** transformation'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正交稀疏二元组（OSB**）转换'
- en: 'Two transformations for numeric variables are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值变量，以下有两种转换方式：
- en: Normalization transformation
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正态化转换
- en: Quantile binning transformation
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分位数装箱转换
- en: 'And one transformation for coupling text with categorical variables:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以及一种用于将文本与分类变量耦合的转换
- en: Cartesian product transformation
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡尔丹产品转换
- en: These transformations are well explained on the Amazon ML documentation ([http://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html](http://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换在Amazon ML文档中有很好的解释（[http://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html](http://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html)）。
- en: Using simple transformations
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用简单的转换
- en: The lowercase transformation takes a text variable as input and returns the
    text in lowercase: `Amazon ML is great for Predictive Analytics` is returned as
    `amazon ml is great for predictive analytics`. Syntax for lowercase transformation
    is `lowercase(text_variable)` or `lowercase(group_of_text_variables)`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 小写转换将文本变量作为输入，并返回小写文本：`Amazon ML is great for Predictive Analytics`返回为`amazon
    ml is great for predictive analytics`。小写转换的语法是`lowercase(text_variable)`或`lowercase(group_of_text_variables)`。
- en: The remove punctuation transformation also takes a text variable as input and
    removes all punctuation signs, with the exception of hyphens within words (`seat-belts`
    will remain as `seat-belts`). It is not possible to define your own set of punctuation
    signs. Syntax for the remove punctuation transformation is `no_punct(text_variable)`
    or `no_punct(group_of_text_variables)`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 删除标点符号转换也接受文本变量作为输入，并删除所有标点符号，但单词内的连字符除外（`seat-belts`将保持为`seat-belts`）。无法定义自己的标点符号集合。删除标点符号转换的语法是`no_punct(text_variable)`或`no_punct(group_of_text_variables)`。
- en: The normalization transformation normalizes numeric variables to have a mean
    of zero and a variance of one. This is a useful transformation when numeric variables
    vary significantly in range. This transformation corresponds to the **z-score**
    normalization also known as  standardization and not to the min-max normalization
    (see [Chapter 2,](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml) *Machine Learning
    Definitions and Concepts*). Syntax for normalization transformation is `normalize(numeric_variable)`
    or `normalize(group_of_numeric_variables)`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正态化转换将数值变量标准化，使其均值为零，方差为1。当数值变量的范围变化很大时，这种转换非常有用。这种转换对应于**z分数**标准化，也称为标准化，而不是最小-最大标准化（参见[第2章](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml)，*机器学习定义和概念*）。正态化转换的语法是`normalize(numeric_variable)`或`normalize(group_of_numeric_variables)`。
- en: Text mining
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本挖掘
- en: The N-gram and the **orthogonal sparse bigram** (**OSB**) transformations are
    the main text-mining transformations available in Amazon ML.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram和**正交稀疏二元组（OSB**）转换是Amazon ML中可用的主要文本挖掘转换。
- en: In text mining, the classic approach is called the **bag-of-words** approach.
    This approach boils down to discarding the order of the word in a given text and
    only considering the relative frequency of the words in the documents. Although
    it may seem to be overly simplistic, since the order of the words is essential
    to understand a message, this approach has given satisfying results in all types
    of natural language processing problems. A key part of the bag-of-words method,
    is driven by the need to extract the words from a given text. However, instead
    of considering single words as the only elements holding information, we could
    extract sequences of words. These sequences are called N-grams. Sequences of two
    words are called bigrams, for three words trigrams, and so forth. Single words
    are also called unigrams. N-grams are also called tokens and the process of extracting
    words, and N-grams from a text is called tokenization.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, consider the sentence: *The brown fox jumps over the dog*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '**Unigrams** are {*The, brown, fox, jumps, over, the, dog*}'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bigrams** are {*The brown, brown fox, fox jumps, jumps over, over the, the
    dog*}'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trigrams** are {*The brown fox, brown fox jumps, fox jumps over, jumps over
    the, over the dog*}'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no rule or heuristic that would let you know if you need N-grams in
    your model or what order of N-grams would be the most beneficial for your model.
    It depends on the type of text you are dealing with. Only experimentation can
    tell.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon ML offers two tokenization transformations: N-gram and OSB.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'The **N-gram** transformation: Takes a text variable and an integer from 2
    to 10 and returns expected N-grams. Note that all text variables are, by default,
    tokenized as unigrams in Amazon ML. There is no need to explicitly specify unigrams
    in the recipe. `ngram(text_variable, n)` will produce bigrams for *n= 2*, trigrams
    for *n=3* and so forth.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The **OSB** or orthogonal sparse bigram transformation is an extension on the
    bigram transformation (N*-gram* with *n=2*). Given a word in a text, compose pairs
    of words by associating the other words separated by *1,2, …, N* words from the
    initial word. *N* being the size of the OSB window.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in the sentence *this is a limited time offer*, first consider
    the word *offer*. The OSBs for a window of four are: *time_offer*, *limited_<skip>_offer*,
    *a_<skip>_<skip>_offer*, *is_<skip>_<skip>_<skip>_offer*, *this_<skip>_<skip>_<skip>_<skip>_offer*.
    We build word pairs by skipping 1,2,..., N words each time.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: The OSB transformation allows us to extract information about the context surrounding
    each word. For instance, the OSB *is_<skip>_<skip>_offer*, could be used to detect
    strings such as *is a special offer* as well as *is our best offer*. OSB extraction
    has been found to generally improve the performance of spam filtering algorithms.
    Syntax for OSB transformation is `osb(text_variable, N)`, with `N` the size of
    the window ranging from 2 to 10.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that some very standard text transformations are absent from
    Amazon ML recipes. Stemming and **Lemmatization** are used to regroup words with
    different endings to a common base form (for instance, *walking*, *walker* and
    *walked* would all be accounted for as *walk*) and are not offered in Amazon ML.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, removing very common words, known as *stopwords*, such as articles
    or prepositions (the, a, but, in, is, are, and so on) from a text is also a very
    standard text-mining transformation but is not an option in Amazon ML recipes.
    It is nonetheless possible that Amazon ML carries out similar transformations
    in the background without explicitly stating so. However, nothing in the available
    documentation indicates that to be the case.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Coupling variables
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Cartesian product transformation combines two categorical or text variables
    into one. Consider, for instance, a dataset of books and for each book, their
    title and genre. We could imagine that the title of a book has some correlation
    with its genre, and creating a new `title_genre` variable would bring forth that
    relation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following four books, their titles, and genres. Coupling the words
    in the title with the genre of the book adds extra information to the words in
    the title. Information that the model could use effectively. This is illustrated
    in the `title_genre` column in the following table:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '| **Title** | **Genre** | **title_genre** |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: '| *All the Birds in the Sky* | scifi | {`all_scifi`, `birds_scifi`, `sky_scifi`}
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
- en: '| *Robots and Empire* | scifi | `{robots_scifi`, `emprire_scifi`} |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
- en: '| *The Real Cool Killers* | crime | {`real_crime`, `cool_crime`, `killers_crime`}
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
- en: '| *Bullet in the Sky* | crime | {`bullet_crime`, `sky_crime`} |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
- en: The word sky now takes a different meaning if it's in the title of a crime novel: `sky_crime`
    or in the title of a SciFi novel: `sky_scifi`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the `Titanic` dataset, we could couple the `sibsp` and `*parch*`
    variables (number of siblings and number of parents) by taking their cartesian
    products: `sibsp*parch` and come up with a new variable that distinguishes between
    passengers with (without) parents and few or no siblings from those with (without)
    parents and many siblings. Syntax is `cartesian(var1, var2)`.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Binning numeric values
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final and most important transformation is quantile binning. The goal with
    quantile binning is to transform a numeric variable into a categorical one in
    order to better extract the relation between the variable and the prediction target.
    This is particularly useful in the presence of nonlinearities between a variable
    and the target. By splitting the original numeric variables values into *n* bins
    of equal size, it is possible to substitute each value by a corresponding bin.
    Since the number of bins is finite (from 2 to 1,000), the variable is now categorical.
    Syntax is `quantile_bin(var, N)` with `N` the number of bins.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of unsupervised binning, equal frequency and equal width
    binning. In equal frequency, each bin has the same number of samples, whereas
    in equal width binning, the variable range is split into N smaller ranges of equal
    width. Quantile binning usually refers to equal frequency binning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Categorizing continuous data is not always a good approach as you are, by definition,
    throwing away information that could be useful for the model. This page lists
    several other problems associated with binning: [http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous](http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous).
    However, Amazon ML seems to be quite fond of the quantile binning technique. In
    fact, of all the datasets we considered, Amazon ML always applied quantile binning
    to all the numeric variables, in the suggested recipe and often used large, sometimes
    very large, number of bins. For instance, the default transformation for the `fare`
    variable in the `Titanic` dataset was quantile binning with 500 bins although
    the variable only ranged from 0 to 512\. We compare the evaluations obtained by
    keeping the original numeric values versus applying quantile binning at the end
    of this chapter, *Keeping variables as numeric or applying quantile binning?*
    section Now that we’ve explored the available recipes, let’s look at how Amazon
    ML suggests we transform our `Titanic` dataset
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon ML always suggests a recipe based on your datasource when you create
    a model. You can choose to use that recipe or to modify it. We will now create
    our first model and during that process analyze the recipe Amazon ML has generated
    for us.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Go to the model dashboard, and click on the Create new... | ML model button.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'You will go through three screens:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Select the datasource, choose the `Titanic train set with 11 variables`.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon ML will validate the datasource and present a summary.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Choose the default or Custom model creation; choose the custom path:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B05028_05_02.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'The next screen is split between the attributes, their type and a sample of
    values on the left side, and the suggested recipe on the right side, as shown
    in the following screenshot:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_03.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Editing the suggested recipe
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is where you can edit the recipe and replace it with a recipe of your own
    creation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: You can find all the JSON in this chapter in the book's GitHub repository, properly
    formatted and indented at [https://github.com/alexperrier/packt-aml/blob/master/ch5/recipes.json](https://github.com/alexperrier/packt-aml/blob/master/ch5/recipes.json).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipe is validated while you type. Anything not respecting the JSON format
    will result in the following error message: `Recipe must be valid JSON`. Some
    common errors include the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Indentation is not respected
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last element between braces should not be followed by a comma
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All strings must be between double quotes
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manually formatting JSON text is not fun. This online JSON editor is very helpful:
    [http://www.cleancss.com/json-editor/](http://www.cleancss.com/json-editor/).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Applying recipes to the Titanic dataset
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recipe generated by Amazon ML for our dataset is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: All numeric values are quantile binned. No further processing is done on the
    text, binary, or categorical variables. The output section of the recipe shows
    that the numeric variables are replaced by the binned equivalent.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Further comments can be made on this recipe:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The `sibsp` and `parch` variables are grouped together. First of all, both `sibsp`
    and `parch` have similar ranges, 0 to 9 and 0 to 8 respectively. It makes sense
    to have the same number of bins for both variables.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Amazon ML chose 50 bins for `sibsp` and `parch`, 100 bins for `age`, and
    10 bins for `fare` is less clear.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We found that the number of bins was very sensitive to the data in the training
    set. Several versions of the initial datasets produced very different binning
    numbers. One constant in all our trials was that all the numeric values went through
    quantile binning with a rather high number of bins. In one instance, Amazon ML
    suggested 500 bins for the `fare` variable and 200 for the `age` variable. In
    both cases, we would have ended with a very small number of samples per bin since
    our total number of training sample consists of just 1,047 passengers. How Amazon
    ML calculates the optimal number of bins is not clear.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other transformations Amazon ML could decide to apply to our `Titanic`
    dataset such as the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Extracting bigrams or OSBs from the passengers' titles
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coupling `sibsp` and `parch` with cartesian product transformation
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing between recipes and data pre-processing.
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have transformed our initial dataset via scripts and Amazon ML recipes.
    The two techniques are complementary. Some transformation and data manipulation
    can only be done by preprocessing the data. We did so in [Chapter 4,](08d9b49a-a25c-4706-8846-36be9538b087.xhtml) *Loading
    and Preparing the Dataset* with Athena and SQL. We could have achieved similar
    data processing with other scripting languages such as Python or R, which are
    most fruitful for creative feature engineering. SQL and scripts can also better
    deal with outliers and missing values — corrections that are not available with
    Amazon ML recipes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the Amazon ML transformations is to prepare the data for consumption
    by the Amazon ML algorithm, whereas scripted feature engineering is about cleaning
    up the data and creating new variables out of the original dataset.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Although Amazon ML recipes are quite restrained, they offer an easy way to
    fiddle around with the dataset and quickly compare models based on different recipes.
    Creating a new model and associated evaluation from a given datasource and schema only
    takes a few clicks. And by choosing to write different recipes for each model,
    it becomes possible to experiment with a wide range of datasets. Recipes allow
    us to create a fast try-fail loop. The associated workflow becomes the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Specify the datasource.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with different recipes.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create or remove variables.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and select the model associated to that recipe.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the best recipe is found, then start optimizing the model parameters, regularization,
    passes, and memory.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can compare how we transformed the data with scripting (Athena and SQL)
    and with recipes:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '**Recipes:**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Removing features (`boat`, `body`, and `home.dest`). This can also be done via
    the schema or directly by removing the columns from the dataset CSV file.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cartesian product for an indication of family by aggregating `parch` and `sibsp`.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization of numeric values (a possibility).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization of all text variables, names, destinations, and so on.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantile binning of all numeric values; although the number of bins were large
    this transformation produced good results.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scripting (SQL):**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values for `age`: We replaced all missing values by the mean
    of the *age*
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text processing: We extracted `titles` from the `name` variables'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Created a new feature, the `family_size` as the sum of `parch` and `sibsp`
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extraction of the `deck` from the cabin number
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both approaches are very complementary.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Parametrizing the model
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that our data has been prepared for the SGD algorithm, we are ready to
    set the parameters of our experiment. In a way similar to scientific experimentation,
    we will want to try out several sets of parameters to test several models and
    pick up the best one. The next screenshot shows where we actually specify our model
    parameters:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Model memory
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data passes
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuffling
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B05028_05_04.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: Setting model memory
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model memory is related to the memory Amazon ML must set aside to build and
    evaluate your model. It is set, by default, to 100Mb. In the case of the `Titanic`
    dataset, the model memory was always below 1Mb as shown by the logs. Model memory
    is also used to set aside memory when dealing with streaming data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Setting the number of data passes
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon ML will use the training set of samples several times, each time shuffling
    it and using the new sequence to increase prediction. It's similar to squeezing
    a wet piece of cloth — each time you wring it, more water comes out of it. Set
    by default to 10 passes, it does not hurt to set it to the maximum value of a
    100 at the expense of a longer training time for the model and a higher cost of
    operation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Choosing regularization
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen in [Chapter 2,](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml) *Machine
    Learning Definitions and Concepts*, regularization makes your model more robust
    and allows it to better handle previously unseen data by reducing overfitting.
    The rule of thumb is to lower regularization if your evaluation score is poor
    (underfitting) and increase it if your model shows great performance on the training
    set but poor results on the evaluation set (overfitting).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Creating an evaluation
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Evaluations and models are independent in Amazon ML. You can train a model
    and carry out several evaluations by specifying different evaluation datasets.
    The evaluation page, shown in the following screenshot, lets you name and specify
    how the model will be evaluated:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_05.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: As you know by now, to evaluate a model, you need to split your dataset into
    two parts, the training and the evaluation sets with a 70/30 split. The training
    part is used to train your model, while the evaluation part is used to evaluate
    the model. At this point, you can let Amazon ML split the dataset into training
    and evaluation or specify a different datasource for evaluation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the initial `Titanic` file was ordered by class and passenger alphabetical
    order. Using this ordered dataset and splitting it without shuffling, that is,
    taking sequentially the first 70% samples, would give the model a very different
    data for the training and the evaluation sets. The evaluation would not be relevant.
    However, if your data is not already shuffled, you can tell Amazon ML to shuffle
    it. It is a good practice to let Amazon ML reshuffle your data by default just
    in case your own randomizing left some sequential patterns in the dataset.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ML will make some verifications regarding your training and validation
    sets, checking that there is enough data for the validation, that the two sets
    follow similar distributions, and that the evaluation set has valid samples. Take
    a look at [http://docs.aws.amazon.com/machine-learning/latest/dg/evaluation-alerts.html](http://docs.aws.amazon.com/machine-learning/latest/dg/evaluation-alerts.html) for
    more information on Evaluation Alerts.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you choose to let Amazon ML split the data, it will create two
    new datasources titled in a way that lets you see how the split was performed.
    You can reuse these new datasources if you decide to test another model with different
    recipes or model parameters such as regularization.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '`Titanic.csv_[percentBegin=0, percentEnd=70, strategy=sequential]`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Titanic.csv_[percentBegin=70, percentEnd=100, strategy=random]`'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click on Review, make sure your model is as expected, and click on the final
    *Create ML model* button. Creating the model usually takes a few minutes.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, Amazon ML will use the training set to train several models and
    the evaluation sets to select the best one.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ML runs several model training in parallel, each time trying new parameters
    and shuffling the training set at each new pass. Once the number of passes initially
    set has been exhausted or the algorithm has converged, whichever comes first,
    the model is considered trained. For each model it trains, Amazon ML uses it for
    prediction on the validation subset to obtain an evaluation score per model. Once
    all the models have been trained and evaluated this way, Amazon ML simply selects
    the one with the best evaluation score.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation metric depends on the type of prediction at hand. AUC and `F1`
    score for classification (binary and multiclass), and RMSE for regression. How
    the evaluation results are displayed by Amazon ML also depends on the type of
    prediction at hand.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with evaluation for binary classification for our Titanic prediction,
    followed by the regression case with a new dataset related to Air traffic delays,
    and finally perform multiclass classification with the classic `Iris` dataset.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating binary classification
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once your model is ready, click on the model's title from the service dashboard
    to access the model's result page, which contains the summary of the model, its
    settings and the evaluation results.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot shows that we obtained an *AUC* score of `0.880`, which
    is considered very good for most machine-learning applications. **AUC** stands
    for the **Area under the Curve** and was introduced in [Chapter 2](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml), *Machine
    Learning Definitions and Concepts*. It is the de-facto metric for classification
    problems.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'The baseline for Binary classification is an AUC of 0.5, which is the score
    for a model that would randomly predict 0 or 1:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_06.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: 'Amazon ML validates the model by checking the following conditions and raising
    alerts in case the conditions are not met:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_07.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: 'In our case, no alerts were raised:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The training and validation datasets were separate
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The validation dataset had a sufficient number of samples
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The validation and training sets shared the same schema
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All samples of the validation set were valid and used for the evaluation, implying
    that the target was not missing for one or more samples
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distribution of the target variable was similar in the training and validation
    sets
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these alerts will not happen if we let Amazon ML handle the training
    validation data split, but they might be more frequent if we provide the validation
    set ourselves.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The AUC score is not the only element Amazon ML gives us to evaluate the quality
    of our model. By clicking on the *Explore performance* link, we can analyze further
    the performance of our model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the model performances
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may recall from [Chapter 2](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml):
    *Machine Learning Definitions and Concepts,* that in a binary classification context,
    a logistic regression model calculates for each sample to be predicted a probability
    — the probability of belonging to one class or the other. The model will not directly
    output the class of the sample to be predicted. The sample is assigned to one
    class or the other depending on whether the probability is below or above a certain
    threshold. By default, this threshold is set to 0.5\. Although the AUC score given
    by the evaluation does not depend on the value of the decision threshold, other
    classification metrics do. We can change the value of the threshold and see how
    that impacts our predictions.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: The **Explore performance** page of the evaluation shows several other classification
    metrics as well as the confusion matrix of the model. The vertical bar in the
    graph below is a cursor that can slide left or right. By sliding the cursor, we
    increase or decrease the decision threshold used to classify a prediction sample
    as belonging to one class or another. As we move that cursor, the following metrics
    vary accordingly.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: False positive rate
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision: proportion of predicted positives that are truly positives'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall (the proportion of positives that are correctly identified)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a threshold of *0.5*, we have the following sceenshot:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_08.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: 'If we lower the threshold to *0.4*, accuracy decreases while recall increases,
    as you can see in the following screenshot:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_09.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'And if we raise the threshold to *0.7*, accuracy increases slightly while recall
    decreases:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_10.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: In our context, the predictions are quite clearly separated between survived
    and did not survive values. Slightly changing the threshold does not have a huge impact
    on the metrics.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating linear regression
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon ML uses the standard metric RMSE for linear regression. RMSE is defined
    as the sum of the squares of the difference between the real values and the predicted
    values:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_015.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: Where *ŷ* are the predicted values and *y* the real values. The closer the predictions
    are to the real values, the lower the RMSE is; therefore, a lower RMSE is interpreted
    as a better predictive model.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the evaluation in the regression context, we will consider a
    simplified version of the **Airlines delay** dataset available on Kaggle at [https://www.kaggle.com/giovamata/airlinedelaycauses](https://www.kaggle.com/giovamata/airlinedelaycauses).
    The full dataset is quite large (*~250Mb*). We extracted roughly *19,000* rows
    from the year 2008, filtering out cancelled flights. We also removed several variables
    that were too correlated with our target, which is the `Airdelay` variable. The
    resulting dataset and schema are available on GitHub at [https://github.com/alexperrier/packt-aml/tree/master/ch5](https://github.com/alexperrier/packt-aml/tree/master/ch5).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'We upload the dataset to S3, create a datasource, train and evaluate a model
    and finally obtain an RMSE of 7.0557 with a baseline of 31.312. The baseline for
    regression is given by a model that always predicts the average of the target:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_14.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: 'Exploring further, we obtain the following histograms of residuals. As we can
    see in the next screenshot, the errors are roughly bell-shaped and centered around
    *0*, meaning that our errors are half the time overestimating/underestimating
    the real values. All the information available in the dataset has been consumed
    by the model:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_15.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: Evaluating multiclass classification
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The classic dataset for multiclass classification is the `Iris` dataset composed
    of three types of Iris flowers. This dataset is quite simple, very popular and
    using it to illustrate the performance of a platform as powerful as Amazon ML
    seems overkill. Luckily, there are another three class datasets composed of seeds.
    The seeds dataset is available at [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)
    and of course on the GitHub repository accompanying this book (as well as the
    schema).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The seed dataset has 210 samples distributed evenly among three different `seedTypes`
    and seven attributes. The dataset has an ID, which must be set to categorical,
    all attributes are NUMERIC, and the target is the `seedType.` We upload the dataset
    to S3, and create a datasource and a model.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'The metric for multiclass classification is the *F1* score defined as the harmonic
    mean of precision and recall:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_13.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: 'The baseline for a multiclass classification problem is the macro average *F1*
    score for a model that would always predict the most common class. In the case
    of the seed dataset, we obtain a *F1* score of 0.870 for baseline of 0.143:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_11.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: 'Performance exploration is not as developed as in the binary classification
    case. Amazon ML gives us the confusion matrix which shows, for each class, the
    ratio of correctly predicted samples over the real number of samples in that class:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_12.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: Analyzing the logs
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For every operation it carries out, Amazon ML gives us access to the related
    logs. We can download and analyze the model training logs and infer a few things
    on how Amazon ML trains and selects the best model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Go back to the last Titanic model, and in the summary part, click on the Download
    Log link. The log file is too long to be reproduced here but is available at [https://github.com/alexperrier/packt-aml/blob/master/ch5/titanic_training.log](https://github.com/alexperrier/packt-aml/blob/master/ch5/titanic_training.log):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_16.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'Amazon ML launches five versions of the SGD algorithm in parallel. Each version
    is called a learner and corresponds to a different value for the learning rate:
    0.01, 0.1,1, 10, and 100\. The following five metrics are calculated at each new
    pass of the algorithm:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F1-score
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUC
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `negative-log-likelihood` is also calculated to assess whether the last
    iterations have brought significant improvement in reducing the residual error.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the learning rate
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you recall from [Chapter 2](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml), *Machine
    Learning Definitions and Concepts*, under the section *Regularization on linear
    models*, the **Stochastic Gradient Descent (SGD)** algorithm has a parameter called
    the learning rate.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The SGD is based on the idea of taking each new (block of) data sample to make
    little corrections to the linear regression model coefficients. At each iteration,
    the input data samples are used either on a sample-by-sample basis or on a block-by-block
    basis to estimate the best correction (the so-called gradient) to make to the
    linear regression coefficients to further reduce the estimation error. It has
    been shown that the SGD algorithm converges to an optimal solution for the linear
    regression weights. These corrections are multiplied by a parameter called the
    `learning rate` , which drives the amount of correction brought to the coefficients
    at each iteration.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: SGD calculations are low in computation costs. It's a fascinating yet simple
    algorithm that is used in many applications.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a marble in a bowl. Set the marble on the rim of the bowl and let it
    drop into the bowl with a circular movement. It will circle around the bowl while
    falling to the bottom. At the end of its descent, it will tend to circle around
    the bottom of the bowl and finally come to rest at the lowest point of the bowl.
    The SGD behaves similarly when you consider the marble as the prediction error
    at each iteration and the bottom of the bowl as the ultimate and most optimal
    coefficients that could be estimated. At each iteration, the prediction error
    becomes smaller on average. The error will not follow the most direct path to
    the bottom of the bowl like the marble does, nor will it reach the lowest most
    optimal solution, but on average, the predictions get better and the error decreases
    iteration after iteration. After a certain number of iterations, the error will
    approach its potential optimal minimum. How fast and how close it gets to the
    minimum error and the best coefficients depends directly on the value of the learning
    rate.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate controls how much the weights are corrected at each iteration.
    The learning rate drives the convergence of the algorithm. The larger the learning
    rate, the faster the convergence and potentially, the larger the residual error
    once converged.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, choosing an optimal learning rate will be a balance between the following:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: A faster convergence and poorer estimation
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A slower convergence and more accurate estimation
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, if the learning rate is too small, the convergence can be too slow
    and take too long to reach an optimal solution. One standard strategy is to decrease
    the learning rate as the algorithm converges, thus ensuring a fast convergence
    at the beginning, which will slow down as the prediction error becomes slower.
    As the learning rate decreases, the coefficient estimation becomes more accurate.
    Small learning rates mean that the algorithm converges slowly, while higher values
    mean each new sample has a bigger impact on the correcting factor. Amazon ML does
    not use that strategy and keeps the learning rate constant. In Amazon ML, the
    learning rate is set for you. You cannot choose a value.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing convergence
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By parsing the logs, we can extract the following convergence plots for our
    Titanic model:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_17.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: 'The previous screenshot of different plots shows four metrics: Accuracy, AUC,
    F1 score, and Precision for the five different values of the learning rate. The
    model was set to 50 passes with mild (10^-6) L2 regularization on the Titanic
    training dataset. We can see that, for all metrics, the best value for the learning
    rate is either 10 or 100, with a slight advantage for learning rate=100\. These
    values converge faster and reach better scores. The smallest learning rate (0.01)
    converges far slower. In our context, faster convergence and large learning rate
    values beat smaller rate values.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: The default number of passes when creating a model is *10*. We can see that
    10 iterations would not have been sufficient for the score to stabilize and converge.
    At the 10th iteration, the curves are barely out of the chaotic initialization
    phase.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the negative log likelihood graph extracted from the logs, we also
    see that the best learner corresponds to a learning rate of 100 shown here by
    the curve with diamond shapes:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_18.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: One conclusion that can be made from these graphs is that you should not limit
    your model to the default 10 passes.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: These two convergence graphs are entirely dependent on the problem at hand.
    For a different dataset, we would have ended with entirely different graphs in
    terms of convergence rate, learning rate, and score achieved.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Impact of regularization
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following graph compares AUC for three different models:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: No regularization
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mild regularization (10^-6)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggressive regularization (10^-2)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B05028_05_19.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: We notice that there is no significant difference between having no regularization
    and having mild regularization. Aggressive regularization, however, has a direct
    impact on the model performance. The algorithm converges to a lower AUC, and the
    optimal learning rate is no longer 100 but 1.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing the performance graph given by Amazon ML for mild and aggressive
    regularization, we see that although the scores (AUC, accuracy, and so on) are
    very similar in both cases, the difference lies with the certainty of the predictions.
    In the mild regularization case (left graph), the predictions are far apart. The
    probabilities or predictions that a sample is zero or one are very distinct. In
    the aggressive regularization case (right graph), this separation is far less
    obvious. The probabilities for samples to belong to one class versus the other
    are much closer. The decision boundary is less clear:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '| **Mild regularization** | **Aggressive regularization** |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| ![](img/B05028_05_20.png) | ![](img/B05028_05_21.png) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: The goal of regularization being to decouple the performance of the model from
    the training data in order to reduce overfitting, it may well be that, on the
    held-out dataset on previously unseen data, heavy regularization would give better
    results and no regularization would perform worse than mild regularization. Less
    optimal performance in the training-validation phase is sometimes more robust
    during the real prediction phase. It’s important to keep in mind that performance
    in the validation phase does not always translate into performance in the prediction
    phase.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Comparing different recipes on the Titanic dataset
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this last section, we would like to compare several recipes and see if our
    SQL, based feature engineering drives a better model performance. In all our experimentation,
    the one thing that stood out with regards to the recipes Amazon ML suggested was
    that all the numeric variables ended up being categorized via quantile binning.
    The large number of bins was also in question. We compare the following scenarios
    on the `Titanic` dataset:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Suggested Amazon ML recipe
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numeric values are kept as numeric. No quantile binning is involved in the recipe.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The extended Titanic datasource we created in [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml), *Loading
    and Preparing the Dataset* is used with the suggested Amazon ML recipe
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We slightly modified the extended Titanic dataset that was used in [Chapter
    4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml), *Loading and Preparing the Dataset:*
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: There was no need to have both `fare` and `log_fare`. We removed `fare`.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We manually corrected some titles that were not properly extracted from the
    names.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new extended dataset is available in the GitHub repository for his chapter
    as `ch5_extended_titanic_training.csv`.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all three cases, we apply L2 mild regularization.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Keeping variables as numeric or applying quantile binning?
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We found that keeping all numeric variables as numeric and avoiding any quantile
    binning had a very direct and negative effect on the model performance. The overall
    score was far lower in the numeric case than in the quantile binning case: `AUC:
    0.81` for all numeric versus  `AUC: 0.88` for QB.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the convergence graph for the *All Numeric* model, it appears that
    the algorithm converged much more slowly than it had for the quantile binning
    model. It obviously had not converged after 50 passes, so we increased the number
    of passes to 100\. We also noticed that in the *All Numeric* case, the best learning
    rate was equal to 0.01, whereas in the quantile binning model, the best learning
    rate was much larger (10 or 100). A smaller learning rate induces a slower convergence
    rate:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_05_22.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: 'We also see on the following performance charts that the quantile binning model
    separates the classes much better than the All Numeric model:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '| **Quantile Binning 50 passes** | **All Numeric 100 Passes** |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| ![](img/B05028_05_23.png) | ![](img/B05028_05_24.png) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: 'So quantile binning is definitely preferable to no quantile binning. What about
    our efforts to extend the initial dataset with new features? Well, somehow, our
    extended model did not produce better results than the initial dataset. Extracting
    the `title` from the `name`, replacing missing values for the `age`, and extracting
    the `deck` from the `cabin` did not generate an obviously better model:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Original Titanic dataset: AUC 0.88'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Extended Titanic dataset with feature engineering: AUC 0.82'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convergence and performance charts were similar for both models and are not
    reproduced here. Several factors can be at play here to explain why our improved
    dataset did not produce a better model, and further analysis would be required
    to understand which feature engineering had a positive impact on the model and
    which one did not. However, we will see in the next chapter that this may also
    have been dependent on the actual samples in the evaluation set. On average, the
    extended dataset generates better performances but for this particular trial,
    the associated model performed roughly the same as the one trained on the original
    dataset. The conclusion being that it is worth the effort to run several trials
    to assess the quality and performance of a model, and not rely on a unique trial
    where the particularities of the evaluation set may influence the comparison between
    models.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the model logs
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The convergence plots were obtained by parsing the Amazon ML model logs to
    extract the data into a CSV file that could be used later on to create plots.
    The process is simple and mostly based on command line scripting using the `grep`
    and the `sed` commands. We want to extract and parse the following lines from
    the log file:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And convert them into a CSV format as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '| iteration | alpha | learner | accuracy | recall | precision | f1 | auc |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.01 | 1050 | 0.5937 | 0.56 | 0.4828 | 0.5185 | 0.6015 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: The first step is to extract the right lines from the log file. We notice that
    they all contain the string `model-performance:`. We use grep to extract all the
    lines containing this string into a temporary file that we name `model_performance.tmp`.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy-paste the log data from the Amazon ML Model page into a log file (`model.log`)
    and in the terminal run the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The trick then is to replace the right sub-strings by commas using the `sed`
    command. The `sed` command follows this syntax:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `-i.bak` option makes it possible to replace the string within the file
    itself without the need to create a temporary file.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for instance, replacing the string `INFO: learner-id=` by a comma in the
    `model_performance.tmp` file is obtained by running the following line in a terminal:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With the following commands, most of the original log file will have been transformed
    into a CSV formatted file, which you can use as a base for visualizing the convergence
    of the Amazon ML model. The rest of the file cleaning can be done in a spreadsheet
    editor:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A similar pattern can be used to extract the negative log likelihood data from
    the Amazon ML model logs:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We end up with a CSV file with a row for each iteration and a column for the
    learning rate and each metric.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we created predictive models in Amazon ML--from selecting
    the datasource, applying transformations to the initial data with recipes, and
    analyzing the performance of the trained model. The model performance exploration
    depends on the type of prediction problem at hand: binary, multi-classification,
    or regression. We also looked at the model logs for the Titanic dataset and learned
    how the SGD algorithm trains and selects the best model out of several different
    ones with different learning rates.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we compared several data transformation strategies and their impact
    on the model performance and algorithm convergence in the context of the Titanic
    dataset. We found out that quantile binning of numeric values is a key strategy in
    boosting the convergence speed of the algorithm, which overall generated much
    better models.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: So far, these models and performance evaluation are all obtained on training
    data. That is data that is fully available to the model from the start. The raison d'être of
    these models is not to run on subsets of the training data, but to make robust
    predictions on previously unseen data.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will apply these models on the held-out datasets we
    created in [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml), *Loading and
    preparing the dataset,* to make real predictions.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
