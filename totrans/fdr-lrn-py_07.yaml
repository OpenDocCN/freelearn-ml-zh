- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Model Aggregation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型聚合
- en: In the *Model aggregation basics* section of [*Chapter 3*](B18369_03.xhtml#_idTextAnchor058),
    *Workings of the Federated Learning System*, we introduced the concept of aggregation
    within the **federated learning** (**FL**) process at a high level. Recall that
    aggregation is the means by which an FL approach uses the models trained locally
    by each agent to produce a model with strong global performance. It is clear to
    see that the strength and robustness of the aggregation method employed are directly
    correlated to the resulting performance of the end global model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](B18369_03.xhtml#_idTextAnchor058)的“模型聚合基础”部分，我们介绍了联邦学习（**FL**）过程中聚合的概念。回想一下，聚合是联邦学习方法使用每个代理在本地训练的模型来产生具有强大全局性能的模型的方式。很明显，所采用的聚合方法的强度和鲁棒性与最终全局模型的性能直接相关。
- en: As a result, choosing the appropriate aggregation method based on the local
    datasets, agents, and FL system hierarchy is key to achieving good performance
    with FL. In fact, the focal point of many publications in this field is providing
    mathematically backed convergence guarantees for these methods in a variety of
    theoretical scenarios.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据本地数据集、代理和联邦学习系统层次结构选择合适的聚合方法是实现联邦学习良好性能的关键。实际上，该领域许多出版物的研究焦点是提供这些方法在各种理论场景下的数学保证收敛性。
- en: The goal of this chapter is to cover some of the research that has been done
    on aggregation methods and their convergence in both ideal and non-ideal cases,
    tying these methods to their strengths in the different scenarios that arise in
    the practical applications of FL. After reading the chapter, you should be able
    to understand how different characterizations of an FL scenario call for different
    aggregation methods, and you should have an idea of how these algorithms can actually
    be implemented.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是介绍一些关于聚合方法及其在理想和非理想情况下的收敛性的研究，将这些方法与它们在联邦学习实际应用中出现的不同场景中的优势联系起来。阅读完本章后，你应该能够理解不同的联邦学习场景特征如何要求不同的聚合方法，并且应该对如何实现这些算法有一个大致的了解。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Revisiting aggregation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新审视聚合
- en: Understanding FedAvg
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解FedAvg
- en: Modifying aggregation for non-ideal cases
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改非理想情况下的聚合
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The Python algorithm implementations presented in the book can all be found
    in the `ch7` folder, which is located at [https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7](https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 书中展示的Python算法实现都可以在`ch7`文件夹中找到，该文件夹位于[https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7](https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7)。
- en: Important note
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: You can use the code files for personal or educational purposes. Please note
    that we will not support deployments for commercial use and will not be responsible
    for any errors, issues, or damages caused by using the code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用代码文件用于个人或教育目的。请注意，我们不会支持商业部署，并且不会对使用代码造成的任何错误、问题或损害负责。
- en: For the pure aggregation algorithms, auxiliary code is included to display example
    output from preset local parameters. The aggregation methods that modify the local
    training process require an FL system in order to operate – for these, full implementations
    using STADLE are included. Also, the pure aggregation algorithms can be directly
    tested with STADLE by configuring the aggregation method. Information on how to
    run the examples can be found in the associated `README` files.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于纯聚合算法，包括辅助代码以显示从预设的本地参数中获取的示例输出。修改本地训练过程的聚合方法需要一个联邦学习系统来运行——对于这些，包括使用STADLE的完整实现。此外，纯聚合算法可以通过配置聚合方法直接使用STADLE进行测试。有关运行示例的信息可以在相关的`README`文件中找到。
- en: 'The installation of the `stadle-client` package through `pip` is necessary
    to run the full FL process examples. The following command can be used to perform
    this installation:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`pip`安装`stadle-client`包是运行完整的联邦学习过程示例所必需的。以下命令可以用来执行此安装：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using a virtual environment is recommended to isolate the specific package versions
    installed with `stadle-client` from other installations on the system.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 建议使用虚拟环境来隔离`stadle-client`安装的特定包版本与其他系统上的安装。
- en: Revisiting aggregation
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视聚合
- en: 'To solidly contextualize aggregation within FL, first, we describe the components
    of a system that are necessary for FL to be applied:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在联邦学习中将聚合的上下文语境化，首先，我们描述了应用联邦学习所必需的系统组件：
- en: A set of computational agents that perform the local training portion of FL.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组执行联邦学习本地训练部分的计算代理。
- en: Each agent possesses a local dataset (static or dynamic), of which no portion
    can be communicated to another agent under the strictest FL scenario.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个代理都拥有一个本地数据集（静态或动态），在严格的联邦学习场景下，其任何部分都不能被发送给另一个代理。
- en: Each agent possesses a parameterized model that can be trained on the local
    dataset, a process that produces the local optima parameter set for the model.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个代理都拥有一个参数化的模型，可以在本地数据集上进行训练，这个过程产生了模型的本地最优参数集。
- en: A parameter server, or aggregator, which receives the locally trained models
    at each iteration from the agents and sends back the resulting model produced
    by the aggregation method chosen to be used.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数服务器或聚合器，在每个迭代中从代理那里接收本地训练的模型，并返回由所选聚合方法产生的结果模型。
- en: 'Every FL communication round can then be broken down into the following two
    phases:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 每一轮联邦学习通信都可以分解为以下两个阶段：
- en: The *local training phase*, where agents train their local models on their local
    datasets for some number of iterations
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*本地训练阶段*，在这个阶段，代理在其本地数据集上对本地模型进行多次迭代训练'
- en: The *aggregation phase*, where the agents send the resulting trained local models
    from the previous phase to the aggregator and receive the aggregated model for
    use as the starting model in the local training phase of the next round.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚合阶段*，在这个阶段，代理将上一阶段训练好的本地模型发送给聚合器，并接收聚合模型作为下一轮本地训练阶段的起始模型。'
- en: So, what exactly does it mean for an agent to send a locally trained model during
    the aggregation phase? The general approach is to use the *parameter sets* that
    define the local models, allowing for some degree of generalization across all
    models that can be parameterized in such a way. However, a second approach focuses
    on sending the *local gradients* accumulated during the local training when using
    a gradient-based optimization approach to the aggregator, with the agents updating
    their models using the received aggregate gradient at the end of the round. While
    this approach restricts usage to models with gradient-based local training methods,
    the prevalence of such methods when training deep learning models has led to a
    subset of aggregation methods based on gradient aggregation. In this chapter,
    we choose to frame model aggregation through the lens of the FedAvg algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在聚合阶段，一个代理发送一个本地训练的模型究竟意味着什么？一般的方法是使用定义本地模型的*参数集*，允许在所有可以以这种方式参数化的模型之间实现一定程度的泛化。然而，第二种方法侧重于在基于梯度的优化方法中，将本地训练期间累积的*本地梯度*发送给聚合器，代理在每一轮结束时使用接收到的聚合梯度更新他们的模型。虽然这种方法限制了只能使用基于梯度的本地训练方法的模型，但这种方法在训练深度学习模型时的普遍性导致了基于梯度聚合的聚合方法的一个子集。在本章中，我们选择通过FedAvg算法的视角来构建模型聚合的框架。
- en: Understanding FedAvg
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解FedAvg
- en: 'In [*Chapter 3*](B18369_03.xhtml#_idTextAnchor058), *Workings of the Federated
    Learning System*, the aggregation algorithm known as FedAvg was introduced to
    help clarify the general structure and represent the more abstract concepts discussed
    earlier with a specific example. FedAvg was used for two reasons: simplicity in
    the underlying algorithm, and generalizability across more model types than gradient-based
    approaches. It also benefits from extensive references by researchers, with performance
    analysis in different theoretical scenarios using FedAvg as a baseline when proposing
    new aggregation methods. This focus in the research community can most likely
    be attributed to the fact that the original FedAvg paper was published by the
    team working at Google that first brought exposure to the concept and benefits
    of FL. For further reading, this paper can be found at [https://arxiv.org/abs/1602.05629?context=cs](https://arxiv.org/abs/1602.05629?context=cs).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第3章*](B18369_03.xhtml#_idTextAnchor058)“联邦学习系统的工作原理”中，介绍了名为FedAvg的聚合算法，以帮助阐明一般结构和用具体示例表示之前讨论的更抽象的概念。FedAvg被用于两个原因：底层算法的简单性和比基于梯度的方法更广泛模型类型的通用性。它还受益于研究者的广泛引用，在提出新的聚合方法时，使用FedAvg作为基准在不同理论场景中进行性能分析。这种研究界的关注很可能归因于原始FedAvg论文是由第一个将FL的概念和好处公之于众的谷歌团队发表的。关于进一步阅读，这篇论文可以在[https://arxiv.org/abs/1602.05629?context=cs](https://arxiv.org/abs/1602.05629?context=cs)找到。
- en: FedAvg is predated by an aggregation approach known as **Federated Stochastic
    Gradient Descent** (**FedSGD**). FedSGD can be viewed as the gradient aggregation
    analog of the model parameter averaged performed by FedAvg. In addition, the concept
    of averaging model parameters was examined prior to FedAvg for parallelized SGD
    approaches, outside of the context of FL. Essentially, the analysis of these parallelized
    SGD approaches mirrors the **Independently and Identically Distributed** (**IID**)
    case of FedAvg – this concept will be discussed later in the section. Regardless,
    the simplicity, generalizability, and popularity of FedAvg make it a good base
    to delve deeper into, contextualizing the need for the numerous aggregation approaches
    discussed in later sections that have built upon or, otherwise, improved on FedAvg.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: FedAvg之前有一个称为**联邦随机梯度下降**（**FedSGD**）的聚合方法。FedSGD可以看作是FedAvg执行的模型参数平均的梯度聚合类似物。此外，在FL的背景下，在FedAvg之前还研究了平均模型参数的概念，用于并行化SGD方法。本质上，这些并行化SGD方法的分析反映了FedAvg的**独立同分布**（**IID**）情况——这一概念将在后面的章节中讨论。无论如何，FedAvg的简单性、通用性和流行性使其成为深入研究的好基础，为后面章节中讨论的众多聚合方法提供了背景，这些方法基于或改进了FedAvg。
- en: 'Previously, FedAvg was only presented as an algorithm that takes models *![](img/B18369_07_F05.png)*with
    a respective local dataset size of ![](img/B18369_07_F04.png), where the sum equals
    *N* and returns:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，FedAvg仅被呈现为一个算法，它接受具有相应本地数据集大小的模型 *![](img/B18369_07_F05.png)*，其中总和等于 *N*，并返回：
- en: '![](img/B18369_07_F06.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18369_07_F06.jpg)'
- en: 'As shown in the *Aggregating local models* section of [*Chapter 4*](B18369_04.xhtml#_idTextAnchor085),
    *Federated Learning Server Implementation with Python*, *simple-fl* uses the following
    function to compute a weighted average of the buffered models (models sent from
    clients during the current round) based on the amount of data used to locally
    train each model:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第4章*](B18369_04.xhtml#_idTextAnchor085)中“聚合本地模型”部分所示，*使用Python实现的联邦学习服务器实现*（simple-fl）使用以下函数根据每个模型本地训练所使用的数据量来计算缓冲模型（当前轮次中客户端发送的模型）的加权平均值：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The original algorithm does not differ too greatly from this portrayal. The
    high-level steps of the algorithm are as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 原始算法与这种描述差异不大。算法的高级步骤如下：
- en: The server randomly samples *K * C* clients, where *K* is the total number of
    clients and *C* is a parameter between 0 and 1.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务器随机抽取 *K * C* 个客户端，其中 *K* 是客户端的总数，*C* 是介于0和1之间的一个参数。
- en: The selected *K * C* clients receive the most recent aggregate model and begin
    to train the model on their local data.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选定的 *K * C* 客户端接收最新的聚合模型，并开始在他们的本地数据上训练模型。
- en: Each client sends its locally trained model back to the server after some desired
    amount of training is completed.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个客户端在完成一定量的训练后，将其本地训练的模型发送回服务器。
- en: The server computes the parameter-wise arithmetic mean of the received models
    to compute the newest aggregate model.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务器计算接收到的模型的参数算术平均值，以计算最新的聚合模型。
- en: Parallels can be immediately drawn between this formal representation and our
    presentation of the FL process, with `ClientUpdate` performing local training
    for an agent and the server performing aggregation using the same weighted averaging
    algorithm. One important point is the sampling of a subset of clients to perform
    the local training and model transmission during each round, allowing for client
    subsampling parameterized by C. This parameter is included to experimentally determine
    the convergence rates of various client set sizes – in an ideal scenario, this
    will be set to `1`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可以立即将这种形式表示与我们对联邦学习过程的介绍进行比较，其中 `ClientUpdate` 为代理执行本地训练，服务器使用相同的加权平均算法进行聚合。一个重要点是，在每一轮中采样一部分客户端进行本地训练和模型传输，允许通过
    C 参数进行客户端子采样。这个参数包括实验性地确定各种客户端集大小的收敛速度——在理想情况下，这个值将被设置为 `1`。
- en: As previously mentioned, FedAvg is the ideal FL scenario that essentially mirrors
    an approach to parallelized stochastic gradient descent. In **parallelized SGD**
    (**pSGD**), the goal is to leverage hardware parallelization (for example, running
    on multiple cores in parallel) in order to speed up SGD convergence on a specific
    machine learning task. One approach for this task is for each core to train a
    base model on some subset of the data in parallel for some number of iterations,
    then aggregate the partially trained models and use the aggregated models as the
    next base for training. In this case, if the cores are considered to be agents
    in an FL scenario, the parallelized SGD approach is the same as FedAvg in an ideal
    scenario. This means all of the convergence guarantees and respective analyses
    that were done for pSGD can be directly applied to FedAvg, assuming the ideal
    FL scenario. From this prior work, it has, therefore, been shown that FedAvg demonstrates
    strong convergence rates.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，FedAvg 是一种理想的联邦学习场景，本质上反映了并行化随机梯度下降的方法。在 **并行化随机梯度下降（pSGD**）中，目标是利用硬件并行化（例如，在多个核心上并行运行）来加速特定机器学习任务上的
    SGD 收敛。为此任务的一种方法是每个核心并行地在数据的一些子集上训练基础模型若干次迭代，然后聚合部分训练好的模型，并使用聚合模型作为下一次训练的基础。在这种情况下，如果将核心视为联邦学习场景中的代理，那么并行化
    SGD 方法与理想情况下的 FedAvg 是相同的。这意味着为 pSGD 所做的所有收敛保证和相应的分析都可以直接应用于 FedAvg，假设是理想联邦学习场景。因此，从这项先前工作中可以看出，FedAvg
    显示出强大的收敛速度。
- en: After all this praise for FedAvg, it is only natural to question why more complex
    aggregation methods are even necessary. Recall that the phrase “ideal FL scenario”
    was used several times when discussing FedAvg convergence. The unfortunate reality
    is that most practical FL applications will fail to meet one or more of the conditions
    stipulated by that phrase.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 FedAvg 进行了所有这些赞誉之后，自然会质疑为什么更复杂的聚合方法甚至有必要。回想一下，在讨论 FedAvg 收敛时，多次使用了“理想联邦学习场景”这个短语。不幸的现实是，大多数实际的联邦学习应用将无法满足该短语所规定的条件之一或多个。
- en: 'The ideal FL scenario can be broken down into three main conditions:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 理想联邦学习场景可以分解为三个主要条件：
- en: The local datasets used for training are IID (the datasets are independently
    drawn from the same data distribution).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练的本地数据集是 IID（数据集是从相同的数据分布中独立抽取的）。
- en: The computational agents are relatively homogeneous in computational power.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算代理在计算能力上相对同质。
- en: All agents can be assumed to be non-adversarial.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以假设所有代理都不是对抗性的。
- en: At a high level, it is clear why these qualities would be desirable in an FL
    scenario. To understand, in greater detail, why these three conditions are necessary,
    the performance of FedAvg in the absence of each condition will be examined in
    the upcoming subsections.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，为什么这些特性在联邦学习场景中是可取的是显而易见的。为了更详细地了解为什么这三个条件是必要的，将在接下来的小节中检查在没有每个条件的情况下
    FedAvg 的性能。
- en: Dataset distributions
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集分布
- en: To examine FedAvg in the non-IID case, first, it is important to define what
    exactly is being referred to by the distribution of a dataset. In classification
    problems, the data distribution often refers to the distribution of the true classes
    associated with each data point. For example, consider the MNIST dataset, where
    each image is a handwritten digit from 0 to 9\. If a uniform random sample of
    1,000 images was to be taken from the dataset, the expected number of images from
    each class would be the same – this could be considered a uniform data distribution.
    Alternatively, a sample with 910 images of the digit 0 along with 10 images of
    the other digits would be a heavily skewed data distribution.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检验非IID情况下的FedAvg，首先，定义数据集的分布究竟指的是什么非常重要。在分类问题中，数据分布通常指的是与每个数据点相关的真实类别的分布。例如，考虑MNIST数据集，其中每个图像是从0到9的手写数字。如果从数据集中抽取1,000个图像的均匀随机样本，每个类别的预期图像数量将是相同的——这可以被认为是一种均匀数据分布。或者，一个包含910个数字0的图像和10个其他数字的图像的样本将是一个严重偏斜的数据分布。
- en: To generalize outside of classification tasks, this definition can be extended
    to refer to the distribution of *features* present across the dataset. These features
    could be manually crafted and provided to the model (such as linear regression),
    or they could be extracted from the raw data as part of the model pipeline (such
    as deep CNN models). For classification problems, the class distribution is generally
    contained within the feature distribution, due to the implicit belief that the
    features are sufficient for correctly predicting the class. The benefit of looking
    at feature distributions is the data-centric focus on features (versus the task-centric
    focus on classes), allowing for generalization across machine learning tasks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将定义推广到分类任务之外，可以将它扩展到指代数据集中存在的*特征*的分布。这些特征可能是手动制作并提供给模型的（例如线性回归），或者它们可以作为模型管道的一部分从原始数据中提取（例如深度CNN模型）。对于分类问题，类分布通常包含在特征分布中，这是由于隐含的信念，即特征足以正确预测类别。查看特征分布的好处是，它关注于数据（相对于关注于任务的类别），允许在机器学习任务中进行泛化。
- en: However, in the context of experimental analysis, the ability to easily construct
    non-IID samples from a dataset makes classification tasks ideal for testing the
    robustness of FedAvg and different aggregation methods within an FL context. To
    examine FedAvg in this section, consider a toy FL scenario where each agent trains
    a CNN on data samples taken from the MNIST dataset described earlier. There are
    two main cases, which are detailed next.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实验分析的情况下，从数据集中轻松构建非IID样本的能力使得分类任务非常适合测试FedAvg在FL环境中的鲁棒性以及不同的聚合方法。为了在本节中检验FedAvg，考虑一个玩具FL场景，其中每个代理在前面描述的MNIST数据集的数据样本上训练CNN。有两种主要情况，下面将详细说明。
- en: IID case
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IID情况
- en: The convergence of the models can be represented through the use of the model
    parameter space. The parameter space of a model with *n* parameters can be thought
    of as an *n*-dimensional Euclidean space, where each parameter corresponds to
    one dimension in the space. Consider an initialized model; the initial parameters
    of this model can then be represented as a *point* in the parameter space. As
    local training and aggregation occur, this representative point will move in the
    parameter space, with the end goal being convergence to a point in the space corresponding
    to a local optimum of the loss or error function being minimized.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的收敛可以通过使用模型参数空间来表示。具有*n*个参数的模型参数空间可以被视为一个*n*-维欧几里得空间，其中每个参数对应于空间中的一个维度。考虑一个初始化的模型；这个模型的初始参数可以表示为参数空间中的一个*点*。随着局部训练和聚合的发生，这个代表点将在参数空间中移动，最终目标是收敛到空间中的一个点，该点对应于损失或误差函数的最小化局部最优。
- en: 'One key point of these functions is the dependence on the data used during
    the local training process – when the datasets across the agents are IID, there
    is a general tendency for the optima of the respective loss/error functions to
    be relatively close in the parameter space. Consider a trivial case where the
    datasets are IID and all models are initialized with the same parameters. As shown
    in the *Model aggregation basics* section of [*Chapter 3*](B18369_03.xhtml#_idTextAnchor058),
    *Workings of the Federated Learning System*, a simplified version of the parameter
    space can be depicted:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数的一个关键点是它们依赖于在本地训练过程中使用的数据 – 当代理之间的数据集是 IID 时，相应的损失/误差函数的最优解在参数空间中相对接近。考虑一个简单的情况，即数据集是
    IID，并且所有模型都使用相同的参数初始化。如[*第 3 章*](B18369_03.xhtml#_idTextAnchor058)中[*联邦学习系统的工作原理*](B18369_03.xhtml#_idTextAnchor058)的[*模型聚合基础*](B18369_03.xhtml#_idTextAnchor058)部分所示，参数空间的一个简化版本可以表示如下：
- en: '![Figure 7.1 – Models with the same initialization and IID datasets'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.1 – 具有相同初始化和 IID 数据集的模型'
- en: '](img/B18369_07_01.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18369_07_01.jpg]'
- en: Figure 7.1 – Models with the same initialization and IID datasets
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 具有相同初始化和 IID 数据集的模型
- en: Observe how both models start at the same point (purple x) and move toward the
    same optima (purple dot), resulting in aggregate models close to the optima shared
    by both models.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 观察两个模型如何从相同的起点（紫色 x）开始，并朝向相同的最优解（紫色点）移动，从而产生接近两个模型共享的最优解的聚合模型。
- en: Due to the resulting similarity of the error/loss functions across the agents,
    the models tend to converge toward the same or similar optima in the space during
    training. This means that the change in the models after each aggregation step
    is relatively small, resulting in the convergence rates mirroring the single local
    model case. If the underlying data distribution is representative of the true
    data distribution (for example, uniform across the 10 different digits for MNIST),
    the resulting aggregated model will demonstrate strong performance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代理之间误差/损失函数的相似性，模型在训练过程中倾向于收敛到相同或相似的最优解。这意味着在每个聚合步骤之后，模型的变化相对较小，导致收敛速度与单本地模型情况相匹配。如果底层数据分布代表真实数据分布（例如，MNIST
    中的 10 个不同数字是均匀的），则生成的聚合模型将表现出强大的性能。
- en: 'Next, consider the generalized IID case where each model is initialized separately:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，考虑每个模型分别初始化的广义 IID 情况：
- en: '![Figure 7.2 – Models with different initializations and IID datasets'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2 – 具有不同初始化和 IID 数据集的模型'
- en: '](img/B18369_07_02.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18369_07_02.jpg]'
- en: Figure 7.2 – Models with different initializations and IID datasets
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 具有不同初始化和 IID 数据集的模型
- en: In this scenario, observe how both models start at different points (bold/dotted
    *x*) and initially move toward different optima, producing a poor first model.
    However, after the first aggregation, both models start at the same point and
    move toward the same optima, resulting in similar convergence to the first case.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，观察两个模型如何从不同的起点（粗体/虚线 *x*）开始，并最初朝向不同的最优解移动，从而产生一个较差的第一个模型。然而，在第一次聚合之后，两个模型从相同的起点开始，并朝向相同的最优解移动，导致与第一个案例相似的收敛。
- en: It should be clear that this reduces to the previous case after the first aggregation
    step since each model starts the second round with the resulting aggregated parameters.
    As a result, the convergence properties previously stated can be extended to the
    general case of FedAvg with IID local datasets.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 应该很明显，在第一次聚合步骤之后，这会简化为之前的情况，因为每个模型都从聚合参数开始第二轮。因此，之前所述的收敛属性可以扩展到具有 IID 本地数据集的
    FedAvg 的一般情况。
- en: Non-IID Case
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非IID情况
- en: The key property in the IID local dataset case that allows for convergence speeds
    mirroring the single model case is the similarity of the local optima of the loss/error
    functions, due to their construction from similar data distributions. In the non-IID
    case, similarity in the optima is generally no longer observed.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 IID 本地数据集情况下，允许收敛速度与单模型情况相匹配的关键属性是由于它们从相似的数据分布中构建，损失/误差函数的局部最优相似性。在非 IID 情况下，最优相似性通常不再观察到。
- en: 'Using the MNIST example, let’s consider an FL scenario with two agents such
    that the first agent only has images with digits 0 to 4 and the second agent only
    has images with digits 5 to 9; that is, the datasets are not IID. These datasets
    would essentially lead to two completely different five-class classification tasks
    at the local training level, as opposed to the original 10-class classification
    problem – this will result in completely different parameter space optima between
    the first agent and the second agent. Consider the simplified representation of
    this parameter space as follows, with both models having the same initialization:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Models with different initializations and non-IID datasets'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18369_07_03.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – Models with different initializations and non-IID datasets
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Now that optima are no longer shared (triangles/squares representing optima
    for bold/dotted model, respectively), even repeated aggregations cannot create
    an aggregate model close to optima of either model. The models diverge, or drift,
    during each local training phase due to the different target optima in each round.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Only a small subset of the optima will be shared between the loss/error functions
    of both agents. As a result, there is a high probability that each model will
    move toward optima that are not shared during local training, leading the models
    to drift apart in the parameter space. Each aggregation step will then pull the
    models toward the wrong optima, reverting the progress made during local training
    and hampering convergence. Note that just taking the average of optima from different
    agents is very unlikely to be near optima from any of the agents in the parameter
    space, so in this case, the result of continued aggregation is generally a model
    that performs poorly across the whole dataset. Convergence to a shared optimum
    might eventually occur due to stochasticity observed during local training, inducing
    movement of the aggregate model in the parameter space, but this does not have
    theoretical guarantees and will be far slower than convergence in the IID case
    when it does occur.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: This MNIST example is a theoretical extreme of non-IID datasets. In practice,
    non-IID datasets might refer to different skews in the data distributions across
    agents (for example, twice as many images with digits 0–4 versus 5–9, and vice
    versa). The severity of the difference is correlated to the performance of FedAvg,
    so adequate performance can still be reached in less severe cases. However, in
    these cases, the performance of FedAvg will generally always be inferior to the
    analogous centralized training task where a single model is trained on all of
    the local datasets at once – the theoretically optimal model achievable by FL.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: While this section focused on the statistical basis for the issues that arise
    from non-IID datasets, the next section examines a far more direct problem that
    can arise – especially when deploying at larger scales.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Computational power distributions
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算能力分布
- en: An unstated assumption of the agents participating in FL is that each agent
    is capable of performing local training if given infinite time. Agents with limited
    computational power (memory and speed) might take significantly more time than
    other agents to finish local training, or they might require techniques such as
    quantization to support the model and training process. However, agents that cannot
    complete local training during some rounds will trivially prevent convergence
    by stalling the FL process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 参与联邦学习的代理的一个未声明的假设是，如果给予无限的时间，每个代理都能够执行本地训练。计算能力有限（内存和速度）的代理可能比其他代理花费更多的时间来完成本地训练，或者他们可能需要量化等技术来支持模型和训练过程。然而，在某些轮次中无法完成本地训练的代理将简单地通过阻碍联邦学习过程来阻止收敛。
- en: Generally, convergence bounds and experimental results focus on the number of
    communication rounds required to reach some level of performance. Under this metric
    and the aforementioned assumption, convergence is completely independent of the
    computational power afforded to each agent, since computational power only affects
    the actual time necessary to complete one round. However, convergence speed in
    practical applications is measured by the actual time taken, not the number of
    completed communication rounds – this means that the time to complete each round
    is as important as the number of rounds. This metric of the total time taken is
    where naïve FedAvg demonstrates poor performance when heterogeneous computation
    power is observed in the agents participating in FL
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，收敛界限和实验结果关注的是达到一定性能水平所需的通信轮数。在这个指标和上述假设下，收敛与分配给每个代理的计算能力完全无关，因为计算能力只影响完成一轮所需的实际时间。然而，在实际应用中，收敛速度是通过实际花费的时间来衡量的，而不是完成的通信轮数——这意味着完成每一轮的时间与轮数一样重要。这个总时间的指标是，当在参与联邦学习的代理中观察到异构计算能力时，简单的FedAvg表现不佳的地方。
- en: Specifically, the time to complete each round is bottlenecked by the local training
    time of the slowest agent participating in the round; this is because aggregation
    is trivially fast compared to training in most cases and must wait for all agents
    to complete local training. When all agents are participating in the round, this
    bottleneck becomes the slowest overall agent. In the homogeneous computational
    power case, the difference in local training time between the fastest agent and
    the slowest agent will be relatively insignificant. In the heterogeneous case,
    a single straggler will greatly reduce the convergence time of FedAvg and lead
    to significant idle time in the faster agents waiting to receive the aggregated
    model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，每轮完成的时间瓶颈在于参与该轮的最慢代理的本地训练时间；这是因为与大多数情况下的训练相比，聚合是极其快速的，并且必须等待所有代理完成本地训练。当所有代理都参与该轮时，这个瓶颈成为最慢的整体代理。在计算能力同质化的情况下，最快代理和最慢代理之间的本地训练时间差异将相对微不足道。在异质化的情况下，单个落后代理将大大减少FedAvg的收敛时间，并导致更快代理在等待接收聚合模型时产生显著的空闲时间。
- en: 'Two modifications to FedAvg with full agent participation might initially seem
    to address this problem; however, both have drawbacks that lead to suboptimal
    performance:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 两个针对完全参与代理的FedAvg的修改可能最初看起来可以解决这个问题；然而，两者都有缺点，导致性能次优：
- en: One approach is to rely on agent subsampling in each round, leading to the probability
    of the straggler effect occurring in each round depending on the number of agents
    and the sample size taken in each round. This can be sufficient in cases with
    only a few straggling agents, but it becomes proportionately worse as this number
    increases and does not completely eliminate the problem from occurring. In addition,
    small sample sizes lose out on the robustness benefits from aggregation over more
    agents.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种方法是依靠每轮的代理子采样，导致每轮发生落后效应的概率取决于代理的数量和每轮采样的样本大小。在只有少数落后代理的情况下，这可能足够，但随着这个数量的增加，问题会成比例地恶化，并且并不能完全消除问题发生的可能性。此外，小样本大小会失去从更多代理的聚合中获得的鲁棒性优势。
- en: A second approach is to allow all agents to begin local training at the beginning
    of each round and prematurely begin aggregation after some number of models have
    been received. This method has the benefit of being able to completely eliminate
    the straggler effect without greatly restricting the number of agents participating
    in aggregation during each round. However, it results in the slowest agents never
    participating in aggregation over all rounds, essentially reducing the number
    of active agents and potentially limiting the variety of data used during training.
    In addition, the agents that are too slow to participate in aggregation will have
    done computational work for no benefit.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种方法是允许所有代理在每个回合开始时开始本地训练，并在接收到一定数量的模型后提前开始聚合。这种方法的好处是能够在不大大限制每个回合参与聚合的代理数量的情况下，完全消除拖沓效应。然而，它会导致最慢的代理在整个回合中都不会参与聚合，这实际上减少了活跃代理的数量，并可能限制训练期间使用的数据的多样性。此外，那些太慢而无法参与聚合的代理将进行无益的计算工作。
- en: It is clear that some local adjustment based on available computational power
    is necessary for aggregation to be performed efficiently, regardless of the end
    aggregation method applied to the received models at the end of each round.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，无论在每个回合结束时应用于接收到的模型的最终聚合方法是什么，都需要基于可用的计算能力进行一些本地调整，以便有效地执行聚合。
- en: Both the non-IID case and the heterogeneous computation power case focus on
    properties of an FL system that are generally easy to observe and under some level
    of administrative control. The next case we present deviates from this by challenging
    a key assumption when considering practical FL systems.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 非独立同分布案例和异构计算能力案例都关注FL系统的一般容易观察到的属性，并在一定程度上受到行政控制。我们接下来提出的案例与此不同，因为它挑战了在考虑实际FL系统时的一项关键假设。
- en: Protecting against adversarial agents
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防御对抗性代理
- en: So far, it has been assumed that every agent participating in an FL scenario
    always acts in the desired way; that is, actively and correctly training the received
    model locally and participating in model transmission to/from the aggregator.
    This is easily achieved in a research setting, where the federated setting is
    simulated and the agents are singularly controlled; however, this assumption of
    agents behaving correctly does not always hold in practice.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一直假设每个参与FL场景的代理总是以期望的方式行事；也就是说，积极且正确地在本地训练接收到的模型，并参与模型向聚合器或从聚合器传输。这在研究环境中很容易实现，其中联邦设置被模拟，代理被单独控制；然而，代理行为正确的这种假设在实践中并不总是成立。
- en: One example that does not involve targeted malicious intent is an error in the
    model weights being transmitted by an agent to the aggregator. This can happen
    when the dataset used by an agent is flawed or the training algorithm is incorrectly
    implemented (the corruption of the parameter data during transmission is also
    possible). In the worst case, this can essentially lead to the parameters of one
    or many models being statistically equivalent to random noise. When the L2 norm
    (the extension of vector magnitude for *n*-dimensional tensors) of the random
    noise is not significantly greater than that of the valid models, FedAvg will
    suffer performance loss that is proportional to the ratio of faulty agents to
    all agents – which is relatively acceptable when this ratio is small. However,
    even a single faulty agent can induce a near-random aggregate model if the norm
    of the agent’s noise is significantly high. This is due to the nature of the arithmetic
    mean being performed internally during the FedAvg aggregation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不涉及针对性恶意意图的例子是代理向聚合器传输的模型权重中的错误。这可能发生在代理使用的数据集有缺陷或训练算法实现不正确（参数数据在传输过程中的损坏也是可能的）。在最坏的情况下，这可能导致一个或多个模型的参数在统计上等同于随机噪声。当随机噪声的L2范数（n维张量向量大小的扩展）不显著大于有效模型的范数时，FedAvg将遭受与故障代理与所有代理的比例成比例的性能损失——当这个比例较小时，这是相对可以接受的。然而，即使只有一个故障代理，如果代理噪声的范数显著较高，它也会导致几乎随机的聚合模型。这是由于FedAvg聚合过程中内部执行算术平均的性质。
- en: The problem becomes worse when agents can be controlled by malicious adversaries.
    A single malicious agent with sufficient information is capable of producing any
    desired model after aggregation through large modifications to the parameters
    of the model it submits. Even without direct knowledge of the model parameters
    and associated weights of the other agents, a malicious agent can leverage relatively
    small changes between the local models and the aggregate model in later rounds
    to use the previous aggregate model as an estimate of the expected local model
    parameters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理可以被恶意对手控制时，问题变得更加严重。一个拥有足够信息的恶意代理可以通过对其提交的模型参数进行大量修改，在聚合后产生任何期望的模型。即使没有直接了解其他代理的模型参数和相关权重，恶意代理也可以利用在后续回合中本地模型和聚合模型之间的相对较小变化，将之前的聚合模型作为对预期本地模型参数的估计。
- en: Therefore, FedAvg offers little to no robustness against both random and controlled
    adversarial agents in an FL setting. While one potential means of mitigation would
    be to separately monitor the agents and prevent adversarial agents from transmitting
    models, significant damage to the convergence of the final model might have already
    occurred in the time necessary to identify such agents.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，FedAvg在FL环境中对随机和受控的对抗性代理提供的鲁棒性很少或没有。虽然一种可能的缓解方法是对代理进行单独监控并防止对抗性代理传输模型，但在识别这些代理所需的时间内，最终模型的收敛可能已经受到了重大损害。
- en: It should now be clear that FedAvg trades robustness in these non-ideal cases
    for simplicity in the calculation. Unfortunately, this robustness is a key consideration
    for practical applications of FL due to the lack of control compared to the research
    setting. The next section focuses on methods of achieving robustness against the
    three non-ideal cases presented in this section.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该很清楚，FedAvg在这些非理想情况下牺牲了鲁棒性以换取计算的简单性。不幸的是，由于与研究环境相比缺乏控制，这种鲁棒性是FL实际应用中的一个关键考虑因素。下一节将重点介绍实现针对本节中提出的三个非理想情况鲁棒性的方法。
- en: Modifying aggregation for non-ideal cases
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改聚合以适应非理想情况
- en: In practical FL applications, at least one of the aforementioned assumptions
    that constitute an ideal FL scenario generally does not hold; therefore, the usage
    of alternative aggregation methods might be necessary to best perform FL. The
    goal of this section is to cover examples of aggregation methods that target heterogeneous
    computational power, adversarial agents, and non-IID datasets, in order of difficulty.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际的联邦学习（FL）应用中，上述构成理想FL场景的假设通常并不成立；因此，可能需要使用替代的聚合方法来最佳地执行FL。本节的目标是介绍针对异构计算能力、对抗性代理和非独立同分布（non-IID）数据集的聚合方法示例，按照难度顺序排列。
- en: Handling heterogeneous computational power
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理异构计算能力
- en: As mentioned earlier, the ideal aggregation approach, in this case, consistently
    avoids the straggler effect while maximizing the number of agents participating
    in FL and allowing all agents to contribute to some extent, regardless of computational
    power differences. Agents become stragglers during a round when their local training
    takes significantly more time than the majority of the agents. Therefore, effectively
    addressing this problem actually requires some level of adaptability at the agent
    level in the local training process, based on the computational power available
    to each agent.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在这种情况下，理想的聚合方法始终避免出现落后效应（straggler effect），同时最大化参与FL的代理数量，并允许所有代理在一定程度上做出贡献，无论计算能力差异如何。当代理的本地训练时间显著长于大多数代理时，它们在某一回合中成为落后者。因此，有效地解决这个问题实际上需要在代理级别上在本地训练过程中具有一定的适应性，基于每个代理可用的计算能力。
- en: Manual adjustment
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动调整
- en: One straightforward way of accomplishing this is to change the number of local
    training iterations based on the time necessary for each iteration. In other words,
    the local training time is fixed and each agent performs as many iterations as
    possible within this time, as opposed to performing a fixed number of iterations.
    This trivially eliminates the straggler problem but might result in poor performance
    if a large amount of local training time must be allocated for the slow agents
    to meaningfully contribute due to the model drift from faster agents potentially
    performing too many local training iterations. This can be mitigated by setting
    a maximum number of local training iterations. However, a careful balance in the
    allocated local training must be found to have enough time for slow agents to
    produce adequate models while preventing faster agents from sitting idle after
    reaching the maximum number of iterations. It is also unclear how such a threshold
    could be preemptively determined to achieve optimal performance instead of relying
    on experimental results to search for the best configuration.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的简单方法是根据每次迭代所需的时间来改变本地训练迭代的次数。换句话说，本地训练时间是固定的，每个代理尽可能在这个时间内完成尽可能多的迭代，而不是执行固定数量的迭代。这可以简单地消除落后者问题，但如果必须分配大量的本地训练时间给慢速代理以有意义地贡献，因为快速代理可能执行了过多的本地训练迭代而导致模型漂移，这可能会导致性能不佳。这可以通过设置最大本地训练迭代次数来缓解。然而，必须仔细平衡分配的本地训练时间，以便慢速代理有足够的时间产生足够的模型，同时防止快速代理在达到最大迭代次数后闲置。此外，如何预先确定这样的阈值以实现最佳性能，而不是依赖于实验结果来寻找最佳配置，目前还不清楚。
- en: Automatic adjustment – FedProx
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动调整 – FedProx
- en: An aggregation method known as FedProx follows this same methodology of dynamically
    adjusting the local training processes for each agent based on computational power,
    while also revising the termination condition for local training to aid in the
    theoretical analysis of convergence. Specifically, the fixed number of local training
    iterations is replaced by a termination condition for the training loop that accommodates
    agents with varying levels of computational power.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为 FedProx 的聚合方法遵循了基于计算能力动态调整每个代理的本地训练过程的相同方法，同时修改了本地训练的终止条件，以帮助进行收敛的理论分析。具体来说，固定的本地训练迭代次数被替换为训练循环的终止条件，该条件可以适应具有不同计算能力的代理。
- en: The underlying concept for this termination condition is the γ-inexact solution,
    which is satisfied when the magnitude of the gradient at the γ-inexact optima
    is less than γ times the magnitude of the gradient at the beginning of local training.
    Intuitively, γ is a value between 0 and 1, with values closer to 0 leading to
    more local training iterations due to the stricter termination condition. Therefore,
    γ allows for the parameterization of an agent’s computational power.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种终止条件的潜在概念是 γ-不精确解，当 γ-不精确最优点的梯度幅度小于本地训练开始时梯度幅度的 γ 倍时，该条件得到满足。直观地说，γ 是介于 0
    和 1 之间的一个值，值越接近 0，由于更严格的终止条件，会导致更多的本地训练迭代。因此，γ 允许参数化代理的计算能力。
- en: 'One potential problem with the termination condition approach is the divergence
    of the locally trained model from the aggregate model after many iterations of
    local training resulting from a strict condition. To combat this, FedProx adds
    a proximal term to the objective function being minimized equal to the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用终止条件方法的一个潜在问题是，由于严格的条件，在多次本地训练迭代后，局部训练模型可能会从聚合模型中发散。为了解决这个问题，FedProx 在被最小化的目标函数中添加了一个近端项，等于以下内容：
- en: '![](img/B18369_07_F01.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18369_07_F01.jpg)'
- en: Here, ![](img/B18369_07_F02.png) represents the received aggregate model weights.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![图片](img/B18369_07_F02.png) 表示接收到的聚合模型权重。
- en: The proximal term penalizes differences between the current weights and the
    aggregated model weights, restricting the aforementioned local model divergence
    with the strength parameterized by μ. From these two concepts, FedProx allows
    for a variable number of iterations proportional to the computational power to
    be performed by each agent without requiring manually tuned iteration counts for
    each agent or a set amount of allocated training time. Because of the addition
    of the proximal term, FedProx requires gradient-based optimization methods to
    be employed in order to work – more information on the underlying theory and comparison
    to FedAvg can be found in the original paper (which is at [https://arxiv.org/abs/1812.06127](https://arxiv.org/abs/1812.06127)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 近端项惩罚当前权重与聚合模型权重之间的差异，通过 μ 参数参数化的强度限制上述本地模型发散。从这两个概念出发，FedProx 允许每个代理执行与每个代理的计算能力成比例的变量次数，而无需为每个代理手动调整迭代次数或分配一定量的训练时间。由于添加了近端项，FedProx
    需要使用基于梯度的优化方法才能工作——有关底层理论和与 FedAvg 的比较的更多信息，可以在原始论文中找到（该论文位于 [https://arxiv.org/abs/1812.06127](https://arxiv.org/abs/1812.06127))。
- en: Implementing FedProx
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现FedProx
- en: Because the modifications made by FedProx to FedAvg are all on the client side,
    the actual implementation of FedProx consists entirely of modifications to the
    local training framework. Specifically, FedProx involves a new termination condition
    for local training and the addition of a constraining term to the local loss function.
    Therefore, it is helpful to use an example of the local training code to frame
    exactly how FedProx can be integrated.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 FedProx 对 FedAvg 的修改都是在客户端进行的，因此 FedProx 的实际实现完全由对本地训练框架的修改组成。具体来说，FedProx
    涉及本地训练的新终止条件，以及将约束项添加到本地损失函数中。因此，使用本地训练代码的示例可以帮助精确地说明如何集成 FedProx。
- en: 'Let’s consider the following generic training code using PyTorch:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下使用 PyTorch 的通用训练代码：
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let this be the code that performs `num_epochs` epochs of training on the local
    dataset using the received aggregate model for each round. The first necessary
    modification for FedProx is to replace the fixed number of epochs with a dynamic
    termination condition, checking whether a γ-inexact solution has been found with
    the aggregated model as the initial model. To do this, the total gradient over
    the entire training dataset for the aggregate model and the current local model
    must be stored – this can be performed as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让这段代码在每个回合使用接收到的聚合模型在本地数据集上执行 `num_epochs` 个训练周期。FedProx 的第一个必要修改是将固定的训练周期数替换为动态终止条件，检查是否已找到以聚合模型为初始模型的
    γ-不精确解。为此，必须存储整个训练数据集上聚合模型和当前本地模型的总体梯度——这可以按以下方式执行：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Values for the two FedProx parameters, `gamma` and `mu`, are set, and variables
    to store the gradients of both the aggregate model and the latest local model
    are defined.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 两个 FedProx 参数 `gamma` 和 `mu` 的值已设置，并定义了存储聚合模型和最新本地模型梯度的变量。
- en: 'We then define the γ-inexact new termination condition for local training using
    these gradient variables:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后使用这些梯度变量定义本地训练的 γ-不精确新终止条件：
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This condition is now checked before each training loop iteration to determine
    when to stop local training. The `total_grad` variable is created to store the
    cumulative gradients that were created from each minibatch during backpropagation:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次训练循环迭代之前检查此条件，以确定何时停止本地训练。创建 `total_grad` 变量以存储在反向传播期间从每个小批量中创建的累积梯度：
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To compute the proximal term, the weights of both the aggregate model and the
    latest local model are computed. From these weights, the proximal term is computed
    and added to the loss term:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算近端项，计算聚合模型和最新本地模型的权重。从这些权重中，计算近端项并将其添加到损失项中：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The gradients are computed and added to the cumulative sum stored in `total_grad`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 计算梯度并将其添加到存储在 `total_grad` 中的累积和中：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we update `agg_grad` (if the gradients were computed with the aggregate
    weights) and `curr_grad` after the current local training iteration is completed:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在完成当前本地训练迭代后，我们更新 `agg_grad`（如果梯度是用聚合权重计算的）和 `curr_grad`：
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: These modifications allow for FedProx to be implemented on top of FedAvg. The
    full FL example using FedProx can be found at [https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedprox_example](https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedprox_example).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这些修改使得FedProx可以在FedAvg之上实现。使用FedProx的完整FL示例可以在[https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedprox_example](https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedprox_example)找到。
- en: An auxiliary approach to handle the heterogeneous computational power scenario
    that helps with computational efficiency when only mild heterogeneity is observed
    is the idea of *compensation in aggregation*. Consider the case where aggregation
    occurs once the number of received models surpasses some threshold (generally,
    this is less than the number of participating agents). Using this threshold allows
    the straggler effect to be mitigated; however, the work done by slower agents
    ends up being discarded each round, leading to training inefficiency.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 处理异构计算能力场景的辅助方法，当观察到轻微异质性时有助于提高计算效率的想法是*聚合中的补偿*。考虑聚合发生时接收到的模型数量超过某个阈值的情况（通常，这小于参与代理的数量）。使用这个阈值可以减轻拖沓效应；然而，较慢代理所做的每轮工作最终都会被丢弃，导致训练效率低下。
- en: The core idea of compensation is to allow for the local training that is done
    by a slower agent in one round to instead be included in the model aggregation
    of a subsequent round. The age of the model is compensated for in this subsequent
    round by multiplying the weight used for the weighted average and a penalizing
    term during aggregation. By doing so, slower agents can be given two or three
    times as much training time as that used by the faster agents while avoiding the
    straggler effect. Mild heterogeneity is required in order to prevent cases where
    slower agents require too much extra time for training. This is due to the associated
    penalty given to the model after many rounds have passed; it will be severe enough
    to effectively lead to no contribution and reduce aggregation without compensation
    – this is necessary to prevent models that are too old from hampering the convergence
    of the aggregate model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 补偿的核心思想是允许在某一轮次中由较慢的代理进行的本地训练被包含到下一轮次的模型聚合中。在下一轮次中，通过在聚合期间乘以用于加权平均的权重和惩罚项来补偿模型的年龄。通过这样做，较慢的代理可以获得的训练时间可以比快速代理多两到三倍，同时避免拖沓效应。为了防止较慢的代理需要过多的额外训练时间，需要轻微的异质性。这是由于在经过多轮之后给予模型的关联惩罚；它将足够严重，足以有效地导致没有贡献并减少补偿聚合
    - 这是必要的，以防止过旧的模型阻碍聚合模型的收敛。
- en: Finally, we examine methods that help to address the third non-ideal property,
    where some subset of agents are controlled by an adversary or are, otherwise,
    behaving in an undesirable way.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们检查帮助解决第三个非理想属性的方法，即某些代理子集被对手控制或以其他方式表现出不可取的行为。
- en: Adversarial agents
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗性代理
- en: In the previous section, it was shown that the core problem with FedAvg in the
    presence of adversarial agents was the lack of robustness to outliers in the underlying
    arithmetic mean used during aggregation. This naturally raises the question of
    whether this mean can be estimated in such a manner that does offer such robustness.
    The answer is the class of robust mean estimators. There are many such estimators
    that offer varying trade-offs between robustness, distance from the true arithmetic
    mean, and computational efficiency.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，已经表明，在存在对抗性代理的情况下，FedAvg的核心问题是对聚合过程中使用的底层算术平均值的异常值缺乏鲁棒性。这自然引发了一个问题：这种均值是否可以以提供这种鲁棒性的方式估计。答案是鲁棒均值估计器类别。有许多这样的估计器，它们在鲁棒性、与真实算术平均值的距离和计算效率之间提供了不同的权衡。
- en: 'For use as a base for the implementation of the following aggregation methods,
    consider the following general aggregation function:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作为以下聚合方法实现的基，考虑以下通用聚合函数：
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This function takes a list of parameter vectors and returns the resulting aggregated
    parameter vector.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受一个参数向量的列表，并返回结果聚合参数向量。
- en: Now we will examine three example implementations of robust mean estimators.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将检查鲁棒均值估计器的三个示例实现。
- en: Aggregation using the geometric median
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用几何中值进行聚合
- en: The geometric median of a sample is the point minimizing the sum of L1 distances
    between itself and the sample. This is conceptually similar to the arithmetic
    mean, which is the point minimizing the sum of L2 distances between itself and
    the sample. The use of L1 distances allows for greater robustness to outliers;
    in fact, an arbitrary point can only be induced in the geometric median if at
    least half of the points are from adversarial agents. However, the geometric median
    cannot be directly computed, instead relying on numerical approximations or iterative
    algorithms to compute.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 样本的空间中位数是指使自身与样本之间的L1距离之和最小的点。从概念上讲，这与算术平均数相似，算术平均数是使自身与样本之间的L2距离之和最小的点。使用L1距离可以提供对异常值更大的鲁棒性；事实上，只有当至少一半的点来自对抗性代理时，才能在空间中位数中诱导出任意点。然而，空间中位数不能直接计算，而是依赖于数值近似或迭代算法来计算。
- en: 'To compute the geometric mean iteratively, Weiszfeld’s algorithm can be used
    as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要迭代地计算几何平均数，可以使用Weiszfeld算法如下：
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This algorithm uses the fact that the geometric median of a set of points is
    the point that minimizes the sum of Euclidean distances over the set, performing
    a form of weighted least squares with weights inversely proportional to the Euclidean
    distance between the point and the current median estimate at each iteration.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法利用了这样一个事实：一组点的空间中位数是使该集合上欧几里得距离之和最小的点，在每个迭代中执行一种加权最小二乘法，权重与点与当前中位数估计的欧几里得距离成反比。
- en: Aggregation using the coordinate-wise median
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用坐标中位数进行聚合
- en: The coordinate-wise median is constructed by taking the median of each coordinate
    across the sample, as the name suggests. This median can be directly computed,
    unlike the geometric median, and intuitively offers similar robustness to outliers
    due to the properties of the median in univariate statistics. However, it is unclear
    whether the resulting model displays any theoretical similarities to the arithmetic
    mean in regard to performance on the dataset and convergence.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 坐标中位数是通过取样本中每个坐标的中值来构建的，正如其名称所暗示的。这个中值可以直接计算，与空间中位数不同，并且由于单变量统计中中位数的性质，直观上提供了对异常值的类似鲁棒性。然而，不清楚所得到的模型在数据集性能和收敛性方面是否显示出与算术平均数有任何理论上的相似性。
- en: 'NumPy makes the implementation of this function quite simple, as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy使得实现这个函数变得非常简单，如下所示：
- en: '[PRE11]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It is clear that the coordinate-wise median is far more computationally efficient
    to compute than the geometric median, trading off theoretical guarantees for speed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，坐标中位数比空间中位数更易于计算，这是以牺牲理论保证为代价来换取速度的。
- en: Aggregation using the Krum algorithm
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Krum算法进行聚合
- en: An alternative approach is to isolate outlier points from adversarial agents
    prior to aggregation. The most well-known example of this approach is the **Krum
    algorithm**, where distance-based scoring is performed prior to aggregation as
    a means of locating outlier points.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是，在聚合之前从对抗性代理中隔离异常值点。这种方法的最著名例子是**Krum算法**，在该算法中，在聚合之前执行基于距离的评分，作为定位异常值点的一种手段。
- en: Specifically, the Krum algorithm first computes the pairwise L2 distance between
    each point – these distances are then used to compute a score for each point equal
    to the sum of the *n-f-2* smallest L2 distances (*f* is a parameter that is set).
    Then, Krum outputs the received point with the lowest score, effectively returning
    the point with a minimal total L2 distance with *f* outlier points that are ignored.
    Alternatively, the scoring approach used by Krum can be used to trim outlier points
    prior to the computation of the arithmetic mean. In both cases, for sufficiently
    large *n* and *2f+2 < n*, convergence rates similar to those of FedAvg in the
    non-adversarial case are achieved. More information on the Krum algorithm can
    be found in the original paper, which is located at [https://papers.nips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html](https://papers.nips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'The Krum algorithm can be used to perform aggregation as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that a flag has been included to determine which of the two Krum aggregation
    approaches (single selection versus trimmed mean) should be used. Vectorizing
    the distance computation is possible, but the iterative approach was preferred
    due to the expectation of large parameter vectors and smaller agent counts.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Non-IID datasets
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The theoretical underpinning granted to FL by working with IID datasets plays
    a significant role in allowing performant aggregate models to be achieved through
    FL. At a high level, this can be explained by the discrepancy between the learning
    done by models in different datasets. No theoretical guarantees can be made for
    the convergence of such models when dataset-agnostic aggregation methods are applied
    – unless constraints on the non-IID nature of the datasets are applied. The key
    hindering factor is the high probability of local models moving toward non-shared
    optima in the parameter space, leading to consistent drift between the local models
    and the aggregate model after each local training phase.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: There are methods that attempt to restrict the modifications made to the aggregate
    model based on the local machine learning task, relying on the overparameterization
    of deep learning models to find relatively disjointed parameter subsets to optimize
    the aggregate model of each task. One such aggregation approach is **FedCurv**,
    which uses the Fisher information matrix of the previous aggregate model to act
    as a regulator for auxiliary parameter modifications during local training. However,
    the robustness of this approach for extreme non-IID cases in practical applications
    likely needs to be tested further to ensure acceptable performance.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Implementing FedCurv
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The implementation of FedCurv involves two key modifications to the standard
    FedAvg approach. First, the local loss function must be modified to include the
    regularization term incorporating the aggregated Fisher information from the previous
    round. Second, the Fisher information matrix of the parameters must be calculated
    and aggregated correctly for use in the next round.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: The local training example code, as shown in the *Implementing FedProx* section,
    will be used again to demonstrate an implementation of FedCurv. In [*Chapter 4*](B18369_04.xhtml#_idTextAnchor085),
    *Federated Learning Server Implementation with Python*, we saw that a model conversion
    layer allows for framework-agnostic model representations to be operated on by
    the aggregator. Previously, these representations only contained the respective
    parameters from the original models; however, this agnostic representation actually
    allows for any desired parameter to be aggregated, even those only loosely tied
    to the true model parameters. This means that the secondary parameters can be
    bundled and sent with the local model, aggregated, and then separated from the
    aggregate model in the next round.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: In FedCurv, there are two sets of parameters that must be computed locally and
    aggregated for use in the next round; therefore, it can be assumed that these
    parameters are sent with the local model after training and separated from the
    aggregate model before training, for the sake of brevity in the example code (the
    implementation of this functionality is straightforward). As a result, the two
    key modifications for FedCurv, as mentioned earlier, can be simplified down into
    computing the Fisher information parameters after locally training the model and
    computing the regularization term with the received aggregate Fisher information
    parameters.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The Fisher information matrix refers to the covariance of the gradient of the
    log-likelihood function of a model with respect to its parameters, often empirically
    evaluated over the data present. FedCurv only utilizes the diagonal entries of
    this matrix, the variances between the gradient parameters, and their expected
    values of zero.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, this variance term can be considered an estimate of how influential
    the parameter is in changing the performance of the model on the data. This information
    is essential for preventing the modification of parameters key to good performance
    on one dataset during the local training of other agents – the underlying idea
    behind FedCurv.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Relaxing the measure of model performance from the gradient of the log-likelihood
    to the gradient of any objective function allows for the direct use of the gradient
    terms computed during backpropagation when computing the variance terms for models
    using gradient-based optimization methods, such as deep learning models. Specifically,
    the variance term of a parameter is equal to the square of its respective gradient
    term, allowing for the terms to be directly computed from the net gradients calculated
    during local training.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create two variables to store the agent’s most recent Fisher information
    parameters and the received aggregate Fisher information parameters, which are
    used to determine the Fisher information from the other agents. The value of the
    lambda parameter of FedCurv is fixed, and `total_grad` is initialized as a container
    for the cumulative gradient from each training loop:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we compute the FedCurv regularization term from the model weights and
    the aggregate Fisher information parameters. This term is weighted by lambda and
    added to the loss term before computing the gradients:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The gradients are then computed and stored in `total_grad` before updating
    the model weights:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we compute and store the agent’s most recent Fisher information parameters
    for use in the next round:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Therefore, framework-agnostic aggregation can be used to implement FedCurv on
    top of FedAvg. The full FL example using FedCurv can be found at [https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedcurv_example](https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedcurv_example).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Data-sharing approach
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To make further progress, changes to external aspects of the FL scenario are
    necessary. For example, let’s assume that the data privacy restriction is loosened,
    such that small subsets of the local datasets from each agent can be shared with
    the other agents. This data-sharing approach allows for homogeneity in the local
    data distributions proportional to the amount of shared data to be achieved, at
    the expense of the key stationary data property of FL that makes it desirable
    in many privacy-oriented applications. Thus, data-sharing approaches are generally
    unsuitable for the majority of applications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Personalization through fine-tuning
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is clear that producing a single model that demonstrates strong performance
    across the local datasets is not easy when the datasets are IID. However, what
    would happen if the single model restriction was removed from the FL process?
    If the goal is to produce local models that perform well on the same edge devices
    where training is conducted, removing the single model restriction allows for
    the use of different local models that have been trained on the exact data distributions
    where inference is being applied.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: This concept is called *personalization*, in which agents use versions of the
    aggregate model tuned for the local data distribution to achieve strong performance.
    The key point of this approach is to balance the local performance of the locally
    trained model with the global performance and the resulting robustness of the
    aggregate model received in each round. One method of accomplishing this is for
    each agent to maintain their local models across the rounds, updating the local
    model with the weighted average of the previous local model and the received aggregate
    model during each round.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, consider a relaxation that allows for multiple aggregate models
    to be produced in each round. In cases where the local data distributions can
    be clustered into just a few separated groups, distribution-aware aggregation
    would allow for the selective application of aggregation methods to groups of
    models belonging to the same distribution cluster.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: One example of this approach is the **Performance-Based Neighbor Selection**
    (**PENS**) algorithm, where agents receive locally trained models from other agents
    and test them on their own local dataset during the first phase. Using the assumption
    that models trained on similar datasets will perform better than models trained
    on different datasets, the agents then determine the set of other agents with
    similar data distributions, allowing for aggregation to only be performed with
    similar agents in the second phase.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: A second approach is to *add an intermediate aggregation* step between the local
    models and the global aggregate model called a cluster model. By leveraging knowledge
    about the agent data distributions or through a dynamic allocation method, agents
    with similar data distributions can be assigned to a cluster aggregator, which
    is then known to produce a strong model due to its agents having IID datasets.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Balancing the performance of the cluster models with the robustness of global
    aggregation leads to the concept of the semi-global model, in which subsamples
    of the cluster models can be selected (potentially based on data distribution)
    to create a smaller set of partially global aggregate models that maintain performance
    and robustness. Therefore, the cluster and semi-global model approach is beneficial
    for both aggregation and achieving a fully distributed FL system.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this chapter was to provide a conceptual overview of the current
    knowledge of aggregation, the key theoretical step in FL that allows for the disjoint
    training done by each agent to be pooled together with minimal transmission required.
    FedAvg is a simple, yet surprisingly powerful aggregation algorithm that performs
    well in an ideal FL scenario. This scenario is achieved when training is done
    across IID datasets using machines with similar levels of computational power
    and no adversarial or otherwise incorrectly performing agents.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, these conditions are often not met when deploying an FL system
    in the real world. To address these cases, we introduced and implemented modified
    aggregation approaches: FedProx, FedCurv, and three different robust mean estimators.
    After reading this chapter, you should have a solid understanding of the considerations
    that must be taken into account for practical FL applications, and you should
    be able to integrate the aforementioned algorithms into these applications.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will do a deep dive into some of the existing FL frameworks
    with several toy examples to demonstrate the functionalities provided by each.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Part 3 Moving Toward the Production of Federated Learning Applications
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will be introduced to the existing **federated learning**
    (**FL**) frameworks, such as **TensorFlow Federated** (**TFF**), PySyft, Flower,
    and STADLE, and learn about their libraries and how to actually run those frameworks.
    In addition, you will understand what is happening with FL in the real world by
    learning about the current and potential use cases that are being implemented
    in industries worldwide, especially in global enterprise companies. The book concludes
    by looking at the future trends and developments of FL to understand where AI
    technology itself is heading to envision a wisdom-driven future.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18369_08.xhtml#_idTextAnchor191), *Introducing Existing Federated
    Learning Frameworks*'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18369_09.xhtml#_idTextAnchor224), *Case Studies with Key Use
    Cases of Federated Learning Applications*'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18369_10.xhtml#_idTextAnchor256), *Future Trends and Developments*'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
