- en: Chapter 3. Detecting Objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the common applications of computer vision is to detect objects in an
    image or video. For example, we can use this method to detect a particular book
    in a heap of many books. One of the methods to detect objects is **feature matching**.
    In this chapter, we will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature detection, description, and matching in images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SIFT detector and descriptor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SURF detector and descriptor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ORB detector and descriptor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BRISK detector and descriptor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FREAK descriptor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are features?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Features are specific patterns that are unique and can be easily tracked and
    compared. Good features are those that can be distinctly localized. The following
    image shows the different kind of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What are features?](img/B02052_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Explains types of features
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding image, patch A is a flat area and is difficult to locate precisely.
    If we move the rectangle anywhere within the box, the patch contents remain the
    same. Patch B, being along an edge, is a slightly better feature because if you
    move it perpendicular to the edge, it changes. However, if you move it parallel
    to the edge, it is identical to the initial patch. Thus, we can localize these
    kind of features in at least one dimension. Patch C, being a corner, is a good
    feature because on moving the rectangle in any direction, the contents of the
    patch change and can be easily localized. Thus, good features are those which
    can be easily localized and thus are easy to track.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we have seen some of the edge and corner detection
    algorithms. In this chapter, we will take a look at some more algorithms by which
    we can find features. This is called **feature detection**. Just detecting features
    is not enough. We need to be able to differentiate one feature from the other.
    Hence, we use **feature description** to describe the detected features. The descriptions
    enable us to find similar features in other images, thereby enabling us to identify
    objects. Features can also be used to align images and to stitch them together.
    We will take a look at these applications in the later chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will take a look at some common algorithms available to detect features,
    such as SIFT, SURF, BRIEF, FAST, and BRISK.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that SIFT and SURF are patented algorithms and hence, their free use is
    only limited to academic and research purposes. For any commercial use of these
    algorithms, you need to abide by the patent rules and regulations, or speak to
    the concerned personal.
  prefs: []
  type: TYPE_NORMAL
- en: Scale Invariant Feature Transform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scale Invariant Feature Transform** (**SIFT**) is one of the most widely
    recognized feature detection algorithms. It was proposed by David Lowe in 2004.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Link to the paper: [http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the properties of SIFT are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is invariant to scaling and rotation changes in objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also partially invariant to 3D viewpoint and illumination changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large number of keypoints (features) can be extracted from a single image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how SIFT works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SIFT follows a strategy of matching robust local features. It is divided into
    four parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Scale-space extrema detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoint localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orientation assignment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoint descriptor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale-space extrema detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this step, an image is progressively blurred out using Gaussian blur to get
    rid of some details in the images. It has been mathematically proven (under reasonable
    assumptions) that performing Gaussian blur is the only way to carry this out effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scale-space extrema detection](img/B02052_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Images of one octave
  prefs: []
  type: TYPE_NORMAL
- en: Progressively blurred images constitute an octave. A new octave is formed by
    resizing the original image of the previous octave to half and then progressively
    blurring it. Lowe recommends that you use four octaves of five images each for
    the best results.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we see that the images in the first octave are formed by progressively
    blurring the original image. The first image of the second octave is obtained
    by resizing the original image in the first octave. Other images in the second
    octave are formed by the progressive blurring of the first image in the second
    octave, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scale-space extrema detection](img/B02052_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Images of all octaves
  prefs: []
  type: TYPE_NORMAL
- en: To precisely detect edges in an image, we use the Laplacian operator. In this
    method, second we blur the image a little and then calculate its second derivative.
    This locates the edges and corners that are good for finding the keypoints. This
    operation is called the Laplacian of Gaussian.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second order derivative is extremely sensitive to noise. The blur helps
    in smoothing out the noise and in stabilizing the second order derivative. The
    problem is that calculating all these second order derivatives is computationally
    expensive. So, we cheat a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scale-space extrema detection](img/B02052_03_21.jpg)![Scale-space extrema
    detection](img/B02052_03_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *k* is a constant multiplicative factor, which represents the amount
    of blurring in each image in the scale space. A scale space represents the set
    of images that have been either scaled-up or scaled-down for the purpose of computing
    keypoints. For example, as shown in the following figure, there are two sets of
    images: one set is the original set of five images that have been blurred with
    different blurring radius and another set of scaled down images. The different
    parameter values can be seen in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Scale |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Octave** | 0.707107 | 1.000000 | 1.414214 | 2.000000 | 2.828427 |'
  prefs: []
  type: TYPE_TB
- en: '|   | 1.414214 | 2.000000 | 2.828427 | 4.000000 | 5.656854 |'
  prefs: []
  type: TYPE_TB
- en: '|   | 2.828427 | 4.000000 | 5.656854 | 8.000000 | 11.313708 |'
  prefs: []
  type: TYPE_TB
- en: '|   | 5.656854 | 8.000000 | 11.313708 | 16.000000 | 22.627417 |'
  prefs: []
  type: TYPE_TB
- en: To generate the Laplacian of Gaussian images, we calculate the difference between
    two consecutive images in an octave. This is called the **Difference of Gaussian**
    (**DoG**). These DoG images are approximately equal to those obtained by calculating
    the Laplacian of Gaussian. Using DoG also has an added benefit. The images obtained
    are also scale invariant.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scale-space extrema detection](img/B02052_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Difference of Gaussian
  prefs: []
  type: TYPE_NORMAL
- en: Using the Laplacian of Gaussian is not only computationally expensive, but it
    also depends on the amount of blur applied. This is taken care of in the DoG images
    as a result of normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoint localization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now these images have been sufficiently preprocessed to enable us to find local
    extremas. To locate keypoints, we need to iterate over each pixel and compare
    it with all its neighbors. Instead of just comparing the eight neighbors in that
    image, we compare the value with its neighbors in that image and also with the
    images above and below it in that octave, which have nine pixels each:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Keypoint localization](img/B02052_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Keypoint localization
  prefs: []
  type: TYPE_NORMAL
- en: So, we can see that we compare a pixel's value with its **26 neighbors**. A
    pixel is a keypoint if it is the minimum or the maximum among all its 26 neighbors.
    Usually, a non-maxima or a non-minima doesn't have to go through all 26 comparisons
    as we may have found its result within a few comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: We do not calculate the keypoints in the uppermost and lowermost images in an
    octave because we do not have enough neighbors to identify the extremas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the time, the extremas are never located at the exact pixels. They
    may be present in between the pixels, but we have no way to access this information
    in an image. The keypoints located are just their average positions. We use the
    Taylor series expansion of the scale space function ![Keypoint localization](img/B02052_03_24.jpg)
    (up to the quadratic term) shifted till the current point as origin gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Keypoint localization](img/B02052_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *D* and its derivatives are calculated at the point we are currently
    testing for extrema. Using this formula, by differentiating and equating the result
    to zero, we can easily find the subpixel keypoint locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Keypoint localization](img/B02052_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Subpixel extrema localization
  prefs: []
  type: TYPE_NORMAL
- en: SIFT recommends that you generate two such extrema images. Thus, to generate
    two extremas, we need four DoG images. To generate these four DoG images, we need
    five Gaussian blurred images. Thus, we need five images in a single octave. It
    has also been found that the optimal results are obtained when ![Keypoint localization](img/B02052_03_26.jpg)
    and ![Keypoint localization](img/B02052_03_27.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: The number of keypoints located so far is quite high. Some of these keypoints
    either lie on an edge or don't have enough contrast to be useful to us. So we
    need to get rid of these keypoints. This approach is similar to that used in the
    **Harris corner detector** to remove edges.
  prefs: []
  type: TYPE_NORMAL
- en: To remove low contrast keypoints, we simply compare the intensity value of the
    current pixel to a preselected threshold value. If it is less than the threshold
    value, it is rejected. Because we have used subpixel keypoints, we again need
    to use the Taylor series expansion to get the intensity value at subpixel locations.
  prefs: []
  type: TYPE_NORMAL
- en: For stability, it is not sufficient to reject keypoints with low contrast. The
    DoG function will have a strong response along edges, even if the location along
    the edge is poorly determined and therefore, unstable to small amounts of noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'To eliminate keypoints along the edges, we calculate two gradients at the keypoint,
    which are perpendicular to each other. The region around the keypoint can be one
    of the following three types:'
  prefs: []
  type: TYPE_NORMAL
- en: A flat region (both gradients will be small)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An edge (here, the gradient parallel to the edge will be small, but the one
    perpendicular to it will be large)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A corner (both gradients will be large)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we want only corners as our keypoints, we only accept those keypoints whose
    both gradient values are high.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate this, we use the **Hessian matrix**. This is similar to the Harris
    corner detector. In the Harris corner detector, we calculate two different eigenvalues,
    whereas, in SIFT, we save the computation by just calculating their ratios directly.
    The Hessian matrix is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Keypoint localization](img/B02052_03_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Orientation assignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Till now, we have stable keypoints and we know the scales at which these were
    detected. So, we have scale invariance. Now we try to assign an orientation to
    each keypoint. This orientation helps us achieve rotation invariance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We try to compute the magnitude and direction of the Gaussian blurred images
    for each keypoint. The magnitudes and directions are calculated using these formulae:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Orientation assignment](img/B02052_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The magnitude and orientation are calculated for all pixels around the keypoint.
    We create a 36-bin histogram covering the 360-degree range of orientations. Each
    sample added to the histogram is weighted by its gradient magnitude and by a Gaussian-weighted
    circular window with σ, which is 1.5 times that of the scale of the keypoint.
    Suppose you get a histogram, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Orientation assignment](img/B02052_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After this has been done for all the neighboring pixels of a particular keypoint,
    we will get a peak in the histogram. In the preceding figure, we can see that
    the histogram peaks in the region **20-29**. So, we assign this orientation to
    the keypoint. Also, any peaks above **80%** value are also converted into keypoints.
    These new keypoints have the same location and scale as the original keypoint,
    but its orientation is assigned to the value corresponding to the new peak.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoint descriptor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Till now, we have achieved scale and rotation invariance. We now need to create
    a descriptor for various keypoints so as to be able to differentiate it from the
    other keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate a descriptor, we take a 16x16 window around the keypoint and break
    it into 16 windows of size 4x4\. This can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Keypoint descriptor](img/B02052_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We do this in order to incorporate the fact that objects in two images are rarely
    never exactly the same. Hence, we try to lose some precision in our calculations.
    Within each 4x4 window, gradient magnitudes and orientations are calculated. These
    orientations are put in an 8-bin histogram. Each bin represents an orientation
    angle of 45 degrees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a large area to consider, we need to take the distance of
    the vectors from the keypoint into consideration. To achieve this, we use the
    Gaussian weighting function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Keypoint descriptor](img/B02052_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We put the 16 vectors into 8-bin histograms each, and doing this for each of
    the 4x4 windows we get 4x4x8 = 128 numbers. Once we have all these 128 numbers,
    we normalize the numbers (by dividing each by the sum of their squares). This
    set of 128 normalized numbers forms the feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the introduction of the feature vector, some unwanted dependencies arise,
    which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rotation dependence**: The feature vector uses gradient orientations. So,
    if we rotate the image, our feature vector changes and the gradient orientations
    are also affected. To achieve rotation independence, we subtract the keypoint''s
    rotation from each orientation. Thus, each gradient orientation is now relative
    to the keypoint''s orientation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Illumination dependence**: Illumination independence can be achieved by thresholding
    large values in the feature vector. So any value greater than 0.2 is changed to
    0.2 and the resultant feature vector is normalized again. We have now obtained
    an illumination independent feature vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, now that we have seen how SIFT works in theory, let's see how it works in
    OpenCV and its capability to match objects.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Images and a simplified explanation of SIFT by Utkarsh Sinha can be found at
    [http://www.aishack.in/](http://www.aishack.in/).
  prefs: []
  type: TYPE_NORMAL
- en: SIFT in OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will set up a new application called `Chapter3`, which is similar to the
    one created in the earlier chapters. We will make changes to `MainActivity.java`.
    Some changes also have to be made to `HomeActivity.java`, but they will be self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we open `res` | `main_menu.xml`. In this file, we will create two items.
    One to select each image to be matched. As a convention, we will have the first
    image as the object to detect and the second image as the scene in which we want
    to detect it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now we need to program these items in to our Java code. This is similar to [Chapter
    1](ch01.html "Chapter 1. Applying Effects to Images"), *Applying Effects to Images*,
    where we opened the photo picker using intents. We will have two flag variables
    that will store each image that has been selected. If it is selected, we will
    perform our computations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will perform our actual computations in `AsyncTask`, as these tasks are
    computationally expensive; and to avoid blocking the UI thread for a long time,
    we offload the computation onto an asynchronous background worker—`AsyncTasks`
    that enables us to perform threading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `executeTask` function has been called, which will perform all our
    computations. First, we need to detect the keypoints, and then we need to use
    descriptors to describe them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first declare all our variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, depending on the algorithm, we initialize these variables. For SIFT,
    we use the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we detect the keypoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the keypoints, we will compute their descriptors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Matching features and detecting objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have detected features in two or more objects, and have their descriptors,
    we can match the features to check whether the images have any similarities. For
    example, suppose we want to search for a particular book in a heap of many books.
    OpenCV provides us with two feature matching algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Brute-force matcher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FLANN based matcher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will see how the two work in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'For matching, we first need to declare some variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Brute-force matcher
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It takes the descriptor of one feature in the first set and matches it with
    all other features in the second set, using distance calculations, and the closest
    one is returned.
  prefs: []
  type: TYPE_NORMAL
- en: The BF matcher takes two optional parameters. The first one is the distance
    measurement type, `normType`. We should use `NORM_L2` for descriptors such as
    SIFT and SURF. For descriptors that are based on a binary string, such as ORB
    and BRISK, we use `NORM_HAMMING` as the distance measurement. The second one is
    `crosscheck`. If it is set to true, the matcher only returns matches with values
    (i, j) such that the i^(th) descriptor in the first image has the j^(th) descriptor
    in the second set, as the best matches, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case for SIFT, we add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: FLANN based matcher
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**FLANN** stands for **Fast Library for Approximate Nearest Neighbors**. It
    contains a collection of algorithms optimized for a fast nearest neighbor search
    in large datasets and for high-dimensional features. It works faster than the
    BF matcher for large datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: For FLANN based matcher, we need to pass two dictionaries, which specifies the
    algorithm to be used, its related parameters, and so on. The first one is `IndexParams`.
    For various algorithms, the information to be passed is explained in the FLANN
    docs.
  prefs: []
  type: TYPE_NORMAL
- en: The second dictionary is `SearchParams`. It specifies the number of times the
    trees in the index should be recursively traversed. Higher values give better
    precision, but also take more time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the FLANN based matcher, we need to initialize it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Matching the points
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have the `DescriptorMatcher` object, we use the `match()` and `knnMatch()`
    functions. The first one returns all the matches, while the second one returns
    *k* matches, where *k* is defined by the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have computed the descriptors, we can use the following to match the
    keypoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now we show the matches obtained using `drawMatches()`, which helps us draw
    the matches. It stacks two images horizontally and draws lines from the first
    image to the second image, showing the best matches. There is also a `drawMatchesKnn()`
    function, which draws all the *k* best matches. If *k = 2*, it will draw two match
    lines for each keypoint. So, we have to pass a mask if we want to selectively
    draw it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To draw the matches, we will add a function that will merge the query and train
    image into one and also display the matches in the same image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Because SIFT and SURF are patented algorithms, they are not automatically built
    by OpenCV. We need to manually build the `nonfree` module so as to be able to
    use them in OpenCV. For this, you will need to download Android NDK, which allows
    us to use the native C++ code along with the Java code. It is available at [https://developer.android.com/tools/sdk/ndk/index.html](https://developer.android.com/tools/sdk/ndk/index.html).
    Then, extract it to a suitable location.
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to download some files from OpenCV's source repository, which
    is located at [https://github.com/Itseez/opencv/tree/master/modules](https://github.com/Itseez/opencv/tree/master/modules).
    These are `nonfree_init.cpp`, `precomp.cpp`, `sift.cpp`, and `surf.cpp`. These
    will also be available with the code for this chapter, so you can download them
    directly from there as well. Now, create a folder in your `src` directory called
    `jni` and copy these files to there. We need to modify these files a bit.
  prefs: []
  type: TYPE_NORMAL
- en: Open `precomp.hpp` and remove the lines `#include "cvconfig.h"` and `#include
    "opencv2/ocl/private/util.hpp"`.
  prefs: []
  type: TYPE_NORMAL
- en: Open `nonfree_init.cpp` and remove the lines of code starting from `#ifdef HAVE_OPENCV_OCL`
    and ending at `#endif`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will create a file called `Android.mk` and copy the following lines
    of code to it. You need to replace `<OpenCV4Android_SDK_location>` accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a file named `Application.mk` and copy the following lines of
    code to it. These define the architecture for which our library would be built:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the `build.gradle` file in your `app` folder. Under the `android` section,
    add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Open a terminal or a command window if you are on Windows. Then, change the
    directory to your project using the `cd` command. Type the following in the command
    window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the terminal window, type the following, replacing `<ndk_dir>` with the
    appropriate directory location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: After this, our library should have been successfully built and should be available
    in the `src` | `obj` folder, under the correct architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to load this library from our Java code. Open `MainActivity.java`,
    and in our OpenCV Manager''s callback variable (the `mOpenCVCallback` file''s
    `onManagerConnected` function) within the case for `LoaderCallbackInterface.SUCCESS`,
    add the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The name of the library, `nonfree`, is the same as the module name defined in
    the `Android.mk` file.
  prefs: []
  type: TYPE_NORMAL
- en: '![Matching the points](img/B02052_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: SIFT feature matching
  prefs: []
  type: TYPE_NORMAL
- en: Detecting objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we detected features in multiple images and matched
    them to their corresponding features in the other images. The information we obtained
    is enough to locate objects in a scene.
  prefs: []
  type: TYPE_NORMAL
- en: We use a function from OpenCV's `calib3d` module, `findHomography()`.Using this
    function, we can find a perspective transformation of the object, that is, a rotated
    and skewed result. Then we use `perspectiveTransform()` to locate the object in
    the scene. We need at least four matching points to calculate the transformation
    successfully.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that there can be some possible errors while matching, which may
    affect the result. To solve this problem, the algorithm uses either `RANSAC` or
    `LEAST_MEDIAN` (which can be specified by the flags). Good matches that provide
    the correct estimation are called inliers and the remaining are called outliers.
    `findHomography()` returns a mask, which specifies the inlier and outlier points.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will look at the algorithm to implement it.
  prefs: []
  type: TYPE_NORMAL
- en: First, we detect and match keypoints in both the images. This has already been
    done in the previous sections. Then we set a condition that there has to be a
    certain number of matches to detect an object.
  prefs: []
  type: TYPE_NORMAL
- en: If enough matches are found, we extract the locations of matched keypoints in
    both the images. They are passed to find the perspective transformation. Once
    we get this 3x3 transformation matrix, we use it to transform the corners of `queryImage`
    to the corresponding points in `trainImage`. Then, we draw it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we draw our inliers (if we successfully find the object) or matching
    keypoints (if it failed).
  prefs: []
  type: TYPE_NORMAL
- en: Speeded Up Robust Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Speeded Up Robust Features** (**SURF**) was proposed by Herbert Bay, Tinne
    Tuytelaars, and Luc Van Gool in 2006\. Some of the drawbacks of SIFT are that
    it is slow and computationally expensive. To target this problem, SURF was thought
    of. Apart from the increase in speed, the other motivations behind SURF were as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Fast interest point detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distinctive interest point description
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speeded up descriptor matching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Invariant to the following common image transformations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image rotation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale changes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Illumination changes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Small changes in viewpoint
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SURF detector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as SIFT approximate Laplacian of Gaussian images to Difference of Gaussian,
    SURF uses integral images to approximate Laplacian of Gaussian images. An integral
    image (summed area tables) is an intermediate representation of the image and
    contains the sum of grayscale pixel values of the image. It is called the **fast
    Hessian** detector. The descriptor, on the other hand, describes a distribution
    of Haar wavelet responses within the interest point neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can refer to the paper at [http://www.vision.ee.ethz.ch/~surf/eccv06.pdf](http://www.vision.ee.ethz.ch/~surf/eccv06.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'To select the location and scale of keypoints, SURF uses the determinant of
    the Hessian matrix. SURF proves that Gaussian is overrated as the property that
    no new structures can appear while going down to lower resolutions has only been
    proved in 1D, but does not apply to the 2D case. Given SIFT''s success with the
    LoG approximation, SURF further approximates LoG using box filters. Box filters
    approximate Gaussians and can be calculated very quickly. The following image
    shows an approximation of Gaussians as box filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SURF detector](img/B02052_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Due to the use of box filters and integral images, we no longer have to perform
    repeated Gaussian smoothing. We apply box filters of different sizes directly
    to the integral image. Instead of iteratively down-scaling images, we up-scale
    the filter size. Hence, scale analysis is done using only a single image. The
    output of the preceding 9x9 filter is considered as the initial scale layer. Other
    layers are obtained by filtering, using gradually bigger filters. Images of the
    first octave are obtained using filters of size 9x9, 15x15, 21x21, and 27x27\.
    At larger scales, the step size between the filters should also scale accordingly.
    Hence, for each new octave, the filter size step is doubled (that is, from 6 to
    12 to 24). In the next octave, the filter sizes are 39x39, 51x51, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In order to localize interest points in the image and over scales, a non-maximum
    suppression in a 3x3x3 neighborhood is applied. The maxima of the determinant
    of the Hessian matrix is then interpolated in scale and image space using the
    method proposed by Brown, and others. Scale space interpolation is especially
    important in our case, as the difference in scale between the first layers of
    every octave is relatively large.
  prefs: []
  type: TYPE_NORMAL
- en: SURF descriptor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have localized the keypoints, we need to create a descriptor for
    each, so as to uniquely identify it from other keypoints. SURF works on similar
    principles of SIFT, but with lesser complexity. Bay and others also proposed a
    variation of SURF that doesn't take rotation invariance into account, which is
    called **U-SURF** (upright SURF). In many applications, the camera orientation
    remains more or less constant. Hence, we can save a lot of computation by ignoring
    rotation invariance.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to fix a reproducible orientation based on the information obtained
    from a circular region centered about the keypoint. Then we construct a square
    region that is rotated and aligned based on the selected orientation, and then
    we can extract the SURF descriptor from it.
  prefs: []
  type: TYPE_NORMAL
- en: Orientation assignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to add rotation invariance, the orientation of the keypoints must
    be robust and reproducible. For this, SURF proposes calculating Haar wavelet responses
    in the *x* and *y* directions. The responses are calculated in a circular neighborhood
    of radius 6 s around the keypoint, where s is the scale of the image (that is,
    the value of σ). To calculate the Haar wavelet responses, SURF proposes using
    a wavelet size of 4 s. After obtaining the wavelet responses and weighing them
    with a Gaussian kernel ![Orientation assignment](img/B02052_03_29.jpg) centered
    about the keypoint, the responses are represented as vectors. The vectors are
    represented as the response strength in the horizontal direction along the abscissa,
    and the response strength in the vertical direction along the ordinate. All the
    responses within a sliding orientation window covering an angle of 60 degrees
    are then summed up. The longest vector calculated is set as the direction of the
    descriptor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Orientation assignment](img/B02052_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Haar wavelet responses in a 60 degree angle
  prefs: []
  type: TYPE_NORMAL
- en: The size of the sliding window is taken as a parameter, which has to be calculated
    experimentally. Small window sizes result in single dominating wavelet responses,
    whereas large window sizes result in maxima in vector lengths that are not descriptive
    enough. Both result in an unstable orientation of the interest region. This step
    is skipped for U-SURF, as it is doesn't require rotation invariance.
  prefs: []
  type: TYPE_NORMAL
- en: Descriptor based on Haar wavelet responses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the extraction of the descriptor, the first step consists of constructing
    a square region centered around the interest point and oriented along the orientation
    selected in the previous section. This is not required for U-SURF. The size of
    the window is 20 s. The steps to find the descriptor are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the interest region into 4x4 square subregions with 5x5 regularly spaced
    sample points inside.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate Haar wavelet responses *d^x* and *d^y* [*d^x* = Haar wavelet response
    in *x* direction; d^y = Haar wavelet response in *y* direction. The filter size
    used is 2 s].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Weight the response with a Gaussian kernel centered at the interest point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sum the response over each subregion for *d^x* and *d^y* separately, to form
    a feature vector of length 32.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to bring in information about the polarity of the intensity changes,
    extract the sum of the absolute value of the responses, which is a feature vector
    of length 64.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the vector to unit length.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The wavelet responses are invariant to a bias in illumination (offset). Invariance
    to contrast (a scale factor) is achieved by turning the descriptor into a unit
    vector (normalization).
  prefs: []
  type: TYPE_NORMAL
- en: Experimentally, Bay and others tested a variation of SURF that adds some more
    features (SURF-128). The sums of *d^x* and *|d^x|* are computed separately for
    *d^y < 0* and *d^y ≥ 0*. Similarly, the sums of *d^y* and *|d^y|* are split according
    to the sign of *d^x*, thereby doubling the number of features. This version of
    SURF-128 outperforms SURF.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows a comparison between the various algorithms in finding
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | U-SURF | SURF | SIFT |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Time (ms)** | 225 | 354 | 1036 |'
  prefs: []
  type: TYPE_TB
- en: While SIFT and SURF work well in finding good features, they are **patented**
    for commercial use. So, you have to pay some money if you use them for commercial
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the results we obtain from SURF are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: SURF is faster than SIFT by three times and has a recall precision no worse
    than SIFT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SURF is good at handling images with blurring or rotation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SURF is poor at handling images with viewpoint changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SURF in OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code for SURF needs only a little modification. We just need to add a case
    in our switch case construct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![SURF in OpenCV](img/B02052_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: SURF feature matching
  prefs: []
  type: TYPE_NORMAL
- en: Oriented FAST and Rotated BRIEF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Oriented FAST and Rotated BRIEF** (**ORB**) was developed at OpenCV labs
    by Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary R. Bradski in 2011, as
    an efficient and viable alternative to SIFT and SURF. ORB was conceived mainly
    because SIFT and SURF are patented algorithms. ORB, however, is free to use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ORB performs as well as SIFT on these tasks (and better than SURF), while being
    almost two order of magnitude faster. ORB builds on the well-known FAST keypoint
    detector and the BRIEF descriptor. Both these techniques are attractive because
    of their good performance and low cost. ORB''s main contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The addition of a fast and accurate orientation component to FAST
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The efficient computation of oriented BRIEF features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of variance and correlation of oriented BRIEF features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A learning method for decorrelating BRIEF features under rotational invariance,
    leading to better performance in nearest-neighbor applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: oFAST – FAST keypoint orientation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FAST is a feature detection algorithm that is widely recognized due its fast
    computation properties. It doesn't propose a descriptor to uniquely identify features.
    Moreover, it does not have any orientation component, so it performs poorly to
    in-plane rotation and scale changes. We will take a look at how ORB added an orientation
    component to FAST features.
  prefs: []
  type: TYPE_NORMAL
- en: FAST detector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we detect FAST keypoints. FAST takes one parameter from the user, the
    threshold value between the center pixel, and those in a circular ring around
    it. We use a ring radius of 9 pixels as it gives good performance. FAST also produces
    keypoints that are along edges. To overcome this, we use the Harris corner measure
    to order the keypoints. If we want N keypoints, we first keep the threshold low
    enough to generate more than N keypoints, and then pick the topmost N based on
    the Harris corner measure.
  prefs: []
  type: TYPE_NORMAL
- en: FAST does not produce multiscale features. ORB employs a scale pyramid of the
    image and produces FAST features (filtered by Harris) at each level in the pyramid.
  prefs: []
  type: TYPE_NORMAL
- en: Orientation by intensity centroid
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To assign orientation to corners, we use the intensity centroid. We assume that
    the corner is offset from the intensity centroid and this vector is used to assign
    orientation to a keypoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the coordinates of the centroid, we use moments. Moments are calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Orientation by intensity centroid](img/B02052_03_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The coordinates of the centroid can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Orientation by intensity centroid](img/B02052_03_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We construct a vector ![Orientation by intensity centroid](img/B02052_03_32.jpg)
    from the keypoint''s center, *O*, to the centroid, *C*. The orientation of the
    patch is obtained by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Orientation by intensity centroid](img/B02052_03_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *atan2* is the quadrant-aware version of *arctan*. To improve the rotation
    invariance of this measure, we make sure that the moments are computed with *x*
    and *y* remaining within a circular region of radius *r*. We empirically choose
    *r* to be the patch size so that *x* and *y* run from *[−r, r]*. As *|C|* approaches
    *0*, the measure becomes unstable; with FAST corners, we have found that this
    is rarely the case. This method can also work well in images with heavy noise.
  prefs: []
  type: TYPE_NORMAL
- en: rBRIEF – Rotation-aware BRIEF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'BRIEF is a feature description algorithm that is also known for its fast speed
    of computation. However, BRIEF also isn''t invariant to rotation. ORB tries to
    add this functionality, without losing out on the speed aspect of BRIEF. The feature
    vector obtained by *n* binary tests in BRIEF is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![rBRIEF – Rotation-aware BRIEF](img/B02052_03_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where ![rBRIEF – Rotation-aware BRIEF](img/B02052_03_35.jpg) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![rBRIEF – Rotation-aware BRIEF](img/B02052_03_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*p(x)* is the intensity value at pixel *x*.'
  prefs: []
  type: TYPE_NORMAL
- en: Steered BRIEF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The matching performance of BRIEF falls off sharply for in-plane rotation of
    more than a few degrees. ORB proposes a method to steer BRIEF according to the
    orientation of the keypoints. For any feature set of n binary tests at location
    (x^i, y^i), we define the *2 x n* matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Steered BRIEF](img/B02052_03_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We use the patch orientation *θ* and the corresponding rotation matrix *R^θ*,
    and construct a *steered* version *S^θ* of *S*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Steered BRIEF](img/B02052_03_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now the steered BRIEF operator becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Steered BRIEF](img/B02052_03_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We discretize the angle to increments of *2π/30* (12 degrees), and construct
    a lookup table of precomputed BRIEF patterns. As long as the keypoint orientation
    *θ* is consistent across views, the correct set of points *S^θ* will be used to
    compute its descriptor.
  prefs: []
  type: TYPE_NORMAL
- en: Variance and correlation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the properties of BRIEF is that each bit feature has a large variance
    and a mean near 0.5\. A mean of 0.5 gives a maximum sample variance of 0.25 for
    a bit feature. Steered BRIEF produces a more uniform appearance to binary tests.
    High variance causes a feature to respond more differently to inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Having uncorrelated features is desirable as in that case, each test has a contribution
    to the results. We search among all the possible binary tests to find ones that
    have a high variance (and a mean close to 0.5) as well as being uncorrelated.
  prefs: []
  type: TYPE_NORMAL
- en: 'ORB specifies the rBRIEF algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up a training set of some 300 k keypoints drawn from images in the PASCAL
    2006 set. Then, enumerate all the possible binary tests drawn from a 31x31 pixel
    patch. Each test is a pair of 5x5 subwindows of the patch. If we note the width
    of our patch as *w^p = 31* and the width of the test subwindow as *w^t = 5*, then
    we have *N = (wp − w^t)²* possible subwindows. We would like to select pairs of
    two from these, so we have ![Variance and correlation](img/B02052_03_40.jpg) 2
    binary tests. We eliminate tests that overlap, so we end up with *N = 205590*
    possible tests. The algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Run each test against all training patches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Order the tests by their distance from a mean of 0.5, forming the vector T.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perform a greedy search:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put the first test into the result vector R and remove it from T.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Take the next test from T, and compare it against all tests in R. If its absolute
    correlation is greater than a threshold, discard it; else add it to R.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the previous step until there are 256 tests in R. If there are fewer
    than 256, raise the threshold and try again.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: rBRIEF shows significant improvement in the variance and correlation over steered
    BRIEF. ORB outperforms SIFT and SURF on the outdoor dataset. It is about the same
    on the indoor set; note that blob detection keypoints, such as SIFT, tend to be
    better on graffiti type images.
  prefs: []
  type: TYPE_NORMAL
- en: ORB in OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code for ORB is similar to SIFT and SURF. However, ORB being a binary string-based
    descriptor, we will use the hamming code in our BF matcher.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for SURF needs only a little modification. We just need to add a case
    to our switch case construct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![ORB in OpenCV](img/B02052_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ORB feature matching
  prefs: []
  type: TYPE_NORMAL
- en: Binary Robust Invariant Scalable Keypoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Binary Robust Invariant Scalable Keypoints** (**BRISK**) was conceived by
    Leutenegger, Chli, and Siegwart to be an efficient replacement to the state-of-the-art
    feature detection, description, and matching algorithms. The motivation behind
    BRISK was to develop a robust algorithm that can reproduce features in a computationally
    efficient manner. In some cases, BRISK achieves comparable quality of feature
    matching as SURF, while requiring much less computation time.'
  prefs: []
  type: TYPE_NORMAL
- en: Scale-space keypoint detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The BRISK detector is based on the AGAST detector, which is an extension of
    a faster performance version of FAST. To achieve scale invariance, BRISK searches
    for the maxima in a scale space using the FAST score(s) as the comparison parameter.
    Despite discretizing the scale axis at coarser intervals than in alternative high-performance
    detectors (for example, the fast Hessian), the BRISK detector estimates the true
    scale of each keypoint in the continuous scale space. The BRISK scale space comprises
    of n octaves, c^i and *n* intra-octaves, and *d^i [i = {0, 1, 2, …, n-1}]*. BRISK
    suggests using *n = 4*.
  prefs: []
  type: TYPE_NORMAL
- en: The original image is taken as *c⁰*, and each successive octave is half-sampled
    from the previous octave. Each intra-octave d^i is down-sampled such that it lies
    between *c^i* and *c^i+1*. The first intra-octave *d⁰* is obtained by down sampling
    *c⁰* by a factor of 1.5\. The subsequent intra-octaves are obtained by half sampling
    the previous intra-octave.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scale-space keypoint detection](img/B02052_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An image showing octaves and intra-octaves
  prefs: []
  type: TYPE_NORMAL
- en: The FAST 9-16 detector requires that in a 16 pixel circular radius, at least
    9 pixels must be brighter than or darker than the center pixel for the FAST criterion
    to be fulfilled. BRISK proposes the use of this FAST 9-16 detector.
  prefs: []
  type: TYPE_NORMAL
- en: The FAST score is computed for each octave and intra-octave separately. The
    FAST detector score, *s*, is calculated for each pixel as the maximum threshold
    for FAST detection, such that an image point is considered as a corner.
  prefs: []
  type: TYPE_NORMAL
- en: A non-maximum suppression in scale space is carried out on the keypoints obtained
    after applying the FAST 9-16 detector. The keypoint should be the maximum among
    its eight neighboring FAST scores in the same octave or intra-octave. This point
    must also have a higher FAST score than points in the layers above and below it.
    We then check inside the equally sized square patches having a 2 pixel side length
    in the layer, where the maximum value is suspected to be present. Interpolation
    is carried out at the boundaries of the patch, as neighboring layers are represented
    with different discretizations than that of the current later.
  prefs: []
  type: TYPE_NORMAL
- en: We try to calculate a subpixel location for each maximum detected in the earlier
    step. A 2D quadratic function is fitted to the 3x3 patch surrounding the pixel,
    and the subpixel maximum is determined. This is also done for the layers above
    and below the current layer. These maximas are then interpolated using a 1D parabola
    across the scale space, and the local maximum is chosen as the scale for the feature
    is found.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoint description
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The BRISK descriptor is composed of a binary string by concatenating the results
    of simple brightness comparison tests. In BRISK, we need to identify the characteristic
    direction of each keypoint to achieve the rotation invariance.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling pattern and rotation estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The BRISK descriptor makes use of a pattern used for sampling the neighborhood
    of the keypoint. The pattern defines N locations equally spaced on circles concentric
    with the keypoint, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling pattern and rotation estimation](img/B02052_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: BRISK sampling pattern
  prefs: []
  type: TYPE_NORMAL
- en: In order to avoid aliasing effects when sampling the image intensity of a point
    p^i in the pattern, we apply Gaussian smoothing with the standard deviation ![Sampling
    pattern and rotation estimation](img/B02052_03_41.jpg) proportional to the distance
    between the points on the respective circles. We then calculate the gradient between
    two sampling points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula used is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling pattern and rotation estimation](img/B02052_03_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'BRISK defines a subset of short distance pairings, *S*, and another subset
    of long distance pairings, *L*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling pattern and rotation estimation](img/B02052_03_43.jpg)![Sampling
    pattern and rotation estimation](img/B02052_03_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *A* is the set of all sampling point pairs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling pattern and rotation estimation](img/B02052_03_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The threshold distances are set to ![Sampling pattern and rotation estimation](img/B02052_03_46.jpg)
    and ![Sampling pattern and rotation estimation](img/B02052_03_47.jpg) (*t* is
    the scale of the keypoint). BRISK estimates the overall characteristic pattern
    direction of the keypoint *k* to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling pattern and rotation estimation](img/B02052_03_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Building the descriptor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to develop a rotation and scale invariant descriptor, BRISK applies
    the sampling pattern rotated by an angle, ![Building the descriptor](img/B02052_03_49.jpg),
    around the keypoint *k*. Short distance intensity comparisons of point pairs,
    ![Building the descriptor](img/B02052_03_50.jpg) (that is, in the rotated pattern),
    are calculated to get the bit vector descriptor *d^k*. Each bit *b* corresponds
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the descriptor](img/B02052_03_51.jpg)![Building the descriptor](img/B02052_03_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: BRISK uses a deterministic sampling pattern, resulting in a uniform sampling
    point density at a given radius around the keypoint. Due to this, the Gaussian
    smoothing does not modify the information content of a brightness comparison by
    blurring two close sampling points while comparing them. BRISK uses a lesser number
    of sampling points than a simple pairwise comparison (because a single point participates
    in more comparisons), thereby reducing the complexity of looking up the intensity
    values. As the brightness variations only need to be locally consistent, the comparisons
    done here are restricted spatially. We obtain a bit string of length 512 using
    the sampling pattern and the distance thresholds as shown previously.
  prefs: []
  type: TYPE_NORMAL
- en: BRISK In OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Again, the only change that we will make is to add another case to our switch
    case construct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![BRISK In OpenCV](img/B02052_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: BRISK feature matching
  prefs: []
  type: TYPE_NORMAL
- en: Fast Retina Keypoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Fast Retina Keypoint** (**FREAK**) proposes a robust descriptor to uniquely
    identify keypoints and in the process, require less computation time and memory.
    FREAK has been inspired by the human retina.'
  prefs: []
  type: TYPE_NORMAL
- en: A retinal sampling pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FREAK proposes to use the retinal sampling grid, which is also circular, with
    the difference of having higher density of points near the center. The density
    of points drops exponentially as we move away from the center point. This is similar
    to BRISK, except for the exponential decrease.
  prefs: []
  type: TYPE_NORMAL
- en: Each keypoint needs to be smoothed to be less sensitive to noise. Unlike BRIEF
    and ORB, which use the same kernel for all points, FREAK uses a different kernel
    for each keypoint. The radius of the Gaussian kernel is proportional to the value
    of σ.
  prefs: []
  type: TYPE_NORMAL
- en: '![A retinal sampling pattern](img/B02052_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Retinal sampling pattern
  prefs: []
  type: TYPE_NORMAL
- en: FREAK follows ORB's approach and tries to learn about the pairs by maximizing
    the variance of the pairs and taking pairs that are not correlated, so as to provide
    maximum information on each keypoint.
  prefs: []
  type: TYPE_NORMAL
- en: A coarse-to-fine descriptor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to find pairs of sampling points in order to create a bit-vector. We
    use a method similar to ORB, that is, instead of matching each pair, we try to
    learn about which pairs would give the best results. We need to find points that
    are not correlated. The algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We create a matrix D of nearly 50,000 extracted keypoints. Each row corresponds
    to a keypoint that is represented with its large descriptor made of all possible
    pairs in the retina sampling pattern. We use 43 receptive fields, leading to about
    1,000 pairs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compute the mean of each column. A mean of 0.5 produces the highest variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Order the columns according to the variance in descending order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the best column and iteratively add the remaining columns so that they
    have low correlation with the chosen columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this approach, we first select pairs that compare sampling points in the
    outer regions, whereas the last pairs are comparison points in the inner rings
    of the pattern. This is similar to how our retina works in the sense that we first
    try to locate an object and then try to verify it by precisely matching points
    that are densely located near the object.
  prefs: []
  type: TYPE_NORMAL
- en: Saccadic search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Humans do not look at a scene in a fixed manner. Their eyes move around with
    discontinuous individual movements called saccades. The fovea captures high-resolution
    information; hence, it is critical in the recognition and matching of objects.
    The perifoveal region captures low-resolution information, and hence, it used
    to approximately locate objects.
  prefs: []
  type: TYPE_NORMAL
- en: FREAK tries to mimic this function of the retina by searching the first 16 bytes
    of the descriptor, representing the coarse information. If the distance is smaller
    than a threshold, we continue by searching the next bytes to obtain a more refined
    result. Due to this, a cascade of comparisons is performed, accelerating the matching
    step even further as more than 90 percent of the sampling points are discarded
    with the first 16 byte comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: Orientation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The method FREAK uses to assign orientation is similar to that of BRISK with
    the difference being that, instead of using long distance pairs, FREAK uses a
    predefined set of 45 symmetric sampling pairs.
  prefs: []
  type: TYPE_NORMAL
- en: FREAK in OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code for FREAK is similar to that used for the previous algorithms. However,
    given that FREAK just provides a descriptor, we will use the FAST detector to
    detect keypoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![FREAK in OpenCV](img/B02052_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FREAK feature matching
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen how to detect features in an image and match them
    to features in other images. To perform this task, we looked at various algorithms,
    such as SIFT, SURF, ORB, BRISK, and FREAK, and their pros and cons. We also saw
    how we can use these to localize specific objects in a scene. There is one restriction
    to these methods in that the exact object must be present in the scene image to
    be detected correctly. In the next chapter, we will take a step forward to detect
    more general classes of objects, such as human beings, faces, hands, and so on.
  prefs: []
  type: TYPE_NORMAL
