- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From Model-Centric to Data-Centric – ML’s Evolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now, you might be thinking: if data-centricity is essential to the further
    evolution of AI and ML, how come model-centricity is the dominant approach?'
  prefs: []
  type: TYPE_NORMAL
- en: This is a very relevant question to ask, and one we will answer in this chapter.
    To understand what it takes to shift to a data-centric approach, we must understand
    the forces that have led to model-centricity being the predominant approach, and
    how to overcome them.
  prefs: []
  type: TYPE_NORMAL
- en: We will start this chapter by exploring why the evolution of AI and ML has predominately
    followed a model-centric approach, before diving into the huge opportunity that
    can be unlocked through data-centricity.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we will challenge the notion that ML requires big datasets
    and that more data is always better. There is a long tail of *small data* ML use
    cases that open up when we shift our mindset from *bigger data* to *better data*.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a clear understanding of the progression
    of ML to date, and know what it takes to build on the current paradigm and achieve
    even better results with ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring why ML development ended up being mostly model-centric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The opportunity for small-data ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why we need data-centric ML more than ever
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring why ML development ended up being mostly model-centric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A short history lesson is in order to truly appreciate why a data-centric approach
    is the key to unlocking the full potential of ML.
  prefs: []
  type: TYPE_NORMAL
- en: The fields of data science and ML have achieved significant advancements since
    the earliest attempts to make electronic computers act *intelligently*. The *intelligent*
    tasks performed by most smartphones today were nearly unimaginable at the turn
    of the 21st century. Moreover, we are producing more data every single day than
    was created from the beginning of human civilization to the 21st century – and
    we’re doing so at an estimated growth rate of 23% per annum1.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these incredible developments in technology and data volumes, some elements
    of data science are very old. Statistics and data analysis have been in use for
    centuries and the mathematical components of today’s ML models were mostly developed
    long before the advent of digital computers.
  prefs: []
  type: TYPE_NORMAL
- en: For our purposes, the history of ML and AI starts with the introduction of the
    first electronic calculation machines during World War II.
  prefs: []
  type: TYPE_NORMAL
- en: The 1940s to 1970s – the early days
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Historian and former US Army officer Adrian R. Lewis wrote in his book *The
    American Culture of War* that “war created the conditions for great advances in
    technology… without war, men would not traverse oceans in hours, travel in space,
    or microwave popcorn2.”
  prefs: []
  type: TYPE_NORMAL
- en: This was indeed the case during World War II, and in the decades that followed.
    Huge leaps were made in computer science, cryptology, and hardware technology,
    as fighting nations around the world were racing each other for dominance on every
    front.
  prefs: []
  type: TYPE_NORMAL
- en: In the 1940s and 1950s, innovations such as compilers, semiconductor transistors,
    integrated circuits, and computer chips made digital electronic computers capable
    of performing more complex processes (until this point, a *computer* was predominately
    the job title of mathematically gifted humans employed to perform complex calculations3).
    This, in turn, led to some early innovations that underpin today’s ML models.
  prefs: []
  type: TYPE_NORMAL
- en: In 1943, American scientists Walter Pitts and Warren McCullough created the
    world’s first computational model for neural networks. This formed the basis for
    other innovations in AI, including Arthur Samuel’s self-improving checkers-playing
    program in 1952 and the **perceptron**, a neural network for classifying images
    funded by the US Navy and IBM in 1958.
  prefs: []
  type: TYPE_NORMAL
- en: In 1950, British mathematician and computer scientist Alan Turing introduced
    the *Turing test* for assessing a computer’s ability to perform intelligent operations
    comparable to those of humans. The test was often used as a benchmark for the
    *intelligence* of a computer and became very influential to the philosophy of
    AI in general.
  prefs: []
  type: TYPE_NORMAL
- en: The expansion of ML research continued throughout the 1960s, with the development
    of the nearest neighbor algorithm being one of the most noticeable advances. The
    work of Stanford researchers Thomas Cover and Peter Hart formed the basis for
    the rise of the k-nearest neighbor algorithm as a powerful statistical classification
    method4.
  prefs: []
  type: TYPE_NORMAL
- en: In 1965, co-founder of Fairchild Semiconductor and Intel, Gordon Moore proposed
    that processing power and hard drive storage for computers would double every
    two years, also known as *Moore’s law*5\. Even though Moore’s law proved to be
    reasonably accurate, it would take many decades to reach a point where vast amounts
    of data could be processed at a reasonable speed and cost.
  prefs: []
  type: TYPE_NORMAL
- en: To put things into perspective, IBM’s leading product in 1970 was the System/370
    Model 145, which had 500 KB of RAM and 233 MB of hard disk space6\. The computer
    took up a whole room and cost $705,775 to $1,783,0007, circa $5 to $13 million
    in today’s inflation-adjusted dollars. At the time of writing, the latest iPhone
    14 has 12,000 times the amount of RAM and up to 2,200 times the amount of hard
    disk space of the System/370 Model 145, depending on the iPhone configuration8.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – The IBM System/370 Model 145\. Everything in this picture is
    part of the computer’s operation (except the clock on the wall). Source: Jean
    Weber/INRA, DIST](img/B19297_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1 – The IBM System/370 Model 145\. Everything in this picture is part
    of the computer’s operation (except the clock on the wall). Source: Jean Weber/INRA,
    DIST'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the 1970s are widely recognized as a period of “AI Winter” – a period
    with very little ground-breaking research or developments in the field of AI.
    The business world saw little short-term potential in AI, mainly because computer
    processing power and data storage capacity were underdeveloped and prohibitively
    expensive.
  prefs: []
  type: TYPE_NORMAL
- en: The 1980s to 1990s – the rise of personal computing and the internet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 1982, IBM introduced the first personal computer (IBM PC), which sparked
    a revolution in computer technology at work and in people’s homes. It also led
    to the meteoric rise of companies such as Apple, Microsoft, Hewlett-Packard, Intel,
    and many other hardware and software enterprises that rode the wave of technological
    innovation.
  prefs: []
  type: TYPE_NORMAL
- en: The increased ability to digitize processes and information also amplified the
    corporate world’s interest in using stored data for analytical purposes. Relational
    databases became mainstream, at the expense of network and hierarchical database
    models9.
  prefs: []
  type: TYPE_NORMAL
- en: The query language SQL was developed in the 1970s; throughout the 1980s, it
    became widely accepted as the main database language, achieving the ISO and ANSI
    certifications in 198610.
  prefs: []
  type: TYPE_NORMAL
- en: The explosion in digital information created a need for new techniques to make
    sense of data from a statistical point of view. Stanford University researchers
    developed the first software to generate classification and regression trees in
    1984, and innovations such as the lexical database WordNet created the early foundations
    for text analysis and natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Personal computers continued to replace typewriters and mainframes into the
    1990s, which allowed for the World Wide Web to be formed in 1991\. Websites, blogs,
    internet forums, emails, instant messages, and VoIP calls created yet another
    explosion in the volume, variety, and velocity of data.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, new methods for organizing more complex and disparate types of
    data evolved. Gradient boosting algorithms such as AdaBoost and gradient boosting
    machines were developed by Stanford researchers throughout the late nineties,
    paving the way for search engines to rank all sorts of information.
  prefs: []
  type: TYPE_NORMAL
- en: The rise of the internet also created a huge business opportunity for those
    who could organize the information on it. Companies such as Amazon, Alibaba, Yahoo!,
    and Google were founded during this period to fight for dominance in e-commerce
    and web search. These companies saw enormous potential in computer science, AI,
    and ML and invested heavily in developing algorithms to manage their vast stores
    of information.
  prefs: []
  type: TYPE_NORMAL
- en: The 2000s – the rise of tech giants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML research picked up pace throughout the 2000s, whether it be in universities
    or corporate **research and development** (**R&D**) departments. Computer processing
    power had finally reached a point where large-scale data processing was feasible
    for most corporations and researchers.
  prefs: []
  type: TYPE_NORMAL
- en: While internet search engine providers were busy developing algorithms to sort
    and categorize the ever-growing information being published online, university
    researchers were creating new tools and techniques that would fuel the evolution
    of ML.
  prefs: []
  type: TYPE_NORMAL
- en: In 2003, The R Foundation was created to develop and support the open source
    ML tool and programming language R. As a freely available and open source programming
    language for statistical computing and graphics11, R significantly lowered the
    barrier to entry for researchers looking to use statistical programming in their
    work and for data enthusiasts wanting to practice and learn ML techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest algorithms were introduced in 2001 and later patented in 2006
    by statisticians and ML pioneers Leo Breiman from the University of California,
    Berkley, and Adele Cutler from Utah State University12.
  prefs: []
  type: TYPE_NORMAL
- en: Stanford professor Fei-Fei Li introduced the ImageNet project in 2008 as a free
    and open image database for training object recognition models13\. The database
    was created to provide a high-quality, standardized dataset for object categorization
    models to be trained and benchmarked on. At the time of writing, ImageNet contains
    more than 14 million labeled images, organized according to the WordNet hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: This period also saw the meteoric rise of the network-based business model as
    a way to create internet dominance. Social media platforms such as LinkedIn, Facebook,
    Twitter, and YouTube were launched during this period and became supernational
    tech giants by using ML algorithms to organize information and content created
    by their users.
  prefs: []
  type: TYPE_NORMAL
- en: As data volumes exploded, so did the need for cheap and flexible data storage.
    Cloud compute and storage services such as AWS, Dropbox, and Google Drive were
    launched, while universities joined forces with Google and IBM to establish server
    farms that could be used for data-intensive research14\. Increasingly, the availability
    of processing power was now based on the user’s economic justification rather
    than technical limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 2010–now – big data drives AI innovation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network-based businesses continued to define the direction for the internet
    and ML development. Search engines, social media platforms, and software and hardware
    providers invested heavily in R&D activities surrounding AI. As an example, the
    Google Brain research team was founded in 2011 to provide cutting-edge AI research
    on big data.
  prefs: []
  type: TYPE_NORMAL
- en: New network-based companies were disrupting industries such as taxis, hotels,
    travel services, payments, restaurant and food services, media, music, banking,
    consumer retail, and education – utilizing digital platforms, ML, and vast amounts
    of consumer data as their powerful competitive advantage.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional research institutions formed tight collaborations with big tech
    companies, resulting in big leaps in deep learning techniques for audio and image
    recognition, natural language understanding, anomaly detection, synthetic data
    generation, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: By 2017, three out of four teams competing in the annual ImageNet Challenge
    achieved greater than 95% accuracy, proving that image recognition algorithms
    were now highly advanced.
  prefs: []
  type: TYPE_NORMAL
- en: Powerful algorithms for generating new data were also developed during this
    golden decade of AI. In 2014, a researcher from the Google Brain team named Ian
    Goodfellow invented the **Generative Adversarial Network** (**GAN**), a neural
    network that works by pairing two models against each other15\. Another form of
    generative model framework, the **Generative Pre-trained Transformer** (**GPT**),
    entered the scene in 2018, courtesy of the OpenAI research lab.
  prefs: []
  type: TYPE_NORMAL
- en: With generative models in operation, it was now possible to produce *human-like*
    outputs such as text snippets, images, artwork, music, and deepfakes – audio and
    video impersonations of someone’s voice and mannerisms.
  prefs: []
  type: TYPE_NORMAL
- en: As *big data*, *ML*, and *AI* became part of the vernacular, the demand for
    analysts, data scientists, data engineers, and other data professionals increased
    substantially. In 2011, job listings for data scientists increased by 15,000%
    year on year16\. The massive enthusiasm for the potential of data and ML caused
    analytics pioneers Tom Davenport and DJ Patil to label data science as *the sexiest
    job of the 21*st *century* in 201217.
  prefs: []
  type: TYPE_NORMAL
- en: Millions of data enthusiasts around the world sought out places to learn the
    latest ML and data mining techniques. Platforms such as Kaggle and Coursera allowed
    millions of users to learn through open online courses, enter ML contests, access
    quality datasets, and share knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the tooling front, the proliferation of freely downloadable software programs
    and packages running on R, Python, or SQL made it relatively easy to access advanced
    data science techniques at a low cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – A history of ML from 1940 to now](img/B19297_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – A history of ML from 1940 to now
  prefs: []
  type: TYPE_NORMAL
- en: As the advancements in information technology, data, and AI converged during
    AI’s golden decade of 2010 to 2020, ML model architectures have matured significantly.
    At this point, most of the opportunities to create better models lie in improving
    data quality.
  prefs: []
  type: TYPE_NORMAL
- en: Model-centricity was the logical evolutionary outcome
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last eight decades of data science history have followed a logical evolutionary
    path that has led to model-centricity being the principal approach to ML.
  prefs: []
  type: TYPE_NORMAL
- en: The ideas and mathematical concepts behind ML were imagined long before the
    technology was mature enough to match them. Before the 1990s, computers were not
    powerful enough to allow university researchers to evolve the field of ML substantially.
    These technical limitations also meant that there was limited research conducted
    for commercial gain by private enterprises during this period.
  prefs: []
  type: TYPE_NORMAL
- en: At the advent of the internet era in the early 1990s, hardware and software
    solutions were beginning to be advanced enough to eliminate these age-old limitations.
    The internet also sparked an information revolution that increased the volume
    and variety of available data enormously. All of a sudden, ML was not just financially
    viable, it became the driving force behind tech companies such as Amazon, Yahoo!,
    and Google. With more digital information available than ever before, there was
    a need to advance the way we interpreted and modeled various kinds of data. In
    other words, ML research needed a model-centric focus first and foremost.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the 2000s, a new kind of business model came to dominate our lives.
    Network-based digital businesses such as social media platforms, search engines,
    software creators, and online marketplaces created platforms where users could
    create and interact with content and products. By applying ML to massive amounts
    of user-generated data, these businesses watched and optimized every interaction
    along the way.
  prefs: []
  type: TYPE_NORMAL
- en: These “AI-first” big tech businesses were less constrained by data quality or
    volume. Their constraints lay mostly in fast and affordable compute and storage
    capacity, and the sophistication of ML techniques. Through in-house research,
    partnerships with universities, and strategic investments in promising AI technologies,
    big tech companies have been able to drive the agenda for ML development over
    the last two decades. What these companies needed primarily was a model-centric
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of the model-centric research that has occurred since the mid-1990s,
    we now have algorithms that can organize all the world’s information, identify
    individuals in a crowd, drive vehicles in open traffic, recognize and generate
    sound, speech, and imagery, and much more. Our ability to make accurate models
    *given the input data* is very advanced thanks to this period of innovation.
  prefs: []
  type: TYPE_NORMAL
- en: 'As data continued to become a more ubiquitous asset, there was a sudden strong
    need to train more data scientists and other data professionals. Today, there
    is no shortage of learning opportunities through online learning platforms, university
    courses, and ML competitions, but they typically have one thing in common: the
    initial input dataset is predefined.'
  prefs: []
  type: TYPE_NORMAL
- en: It makes a lot of sense to teach ML on a fixed dataset. Without a replicable
    output, it is difficult to verify whether learners have mastered a particular
    technique, or benchmark different models against each other. However, the natural
    consequence is that learning is centered around model improvement through model-centric
    tasks such as model selection, hyperparameter tuning, feature engineering, and
    other enhancements of the *existing* dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model-centric skills must be mastered by experienced data scientists, but they
    are just the foundation of a data-centric paradigm. This is because ML progress
    comes in four parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Improving computer power
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improving algorithms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improving data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improving measurement
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So far, we have made huge progress on items 1 and 2, to a point where they are
    largely a solved problem for the majority of ML use cases. Most of the opportunity
    now lies in evolving our approach to improving data and measurement. When we improve
    our data, we can build better models, but we also unlock the long tail of ML use
    cases that are often out of reach because we only have a few thousand rows (or
    less) of data to build our models on.
  prefs: []
  type: TYPE_NORMAL
- en: Unlocking the opportunity for small data ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The group of tech companies famously labeled *The Big Nine* by author Amy Webb18
    are examples of consumer internet companies that have leveraged big data and AI
    to build world dominance. Amazon, Apple, Alibaba, Baidu, Meta, Google, IBM, Microsoft,
    and Tencent dominate in the digital era because they utilize enormous amounts
    of user data to power their AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: As network-based *AI-first* businesses, they have amassed customers on an unprecedented
    scale because users are happy to co-create and share their data, so long as it
    is a net benefit to them. For the Big Nine, getting enough modeling data is rarely
    a problem, and investing in the most advanced ML capabilities is a virtuous circle
    that enables more market dominance.
  prefs: []
  type: TYPE_NORMAL
- en: 'For most other organizations – and ML use cases – this sort of scale is unachievable.
    As we explored in [*Chapter 1*](B19297_01.xhtml#_idTextAnchor015), *Exploring
    Data-Centric Machine Learning* the long tail of ML opportunities doesn’t offer
    the option to build models on large volumes of training data because of the following
    challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The lack of training data observations**: Datasets are smaller in the long
    tail – typically in the order of only a few thousand rows or less. On top of that,
    most organizations are capturing data in the non-digitized physical world, which
    makes it harder to capture and finetune some data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dirty data**: Unlike network-based *AI-first* businesses, most organizations
    generate data through a large variety of sources such as internal (but externally
    developed) IT systems, third-party platforms, and manual collection by staff or
    customers. This creates a complex patchwork of data sources that come with a variety
    of data quality challenges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk of bias and unfairness in high-stakes domains**: Poor data quality in
    high-stakes domains such as healthcare, legal services, education, public safety,
    and crime prevention may lead to disastrous impacts on individuals or vulnerable
    populations. For example, predicting whether a person has cancer based on medical
    images is a high-stakes activity – recommending the next video to watch based
    on your YouTube history video is not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model complexity and lack of economies of scale**: Even though there is plenty
    of value to be found in the long tail, individual ML projects typically need a
    lot of customization to deal with distinct scenarios. Customization is costly
    as it creates an accumulation of many models, datasets, and processes that must
    be maintained pre- and post-model implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The need for domain expertise in data and model development**: The combination
    of small datasets, higher stakes, and more complex scenarios makes it difficult
    to build ML models without the involvement of subject-matter experts during data
    collection, labeling and validation, model development, and testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that many companies have the opportunity to unlock significant
    value with *small data* ML. For example, only a few organizations will have individual
    ML projects worth $50 million or more, but many more organizations will have 50
    potential ML opportunities worth $1 million each. In practice, this means we must
    get maximum value out of our raw material if we want smaller projects to become
    feasible and financially viable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dr Andrew Ng, CEO and founder of Landing AI, summarizes these challenges as
    follows19:'
  prefs: []
  type: TYPE_NORMAL
- en: “In the consumer software Internet, we could train a handful of ML models to
    serve a billion users. In manufacturing, you might have 10,000 manufacturers building
    10,000 custom AI models.”
  prefs: []
  type: TYPE_NORMAL
- en: “In many industries, where giant data sets simply don’t exist, I think the focus
    has to shift from big data to good data. Having 50 thoughtfully engineered examples
    can be sufficient to explain to the neural network what you want it to learn.”
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.3* illustrates the challenge and opportunity of *small data* ML.
    While the low-hanging fruits of big data/high-value ML use cases have been picked
    by *AI-first* businesses, the long tail of small data/moderate value is underexploited.
    In reality, most ML use cases exist in the long tail of smaller datasets and low
    economies of scale. A strong focus on data quality is needed to make ML useful
    when datasets are small:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.3 – \uFEFFThe long tail of ML opportunities](img/B19297_02_3.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – The long tail of ML opportunities
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore the challenges in working with smaller
    and more complex datasets, and how you can overcome them.
  prefs: []
  type: TYPE_NORMAL
- en: Why we need data-centric AI more than ever
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The leading organizations in AI, such as the Big Nine, have achieved incredible
    results with ML since the turn of the century, but how is AI being used in the
    long tail?
  prefs: []
  type: TYPE_NORMAL
- en: A 2020 survey published by MIT Sloan Management Review and Boston Consulting
    Group concluded that most companies struggle to turn their vision for AI into
    reality. In a survey of over 3,000 business leaders from 29 industries in 112
    countries, 70% of respondents understood how AI can generate business value and
    57% had piloted or productionized AI solutions. However, only 1 in 10 had been
    able to generate significant financial benefits with AI.20
  prefs: []
  type: TYPE_NORMAL
- en: 'The survey authors found that companies that were realizing significant financial
    benefits with AI had built their success on two pillars:'
  prefs: []
  type: TYPE_NORMAL
- en: They had a solid foundation of the right data, technology, and talent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They had defined several effective ways for humans and AI to work and learn
    together. In other words, they had created an iterative feedback loop between
    humans and AI, going from data collection and curation to solution deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are these two pillars critical to success with ML and AI? Because the ML
    model is only a small part of an ML system.
  prefs: []
  type: TYPE_NORMAL
- en: In 2015, Google researchers Sculley et al.21 published a seminal paper called
    *Hidden Technical Debt in Machine Learning Systems*, in which they describe how
    “*only a small fraction of real-world ML systems are composed of the ML code…
    the required surrounding infrastructure is vast* *and complex.*”
  prefs: []
  type: TYPE_NORMAL
- en: In traditional information technology jargon, *technical debt* refers to the
    long-term costs incurred by cutting corners in the software development life cycle.
    It’s the hardcoded logic, the missing documentation, the lack of integration with
    other platforms, inefficient code, and anything else that is a roadblock to better
    system performance and future improvements. Technical debt can be “paid down”
    by removing these issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'ML systems are different in that they can carry technical debt in code, but
    they also have the added complexity that technical debt may exist in the data
    components of the system. Input data is the foundational ingredient in the system
    and the data is variable. Because ML models are driven by weighted impacts from
    many features in both data and code, a change in one variable may change the logical
    structure of the rest of the model. This is also known as the CACE principle:
    *Changing Anything* *Changes Everything*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in *Figure 2**.4*, a productionized ML system is much more than
    the model code. In a typical ML project, it is estimated that only 5-10% of the
    overall system is the model code22\. The remaining 90-95% of the solution is related
    to data and infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.4 – ML systems are much more than code. Source: Adapted from Sculley\
    \ et al\uFEFF., 2015](img/B19297_02_4.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4 – ML systems are much more than code. Source: Adapted from Sculley
    et al., 2015'
  prefs: []
  type: TYPE_NORMAL
- en: As Sculley et al. described, the data collection and curation activities in
    an ML solution are often significantly more resource-intensive than direct model
    development activities. Given this, data engineering should be a data scientist’s
    best friend. Yet, there is a disconnect between the importance of data quality
    and how most ML solutions are developed in practice.
  prefs: []
  type: TYPE_NORMAL
- en: The cascading effects of data quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2021, Google researchers Sambasivan et al.23 conducted a research study of
    the practices of 53 ML practitioners from the US, India, and East and West Africa
    working in a variety of industries. The study participants were selected from
    high-stakes domains such as healthcare, agriculture, finance, public safety, environmental
    conversation, and education.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the study was to identify and describe the downstream impact
    of data quality on ML systems and present empirical evidence of what they call
    *data cascades* – compounding negative effects stemming from data quality issues.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data cascades** are caused by conventional model-centric ML practices that
    undervalue data quality and typically lead to invisible and delayed impacts on
    model performance – in other words, ML-specific technical debt. According to the
    researchers, data cascades are highly prevalent, with 92% of ML practitioners
    in the study experiencing one or more data cascades in a given project.'
  prefs: []
  type: TYPE_NORMAL
- en: The causes of data cascades fit into four categories explained in the following
    subsections.
  prefs: []
  type: TYPE_NORMAL
- en: The perceived low value of data work and lack of reward systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are often two underlying reasons for the lack of available data in the
    long tail of ML opportunities:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, the events being modeled are bespoke and rare, so there is a physical
    limit to the amount of data that can be collected for a given use case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, data collection and curation activities are considered relatively
    expensive and difficult, especially when they involve manual collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In truth, most data-related work is not done by data scientists. The roles directly
    responsible for creating, collecting, and curating data are often performing these
    tasks as a secondary duty in their job. The responsibility of collecting high-quality
    data is frequently at odds with other duties because of competing priorities,
    time constraints, technical limitations of collection systems, or simply a lack
    of understanding of how to carry out good data collection.
  prefs: []
  type: TYPE_NORMAL
- en: Take, for example, a hospital nurse who is responsible for a wide variety of
    tasks relating to the care of patients, some of which are data collection. High-quality
    data in healthcare has the potential to create huge benefits for patients and
    healthcare providers around the world if it can be aggregated and generalized
    through ML. However, for the individual nurse, there is more incentive to do the
    minimum required to document patient status and medical interventions, so more
    time can be spent on primary patient care. The typical result of this kind of
    scenario is suboptimal data collection in terms of depth of detail and consistency
    of labeling.
  prefs: []
  type: TYPE_NORMAL
- en: ML practitioners face a similar challenge further downstream. Sambasivan et
    al. describe how business and project goals such as cost, revenue, time to market,
    and competitive pressures lead data scientists to hurry through model development,
    leaving insufficient room for data quality and ethics concerns. As one practitioner
    states, *everyone wants to do the model work, not the* *data work*.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of cross-functional collaboration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to high-stakes or bespoke ML projects, subject-matter experts
    are often critical participants in upstream data collection as well as the ultimate
    consumers of model outputs.
  prefs: []
  type: TYPE_NORMAL
- en: On the face of it, subject-matter experts should be very willing to participate
    actively in ML projects because they get to reap the benefits of useful models.
    However, the opposite is often the case.
  prefs: []
  type: TYPE_NORMAL
- en: A requirement to collect additional information for ML purposes typically means
    that data collectors and curators have to work harder to get their job done. It
    can be difficult for frontline workers with limited data literacy to appreciate
    the importance of data collection, and unfortunately, the cascading effect of
    this conduct shows up much later in the project life cycle – often after deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists should also play a critical role in data collection as they
    will make many decisions on how to interpret and manipulate datasets during model
    development. Therefore, an ML practitioner’s curiosity and willingness to understand
    the technical and social contexts of a given domain is a critical part of any
    project’s success. It is the invisible glue that makes ML solutions relevant and
    accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, data scientists often lack domain-specific expertise and rely
    on subject-matter experts to validate their interpretation of datasets. If ML
    practitioners do not constantly question their assumptions, rely too heavily on
    their technical expertise, and take the accuracy of input data for granted, they
    will miss the finer points of the context they’re trying to model. When this happens,
    ML projects will suffer from data cascades.
  prefs: []
  type: TYPE_NORMAL
- en: Insufficient cross-functional collaboration results in costly project challenges
    such as additional data collection, misinterpretation of results, and lack of
    trust in ML as a relevant solution to a given problem.
  prefs: []
  type: TYPE_NORMAL
- en: Educational and knowledge gaps for ML practitioners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even the most technically skilled ML practitioners may fail to build useful
    models for real-life scenarios if they lack end-to-end knowledge of ML pipelines.
    Unfortunately, most learning paths for data scientists lack appropriate attention
    to data engineering practices.
  prefs: []
  type: TYPE_NORMAL
- en: Graduate programs and online training courses are built on clean datasets, but
    real life is full of dirty data. Data scientists are simply not trained in building
    ML solutions from scratch, including data collection design, data management,
    and data governance processes, training data collectors, cleaning dirty data,
    and building domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, data engineering and MLOps practices are poorly understood and
    under-appreciated by those who are directly responsible for turning raw data into
    useful insights.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of measurement of and accountability for data quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Conventional ML practices rely on statistical accuracy tests, such as *precision*
    and *recall*, as proxies for model *and* data quality. These measures don’t provide
    any direct information on the quality of a dataset as it pertains to representing
    specific events and relevant situational context. The lack of standardized approaches
    for identifying and rectifying data quality issues early in the process makes
    data improvement work reactive, as opposed to planned and aligned to project goals.
  prefs: []
  type: TYPE_NORMAL
- en: The much-used management phrase *what gets measured gets managed* is also true
    in a data quality setting. Without appropriate processes in place for identifying
    data quality issues, it is difficult to incentivize and assign accountability
    to individuals for good data collection.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of assigning accountability for data quality in high-stakes domains
    is underpinned by the fact that model accuracy typically has to be very high,
    based on small datasets. For example, a poorly performing model in a low-risk
    and data-rich industry, such as online retailing or digital advertising, can be
    modified relatively quickly given the automated and persistent nature of data
    collection.
  prefs: []
  type: TYPE_NORMAL
- en: ML models deployed in the long tail are often harder to validate because of
    a much lower frequency of events. At the same time, high-stakes domains typically
    demand a higher model accuracy threshold. Online advertisers can probably live
    with an accuracy score of 75%, but a model built for cancer diagnosis typically
    has to have an error rate of less than 1% to be viable.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding data cascades and technical debt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The pervasiveness of data cascades highlights a larger underlying problem:
    the dominant conventions in ML development are drawn from the practices of *big
    data* companies. These practices have been developed in an environment of plentiful
    and expendable data where each user has one account24\. Combine this with a culture
    of *move fast and break things*25 while viewing data work as undesirable drudgery,
    and you have an approach that will fail in most high-stakes domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cascading effects of poor data are opaque and hard to track in any standardized
    way, even though they occur frequently and persistently. Fortunately, data cascades
    are also fixable. Sambasivan et al. define the concept of *data excellence* as
    the solution: a cultural shift toward recognizing data management as a core business
    discipline and establishing the right processes and incentives for those who are
    a part of the ML pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: As data professionals, it’s up to us to decide whether ML should remain a tool
    for the few or whether it’s time to allow projects with smaller financial value
    or higher stakes to become viable. To do this, we must strive for data excellence.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s summarize the key takeaways from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed the history of ML to give us a clear understanding
    of why model-centric ML is the dominant approach today. We also learned how a
    model-centric approach limits us from unlocking the potential value tied up in
    the long tale of ML opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should have a strong appreciation for why data-centricity is needed
    for the discipline of ML to achieve its full potential but also recognize that
    it will require substantial effort to make the shift. To become an effective data-centric
    ML practitioner, old habits must be broken and new ones formed.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it’s time to start exploring the tools and techniques to make that shift.
    In the next chapter, we will discuss the principles of data-centric ML and the
    techniques and approaches associated with each principle.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://www.idc.com/getdoc.jsp?containerId=prUS47560321](https://www.idc.com/getdoc.jsp?containerId=prUS47560321),
    viewed on 23 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lewis, A. R., 2006, *The American Culture of War*, Routledge, New York, USA
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.nasa.gov/feature/when-the-computer-wore-a-skirt-langley-s-computers-1935-1970](https://www.nasa.gov/feature/when-the-computer-wore-a-skirt-langley-s-computers-1935-1970),
    viewed on 23 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/](https://www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/),
    viewed on 23 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://large.stanford.edu/courses/2012/ph250/lee1/docs/Excepts_A_Conversation_with_Gordon_Moore.pdf](http://large.stanford.edu/courses/2012/ph250/lee1/docs/Excepts_A_Conversation_with_Gordon_Moore.pdf),
    viewed on 23 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.businessinsider.com/ibm-1970-mainframe-specs-are-ridiculous-today-2014-5](https://www.businessinsider.com/ibm-1970-mainframe-specs-are-ridiculous-today-2014-5),
    viewed on 22 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP3145.html](https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP3145.html),
    viewed on 22 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.apple.com/au/iphone-14/specs/](https://www.apple.com/au/iphone-14/specs/),
    viewed on 23 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.quickbase.com/articles/timeline-of-database-history](https://www.quickbase.com/articles/timeline-of-database-history),
    viewed on 24 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.dataversity.net/brief-history-database-management/](https://www.dataversity.net/brief-history-database-management/),
    viewed on 24 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.r-project.org/about.html](https://www.r-project.org/about.html),
    viewed on 24 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.historyofdatascience.com/leo-breiman-statistics-at-the-service-of-others/](https://www.historyofdatascience.com/leo-breiman-statistics-at-the-service-of-others/),
    viewed on 24 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.image-net.org/about.php](https://www.image-net.org/about.php),
    viewed on 24 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.dataversity.net/brief-history-cloud-computing/](https://www.dataversity.net/brief-history-cloud-computing/),
    viewed on 25 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://thenextweb.com/news/2010-2019-the-rise-of-deep-learning](https://thenextweb.com/news/2010-2019-the-rise-of-deep-learning),
    viewed on 25 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.dataversity.net/brief-history-data-science/](https://www.dataversity.net/brief-history-data-science/),
    viewed on 25 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century),
    viewed on 25 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Webb, A., 2019, *The Big Nine: How Tech Titans and Their Thinking Machines
    Could Warp Humanity*, Hachette Book Group, New York, USA'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://spectrum.ieee.org/andrew-ng-data-centric-ai](https://spectrum.ieee.org/andrew-ng-data-centric-ai),
    viewed on 25 September 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ransbotham, S., Khodabandeh, S., Kiron, D., Candelon, F., Chu, M., and LaFountain,
    B., *Expanding AI’s Impact With Organizational Learning*, MIT Sloan Management
    Review and Boston Consulting Group, October 2020
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf),
    Sculley et al., 2015, viewed 23 July 2022,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yang, K., 2022, *Landing AI – Moving Beyond the Software* *Industry*, [https://community.ai-infrastructure.org/public/videos/landing-ai-ai-moving-beyond-the-software-industry-2022-09-30](https://community.ai-infrastructure.org/public/videos/landing-ai-ai-moving-beyond-the-software-industry-2022-09-30)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sambasivan, N., Kapania, S., Highfill, H., Akrong, D., Paritosh, P., Aroyo,
    L., 2021, *Everyone wants to do the model work, not the data work:* *Data Cascades
    in* *High-Stakes AI*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://hbr.org/2019/01/the-era-of-move-fast-and-break-things-is-over](https://hbr.org/2019/01/the-era-of-move-fast-and-break-things-is-over),
    viewed on 8 October 2022'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Part 2: The Building Blocks of Data-Centric ML'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we lay the groundwork for data-centric ML with four key principles
    that underpin this approach, giving you essential context before exploring specific
    techniques. Then we explore human-centric and non-technical approaches to data
    quality, examining how expert knowledge, trained labelers, and clear instructions
    can enhance your ML output.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B19297_03.xhtml#_idTextAnchor043)*, Principles of Data-Centric
    ML*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B19297_04.xhtml#_idTextAnchor056)*, Data Labeling Is a Collaborative
    Process*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
