- en: Chapter 6. Instance and Kernel Methods Based Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章：基于实例和核方法的实例学习
- en: We have covered Decision tree models for solving classification and regression
    problems in the previous chapter. In this chapter, we will cover two important
    models of supervised and unsupervised learning techniques which are the Nearest
    Neighbors method, which uses the instance-based learning model, and the **Support
    Vector Machines** (**SVM**) model, which uses kernel methods based learning model.
    For both methods, we will learn the basics of the technique and see how it can
    be implemented in Apache Mahout, R, Julia, Apache Spark, and Python. The following
    figure depicts different learning models covered in this book and the techniques
    highlighted will be dealt in covered in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了用于解决分类和回归问题的决策树模型。在本章中，我们将介绍两种重要的监督学习和无监督学习技术模型，即使用基于实例学习模型的最近邻方法，以及使用基于核方法学习模型的**支持向量机**（**SVM**）模型。对于这两种方法，我们将学习其技术基础，并了解如何在
    Apache Mahout、R、Julia、Apache Spark 和 Python 中实现。以下图展示了本书中涵盖的不同学习模型以及将在本章中讨论的技术。
- en: '![Instance and Kernel Methods Based Learning](img/B03980_06_01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![基于实例和核方法的实例学习](img/B03980_06_01.jpg)'
- en: 'The following topics are covered in-depth in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节深入探讨了以下主题：
- en: Instance-based learning models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于实例的学习模型
- en: Introduction to instance-based learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于实例学习的简介
- en: Lazy and eager learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 懒学习和积极学习
- en: A brief look at different algorithms/approaches of instance-based learning techniques
    Nearest Neighbor method, Case-based reasoning, Locally weighed regression, and
    Radial basis functions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简要介绍基于实例学习的不同算法/方法，如最近邻方法、案例推理、局部加权回归和径向基函数
- en: A deep dive into KNN (k-Nearest Neighbor) algorithm with a real-world use case
    example; mechanisms to speed up KNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入探讨KNN（k-最近邻）算法，并附上实际应用案例；加快KNN的机制
- en: Sample implementation of Apache Mahout, R, Apache Spark, Julia and Python (scikit-learn)
    libraries and modules
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Mahout、R、Apache Spark、Julia 和 Python（scikit-learn）库和模块的示例实现
- en: Kernel-based learning models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于核的学习模型
- en: Introduction to kernel-based learning
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于核的学习简介
- en: A brief look at different algorithms/approaches of Kernel-based learning techniques,
    Support Vector Machines (SVM), Linear Discriminate Analysis (LDA), and more
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简要介绍基于核的学习技术、支持向量机（SVM）、线性判别分析（LDA）等不同算法/方法
- en: A deep dive into SVM algorithm with a real-world use case example
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入探讨SVM算法，并附上实际应用案例
- en: Instance-based learning (IBL)
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于实例的学习（IBL）
- en: The IBL technique approaches learning by simply storing the provided training
    data and using it as a reference for predicting/determining the behavior of a
    new query. As learned in [Chapter 1](ch01.html "Chapter 1. Introduction to Machine
    learning"), *Introduction to Machine learning*, instances are nothing but subsets
    of datasets. The instance-based learning model works on an identified instance
    or groups of instances that are critical to the problem. The results across instances
    are compared and can include an instance of new data as well. This comparison
    uses a particular similarity measure to find the best match and predict. Since
    it uses historical data stored in memory, this learning technique is also called
    memory-based or case-based learning. Here, the focus is on the representation
    of the instances and similarity measures for comparison between them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基于实例学习的IBL技术通过简单地存储提供的训练数据，并将其用作预测/确定新查询行为时的参考。正如在[第1章](ch01.html "第1章。机器学习简介")中学习的，*机器学习简介*，实例不过是数据集的子集。基于实例的学习模型在识别的实例或对问题至关重要的实例组上工作。实例之间的结果进行比较，可以包括新数据的一个实例。这种比较使用特定的相似性度量来找到最佳匹配并进行预测。由于它使用存储在内存中的历史数据，这种学习技术也被称为基于内存或基于案例的学习。在这里，重点是实例的表示以及它们之间比较的相似性度量。
- en: '![Instance-based learning (IBL)](img/B03980_06_02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![基于实例的学习（IBL）](img/B03980_06_02.jpg)'
- en: Every time a new query instance is received for processing, a set of similar,
    related instances are retrieved from memory, and then this data is used to classify
    the new query instance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 每当接收到一个新的查询实例进行处理时，都会从内存中检索出一组相似的、相关的实例，然后使用这些数据来对新查询实例进行分类。
- en: Instance-based learners are also called lazy learners. Overall, the entire database
    is used to predict behavior. A set of data points referred to as **neighbors**
    are identified, having a history of agreeing with the target attribute. Once a
    neighborhood of data points is formed, the preferences of neighbors are combined
    to produce a prediction or **top-K recommendation** for the active target attribute.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于实例的学习者也被称为懒惰学习者。总的来说，整个数据库被用来预测行为。一组被称为**邻居**的数据点被识别出来，它们有与目标属性一致的历史。一旦形成数据点的邻域，邻居的偏好将被结合以产生对活动目标属性的预测或**top-K推荐**。
- en: These methods are applicable for complex target functions that can be expressed
    using less complex local approximations. Unfortunately, with these methods, the
    cost of classifying a new instance is always high and in cases where there is
    a curse of dimensionality, these methods might end up with a bigger footprint
    as all the attributes of all the instances are considered. Classifiers and regressions
    are what we will cover in this and the next chapters that are to come. With classifiers,
    we try to predict a category and, with regression, we predict a real number. We
    will first look at the Nearest Neighbor algorithm that can be used both for classification
    and regression problems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法适用于可以用更简单的局部近似表示的复杂目标函数。不幸的是，使用这些方法，对新的实例进行分类的成本总是很高的，在存在维度诅咒的情况下，这些方法可能会留下更大的足迹，因为所有实例的所有属性都被考虑在内。分类器和回归将在本节和即将到来的下一章中介绍。使用分类器，我们试图预测一个类别，而使用回归，我们预测一个实数。我们将首先查看最近邻算法，它可以用于分类和回归问题。
- en: Rote Learner is one of the instance-based classifiers and focuses on memorizing
    the entire training data. Classification is primarily done only if the target
    attribute value exactly matches with the attribute value in the training example.
    The other classifier is the Nearest Neighbor, which classifies based on the closest
    neighbor(s). In the next section, let's dive deeply into the Nearest Neighbor
    algorithm.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 死记硬背学习者是实例化分类器之一，专注于记忆整个训练数据。分类主要是在目标属性值与训练示例中的属性值完全匹配时才进行。另一个分类器是最近邻，它根据最近的邻居进行分类。在下节中，我们将深入探讨最近邻算法。
- en: Nearest Neighbors
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最近邻
- en: 'Before we start understanding what the Nearest Neighbor algorithm is all about,
    let''s start with an example; the following graph shows the plotting of data points
    *X* and *Y* that have two classes: stars and triangles. Let''s not really worry
    about what is the exact data representation or the data points. If we had to solve
    intuitively the problem of finding what that particular red square box data point
    is, then the answer would obviously be a green triangle. This is an intuition
    and, without actually understanding or analyzing the data points, we can arrive
    at this conclusion. But what actually happened here is that we have seen the traits
    of the neighbors of the data point in context and have predicted the class to
    which the new data point could possibly belong to. Overall, the basis for the
    learning algorithm is actually the behavior of the nearby or neighboring points.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始理解最近邻算法是什么之前，让我们从一个例子开始；以下图表显示了数据点 *X* 和 *Y* 的绘制，它们有两个类别：星星和三角形。我们不必真正担心确切的数据表示或数据点。如果我们必须直观地解决找出那个特定红色方块数据点的问题，那么答案显然是一个绿色三角形。这是一种直觉，而且，在不真正理解或分析数据点的情况下，我们可以得出这个结论。但实际上发生的情况是，我们已经看到了数据点上下文中邻居的特征，并预测了新数据点可能属于的类别。总的来说，学习算法的基础实际上是附近或邻近点的行为。
- en: '![Nearest Neighbors](img/B03980_06_03.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![最近邻](img/B03980_06_03.jpg)'
- en: Nearest Neighbor is an algorithm that uses this basic technique of intuition.
    This algorithm finds the Nearest Neighbor using some distance measurement techniques
    that will be discussed in the following sections. Let's now extend this to another
    example data set; and again, the new data point with a question mark (?) will
    be needed for classification.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻算法是一种利用直觉基本技术的算法。该算法使用一些将在下文讨论的距离测量技术来找到最近邻。现在让我们扩展到另一个示例数据集；再次，需要一个新的带有问号（?）的数据点来进行分类。
- en: '![Nearest Neighbors](img/B03980_06_04.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![最近邻](img/B03980_06_04.jpg)'
- en: Let's now assume that the class the new data point belongs to is the yellow
    star. An important aspect of the distance measure is that the Nearest Neighbor
    is never just a single point but is usually a region. The following figure shows
    a region and all the data points that fall in this region belong to the class
    yellow star. This region is called the **Voronoi cell**. This region is usually
    a polygon with straight lines in case the distance measure used is the **Euclidean**
    distance measure.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设新的数据点所属的类别是黄色星号。距离度量的重要方面之一是最近邻永远不会只是一个单独的点，而通常是一个区域。以下图显示了该区域以及所有属于该区域的点都属于黄色星号类别。这个区域被称为**Voronoi
    单元**。这个区域通常是具有直线边界的多边形，如果使用的距离度量是**欧几里得**距离度量。
- en: '![Nearest Neighbors](img/B03980_06_05.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![最近邻](img/B03980_06_05.jpg)'
- en: For each training example if the Voronoi cells are computed, we can see the
    Voronoi tessellation as shown next. This tessellation represents the partition
    of space into the non-overlapping region and typically each region has one example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个训练示例，如果计算了 Voronoi 单元，我们可以看到如以下图所示的 Voronoi 网格。这种网格表示空间被划分为非重叠区域，通常每个区域有一个示例。
- en: '![Nearest Neighbors](img/B03980_06_06.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![最近邻](img/B03980_06_06.jpg)'
- en: The size of the cell is determined by the number of examples available. The
    more the examples, the less the size of the regions. Another interesting aspect
    of the Voronoi tessellation is that there can be boundaries carved that form a
    separation for the classes, as shown in the following figure. The right side of
    the bold line belongs to the triangle class, and the left side belongs to the
    star class.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 单元的大小由可用示例的数量确定。示例越多，区域的大小就越小。Voronoi 网格的另一个有趣方面是，可以雕刻出边界，形成类别的分隔，如下面的图所示。粗线的右侧属于三角形类别，左侧属于星号类别。
- en: '![Nearest Neighbors](img/B03980_06_07.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![最近邻](img/B03980_06_07.jpg)'
- en: One of the major complications with the Nearest Neighbor approach is its insensitivity
    to the outliers, thus really messing up the boundaries, and one way of solving
    this problem is to consider more than one neighbor, which this would make the
    model more stable and smooth. Hence, a consideration of k-Neighbors signifies
    the k-Nearest Neighbor algorithm.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最近邻方法的一个主要问题是其对异常值的敏感性，这会真正搞乱边界，解决这个问题的方法之一是考虑多个邻居，这将使模型更加稳定和光滑。因此，考虑 k 个邻居意味着
    k-最近邻算法。
- en: '![Nearest Neighbors](img/B03980_06_08.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![最近邻](img/B03980_06_08.jpg)'
- en: 'Let''s now look at how the KNN classification algorithm works:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下 KNN 分类算法是如何工作的：
- en: 'Given the *{x[i], y[i]}* training examples, where *x*[i] represents attribute
    values, *y*[i] represents class labels, and there is a new test point *X* that
    needs to be classified, the following steps are performed in a KNN classification
    algorithm:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 *{x[i], y[i]}* 训练示例，其中 *x*[i] 表示属性值，*y*[i] 表示类别标签，并且有一个新的测试点 *X* 需要被分类，在
    KNN 分类算法中执行以下步骤：
- en: Distance is computed between *x* and *x*[i] for every given value of *x*[i].
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个给定的 *x*[i] 值之间计算 *x* 和 *x*[i] 的距离。
- en: Choose k nearest neighbors *xi1, … xik* and the respective class labels *yi1,
    … yik*.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 k 个最近的邻居 *xi1, … xik* 以及相应的类别标签 *yi1, … yik*。
- en: Return a *y* that is the most frequent in the list of labels *yi1, … yik*.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回列表 *yi1, … yik* 中出现频率最高的 *y*。
- en: Let's now see how different the KNN regression algorithm is among the important
    differences. Instead of outputting a class, we will output real numbers like ratings
    or age, and so on. The algorithm is identical, but the only variation is in the
    return value, Step 3, and instead of a most frequent value, we take the mean of
    *y's*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下 KNN 回归算法在重要差异中的不同之处。与输出一个类别不同，我们将输出实数，如评分或年龄等。算法是相同的，但唯一的区别在于返回值，即第
    3 步，我们取 *y's* 的平均值，而不是最频繁的值。
- en: Value of k in KNN
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KNN 中的 k 值
- en: The value of *k* has a tremendous effect on the KNN performance. If the value
    of *k* is too large, the KNN algorithm uses the previous value and thus might
    result in inaccuracies. And in the case where the *k* value is too small, the
    model would become too sensitive to outliers as we saw in the previous section.
    Hence, an accurate *k* value usually lies midway through the smallest and largest
    value. The approach is to choose a value in this range and measure the error on
    the training data and pick a *k* that gives the best generalization performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 的值对KNN的性能有巨大的影响。如果 *k* 的值太大，KNN算法会使用前一个值，从而可能导致不准确。在 *k* 值太小的情况下，模型会像前一个章节中看到的那样对异常值过于敏感。因此，准确的
    *k* 值通常位于最小值和最大值之间。方法是选择这个范围内的一个值，并在训练数据上测量误差，选择一个给出最佳泛化性能的 *k* 值。'
- en: 'The following figure depicts 1, 2, and 3 Nearest Neighbors for the point *x*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了点 *x* 的1、2和3个最近邻：
- en: '![Value of k in KNN](img/B03980_06_09.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![KNN中的k值](img/B03980_06_09.jpg)'
- en: k-Nearest Neighbors for the point *x* are all the data points that have the
    k smallest distance from *x*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 点 *x* 的k-最近邻是所有与 *x* 距离最小的k个数据点。
- en: Distance measures in KNN
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KNN中的距离度量
- en: This is one of the attributes of the Nearest Neighbor algorithm and possibly
    the only area that one can experiment or try alternatives in. There are many distance
    measurement options and in this section, we will discuss some of the commonly
    used measures. The primary purpose of distance measure is to identify the examples
    that are similar or dissimilar. Similar to the k value, distance measure determines
    the performance of KNN.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最近邻算法的一个属性，可能是唯一可以实验或尝试替代方法的领域。有许多距离测量选项，在本节中，我们将讨论一些常用的度量。距离度量的主要目的是识别相似或不相似的例子。与k值类似，距离度量决定了KNN的性能。
- en: Euclidean distance
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'The Euclidean distance is the default option for numeric attributes. The distance
    measure formula is given here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离是数值属性的默认选项。距离度量公式如下：
- en: '![Euclidean distance](img/B03980_06_24.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![欧几里得距离](img/B03980_06_24.jpg)'
- en: The Euclidean distance measure is symmetrical and spherical and treats all the
    dimensions equally. One of the drawbacks of this measure is its sensitivity to
    the extreme values within a single attribute. This is similar to the mean squared
    error.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离度量是对称的和球形的，并且对所有的维度都同等对待。这个度量方法的一个缺点是对单个属性中极端值的敏感性。这与均方误差类似。
- en: Hamming distance
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 汉明距离
- en: 'The Hamming distance measure is a default option if we need to deal with categorical
    attributes. The primary function of a Hamming distance measure is to check whether
    the two attributes are equal or not. When they are equal, the distance is 0, otherwise
    it is 1; in effect, we check the number of attributes between two instances. The
    formula for the Hamming distance measure is as given here:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 汉明距离度量是我们需要处理分类属性时的默认选项。汉明距离度量的主要功能是检查两个属性是否相等。当它们相等时，距离为0，否则为1；实际上，我们检查两个实例之间的属性数量。汉明距离度量的公式如下：
- en: '![Hamming distance](img/B03980_06_25.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![汉明距离](img/B03980_06_25.jpg)'
- en: Different attributes are measured on different scales, and there is a need to
    normalize the attributes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的属性在不同的尺度上被测量，因此需要对属性进行归一化。
- en: Minkowski distance
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Minkowski距离
- en: We will now look at the *p*-norm distance measures family that is a generalization
    of the Euclidean distance measures. These measures are relatively quite flexible.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨*p*-范数距离度量族，它是欧几里得距离度量的推广。这些度量相对比较灵活。
- en: 'The Minkowski distance formula looks similar to Euclidean and is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Minkowski距离公式与欧几里得距离类似，如下所示：
- en: '![Minkowski distance](img/B03980_06_26.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![Minkowski距离](img/B03980_06_26.jpg)'
- en: If *p=0*, the distance measure is the Hamming measure.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *p=0*，距离度量是汉明度量。
- en: If *p=1*, the distance measure is the Manhattan measure.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *p=1*，距离度量是曼哈顿度量。
- en: If *p=2*, the distance measure is the Euclidean measure.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *p=2*，距离度量是欧几里得度量。
- en: '![Minkowski distance](img/B03980_06_10.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![Minkowski距离](img/B03980_06_10.jpg)'
- en: Case-based reasoning (CBR)
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于案例推理（CBR）
- en: CBR is an advanced instance-based learning method used with more complex instance
    objects. In addition to having a fixed database of past cases, CBR accumulates
    and stores the new data that is classified. Like all other instance-based learning
    methods, CBR matches new cases to find similar past cases. Semantic nets-based
    distance measures for matching the data is applied in this case. This is a diagrammatic
    matching method unlike other methods such as the Euclidean distance measure.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: CBR是一种高级基于实例的学习方法，用于更复杂的实例对象。除了拥有固定的过去案例数据库外，CBR还积累和存储了分类的新数据。像所有其他基于实例的学习方法一样，CBR通过匹配新案例来寻找相似的过去案例。在这种情况下，应用基于语义网的距离度量进行匹配数据。这是一种不同于欧几里得距离度量等方法的图示匹配方法。
- en: Similar to the other instance-based learning methods, CBR is a lazy learner,
    and the power comes from the organization and content of the cases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他基于实例的学习方法类似，CBR是一种懒惰学习器，其力量来自于案例的组织和内容。
- en: Reusing past cases is one of the key factors in the way human problem solving
    and reasoning works. Since CBR is modeled on human problem solving, it is more
    understandable to humans. This means the way CBR works can be altered by experts
    or with the consultation of experts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 重复使用过去的案例是人类解决问题和推理方式的关键因素之一。由于CBR是基于人类问题解决建模的，因此对人类来说更容易理解。这意味着CBR的工作方式可以通过专家或专家咨询来改变。
- en: By the virtue of its ability to handle very complex instances, CBR is often
    used in medical diagnosis for detecting heart diseases, hearing defects, and other
    relatively complex conditions. The following figure depicts a typical CBR learning
    flow and is famously called the R4 Model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其处理非常复杂实例的能力，CBR常用于医学诊断，用于检测心脏病、听力缺陷和其他相对复杂的情况。以下图展示了典型的CBR学习流程，并被称为R4模型。
- en: 'Lazy learning in Machine learning is all about delaying the process of generalization
    beyond the training data until the time of the query. The advantage is that we
    can now perform parallel processing while the downside is higher memory requirements.
    The following diagram presents a process flow for a CBR function:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的懒惰学习就是将泛化过程推迟到查询时间，超出训练数据。其优点是现在可以执行并行处理，但缺点是内存需求更高。以下图表展示了一个CBR函数的过程流程：
- en: '![Case-based reasoning (CBR)](img/B03980_06_11.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![基于案例推理（CBR）](img/B03980_06_11.jpg)'
- en: First, a New Case is received.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，接收到一个新案例。
- en: Then, a matching process is triggered where the received case is matched to
    the Case Base that has existing cases and already classified cases. This is the
    retrieval process.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，触发一个匹配过程，将接收到的案例与具有现有案例和已分类案例的案例库进行匹配。这是检索过程。
- en: Check if the Matched Cases perfectly fits the new case.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查匹配的案例是否完美地符合新案例。
- en: If yes, Reuse it, otherwise Revise it.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果是，则重复使用，如果不是，则修改。
- en: Output the final recommended solution.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出最终的推荐解决方案。
- en: At a later point in time, based on the facts, if the recommendation is in agreement,
    retain the learning and add to the case base. The learning phase may also add
    rules to the knowledge base that the eventual facts suggest.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以后的某个时间点，基于事实，如果推荐是一致的，保留学习并添加到案例库中。学习阶段也可能向知识库添加规则，这些规则最终的事实建议。
- en: Locally weighed regression (LWR)
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地加权回归（LWR）
- en: LWR is a particular case of linear regression where, due to noise, the dataset
    is no more linear, and linear regression underfits the training data. The problem
    of non-linearity is solved by assigning weights to the Nearest Neighbors. The
    assigned weights are usually bigger for data points that are closer to the data
    that needs a prediction.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LWR是线性回归的一个特例，由于噪声，数据集不再是线性的，线性回归对训练数据欠拟合。通过为最近邻分配权重来解决非线性问题。分配的权重通常对于需要预测的数据点更接近的数据点更大。
- en: Implementing KNN
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现KNN
- en: Refer to the source code provided for this chapter for implementing the k-Nearest
    Neighbor algorithm (source code path `.../chapter6/...` under each of the folders
    for the technology).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本章提供的源代码以实现k-Nearest Neighbor算法（在每个技术文件夹下的`.../chapter6/...`路径下）。
- en: Using Mahout
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Mahout
- en: Refer to the folder `.../mahout/chapter6/knnexample/`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../mahout/chapter6/knnexample/`。
- en: Using R
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用R
- en: Refer to the folder `.../r/chapter6/knnexample/`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../r/chapter6/knnexample/`。
- en: Using Spark
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark
- en: Refer to the folder `.../spark/chapter6/knnexample/.`
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../spark/chapter6/knnexample/`。
- en: Using Python (scikit-learn)
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Python（scikit-learn）
- en: Refer to the folder `.../python scikit learn/ chapter6/knnexample/`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../python scikit learn/ chapter6/knnexample/`。
- en: Using Julia
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Julia
- en: Refer to the folder `.../julia/chapter6/knnexample/`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅文件夹 `.../julia/chapter6/knnexample/`。
- en: Kernel methods-based learning
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于核方法的机器学习
- en: We have just seen what instance-based learning methods are, and we have taken
    a deep dive into the Nearest Neighbor algorithm and covered specific implementation
    aspects. In this section, we will look into kernels and the kernel-based Machine
    learning algorithms.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚了解了基于实例的学习方法，并深入探讨了最近邻算法及其特定的实现方面。在本节中，我们将探讨核函数以及基于核的机器学习算法。
- en: A kernel, in simple terms, is a similarity function that is fed into a Machine
    learning algorithm. It takes two inputs and suggests how similar they are. For
    example, if we are dawned with a task of classifying images, the input data is
    a key-value pair (image, label). So, in terms of the flow, the image data is taken,
    features are computed, and a vector of features are fed into the Machine learning
    algorithm. But, in the case of similarity functions, we can define a kernel function
    that internally computes the similarity between images, and feed this into the
    learning algorithm along with the images and label data. The outcome of this is
    a classifier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，核是一个相似性函数，它被输入到机器学习算法中。它接受两个输入并建议它们有多相似。例如，如果我们面临一个分类图像的任务，输入数据是一个键值对（图像，标签）。因此，在流程方面，图像数据被提取，计算特征，然后将特征向量输入到机器学习算法中。但是，在相似性函数的情况下，我们可以定义一个核函数，它内部计算图像之间的相似性，并将其与图像和标签数据一起输入到学习算法中。这样做的结果是得到一个分类器。
- en: The standard regression or SVM or Perceptron frameworks work with kernels and
    only use vectors. To address this requirement, we will have the Machine learning
    algorithms expressed as dot products so that kernel functions can be used.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的回归或SVM或感知器框架使用核函数，并且只使用向量。为了满足这一需求，我们将机器学习算法表示为点积，以便可以使用核函数。
- en: Kernels are preferable to feature vectors. There are many advantages; one of
    the key reasons being the ease of computing. Also, feature vectors need more storage
    space in comparison to dot products. It is possible to write Machine learning
    algorithms to use dot products and later map them to use kernels. This way, the
    usage of feature vectors can be completely avoided. This will support us in working
    with highly complex, efficient-to-compute, and yet high performing kernels effortlessly,
    without really developing multi-dimensional vectors.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数比特征向量更可取。有许多优点；其中一个关键原因在于计算的简便性。此外，与点积相比，特征向量需要更多的存储空间。可以编写机器学习算法来使用点积，并将其映射到使用核函数。这样，就可以完全避免使用特征向量。这将使我们能够轻松地处理高度复杂、计算效率高且性能优异的核函数，而无需真正开发多维向量。
- en: Kernel functions
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核函数
- en: 'Let''s understand what exactly kernel functions are; the following figure represents
    a 1D function using a simple 1-Dimensional example. Assume that given points are
    as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解核函数究竟是什么；以下图示通过一个简单的1维示例来表示1维函数。假设给定的点如下：
- en: '![Kernel functions](img/B03980_06_12.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![核函数](img/B03980_06_12.jpg)'
- en: A general 1-Dimensional hyperplane, as depicted previously, will be a vertical
    line and no other vertical lines will separate the dataset. If we look at the
    2-Dimensional representation, as shown next, there is a hyperplane (an arbitrary
    line in 2-Dimensions) that separates red and blue points, thus eligible for a
    separation using SVMs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的一般1维超平面将是一条垂直线，没有其他垂直线可以分离数据集。如果我们观察下一个2维表示，如所示，有一个超平面（2维空间中的任意线）将红色和蓝色点分开，因此可以使用SVM进行分离。
- en: '![Kernel functions](img/B03980_06_13.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![核函数](img/B03980_06_13.jpg)'
- en: With the growing dimensional space, the need to be able to separate data increases.
    This mapping, *x* -> (*x*, *x2*), is called the kernel function.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 随着维度空间的增长，需要能够分离数据的需求增加。这种映射，*x* -> (*x*, *x2*)，被称为核函数。
- en: In case of growing dimensional space, the computations become more complex and
    **kernel trick** needs to be applied to address these computations cheaply.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在维度空间不断增长的情况下，计算变得更加复杂，**核技巧**需要被应用以廉价地解决这些计算。
- en: Support Vector Machines (SVM)
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机 (SVM)
- en: SVMs are used in solving classification problems. Overall, as an approach, the
    goal is to find that hyperplane effectively divides the class representation of
    data. Hyperplane can be defined as a generalization of a line in 2-Dimensions
    and a plane in 3-Dimensions. Let's now take an example to understand how SVM works
    for linearly separable binary datasets. We will use the same example as we have
    in the Nearest Neighbor algorithms. The following diagram represents data with
    two features *X* and *Y* and available classes being triangles and stars.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机用于解决分类问题。总的来说，作为一个方法，目标是找到一个能够有效划分数据类别表示的超平面。超平面可以定义为二维空间中线的推广和三维空间中平面的推广。现在让我们举一个例子来了解SVM如何对线性可分二元数据集起作用。我们将使用与最近邻算法中相同的例子。以下图表表示具有两个特征*X*和*Y*的数据以及可用的类别为三角形和星星。
- en: '![Support Vector Machines (SVM)](img/B03980_06_14.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机 (SVM)](img/B03980_06_14.jpg)'
- en: The goal of SVM is to find the hyperplane that separates these two classes.
    The following diagram depicts some of the possible hyperplanes that can divide
    the datasets. The choice of the best hyperplane is defined by the extent to which
    a maximum margin is left for both classes. The margin is the distance between
    the hyperplane and the closest point in the classification.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的目标是找到能够分离这两个类别的超平面。以下图表描绘了一些可以划分数据集的可能超平面。最佳超平面的选择由为两个类别留下的最大边缘程度定义。边缘是超平面与分类中最接近的点之间的距离。
- en: '![Support Vector Machines (SVM)](img/B03980_06_15.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机 (SVM)](img/B03980_06_15.jpg)'
- en: Let's take two hyperplanes among others and check the margins represented by
    **M1** and **M2**. It is very clear that margin **M1** > **M2**, so the choice
    of the hyperplane that separates best is the new plane between the green and blue
    planes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们取两个超平面中的两个，并检查由**M1**和**M2**表示的边缘。很明显，边缘**M1** > **M2**，因此选择最佳分离的超平面是新平面，位于绿色和蓝色平面之间。
- en: '![Support Vector Machines (SVM)](img/B03980_06_16.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机 (SVM)](img/B03980_06_16.jpg)'
- en: 'The new plane can be represented by a linear equation as:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 新的平面可以用线性方程表示为：
- en: f(x) = ax + b
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = ax + b
- en: Let's assume that this equation delivers all values ≥ 1 from the triangle class
    and ≤ -1 for the star class. The distance of this plane from the closest points
    in both the classes is at least one; the modulus is one.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这个方程给出了三角形类别的所有≥1的值，对于星星类别，≤-1。这个平面与两个类别中最接近的点的距离至少为1；模长为1。
- en: '*f(x) ≥ 1* for triangles and *f(x) ≤ 1* or *|f(x)| = 1* for star'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三角形，*f(x) ≥ 1*，对于星星，*f(x) ≤ 1*或*|f(x)| = 1*。
- en: The distance between the hyperplane and the point can be computed using the
    following equation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下公式可以计算超平面与点之间的距离。
- en: M1 = |f(x)| / ||a|| = 1 / ||a||
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: M1 = |f(x)| / ||a|| = 1 / ||a||
- en: The total margin is *1 / ||a|| + 1 / ||a|| = 2 / ||a|*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 总边缘是*1 / ||a|| + 1 / ||a|| = 2 / ||a|*。
- en: To maximize the separability, which is the goal of SVM, we will need to maximize
    the *||a||* value. This value is referred to as a weight vector. This process
    of minimizing the *a* weight value is a non-linear optimization task. One method
    is to use the **Karush-Kuhn-Tucker** (**KKT**) condition, using the Lagrange multiplier
    *λ*[i].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化可分性，这是SVM的目标，我们需要最大化*||a||*值。这个值被称为权重向量。最小化*a*权重值的过程是一个非线性优化任务。一种方法是使用**Karush-Kuhn-Tucker**（**KKT**）条件，使用拉格朗日乘子*λ*[i]。
- en: '![Support Vector Machines (SVM)](img/B03980_06_17.jpg)![Support Vector Machines
    (SVM)](img/B03980_06_18.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机 (SVM)](img/B03980_06_17.jpg)![支持向量机 (SVM)](img/B03980_06_18.jpg)'
- en: Let's take an example of two points between the two attributes *X* and *Y*.
    We need to find a point between these two points that has a maximum distance between
    these points. This requirement is represented in the graph depicted next. The
    optimal point is depicted using the red circle.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们取两个属性*X*和*Y*之间的两个点的例子。我们需要找到一个点，这个点在这两个点之间具有最大的距离。这个要求在下面的图中表示。最优点用红色圆圈表示。
- en: '![Support Vector Machines (SVM)](img/B03980_06_19.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机 (SVM)](img/B03980_06_19.jpg)'
- en: The maximum margin weight vector is parallel to the line from *(1, 1)* to *(2,
    3)*. The weight vector is at *(1,2)*, and this becomes a decision boundary that
    is halfway between and in perpendicular, that passes through *(1.5, 2)*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最大边缘权重向量与从*(1, 1)*到*(2, 3)*的线平行。权重向量在*(1,2)*，这成为一条决策边界，位于中间，垂直于通过*(1.5, 2)*的线。
- en: So, *y = x1 +2x2 − 5.5* and the geometric margin is computed as *√5*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*y = x1 +2x2 − 5.5*，几何边缘计算为*√5*。
- en: 'Following are the steps to compute SVMs:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 计算SVM的步骤如下：
- en: 'With *w = (a, 2a)* for *a* the functions of the points (1,1) and (2,3) can
    be represented as shown here:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *w = (a, 2a)*，其中 *a* 是点 (1,1) 和 (2,3) 的函数，可以表示如下：
- en: a + 2a + ω[0] = -1 for the point (1,1)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于点 (1,1)，有 a + 2a + ω[0] = -1
- en: 2a + 6a + ω[0] = 1 for the point (2,3)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于点 (2,3)，有 2a + 6a + ω[0] = 1
- en: 'The weights can be computed as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 权重可以按以下方式计算：
- en: '![Support Vector Machines (SVM)](img/B03980_06_27.jpg)![Support Vector Machines
    (SVM)](img/B03980_06_28.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机 (SVM)](img/B03980_06_27.jpg)![支持向量机 (SVM)](img/B03980_06_28.jpg)'
- en: 'These are the support vectors:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是支持向量：
- en: '![Support Vector Machines (SVM)](img/B03980_06_29.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机 (SVM)](img/B03980_06_29.jpg)'
- en: 'Lastly, the final equation is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最终的方程如下：
- en: '![Support Vector Machines (SVM)](img/B03980_06_30.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机 (SVM)](img/B03980_06_30.jpg)'
- en: Inseparable Data
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不可分数据
- en: SVMs can probably help you to find out a separating hyperplane if it exists.
    There might be cases where there is no possibility to define a hyperplane, which
    can happen due to noise in the data. In fact, another reason can be a non-linear
    boundary as well. The following first graph depicts noise and the second one shows
    a non-linear boundary.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs 可能能帮助你找到存在的分离超平面。可能存在无法定义超平面的情况，这可能是由于数据中的噪声引起的。实际上，另一个原因可能是非线性边界。下面的第一个图表展示了噪声，第二个图表展示了非线性边界。
- en: '![Inseparable Data](img/B03980_06_20.jpg)![Inseparable Data](img/B03980_06_21.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![不可分数据](img/B03980_06_20.jpg)![不可分数据](img/B03980_06_21.jpg)'
- en: In the case of problems that arise due to noise in the data, the best way to
    look at it is to reduce the margin itself and introduce slack.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据噪声引起的问题的情况下，最好的方法是减少边际并引入松弛。
- en: '![Inseparable Data](img/B03980_06_22.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![不可分数据](img/B03980_06_22.jpg)'
- en: 'The non-linear boundary problem can be solved by introducing a kernel. Some
    of the kernel functions that can be introduced are depicted in the following diagram:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入核函数可以解决非线性边界问题。以下图表展示了可以引入的一些核函数：
- en: '![Inseparable Data](img/B03980_06_23.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![不可分数据](img/B03980_06_23.jpg)'
- en: Implementing SVM
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现SVM
- en: Refer to the source code provided for this chapter to implement the SVM algorithm
    (source code path `.../chapter6/...` under each of the folders for the technology).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本章提供的源代码以实现 SVM 算法（源代码路径为 `.../chapter6/...`，位于每个技术文件夹下）。
- en: Using Mahout
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Mahout
- en: Refer to the folder `.../mahout/chapter6/svmexample/`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../mahout/chapter6/svmexample/`。
- en: Using R
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 R
- en: Refer to the folder `.../r/chapter6/svmexample/`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../r/chapter6/svmexample/`。
- en: Using Spark
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Spark
- en: Refer to the folder `.../spark/chapter6/svmexample/`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../spark/chapter6/svmexample/`。
- en: Using Python (Scikit-learn)
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python (Scikit-learn)
- en: Refer to the folder `.../python-scikit-learn/chapter6/svmexample/`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../python-scikit-learn/chapter6/svmexample/`。
- en: Using Julia
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Julia
- en: Refer to the folder `.../julia/chapter6/svmexample/`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../julia/chapter6/svmexample/`。
- en: Summary
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have explored two learning algorithms, instance-based and
    kernel methods, and we have seen how they address the classification and prediction
    requirements. In the instance-based learning methods, we explored the Nearest
    Neighbor algorithm in detail and have seen how to implement this using our technology
    stack, Mahout, Spark, R, Julia, and Python. Similarly, in the kernel-based methods,
    we have explored SVM. In the next chapter, we will cover the Association Rule-based
    learning methods with a focus on Apriori and FP-growth algorithms.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了两种学习算法，基于实例和核方法，并看到了它们如何解决分类和预测需求。在基于实例的学习方法中，我们详细探讨了最近邻算法，并看到了如何使用我们的技术栈
    Mahout、Spark、R、Julia 和 Python 来实现它。同样，在基于核的方法中，我们探讨了 SVM。在下一章中，我们将介绍基于关联规则的学习方法，重点关注
    Apriori 和 FP-growth 算法。
