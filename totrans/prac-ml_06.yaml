- en: Chapter 6. Instance and Kernel Methods Based Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered Decision tree models for solving classification and regression
    problems in the previous chapter. In this chapter, we will cover two important
    models of supervised and unsupervised learning techniques which are the Nearest
    Neighbors method, which uses the instance-based learning model, and the **Support
    Vector Machines** (**SVM**) model, which uses kernel methods based learning model.
    For both methods, we will learn the basics of the technique and see how it can
    be implemented in Apache Mahout, R, Julia, Apache Spark, and Python. The following
    figure depicts different learning models covered in this book and the techniques
    highlighted will be dealt in covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Instance and Kernel Methods Based Learning](img/B03980_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following topics are covered in-depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Instance-based learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to instance-based learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lazy and eager learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief look at different algorithms/approaches of instance-based learning techniques
    Nearest Neighbor method, Case-based reasoning, Locally weighed regression, and
    Radial basis functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep dive into KNN (k-Nearest Neighbor) algorithm with a real-world use case
    example; mechanisms to speed up KNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample implementation of Apache Mahout, R, Apache Spark, Julia and Python (scikit-learn)
    libraries and modules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel-based learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to kernel-based learning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief look at different algorithms/approaches of Kernel-based learning techniques,
    Support Vector Machines (SVM), Linear Discriminate Analysis (LDA), and more
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep dive into SVM algorithm with a real-world use case example
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Instance-based learning (IBL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The IBL technique approaches learning by simply storing the provided training
    data and using it as a reference for predicting/determining the behavior of a
    new query. As learned in [Chapter 1](ch01.html "Chapter 1. Introduction to Machine
    learning"), *Introduction to Machine learning*, instances are nothing but subsets
    of datasets. The instance-based learning model works on an identified instance
    or groups of instances that are critical to the problem. The results across instances
    are compared and can include an instance of new data as well. This comparison
    uses a particular similarity measure to find the best match and predict. Since
    it uses historical data stored in memory, this learning technique is also called
    memory-based or case-based learning. Here, the focus is on the representation
    of the instances and similarity measures for comparison between them.
  prefs: []
  type: TYPE_NORMAL
- en: '![Instance-based learning (IBL)](img/B03980_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Every time a new query instance is received for processing, a set of similar,
    related instances are retrieved from memory, and then this data is used to classify
    the new query instance.
  prefs: []
  type: TYPE_NORMAL
- en: Instance-based learners are also called lazy learners. Overall, the entire database
    is used to predict behavior. A set of data points referred to as **neighbors**
    are identified, having a history of agreeing with the target attribute. Once a
    neighborhood of data points is formed, the preferences of neighbors are combined
    to produce a prediction or **top-K recommendation** for the active target attribute.
  prefs: []
  type: TYPE_NORMAL
- en: These methods are applicable for complex target functions that can be expressed
    using less complex local approximations. Unfortunately, with these methods, the
    cost of classifying a new instance is always high and in cases where there is
    a curse of dimensionality, these methods might end up with a bigger footprint
    as all the attributes of all the instances are considered. Classifiers and regressions
    are what we will cover in this and the next chapters that are to come. With classifiers,
    we try to predict a category and, with regression, we predict a real number. We
    will first look at the Nearest Neighbor algorithm that can be used both for classification
    and regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Rote Learner is one of the instance-based classifiers and focuses on memorizing
    the entire training data. Classification is primarily done only if the target
    attribute value exactly matches with the attribute value in the training example.
    The other classifier is the Nearest Neighbor, which classifies based on the closest
    neighbor(s). In the next section, let's dive deeply into the Nearest Neighbor
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Nearest Neighbors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we start understanding what the Nearest Neighbor algorithm is all about,
    let''s start with an example; the following graph shows the plotting of data points
    *X* and *Y* that have two classes: stars and triangles. Let''s not really worry
    about what is the exact data representation or the data points. If we had to solve
    intuitively the problem of finding what that particular red square box data point
    is, then the answer would obviously be a green triangle. This is an intuition
    and, without actually understanding or analyzing the data points, we can arrive
    at this conclusion. But what actually happened here is that we have seen the traits
    of the neighbors of the data point in context and have predicted the class to
    which the new data point could possibly belong to. Overall, the basis for the
    learning algorithm is actually the behavior of the nearby or neighboring points.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Nearest Neighbors](img/B03980_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbor is an algorithm that uses this basic technique of intuition.
    This algorithm finds the Nearest Neighbor using some distance measurement techniques
    that will be discussed in the following sections. Let's now extend this to another
    example data set; and again, the new data point with a question mark (?) will
    be needed for classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![Nearest Neighbors](img/B03980_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's now assume that the class the new data point belongs to is the yellow
    star. An important aspect of the distance measure is that the Nearest Neighbor
    is never just a single point but is usually a region. The following figure shows
    a region and all the data points that fall in this region belong to the class
    yellow star. This region is called the **Voronoi cell**. This region is usually
    a polygon with straight lines in case the distance measure used is the **Euclidean**
    distance measure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Nearest Neighbors](img/B03980_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For each training example if the Voronoi cells are computed, we can see the
    Voronoi tessellation as shown next. This tessellation represents the partition
    of space into the non-overlapping region and typically each region has one example.
  prefs: []
  type: TYPE_NORMAL
- en: '![Nearest Neighbors](img/B03980_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The size of the cell is determined by the number of examples available. The
    more the examples, the less the size of the regions. Another interesting aspect
    of the Voronoi tessellation is that there can be boundaries carved that form a
    separation for the classes, as shown in the following figure. The right side of
    the bold line belongs to the triangle class, and the left side belongs to the
    star class.
  prefs: []
  type: TYPE_NORMAL
- en: '![Nearest Neighbors](img/B03980_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One of the major complications with the Nearest Neighbor approach is its insensitivity
    to the outliers, thus really messing up the boundaries, and one way of solving
    this problem is to consider more than one neighbor, which this would make the
    model more stable and smooth. Hence, a consideration of k-Neighbors signifies
    the k-Nearest Neighbor algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![Nearest Neighbors](img/B03980_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now look at how the KNN classification algorithm works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the *{x[i], y[i]}* training examples, where *x*[i] represents attribute
    values, *y*[i] represents class labels, and there is a new test point *X* that
    needs to be classified, the following steps are performed in a KNN classification
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Distance is computed between *x* and *x*[i] for every given value of *x*[i].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose k nearest neighbors *xi1, … xik* and the respective class labels *yi1,
    … yik*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return a *y* that is the most frequent in the list of labels *yi1, … yik*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's now see how different the KNN regression algorithm is among the important
    differences. Instead of outputting a class, we will output real numbers like ratings
    or age, and so on. The algorithm is identical, but the only variation is in the
    return value, Step 3, and instead of a most frequent value, we take the mean of
    *y's*.
  prefs: []
  type: TYPE_NORMAL
- en: Value of k in KNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The value of *k* has a tremendous effect on the KNN performance. If the value
    of *k* is too large, the KNN algorithm uses the previous value and thus might
    result in inaccuracies. And in the case where the *k* value is too small, the
    model would become too sensitive to outliers as we saw in the previous section.
    Hence, an accurate *k* value usually lies midway through the smallest and largest
    value. The approach is to choose a value in this range and measure the error on
    the training data and pick a *k* that gives the best generalization performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts 1, 2, and 3 Nearest Neighbors for the point *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Value of k in KNN](img/B03980_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: k-Nearest Neighbors for the point *x* are all the data points that have the
    k smallest distance from *x*.
  prefs: []
  type: TYPE_NORMAL
- en: Distance measures in KNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is one of the attributes of the Nearest Neighbor algorithm and possibly
    the only area that one can experiment or try alternatives in. There are many distance
    measurement options and in this section, we will discuss some of the commonly
    used measures. The primary purpose of distance measure is to identify the examples
    that are similar or dissimilar. Similar to the k value, distance measure determines
    the performance of KNN.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Euclidean distance is the default option for numeric attributes. The distance
    measure formula is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Euclidean distance](img/B03980_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Euclidean distance measure is symmetrical and spherical and treats all the
    dimensions equally. One of the drawbacks of this measure is its sensitivity to
    the extreme values within a single attribute. This is similar to the mean squared
    error.
  prefs: []
  type: TYPE_NORMAL
- en: Hamming distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Hamming distance measure is a default option if we need to deal with categorical
    attributes. The primary function of a Hamming distance measure is to check whether
    the two attributes are equal or not. When they are equal, the distance is 0, otherwise
    it is 1; in effect, we check the number of attributes between two instances. The
    formula for the Hamming distance measure is as given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hamming distance](img/B03980_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Different attributes are measured on different scales, and there is a need to
    normalize the attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Minkowski distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will now look at the *p*-norm distance measures family that is a generalization
    of the Euclidean distance measures. These measures are relatively quite flexible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Minkowski distance formula looks similar to Euclidean and is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Minkowski distance](img/B03980_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If *p=0*, the distance measure is the Hamming measure.
  prefs: []
  type: TYPE_NORMAL
- en: If *p=1*, the distance measure is the Manhattan measure.
  prefs: []
  type: TYPE_NORMAL
- en: If *p=2*, the distance measure is the Euclidean measure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Minkowski distance](img/B03980_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Case-based reasoning (CBR)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CBR is an advanced instance-based learning method used with more complex instance
    objects. In addition to having a fixed database of past cases, CBR accumulates
    and stores the new data that is classified. Like all other instance-based learning
    methods, CBR matches new cases to find similar past cases. Semantic nets-based
    distance measures for matching the data is applied in this case. This is a diagrammatic
    matching method unlike other methods such as the Euclidean distance measure.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the other instance-based learning methods, CBR is a lazy learner,
    and the power comes from the organization and content of the cases.
  prefs: []
  type: TYPE_NORMAL
- en: Reusing past cases is one of the key factors in the way human problem solving
    and reasoning works. Since CBR is modeled on human problem solving, it is more
    understandable to humans. This means the way CBR works can be altered by experts
    or with the consultation of experts.
  prefs: []
  type: TYPE_NORMAL
- en: By the virtue of its ability to handle very complex instances, CBR is often
    used in medical diagnosis for detecting heart diseases, hearing defects, and other
    relatively complex conditions. The following figure depicts a typical CBR learning
    flow and is famously called the R4 Model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lazy learning in Machine learning is all about delaying the process of generalization
    beyond the training data until the time of the query. The advantage is that we
    can now perform parallel processing while the downside is higher memory requirements.
    The following diagram presents a process flow for a CBR function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Case-based reasoning (CBR)](img/B03980_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: First, a New Case is received.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, a matching process is triggered where the received case is matched to
    the Case Base that has existing cases and already classified cases. This is the
    retrieval process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check if the Matched Cases perfectly fits the new case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If yes, Reuse it, otherwise Revise it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output the final recommended solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At a later point in time, based on the facts, if the recommendation is in agreement,
    retain the learning and add to the case base. The learning phase may also add
    rules to the knowledge base that the eventual facts suggest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locally weighed regression (LWR)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LWR is a particular case of linear regression where, due to noise, the dataset
    is no more linear, and linear regression underfits the training data. The problem
    of non-linearity is solved by assigning weights to the Nearest Neighbors. The
    assigned weights are usually bigger for data points that are closer to the data
    that needs a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing KNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the source code provided for this chapter for implementing the k-Nearest
    Neighbor algorithm (source code path `.../chapter6/...` under each of the folders
    for the technology).
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter6/knnexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter6/knnexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter6/knnexample/.`
  prefs: []
  type: TYPE_NORMAL
- en: Using Python (scikit-learn)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../python scikit learn/ chapter6/knnexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter6/knnexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel methods-based learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have just seen what instance-based learning methods are, and we have taken
    a deep dive into the Nearest Neighbor algorithm and covered specific implementation
    aspects. In this section, we will look into kernels and the kernel-based Machine
    learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: A kernel, in simple terms, is a similarity function that is fed into a Machine
    learning algorithm. It takes two inputs and suggests how similar they are. For
    example, if we are dawned with a task of classifying images, the input data is
    a key-value pair (image, label). So, in terms of the flow, the image data is taken,
    features are computed, and a vector of features are fed into the Machine learning
    algorithm. But, in the case of similarity functions, we can define a kernel function
    that internally computes the similarity between images, and feed this into the
    learning algorithm along with the images and label data. The outcome of this is
    a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The standard regression or SVM or Perceptron frameworks work with kernels and
    only use vectors. To address this requirement, we will have the Machine learning
    algorithms expressed as dot products so that kernel functions can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Kernels are preferable to feature vectors. There are many advantages; one of
    the key reasons being the ease of computing. Also, feature vectors need more storage
    space in comparison to dot products. It is possible to write Machine learning
    algorithms to use dot products and later map them to use kernels. This way, the
    usage of feature vectors can be completely avoided. This will support us in working
    with highly complex, efficient-to-compute, and yet high performing kernels effortlessly,
    without really developing multi-dimensional vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s understand what exactly kernel functions are; the following figure represents
    a 1D function using a simple 1-Dimensional example. Assume that given points are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kernel functions](img/B03980_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A general 1-Dimensional hyperplane, as depicted previously, will be a vertical
    line and no other vertical lines will separate the dataset. If we look at the
    2-Dimensional representation, as shown next, there is a hyperplane (an arbitrary
    line in 2-Dimensions) that separates red and blue points, thus eligible for a
    separation using SVMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Kernel functions](img/B03980_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With the growing dimensional space, the need to be able to separate data increases.
    This mapping, *x* -> (*x*, *x2*), is called the kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: In case of growing dimensional space, the computations become more complex and
    **kernel trick** needs to be applied to address these computations cheaply.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines (SVM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SVMs are used in solving classification problems. Overall, as an approach, the
    goal is to find that hyperplane effectively divides the class representation of
    data. Hyperplane can be defined as a generalization of a line in 2-Dimensions
    and a plane in 3-Dimensions. Let's now take an example to understand how SVM works
    for linearly separable binary datasets. We will use the same example as we have
    in the Nearest Neighbor algorithms. The following diagram represents data with
    two features *X* and *Y* and available classes being triangles and stars.
  prefs: []
  type: TYPE_NORMAL
- en: '![Support Vector Machines (SVM)](img/B03980_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The goal of SVM is to find the hyperplane that separates these two classes.
    The following diagram depicts some of the possible hyperplanes that can divide
    the datasets. The choice of the best hyperplane is defined by the extent to which
    a maximum margin is left for both classes. The margin is the distance between
    the hyperplane and the closest point in the classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![Support Vector Machines (SVM)](img/B03980_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's take two hyperplanes among others and check the margins represented by
    **M1** and **M2**. It is very clear that margin **M1** > **M2**, so the choice
    of the hyperplane that separates best is the new plane between the green and blue
    planes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Support Vector Machines (SVM)](img/B03980_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The new plane can be represented by a linear equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = ax + b
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that this equation delivers all values ≥ 1 from the triangle class
    and ≤ -1 for the star class. The distance of this plane from the closest points
    in both the classes is at least one; the modulus is one.
  prefs: []
  type: TYPE_NORMAL
- en: '*f(x) ≥ 1* for triangles and *f(x) ≤ 1* or *|f(x)| = 1* for star'
  prefs: []
  type: TYPE_NORMAL
- en: The distance between the hyperplane and the point can be computed using the
    following equation.
  prefs: []
  type: TYPE_NORMAL
- en: M1 = |f(x)| / ||a|| = 1 / ||a||
  prefs: []
  type: TYPE_NORMAL
- en: The total margin is *1 / ||a|| + 1 / ||a|| = 2 / ||a|*.
  prefs: []
  type: TYPE_NORMAL
- en: To maximize the separability, which is the goal of SVM, we will need to maximize
    the *||a||* value. This value is referred to as a weight vector. This process
    of minimizing the *a* weight value is a non-linear optimization task. One method
    is to use the **Karush-Kuhn-Tucker** (**KKT**) condition, using the Lagrange multiplier
    *λ*[i].
  prefs: []
  type: TYPE_NORMAL
- en: '![Support Vector Machines (SVM)](img/B03980_06_17.jpg)![Support Vector Machines
    (SVM)](img/B03980_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's take an example of two points between the two attributes *X* and *Y*.
    We need to find a point between these two points that has a maximum distance between
    these points. This requirement is represented in the graph depicted next. The
    optimal point is depicted using the red circle.
  prefs: []
  type: TYPE_NORMAL
- en: '![Support Vector Machines (SVM)](img/B03980_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The maximum margin weight vector is parallel to the line from *(1, 1)* to *(2,
    3)*. The weight vector is at *(1,2)*, and this becomes a decision boundary that
    is halfway between and in perpendicular, that passes through *(1.5, 2)*.
  prefs: []
  type: TYPE_NORMAL
- en: So, *y = x1 +2x2 − 5.5* and the geometric margin is computed as *√5*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are the steps to compute SVMs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With *w = (a, 2a)* for *a* the functions of the points (1,1) and (2,3) can
    be represented as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: a + 2a + ω[0] = -1 for the point (1,1)
  prefs: []
  type: TYPE_NORMAL
- en: 2a + 6a + ω[0] = 1 for the point (2,3)
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support Vector Machines (SVM)](img/B03980_06_27.jpg)![Support Vector Machines
    (SVM)](img/B03980_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These are the support vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support Vector Machines (SVM)](img/B03980_06_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, the final equation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support Vector Machines (SVM)](img/B03980_06_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Inseparable Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SVMs can probably help you to find out a separating hyperplane if it exists.
    There might be cases where there is no possibility to define a hyperplane, which
    can happen due to noise in the data. In fact, another reason can be a non-linear
    boundary as well. The following first graph depicts noise and the second one shows
    a non-linear boundary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Inseparable Data](img/B03980_06_20.jpg)![Inseparable Data](img/B03980_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the case of problems that arise due to noise in the data, the best way to
    look at it is to reduce the margin itself and introduce slack.
  prefs: []
  type: TYPE_NORMAL
- en: '![Inseparable Data](img/B03980_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The non-linear boundary problem can be solved by introducing a kernel. Some
    of the kernel functions that can be introduced are depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inseparable Data](img/B03980_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Implementing SVM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the source code provided for this chapter to implement the SVM algorithm
    (source code path `.../chapter6/...` under each of the folders for the technology).
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter6/svmexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter6/svmexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter6/svmexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python (Scikit-learn)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../python-scikit-learn/chapter6/svmexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter6/svmexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored two learning algorithms, instance-based and
    kernel methods, and we have seen how they address the classification and prediction
    requirements. In the instance-based learning methods, we explored the Nearest
    Neighbor algorithm in detail and have seen how to implement this using our technology
    stack, Mahout, Spark, R, Julia, and Python. Similarly, in the kernel-based methods,
    we have explored SVM. In the next chapter, we will cover the Association Rule-based
    learning methods with a focus on Apriori and FP-growth algorithms.
  prefs: []
  type: TYPE_NORMAL
