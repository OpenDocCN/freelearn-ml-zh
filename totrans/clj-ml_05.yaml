- en: Chapter 5. Selecting and Evaluating Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we studied **Artificial Neural Networks** (**ANNs**)
    and how they can be used to effectively model nonlinear sample data. So far, we''ve
    discussed several machine learning techniques that can be used to model a given
    training set of data. In this chapter, we will explore the following topics that
    focus on how to select appropriate features from the sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: We will study methods to evaluate or quantify how accurately a formulated model
    fits the supplied training data. These techniques will be useful when we have
    to extend or debug an existing model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also explore how we can use the `clj-ml` library to perform this process
    on a given machine learning model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Towards the end of the chapter, we will implement a working spam classifier
    that incorporates a model evaluation technique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term **machine learning diagnostic** is often used to describe a test that
    can be run to gain insight about what is and isn't working in a machine learning
    model. This information generated by the diagnostic can then be used to improve
    the performance of the given model. Generally, when designing a machine learning
    model, it's advisable to formulate a diagnostic for the model in parallel. Implementing
    a diagnostic for a given model can take around the same time as formulating the
    model itself, but implementing a diagnostic is a good investment of time since
    it would help in quickly determining what needs to be changed in the model in
    order to improve it. Thus, machine learning diagnostics are helpful in saving
    time with respect to debugging or improving a formulated learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting aspect of machine learning is that without knowing the
    nature of the data we are trying to fit, we can make no assumption about which
    machine learning model we can use to fit the sample data. This axiom is known
    as the **No Free Lunch** theorem, and can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Without prior assumptions about the nature of a learning algorithm, no learning
    algorithm is superior or inferior to any other (or even random guessing)."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding underfitting and overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we've talked about minimizing the error or cost function
    of a formulated machine learning model. It's apt for the overall error of the
    estimated model to be low, but a low error is generally not enough to determine
    how well the model fits the supplied training data. In this section, we will revisit
    the concepts of *overfitting* and *underfitting*.
  prefs: []
  type: TYPE_NORMAL
- en: An estimated model is said to be **underfit** if it exhibits a large error in
    prediction. Ideally, we should strive to minimize this error in the model. However,
    a formulated model with a low error or cost function could also indicate that
    the model doesn't understand the underlying relationship between the given features
    of the model. Rather, the model is *memorizing* the supplied data, and this could
    even result in modeling random noise. In this case, the model is said to be **overfit**.
    A general symptom of an overfit model is failure to correctly predict the output
    variable from unseen data. An underfit model is also said to exhibit **high bias**
    and an overfit model is said to have **high variance**.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we are modeling a single dependent and independent variable in our model.
    Ideally, the model should fit the training data while generalizing on data that
    hasn't yet been observed in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variance of the dependent variables with the independent variable in an
    underfit model can be represented using the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding underfitting and overfitting](img/4351OS_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the red crosses represent data points in our sample
    data. As shown in the diagram, an underfit model will exhibit a large overall
    error, and we must try to reduce this error by appropriately selecting the features
    for our model and using regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, a model could also be overfit, in which the overall error
    in the model has a low value, but the estimated model fails to correctly predict
    the dependent variable from previously unseen data. An overfit model can be depicted
    using the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding underfitting and overfitting](img/4351OS_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, the estimated model plot closely but inappropriately
    fits the training data and thus has a low overall error. But, the model fails
    to respond correctly to new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model that describes a good fit for the sample data will have a low overall
    error and can predict the dependent variable correctly from previously unseen
    values for the independent variables in our model. An appropriately fit model
    should have a plot similar to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding underfitting and overfitting](img/4351OS_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ANNs can also be underfit or overfit on the provided sample data. For example,
    an ANN with a few hidden nodes and layers could be an underfit model, while an
    ANN with a large number of hidden nodes and layers could exhibit overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can plot the variance of the dependent and independent variables of a model
    to determine if the model is underfit or overfit. However, with a larger number
    of features, we need a better way to visualize how well the model generalizes
    the relationship of the dependent and independent variables of the model over
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: We can evaluate a trained machine learning model by determining the cost function
    of the model on some different data. Thus, we need to split the available sample
    data into two subsets—one for training the model and another for testing it. The
    latter subset is also called the **test set** of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost function is then calculated for the ![Evaluating a model](img/4351OS_05_06.jpg)
    samples in the test set. This gives us a measure of the overall error in the model
    when used on previously unseen data. This value is represented by the term ![Evaluating
    a model](img/4351OS_05_07.jpg) of the estimated model ![Evaluating a model](img/4351OS_05_08.jpg)
    and is also called the **test error** of the formulated model. The overall error
    in the training data is called the **training error** of the model and is represented
    by the term ![Evaluating a model](img/4351OS_05_09.jpg). A linear regression model''s
    test error can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating a model](img/4351OS_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the test error in a binary classification model can be formally
    expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating a model](img/4351OS_05_11.jpg)![Evaluating a model](img/4351OS_05_12.jpg)![Evaluating
    a model](img/4351OS_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The problem of determining the features of a model such that the test error
    is low is termed as **model selection** or **feature selection**. Also, to avoid
    overfitting, we must measure how well the model generalizes over the training
    data. The test error on its own is an optimistic estimate of the generalization
    error in the model over the training data. However, we must also measure the generalization
    error in data that hasn't yet been seen by the model. If the model has a low error
    over unseen data as well, we can be certain that the model does not overfit the
    data. This process is termed as **cross-validation**.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to ensure that the model can perform well on unseen data, we will require
    an additional set of data, called the **cross-validation set**. The number of
    samples in the cross-validation set is represented by the term ![Evaluating a
    model](img/4351OS_05_14.jpg). Typically, the sample data is partitioned into the
    training, test, and cross-validation sets such that the number of samples in the
    training data are significantly greater than those in the test and cross-validate
    sets. The error in generalization, or rather the cross-validation error ![Evaluating
    a model](img/4351OS_05_15.jpg), thus indicates how well the estimated model fits
    unseen data. Note that we don't modify the estimated model when we use the cross-validation
    and test sets on it. We will study more about cross-validation in the following
    sections of this chapter. As we will see later, we can also use cross-validation
    to determine the features of a model from some sample data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we have 100 samples in our training data. We partition
    this sample data into three sets. The first 60 samples will be used to estimate
    a model that fits the data appropriately. Out of the 40 remaining samples, 20
    will be used to cross-validate the estimated model, and the other 20 will be used
    to finally test the cross-validated model.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of classification, a good representation of the accuracy of a
    given classifier is a *confusion matrix*. This representation is often used to
    visualize the performance of a given classifier based on a supervised machine
    learning algorithm. Each column in this matrix represents the number of samples
    that belong to a particular class as predicted by the given classifier. The rows
    of the confusion matrix represent the actual classes of the samples. The confusion
    matrix is also called the **contingency matrix** or the **error matrix** of the
    trained classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say we have two classes in a given classification model. The confusion
    matrix of this model might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Predicted class |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| A | B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual class** | A | 45 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 30 | 10 |'
  prefs: []
  type: TYPE_TB
- en: In a confusion matrix, the predicted classes in our model are represented by
    vertical columns and the actual classes are represented by horizontal rows. In
    the preceding example of a confusion matrix, there are a total of 100 samples.
    Out of these, 45 samples from class A and 10 samples from class B were predicted
    to have the correct class. However, 15 samples of class A have been classified
    as class B and similarly 30 samples of class B have been predicted to have class
    A.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following confusion matrix of a different classifier that
    uses the same data as the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Predicted class |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| A | B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual class** | A | 45 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 0 | 50 |'
  prefs: []
  type: TYPE_TB
- en: In the preceding confusion matrix, the classifier classifies all samples of
    class B correctly. Also, only 5 samples of class A are classified incorrectly.
    Thus, this classifier better understands the distinction between the two classes
    of data when compared to the classifier used in the previous example. In practice,
    we must strive to train a classifier such that it has values close to *0* for
    all the elements other than the diagonal elements in its confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier, we need to determine an appropriate set of features
    from the sample data on which we must base our model. We can use cross-validation
    to determine which set of features to use from the training data, which can be
    explained as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each set or combination of feature variables, we determine the training
    and cross-validation error of a model based on the selected set of features. For
    example, we might want to add polynomial features derived from the independent
    variables of our model. We evaluate the training and cross-validation errors for
    each set of features depending on the highest degree of polynomial used to model
    the training data. We can plot the variance of these error functions over the
    degree of polynomial used, similar to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding feature selection](img/4351OS_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding diagram, we can determine which set of features produce an
    underfit or overfit estimated model. If a selected model has a high value for
    both the training and cross-validation errors, which is found towards the left
    of the plot, then the model is underfitting the supplied training data. On the
    other hand, a low training error and a high cross-validation error, as shown towards
    the right of the plot, indicates that the model is overfit. Ideally, we must select
    the set of features with the lowest possible values of the training and cross-validation
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: Varying the regularization parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To produce a better fit of the training data, we can use regularization to
    avoid the problem of overfitting our data. The value ![Varying the regularization
    parameter](img/4351OS_05_17.jpg) of a given model must be appropriately selected
    depending on the behavior of the model. Note that a high regularization parameter
    could result in a high training error, which is an undesirable effect. We can
    vary the regularization parameter in a formulated machine learning model to produce
    the following plot of the error values over the value of the regularization parameter
    in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Varying the regularization parameter](img/4351OS_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, as shown in the preceding plot, we can also minimize the training and
    cross-validation error in the model by changing the regularization parameter.
    If a model exhibits a high value for both these error values, we must consider
    reducing the value of the regularization parameter until both the error values
    are significantly low for the supplied sample data.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding learning curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another useful way to visualize the performance of a machine learning model
    is to use learning curves. A **learning curve** is essentially a plot of the error
    values in a model over the number of samples by which it is trained and cross-validated.
    For example, a model could have the following learning curve for the training
    and cross-validation errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding learning curves](img/4351OS_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Learning curves can be used to diagnose an underfit and overfit model. For
    example, the training error could be observed to increase quickly and converge
    towards a value close to the cross-validation with the number of samples provided
    to the model. Also, both the error values in our model have a significantly high
    value. A model that exhibits this kind of variance of error with the number of
    samples is underfit and has a learning curve similar to the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding learning curves](img/4351OS_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other hand, a model''s training error could be observed to increase
    slowly with the number of samples provided to the model, and there might also
    be a large difference between the training and cross-validation errors in the
    model. This model is said to be overfit and has a learning curve similar to the
    following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding learning curves](img/4351OS_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, learning curve is a good supplementary tool to cross-validation for determining
    what is not working and what needs to be changed in a given machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Improving a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have determined whether a model is underfit or overfit over the given
    sample data, we must decide on how to improve the model''s understanding of the
    relationship between the independent and dependent variables in our model. Let''s
    briefly discuss a few of these techniques, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Add or remove some features. As we will explore later, this technique can be
    used to improve both an underfit and an overfit model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vary the value of the regularization parameter ![Improving a model](img/4351OS_05_17.jpg).
    Like adding or removing features, this method can be applied to both underfit
    and overfit models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gather more training data. This method is a fairly obvious solution for improving
    an overfit model as it's needed to formulate a more generalized model to fit the
    training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add features which are polynomial terms of other features in the model. This
    method can be used to improve an underfit model. For example, if we are modeling
    two independent feature variables, ![Improving a model](img/4351OS_05_22.jpg)
    and ![Improving a model](img/4351OS_05_23.jpg), we could add the terms ![Improving
    a model](img/4351OS_05_24.jpg) as additional features to improve the model. The
    polynomial terms could be of even higher degrees, such as ![Improving a model](img/4351OS_05_25.jpg)
    and ![Improving a model](img/4351OS_05_26.jpg) , although this could result in
    overfitting the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we briefly mentioned earlier, cross-validation is a common validation technique
    that can be used to evaluate machine learning models. Cross-validation essentially
    measures how well the estimated model will generalize some given data. This data
    is different from the training data supplied to our model, and is called the **cross-validation
    set**, or simply **validation set**, of our model. Cross-validation of a given
    model is also called **rotation estimation**.
  prefs: []
  type: TYPE_NORMAL
- en: If an estimated model performs well during cross-validation, we can assume that
    the model can understand the relationship between its various independent and
    dependent variables. The goal of cross-validation is to provide a test to determine
    if a formulated model is overfit on the training data. In the perspective of implementation,
    cross-validation is a kind of unit test for a machine learning system.
  prefs: []
  type: TYPE_NORMAL
- en: A single round of cross-validation generally involves partitioning all the available
    sample data into two subsets and then performing training on one subset and validation
    and/or testing on the other subset. Several such rounds, or *folds*, of cross-validation
    must be performed using different sets of data to reduce the variance of the overall
    cross-validation error of the given model. Any particular measure of the cross-validation
    error should be calculated as the average of this error over the different folds
    in cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several types of cross-validation we can implement as a diagnostic
    for a given machine learning model or system. Let''s briefly explore a few of
    them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A common type is *k-fold* cross-validation, in which we partition the cross-validation
    data into *k* equal subsets. The training of the model is then performed on ![Using
    cross-validation](img/4351OS_05_27.jpg) subsets of the data and the cross-validation
    is performed on a single subset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple variation of *k-fold* cross-validation is *2-fold* cross-validation,
    which is also called the *holdout method*. In *2-fold* cross-validation, the training
    and cross-validation subsets of data will be almost equal in proportion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Repeated random subsampling** is another simple variant of cross-validation
    in which the sample data is first randomized or shuffled and then used as training
    and cross-validation data. This method is notably not dependent on the number
    of folds used for cross-validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another form of *k-fold* cross-validation is **leave-one-out** cross-validation,
    in which only a single record from the available sample data is used for cross-validation.
    Leave-one-out cross-validation is essentially *k-fold* cross-validation in which
    *k* is equal to the number of samples or observations in the sample data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cross-validation basically treats the estimated model as a black box, that
    is, it makes no assumptions about the implementation of the model. We can also
    use cross-validation to select features in a given model by using cross-validation
    to determine the feature set that produces the best fit model over the given sample
    data. Of course, there are a couple of limitations of classification, which can
    be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If a given model is needed to perform feature selection internally, we must
    perform cross-validation for each selected feature set in the given model. This
    can be computationally expensive depending on the amount of available sample data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation is not very useful if the sample data comprises exactly or
    nearly equal samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, it's a good practice to implement cross-validation for any machine
    learning system that we build. Also, we can choose an appropriate cross-validation
    technique depending on the problem we are trying to model as well as the nature
    of the collected sample data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `clj-ml` library to cross-validate the classifier we built for
    the fish packaging plant in [Chapter 3](ch03.html "Chapter 3. Categorizing Data"),
    *Categorizing Data*. Essentially, we built a classifier to determine whether a
    fish is a salmon or a sea bass using the `clj-ml` library. To recap, a fish is
    represented as a vector containing the category of the fish and values for the
    various features of the fish. The attributes of a fish are its length, width,
    and lightness of skin. We also described a template for a sample fish, which is
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fish-template` vector defined in the preceding code can be used to train
    a classifier with some sample data. For now, we will not bother about which classification
    algorithm we have used to model the given training data. We can only assume that
    the classifier was created using the `make-classifier` function from the `clj-ml`
    library. This classifier is stored in the `*classifier*` variable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose the classifier was trained with some sample data. We must now evaluate
    this trained classification model. To do this, we must first create some sample
    data to cross-validate. For the sake of simplicity, we will use randomly generated
    data in this example. We can generate this data using the `make-sample-fish` function,
    which we defined in [Chapter 3](ch03.html "Chapter 3. Categorizing Data"), *Categorizing
    Data*. This function simply creates a new vector of some random values representing
    a fish. Of course, we must not forget the fact that the `make-sample-fish` function
    has an in-built partiality, so we create a meaningful pattern in a number of samples
    created using this function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to use a dataset from the `clj-ml` library, and we can create
    one using the `make-dataset` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To cross-validate the classifier, we must use the `classifier-evaluate` function
    from the `clj-ml.classifiers` namespace. This function essentially performs *k-fold*
    cross-validation on the given data. Other than the classifier and the cross-validation
    dataset, this function requires the number of folds that we must perform on the
    data to be specified as the last parameter. Also, we will first need to set the
    class field of the records in `fish-cv-dataset` using the `dataset-set-class`
    function. We can define a single function to perform these operations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use 10 folds of cross-validation on the classifier. Since the `classifier-evaluate`
    function returns a map, we bind this return value to a variable for further use,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can fetch and print the summary of the preceding cross-validation using
    the `:summary` keyword as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, we can view several statistical measures of
    performance for our trained classifier. Apart from the correctly and incorrectly
    classified records, this summary also describes the **Root Mean Squared Error**
    (**RMSE**) and several other measures of error in our classifier. For a more detailed
    view of the correctly and incorrectly classified instances in the classifier,
    we can print the confusion matrix of the cross-validation using the `:confusion-matrix`
    keyword, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding example, we can use the `clj-ml` library's `classifier-evaluate`
    function to perform a *k-fold* cross-validation on any given classifier. Although
    we are restricted to using classifiers from the `clj-ml` library when using the
    `classifier-evaluate` function, we must strive to implement similar diagnostics
    in any machine learning system we build.
  prefs: []
  type: TYPE_NORMAL
- en: Building a spam classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are familiar with cross-validation, we will build a working machine
    learning system that incorporates cross-validation. The problem at hand will be
    that of **spam classification**, in which we will have to determine the likelihood
    of a given e-mail being a spam e-mail. Essentially, the problem boils down to
    binary classification with a few tweaks to make the machine learning system more
    sensitive to spam (for more information, refer to *A Plan for Spam*). Note that
    we will not be implementing a classification engine that is integrated with an
    e-mail server, but rather we will be concentrating on the aspects of training
    the engine with some data and classifying a given e-mail.
  prefs: []
  type: TYPE_NORMAL
- en: The way this would be used in practice can be briefly explained as follows.
    A user will receive and read a new e-mail, and will decide whether to mark the
    e-mail as spam or not. Depending on the user's decision, we must train the e-mail
    service's spam engine using the new e-mail as data.
  prefs: []
  type: TYPE_NORMAL
- en: In order to train our spam classifier in a more automated manner, we'll have
    to simply gather data to feed into the classifier. We will need a large amount
    of data to effectively train a classifier with the English language. Luckily for
    us, sample data for spam classification can be found easily on the Web. For this
    implementation, we will use data from the **Apache SpamAssassin** project.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Apache SpamAssassin project is an open source implementation of a spam classification
    engine in Perl. For our implementation, we will use the sample data from this
    project. You can download this data from [http://spamassassin.apache.org/publiccorpus/](http://spamassassin.apache.org/publiccorpus/).
    For our example, we have used the `spam_2` and `easy_ham_2` datasets. A Clojure
    Leiningen project housing our spam classifier implementation will require that
    these datasets be extracted and placed in the `ham/` and `spam/` subdirectories
    of the `corpus/` folder. The `corpus/` folder should be placed in the root directory
    of the Leiningen project that is the same folder of the `project.clj` file.
  prefs: []
  type: TYPE_NORMAL
- en: The features of our spam classifier will be the number of occurrences of all
    previously encountered words in spam and ham e-mails. By the term **ham**, we
    mean "not spam". Thus, there are effectively two independent variables in our
    model. Also, each word has an associated probability of occurrence in e-mails,
    which can be calculated from the number of times it's found in spam and ham e-mails
    and the total number of e-mails processed by the classifier. A new e-mail would
    be classified by finding all known words in the e-mail's header and body and then
    somehow combining the probabilities of occurrences of these words in spam and
    ham e-mails.
  prefs: []
  type: TYPE_NORMAL
- en: For a given word feature in our classifier, we must calculate the total probability
    of occurrence of the word by taking into account the total number of e-mails analyzed
    by the classifier (for more information, refer to *Better Bayesian Filtering*).
    Also, an unseen term is neutral in the sense that it is neither spam nor ham.
    Thus, the initial probability of occurrence of any word in the untrained classifier
    is 0.5\. Hence, we use a **Bayesian probability** function to model the occurrence
    of a particular word.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to classify a new e-mail, we also need to combine the probabilities
    of occurrences of all the known words found in it. For this implementation, we
    will use **Fisher''s method**, or **Fisher''s combined probability test**, to
    combine the calculated probabilities. Although the mathematical proof of this
    test is beyond the scope of this book, it''s important to know that this method
    essentially estimates the probabilities of several independent probabilities in
    a given model as a ![Building a spam classifier](img/4351OS_05_28.jpg) (pronounced
    as **chi-squared**) distribution (for more information, refer to *Statistical
    Methods for Research Workers*). Such a distribution has an associated number of
    degrees of freedom. It can be shown that an ![Building a spam classifier](img/4351OS_05_28.jpg)
    distribution with degrees of freedom equal to twice the number of combined probabilities
    *k* can be formally expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a spam classifier](img/4351OS_05_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This means that using an ![Building a spam classifier](img/4351OS_05_28.jpg)
    distribution with ![Building a spam classifier](img/4351OS_05_30.jpg) degrees
    of freedom, the **Cumulative Distribution Function** (**CDF**), of the probabilities
    of the e-mail being a spam or a ham can be combined to reflect a total probability
    that is high when there are a large number of probabilities with values close
    to 1.0\. Thus, an e-mail is classified as spam only when most of the words in
    the e-mail have been previously found in spam e-mails. Similarly, a large number
    of ham keywords would indicate the e-mail is in fact a ham e-mail. On the other
    hand, a low number of occurrences of spam keywords in an e-mail would have a probability
    closer to 0.5, in which case the classifier will be unsure of whether the e-mail
    is spam or ham.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the example that will follow, we will require the `file` and `cdf-chisq`
    functions from the `clojure.java.io` and `Incanter` libraries, respectively. The
    namespace declaration of the example should look similar to the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'A classifier trained using Fisher''s method, as described earlier, will be
    very sensitive to new spam e-mails. We represent the dependent variable of our
    model by the probability of a given e-mail being spam. This probability is also
    termed as the **spam score** of the e-mail. A low score indicates that an e-mail
    is ham, while a high score indicates that the e-mail is spam. Of course, we must
    also include a third class to represent an unknown value in our model. We can
    define some reasonable limits for the scores of these categories as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As defined earlier, if an e-mail has a score of 0.7 or more, it's a spam e-mail.
    And a score of 0.5 or less indicates that the e-mail is ham. Also, if the score
    lies between these two values, we can't effectively decide whether the e-mail
    is spam or not. We represent these three categories using the keywords `:ham`,
    `:spam`, and `:unsure`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The spam classifier must read several e-mails, determine all the words, or
    *tokens*, in the e-mails'' text and header, and store this information as empirical
    knowledge to use later. We need to store the number of occurrences a particular
    word is found in spam and ham e-mails. Thus, every word that the classifier has
    encountered represents a feature. To represent this information for a single word,
    we will use a record with three fields as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The record `TokenFeature` defined in the preceding code can be used to store
    the needed information for our spam classifier. The `new-token` function simply
    creates a new record for a given token by invoking the records, constructor. Obviously,
    a word is initially seen zero times in both spam and ham e-mails. We will also
    need to update these values, and we define the `inc-count` function to perform
    an update on the record using the `update-in` function. Note that the `update-in`
    function expects a function to apply to a particular field in the record as the
    last parameter. We are already dealing with a small amount of a mutable state
    in our implementation, so let''s delegate access to this state through an agent.
    We would also like to keep track of the total number of ham and spam e-mails;
    so, we''ll wrap these values with agents as well, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `feature-db` agent defined in the preceding code will be used to store
    all word features. We define a simple error handler for this agent using the `:error-handler`
    keyword parameter. The agent''s `total-ham` and `total-spam` functions will keep
    track of the total number of ham and spam e-mails, respectively. We will now define
    a couple of functions to access these agents as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In case you are not familiar with agents in Clojure, we can use the `send` function
    to alter the value contained in an agent. This function expects a single argument,
    that is, the function to apply to its encapsulated value. The agent applies this
    function on its contained value and updates it if there are no errors. The `clear-db`
    function simply initializes all the agents we've defined with an initial value.
    This is done by using the `constantly` function that wraps a value in a function
    that returns the same value. The `update-feature!` function modifies the value
    of a given token in the `feature-db` map and creates a new token if the supplied
    token is not present in the map of `feature-db`. Since we will only be incrementing
    the number of occurrences of a given token, we will pass the `inc-count` function
    as a parameter to the `update-feature!` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s define how the classifier will extract words from a given e-mail.
    We''ll use regular expressions to do this. If we want to extract all the words
    from a given string, we can use the regular expression `[a-zA-Z]{3,}`. We can
    define this regular expression using a literal syntax in Clojure, as shown in
    the following code. Note that we could also use the `re-pattern` function to create
    a regular expression. We will also define all the MIME header fields from which
    we should also extract tokens. We will do all this with the help of the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To match tokens with the regular expression defined by `token-regex`, we will
    use the `re-seq` function, which returns all matching tokens in a given string
    as a sequence of strings. For the MIME headers of an e-mail, we need to use a
    different regular expression to extract tokens. For example, we can extract tokens
    from the `"From"` MIME header as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note the use of the newline character at the end of the regular expression,
    which is used to indicate the end of a MIME header in an e-mail.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then proceed to extract words from the values returned by matching the
    regular expression defined in the preceding code. Let''s define the following
    few functions to extract tokens from a given e-mail''s headers and body using
    this logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `header-token-regex` function defined in the preceding code returns a regular
    expression for a given header, such as `From:(.*)\n` for the `"From"` header.
    The `extract-tokens-from-headers` function uses this regular expression to determine
    all words in the various header fields of an e-mail and appends the header name
    to all the tokens found in the header text. The `extract-tokens` function applies
    the regular expression over the text and headers of an e-mail and then flattens
    the resulting lists into a single list using the `apply` and `concat` functions.
    Note that the `extract-tokens-from-headers` function returns empty lists for the
    headers defined in `header-fields`, which are not present in the supplied e-mail
    header. Let''s try this function out in the REPL with the help of the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `extract-tokens-from-headers` function and the regular expression
    defined by `token-regex`, we can extract all words comprising of three or more
    characters from an e-mail''s header and text. Now, let''s define a function to
    apply the `extract-tokens` function on a given e-mail and update the feature map
    using the `update-feature!` function with all the words found in the e-mail. We
    will do all this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `update-features!` function in the preceding code, we can train our
    spam classifier with a given e-mail. In order to keep track of the total number
    of spam and ham e-mails, we will have to send the `inc` function to the `total-spam`
    or `total-ham` agents depending on whether a given e-mail is spam or ham. We will
    do this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `inc-total-count!` function defined in the preceding code updates the total
    number of spam and ham e-mails in our feature database. The `train!` function
    simply calls the `update-features!` and `inc-total-count!` functions to train
    our spam classifier with a given e-mail and its type. Note that we pass the `inc-count`
    function to the `update-features!` function. Now, in order to classify a new e-mail
    as spam or ham, we must first define how to extract the known features from a
    given e-mail using our trained feature database. We will do this with the help
    of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `extract-features` function defined in the preceding code looks up all known
    features in a given e-mail by dereferencing the map stored in `feature-db` and
    applying it as a function to all the values returned by the `extract-tokens` function.
    As mapping the closure `#(@feature-db %1)` can return `()` or `nil` for all tokens
    that are not present in a `feature-db` agent, we will need to remove all empty
    values from the list of extracted features. To do this, we will use the `keep`
    function, which expects a function to apply to the non-nil values in a collection
    and the collection from which all nil values must be filtered out. Since we do
    not intend to transform the known features from the e-mail, we will pass the `identity`
    function, which returns its argument itself as the first parameter to the `keep`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have extracted all known features from a given e-mail, we must
    calculate all the probabilities of these features occurring in a spam e-mail.
    We must then combine these probabilities using Fisher''s method we described earlier
    to determine the spam score of a new e-mail. Let''s define the following functions
    to implement the Bayesian probability and Fisher''s method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `spam-probability` function defined in the preceding code calculates the
    probability of occurrence of a given word feature in a spam e-mail using the number
    of occurrences of the word in spam and ham e-mails and the total number of spam
    and ham e-mails processed by the classifier. To avoid division-by-zero errors,
    we ensure that the value of the number of spam and ham e-mails is at least 1 before
    performing division. The `bayesian-spam-probability` function uses this probability
    returned by the `spam-probability` function to calculate a weighted average with
    the initial probability of 0.5 or *1/2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now implement Fisher''s method of combining the probabilities returned
    by the `bayesian-spam-probability` function for all the known features found in
    an e-mail. We will do this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fisher` function defined in the preceding code uses the `cdf-chisq` function
    from the `Incanter` library to calculate the CDF of the several probabilities
    transformed by the expression ![Building a spam classifier](img/4351OS_05_31.jpg).
    We specify the number of degrees of freedom to this function using the `:df` optional
    parameter. We now need to apply the `fisher` function to the combined Bayesian
    probabilities of an e-mail being spam or ham, and combine these values into a
    final spam score. These two probabilities must be combined such that only a high
    number of occurrences of high probabilities indicate a strong probability of spam
    or ham. It has been shown that the simplest way to do this is to average the probability
    of a spam e-mail and the negative probability of a ham e-mail (or 1 minus the
    probability of a ham e-mail). We will do this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Hence, the `score` function will return the final spam score of a given e-mail.
    Let''s define a function to extract the known word features from a given e-mail,
    combine the probabilities of occurrences of these features to produce the e-mail''s
    spam score, and finally classify this spam score as a ham or spam e-mail, represented
    by the keywords `:ham` and `:spam` respectively, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, we have implemented how we train our spam classifier and use it to
    classify a new e-mail. Now, let''s define some functions to load the sample data
    from the project''s `corpus/` folder and use this data to train and cross-validate
    our classifier, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `populate-emails` function defined in the preceding code returns a sequence
    of vectors to represent all the ham e-mails from the `ham/` folder and the spam
    e-mails from the `spam/` folder in our sample data. Each vector in this returned
    sequence has two elements. The first element in this vector is a given e-mail's
    relative file path and the second element is either `:spam` or `:ham` depending
    on whether the e-mail is spam or ham. Note the use of the `file-seq` function
    to read the files in a directory as a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use the `train!` function to feed the content of all e-mails into
    our spam classifier. To do this, we can use the `slurp` function to read the content
    of a file as a string. For cross-validation, we will classify each e-mail in the
    supplied cross-validation data using the `classify` function and return a list
    of maps representing the test result of the cross-validation. We will do this
    with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train-from-corpus!` function defined in the preceding code will train
    our spam classifier with all e-mails found in the `corpus/` folder. The `cv-from-corpus`
    function classifies the supplied e-mails as spam or ham using the trained classifier
    and returns a sequence of maps indicating the results of the cross-validation
    process. Each map in the sequence returned by the `cv-from-corpus` function contains
    the file of the e-mail, the actual type (spam or ham) of the e-mail, the predicted
    type of the e-mail, and the spam score of the e-mail. Now, we need to call these
    two functions on two appropriately partitioned subsets of the sample data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `test-classifier!` function defined in the preceding code will randomly
    shuffle the sample data and select a specified fraction of this randomized data
    as the cross-validation set for our classifier. The `test-classifier!` function
    then calls the `train-from-corpus!` and `cv-from-corpus` functions to train and
    cross-validate the data. Note that the use of the `await` function is to wait
    until the `feature-db` agent has finished applying all functions that have been
    sent to it via the `send` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to analyze the results of cross-validation. We must first determine
    the number of incorrectly classified and missed e-mails from the actual and expected
    class of a given e-mail as returned by the `cv-from-corpus` function. We will
    do this with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `result-type` function will determine the number of incorrectly classified
    and missed e-mails in the cross-validation process. We can now apply the `result-type`
    function to all the maps in the results returned by the `cv-from-corpus` function
    and print a summary of the cross-validation results with the help of the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `analyze-results` function defined in the preceding code simply applies
    the `result-type` function to all the map values in the sequence returned by the
    `cv-from-corpus` function, while maintaining the total number of incorrectly classified
    and missed e-mails. The `print-result` function simply prints the analyzed result
    as a string. Finally, let''s define a function to load all the e-mails using the
    `populate-emails` function and then use this data to train and cross-validate
    our spam classifier. Since the `populate-emails` function will return an empty
    list, or `nil` when there are no e-mails, we will check this condition to avoid
    failing at a later stage in our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the `train-and-cv-classifier` function shown in the preceding code, we first
    call the `populate-emails` function and convert the result to a sequence using
    the `seq` function. If the sequence has any elements, we train and cross-validate
    the classifier. If there are no e-mails found, we simply throw an error. Note
    that the `if-let` function is used to check whether the sequence returned by the
    `seq` function has any elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have all the parts needed to create and train a spam classifier. Initially,
    as the classifier hasn''t seen any e-mails, the probability of any e-mail or text
    being spam is 0.5\. This can be verified by using the `classify` function, as
    shown in the following code, which initially classifies any text as the `:unsure`
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We now train the classifier and cross-validate it using the `train-and-cv-classifier`
    function. We will use one-fifth of all the available sample data as our cross-validation
    set. This is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Cross-validating our spam classifier asserts that it''s appropriately classifying
    e-mails. Of course, there is still a small amount of error, which can be corrected
    by using more training data. Now, let''s try to classify some text using our trained
    spam classifier, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Interestingly, the text `"Make money fast"` is classified as spam and the text
    `"Job interview … GNU project"` is classified as ham, as shown in the preceding
    code. Let''s have a look at how the trained classifier extracts features from
    some text using the `extract-features` function. Since the classifier will initially
    have read no tokens, this function will obviously return an empty list or `nil`
    when the classifier is untrained, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, each `TokenFeature` record will contain the
    number of times a given word is seen in spam and ham e-mails. Also, the word `"to"`
    is not recognized as a feature since we only consider words comprising of three
    or more characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s check how sensitive to spam e-mail our spam classifier actually
    is. We''ll first have to select some text or a particular term that is classified
    as neither spam nor ham. For the training data selected for this example, the
    word `"Job"` fits this requirement, as shown in the following code. Let''s train
    the classifier with the word `"Job"` while specifying the type of the text as
    ham. We can do this using the `train!` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After training the classifier with the given text as ham, the probability of
    the term being spam is observed to decrease by a small amount. If the term `"Job"`
    occurred in several more e-mails that were ham, the classifier would eventually
    classify this word as ham. Thus, the classifier doesn''t show much of a reaction
    to a new ham e-mail. On the contrary, the classifier is observed to be very sensitive
    to spam e-mails, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: An occurrence of a particular word in a single spam e-mail is observed to greatly
    increase a classifier's predicted probability of the given term belonging to a
    spam e-mail. The term `"Job"` will subsequently be classified as spam by our classifier,
    at least until it's seen to appear in a sufficiently large number of ham e-mails.
    This is due to the nature of the chi-squared distribution that we are modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also improve the overall error of our spam classifier by supplying it
    with more training data. To demonstrate this, let''s cross-validate the classifier
    with only one-tenth of the sample data. Thus, the classifier would be effectively
    trained with nine-tenths of the available data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the number of misses and wrongly classified
    e-mails is observed to reduce when we use more training data. Of course, this
    is only shown as an example, and we should instead collect more e-mails to feed
    into the classifier as training data. Using a significant amount of the sample
    data for cross-validation is a good practice.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we have effectively built a spam classifier that is trained using
    Fisher's method. We have also implemented a cross-validation diagnostic, which
    serves as a kind of unit test for our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the exact values produced by the `train-and-cv-classifier` function
    will vary depending on the spam and ham emails used as training data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have explored techniques that can be used to diagnose and
    improve a given machine learning model. The following are some of the other points
    that we have covered:'
  prefs: []
  type: TYPE_NORMAL
- en: We have revisited the problems of underfitting and overfitting of sample data
    and also discussed how we can evaluate a formulated model to diagnose whether
    it's underfit or overfit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have explored cross-validation and how it can be used to determine how well
    a formulated model will respond to previously unseen data. We have also seen that
    we can use cross-validation to select the features and the regularization parameter
    of a model. We also studied a few kinds of cross-validation that we can implement
    for a given model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We briefly explored learning curves and how they can be used to diagnose the
    underfit and overfit models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've explored the tools provided by the `clj-ml` library to cross-validate
    a given classifier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we've built an operational spam classifier that incorporates cross-validation
    to determine whether the classifier is appropriately classifying e-mails as spam.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following chapters, we will continue exploring more machine learning
    models, and we'll also study **Support Vector Machines** (**SVMs**) in detail.
  prefs: []
  type: TYPE_NORMAL
