- en: Chapter 5. Selecting and Evaluating Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章. 选择和评估数据
- en: 'In the previous chapter, we studied **Artificial Neural Networks** (**ANNs**)
    and how they can be used to effectively model nonlinear sample data. So far, we''ve
    discussed several machine learning techniques that can be used to model a given
    training set of data. In this chapter, we will explore the following topics that
    focus on how to select appropriate features from the sample data:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了**人工神经网络**（**ANNs**）以及它们如何有效地对非线性样本数据进行建模。到目前为止，我们已经讨论了几种可以用来对给定的训练数据集进行建模的机器学习技术。在本章中，我们将探讨以下主题，重点关注如何从样本数据中选择合适的特征：
- en: We will study methods to evaluate or quantify how accurately a formulated model
    fits the supplied training data. These techniques will be useful when we have
    to extend or debug an existing model.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将研究评估或量化制定模型与提供的训练数据拟合准确性的方法。当我们需要扩展或调试现有模型时，这些技术将非常有用。
- en: We will also explore how we can use the `clj-ml` library to perform this process
    on a given machine learning model.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将探索如何使用`clj-ml`库在给定的机器学习模型上执行此过程。
- en: Towards the end of the chapter, we will implement a working spam classifier
    that incorporates a model evaluation technique.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章的末尾，我们将实现一个包含模型评估技术的有效垃圾邮件分类器。
- en: The term **machine learning diagnostic** is often used to describe a test that
    can be run to gain insight about what is and isn't working in a machine learning
    model. This information generated by the diagnostic can then be used to improve
    the performance of the given model. Generally, when designing a machine learning
    model, it's advisable to formulate a diagnostic for the model in parallel. Implementing
    a diagnostic for a given model can take around the same time as formulating the
    model itself, but implementing a diagnostic is a good investment of time since
    it would help in quickly determining what needs to be changed in the model in
    order to improve it. Thus, machine learning diagnostics are helpful in saving
    time with respect to debugging or improving a formulated learning model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: “**机器学习诊断**”这个术语通常用来描述一种可以运行的测试，以获得关于机器学习模型中哪些工作得好，哪些工作得不好的洞察。诊断生成的信息可以用来提高给定模型的表现。一般来说，在设计机器学习模型时，建议并行地为模型制定一个诊断。为给定模型实现诊断可能需要与制定模型本身相同的时间，但实现诊断是值得投入时间的，因为它有助于快速确定模型中需要改变什么才能改进它。因此，机器学习诊断有助于在调试或改进制定的学习模型方面节省时间。
- en: 'Another interesting aspect of machine learning is that without knowing the
    nature of the data we are trying to fit, we can make no assumption about which
    machine learning model we can use to fit the sample data. This axiom is known
    as the **No Free Lunch** theorem, and can be summarized as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的另一个有趣方面是，如果我们不知道我们试图拟合的数据的性质，我们就无法对可以使用哪种机器学习模型来拟合样本数据做出任何假设。这个公理被称为**没有免费午餐**定理，可以总结如下：
- en: '"Without prior assumptions about the nature of a learning algorithm, no learning
    algorithm is superior or inferior to any other (or even random guessing)."'
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “如果没有关于学习算法性质的前置假设，没有任何学习算法比其他任何（甚至随机猜测）更优越或更劣。”
- en: Understanding underfitting and overfitting
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解欠拟合和过拟合
- en: In the previous chapters, we've talked about minimizing the error or cost function
    of a formulated machine learning model. It's apt for the overall error of the
    estimated model to be low, but a low error is generally not enough to determine
    how well the model fits the supplied training data. In this section, we will revisit
    the concepts of *overfitting* and *underfitting*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们讨论了最小化一个机器学习模型的误差或损失函数。估计模型的总体误差低是合适的，但低误差通常不足以确定模型与提供的训练数据拟合得有多好。在本节中，我们将重新审视**过拟合**和**欠拟合**的概念。
- en: An estimated model is said to be **underfit** if it exhibits a large error in
    prediction. Ideally, we should strive to minimize this error in the model. However,
    a formulated model with a low error or cost function could also indicate that
    the model doesn't understand the underlying relationship between the given features
    of the model. Rather, the model is *memorizing* the supplied data, and this could
    even result in modeling random noise. In this case, the model is said to be **overfit**.
    A general symptom of an overfit model is failure to correctly predict the output
    variable from unseen data. An underfit model is also said to exhibit **high bias**
    and an overfit model is said to have **high variance**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果估计模型在预测中表现出较大的误差，则称其为**欠拟合**。理想情况下，我们应该努力最小化模型中的这个误差。然而，具有低误差或成本函数的公式化模型也可能表明模型不理解模型给定特征之间的潜在关系。相反，模型是*记忆*提供的数据，这甚至可能导致对随机噪声的建模。在这种情况下，该模型被称为**过拟合**。过拟合模型的一般症状是未能从未见过的数据中正确预测输出变量。欠拟合模型也被称为表现出**高偏差**，而过拟合模型则被认为具有**高方差**。
- en: Suppose we are modeling a single dependent and independent variable in our model.
    Ideally, the model should fit the training data while generalizing on data that
    hasn't yet been observed in the training data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在模型中建模一个单一的自变量和因变量。理想情况下，模型应该拟合训练数据，同时在尚未观察到的训练数据上泛化。
- en: 'The variance of the dependent variables with the independent variable in an
    underfit model can be represented using the following plot:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在欠拟合模型中，可以使用以下图表表示因变量与自变量之间的方差：
- en: '![Understanding underfitting and overfitting](img/4351OS_05_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![理解欠拟合和过拟合](img/4351OS_05_01.jpg)'
- en: In the preceding diagram, the red crosses represent data points in our sample
    data. As shown in the diagram, an underfit model will exhibit a large overall
    error, and we must try to reduce this error by appropriately selecting the features
    for our model and using regularization.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前图中，红色交叉表示样本数据中的数据点。如图所示，欠拟合模型将表现出较大的总体误差，我们必须通过适当选择模型的特征和使用正则化来尝试减少这个误差。
- en: 'On the other hand, a model could also be overfit, in which the overall error
    in the model has a low value, but the estimated model fails to correctly predict
    the dependent variable from previously unseen data. An overfit model can be depicted
    using the following plot:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，模型也可能过拟合，在这种情况下，模型的总体误差值很低，但估计的模型无法从先前未见过的数据中正确预测因变量。可以使用以下图表来表示过拟合模型：
- en: '![Understanding underfitting and overfitting](img/4351OS_05_03.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![理解欠拟合和过拟合](img/4351OS_05_03.jpg)'
- en: As shown in the preceding diagram, the estimated model plot closely but inappropriately
    fits the training data and thus has a low overall error. But, the model fails
    to respond correctly to new data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，估计的模型图紧密但不适当地拟合了训练数据，因此总体误差较低。但是，模型无法对新数据做出正确响应。
- en: 'The model that describes a good fit for the sample data will have a low overall
    error and can predict the dependent variable correctly from previously unseen
    values for the independent variables in our model. An appropriately fit model
    should have a plot similar to the following diagram:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 描述样本数据的良好拟合模型的总体误差将很低，并且可以从模型中独立变量的先前未见过的值正确预测因变量。一个适当拟合的模型应该有一个类似于以下图表的图形：
- en: '![Understanding underfitting and overfitting](img/4351OS_05_05.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![理解欠拟合和过拟合](img/4351OS_05_05.jpg)'
- en: ANNs can also be underfit or overfit on the provided sample data. For example,
    an ANN with a few hidden nodes and layers could be an underfit model, while an
    ANN with a large number of hidden nodes and layers could exhibit overfitting.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络也可能在提供的样本数据上欠拟合或过拟合。例如，具有少量隐藏节点和层的神经网络可能是一个欠拟合模型，而具有大量隐藏节点和层的神经网络可能表现出过拟合。
- en: Evaluating a model
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: We can plot the variance of the dependent and independent variables of a model
    to determine if the model is underfit or overfit. However, with a larger number
    of features, we need a better way to visualize how well the model generalizes
    the relationship of the dependent and independent variables of the model over
    the training data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制模型的因变量和自变量的方差图，以确定模型是欠拟合还是过拟合。然而，随着特征数量的增加，我们需要更好的方法来可视化模型在训练数据上对模型因变量和自变量关系的泛化程度。
- en: We can evaluate a trained machine learning model by determining the cost function
    of the model on some different data. Thus, we need to split the available sample
    data into two subsets—one for training the model and another for testing it. The
    latter subset is also called the **test set** of our model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过确定模型在某些不同数据上的成本函数来评估训练好的机器学习模型。因此，我们需要将可用的样本数据分成两个子集——一个用于训练模型，另一个用于测试模型。后者也被称为我们模型的**测试集**。
- en: 'The cost function is then calculated for the ![Evaluating a model](img/4351OS_05_06.jpg)
    samples in the test set. This gives us a measure of the overall error in the model
    when used on previously unseen data. This value is represented by the term ![Evaluating
    a model](img/4351OS_05_07.jpg) of the estimated model ![Evaluating a model](img/4351OS_05_08.jpg)
    and is also called the **test error** of the formulated model. The overall error
    in the training data is called the **training error** of the model and is represented
    by the term ![Evaluating a model](img/4351OS_05_09.jpg). A linear regression model''s
    test error can be calculated as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算测试集中![评估模型](img/4351OS_05_06.jpg)样本的成本函数。这为我们提供了一个度量，即当模型用于之前未见过的数据时，模型的整体误差。这个值由估计模型![评估模型](img/4351OS_05_08.jpg)的术语![评估模型](img/4351OS_05_07.jpg)表示，也被称为该公式的**测试误差**。训练数据中的整体误差被称为模型的**训练误差**，由术语![评估模型](img/4351OS_05_09.jpg)表示。线性回归模型的测试误差可以按以下方式计算：
- en: '![Evaluating a model](img/4351OS_05_10.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型](img/4351OS_05_10.jpg)'
- en: 'Similarly, the test error in a binary classification model can be formally
    expressed as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，二元分类模型中的测试误差可以正式表示如下：
- en: '![Evaluating a model](img/4351OS_05_11.jpg)![Evaluating a model](img/4351OS_05_12.jpg)![Evaluating
    a model](img/4351OS_05_13.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型](img/4351OS_05_11.jpg)![评估模型](img/4351OS_05_12.jpg)![评估模型](img/4351OS_05_13.jpg)'
- en: The problem of determining the features of a model such that the test error
    is low is termed as **model selection** or **feature selection**. Also, to avoid
    overfitting, we must measure how well the model generalizes over the training
    data. The test error on its own is an optimistic estimate of the generalization
    error in the model over the training data. However, we must also measure the generalization
    error in data that hasn't yet been seen by the model. If the model has a low error
    over unseen data as well, we can be certain that the model does not overfit the
    data. This process is termed as **cross-validation**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 确定模型特征以使测试误差低的问题被称为**模型选择**或**特征选择**。此外，为了避免过拟合，我们必须测量模型在训练数据上的泛化程度。测试误差本身是对模型在训练数据上泛化误差的乐观估计。然而，我们还必须测量模型在尚未被模型看到的数据上的泛化误差。如果模型在未见过的数据上也有低误差，我们可以确信该模型没有过拟合数据。这个过程被称为**交叉验证**。
- en: Thus, to ensure that the model can perform well on unseen data, we will require
    an additional set of data, called the **cross-validation set**. The number of
    samples in the cross-validation set is represented by the term ![Evaluating a
    model](img/4351OS_05_14.jpg). Typically, the sample data is partitioned into the
    training, test, and cross-validation sets such that the number of samples in the
    training data are significantly greater than those in the test and cross-validate
    sets. The error in generalization, or rather the cross-validation error ![Evaluating
    a model](img/4351OS_05_15.jpg), thus indicates how well the estimated model fits
    unseen data. Note that we don't modify the estimated model when we use the cross-validation
    and test sets on it. We will study more about cross-validation in the following
    sections of this chapter. As we will see later, we can also use cross-validation
    to determine the features of a model from some sample data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了确保模型能够在未见过的数据上表现良好，我们需要额外的一组数据，称为**交叉验证集**。交叉验证集中样本的数量由术语![评估模型](img/4351OS_05_14.jpg)表示。通常，样本数据被划分为训练集、测试集和交叉验证集，使得训练数据中的样本数量显著多于测试集和交叉验证集。因此，泛化误差，或者说交叉验证误差![评估模型](img/4351OS_05_15.jpg)，表明估计模型与未见数据拟合得有多好。请注意，当我们使用交叉验证和测试集对估计模型进行修改时，我们不会修改估计模型。我们将在本章的后续部分更详细地研究交叉验证。正如我们稍后将会看到的，我们还可以使用交叉验证来确定从某些样本数据中模型的特征。
- en: For example, suppose we have 100 samples in our training data. We partition
    this sample data into three sets. The first 60 samples will be used to estimate
    a model that fits the data appropriately. Out of the 40 remaining samples, 20
    will be used to cross-validate the estimated model, and the other 20 will be used
    to finally test the cross-validated model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们的训练数据中有100个样本。我们将这些样本数据分成三个集合。前60个样本将用于估计一个适合数据的模型。在剩下的40个样本中，20个将用于交叉验证估计的模型，其余的20个将用于最终测试交叉验证后的模型。
- en: In the context of classification, a good representation of the accuracy of a
    given classifier is a *confusion matrix*. This representation is often used to
    visualize the performance of a given classifier based on a supervised machine
    learning algorithm. Each column in this matrix represents the number of samples
    that belong to a particular class as predicted by the given classifier. The rows
    of the confusion matrix represent the actual classes of the samples. The confusion
    matrix is also called the **contingency matrix** or the **error matrix** of the
    trained classifier.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类的背景下，一个给定分类器准确性的良好表示是*混淆矩阵*。这种表示通常用于根据监督机器学习算法可视化给定分类器的性能。这个矩阵中的每一列代表由给定分类器预测属于特定类别的样本数量。混淆矩阵的行代表样本的实际类别。混淆矩阵也称为训练分类器的**列联表**或**误差矩阵**。
- en: 'For example, say we have two classes in a given classification model. The confusion
    matrix of this model might look like the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设在给定的分类模型中有两个类别。这个模型的混淆矩阵可能如下所示：
- en: '|   | Predicted class |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|   | 预测类别 |'
- en: '| --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| A | B |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| A | B |'
- en: '| --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Actual class** | A | 45 | 15 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **实际类别** | A | 45 | 15 |'
- en: '| B | 30 | 10 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| B | 30 | 10 |'
- en: In a confusion matrix, the predicted classes in our model are represented by
    vertical columns and the actual classes are represented by horizontal rows. In
    the preceding example of a confusion matrix, there are a total of 100 samples.
    Out of these, 45 samples from class A and 10 samples from class B were predicted
    to have the correct class. However, 15 samples of class A have been classified
    as class B and similarly 30 samples of class B have been predicted to have class
    A.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在混淆矩阵中，我们模型中的预测类别由垂直列表示，实际类别由水平行表示。在先前的混淆矩阵示例中，总共有100个样本。在这些样本中，来自类别A的45个样本和来自类别B的10个样本被预测为正确的类别。然而，15个类别A的样本被错误地分类为类别B，同样，30个类别B的样本被预测为类别A。
- en: 'Let''s consider the following confusion matrix of a different classifier that
    uses the same data as the previous example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个使用与上一个示例相同数据的不同分类器的混淆矩阵：
- en: '|   | Predicted class |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|   | 预测类别 |'
- en: '| --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| A | B |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| A | B |'
- en: '| --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Actual class** | A | 45 | 5 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **实际类别** | A | 45 | 5 |'
- en: '| B | 0 | 50 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| B | 0 | 50 |'
- en: In the preceding confusion matrix, the classifier classifies all samples of
    class B correctly. Also, only 5 samples of class A are classified incorrectly.
    Thus, this classifier better understands the distinction between the two classes
    of data when compared to the classifier used in the previous example. In practice,
    we must strive to train a classifier such that it has values close to *0* for
    all the elements other than the diagonal elements in its confusion matrix.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的混淆矩阵中，分类器正确地将类别B的所有样本分类。此外，只有5个类别A的样本被错误分类。因此，与上一个示例中使用的分类器相比，这个分类器更好地理解了两种数据类别的区别。在实践中，我们必须努力训练一个分类器，使其混淆矩阵中除对角线元素外的所有元素值都接近于*0*。
- en: Understanding feature selection
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解特征选择
- en: As we mentioned earlier, we need to determine an appropriate set of features
    from the sample data on which we must base our model. We can use cross-validation
    to determine which set of features to use from the training data, which can be
    explained as follows.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，我们需要从样本数据中确定一个合适的特征集，这是我们建立模型的基础。我们可以使用交叉验证来确定从训练数据中应使用哪个特征集，这可以解释如下。
- en: 'For each set or combination of feature variables, we determine the training
    and cross-validation error of a model based on the selected set of features. For
    example, we might want to add polynomial features derived from the independent
    variables of our model. We evaluate the training and cross-validation errors for
    each set of features depending on the highest degree of polynomial used to model
    the training data. We can plot the variance of these error functions over the
    degree of polynomial used, similar to the following diagram:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个特征变量集或组合，我们根据所选特征集确定模型的训练和交叉验证错误。例如，我们可能想要添加由模型独立变量导出的多项式特征。我们根据用于建模训练数据的最高多项式度数评估每个特征集的训练和交叉验证错误。我们可以绘制这些错误函数的方差随多项式度数的变化，类似于以下图表：
- en: '![Understanding feature selection](img/4351OS_05_16.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![理解特征选择](img/4351OS_05_16.jpg)'
- en: From the preceding diagram, we can determine which set of features produce an
    underfit or overfit estimated model. If a selected model has a high value for
    both the training and cross-validation errors, which is found towards the left
    of the plot, then the model is underfitting the supplied training data. On the
    other hand, a low training error and a high cross-validation error, as shown towards
    the right of the plot, indicates that the model is overfit. Ideally, we must select
    the set of features with the lowest possible values of the training and cross-validation
    errors.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以确定哪个特征集会产生欠拟合或过拟合的估计模型。如果所选模型在图表左侧具有高训练和交叉验证错误值，则表示模型对提供的训练数据欠拟合。另一方面，如图表右侧所示，低训练错误和高交叉验证错误表明模型过拟合。理想情况下，我们必须选择具有最低可能训练和交叉验证错误值的特征集。
- en: Varying the regularization parameter
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整正则化参数
- en: 'To produce a better fit of the training data, we can use regularization to
    avoid the problem of overfitting our data. The value ![Varying the regularization
    parameter](img/4351OS_05_17.jpg) of a given model must be appropriately selected
    depending on the behavior of the model. Note that a high regularization parameter
    could result in a high training error, which is an undesirable effect. We can
    vary the regularization parameter in a formulated machine learning model to produce
    the following plot of the error values over the value of the regularization parameter
    in our model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了产生更好的训练数据拟合，我们可以使用正则化来避免过度拟合数据的问题。给定模型的![调整正则化参数](img/4351OS_05_17.jpg)值必须根据模型的行为适当选择。请注意，高正则化参数可能导致高训练错误，这是不希望看到的效果。我们可以在公式化的机器学习模型中调整正则化参数，以产生以下图表，显示错误值随模型中正则化参数值的变化：
- en: '![Varying the regularization parameter](img/4351OS_05_18.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![调整正则化参数](img/4351OS_05_18.jpg)'
- en: Thus, as shown in the preceding plot, we can also minimize the training and
    cross-validation error in the model by changing the regularization parameter.
    If a model exhibits a high value for both these error values, we must consider
    reducing the value of the regularization parameter until both the error values
    are significantly low for the supplied sample data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如图所示，我们也可以通过改变正则化参数来最小化模型中的训练和交叉验证错误。如果模型在这两个错误值上都有高值，我们必须考虑降低正则化参数的值，直到对于提供的样本数据，这两个错误值都显著降低。
- en: Understanding learning curves
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解学习曲线
- en: 'Another useful way to visualize the performance of a machine learning model
    is to use learning curves. A **learning curve** is essentially a plot of the error
    values in a model over the number of samples by which it is trained and cross-validated.
    For example, a model could have the following learning curve for the training
    and cross-validation errors:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可视化机器学习模型性能的有用方法是使用学习曲线。**学习曲线**本质上是在模型训练和交叉验证的样本数量上绘制错误值的图表。例如，一个模型可能具有以下训练和交叉验证错误的学习曲线：
- en: '![Understanding learning curves](img/4351OS_05_19.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![理解学习曲线](img/4351OS_05_19.jpg)'
- en: 'Learning curves can be used to diagnose an underfit and overfit model. For
    example, the training error could be observed to increase quickly and converge
    towards a value close to the cross-validation with the number of samples provided
    to the model. Also, both the error values in our model have a significantly high
    value. A model that exhibits this kind of variance of error with the number of
    samples is underfit and has a learning curve similar to the following plot:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线可以用来诊断欠拟合和过拟合模型。例如，训练误差可能会观察到随着提供给模型的样本数量的增加而迅速增加，并收敛到一个接近交叉验证的值。此外，我们模型中的误差值也有显著的高值。表现出这种误差随样本数量变化的变异性模型是欠拟合的，其学习曲线类似于以下图表：
- en: '![Understanding learning curves](img/4351OS_05_20.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![理解学习曲线](img/4351OS_05_20.jpg)'
- en: 'On the other hand, a model''s training error could be observed to increase
    slowly with the number of samples provided to the model, and there might also
    be a large difference between the training and cross-validation errors in the
    model. This model is said to be overfit and has a learning curve similar to the
    following plot:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，模型的训练误差可能会随着提供给模型的样本数量的增加而缓慢增加，并且模型中训练误差和交叉验证误差之间可能存在很大的差异。这种模型被称为过拟合，其学习曲线类似于以下图表：
- en: '![Understanding learning curves](img/4351OS_05_21.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![理解学习曲线](img/4351OS_05_21.jpg)'
- en: Thus, learning curve is a good supplementary tool to cross-validation for determining
    what is not working and what needs to be changed in a given machine learning model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，学习曲线是确定给定机器学习模型中哪些地方不起作用以及需要改变的好辅助工具。
- en: Improving a model
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进模型
- en: 'Once we have determined whether a model is underfit or overfit over the given
    sample data, we must decide on how to improve the model''s understanding of the
    relationship between the independent and dependent variables in our model. Let''s
    briefly discuss a few of these techniques, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了模型在给定的样本数据上是欠拟合还是过拟合，我们必须决定如何改进模型对独立变量和因变量之间关系的理解。以下简要讨论几种这些技术：
- en: Add or remove some features. As we will explore later, this technique can be
    used to improve both an underfit and an overfit model.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加或删除一些功能。正如我们稍后将要探讨的，这种技术可以用来改善欠拟合和过拟合的模型。
- en: Vary the value of the regularization parameter ![Improving a model](img/4351OS_05_17.jpg).
    Like adding or removing features, this method can be applied to both underfit
    and overfit models.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整正则化参数的值 ![改进模型](img/4351OS_05_17.jpg)。像添加或删除特征一样，这种方法可以应用于欠拟合和过拟合的模型。
- en: Gather more training data. This method is a fairly obvious solution for improving
    an overfit model as it's needed to formulate a more generalized model to fit the
    training data.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集更多的训练数据。这种方法是改善过拟合模型的一个相当明显的解决方案，因为它需要制定一个更通用的模型来拟合训练数据。
- en: Add features which are polynomial terms of other features in the model. This
    method can be used to improve an underfit model. For example, if we are modeling
    two independent feature variables, ![Improving a model](img/4351OS_05_22.jpg)
    and ![Improving a model](img/4351OS_05_23.jpg), we could add the terms ![Improving
    a model](img/4351OS_05_24.jpg) as additional features to improve the model. The
    polynomial terms could be of even higher degrees, such as ![Improving a model](img/4351OS_05_25.jpg)
    and ![Improving a model](img/4351OS_05_26.jpg) , although this could result in
    overfitting the training data.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加模型中其他特征的多项式项作为特征。这种方法可以用来改善欠拟合模型。例如，如果我们正在模拟两个独立的特征变量，![改进模型](img/4351OS_05_22.jpg)和![改进模型](img/4351OS_05_23.jpg)，我们可以添加![改进模型](img/4351OS_05_24.jpg)作为额外的特征来改善模型。多项式项可以是更高的次数，例如![改进模型](img/4351OS_05_25.jpg)和![改进模型](img/4351OS_05_26.jpg)，尽管这可能会导致训练数据过拟合。
- en: Using cross-validation
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用交叉验证
- en: As we briefly mentioned earlier, cross-validation is a common validation technique
    that can be used to evaluate machine learning models. Cross-validation essentially
    measures how well the estimated model will generalize some given data. This data
    is different from the training data supplied to our model, and is called the **cross-validation
    set**, or simply **validation set**, of our model. Cross-validation of a given
    model is also called **rotation estimation**.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前简要提到的，交叉验证是一种常见的验证技术，可以用来评估机器学习模型。交叉验证本质上衡量的是估计模型将如何泛化一些给定数据。这些数据与提供给我们的模型训练数据不同，被称为模型的**交叉验证集**，或简单地称为**验证集**。给定模型的交叉验证也称为**旋转估计**。
- en: If an estimated model performs well during cross-validation, we can assume that
    the model can understand the relationship between its various independent and
    dependent variables. The goal of cross-validation is to provide a test to determine
    if a formulated model is overfit on the training data. In the perspective of implementation,
    cross-validation is a kind of unit test for a machine learning system.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果估计模型在交叉验证期间表现良好，我们可以假设该模型可以理解其各种独立和依赖变量之间的关系。交叉验证的目的是提供一个测试，以确定所提出的模型是否在训练数据上过度拟合。从实施的角度来看，交叉验证是机器学习系统的一种单元测试。
- en: A single round of cross-validation generally involves partitioning all the available
    sample data into two subsets and then performing training on one subset and validation
    and/or testing on the other subset. Several such rounds, or *folds*, of cross-validation
    must be performed using different sets of data to reduce the variance of the overall
    cross-validation error of the given model. Any particular measure of the cross-validation
    error should be calculated as the average of this error over the different folds
    in cross-validation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 单轮交叉验证通常涉及将所有可用的样本数据划分为两个子集，然后在其中一个子集上进行训练，在另一个子集上进行验证和/或测试。必须使用不同的数据集进行多个这样的交叉验证轮次，或称为“折”，以减少给定模型的整体交叉验证误差的方差。任何特定的交叉验证误差度量都应该计算为不同折在交叉验证中的平均误差。
- en: 'There are several types of cross-validation we can implement as a diagnostic
    for a given machine learning model or system. Let''s briefly explore a few of
    them as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的机器学习模型或系统，我们可以实现多种类型的交叉验证作为诊断。以下简要探讨其中几种：
- en: A common type is *k-fold* cross-validation, in which we partition the cross-validation
    data into *k* equal subsets. The training of the model is then performed on ![Using
    cross-validation](img/4351OS_05_27.jpg) subsets of the data and the cross-validation
    is performed on a single subset.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种常见类型是*k-折交叉验证*，其中我们将交叉验证数据划分为*k*个相等的子集。然后，在数据的一个子集上执行模型的训练，在另一个子集上进行交叉验证。
- en: A simple variation of *k-fold* cross-validation is *2-fold* cross-validation,
    which is also called the *holdout method*. In *2-fold* cross-validation, the training
    and cross-validation subsets of data will be almost equal in proportion.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k-折交叉验证*的一种简单变体是*2-折交叉验证*，也称为*留出法*。在*2-折交叉验证中，训练和交叉验证数据子集的比例几乎相等。'
- en: '**Repeated random subsampling** is another simple variant of cross-validation
    in which the sample data is first randomized or shuffled and then used as training
    and cross-validation data. This method is notably not dependent on the number
    of folds used for cross-validation.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复随机子采样**是交叉验证的另一种简单变体，其中首先对样本数据进行随机化或洗牌，然后将其用作训练和交叉验证数据。这种方法特别不依赖于交叉验证中使用的折数。'
- en: Another form of *k-fold* cross-validation is **leave-one-out** cross-validation,
    in which only a single record from the available sample data is used for cross-validation.
    Leave-one-out cross-validation is essentially *k-fold* cross-validation in which
    *k* is equal to the number of samples or observations in the sample data.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k-折交叉验证*的另一种形式是**留一法交叉验证*，其中仅使用可用样本数据中的一个记录进行交叉验证。留一法交叉验证本质上等同于*k-折交叉验证*，其中*k*等于样本数据中的样本或观察数量。'
- en: 'Cross-validation basically treats the estimated model as a black box, that
    is, it makes no assumptions about the implementation of the model. We can also
    use cross-validation to select features in a given model by using cross-validation
    to determine the feature set that produces the best fit model over the given sample
    data. Of course, there are a couple of limitations of classification, which can
    be summarized as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证基本上将估计模型视为一个黑盒，即它不对模型的实现做出任何假设。我们还可以使用交叉验证来通过确定在给定样本数据上产生最佳拟合模型的特征集来选择给定模型中的特征。当然，分类有一些局限性，可以总结如下：
- en: If a given model is needed to perform feature selection internally, we must
    perform cross-validation for each selected feature set in the given model. This
    can be computationally expensive depending on the amount of available sample data.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要给定的模型进行内部特征选择，我们必须对给定模型中每个选定的特征集进行交叉验证。这可能会根据可用样本数据的数量而变得计算成本高昂。
- en: Cross-validation is not very useful if the sample data comprises exactly or
    nearly equal samples.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果样本数据恰好或几乎完全相等，交叉验证就不是很有效。
- en: In summary, it's a good practice to implement cross-validation for any machine
    learning system that we build. Also, we can choose an appropriate cross-validation
    technique depending on the problem we are trying to model as well as the nature
    of the collected sample data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，对于任何我们构建的机器学习系统，实现交叉验证都是一个好的实践。此外，我们可以根据我们试图建模的问题以及收集到的样本数据的性质来选择合适的交叉验证技术。
- en: Note
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的示例，命名空间声明应类似于以下声明：
- en: '[PRE0]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can use the `clj-ml` library to cross-validate the classifier we built for
    the fish packaging plant in [Chapter 3](ch03.html "Chapter 3. Categorizing Data"),
    *Categorizing Data*. Essentially, we built a classifier to determine whether a
    fish is a salmon or a sea bass using the `clj-ml` library. To recap, a fish is
    represented as a vector containing the category of the fish and values for the
    various features of the fish. The attributes of a fish are its length, width,
    and lightness of skin. We also described a template for a sample fish, which is
    defined as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `clj-ml` 库来交叉验证我们在[第3章](ch03.html "第3章。分类数据")，*分类数据*中为鱼包装厂构建的分类器。本质上，我们使用
    `clj-ml` 库构建了一个分类器，用于确定一条鱼是鲑鱼还是海鲈鱼。为了回顾，一条鱼被表示为一个包含鱼类别和鱼的各种特征的值的向量。鱼的属性是它的长度、宽度和皮肤的光泽。我们还描述了一个样本鱼的模板，其定义如下：
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `fish-template` vector defined in the preceding code can be used to train
    a classifier with some sample data. For now, we will not bother about which classification
    algorithm we have used to model the given training data. We can only assume that
    the classifier was created using the `make-classifier` function from the `clj-ml`
    library. This classifier is stored in the `*classifier*` variable as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的 `fish-template` 向量可以用来使用一些样本数据训练一个分类器。现在，我们暂时不必担心我们使用了哪种分类算法来建模给定的训练数据。我们只能假设这个分类器是使用
    `clj-ml` 库中的 `make-classifier` 函数创建的。这个分类器存储在 `*classifier*` 变量中，如下所示：
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Suppose the classifier was trained with some sample data. We must now evaluate
    this trained classification model. To do this, we must first create some sample
    data to cross-validate. For the sake of simplicity, we will use randomly generated
    data in this example. We can generate this data using the `make-sample-fish` function,
    which we defined in [Chapter 3](ch03.html "Chapter 3. Categorizing Data"), *Categorizing
    Data*. This function simply creates a new vector of some random values representing
    a fish. Of course, we must not forget the fact that the `make-sample-fish` function
    has an in-built partiality, so we create a meaningful pattern in a number of samples
    created using this function as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 假设分类器已经使用一些样本数据进行训练。我们现在必须评估这个训练好的分类模型。为此，我们必须首先创建一些用于交叉验证的样本数据。为了简化，在这个例子中我们将使用随机生成的数据。我们可以使用我们定义在[第3章](ch03.html
    "第3章。分类数据")，*分类数据*中的 `make-sample-fish` 函数来生成这些数据。这个函数简单地创建一个包含一些随机值的新的向量，代表一条鱼。当然，我们不应该忘记
    `make-sample-fish` 函数有一个内置的偏置，因此我们使用这个函数创建的多个样本中创建一个有意义的模式，如下所示：
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will need to use a dataset from the `clj-ml` library, and we can create
    one using the `make-dataset` function, as shown in the following code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用来自 `clj-ml` 库的数据集，并且我们可以使用 `make-dataset` 函数来创建一个，如下面的代码所示：
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To cross-validate the classifier, we must use the `classifier-evaluate` function
    from the `clj-ml.classifiers` namespace. This function essentially performs *k-fold*
    cross-validation on the given data. Other than the classifier and the cross-validation
    dataset, this function requires the number of folds that we must perform on the
    data to be specified as the last parameter. Also, we will first need to set the
    class field of the records in `fish-cv-dataset` using the `dataset-set-class`
    function. We can define a single function to perform these operations as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了交叉验证分类器，我们必须使用来自 `clj-ml.classifiers` 命名空间的 `classifier-evaluate` 函数。这个函数本质上在给定数据上执行
    *k-fold* 交叉验证。除了分类器和交叉验证数据集之外，这个函数还需要指定作为最后一个参数的数据的折数。此外，我们首先需要使用 `dataset-set-class`
    函数设置 `fish-cv-dataset` 记录的类字段。我们可以定义一个单独的函数来执行这些操作，如下所示：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will use 10 folds of cross-validation on the classifier. Since the `classifier-evaluate`
    function returns a map, we bind this return value to a variable for further use,
    as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在分类器上使用10折交叉验证。由于`classifier-evaluate`函数返回一个映射，我们将此返回值绑定到一个变量以供进一步使用，如下所示：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can fetch and print the summary of the preceding cross-validation using
    the `:summary` keyword as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`:summary`关键字获取并打印前面交叉验证的摘要，如下所示：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As shown in the preceding code, we can view several statistical measures of
    performance for our trained classifier. Apart from the correctly and incorrectly
    classified records, this summary also describes the **Root Mean Squared Error**
    (**RMSE**) and several other measures of error in our classifier. For a more detailed
    view of the correctly and incorrectly classified instances in the classifier,
    we can print the confusion matrix of the cross-validation using the `:confusion-matrix`
    keyword, as shown in the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们可以查看我们训练好的分类器的多个性能统计指标。除了正确和错误分类的记录外，此摘要还描述了分类器中的**均方根误差**（**RMSE**）和其他几个误差度量。为了更详细地查看分类器中正确和错误分类的实例，我们可以使用`:confusion-matrix`关键字打印交叉验证的混淆矩阵，如下所示：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As shown in the preceding example, we can use the `clj-ml` library's `classifier-evaluate`
    function to perform a *k-fold* cross-validation on any given classifier. Although
    we are restricted to using classifiers from the `clj-ml` library when using the
    `classifier-evaluate` function, we must strive to implement similar diagnostics
    in any machine learning system we build.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，我们可以使用`clj-ml`库的`classifier-evaluate`函数对任何给定的分类器执行*k*折交叉验证。尽管在使用`classifier-evaluate`函数时我们被限制只能使用`clj-ml`库中的分类器，但我们必须努力在我们构建的任何机器学习系统中实现类似的诊断。
- en: Building a spam classifier
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建垃圾邮件分类器
- en: Now that we are familiar with cross-validation, we will build a working machine
    learning system that incorporates cross-validation. The problem at hand will be
    that of **spam classification**, in which we will have to determine the likelihood
    of a given e-mail being a spam e-mail. Essentially, the problem boils down to
    binary classification with a few tweaks to make the machine learning system more
    sensitive to spam (for more information, refer to *A Plan for Spam*). Note that
    we will not be implementing a classification engine that is integrated with an
    e-mail server, but rather we will be concentrating on the aspects of training
    the engine with some data and classifying a given e-mail.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了交叉验证，我们将构建一个包含交叉验证的工作机器学习系统。当前的问题将是**垃圾邮件分类**，其中我们必须确定一封给定邮件是否为垃圾邮件的可能性。本质上，这个问题归结为二元分类，并做了一些调整以使机器学习系统对垃圾邮件更加敏感（更多信息，请参阅*垃圾邮件计划*）。请注意，我们不会实现一个与电子邮件服务器集成的分类引擎，而是将专注于使用一些数据训练引擎和分类给定邮件的方面。
- en: The way this would be used in practice can be briefly explained as follows.
    A user will receive and read a new e-mail, and will decide whether to mark the
    e-mail as spam or not. Depending on the user's decision, we must train the e-mail
    service's spam engine using the new e-mail as data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在实际中的使用方法可以简要说明如下。用户将接收并阅读一封新邮件，并决定是否将该邮件标记为垃圾邮件。根据用户的决定，我们必须使用新邮件作为数据来训练邮件服务的垃圾邮件引擎。
- en: In order to train our spam classifier in a more automated manner, we'll have
    to simply gather data to feed into the classifier. We will need a large amount
    of data to effectively train a classifier with the English language. Luckily for
    us, sample data for spam classification can be found easily on the Web. For this
    implementation, we will use data from the **Apache SpamAssassin** project.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以更自动化的方式训练我们的垃圾邮件分类器，我们只需简单地收集数据以供分类器使用。我们需要大量的数据才能有效地用英语训练一个分类器。幸运的是，垃圾邮件分类的样本数据在互联网上很容易找到。对于这个实现，我们将使用来自**Apache
    SpamAssassin**项目的数据。
- en: Note
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: The Apache SpamAssassin project is an open source implementation of a spam classification
    engine in Perl. For our implementation, we will use the sample data from this
    project. You can download this data from [http://spamassassin.apache.org/publiccorpus/](http://spamassassin.apache.org/publiccorpus/).
    For our example, we have used the `spam_2` and `easy_ham_2` datasets. A Clojure
    Leiningen project housing our spam classifier implementation will require that
    these datasets be extracted and placed in the `ham/` and `spam/` subdirectories
    of the `corpus/` folder. The `corpus/` folder should be placed in the root directory
    of the Leiningen project that is the same folder of the `project.clj` file.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Apache SpamAssassin 项目是一个用 Perl 实现的垃圾邮件分类引擎的开源实现。对于我们的实现，我们将使用该项目中的样本数据。您可以从
    [http://spamassassin.apache.org/publiccorpus/](http://spamassassin.apache.org/publiccorpus/)
    下载这些数据。在我们的示例中，我们使用了 `spam_2` 和 `easy_ham_2` 数据集。一个容纳我们的垃圾邮件分类器实现的 Clojure Leiningen
    项目将要求这些数据集被提取并放置在 `corpus/` 文件夹的 `ham/` 和 `spam/` 子目录中。`corpus/` 文件夹应放置在 Leiningen
    项目的根目录中，与 `project.clj` 文件相同的文件夹。
- en: The features of our spam classifier will be the number of occurrences of all
    previously encountered words in spam and ham e-mails. By the term **ham**, we
    mean "not spam". Thus, there are effectively two independent variables in our
    model. Also, each word has an associated probability of occurrence in e-mails,
    which can be calculated from the number of times it's found in spam and ham e-mails
    and the total number of e-mails processed by the classifier. A new e-mail would
    be classified by finding all known words in the e-mail's header and body and then
    somehow combining the probabilities of occurrences of these words in spam and
    ham e-mails.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们垃圾邮件分类器的特征将是所有之前遇到的单词在垃圾邮件和正常邮件中的出现次数。术语 **ham** 指的是“非垃圾邮件”。因此，在我们的模型中实际上有两个独立的变量。此外，每个单词都有一个与电子邮件中出现的概率相关联，这可以通过它在垃圾邮件和正常邮件中出现的次数以及分类器处理的电子邮件总数来计算。新电子邮件将通过找到电子邮件标题和正文中所有已知单词，然后以某种方式结合这些单词在垃圾邮件和正常邮件中的出现概率来进行分类。
- en: For a given word feature in our classifier, we must calculate the total probability
    of occurrence of the word by taking into account the total number of e-mails analyzed
    by the classifier (for more information, refer to *Better Bayesian Filtering*).
    Also, an unseen term is neutral in the sense that it is neither spam nor ham.
    Thus, the initial probability of occurrence of any word in the untrained classifier
    is 0.5\. Hence, we use a **Bayesian probability** function to model the occurrence
    of a particular word.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们分类器中的给定单词特征，我们必须通过考虑分类器分析的电子邮件总数来计算单词出现的总概率（更多信息，请参阅 *更好的贝叶斯过滤*）。此外，一个未见过的术语在意义上是中性的，即它既不是垃圾邮件也不是正常邮件。因此，未经训练的分类器中任何单词出现的初始概率是
    0.5。因此，我们使用 **贝叶斯概率** 函数来模拟特定单词的出现。
- en: 'In order to classify a new e-mail, we also need to combine the probabilities
    of occurrences of all the known words found in it. For this implementation, we
    will use **Fisher''s method**, or **Fisher''s combined probability test**, to
    combine the calculated probabilities. Although the mathematical proof of this
    test is beyond the scope of this book, it''s important to know that this method
    essentially estimates the probabilities of several independent probabilities in
    a given model as a ![Building a spam classifier](img/4351OS_05_28.jpg) (pronounced
    as **chi-squared**) distribution (for more information, refer to *Statistical
    Methods for Research Workers*). Such a distribution has an associated number of
    degrees of freedom. It can be shown that an ![Building a spam classifier](img/4351OS_05_28.jpg)
    distribution with degrees of freedom equal to twice the number of combined probabilities
    *k* can be formally expressed as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分类新电子邮件，我们还需要结合其中找到的所有已知单词的出现概率。对于这个实现，我们将使用 **费舍尔方法**，或 **费舍尔组合概率测试**，来结合计算出的概率。尽管这个测试的数学证明超出了本书的范围，但重要的是要知道这种方法本质上是在给定模型中将几个独立概率估计为
    ![构建垃圾邮件分类器](img/4351OS_05_28.jpg)（发音为 **卡方**）分布（更多信息，请参阅 *研究工作者统计方法*）。这种分布有一个相关的自由度数。可以证明，具有等于组合概率数
    *k* 两次的自由度的 ![构建垃圾邮件分类器](img/4351OS_05_28.jpg) 分布可以正式表示如下：
- en: '![Building a spam classifier](img/4351OS_05_29.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![构建垃圾邮件分类器](img/4351OS_05_29.jpg)'
- en: This means that using an ![Building a spam classifier](img/4351OS_05_28.jpg)
    distribution with ![Building a spam classifier](img/4351OS_05_30.jpg) degrees
    of freedom, the **Cumulative Distribution Function** (**CDF**), of the probabilities
    of the e-mail being a spam or a ham can be combined to reflect a total probability
    that is high when there are a large number of probabilities with values close
    to 1.0\. Thus, an e-mail is classified as spam only when most of the words in
    the e-mail have been previously found in spam e-mails. Similarly, a large number
    of ham keywords would indicate the e-mail is in fact a ham e-mail. On the other
    hand, a low number of occurrences of spam keywords in an e-mail would have a probability
    closer to 0.5, in which case the classifier will be unsure of whether the e-mail
    is spam or ham.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着使用具有![构建垃圾邮件分类器](img/4351OS_05_28.jpg)自由度的![构建垃圾邮件分类器](img/4351OS_05_30.jpg)分布，电子邮件是垃圾邮件或正常邮件的概率的**累积分布函数**（**CDF**）可以结合起来反映一个总概率，当有大量值接近1.0的概率时，这个总概率会很高。因此，只有当电子邮件中的大多数单词之前都曾在垃圾邮件中找到时，电子邮件才会被分类为垃圾邮件。同样，大量正常邮件的关键词也会表明该电子邮件实际上是一封正常邮件。另一方面，电子邮件中垃圾邮件关键词出现的次数较少时，其概率会更接近0.5，在这种情况下，分类器将不确定该电子邮件是垃圾邮件还是正常邮件。
- en: Note
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the example that will follow, we will require the `file` and `cdf-chisq`
    functions from the `clojure.java.io` and `Incanter` libraries, respectively. The
    namespace declaration of the example should look similar to the following declaration:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的示例，我们需要从`clojure.java.io`和`Incanter`库中分别获取`file`和`cdf-chisq`函数。示例的命名空间声明应类似于以下声明：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'A classifier trained using Fisher''s method, as described earlier, will be
    very sensitive to new spam e-mails. We represent the dependent variable of our
    model by the probability of a given e-mail being spam. This probability is also
    termed as the **spam score** of the e-mail. A low score indicates that an e-mail
    is ham, while a high score indicates that the e-mail is spam. Of course, we must
    also include a third class to represent an unknown value in our model. We can
    define some reasonable limits for the scores of these categories as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面描述的Fisher方法训练的分类器将非常敏感于新的垃圾邮件。我们用给定电子邮件是垃圾邮件的概率来表示我们模型的因变量。这个概率也被称为电子邮件的**垃圾邮件分数**。低分数表示电子邮件是正常邮件，而高分数表示电子邮件是垃圾邮件。当然，我们还需要在我们的模型中包含一个第三类来表示未知值。我们可以为这些类别的分数定义一些合理的限制，如下所示：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As defined earlier, if an e-mail has a score of 0.7 or more, it's a spam e-mail.
    And a score of 0.5 or less indicates that the e-mail is ham. Also, if the score
    lies between these two values, we can't effectively decide whether the e-mail
    is spam or not. We represent these three categories using the keywords `:ham`,
    `:spam`, and `:unsure`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，如果电子邮件的分数为0.7或更高，则它是垃圾邮件。分数为0.5或更低的电子邮件表示它是正常邮件。此外，如果分数介于这两个值之间，我们无法有效地决定电子邮件是垃圾邮件还是正常邮件。我们使用关键词`:ham`、`:spam`和`:unsure`来表示这三个类别。
- en: 'The spam classifier must read several e-mails, determine all the words, or
    *tokens*, in the e-mails'' text and header, and store this information as empirical
    knowledge to use later. We need to store the number of occurrences a particular
    word is found in spam and ham e-mails. Thus, every word that the classifier has
    encountered represents a feature. To represent this information for a single word,
    we will use a record with three fields as shown in the following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件分类器必须读取几封电子邮件，确定电子邮件文本和标题中的所有单词或*标记*，并将这些信息作为经验知识存储起来以供以后使用。我们需要存储特定单词在垃圾邮件和正常邮件中出现的次数。因此，分类器遇到的每个单词都代表一个特征。为了表示单个单词的信息，我们将使用具有三个字段的记录，如下面的代码所示：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The record `TokenFeature` defined in the preceding code can be used to store
    the needed information for our spam classifier. The `new-token` function simply
    creates a new record for a given token by invoking the records, constructor. Obviously,
    a word is initially seen zero times in both spam and ham e-mails. We will also
    need to update these values, and we define the `inc-count` function to perform
    an update on the record using the `update-in` function. Note that the `update-in`
    function expects a function to apply to a particular field in the record as the
    last parameter. We are already dealing with a small amount of a mutable state
    in our implementation, so let''s delegate access to this state through an agent.
    We would also like to keep track of the total number of ham and spam e-mails;
    so, we''ll wrap these values with agents as well, as shown in the following code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的 `TokenFeature` 记录可以用来存储我们垃圾邮件分类器所需的信息。`new-token` 函数简单地通过调用记录构造函数为给定标记创建一个新的记录。显然，一个单词在垃圾邮件和非垃圾邮件中最初都被看到零次。我们还需要更新这些值，因此我们定义了
    `inc-count` 函数来使用 `update-in` 函数对记录进行更新。请注意，`update-in` 函数期望一个函数作为最后一个参数应用于记录中的特定字段。我们已经在实现中处理了一小部分可变状态，因此让我们通过代理来委托对这个状态的访问。我们还想跟踪垃圾邮件和非垃圾邮件的总数；因此，我们将这些值也用代理包装起来，如下面的代码所示：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `feature-db` agent defined in the preceding code will be used to store
    all word features. We define a simple error handler for this agent using the `:error-handler`
    keyword parameter. The agent''s `total-ham` and `total-spam` functions will keep
    track of the total number of ham and spam e-mails, respectively. We will now define
    a couple of functions to access these agents as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的 `feature-db` 代理将用于存储所有单词特征。我们使用 `:error-handler` 关键字参数为这个代理定义了一个简单的错误处理器。代理的
    `total-ham` 和 `total-spam` 函数将分别跟踪垃圾邮件和非垃圾邮件的总数。现在，我们将定义几个函数来访问这些代理，如下所示：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In case you are not familiar with agents in Clojure, we can use the `send` function
    to alter the value contained in an agent. This function expects a single argument,
    that is, the function to apply to its encapsulated value. The agent applies this
    function on its contained value and updates it if there are no errors. The `clear-db`
    function simply initializes all the agents we've defined with an initial value.
    This is done by using the `constantly` function that wraps a value in a function
    that returns the same value. The `update-feature!` function modifies the value
    of a given token in the `feature-db` map and creates a new token if the supplied
    token is not present in the map of `feature-db`. Since we will only be incrementing
    the number of occurrences of a given token, we will pass the `inc-count` function
    as a parameter to the `update-feature!` function.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对Clojure中的代理不熟悉，我们可以使用 `send` 函数来改变代理中包含的值。这个函数期望一个参数，即应用于其封装值的函数。代理在其包含的值上应用这个函数，如果没有错误，则更新它。`clear-db`
    函数简单地使用初始值初始化我们定义的所有代理。这是通过使用 `constantly` 函数来完成的，该函数将一个值包装在一个返回相同值的函数中。`update-feature!`
    函数修改 `feature-db` 映射中给定标记的值，并在提供的标记不在 `feature-db` 映射中时创建一个新的标记。由于我们只将增加给定标记的出现次数，因此我们将
    `inc-count` 函数作为参数传递给 `update-feature!` 函数。
- en: 'Now, let''s define how the classifier will extract words from a given e-mail.
    We''ll use regular expressions to do this. If we want to extract all the words
    from a given string, we can use the regular expression `[a-zA-Z]{3,}`. We can
    define this regular expression using a literal syntax in Clojure, as shown in
    the following code. Note that we could also use the `re-pattern` function to create
    a regular expression. We will also define all the MIME header fields from which
    we should also extract tokens. We will do all this with the help of the following
    code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义分类器如何从给定的电子邮件中提取单词。我们将使用正则表达式来完成这项工作。如果我们想从给定的字符串中提取所有单词，我们可以使用正则表达式
    `[a-zA-Z]{3,}`。我们可以在Clojure中使用字面量语法来定义这个正则表达式，如下面的代码所示。请注意，我们也可以使用 `re-pattern`
    函数来创建正则表达式。我们还将定义所有应该从中提取标记的MIME头字段。我们将使用以下代码来完成所有这些：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To match tokens with the regular expression defined by `token-regex`, we will
    use the `re-seq` function, which returns all matching tokens in a given string
    as a sequence of strings. For the MIME headers of an e-mail, we need to use a
    different regular expression to extract tokens. For example, we can extract tokens
    from the `"From"` MIME header as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了匹配与 `token-regex` 定义的正则表达式相匹配的标记，我们将使用 `re-seq` 函数，该函数返回给定字符串中所有匹配标记的字符串序列。对于电子邮件的
    MIME 头部，我们需要使用不同的正则表达式来提取标记。例如，我们可以按照以下方式从 `"From"` MIME 头部提取标记：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note the use of the newline character at the end of the regular expression,
    which is used to indicate the end of a MIME header in an e-mail.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意正则表达式末尾的换行符，它用于指示电子邮件中 MIME 头部的结束。
- en: 'We can then proceed to extract words from the values returned by matching the
    regular expression defined in the preceding code. Let''s define the following
    few functions to extract tokens from a given e-mail''s headers and body using
    this logic:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续提取由前述代码中定义的正则表达式匹配得到的值中的单词。让我们定义以下几个函数，使用这种逻辑从给定电子邮件的头和正文中提取标记：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `header-token-regex` function defined in the preceding code returns a regular
    expression for a given header, such as `From:(.*)\n` for the `"From"` header.
    The `extract-tokens-from-headers` function uses this regular expression to determine
    all words in the various header fields of an e-mail and appends the header name
    to all the tokens found in the header text. The `extract-tokens` function applies
    the regular expression over the text and headers of an e-mail and then flattens
    the resulting lists into a single list using the `apply` and `concat` functions.
    Note that the `extract-tokens-from-headers` function returns empty lists for the
    headers defined in `header-fields`, which are not present in the supplied e-mail
    header. Let''s try this function out in the REPL with the help of the following
    code:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中定义的 `header-token-regex` 函数返回一个用于给定头部的正则表达式，例如 `"From:(.*)\n"` 用于 `"From"`
    头部。`extract-tokens-from-headers` 函数使用这个正则表达式来确定电子邮件各种头部字段中的所有单词，并将头部名称附加到头部文本中找到的所有标记上。`extract-tokens`
    函数将正则表达式应用于电子邮件的文本和头部，然后使用 `apply` 和 `concat` 函数将结果列表展平成一个单一的列表。注意，`extract-tokens-from-headers`
    函数对于在 `header-fields` 中定义但不在提供的电子邮件头部中出现的头部返回空列表。让我们通过以下代码在 REPL 中尝试这个函数：
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Using the `extract-tokens-from-headers` function and the regular expression
    defined by `token-regex`, we can extract all words comprising of three or more
    characters from an e-mail''s header and text. Now, let''s define a function to
    apply the `extract-tokens` function on a given e-mail and update the feature map
    using the `update-feature!` function with all the words found in the e-mail. We
    will do all this with the help of the following code:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `extract-tokens-from-headers` 函数和 `token-regex` 定义的正则表达式，我们可以从电子邮件的头和文本中提取所有由三个或更多字符组成的单词。现在，让我们定义一个函数，将
    `extract-tokens` 函数应用于给定的电子邮件，并使用 `update-feature!` 函数更新特征映射，其中包括电子邮件中找到的所有单词。我们将借助以下代码来完成所有这些工作：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Using the `update-features!` function in the preceding code, we can train our
    spam classifier with a given e-mail. In order to keep track of the total number
    of spam and ham e-mails, we will have to send the `inc` function to the `total-spam`
    or `total-ham` agents depending on whether a given e-mail is spam or ham. We will
    do this with the help of the following code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述代码中的 `update-features!` 函数，我们可以使用给定的电子邮件来训练我们的垃圾邮件分类器。为了跟踪垃圾邮件和正常邮件的总数，我们必须根据给定的电子邮件是垃圾邮件还是正常邮件，将
    `inc` 函数发送到 `total-spam` 或 `total-ham` 代理。我们将借助以下代码来完成这项工作：
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `inc-total-count!` function defined in the preceding code updates the total
    number of spam and ham e-mails in our feature database. The `train!` function
    simply calls the `update-features!` and `inc-total-count!` functions to train
    our spam classifier with a given e-mail and its type. Note that we pass the `inc-count`
    function to the `update-features!` function. Now, in order to classify a new e-mail
    as spam or ham, we must first define how to extract the known features from a
    given e-mail using our trained feature database. We will do this with the help
    of the following code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一段代码中定义的 `inc-total-count!` 函数更新了我们特征数据库中垃圾邮件和正常邮件的总数。`train!` 函数简单地调用 `update-features!`
    和 `inc-total-count!` 函数，使用给定的邮件及其类型来训练我们的垃圾邮件分类器。注意，我们将 `inc-count` 函数传递给 `update-features!`
    函数。现在，为了将新邮件分类为垃圾邮件或正常邮件，我们首先必须定义如何使用我们的训练特征数据库从给定的邮件中提取已知特征。我们将借助以下代码来完成这项工作：
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `extract-features` function defined in the preceding code looks up all known
    features in a given e-mail by dereferencing the map stored in `feature-db` and
    applying it as a function to all the values returned by the `extract-tokens` function.
    As mapping the closure `#(@feature-db %1)` can return `()` or `nil` for all tokens
    that are not present in a `feature-db` agent, we will need to remove all empty
    values from the list of extracted features. To do this, we will use the `keep`
    function, which expects a function to apply to the non-nil values in a collection
    and the collection from which all nil values must be filtered out. Since we do
    not intend to transform the known features from the e-mail, we will pass the `identity`
    function, which returns its argument itself as the first parameter to the `keep`
    function.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`extract-features`函数通过解除引用存储在`feature-db`中的映射，并将其作为函数应用于`extract-tokens`函数返回的所有值来查找给定电子邮件中的所有已知特征。由于映射闭包`#(@feature-db
    %1)`可以为不在`feature-db`代理中的所有标记返回`()`或`nil`，因此我们需要从提取的特征列表中删除所有空值。为此，我们将使用`keep`函数，该函数期望一个应用于集合中非nil值的函数以及从其中过滤出所有nil值的集合。由于我们不想转换电子邮件中的已知特征，我们将传递`identity`函数，该函数将返回其参数本身作为`keep`函数的第一个参数。
- en: 'Now that we have extracted all known features from a given e-mail, we must
    calculate all the probabilities of these features occurring in a spam e-mail.
    We must then combine these probabilities using Fisher''s method we described earlier
    to determine the spam score of a new e-mail. Let''s define the following functions
    to implement the Bayesian probability and Fisher''s method:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从一个给定的电子邮件中提取了所有已知特征，我们必须计算这些特征在垃圾邮件中出现的所有概率。然后，我们必须使用我们之前描述的费舍尔方法将这些概率结合起来，以确定新电子邮件的垃圾邮件分数。让我们定义以下函数来实现贝叶斯概率和费舍尔方法：
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `spam-probability` function defined in the preceding code calculates the
    probability of occurrence of a given word feature in a spam e-mail using the number
    of occurrences of the word in spam and ham e-mails and the total number of spam
    and ham e-mails processed by the classifier. To avoid division-by-zero errors,
    we ensure that the value of the number of spam and ham e-mails is at least 1 before
    performing division. The `bayesian-spam-probability` function uses this probability
    returned by the `spam-probability` function to calculate a weighted average with
    the initial probability of 0.5 or *1/2*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`spam-probability`函数使用垃圾邮件和非垃圾邮件中单词出现的次数以及分类器处理的垃圾邮件和非垃圾邮件的总数来计算给定单词特征在垃圾邮件中出现的概率。为了避免除以零错误，我们在执行除法之前确保垃圾邮件和非垃圾邮件的数量至少为1。`bayesian-spam-probability`函数使用`spam-probability`函数返回的这个概率来计算一个加权平均值，初始概率为0.5或*1/2*。
- en: 'We will now implement Fisher''s method of combining the probabilities returned
    by the `bayesian-spam-probability` function for all the known features found in
    an e-mail. We will do this with the help of the following code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将实现费舍尔方法，该方法用于结合由`bayesian-spam-probability`函数返回的所有已知特征的概率。我们将借助以下代码来完成这项工作：
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `fisher` function defined in the preceding code uses the `cdf-chisq` function
    from the `Incanter` library to calculate the CDF of the several probabilities
    transformed by the expression ![Building a spam classifier](img/4351OS_05_31.jpg).
    We specify the number of degrees of freedom to this function using the `:df` optional
    parameter. We now need to apply the `fisher` function to the combined Bayesian
    probabilities of an e-mail being spam or ham, and combine these values into a
    final spam score. These two probabilities must be combined such that only a high
    number of occurrences of high probabilities indicate a strong probability of spam
    or ham. It has been shown that the simplest way to do this is to average the probability
    of a spam e-mail and the negative probability of a ham e-mail (or 1 minus the
    probability of a ham e-mail). We will do this with the help of the following code:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`fisher`函数使用`Incanter`库中的`cdf-chisq`函数来计算由表达式![构建垃圾邮件分类器](img/4351OS_05_31.jpg)转换的几个概率的CDF。我们使用`:df`可选参数指定此函数的自由度。我们现在需要将`fisher`函数应用于电子邮件是垃圾邮件或非垃圾邮件的贝叶斯概率组合，并将这些值组合成一个最终的垃圾邮件分数。这两个概率必须结合，使得只有高概率的高频次出现才表明垃圾邮件或非垃圾邮件的可能性很高。已经证明，这样做最简单的方法是平均垃圾邮件的概率和垃圾邮件的负概率（或1减去垃圾邮件的概率）。我们将借助以下代码来完成这项工作：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Hence, the `score` function will return the final spam score of a given e-mail.
    Let''s define a function to extract the known word features from a given e-mail,
    combine the probabilities of occurrences of these features to produce the e-mail''s
    spam score, and finally classify this spam score as a ham or spam e-mail, represented
    by the keywords `:ham` and `:spam` respectively, as shown in the following code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`score` 函数将返回给定电子邮件的最终垃圾邮件分数。让我们定义一个函数来从给定的电子邮件中提取已知单词特征，将这些特征的出现的概率结合起来产生电子邮件的垃圾邮件分数，并最终将这个垃圾邮件分数分类为正常邮件或垃圾邮件，分别用关键词
    `:ham` 和 `:spam` 表示，如下面的代码所示：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'So far, we have implemented how we train our spam classifier and use it to
    classify a new e-mail. Now, let''s define some functions to load the sample data
    from the project''s `corpus/` folder and use this data to train and cross-validate
    our classifier, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经实现了如何训练我们的垃圾邮件分类器以及如何使用它来分类一封新的电子邮件。现在，让我们定义一些函数来从项目的 `corpus/` 文件夹中加载样本数据，并使用这些数据来训练和交叉验证我们的分类器，如下所示：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `populate-emails` function defined in the preceding code returns a sequence
    of vectors to represent all the ham e-mails from the `ham/` folder and the spam
    e-mails from the `spam/` folder in our sample data. Each vector in this returned
    sequence has two elements. The first element in this vector is a given e-mail's
    relative file path and the second element is either `:spam` or `:ham` depending
    on whether the e-mail is spam or ham. Note the use of the `file-seq` function
    to read the files in a directory as a sequence.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的 `populate-emails` 函数返回一个向量序列，代表我们样本数据中来自 `ham/` 文件夹的所有正常邮件和来自 `spam/`
    文件夹的所有垃圾邮件。这个返回序列中的每个向量都有两个元素。这个向量中的第一个元素是给定电子邮件的相对文件路径，第二个元素是 `:spam` 或 `:ham`，这取决于电子邮件是否为垃圾邮件。请注意，使用
    `file-seq` 函数将目录中的文件作为序列读取。
- en: 'We will now use the `train!` function to feed the content of all e-mails into
    our spam classifier. To do this, we can use the `slurp` function to read the content
    of a file as a string. For cross-validation, we will classify each e-mail in the
    supplied cross-validation data using the `classify` function and return a list
    of maps representing the test result of the cross-validation. We will do this
    with the help of the following code:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `train!` 函数将所有电子邮件的内容输入到我们的垃圾邮件分类器中。为此，我们可以使用 `slurp` 函数将文件内容读取为字符串。对于交叉验证，我们将使用
    `classify` 函数对提供的交叉验证数据中的每封电子邮件进行分类，并返回一个表示交叉验证测试结果的映射列表。我们将通过以下代码来完成这项工作：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `train-from-corpus!` function defined in the preceding code will train
    our spam classifier with all e-mails found in the `corpus/` folder. The `cv-from-corpus`
    function classifies the supplied e-mails as spam or ham using the trained classifier
    and returns a sequence of maps indicating the results of the cross-validation
    process. Each map in the sequence returned by the `cv-from-corpus` function contains
    the file of the e-mail, the actual type (spam or ham) of the e-mail, the predicted
    type of the e-mail, and the spam score of the e-mail. Now, we need to call these
    two functions on two appropriately partitioned subsets of the sample data as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的 `train-from-corpus!` 函数将使用 `corpus/` 文件夹中找到的所有电子邮件来训练我们的垃圾邮件分类器。`cv-from-corpus`
    函数使用训练好的分类器将提供的电子邮件分类为垃圾邮件或正常邮件，并返回一个表示交叉验证过程结果的映射序列。`cv-from-corpus` 函数返回的序列中的每个映射包含电子邮件的文件、电子邮件的实际类型（垃圾邮件或正常邮件）、电子邮件的预测类型和电子邮件的垃圾邮件分数。现在，我们需要在样本数据的两个适当划分的子集上调用这两个函数，如下所示：
- en: '[PRE27]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `test-classifier!` function defined in the preceding code will randomly
    shuffle the sample data and select a specified fraction of this randomized data
    as the cross-validation set for our classifier. The `test-classifier!` function
    then calls the `train-from-corpus!` and `cv-from-corpus` functions to train and
    cross-validate the data. Note that the use of the `await` function is to wait
    until the `feature-db` agent has finished applying all functions that have been
    sent to it via the `send` function.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的 `test-classifier!` 函数将随机打乱样本数据，并选择指定比例的随机数据作为我们的分类器的交叉验证集。然后，`test-classifier!`
    函数调用 `train-from-corpus!` 和 `cv-from-corpus` 函数来训练和交叉验证数据。请注意，使用 `await` 函数是为了等待
    `feature-db` 代理完成通过 `send` 函数发送给它的所有函数的应用。
- en: 'Now we need to analyze the results of cross-validation. We must first determine
    the number of incorrectly classified and missed e-mails from the actual and expected
    class of a given e-mail as returned by the `cv-from-corpus` function. We will
    do this with the help of the following code:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要分析交叉验证的结果。我们必须首先确定由 `cv-from-corpus` 函数返回的给定电子邮件的实际和预期类别中的错误分类和遗漏的电子邮件数量。我们将使用以下代码来完成这项工作：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `result-type` function will determine the number of incorrectly classified
    and missed e-mails in the cross-validation process. We can now apply the `result-type`
    function to all the maps in the results returned by the `cv-from-corpus` function
    and print a summary of the cross-validation results with the help of the following
    code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`result-type` 函数将确定交叉验证过程中错误分类和遗漏的电子邮件数量。现在，我们可以将 `result-type` 函数应用于 `cv-from-corpus`
    函数返回的结果中的所有映射，并使用以下代码帮助打印交叉验证结果的摘要：'
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `analyze-results` function defined in the preceding code simply applies
    the `result-type` function to all the map values in the sequence returned by the
    `cv-from-corpus` function, while maintaining the total number of incorrectly classified
    and missed e-mails. The `print-result` function simply prints the analyzed result
    as a string. Finally, let''s define a function to load all the e-mails using the
    `populate-emails` function and then use this data to train and cross-validate
    our spam classifier. Since the `populate-emails` function will return an empty
    list, or `nil` when there are no e-mails, we will check this condition to avoid
    failing at a later stage in our program:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的 `analyze-results` 函数简单地将 `result-type` 函数应用于 `cv-from-corpus` 函数返回的序列中的所有映射值，同时保持错误分类和遗漏的电子邮件总数。`print-result`
    函数简单地将分析结果打印为字符串。最后，让我们定义一个函数，使用 `populate-emails` 函数加载所有电子邮件，然后使用这些数据来训练和交叉验证我们的垃圾邮件分类器。由于
    `populate-emails` 函数在没有电子邮件时将返回一个空列表或 `nil`，我们将检查这个条件以避免在程序后续阶段失败：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the `train-and-cv-classifier` function shown in the preceding code, we first
    call the `populate-emails` function and convert the result to a sequence using
    the `seq` function. If the sequence has any elements, we train and cross-validate
    the classifier. If there are no e-mails found, we simply throw an error. Note
    that the `if-let` function is used to check whether the sequence returned by the
    `seq` function has any elements.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码中显示的 `train-and-cv-classifier` 函数中，我们首先调用 `populate-emails` 函数，并使用 `seq`
    函数将结果转换为序列。如果序列有任何元素，我们训练并交叉验证分类器。如果没有找到电子邮件，我们简单地抛出一个错误。请注意，`if-let` 函数用于检查 `seq`
    函数返回的序列是否有任何元素。
- en: 'We have all the parts needed to create and train a spam classifier. Initially,
    as the classifier hasn''t seen any e-mails, the probability of any e-mail or text
    being spam is 0.5\. This can be verified by using the `classify` function, as
    shown in the following code, which initially classifies any text as the `:unsure`
    type:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经拥有了创建和训练垃圾邮件分类器所需的所有部分。最初，由于分类器尚未看到任何电子邮件，任何电子邮件或文本被分类为垃圾邮件的概率是 0.5。这可以通过以下代码验证，该代码最初将任何文本分类为
    `:unsure` 类型：
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We now train the classifier and cross-validate it using the `train-and-cv-classifier`
    function. We will use one-fifth of all the available sample data as our cross-validation
    set. This is shown in the following code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用 `train-and-cv-classifier` 函数训练分类器并交叉验证它。我们将使用所有可用样本数据的一分之一作为我们的交叉验证集。这如下面的代码所示：
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Cross-validating our spam classifier asserts that it''s appropriately classifying
    e-mails. Of course, there is still a small amount of error, which can be corrected
    by using more training data. Now, let''s try to classify some text using our trained
    spam classifier, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证我们的垃圾邮件分类器断言它适当地分类了电子邮件。当然，仍然存在一小部分错误，这可以通过使用更多的训练数据来纠正。现在，让我们尝试使用我们的训练好的垃圾邮件分类器对一些文本进行分类，如下所示：
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Interestingly, the text `"Make money fast"` is classified as spam and the text
    `"Job interview … GNU project"` is classified as ham, as shown in the preceding
    code. Let''s have a look at how the trained classifier extracts features from
    some text using the `extract-features` function. Since the classifier will initially
    have read no tokens, this function will obviously return an empty list or `nil`
    when the classifier is untrained, as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，文本“Make money fast”被归类为垃圾邮件，而文本“Job interview … GNU project”被归类为正常邮件，如前面的代码所示。让我们看看训练好的分类器是如何使用`extract-features`函数从某些文本中提取特征的。由于分类器最初没有读取任何标记，因此当分类器未训练时，此函数显然会返回一个空列表或`nil`，如下所示：
- en: '[PRE34]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As shown in the preceding code, each `TokenFeature` record will contain the
    number of times a given word is seen in spam and ham e-mails. Also, the word `"to"`
    is not recognized as a feature since we only consider words comprising of three
    or more characters.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，每个`TokenFeature`记录将包含给定单词在垃圾邮件和正常邮件中出现的次数。此外，单词“to”不被识别为特征，因为我们只考虑由三个或更多字符组成的单词。
- en: 'Now, let''s check how sensitive to spam e-mail our spam classifier actually
    is. We''ll first have to select some text or a particular term that is classified
    as neither spam nor ham. For the training data selected for this example, the
    word `"Job"` fits this requirement, as shown in the following code. Let''s train
    the classifier with the word `"Job"` while specifying the type of the text as
    ham. We can do this using the `train!` function, as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查我们的垃圾邮件分类器对垃圾邮件的敏感性。我们首先需要选择一些文本或特定的术语，这些文本或术语既不被归类为垃圾邮件，也不被归类为正常邮件。对于本例中选定的训练数据，单词“Job”符合这一要求，如下面的代码所示。让我们使用`train!`函数用单词“Job”训练分类器，同时指定文本类型为正常邮件。我们可以这样做，如下所示：
- en: '[PRE35]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After training the classifier with the given text as ham, the probability of
    the term being spam is observed to decrease by a small amount. If the term `"Job"`
    occurred in several more e-mails that were ham, the classifier would eventually
    classify this word as ham. Thus, the classifier doesn''t show much of a reaction
    to a new ham e-mail. On the contrary, the classifier is observed to be very sensitive
    to spam e-mails, as shown in the following code:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在用给定的文本作为正常邮件训练分类器后，观察到该术语被归类为垃圾邮件的概率略有下降。如果术语“Job”出现在更多正常邮件中，分类器最终会将该单词归类为正常邮件。因此，分类器对新的正常邮件的反应并不明显。相反，如以下代码所示，分类器对垃圾邮件的敏感性很高：
- en: '[PRE36]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: An occurrence of a particular word in a single spam e-mail is observed to greatly
    increase a classifier's predicted probability of the given term belonging to a
    spam e-mail. The term `"Job"` will subsequently be classified as spam by our classifier,
    at least until it's seen to appear in a sufficiently large number of ham e-mails.
    This is due to the nature of the chi-squared distribution that we are modeling.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个垃圾邮件中观察到特定单词的出现会显著增加分类器预测该术语属于垃圾邮件的概率。术语“Job”随后将被我们的分类器归类为垃圾邮件，至少直到它在足够多的正常邮件中出现。这是由于我们正在建模的卡方分布的性质。
- en: 'We can also improve the overall error of our spam classifier by supplying it
    with more training data. To demonstrate this, let''s cross-validate the classifier
    with only one-tenth of the sample data. Thus, the classifier would be effectively
    trained with nine-tenths of the available data, as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过向分类器提供更多训练数据来提高我们垃圾邮件分类器的整体错误率。为了演示这一点，让我们只用样本数据中的一分之一来交叉验证分类器。因此，分类器将实际上用可用的九分之八的数据进行训练，如下所示：
- en: '[PRE37]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As shown in the preceding code, the number of misses and wrongly classified
    e-mails is observed to reduce when we use more training data. Of course, this
    is only shown as an example, and we should instead collect more e-mails to feed
    into the classifier as training data. Using a significant amount of the sample
    data for cross-validation is a good practice.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，当我们使用更多训练数据时，漏检和错误分类的邮件数量有所减少。当然，这只是一个示例，我们应收集更多邮件作为训练数据输入到分类器中。使用样本数据的一部分进行交叉验证是一种良好的实践。
- en: In summary, we have effectively built a spam classifier that is trained using
    Fisher's method. We have also implemented a cross-validation diagnostic, which
    serves as a kind of unit test for our classifier.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们有效地构建了一个使用费舍尔方法训练的垃圾邮件分类器。我们还实现了一个交叉验证诊断，这相当于对我们分类器的一种单元测试。
- en: Note
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the exact values produced by the `train-and-cv-classifier` function
    will vary depending on the spam and ham emails used as training data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`train-and-cv-classifier`函数产生的确切值将取决于用作训练数据的垃圾邮件和正常邮件。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we have explored techniques that can be used to diagnose and
    improve a given machine learning model. The following are some of the other points
    that we have covered:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了可以用来诊断和改进给定机器学习模型的技巧。以下是我们已经涵盖的一些其他要点：
- en: We have revisited the problems of underfitting and overfitting of sample data
    and also discussed how we can evaluate a formulated model to diagnose whether
    it's underfit or overfit.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们重新审视了样本数据欠拟合和过拟合的问题，并讨论了如何评估一个已制定模型以诊断它是否欠拟合或过拟合。
- en: We have explored cross-validation and how it can be used to determine how well
    a formulated model will respond to previously unseen data. We have also seen that
    we can use cross-validation to select the features and the regularization parameter
    of a model. We also studied a few kinds of cross-validation that we can implement
    for a given model.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经探讨了交叉验证及其如何被用来确定一个已制定模型对之前未见过的数据的响应效果。我们还看到，我们可以使用交叉验证来选择模型的特征和正则化参数。我们还研究了几种可以针对给定模型实现的交叉验证方法。
- en: We briefly explored learning curves and how they can be used to diagnose the
    underfit and overfit models.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们简要探讨了学习曲线及其如何被用来诊断欠拟合和过拟合模型。
- en: We've explored the tools provided by the `clj-ml` library to cross-validate
    a given classifier.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经探讨了`clj-ml`库提供的工具，用于对给定分类器进行交叉验证。
- en: Lastly, we've built an operational spam classifier that incorporates cross-validation
    to determine whether the classifier is appropriately classifying e-mails as spam.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们构建了一个操作性的垃圾邮件分类器，该分类器结合交叉验证来确定分类器是否适当地将电子邮件分类为垃圾邮件。
- en: In the following chapters, we will continue exploring more machine learning
    models, and we'll also study **Support Vector Machines** (**SVMs**) in detail.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将继续探索更多的机器学习模型，并且我们还将详细研究**支持向量机**（**SVMs**）。
