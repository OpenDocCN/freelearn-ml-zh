- en: '*Chapter 1*: Understanding the End-to-End Machine Learning Process'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第一章*：理解端到端机器学习流程'
- en: Welcome to the second edition of *Mastering Azure Machine Learning*. In this
    first chapter, we want to give you an understanding of what kinds of problems
    require the use of **machine learning** (**ML**), how the full ML process unfolds,
    and what knowledge is required to navigate this vast terrain. You can view it
    as an introduction to ML and an overview of the book itself, where for most topics
    we will provide you with a reference to upcoming chapters so that you can easily
    find your way around the book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到《精通Azure机器学习》的第二版。在本章的第一章，我们希望让你了解哪些类型的问题需要使用**机器学习**（**ML**），完整的机器学习过程是如何展开的，以及在这个广阔领域导航所需的哪些知识。你可以将其视为机器学习的介绍和本书的概述，其中对于大多数主题，我们将提供对后续章节的参考，以便你能够轻松地在书中找到自己的位置。
- en: In the first section, we will ask ourselves what ML is, when we should use it,
    and where it comes from. In addition, we will reflect on how ML is just another
    form of programming.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们将问自己什么是机器学习，我们应该在何时使用它，以及它从何而来。此外，我们将反思机器学习只是编程的另一种形式。
- en: In the second section, we will lay the mathematical groundwork you require to
    process data, and we will understand that the data you work with probably cannot
    be fully trusted. Further, we will look at different classes of ML algorithms,
    how they are defined, and how we can define the performance of a trained model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们将奠定你处理数据所需的数学基础，并且我们将理解你工作的数据可能无法完全信赖。此外，我们将探讨不同类别的机器学习算法，它们的定义，以及我们如何定义训练模型的性能。
- en: Finally, in the third section, we will have a look at the end-to-end process
    of an ML project. We will understand where to get data from, how to preprocess
    data, how to choose a fitting model, and how to deploy this model into production
    environments. This will also get us into the topic of **ML operations**, known
    as **MLOps**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第三部分，我们将探讨机器学习项目的端到端流程。我们将了解从哪里获取数据，如何预处理数据，如何选择合适的模型，以及如何将此模型部署到生产环境中。这也会让我们接触到**机器学习操作**的话题，通常称为**MLOps**。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Grasping the idea behind ML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习背后的理念
- en: Understanding the mathematical basis for statistical analysis and ML modeling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解统计分析和机器学习建模的数学基础
- en: Discovering the end-to-end ML process
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现端到端机器学习流程
- en: Grasping the idea behind ML
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习背后的理念
- en: The terms **artificial intelligence** (**AI**) and—partially—**ML** are omnipresent
    in today's world. However, a lot of what is found under the term *AI* is often
    nothing more than a containerized ML solution, and to make matters worse, ML is
    sometimes unnecessarily used to solve something extremely simple.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 术语**人工智能**（**AI**）和部分地**机器学习**（**ML**）在当今世界无处不在。然而，在**AI**这个术语下找到的很多东西往往不过是一个容器化的机器学习解决方案，而且更糟糕的是，机器学习有时被不必要地用于解决极其简单的问题。
- en: Therefore, in this first section, let's understand the class of problems ML
    tries to solve, in which scenarios to use ML, and when not to use it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本节的第一部分，让我们了解机器学习试图解决的问题类别，在哪些场景下使用机器学习，以及在何时不应使用它。
- en: Problems and scenarios requiring ML
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要机器学习的问题和场景
- en: 'If you look for a definition of ML, you will often find a description such
    as this: *It is the study of self-improving machine algorithms using data*. ML
    is basically described as an algorithm we are trying to evolve, which in turn
    can be seen as one complex mathematical function.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你寻找机器学习的定义，你通常会找到一个这样的描述：*它是使用数据研究自我改进的机器算法的学科*。机器学习基本上被描述为我们试图进化的算法，这反过来又可以被视为一个复杂的数学函数。
- en: Any computer process today follows the simple structure of the **input-process-output
    (IPO) model**. We define allowed inputs, we define a process working with those
    inputs, and we define an output through the type of results the process will show
    us. A simple example would be a word processing application, where every keystroke
    will result in a letter shown as the output on the screen. A completely different
    process might run in parallel to that one, having a time-based trigger to store
    the text file periodically to a hard disk.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的任何计算机过程都遵循简单的**输入-处理-输出（IPO）模型**结构。我们定义允许的输入，我们定义一个处理这些输入的过程，并通过该过程将显示的结果类型定义输出。一个简单的例子是一个文字处理应用程序，其中每个按键都会在屏幕上显示为输出字母。可能并行运行的一个完全不同的过程可能有一个基于时间的触发器，定期将文本文件存储到硬盘上。
- en: All these processes or algorithms have one thing in common—they were manually
    written by someone using a **high-level programming language**. It is clear which
    actions need to be done when someone presses a letter in a word processing application.
    Therefore, we can easily build a process in which we implement which input values
    should create which output values.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些过程或算法有一个共同点——它们都是某人使用**高级编程语言**手动编写的。当有人在文字处理应用程序中按下一个字母时，需要执行哪些操作是显而易见的。因此，我们可以轻松构建一个过程，其中我们实现哪些输入值应该创建哪些输出值。
- en: 'Now, let''s look at a more complex problem. Imagine we have a picture of a
    dog and want an application to just say: *This is a dog*. This sounds simple enough,
    as we know the input *picture of a dog* and the output value *dog*. Unfortunately,
    our brain (our own machine) is far superior to the machines we built, especially
    when it comes to pattern recognition. For a computer, a picture is just a square
    of ![](img/B17928_Formula_1.01.png) pixels, each containing three color channels
    defined by an 8-bit or 10-bit value. Therefore, an image is just a bunch of pixels
    made up of vectors for the computer, so in essence, a lot of numbers.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个更复杂的问题。想象一下，我们有一张狗的图片，并希望一个应用程序能够说：*这是一只狗*。这听起来很简单，因为我们知道输入*狗的图片*和输出值*狗*。不幸的是，我们的大脑（我们自己的机器）比我们构建的机器优越得多，尤其是在模式识别方面。对于计算机来说，一张图片只是一个由
    ![](img/B17928_Formula_1.01.png) 像素组成的正方形，每个像素包含由8位或10位值定义的三个颜色通道。因此，对于计算机来说，图像只是一堆由向量组成的像素，本质上是一堆数字。
- en: We could manually start writing an algorithm that maybe clusters groups of pixels,
    looks for edges and points of interest, and eventually, with a lot of effort,
    we might succeed in having an algorithm that finds dogs in pictures. That is when
    we get a picture of a cat.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以手动开始编写一个算法，可能聚类像素组，寻找边缘和感兴趣点，最终，经过大量努力，我们可能成功拥有一个在图片中找到狗的算法。但那时我们得到了一张猫的图片。
- en: 'It should be clear to you by now that we might run into a problem. Therefore,
    let''s define one problem that ML solves, as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该已经清楚我们可能会遇到问题。因此，让我们定义一个机器学习解决的问题，如下：
- en: '*Building the desired algorithm for a required solution programmatically is
    either extremely time-consuming, completely unfeasible, or impossible.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*以编程方式构建所需解决方案的期望算法要么非常耗时，要么完全不切实际，或者根本不可能。*'
- en: Taking this description, we can surely define good scenarios to use ML, be it
    finding objects in images and videos or understanding voices and extracting their
    intent from audio files. We will further understand what building ML solutions
    entails throughout this chapter (and the rest of the book, for that matter), but
    to make a simple statement, let's just acknowledge that building an ML model is
    also a time-consuming matter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个描述，我们可以确定使用机器学习的良好场景，无论是寻找图像和视频中的对象，还是理解声音并从音频文件中提取其意图。我们将在本章（以及本书的其余部分）中进一步了解构建机器学习解决方案涉及的内容，但简单来说，让我们承认构建机器学习模型也是一个耗时的事情。
- en: In that vein, it should be of utmost importance to avoid ML if we have the chance
    to do so. This might be an obvious statement, but as we (the authors) can attest,
    it is not for a lot of people. We have seen projects realized with ML where the
    output could be defined with a simple combination of `if` statements given some
    input vectors. In such scenarios, a solution could be obtained with a couple of
    hundred lines of code. Instead, months of training and testing an ML algorithm
    occurred, costing a lot of time and resources.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，如果我们有机会避免使用机器学习，那么这应该是最重要的。这或许是一个显而易见的陈述，但正如我们（作者）可以证明的那样，对很多人来说并非如此。我们见过一些使用机器学习实现的项目，如果给定一些输入向量，输出可以通过简单的`if`语句组合来定义。在这种情况下，可以通过几百行代码获得解决方案。相反，对机器学习算法进行数月的训练和测试，耗费了大量时间和资源。
- en: An example of this would be a company wanting to predict fraud (stolen money)
    committed by their own employees in a retail store. You might have heard that
    predicting fraud is a typical scenario for ML. Here, it was *not necessary* to
    use ML, as the company already knew the influencing factors (length of time the
    cashier was open, error codes on return receipts, and so on) and therefore wanted
    to be alerted when certain combinations of these factors occurred. As they knew
    the factors already, they could have just written the code and be done with it.
    But what does this scenario tell us about ML?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一家公司可能想要预测其零售店员工犯下的欺诈（被盗资金）。你可能听说过预测欺诈是机器学习的典型场景。在这里，使用机器学习并不是必要的，因为公司已经知道影响因素（收银员开放的时间长度、退货收据上的错误代码等），因此希望在发生某些因素组合时得到警报。既然他们已经知道这些因素，他们可以直接编写代码并完成。但这个场景告诉我们关于机器学习什么？
- en: 'So far, we have looked at ML as a solution to solve a problem that, in essence,
    is too hard to code. Looking at the preceding scenario, you might understand another
    aspect or another class of problems that ML can solve. Therefore, let''s add a
    second problem description, as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将机器学习视为解决一个本质上难以编码的问题的解决方案。从前面提到的场景来看，你可能理解了机器学习可以解决的另一个方面或另一类问题。因此，让我们添加第二个问题描述，如下：
- en: '*Building the desired algorithm for a required solution is not feasible, as
    the influencing factors for the outcome of the desired outputs are only partially
    known or completely unknown.*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*构建一个所需解决方案的期望算法是不切实际的，因为影响所需输出结果的因素只有部分已知或完全未知。*'
- en: Looking at this problem, you might now understand why ML relies so heavily on
    the field of statistics as, through the application of statistics, we can learn
    how data points influence one another, and therefore we might be able to solve
    such a problem. At the same time, we can build an algorithm that can find and
    predict the desired outcome.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 看到这个问题，你现在可能已经理解为什么机器学习如此依赖于统计学领域，因为通过应用统计学，我们可以了解数据点是如何相互影响的，因此我们可能能够解决这样的问题。同时，我们可以构建一个能够找到并预测所需结果的算法。
- en: In the previously mentioned scenario for detecting fraud, it might be prudent
    to still use ML, as it may be able to find a combination of influencing factors
    no one has thought about. But if this is not your set goal—as it was not in this
    case—you should not use ML for something that is easily written in code.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前提到的检测欺诈的场景中，仍然使用机器学习可能是谨慎的，因为它可能能够找到没有人考虑过的影响因素的组合。但如果这不是你的目标——正如在这个案例中那样——你不应该将机器学习用于那些可以轻易用代码编写的事情。
- en: Now that we have discussed some of the problems solved by ML and have had a
    look at some scenarios for ML, let's have a look at how ML came to be.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了一些机器学习解决的问题，并查看了一些机器学习的场景，让我们来看看机器学习是如何产生的。
- en: The history of ML
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习的历史
- en: To understand ML as a whole, we must first understand where it comes from. Therefore,
    let's delve into the history of ML. As with all events in history, different currents
    are happening simultaneously, adding pieces to the whole picture. We'll now look
    at a few important pillars that birthed the idea of ML as we know it today.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要全面理解机器学习，我们首先必须了解它的来源。因此，让我们深入了解机器学习的历史。与历史上的所有事件一样，不同的潮流同时发生，为整个画面增添了碎片。现在，我们将查看几个重要的支柱，这些支柱孕育了我们今天所知道的机器学习的理念。
- en: Learnings from neuroscience
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经科学的学习
- en: 'A neuropsychologist named Donald O. Hebb published a book titled *The Organization
    of Behavior* in 1949\. In this book, he described his theory of how **neurons**
    (neural cells) in our brain function, and how they contribute to what we understand
    as *learning*. This theory is known as **Hebbian learning**, and it makes the
    following proposition:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一位名叫唐纳德·O·赫布（Donald O. Hebb）的神经心理学家于1949年出版了一本名为《行为组织》（The Organization of Behavior）的书。在这本书中，他描述了他关于我们大脑中的**神经元**（神经细胞）如何工作以及它们如何贡献于我们所理解的**学习**的理论。这个理论被称为**赫布学习**（Hebbian
    learning），并提出了以下命题：
- en: When an axon of cell A is near enough to excite cell B and repeatedly or persistently
    takes part in firing it, some growth process or metabolic change takes place in
    one or both cells such that A's efficiency, as one of the cells firing B, is increased.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当细胞A的轴突足够接近以兴奋细胞B，并且反复或持续地参与其放电时，一个或两个细胞中会发生一些生长过程或代谢变化，使得A作为放电B的细胞之一，其效率得到提高。
- en: This basically describes that there is a process where one cell excites another
    repeatedly (the initiating cell) and maybe even the receiving cell is changed
    through a hidden process. This process is what we call learning.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上描述了一个过程，其中一个细胞反复激发另一个细胞（启动细胞），甚至接收细胞可能通过一个隐藏的过程被改变。这个过程就是我们所说的学习。
- en: 'To understand this a bit more visually, let''s have a look at the biological
    structure of a neuron, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地理解这一点，让我们看一下神经元的生物结构，如下所示：
- en: '![Figure 1.1 – Neuron in a biological neural network ](img/B17928_01_01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – 生物神经网络中的神经元](img/B17928_01_01.jpg)'
- en: Figure 1.1 – Neuron in a biological neural network
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 生物神经网络中的神经元
- en: What is visualized here? Firstly, on the left, we see the main body of the cell
    and its nucleus. The body receives input signals through dendrites that are connected
    to other neurons. In addition, there is a larger exit perturbing from the body
    called the axon, which connects the main body through a chain of Schwann cells
    to the so-called axon terminal, which in turn connects again to other neurons.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可视化的是什么？首先，在左边，我们看到细胞的主体及其细胞核。主体通过与其它神经元相连的树突接收输入信号。此外，还有一个从主体伸出的大轴突，它通过一系列施万细胞将主体与所谓的轴突末端连接起来，而轴突末端反过来又连接到其他神经元。
- en: Looking at this structure with some creativity, it certainly resembles what
    a function or an algorithm might be. We have input signals coming from external
    neurons, we have some hidden process happening with these signals, and we have
    an output in the form of an axon terminal that connects the results to other neurons,
    and therefore other processes again.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 用一些创意来看这个结构，它确实类似于一个函数或算法。我们有来自外部神经元的输入信号，我们有这些信号的一些隐藏过程，我们有一个以轴突末端形式存在的输出，它将结果连接到其他神经元，从而再次连接到其他过程。
- en: It would take another decade again for someone to realize this connection.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 又过了十年，才有人意识到这个联系。
- en: Learnings from computer science
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算机科学的学习经验
- en: It is hard to talk about the history of ML in the context of computer science
    without mentioning one of the fathers of modern machines, Alan Turing. In a paper
    called *Computing Machinery and Intelligence* published in 1950, Turing defines
    a test called the **Imitation Game** (later called the **Turing test**) to evaluate
    whether a machine shows human behavior indistinguishable from a human. There are
    multiple iterations and variants of the test, but in essence, the idea is that
    a person would at no point in a conversation get the feeling they are not speaking
    with a human.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学背景下谈论机器学习的历史，如果不提及现代机器之父之一艾伦·图灵，那就很难。在1950年发表的一篇名为《计算机与智能》的论文中，图灵定义了一个被称为**模仿游戏**（后来称为**图灵测试**）的测试，用以评估机器是否表现出与人类无法区分的行为。这个测试有多个迭代和变体，但本质上，其想法是，在对话中，一个人在任何时候都不会感觉到他们不是在与人类交谈。
- en: Certainly, this test is flawed, as there are ways to give relatively intelligent
    answers to questions while not being intelligent at all. If you want to learn
    more about this, have a look at **ELIZA** built by Joseph Weizenbaum, which passed
    the Turing test.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个测试是有缺陷的，因为有一些方法可以在不真正智能的情况下给出相对智能的答案。如果你想了解更多关于这个的信息，可以看看约瑟夫·魏森鲍姆建造的**ELIZA**，它通过了图灵测试。
- en: Nevertheless, this paper triggered one of the first discussions on what AI could
    be and what it means that a machine can learn.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这篇论文引发了关于人工智能可能是什么以及机器学习意味着什么的第一次讨论。
- en: Living in these exciting times, Arthur Samuel, a researcher working at **International
    Business Machines Corporation** (**IBM**) at that time, started developing a computer
    program that could make the right decisions in a game of checkers. In each move,
    he let the program evaluate a scoring function that tried to measure the chances
    of winning for each available move. Limited by the available resources at the
    time, it was not feasible to calculate all possible combinations of moves all
    the way to the end of the game.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些激动人心的时代生活，亚瑟·塞缪尔，当时在**国际商业机器公司**（**IBM**）工作的研究人员，开始开发一个能够在国际象棋游戏中做出正确决策的计算机程序。在每一步中，他让程序评估一个评分函数，试图衡量每个可用步骤的获胜机会。由于当时可用的资源有限，计算所有可能的移动组合直到游戏结束是不切实际的。
- en: This first step led to the definition of the so-called **minimax algorithm**
    and its accompanying **search tree**, which can commonly be used for any two-player
    adversarial game. Later, the **alpha-beta pruning** algorithm was added to automatically
    trim the tree from decisions that did not lead to better results than the ones
    already evaluated.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这第一步导致了所谓的**最小-最大算法**及其伴随的**搜索树**的定义，这通常可以用于任何两人对抗游戏。后来，添加了**alpha-beta剪枝**算法，以自动从那些没有比已评估的结果更好的决策中剪枝。
- en: 'We are talking about Arthur Samuel, as it was he who coined the name *machine
    learning*, defining it as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在谈论Arthur Samuel，因为正是他提出了“机器学习”这个名称，并将其定义为如下：
- en: The field of study that gives computers the ability to learn without being explicitly
    programmed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 研究领域赋予计算机在不被明确编程的情况下学习的能力。
- en: Combining these first ideas of building an evaluation function for training
    a machine and the research done by Donald O. Hebb in neuroscience, Frank Rosenblatt,
    a researcher at the Cornell Aeronautical Laboratory, invented a new linear classifier
    that he called a **perceptron**. Even though his progress in building this perceptron
    into hardware was relatively short-lived and would not live up to its potential,
    its original definition is nowadays the basis for every neuron in an **artificial
    neural network** (**ANN**).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结合为训练机器构建评估函数的第一个想法和Donald O. Hebb在神经科学领域的研究，Cornell Aeronautical Laboratory的研究员Frank
    Rosenblatt发明了一种新的线性分类器，他称之为**感知器**。尽管他在将这个感知器构建成硬件方面的进展相对短暂，并且没有达到其潜力，但它的原始定义如今是**人工神经网络**（**ANN**）中每个神经元的基石。
- en: Therefore, let's now dive deeper into understanding how ANNs work and what we
    can deduce about the inner workings of an ML algorithm from them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在让我们更深入地了解ANN是如何工作的，以及我们可以从它们中推断出关于ML算法内部工作原理的什么。
- en: Understanding the inner workings of ML through the example of ANNs
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过ANN的例子理解ML的内部工作原理
- en: 'ANNs, as we know them today, are defined by the following two major components,
    one of which we learned about already:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天所知的ANN由以下两个主要组件定义，其中一个我们已经了解过：
- en: '**The neural network**: The base structure of the system. A perceptron is basically
    an NN with only one neuron. By now, this structure comes in multiple facets, often
    involving hidden layers of hundreds of neurons, in the case of **deep neural networks**
    (**DNNs**).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络**：系统的基本结构。感知器基本上是一个只有一个神经元的NN。到目前为止，这种结构已经以多种形式出现，通常涉及数百个神经元的隐藏层，在**深度神经网络**（**DNNs**）的情况下。'
- en: '**The backpropagation function**: A rule for the system to learn and evolve.
    An idea thought of in the 1970s came into appreciation through a paper called
    *Learning Representations by Back-Propagating Errors* by *D. Rumelhart*, *Geoffrey
    E. Hinton*, *Ronald J. Williams* in 1986.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播函数**：系统学习和进化的规则。20世纪70年代的一个想法，通过1986年由D. Rumelhart、Geoffrey E. Hinton和Ronald
    J. Williams发表的名为《通过反向传播错误学习表示》的论文而受到重视。'
- en: To understand these two components and how they work in tandem with each other,
    let's have a deeper look at both.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这两个组件以及它们如何协同工作，让我们更深入地了解它们。
- en: The neural network
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'First, let''s understand how a single neuron operates, which is very close
    to the idea of a perceptron defined by Rosenblatt. The following diagram shows
    the inner workings of such an artificial neuron:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解单个神经元是如何工作的，这非常接近Rosenblatt定义的感知器理念。以下图表显示了这种人工神经元的内部工作原理：
- en: '![Figure 1.2 – Neuron in an ANN ](img/B17928_01_02.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – ANN中的神经元](img/B17928_01_02.jpg)'
- en: Figure 1.2 – Neuron in an ANN
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – ANN中的神经元
- en: We can clearly see the similarities to a real neuron. We get inputs from the
    connected neurons called ![](img/B17928_Formula_1.02.png). Each of those inputs
    is weighted with a corresponding weight ![](img/B17928_Formula_1.03.png), and
    then, in the neuron itself, they are all summed up, including a **bias** ![](img/B17928_Formula_1.04.png).
    This is often referred to as the **net input function**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到它与真实神经元的相似之处。我们从称为 ![](img/B17928_Formula_1.02.png) 的连接神经元那里获得输入。每个输入都对应一个权重
    ![](img/B17928_Formula_1.03.png)，然后在神经元本身中，它们都被加起来，包括一个**偏差** ![](img/B17928_Formula_1.04.png)。这通常被称为**净输入函数**。
- en: 'As the final operation, a so-called **activation function** ![](img/B17928_Formula_1.05.png)
    is applied to this net input that decides how the output signal of the neuron
    should look. This function must be continuous and differentiable and should typically
    create results in the range of [0:1] or [-1:1] to keep results scaled. In addition,
    this function could be linear or non-linear in nature, even though using a linear
    activation function has its downfalls, as described next:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最终操作，一个所谓的**激活函数** ![](img/B17928_Formula_1.05.png) 应用于这个网络输入，决定神经元的输出信号应该如何看起来。这个函数必须是连续且可微分的，并且通常应该在[0:1]或[-1:1]的范围内创建结果，以保持结果缩放。此外，这个函数可以是线性的或非线性的，尽管使用线性激活函数有其缺点，如以下所述：
- en: You cannot learn a non-linear relationship presented in your data through a
    system of linear functions.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您无法通过线性函数系统学习数据中呈现的非线性关系。
- en: A multilayered network made up of nodes with only linear activation functions
    can be broken down to just one layer of nodes with one linear activation function,
    making the network obsolete.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由只有线性激活函数的节点组成的分层网络可以被简化为只有一个线性激活函数的一层节点，从而使网络变得过时。
- en: You cannot use a linear activation function with backpropagation, as this requires
    calculating the derivative of this function, which we will discuss next.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不能使用线性激活函数进行反向传播，因为这需要计算该函数的导数，我们将在下一节讨论。
- en: 'Commonly used activation functions are **sigmoid**, **hyperbolic tangent**
    (**tanh**), **rectified linear unit** (**ReLU**), and **softmax**. Keeping this
    in mind, let''s have a look at how we connect neurons together to achieve an ANN.
    A whole network is typically defined by three types of layers, as outlined here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的激活函数包括**sigmoid**、**双曲正切**（**tanh**）、**整流线性单元**（**ReLU**）和**softmax**。牢记这一点，让我们来看看我们如何连接神经元以实现人工神经网络（ANN）。一个整个网络通常由三种类型的层定义，如下所述：
- en: '**Input layer**: Consists of neurons accepting singular input signals (not
    a weighted sum) to the network. Their weights might be constant or randomized
    depending on the application.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：由接受网络单个输入信号的神经元组成（不是加权求和）。它们的权重可能根据应用是常数或随机化的。'
- en: '**Hidden layer**: Consists of the types of neurons we described before. They
    are defined by an activation function and given weights to the weighted sum of
    the input signals. In DNNs, these layers typically represent specific transformation
    steps.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：由我们之前描述的神经元类型组成。它们由一个激活函数定义，并给输入信号的加权求和分配权重。在深度神经网络（DNNs）中，这些层通常代表特定的转换步骤。'
- en: '**Output layer**: Consists of neurons performing the final transformation of
    the data. They can behave like neurons in hidden layers, but they do not have
    to.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：由执行数据最终转换的神经元组成。它们可以像隐藏层中的神经元一样行为，但不必如此。'
- en: 'These together result in a typical ANN, as shown in the following diagram:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些共同构成了典型的ANN，如下面的图所示：
- en: '![Figure 1.3 – ANN with one hidden layer ](img/B17928_01_03.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3 – 具有一个隐藏层的人工神经网络](img/B17928_01_03.jpg)'
- en: Figure 1.3 – ANN with one hidden layer
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 具有一个隐藏层的人工神经网络
- en: 'With this, we build a generic structure that can receive some input, realize
    some form of mathematical function through different layers of weights and activation
    functions, and in the end, hopefully show the correct output. This process of
    pushing information through the network from inputs to outputs is typically referred
    to as **forward propagation**. This, of course, only shows us what is happening
    with an input that passes through the network. The following question remains:
    *How does it learn the desired function in the first place?* The next section
    will answer this question.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们构建了一个通用的结构，它可以接收一些输入，通过不同层的权重和激活函数实现某种形式的数学函数，最终，希望显示正确的输出。这个过程通常被称为**前向传播**。当然，这只能显示通过网络的输入所发生的事情。以下问题仍然存在：*它最初是如何学习所需函数的？*下一节将回答这个问题。
- en: The backpropagation function
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播函数
- en: 'The question that should have popped up in your mind by now is: *How do we
    define the correct output?* To have a way to change the behavior of the network,
    which mostly boils down to changing the values of the weights in the system, don''t
    we need a way to quantize the error the system made?'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该已经想到一个问题：*我们如何定义正确的输出？*为了有一种方式来改变网络的行为，这主要归结为改变系统中权重的值，我们不需要一种量化系统所犯错误的方法吗？
- en: Therefore, we need a function describing the error or loss, referred to as a
    **loss function** or **error function**. You might have even heard another name—a
    **cost function**. Let's define them next.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要一个描述错误或损失的函数，称为 **损失函数** 或 **误差函数**。你可能甚至听说过另一个名字——**代价函数**。让我们接下来定义它们。
- en: Loss Function versus Cost Function
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数与代价函数
- en: A loss function (error function) computes the error for a single training example.
    A cost function, on the other hand, averages all loss function results for the
    entire training dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数（误差函数）计算单个训练示例的误差。另一方面，代价函数平均整个训练数据集的所有损失函数结果。
- en: This is the correct definition for those terms, but they are often used interchangeably.
    Just keep in mind that we are using some form of metric to measure the error we
    made or the distance we have from the correct results.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这些术语的正确定义，但它们通常可以互换使用。只需记住，我们正在使用某种形式的度量来衡量我们犯的错误或与正确结果的距离。
- en: In classic backpropagation and other ML scenarios, the **mean squared error**
    (**MSE**) between the correct ![](img/B17928_Formula_1.06.png) and the computed
    ![](img/B17928_Formula_1.07.png) is used to define the error or loss of the operation.
    The obvious target is to now minimize this error. Therefore, the actual task to
    perform is to find the total minimum of this function in *n*-dimensional space.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的反向传播和其他机器学习场景中，正确 ![](img/B17928_Formula_1.06.png) 和计算得到的 ![](img/B17928_Formula_1.07.png)
    之间的 **均方误差** (**MSE**) 用于定义操作的错误或损失。显然的目标是现在最小化这个错误。因此，实际要执行的任务是在 *n*-维空间中找到这个函数的总最小值。
- en: To do this, we use something that is often referred to as an **optimizer**,
    defined next.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们使用通常被称为 **优化器** 的东西，定义如下。
- en: Optimizer (Objective Function)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器（目标函数）
- en: An optimizer is a function that implements a specific way to reach the objective
    of minimizing the cost function.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是一个实现达到最小化代价函数目标特定方式的函数。
- en: 'One such optimizer is an iterative process called **gradient descent**. Its
    idea is visualized in the following screenshot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种优化器是一个称为 **梯度下降** 的迭代过程。其思想在以下屏幕截图中进行可视化：
- en: '![Figure 1.4 – Gradient descent with loss function influenced by only one input
    (left: finding global minimum, right: stuck in local minimum) ](img/B17928_01_04.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – 仅受一个输入影响的梯度下降损失函数（左：寻找全局最小值，右：陷入局部最小值）](img/B17928_01_04.jpg)'
- en: 'Figure 1.4 – Gradient descent with loss function influenced by only one input
    (left: finding global minimum, right: stuck in local minimum)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 仅受一个输入影响的梯度下降损失函数（左：寻找全局最小值，右：陷入局部最小值）
- en: In gradient descent, we try to navigate an *n*-dimensional loss function by
    taking reasonably large enough steps, often defined by a *learning rate*, with
    the goal to find the global minimum, while avoiding getting stuck in a local minimum.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降中，我们通过采取合理足够大的步长（通常由 **学习率** 定义），试图导航 *n*-维损失函数，目标是找到全局最小值，同时避免陷入局部最小值。
- en: 'Keeping this in mind and without going into too much detail, let''s finish
    this thought by going through the steps the backpropagation algorithm performs
    on the neural network. These are set out here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这一点，不深入细节，让我们通过回顾反向传播算法在神经网络中执行的步骤来完成这个想法。这些步骤如下：
- en: Pass a pair ![](img/B17928_Formula_1.08.png) through the network (forward propagation).
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一对 ![](img/B17928_Formula_1.08.png) 通过网络（前向传播）。
- en: Compute the loss between the expected ![](img/B17928_Formula_1.09.png) and the
    computed ![](img/B17928_Formula_1.10.png).
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算预期 ![](img/B17928_Formula_1.09.png) 和计算得到的 ![](img/B17928_Formula_1.10.png)
    之间的损失。
- en: Compute all derivatives for all functions and weights throughout the layers
    using a mathematical chain rule.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数学链式法则计算所有函数和权重在所有层中的所有导数。
- en: Update all weights beginning from the back of the network to the front, with
    slightly changed weights defined by the optimizer.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从网络的后面开始更新所有权重到前面，使用优化器定义的略微改变的权重。
- en: Repeat until convergence is achieved (the weights are not receiving any meaningful
    updates anymore).
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行，直到达到收敛（权重不再接收任何有意义的更新）。
- en: This is, in a nutshell, how an ANN learns. Be aware that it is vital to constantly
    change the pairs in *Step 1*, as otherwise, you might push the network too far
    into memorizing these couple of pairs you constantly showed it. We will discuss
    the phenomenon of **overfitting** and **underfitting** later in this chapter.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这就是人工神经网络（ANN）是如何学习的。请注意，在第一步中不断改变对是至关重要的，否则，你可能会使网络过度记住你不断展示给它的这些几个对。我们将在本章后面讨论**过拟合**和**欠拟合**的现象。
- en: As a final step in this section, let's now bring together what we have learned
    so far about ML and what this means for building software solutions in the future.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本节的最后一步，现在让我们将到目前为止关于机器学习所学到的东西以及这对未来构建软件解决方案意味着什么结合起来。
- en: ML and Software 2.0
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习和软件2.0
- en: What we learned so far is that ML seems to be defined by a base structure with
    various knobs and levers (settings and values) that can be changed. In the case
    of ANNs, that would be the structure of the network itself and the weights, bias,
    and activation function we can set in some regard.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所学的似乎表明机器学习是由一个具有各种旋钮和杠杆（设置和值）的基础结构定义的，这些旋钮和杠杆可以改变。在人工神经网络（ANN）的情况下，这将是网络本身的结构以及我们可以设置的一些权重、偏置和激活函数。
- en: Accompanying this base structure is some sort of rule or function as to how
    these knobs and levers should be transformed through a learning process. In the
    case of ANNs, this is defined through the backpropagation function, which combines
    a loss function with an optimizer and some math.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与这个基础结构相伴的是某种规则或函数，用于在学习过程中如何将这些旋钮和杠杆转换。在人工神经网络（ANN）的情况下，这是通过反向传播函数定义的，该函数结合了损失函数、优化器和一些数学。
- en: In 2017, Andrej Karpathy, the **chief technical officer** (**CTO**) of Tesla's
    AI division, proposed that the aforementioned idea could be just another way of
    programming, which he called **Software 2.0** ([https://karpathy.medium.com/software-2-0-a64152b37c35](https://karpathy.medium.com/software-2-0-a64152b37c35)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，特斯拉人工智能部门的首席技术官（CTO）安德烈·卡帕西（Andrej Karpathy）提出，上述想法可能是编程的另一种方式，他称之为**软件2.0**（[https://karpathy.medium.com/software-2-0-a64152b37c35](https://karpathy.medium.com/software-2-0-a64152b37c35)）。
- en: Up to this point, writing software was about explaining to the machine precisely
    what it must do and what outcome it must produce through defining specific commands
    it had to follow. In this classical software development paradigm, we define algorithms
    by their code and let data run through it, typically written in a reasonably readable
    language.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，编写软件是关于通过定义它必须遵循的特定命令，向机器精确地解释它必须做什么以及它必须产生什么结果。在这种经典的软件开发范例中，我们通过代码定义算法，让数据通过它运行，通常是用一种相对可读的语言编写的。
- en: Instead of doing that, another idea could be to define a program we build by
    a base structure, a way to evolve this structure, and the type of data it must
    process. In this case, we get something very human-unfriendly to understand (an
    ANN with weights, for example), but it might be much better to understand for
    a machine.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是这样做，另一个想法可能是定义一个由基础结构、进化这种结构的方式以及它必须处理的数据类型组成的程序。在这种情况下，我们得到的是对人类非常不友好的东西（例如具有权重的ANN），但对于机器来说可能更容易理解。
- en: So, we leave you at the end of this section with the thought that Andrej wanted
    to convey. Perhaps ML is just another form of programming machines.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在本节的结尾留下安德烈想要传达的思想。也许机器学习只是编程机器的另一种形式。
- en: Keeping all this in mind, let's now talk about math.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在记住所有这些的同时，现在让我们谈谈数学。
- en: Understanding the mathematical basis for statistical analysis and ML modeling
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解统计分析和机器学习建模的数学基础
- en: Looking at what we have learned so far, it becomes abundantly clear that ML
    requires an ample understanding of mathematics. We already came across multiple
    mathematical functions we have to handle. Think about the activation function
    of neurons and the optimizer and loss functions for training. On top of that,
    we have not talked about the second aspect of our new programming paradigm—the
    data!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 看看我们到目前为止学到的东西，很明显，机器学习需要充分理解数学。我们已经遇到了多个我们必须处理的数学函数。想想神经元的激活函数和训练的优化器以及损失函数。除此之外，我们还没有谈到我们新编程范式的第二个方面——数据！
- en: 'To choose the right ML algorithm and derive a good metric for a loss function,
    we have to take apart the data points we work with. In addition, we need to bring
    in the data points in relation to the domain we are working with. Therefore, when
    defining the role of a data scientist, you will often find a visual like this
    one:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择正确的机器学习算法并为损失函数推导出良好的指标，我们必须分析我们所处理的数据点。此外，我们还需要将数据点与我们所工作的领域联系起来。因此，在定义数据科学家的角色时，你经常会看到这样的视觉元素：
- en: '![Figure 1.5 – Requirements for data scientists ](img/B17928_01_05.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – 数据科学家所需条件](img/B17928_01_05.jpg)'
- en: Figure 1.5 – Requirements for data scientists
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 数据科学家所需条件
- en: In this section, we will concentrate on what is referred to in *Figure 1.5*
    as *statistical research*. We will understand why we need statistics and what
    base information we can derive from a given dataset, learn what bias is and ways
    to avoid that, mathematically classify possible ML algorithms, and finally, discuss
    how we choose useful metrics to define the performance of our trained models.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将专注于*图1.5*中提到的*统计研究*。我们将了解为什么我们需要统计学，以及我们可以从给定数据集中推导出哪些基本信息，学习什么是偏差以及如何避免它，从数学上对可能的机器学习算法进行分类，最后讨论我们如何选择有用的指标来定义我们训练模型的性能。
- en: The case for statistics in ML
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习中的统计学案例
- en: 'As we have seen, we require statistics to clean and analyze our given data.
    Therefore, let''s start by asking: *What do we understand from the term "statistics"?*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们需要统计学来清理和分析我们的给定数据。因此，让我们先问一下：*我们从“统计学”这个术语中理解了什么？*
- en: '*Statistics is the science of collecting and analyzing a representative sample
    made up of a large quantity of numerical data with the purpose of inferring the
    statistical distribution of the underlying population.*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*统计学是收集和分析大量数值数据构成的代表性样本的科学，目的是推断底层人群的统计分布。*'
- en: A typical example of something such as this would be the prediction for the
    results of an election you see during the campaign or shortly after voting booths
    close. At those points in time, we do not know the precise result of the full
    **population** but we can acquire a **sample**, sometimes referred to as an **observation**.
    We get that by asking people for responses through a questionnaire. Then, based
    on this subset, we make a sound prediction for the full population by applying
    statistical methods.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的一个典型例子就是在竞选期间或投票站关闭后不久预测选举结果。在那个时间点，我们不知道整个**人口**的精确结果，但我们可以获取一个**样本**，有时也称为**观察值**。我们通过让人们填写问卷来获取这些信息。然后，基于这个子集，我们通过应用统计方法对整个群体做出合理的预测。
- en: 'We learned that in ML, we are trying to let the machine figure out a mathematical
    function that fits our problem, such as this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在机器学习中，我们试图让机器找出适合我们问题的数学函数，例如这个：
- en: '![](img/B17928_Formula_1.11.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17928_Formula_1.11.png)'
- en: Thinking back to our ANN, ![](img/B17928_Formula_1.12.png) would be an input
    vector and ![](img/B17928_Formula_1.13.png) would be the resulting output vector.
    In ML jargon, they are known under a different name, as seen next.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们的人工神经网络（ANN），![](img/B17928_Formula_1.12.png) 将是一个输入向量，而 ![](img/B17928_Formula_1.13.png)
    将是产生的输出向量。在机器学习的术语中，它们有另一个名称，如下所示。
- en: Features and Labels
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 特征和标签
- en: One element of the input vector *x* is called a feature; the full output vector
    is called the label. Often, we only deal with a **one-dimensional** label.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量 *x* 的一个元素被称为特征；完整的输出向量被称为标签。通常，我们只处理一个**一维**标签。
- en: Now, to bring this together, when training an ML model, we typically only have
    a sample of the given world, and as with any other time you are dealing with only
    a sample or subset of reality, you want to pick highly representative features
    and samples of the underlying population.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了将这一点结合起来，在训练机器学习模型时，我们通常只有给定世界的样本，并且正如你在处理现实世界中的任何其他样本或子集时一样，你希望选择高度代表性的特征和底层人群的样本。
- en: So, what does this mean? Let's think of an example. Imagine you want to train
    a small little robot car to be able to automatically drive through a tunnel. First,
    we need to think about what our features and labels in this scenario are. As features,
    we probably need something that measures the distance from the edges of the car
    to the tunnel in each direction, as we probably do not want to drive into the
    sides of the tunnel. Let's assume we have some infrared sensors attached to the
    front, the sides, and the back of the vehicle. Then, the output of our program
    would probably control the steering and the speed of the vehicle, which would
    be our labels.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这究竟意味着什么？让我们来想一个例子。想象一下，你想要训练一辆小型机器人汽车能够自动通过隧道。首先，我们需要考虑在这个场景中我们的特征和标签是什么。作为特征，我们可能需要一些能够测量汽车每个方向上与隧道边缘距离的东西，因为我们可能不希望汽车撞到隧道的侧面。假设我们在车辆的前方、侧面和后方都安装了一些红外传感器。那么，我们程序的输出可能将控制车辆的转向和速度，这些就是我们的标签。
- en: Given that, as a next step, we should think of a whole bunch of scenarios in
    which the vehicle could find itself. This might be a simple scenario of the vehicle
    sitting straight-facing in the tunnel, or it could be a bad scenario where the
    vehicle is nearly stuck in a corner and the tunnel is going left or right from
    that point on. In all these cases, we read out the values of our infrared sensors
    and then do the more complicated tasks of making an educated guess as to how the
    steering has to be changed and how the motor has to operate. Eventually, we end
    up with a bunch of example situations and corresponding actions to take, which
    would be our training dataset. This can then be used to train an ANN so that the
    small car can learn how to follow a tunnel.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 既然如此，作为下一步，我们应该考虑车辆可能遇到的各种场景。这可能是一个简单的场景，车辆直直地停在隧道中，或者它可能是一个糟糕的场景，车辆几乎卡在角落里，从那个点开始，隧道向左或向右延伸。在所有这些情况下，我们读取红外传感器的值，然后进行更复杂的任务，即做出有根据的猜测，关于转向应该如何改变，以及电机应该如何运行。最终，我们得到一系列示例情况和相应的行动，这些将成为我们的训练数据集。然后，我们可以使用这个数据集来训练一个人工神经网络（ANN），这样小汽车就可以学会如何跟随隧道行驶。
- en: If you ever get the opportunity, try to perform this training. If you pick very
    good examples, you will understand the full power of ML, as you will most likely
    see something exciting, which I can attest to. In my setup, even though we never
    had a sample where we would instruct the vehicle to drive backward, the optimal
    function the machine trained had values where the vehicle learned to do exactly
    that.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有机会，尝试进行这项训练。如果你挑选了非常好的例子，你将理解机器学习的全部力量，因为你很可能会看到一些令人兴奋的东西，我可以证实这一点。在我的设置中，尽管我们从未有过一个样本，我们会指示车辆倒车，但机器训练出的最优函数的值表明车辆学会了这样做。
- en: In an example such as that, we would do everything from scratch and hopefully
    take representative samples by ourselves. In most cases you will encounter, the
    dataset already exists, and you need to figure out whether it is representative
    or whether we need to introduce additional data to achieve an optimal training
    result.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的例子中，我们会从头开始做所有的事情，并希望我们自己能够抽取有代表性的样本。在大多数情况下，你将遇到的，数据集已经存在，你需要弄清楚它是否具有代表性，或者我们需要引入额外的数据来实现最佳的训练结果。
- en: Therefore, let's have a look at some statistical properties you should familiarize
    yourself with.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们看看一些你应该熟悉的统计特性。
- en: Basics of statistics
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计学基础
- en: We now understand that we need to be able to analyze the statistical properties
    of single features, derive their distribution, and analyze their relationship
    with other features and labels in the dataset.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们明白，我们需要能够分析单个特征的统计特性，推导出它们的分布，并分析它们与数据集中其他特征和标签的关系。
- en: Let's start with the properties of single features and their distribution. All
    the following operations require numerical data. This means that if you work with
    categorical data or something such as media files, you need to transform them
    into some form of numerical representation to get such results.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从单个特征的性质及其分布开始。以下所有操作都需要数值数据。这意味着如果你处理的是分类数据或类似媒体文件这样的东西，你需要将它们转换成某种数值表示形式，才能得到这样的结果。
- en: 'The following screenshot shows the main statistical properties you are after,
    their importance, and how you can calculate them:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了您所追求的主要统计特性、它们的重要性以及如何计算它们：
- en: '![Figure 1.6 – List of major statistical properties ](img/B17928_01_06.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图1.6 – 主要统计特性列表](img/B17928_01_06.jpg)'
- en: Figure 1.6 – List of major statistical properties
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – 主要统计性质列表
- en: From here onward, we can make the reasonable assumption that the underlying
    stochastic process follows a **normal distribution**. Be aware that this must
    not be the case, and therefore you should make yourself comfortable with other
    distributions (see [https://www.itl.nist.gov/div898/handbook/eda/section3/eda36.htm](https://www.itl.nist.gov/div898/handbook/eda/section3/eda36.htm)).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们可以合理地假设基础随机过程遵循**正态分布**。请注意，这并不一定成立，因此你应该熟悉其他分布（参见[https://www.itl.nist.gov/div898/handbook/eda/section3/eda36.htm](https://www.itl.nist.gov/div898/handbook/eda/section3/eda36.htm)）。
- en: 'The following screenshot shows a visual representation of a standard normal
    distribution:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图展示了一个标准正态分布的直观表示：
- en: '![Figure 1.7 – Standard normal distribution and its properties ](img/B17928_01_07.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – 标准正态分布及其性质](img/B17928_01_07.jpg)'
- en: Figure 1.7 – Standard normal distribution and its properties
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 标准正态分布及其性质
- en: 'Now, the strength of this normal distribution is that, based on the mean ![](img/B17928_Formula_1.14.png)
    and standard deviation ![](img/B17928_Formula_1.15.png), we can make assumptions
    for the probabilities of samples to be in a certain range. As shown in *Figure
    1.7*, there is a probability of around **68.27%** for a value to have a distance
    from the mean of 1![](img/B17928_Formula_1.16.png), **95.45%** for a distance
    of ![](img/B17928_Formula_1.17.png), and **99.73%** for a distance of ![](img/B17928_Formula_1.18.png).
    Based on this, we can ask questions such as this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这种正态分布的强度在于，基于均值![img/B17928_Formula_1.14.png](img/B17928_Formula_1.14.png)和标准差![img/B17928_Formula_1.15.png](img/B17928_Formula_1.15.png)，我们可以对样本落在某个范围内的概率做出假设。如图
    1.7 所示，一个值距离均值的距离为 1![img/B17928_Formula_1.16.png](img/B17928_Formula_1.16.png)的概率约为
    **68.27**%，距离为![img/B17928_Formula_1.17.png](img/B17928_Formula_1.17.png)的概率为
    **95.45**%，距离为![img/B17928_Formula_1.18.png](img/B17928_Formula_1.18.png)的概率为
    **99.73**%。基于此，我们可以提出如下问题：
- en: '*How probable is it to find a value with a distance of 5*![](img/B17928_Formula_1.19.png)
    *from the mean?*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*距离平均值 5*![img/B17928_Formula_1.19.png](img/B17928_Formula_1.19.png)*有多大的可能性找到这样的值？*'
- en: Through questions such as this, we can start assessing whether what we see in
    our data is a statistical anomaly of the distribution, is a value that is simply
    false, or whether our suspected distribution is incorrect. This is done through
    a process called **hypothesis testing**, defined next.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样的问题，我们可以开始评估我们数据中看到的是分布的统计异常、一个简单的错误值，还是我们的怀疑分布是不正确的。这是通过称为**假设检验**的过程来完成的，定义如下。
- en: Hypothesis Testing (Definition)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 假设检验（定义）
- en: This is a method of testing if the so-called null hypothesis ![](img/B17928_Formula_1.20.png)
    is false, typically referring to the current suspected distribution. It means
    that the unlikely observation we encounter is pure chance. This hypothesis is
    rejected in favor of an alternative hypothesis ![](img/B17928_Formula_1.21.png),
    if the probability falls below a predefined significance level (typically higher
    than ![](img/B17928_Formula_1.22.png)/lower than 5%). The alternative hypothesis
    thus presumes that the observation we have is due to a real effect that is not
    taken into account in the initial distribution.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种测试所谓的**零假设**![img/B17928_Formula_1.20.png](img/B17928_Formula_1.20.png)是否为假的方法，通常指的是当前怀疑的分布。这意味着我们遇到的罕见观察结果是纯粹的偶然。如果概率低于预定义的显著性水平（通常高于![img/B17928_Formula_1.22.png](img/B17928_Formula_1.22.png)/低于
    5%），则拒绝该假设，转而采用备择假设![img/B17928_Formula_1.21.png](img/B17928_Formula_1.21.png)。因此，备择假设假定我们所观察到的结果是由于一个在初始分布中没有考虑到的真实效应。
- en: We will not go into further details on how to perform this test properly, but
    we urge you to familiarize yourself with this process thoroughly.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入讨论如何正确执行这项测试的细节，但我们敦促你彻底熟悉这个过程。
- en: 'What we will talk about is the types of errors you can make in this process,
    as shown in the following screenshot:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的是在这个过程中可能犯的错误类型，如下面的截图所示：
- en: '![Figure 1.8 – Type I and Type II errors ](img/B17928_01_08.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 – 一类错误和二类错误](img/B17928_01_08.jpg)'
- en: Figure 1.8 – Type I and Type II errors
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – 一类错误和二类错误
- en: 'We define the errors you see in *Figure 1.8* as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 *图 1.8* 中所见的错误定义为如下：
- en: '**Type I error**: This denotes that we reject the hypothesis ![](img/B17928_Formula_1.23.png)
    and the underlying distribution, even though it is correct. This is also referred
    to as a **false-positive** result or an **alpha error**.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一类错误**：这表示我们拒绝了假设![img/B17928_Formula_1.23.png](img/B17928_Formula_1.23.png)和基础分布，尽管它是正确的。这也被称为**假阳性**结果或**α
    错误**。'
- en: '**Type II error**: This denotes that we do not reject the hypothesis ![](img/B17928_Formula_1.24.png)
    and the underlying distribution, even though ![](img/B17928_Formula_1.25.png)
    is correct. This error is also referred to as a **false-negative** result or a
    **beta** **error**.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二类错误**：这表示我们没有拒绝假设 ![](img/B17928_Formula_1.24.png) 和其背后的分布，尽管 ![](img/B17928_Formula_1.25.png)
    是正确的。这种错误也被称为**假阴性**结果或**贝塔** **错误**。'
- en: You might have heard the term *false positive* before. Often, it comes up when
    you take a medical test. A false positive would denote that you have a positive
    result from a test, even though you do not have the disease you are testing for.
    As a medical test is also a **stochastic process**, as with nearly everything
    else in our world, the term is correctly used in this scenario.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能之前听说过**假阳性**这个术语。它通常出现在你进行医学检查时。假阳性表示你在测试中得到了阳性结果，尽管你并没有你正在测试的疾病。由于医学检查也是一个**随机过程**，就像我们世界中的几乎所有其他事物一样，这个术语在这个场景中被正确使用。
- en: At the end of this section, when we talk about errors and metrics in ML model
    training, we will come back to these definitions. As a final step, let's discuss
    relationships among features and between features and labels. Such a relationship
    is referred to as a **correlation**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的最后，当我们谈到机器学习模型训练中的错误和度量时，我们将回到这些定义。作为最后一步，让我们讨论特征之间的关系以及特征和标签之间的关系。这种关系被称为**相关性**。
- en: 'There are multiple ways to calculate a correlation between two vectors ![](img/B17928_Formula_1.26.png)
    and ![](img/B17928_Formula_1.27.png), but what they all have in common is that
    their results will fall in the range of [-1,1]. The result of this operation can
    be broadly defined by the following three categories:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以计算两个向量 ![](img/B17928_Formula_1.26.png) 和 ![](img/B17928_Formula_1.27.png)
    之间的相关性，但它们共同的特点是，它们的结果将落在 [-1,1] 的范围内。这个操作的结果可以大致分为以下三个类别：
- en: '**Negatively correlated**: The result leans toward -1\. When the value of vector
    ![](img/B17928_Formula_1.28.png) rises, the values of vector ![](img/B17928_Formula_1.29.png)
    fall and vice versa.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负相关**：结果偏向于 -1。当向量 ![](img/B17928_Formula_1.28.png) 的值上升时，向量 ![](img/B17928_Formula_1.29.png)
    的值下降，反之亦然。'
- en: '**Uncorrelated**: The result leans toward 0\. There is no real interaction
    between vectors ![](img/B17928_Formula_1.30.png) and ![](img/B17928_Formula_1.31.png).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不相关**：结果偏向于 0。向量 ![](img/B17928_Formula_1.30.png) 和 ![](img/B17928_Formula_1.31.png)
    之间没有真正的交互作用。'
- en: '**Positively correlated**: The result leans toward 1\. When the value of vector
    ![](img/B17928_Formula_1.32.png) rises, the values of vector ![](img/B17928_Formula_1.33.png)
    rise and vice versa.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正相关**：结果偏向于 1。当向量 ![](img/B17928_Formula_1.32.png) 的值上升时，向量 ![](img/B17928_Formula_1.33.png)
    的值也上升，反之亦然。'
- en: Through this, we can get an idea of relationships between data points, but please
    be aware of the differences between causation and correlation, as outlined next.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以了解数据点之间的关系，但请注意因果和相关性之间的区别，如下所述。
- en: Causation versus Correlation
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因果关系与相关性
- en: Even if two vectors are correlated with each other, it does not mean one of
    them is the cause of the other one—it simply means that one of them influences
    the other one. It is not causation as we probably don't see the full picture and
    every single influencing factor.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 即使两个向量相互关联，这并不意味着其中一个向量是另一个的原因——它仅仅意味着其中一个影响了另一个。这不是因果关系，因为我们可能看不到完整的图景和每一个影响因素。
- en: The mathematical theory we discussed so far should give you a good basis to
    build upon. In the next section, we will have a quick look at what kinds of errors
    we can make when taking samples, typically referred to as the bias in the data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止讨论的数学理论应该为你提供一个良好的基础来构建。在下一节中，我们将快速查看在采样过程中可能犯的错误类型，通常被称为数据的偏差。
- en: Understanding bias
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解偏差
- en: At any stage of taking samples and when working with data, it is easily possible
    to introduce what is called **bias**. Typically, this influences the sampling
    quality and therefore has a big impact on any ML model we would like to fit to
    the data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何采样阶段和数据处理过程中，很容易引入所谓的**偏差**。通常，这会影响采样质量，因此对我们想要拟合数据的任何机器学习模型都有重大影响。
- en: 'One example would be the *causation versus correlation* we just discussed.
    Seeing causation where none exists can have consequences in terms of the way you
    continue processing the data points. Other prominent biases that influence data
    are shown next:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子就是我们刚才讨论的*因果关系与相关性*。在没有因果关系的地方看到因果关系，可能会对你的数据处理方式产生后果。接下来展示的是影响数据的其他显著偏差：
- en: '**Selection bias**: This bias happens when samples are taken that are not representative
    of the real-life distribution of data. This is the case when randomization is
    not properly done or when only a certain subgroup is selected for a study—for
    example, when a questionnaire about city planning is only given out to people
    in half of the neighborhoods of the city.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择偏差**：这种偏差发生在样本不是真实数据分布的代表性样本时。这是在随机化没有正确进行或只选择某个特定子群体进行研究时的情况——例如，当关于城市规划的调查问卷只发放给城市一半地区的居民时。'
- en: '**Funding bias**: This bias should be very well known and happens when a study
    or data project is funded by a sponsor and the results will therefore have a tendency
    toward the interests of the funding party.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资助偏差**：这种偏差应该非常为人所知，发生在一项研究或数据项目由赞助商资助，因此结果将倾向于资助方的利益。'
- en: '**Reporting bias**: This bias happens when only a selection of outcomes is
    represented in a dataset due to the fact that it is the tendency of people to
    underreport certain outcomes. Examples of this are given here: when you report
    bad weather events but not when there is sunshine; when you write negative reviews
    for a product but not positive reviews; when you only know about results written
    in your own language or from your own region but not from others.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**报告偏差**：这种偏差发生在由于人们倾向于低估某些结果，导致数据集中只代表了一部分结果。这里给出了一些例子：当你报告恶劣天气事件但不报告晴天时；当你为产品写负面评论但不写正面评论时；当你只了解用你自己的语言或来自你自己的地区的结果，但不了解其他地区的结果时。'
- en: '**Observer bias/confirmation bias**: This bias happens when someone favors
    results that confirm or support their own beliefs and values. Typically, this
    results in ignoring contrary information, not following the agreed guideline,
    or using ambiguous studies that support the existing preconceived opinion. The
    dangerous part here is that this can happen unconsciously.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察者偏差/确认偏差**：这种偏差发生在某人倾向于支持或证实他们自己的信念和价值观的结果。通常，这会导致忽视相反的信息，不遵循已同意的指南，或使用支持现有先入为主的观点的模糊研究。这里危险的部分是，这可能是无意识的。'
- en: '**Exclusion bias**: This bias happens when you remove data points during preprocessing
    that you consider irrelevant but are not. This includes removing null values,
    outliers, or other special data points. The removal might result in the loss of
    accuracy concerning the underlying real-life distribution.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排除偏差**：这种偏差发生在预处理过程中，你移除了你认为无关但实际并非无关的数据点。这包括移除空值、异常值或其他特殊数据点。这种移除可能会导致关于潜在现实分布的准确度下降。'
- en: '**Automation bias**: This bias happens when you favor results generated from
    automated systems over information taken from humans, even if they are correct.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化偏差**：这种偏差发生在你更倾向于来自自动化系统的结果，而不是来自人类的信息，即使它们是正确的。'
- en: '**Overgeneralization bias**: This bias happens when you project a property
    of your dataset toward the whole population. An example would be that you would
    assume that all cats have gray fur because in the large dataset you have, this
    is true.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过度泛化偏差**：这种偏差发生在你将数据集的一个属性投射到整个群体上。一个例子是，你会假设所有猫都有灰色毛发，因为在你的大型数据集中，这是真的。'
- en: '**Group attribution bias**: This bias happens when stereotypes are added as
    attributes to a whole group because of the actions of a few individuals within
    that group.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**群体归因偏差**：这种偏差发生在由于该群体中少数个体的行为，而将刻板印象作为属性添加到整个群体中。'
- en: '**Survivorship bias**: This bias happens when you focus on successful examples
    while completely ignoring failures. An example would be that you study the competition
    of your company while ignoring all companies that failed, merged, or went bankrupt.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幸存者偏差**：这种偏差发生在你专注于成功的例子，而完全忽略失败的情况。一个例子是，你在研究你公司的竞争时，却忽略了所有失败、合并或破产的公司。'
- en: This list should give you a good understanding of problems that may arise when
    gathering and processing data. We can only urge you to read further into this
    topic while following these next guidelines.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表应该能让你对在收集和处理数据时可能出现的各种问题有一个很好的理解。我们只能敦促你进一步阅读这个主题，同时遵循以下指南。
- en: Guidance for Handling Bias in Data
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据偏差的指导
- en: When using existing datasets, figure out the circumstances in which they were
    obtained to be able to judge their quality. When processing data either alone
    or in a team, define clear guidelines on how you define data and how you handle
    certain situations, and always reflect whether you are making assumptions based
    on your own predispositions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用现有数据集时，找出它们被获取的情境，以便能够判断它们的质量。在单独或团队处理数据时，明确界定如何定义数据和如何处理特定情况，并始终反思你是否是在根据自己的先入之见做出假设。
- en: To solidify your understanding that things are—most of the time—not as they
    seem, have a look at what is referred to as **Simpson's paradox** and the corresponding
    **University of California** (**UC**) Berkeley case ([http://corysimon.github.io/articles/simpsons-paradox/](http://corysimon.github.io/articles/simpsons-paradox/)).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固你对事物——大多数时候——并不像它们看起来那样的理解，看看被称为**辛普森悖论**及其相应的**加州大学伯克利分校**（**UC**）案例（[http://corysimon.github.io/articles/simpsons-paradox/](http://corysimon.github.io/articles/simpsons-paradox/)）。
- en: Now that we have a good understanding of what to look out for when working with
    data, let's come back to the basics of ML.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地理解了在处理数据时需要注意的事项，让我们回到机器学习的基本概念。
- en: Classifying ML algorithms
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习算法的分类
- en: 'In the first section of this chapter, we got a glimpse into ANNs. These are
    special in the sense that they can be used in a so-called supervised or unsupervised
    training setup. To understand what is meant by this, let''s define the current
    three major types of ML algorithms, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一节中，我们简要了解了人工神经网络（ANNs）。它们在以下方面很特别，即它们可以用于所谓的监督或无监督训练设置。为了理解这里的含义，让我们定义当前三种主要的机器学习算法类型，如下所示：
- en: '**Supervised learning**: In supervised learning, models are trained with a
    so-called labeled dataset. That means besides knowing the input for the required
    algorithm, we also know the required output. This type of learning is split into
    two groups of problems—namely, **classification problems** and **regression problems**.
    Classification works with discrete results, where the output is a class or group,
    while regression works with continuous results, where the output would be a certain
    value. Examples of classification would be identifying fraud in money transactions
    or doing object detection in images. Examples of regression would be forecasting
    prices for houses or the stock market or predicting population growth. It is important
    to understand that this type of learning *requires* labels, which often results
    in the tedious task of labeling the whole dataset.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：在监督学习中，模型是用所谓的标记数据集训练的。这意味着除了知道所需算法的输入外，我们还知道所需的输出。这种学习分为两个问题组——即**分类问题**和**回归问题**。分类使用离散结果，输出是一个类别或组，而回归使用连续结果，输出将是某个特定值。分类的例子包括在货币交易中识别欺诈或对图像进行目标检测。回归的例子包括预测房价或股市或预测人口增长。重要的是要理解这种学习**需要**标签，这通常会导致标记整个数据集的繁琐任务。'
- en: '**Unsupervised learning**: In unsupervised learning, models are trained on
    unlabeled data. This is basically self-organized learning to find patterns in
    data, referred to as **clustering**. Examples of this would be the filtering of
    spam emails in an inbox or the recommendation of movies or clothing a person might
    like to watch or purchase. Often, the learning algorithms are used in a real-time
    scenario where the data needs to be processed directly. The beauty of this type
    of learning is that we do not have to label the dataset.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：在无监督学习中，模型是在未标记的数据上训练的。这基本上是一种自我组织的学习，用于在数据中寻找模式，被称为**聚类**。这种方法的例子包括在收件箱中过滤垃圾邮件或推荐某人可能喜欢观看或购买的电影或服装。通常，学习算法在需要直接处理数据的实时场景中使用。这种类型学习的优点是我们不需要标记数据集。'
- en: '**Reinforcement learning**: In reinforcement learning, algorithms learn by
    reacting to a given environment on their own. The idea of this comes from how
    we as humans learn as we grow up. We did a certain action, and the outcome of
    that action was either good or bad or somewhere in between. We then either receive
    some sort of reward or we don''t. Another similar example would be the way you
    would train a dog to behave. Technically, this is realized through a so-called
    *agent* that is guided by a *policy map*, deciding the probability to take actions
    when in a specific state. For the environment itself, we define a so-called *state-value
    function* that returns the *value* of being in a specific state. Good examples
    of this type of learning are training navigation control for a robot or an AI
    opponent for a game.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**：在强化学习中，算法通过对自己所处环境的反应来学习。这种想法来源于我们作为人类在成长过程中学习的方式。我们采取了一定的行动，该行动的结果要么是好的，要么是坏的，要么介于两者之间。然后我们可能会收到某种奖励，也可能不会。另一个类似的例子是训练狗的行为方式。技术上，这是通过所谓的
    *代理* 实现的，该代理由 *策略图* 引导，决定在特定状态下采取行动的概率。对于环境本身，我们定义一个所谓的 *状态值函数*，它返回特定状态的价值。这种类型学习的良好例子包括训练机器人的导航控制或游戏的
    AI 对手。'
- en: 'The following diagram provides an overview of the discussed ML types and the
    corresponding algorithms that are utilized in those areas:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表概述了所讨论的机器学习类型及其在这些领域中使用的相应算法：
- en: '![Figure 1.9 – Types of ML algorithms ](img/B17928_01_09.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9 – 机器学习算法类型](img/B17928_01_09.jpg)'
- en: Figure 1.9 – Types of ML algorithms
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 机器学习算法类型
- en: A detailed overview of many of the prominent ML algorithms can be found on the
    *scikit-learn* web page ([https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)),
    which is one of the major Python libraries for ML.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 许多显著的机器学习算法的详细概述可以在 *scikit-learn* 网页上找到 ([https://scikit-learn.org/stable/](https://scikit-learn.org/stable/))，这是主要的
    Python 机器学习库之一。
- en: Now that we have an idea of the types of training we can perform, let's have
    a short look at what types of results we get from a training run and how to interpret
    them.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了我们可以执行的训练类型，让我们简要地看看从训练运行中获得的结果类型以及如何解释它们。
- en: Analyzing errors and the quality of results of model training
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析模型训练中的错误和结果质量
- en: As we discussed in the first section of this chapter, we require a loss function
    that we can minimize to optimize our training results. Typically, this is defined
    through what is referred to in mathematics as a metric. We need to differentiate
    at this point between metrics that are used to define a loss function and therefore
    used in an optimizer to train the model, and metrics that can be calculated to
    give additional hints toward the performance of the trained model. We will have
    a look at both kinds in this section.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章第一节的讨论中提到的，我们需要一个损失函数，我们可以通过最小化它来优化我们的训练结果。通常，这通过数学中所谓的度量来定义。在此处，我们需要区分用于定义损失函数并因此用于优化器训练模型的度量，以及可以计算出来以提供关于训练模型性能的额外提示的度量。在本节中，我们将探讨这两种类型。
- en: As we have seen when looking at types of ML algorithms, we might work with an
    output represented by continuous data (regression), or we might work with an output
    represented by discrete data (classification).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在查看机器学习算法类型时看到的，我们可能处理由连续数据表示的输出（回归），或者我们可能处理由离散数据表示的输出（分类）。
- en: 'The most prominent loss functions used in regression are **MSE** and **root
    MSE** (**RMSE**). Imagine you try to determine a fitted line for a bunch of samples
    in linear regression. The distance between the line and the sample point in **two-dimensional**
    (**2D**) space is your error. To calculate the RMSE for all data points, you would
    take the expected values ![](img/B17928_Formula_1.34.png) and the predicted values
    ![](img/B17928_Formula_1.35.png) and calculate the following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中，最突出的损失函数是 **均方误差 (MSE**) 和 **均方根误差 (RMSE**)。想象一下，你试图为线性回归中的一组样本确定拟合线。该线与二维空间中样本点之间的距离是你的误差。为了计算所有数据点的
    RMSE，你需要取期望值 ![](img/B17928_Formula_1.34.png) 和预测值 ![](img/B17928_Formula_1.35.png)，并计算以下内容：
- en: '![](img/B17928_Formula_1.36.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.36 – 公式](img/B17928_Formula_1.36.png)'
- en: For classifications, this gets a little bit trickier. In most cases, the model
    can predict the correct class or cannot, making it a binary result. Further, we
    might have a binary classification problem (1 or 0—yes or no), or a multi-class
    problem (cat, dog, horse, and so on).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，这会变得稍微复杂一些。在大多数情况下，模型可以预测正确的类别或者不能，这导致结果是一个二元的结果。更进一步，我们可能有一个二元分类问题（1或0——是或否），或者一个多类别问题（猫、狗、马等等）。
- en: 'For both classification problems, there is a prominent loss function used called
    **cross-entropy loss**. To solve the problem of having a binary result, this loss
    function requires a model that outputs a probability ![](img/B17928_Formula_1.37.png)
    between 0 and 1 for a given data point ![](img/B17928_Formula_1.38.png) and a
    suggested prediction ![](img/B17928_Formula_1.39.png). For a binary classification
    model, it is calculated as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，有一个突出的损失函数被称为**交叉熵损失**。为了解决二元结果的问题，这个损失函数需要一个模型输出一个介于0和1之间的概率 ![img/B17928_Formula_1.37.png]，对于给定的数据点
    ![img/B17928_Formula_1.38.png] 和一个建议的预测 ![img/B17928_Formula_1.39.png]。对于二元分类模型，它的计算如下：
- en: '![](img/B17928_Formula_1.40.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![img/B17928_Formula_1.40.png]'
- en: 'For multi-class classification, we sum up this error for all classes ![](img/B17928_Formula_1.41.png),
    as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类别分类，我们将这个错误对所有类别进行求和 ![img/B17928_Formula_1.41.png]，如下所示：
- en: '![](img/B17928_Formula_1.42.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![img/B17928_Formula_1.42.png]'
- en: If you want to look further into this topic, consider other useful loss functions
    for regression, such as the **absolute error** loss and the **Huber loss** functions
    (used in **support vector machines**, or **SVMs**), useful loss functions for
    binary classification, such as the **hinge loss** function, and useful loss functions
    for multi-class classification, such as the **Kullback-Leibler divergence** (**KL-divergence**)
    function. The last one can also be used in RL as a metric to monitor the policy
    function during training.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进一步了解这个话题，可以考虑其他有用的回归损失函数，例如**绝对误差**损失和**Huber损失**函数（用于**支持向量机**，或**SVMs**），有用的二元分类损失函数，例如**hinge损失**函数，以及有用的多类别分类损失函数，例如**Kullback-Leibler散度**（**KL散度**）函数。最后一个也可以在强化学习（RL）中作为一个指标来监控训练过程中的策略函数。
- en: Everything we have discussed so far requires something we can put into a mathematical
    formula. Imagine working with text files to build a model for **natural language
    processing** (**NLP**). In such a case, we do not have a useful mathematical representation
    for text besides something such as **Unicode**. We will learn in [*Chapter 7*](B17928_07_ePub.xhtml#_idTextAnchor112),
    *Advanced Feature Extraction with NLP*, how to represent it in a useful, vectorized
    manner. Having vectors, we can use a different kind of metric to calculate how
    similar vectors are, called the **cosine similarity** metric, which we will discuss
    in [*Chapter 6*](B17928_06_ePub.xhtml#_idTextAnchor102), *Feature Engineering
    and Labeling*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止讨论的所有内容都需要能够放入数学公式中的东西。想象一下，我们使用文本文件来构建一个用于**自然语言处理**（**NLP**）的模型。在这种情况下，我们除了像**Unicode**这样的东西之外，没有有用的数学表示形式。我们将在[*第7章*](B17928_07_ePub.xhtml#_idTextAnchor112)，*高级特征提取与NLP*中学习如何以有用、向量化的方式表示它。有了向量，我们可以使用不同类型的指标来计算向量之间的相似度，称为**余弦相似度**指标，我们将在[*第6章*](B17928_06_ePub.xhtml#_idTextAnchor102)，*特征工程与标注*中讨论。
- en: So far, we have discussed how to calculate loss functions for a couple of scenarios,
    but how can we define the performance of our model overall?
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何计算几种场景下的损失函数，但我们是怎样定义我们模型的整体性能的呢？
- en: For regression models, our loss function was defined over the whole corpus of
    our training set. The error of a single observation or prediction would be ![](img/B17928_Formula_1.43.png).
    Therefore, RMSE is already a cost function and can be used by an optimizer to
    improve the model performance, so we can use it to judge the performance of the
    model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归模型，我们的损失函数是在整个训练集语料库上定义的。单个观察或预测的错误将是 ![img/B17928_Formula_1.43.png]。因此，RMSE已经是一个成本函数，可以被优化器用来提高模型性能，所以我们可以用它来判断模型性能。
- en: For classification models, this gets a little bit more interesting. Cross-entropy
    can be used with an optimizer to train the model and can be used to judge the
    model, but besides that, we can define an additional metric to look out for.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类模型，这会变得更有趣一些。交叉熵可以与优化器一起用来训练模型，也可以用来判断模型，但除此之外，我们还可以定义一个额外的指标来关注。
- en: 'Something obvious would be what is referred to as the **accuracy** of a model,
    calculated as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显而易见的方法就是所谓的模型的**准确率**，计算方法如下：
- en: '![](img/B17928_Formula_1.44.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17928_Formula_1.44.png)'
- en: Now, this looks about right. We just say that the quality of our model is the
    percentage of how often we guessed correctly, and the reality is that a lot of
    people agree with this statement. Remember when we defined **false positives**
    and **false negatives**? These now come into play. Let's look at an example.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这看起来是正确的。我们只是说我们模型的品质是我们猜对次数的百分比，而现实中很多人同意这个说法。记得我们定义**假阳性**和**假阴性**的时候吗？这些现在开始发挥作用了。让我们看看一个例子。
- en: 'Imagine a test that checks for a contagious virus. *Figure 1.10* shows the
    results for 100 people being tested for this virus, including the correctness
    of the results:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个检测传染性病毒的测试。*图1.10*显示了100人接受这种病毒检测的结果，包括结果的正确性：
- en: '![Figure 1.10 – Test results for a group of 100 people ](img/B17928_01_10.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图1.10 – 100人组的测试结果](img/B17928_01_10.jpg)'
- en: Figure 1.10 – Test results for a group of 100 people
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 – 100人组的测试结果
- en: 'Now, what would be the accuracy of this test given these results? Let''s define
    it again using the values for true positive (![](img/B17928_Formula_1.45.png)),
    false positive (![](img/B17928_Formula_1.46.png)), false negative (![](img/B17928_Formula_1.47.png)),
    and true negative (![](img/B17928_Formula_1.48.png)) and calculate the results
    for our example, as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据这些结果，这个测试的准确率会是多少呢？让我们再次定义它，使用以下值：真正例(![](img/B17928_Formula_1.45.png))、假正例(![](img/B17928_Formula_1.46.png))、假负例(![](img/B17928_Formula_1.47.png))和真负例(![](img/B17928_Formula_1.48.png))，并计算我们示例的结果，如下所示：
- en: '![](img/B17928_Formula_1.49.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17928_Formula_1.49.png)'
- en: This sounds like a good test. It gives accurate results in 92% of cases, but
    perhaps you see the problem here. Accuracy sees everything equally. Our test misclassifies
    someone having the virus eight times as someone being virus-free, which might
    have dire ramifications. That means it might be useful having performance metrics
    that put more emphasis on false-positive or false-negative outcomes. Therefore,
    let's define two additional metrics to calculate.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个听起来像是一个很好的测试。它在92%的情况下给出了准确的结果，但你可能在这里看到了问题。准确率对一切同等看待。我们的测试将患有病毒的人错误地分类为无病毒的人八次，这可能会产生严重的后果。这意味着可能需要具有更多强调假阳性或假阴性结果性能指标的指标。因此，让我们定义两个额外的指标来计算。
- en: 'The first one we call **precision**, a value that defines how many positive
    identifications were correct. The formula is shown here:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先称之为**精确度**，这是一个定义有多少个阳性识别是正确的值。公式如下所示：
- en: '![](img/B17928_Formula_1.50.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17928_Formula_1.50.png)'
- en: In our example, only in two out of three cases are we correct when we declare
    someone to be infected. A model with a precision value of 1 would have no false-positive
    results.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，当我们宣布某人感染时，只有三分之二的情况下我们是正确的。具有1精确度值的模型将没有假阳性结果。
- en: 'The second one we call **recall**, a value that defines how many positive results
    we identify correctly. The formula is shown here:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称之为**召回率**的第二个值，这是一个定义我们正确识别多少个阳性结果的价值。公式如下所示：
- en: '![](img/B17928_Formula_1.51.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17928_Formula_1.51.png)'
- en: This means in our example, we correctly identify 20% of all infected patients,
    which is a bad result. A model with a recall value of 1 would have no false-negative
    results.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在我们的例子中，我们正确地识别了所有感染患者的20%，这是一个不好的结果。具有1召回值的模型将没有假阴性结果。
- en: To evaluate our test or classification correctly, we need to evaluate accuracy,
    precision, and recall. Be aware that, as mentioned when we talked about hypothesis
    testing, precision and recall can work against each other. Therefore, you often
    have to decide whether you prefer to be precise when saying "*You have the virus*"
    or whether you prefer to find everyone who has the virus. You might now understand
    why such tests are often designed toward recall.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确评估我们的测试或分类，我们需要评估准确度、精确度和召回率。请注意，正如我们在讨论假设检验时提到的，精确度和召回率可能会相互对立。因此，你通常必须决定你更喜欢在说“*你有病毒*”时保持精确，还是更喜欢找到所有有病毒的人。你现在可能理解为什么这样的测试通常设计为以召回率为目标。
- en: With this, we conclude the section on the mathematical basis required to get
    better at building ML models and working with data. Based on what we have learned
    so far, you should take the next point with you.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们总结了提高构建机器学习模型和数据处理能力所需的数学基础部分。基于我们迄今为止所学的内容，你应该带着下一个要点继续前进。
- en: Important Note
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Never just use methods from ML libraries for data analysis and modeling; understand
    them mathematically.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要只使用机器学习库中的方法进行数据分析建模；要理解它们的数学原理。
- en: In the next section, we will guide you through the structure of the end-to-end
    ML process and the structure of this book.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将引导您了解端到端机器学习过程和本书的结构。
- en: Discovering the end-to-end ML process
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现端到端机器学习过程
- en: We have finally arrived at the main topic of this chapter. After reviewing the
    past and understanding the purpose of ML and how it takes its roots in mathematical
    data analysis, let's now get a clear picture of which steps need to be taken to
    create a high-quality ML model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于到达了本章的主题。在回顾过去并了解机器学习的目的以及它是如何根植于数学数据分析之后，现在让我们清楚地了解需要采取哪些步骤来创建高质量的机器学习模型。
- en: 'The following diagram shows an overview of the (sometimes recursive) steps
    from data to model to deployed model:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了从数据到模型再到部署模型的（有时是递归的）步骤概述：
- en: '![Figure 1.11 – End-to-end ML process ](img/B17928_01_11.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图1.11 – 端到端机器学习过程](img/B17928_01_11.jpg)'
- en: Figure 1.11 – End-to-end ML process
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11 – 端到端机器学习过程
- en: 'Looking at this flow, we can define the following distinct steps to take:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这个流程，我们可以定义以下不同的步骤：
- en: Excavating data and sources
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 挖掘数据和来源
- en: Preparing and cleaning data
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备和清理数据
- en: Defining labels and engineering features
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义标签和特征工程
- en: Training models
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Deploying models
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署模型
- en: These show the steps for running one single ML project. When you deal with a
    lot of projects and data, it becomes increasingly important to adopt some form
    of automation and operationalization, which is typically referred to as MLOps.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这些展示了运行单个机器学习项目的步骤。当你处理大量项目和数据时，采用某种形式的自动化和运营变得越来越重要，这通常被称为MLOps。
- en: 'In this section, we will give an overview of each of these steps, including
    MLOps and its importance, and explain in which chapters we will delve deeper into
    the corresponding topic. Before we start going through those steps, reflect on
    the following question:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将概述这些步骤的每个部分，包括MLOps及其重要性，并解释我们将深入探讨相应主题的章节。在我们开始逐步了解这些步骤之前，请思考以下问题：
- en: '*As a percentage, how much time would you put aside for each of those steps?*'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*你将为每个步骤分配多少百分比的时间？*'
- en: 'After you are done, have a look at the following screenshot, which shows you
    the typical time investment required for those tasks:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，请查看以下截图，它显示了完成这些任务所需的典型时间投入：
- en: '![Figure 1.12 – ML time invested ](img/B17928_01_12.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图1.12 – 机器学习时间投入](img/B17928_01_12.jpg)'
- en: Figure 1.12 – ML time invested
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12 – 机器学习时间投入
- en: Was your guess reasonably close to this? You might be surprised that only 20%
    of the time, you will work on something that has to do with the actual training
    and deployment of ML models. Therefore, you should take the next point to heart.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你的猜测合理吗？你可能惊讶地发现，只有20%的时间，你将从事与实际训练和部署机器学习模型有关的工作。因此，你应该牢记下一点。
- en: Important Note
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In an ML project, you should spend most of your time taking apart your datasets
    and finding other useful data sources.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目中，你应该花大部分时间拆解你的数据集并寻找其他有用的数据来源。
- en: Failure to do so will have ramifications on the quality of your model and its
    performance. Now, having said that, let's go through the steps one by one, starting
    with where to source your data from.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不这样做，将对模型的质量和性能产生负面影响。现在，既然已经说了这一点，让我们一步一步地过一遍，从数据来源开始。
- en: Excavating data and sources
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挖掘数据和来源
- en: When you start an ML project, you probably have some outcome in mind, and often,
    you have some form of existing dataset you or your company wants to start with.
    This is where you start familiarizing yourself with the given data, understanding
    what you have and what is missing by doing analysis, which we will come back to
    in the following steps.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始一个机器学习项目时，你可能已经有一个预期的结果，并且通常，你有一些形式的现有数据集，你或你的公司希望从中开始。这就是你开始熟悉给定数据的时候，通过分析了解你有什么以及缺少什么，我们将在以下步骤中回顾这一点。
- en: 'At some point, you might realize that you are missing additional—but crucial—data
    points to increase the quality of your results. This highly depends on what you
    are missing—whether it is something you or your company can obtain or whether
    you need to find it somewhere else. To give you some ideas, let''s have a look
    at the following options to acquire additional data and what you should be aware
    of:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时候，你可能会意识到你缺少一些额外的——但至关重要的——数据点，以增加你结果的质量。这高度取决于你所缺少的是什么——是你可以或你的公司可以获得的，还是你需要从其他地方找到的。为了给你一些想法，让我们看看以下获取额外数据的方法以及你应该注意的事项：
- en: '**In-house data sources**: If you are running this project in or with a company,
    the first point to look is internally. Advantages of this are that it is free
    of charge, it is often standardized, and you should be able to find a person that
    knows this data and how it was obtained. Depending on the project, it might also
    be the only place you can acquire the required data. Disadvantages of this option
    are that you might not find what you are looking for, that the data is poorly
    documented, and that the quality might be in question due to bias in the data.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部数据源**：如果你在公司内部或与公司合作进行此项目，首先需要考虑的是内部资源。这种选择的优点是免费，通常标准化，你应该能够找到了解这些数据及其获取方式的人。根据项目情况，这也可能是你获取所需数据的唯一途径。这种选择的缺点是可能找不到你想要的东西，数据可能记录不佳，而且由于数据中的偏差，质量可能存在问题。'
- en: '**Open data sources**: Another option is to use freely available datasets.
    Advantages of those are that they are typically gigantic in size (**terabytes**
    (**TB**) of data), they cover different time periods, and they are typically well
    structured and documented. Disadvantages are that some data fields might be hard
    to understand (and the creator is not available), the quality might also vary
    due to bias in the data, and often when used, they require you to publish your
    results. Examples of this would be the **National Oceanic and Atmospheric Administration**
    (**NOAA**) ([https://www.ncei.noaa.gov/weather-climate-links](https://www.ncei.noaa.gov/weather-climate-links))
    and the **European Union** (**EU**) Open Data Portal ([https://data.europa.eu/en](https://data.europa.eu/en)),
    among many others.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公开数据源**：另一个选择是使用免费可用的数据集。这些数据集的优点通常是规模巨大（**千兆字节**（**TB**）的数据），覆盖不同的时间段，并且通常结构良好且文档齐全。缺点是某些数据字段可能难以理解（且创建者不可用），由于数据中的偏差，质量也可能参差不齐，并且在使用时，通常需要你发布你的结果。此类示例包括**国家海洋和大气管理局**（**NOAA**）([https://www.ncei.noaa.gov/weather-climate-links](https://www.ncei.noaa.gov/weather-climate-links))和**欧盟**（**EU**）开放数据门户([https://data.europa.eu/en](https://data.europa.eu/en))，以及其他许多数据源。'
- en: '**Data seller (data as a service, or DaaS)**: A final option would be to buy
    data from a data seller, either by purchasing an existing dataset or by requesting
    the creation of one. Advantages of this option are that it saves you time, it
    can give you access to an individualized dataset, and you might even get access
    to preprocessed data. Disadvantages are that this is expensive, you still need
    to do all the other following steps to make this data useful, and there might
    be questions concerning privacy and ethics.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据卖家（数据即服务，或DaaS）**：最后一个选择是从数据卖家那里购买数据，无论是购买现有的数据集还是请求创建一个。这种选择的优点是节省时间，可以让你访问个性化的数据集，甚至可能获得预处理数据的访问权限。缺点是这很昂贵，你仍然需要完成所有其他后续步骤来使这些数据变得有用，并且可能存在有关隐私和伦理的问题。'
- en: 'Now that we have a good idea of where to get data initially or additionally,
    let''s look at the next step: preparing and cleaning the data.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对最初或额外获取数据的地方有了很好的了解，让我们看看下一步：准备和清理数据。
- en: Preparing and cleaning data
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备和清理数据
- en: As alluded to before, descriptive data exploration is without a doubt one of
    the most important steps in an ML project. If you want to clean data and build
    derived features or select an ML algorithm to predict a target variable in your
    dataset, then you need to understand your data first. Your data will define many
    of the necessary cleaning and preprocessing steps. It will define which algorithms
    you can choose, and it will ultimately define the performance of your predictive
    model.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，描述性数据探索无疑是机器学习项目中最重要的步骤之一。如果你想清理数据、构建派生特征或选择机器学习算法来预测数据集中的目标变量，那么首先你需要了解你的数据。你的数据将定义许多必要的清理和预处理步骤。它将定义你可以选择哪些算法，并最终定义你的预测模型的性能。
- en: The exploration should be done as a structured analytical process rather than
    a set of experimental tasks. Therefore, we will go through a checklist of data
    exploration tasks that you can perform as an initial step in every ML project,
    before starting any data cleaning, preprocessing, **feature engineering**, or
    model selection. By applying these steps, you will be able to understand the data
    and gain knowledge about the required preprocessing tasks.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 探索应该作为一个结构化的分析过程来完成，而不是一系列实验任务。因此，我们将通过一系列数据探索任务清单，这些任务可以作为每个机器学习项目开始时的初步步骤，在开始任何数据清理、预处理、**特征工程**或模型选择之前执行。通过应用这些步骤，你将能够理解数据并获得有关所需预处理任务的知识。
- en: Along with that, it will give you a good estimate of what kinds of difficulties
    you can expect in your prediction task, which is essential for judging the required
    algorithms and validation strategies. You will also gain an insight into which
    possible feature engineering methods could apply to your dataset and have a better
    understanding of how to select a good loss function.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还将为您提供对预测任务可能遇到的困难类型的良好估计，这对于判断所需的算法和验证策略至关重要。您还将深入了解哪些可能的特征工程方法适用于您的数据集，并更好地理解如何选择一个好的损失函数。
- en: Let's have a look at the required steps.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看所需的步骤。
- en: Storing and preparing data
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储和准备数据
- en: Your data might come in a variety of different formats. You might work with
    tabular data stored in a **comma-separated values** (**CSV**) file; you might
    have images stored as **Joint Photographic Experts Group** (**JPEG**) or **Portable
    Network Graphics** (**PNG**) files, text stored in a **JavaScript Object Notation**
    (**JSON**) file, or audio files in **MP3** or **M4V** format. CSV can be a good
    format as it is human-readable and can be parsed efficiently. You can open and
    browse it using any text editor.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据可能以各种不同的格式存在。您可能需要处理存储在**逗号分隔值**（**CSV**）文件中的表格数据；您可能拥有以**联合图像专家组**（**JPEG**）或**可移植网络图形**（**PNG**）文件格式存储的图像，文本存储在**JavaScript对象表示法**（**JSON**）文件中，或者音频文件以**MP3**或**M4V**格式。CSV可以是一个好的格式，因为它可读性强，且可以高效解析。您可以使用任何文本编辑器打开和浏览它。
- en: If you work on your own, you might just store this raw data in a folder on your
    system, but when you are working with a cloud infrastructure or even just a company
    infrastructure in general, you might need some form of cloud storage. Certainly,
    you can just upload your raw data by hand to such storage, but often, the data
    you work with is coming from a live system and needs to be extracted from there.
    This means it might be worthwhile having a look at so-called **extract-transform-load**
    (**ETL**) tools that can automate this process and bring the required raw data
    into cloud storage.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您独立工作，您可能只需将原始数据存储在系统中的一个文件夹中，但当你与云基础设施或一般的公司基础设施一起工作时，您可能需要某种形式的云存储。当然，您可以手动上传原始数据到这种存储，但通常，您处理的数据来自实时系统，需要从中提取。这意味着查看所谓的**提取-转换-加载**（**ETL**）工具可能是有价值的，这些工具可以自动化此过程并将所需的原始数据带入云存储。
- en: After all of the preprocessing steps are done, you will have some form of layered
    data in your storage, from raw to cleaned to labeled to processed datasets.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有预处理步骤完成后，您存储中的数据将呈现某种形式的多层结构，从原始数据到清洗数据，再到标记数据，最后到处理后的数据集。
- en: We will dive deeper into this topic in [*Chapter 4*](B17928_04_ePub.xhtml#_idTextAnchor071),
    *Ingesting Data and Managing Datasets*. For now, just understand that we will
    automate this process of making data available for processing.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第4章*](B17928_04_ePub.xhtml#_idTextAnchor071)“数据摄取与管理数据集”中更深入地探讨这个主题。现在，只需了解我们将自动化使数据可用于处理的过程。
- en: Cleaning data
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清洗数据
- en: In this step, we have a look at inconsistency and structural errors in the data
    itself. This step is often required for tabular data and sometimes text files,
    but not so much for image or audio files. For the latter, we might be able to
    crop images and change their brightness or contrast, but it might be required
    to go back to the source to create better-quality samples. The same goes for audio
    files.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们检查数据本身中的不一致性和结构错误。这一步骤通常适用于表格数据，有时也适用于文本文件，但不太适用于图像或音频文件。对于后者，我们可能能够裁剪图像并改变它们的亮度或对比度，但可能需要回到源数据以创建更好的样本。同样适用于音频文件。
- en: 'For tabular datasets, we have much more options for processing. Let''s go through
    what to look out for, as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对于表格数据集，我们有很多处理选项。以下是我们需要关注的事项：
- en: '**Duplicates**: Through mistakes in copying data or due to a combination of
    different data sources, you might find duplicate samples. Typically, copies can
    be deleted. Just make sure that these are not two different samples that look
    the same.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复项**：由于数据复制错误或不同数据源的组合，您可能会发现重复的样本。通常，副本可以被删除。只需确保这些不是看起来相同但实际不同的样本。'
- en: '**Irrelevant information**: In most cases, you will have datasets with a lot
    of different features, some of which will be completely unnecessary for your project.
    The obvious ones you should just remove in the beginning; others you will be able
    to remove later after analyzing the data further.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无关信息**：在大多数情况下，你将拥有包含许多不同特征的数据库集，其中一些对于你的项目来说可能完全不必要。你应该在开始时直接删除明显的那些；其他的一些你可以在进一步分析数据后删除。'
- en: '`US` and `United States`) or simply typos. These should be standardized or
    cleaned up. A good way to do this is by visualizing all available values of a
    feature.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (`US` 和 `United States`) 或简单的拼写错误。这些应该被标准化或清理。一个好的方法是可视化一个特征的可用值。
- en: '**Anomalies (outliers)**: This refers to very unlikely values for which you
    need to decide whether they are errors or actually true. This is typically done
    after analyzing the data when you know the distribution of a feature.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值（离群值）**：这指的是非常不可能的值，你需要决定它们是错误还是实际上是真的。这通常是在分析数据后进行的，当你知道一个特征的分布时。'
- en: '`NA` or `NaN`. There are different ways to rectify this besides deleting entire
    samples. It is also prudent to wait until you have more insight from analyzing
    the data, as you might see better ways to replace them.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NA` 或 `NaN`。除了删除整个样本之外，还有不同的方法可以纠正这个问题。在分析数据并可能看到更好的替换方法之前等待，也是谨慎的做法。'
- en: After this step, we can start analyzing the cleaned version of our dataset further.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步之后，我们可以开始进一步分析清洗后的数据集。
- en: Analyzing data
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析数据
- en: In this step, we apply our understanding of statistics to get some insights
    into our features and labels. This includes calculating statistical properties
    for each feature, visualizing them, finding correlated features, and measuring
    something that is called **feature importance**, which calculates the impact of
    a feature on the label, also referred to as the **target variable**.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们将应用我们对统计学的理解，以获取对我们特征和标签的洞察。这包括为每个特征计算统计属性，可视化它们，找到相关特征，并测量称为**特征重要性**的东西，它计算特征对标签的影响，也称为**目标变量**。
- en: Through these methods, we get ideas about relationships among features and between
    features and targets, which can help us to make a decision. In this decision-making
    process, we also start adding something vitally important—our **domain knowledge**.
    If you do not know what the data represents, you will have a hard time pruning
    it and choosing optimal features and samples for training.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些方法，我们获得了关于特征之间以及特征与目标之间关系的想法，这可以帮助我们做出决定。在这个决策过程中，我们也开始添加一些至关重要的东西——我们的**领域知识**。如果你不知道数据代表什么，你将很难对其进行修剪，并选择训练的最佳特征和样本。
- en: There are a lot more techniques that can be applied in this step, including
    something called **dimensional reduction**. If you have thousands of features
    (a numerical representation of an image, for example), it gets very complicated
    for humans and even for ML processes to understand relationships. In such cases,
    it might be useful to map this high-dimensional sample to a two-dimensional or
    three-dimensional representation in the form of a vector. Through this, we can
    easily find similarities in different samples.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，还有许多可以应用的技术，包括称为**降维**的东西。如果你有成千上万的特征（例如图像的数值表示），对于人类甚至对于机器学习过程来说，理解关系会变得非常复杂。在这种情况下，将这个高维样本映射到二维或三维的向量表示可能是有用的。通过这种方式，我们可以轻松地找到不同样本之间的相似性。
- en: We will dive deeper into the topics of cleaning and analyzing data in [*Chapter
    5*](B17928_05_ePub.xhtml#_idTextAnchor085), *Performing Data Analysis and Visualization*.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第五章*](B17928_05_ePub.xhtml#_idTextAnchor085) *执行数据分析与可视化*中更深入地探讨清理和分析数据的话题。
- en: Having done all these steps, we will have a good understanding of the data we
    have at hand, and we might already know what we are missing. As the final step
    in preprocessing our data, we will have a look at creating and transforming features,
    typically referred to as **feature engineering**, and creating labels when missing.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有这些步骤后，我们将对手头的数据有一个良好的理解，并且可能已经知道我们缺少什么。作为数据预处理中的最后一步，我们将查看创建和转换特征的过程，通常称为**特征工程**，以及当缺少标签时创建标签。
- en: Defining labels and engineering features
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义标签和工程特征
- en: In the second part of the preprocessing of data, we will discuss the labeling
    of data and the actions we can perform on features. To perform these steps, we
    need the knowledge obtained through the exploratory steps we've discussed so far.
    Let's start by looking at labeling data.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据预处理的第二部分，我们将讨论数据的标注以及我们可以对特征执行的操作。要执行这些步骤，我们需要通过我们之前讨论的探索步骤获得的知识。让我们首先看看标注数据。
- en: Labeling
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标注
- en: 'Let''s start with a bummer: this process is very tedious. Labeling, also called
    **annotation**, is the least exciting part of an ML project yet one of the most
    important tasks in the whole process. The goal is to feed high-quality training
    data into the ML algorithms.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一件令人沮丧的事情开始：这个过程非常繁琐。标注，也称为**注释**，是机器学习项目中最不令人兴奋的部分，但也是整个过程中最重要的任务之一。目标是向机器学习算法提供高质量的训练数据。
- en: While proper labels greatly help to improve prediction performance, the labeling
    process will also help you to study the dataset in greater detail. Let me clarify
    that labeling data requires deep insight and understanding of the context of the
    dataset and the prediction process, which you should have acquired at this point.
    If we were, for example, aiming to predict breast cancer using **computerized
    tomography** (**CT**) scans, we would also need to understand how breast cancer
    can be detected in CT images to label the data.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然适当的标签极大地有助于提高预测性能，但标注过程也将帮助您更详细地研究数据集。让我澄清一下，标注数据需要深入洞察和理解数据集的上下文以及预测过程，您应该已经在这个阶段获得了这些。例如，如果我们旨在使用**计算机断层扫描**（**CT**）来预测乳腺癌，我们还需要了解如何在CT图像中检测乳腺癌以标注数据。
- en: Mislabeling the training data has a couple of consequences, such as **label
    noise**, which you want to avoid as it will affect the performance of every downstream
    process in the ML pipeline. In some cases, your labeling methodology is dependent
    on the chosen ML approach for a prediction problem. A good example is the difference
    between object detection and segmentation, both of which require completely differently
    labeled data.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 错误标注训练数据有几个后果，例如**标签噪声**，您希望避免它，因为它会影响机器学习管道中每个下游过程的性能。在某些情况下，您的标注方法取决于为预测问题选择的机器学习方法。一个很好的例子是目标检测和分割之间的差异，它们都需要完全不同标注的数据。
- en: There are some techniques and tooling available to speed up the labeling process
    that make use of the fact that we can use ML algorithms not only for the desired
    project but also to learn how to label our data. Such models start proposing labels
    during your manual annotation of the dataset.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些技术和工具可以利用我们不仅可以使用机器学习算法来完成所需项目，还可以学习如何标注我们的数据这一事实来加速标注过程。这些模型会在您手动标注数据集时开始提出标签。
- en: Feature engineering
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征工程
- en: In a nutshell, in this step, we will start transforming the features or adding
    new features. Obviously, we are not doing such actions on a whim, but rather due
    to the knowledge we gathered in the previous steps. We might have understood,
    for example, that the full date and time are far too precise, and we need just
    the day of the week or the month. Whatever it might be, we will try to shape and
    extract what we need.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在这一步中，我们将开始转换特征或添加新特征。显然，我们并不是一时兴起地执行这些操作，而是基于我们在前一步骤中收集到的知识。例如，我们可能已经了解到完整的日期和时间过于精确，我们只需要星期几或月份。无论是什么，我们都会尝试塑造和提取我们所需要的。
- en: 'Typically, we will perform one of the following actions:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们将执行以下操作之一：
- en: '**Feature creation**: Create new features from a given set of features or from
    additional information sources.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征创建**：从一组给定的特征或从额外的信息源中创建新特征。'
- en: '**Feature transformation**: Transform single features to make them useful and
    stable for the utilized ML algorithm.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征转换**：将单个特征转换为对所使用的机器学习算法有用的和稳定的。'
- en: '**Feature extraction**: Create derived features from the original data.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：从原始数据中创建派生特征。'
- en: '**Feature selection**: Choose the most prominent and predictive features.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：选择最突出和预测性强的特征。'
- en: We will dive deeper into labeling and the multitude of methods to apply to our
    features in [*Chapter 6*](B17928_06_ePub.xhtml#_idTextAnchor102), *Feature Engineering
    and Labeling*. In addition, we will have a detailed look at a more complex example
    of feature engineering when working with text data in an NLP project. You will
    find this in [*Chapter 7*](B17928_07_ePub.xhtml#_idTextAnchor112), *Advanced Feature
    Extraction with NLP*.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第6章*](B17928_06_ePub.xhtml#_idTextAnchor102)“特征工程和标记”中更深入地探讨标记和应用于我们的特征的各种方法。此外，我们还将详细研究一个更复杂的特征工程示例，当在自然语言处理（NLP）项目中处理文本数据时。您将在[*第7章*](B17928_07_ePub.xhtml#_idTextAnchor112)“使用NLP的高级特征提取”中找到它。
- en: We conclude this step by reiterating how important the whole preprocessing data
    steps are and how much influence they have on the next step, where we will discuss
    model training. Further, we remember that we might need to come back to this after
    model training in case of lackluster performance of our model.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过重申整个预处理数据步骤的重要性以及它们对下一步的影响来结束这一步骤，下一步我们将讨论模型训练。此外，我们记得，如果我们的模型表现不佳，我们可能需要在模型训练后返回到此步骤。
- en: Training models
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: We finally reached the point where we can bring ML algorithms into play. As
    with data experimentation and preprocessing, training an ML model is an analytical,
    step-by-step process. Each step involves a thought process that evaluates the
    pros and cons of each algorithm according to the results of the experimentation
    phase. As in every other scientific process, it is recommended that you come up
    with a hypothesis first and verify whether this hypothesis is true afterward.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终达到了可以应用机器学习算法的点。与数据实验和预处理一样，训练机器学习模型是一个分析性的、逐步的过程。每个步骤都涉及一个思考过程，根据实验阶段的结果评估每个算法的优缺点。正如其他任何科学过程一样，建议您首先提出一个假设，然后验证这个假设是否正确。
- en: 'Let''s look at the steps that define the process of training an ML model, as
    follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看定义训练机器学习模型过程的步骤，如下所示：
- en: '**Define your ML task**: First, we need to define the ML task we are facing,
    which most of the time is defined by the business decision behind your use case.
    Depending on the amount of labeled data, you can choose between unsupervised and
    supervised learning methods, as well as many other subcategories.'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义您的机器学习任务**：首先，我们需要定义我们面临的机器学习任务，这通常由您用例背后的业务决策所定义。根据标记数据的数量，您可以选择无监督学习和监督学习方法，以及许多其他子类别。'
- en: '**Pick a suitable model**: Pick a suitable model for the chosen ML task. This
    might be a logistical regression, a gradient-boosted ensemble tree, or a DNN,
    just to name a few popular ML model choices. The choice is mainly dependent on
    the training (or production) infrastructure (such as Python, R, Julia, C, and
    so on) and the shape and type of the data.'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择一个合适的模型**：为所选的机器学习任务选择一个合适的模型。这可能是一个逻辑回归、梯度提升集成树或深度神经网络（DNN），仅举几个流行的机器学习模型选择。选择主要取决于训练（或生产）基础设施（如Python、R、Julia、C等）以及数据的形状和类型。'
- en: '**Pick or implement a loss function and an optimizer**: During the data experimentation
    phase, you should have already come up with a strategy on how to test your model
    performance. Hence, you should have picked a data split, loss function, and optimizer
    already. If you have not done so, you should at this point evaluate what you want
    to measure and optimize.'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择或实现损失函数和优化器**：在数据实验阶段，您应该已经制定了一个测试模型性能的策略。因此，您应该已经选择了数据分割、损失函数和优化器。如果您还没有这样做，您应该在此点评估您想要衡量和优化的内容。'
- en: '**Pick a dataset split**: Splitting your data into different sets—namely, training,
    validation, and test sets—gives you additional insights into the performance of
    your training and optimization process and helps you to avoid overfitting your
    model to your training data.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择数据集分割**：将您的数据分割成不同的集合——即训练集、验证集和测试集——可以帮助您深入了解训练和优化过程的表现，并帮助您避免模型对训练数据的过度拟合。'
- en: '**Train a simple model using cross-validation**: When all the preceding choices
    are made, you can go ahead and train your ML model. Optimally, this is done as
    cross-validation on a training and validation set, without leaking training data
    into validation. After training a baseline model, it''s time to interpret the
    error metric of the validation runs. Does it make sense? Is it as high or low
    as expected? Is it (hopefully) better than random and better than always predicting
    the most popular target?'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用交叉验证训练简单模型**：在做出所有前面的选择之后，你可以继续训练你的机器学习模型。理想情况下，这是在训练集和验证集上进行交叉验证，而不将训练数据泄露到验证中完成的。在训练基线模型后，是时候解释验证运行中的错误度量了。它有意义吗？它是否如预期的那样高或低？它是（希望）比随机预测更好，并且比总是预测最流行的目标更好吗？'
- en: '**Tune the model**: Finally, you can either tune the outcome of the model by
    working with the so-called hyperparameters of a model, do model stacking or other
    advanced methods, or you might have to go back to the initial data and work on
    that before training the model again.'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调整模型**：最后，你可以通过调整模型的超参数、进行模型堆叠或其他高级方法来调整模型的输出，或者你可能需要回到初始数据并在重新训练模型之前对其进行处理。'
- en: These are the base steps we perform when training our model. In the following
    section, we will give some more insights into the aforementioned steps, starting
    with how to choose a model.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们训练模型时执行的基线步骤。在下一节中，我们将更深入地探讨上述步骤，从如何选择模型开始。
- en: Choosing a model
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择模型
- en: When it comes to choosing a good model for your data, it is recommended that
    you favor simple traditional models before going toward the more complex options.
    An example would be ensemble models, such as **gradient-boosted tree ensembles**,
    when training data is limited. These models perform well on a broad set of input
    values (ordinal, nominal, and numeric) as well as training efficiently, and they
    are understandable.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到为你的数据选择一个好的模型时，建议你在转向更复杂选项之前优先考虑简单的传统模型。例如，当训练数据有限时，可以采用集成模型，如**梯度提升树集成**。这些模型在广泛的输入值（有序、名义和数值）上表现良好，并且训练效率高，易于理解。
- en: Tree-based ensemble models combine many weak learners into a single predictor
    based on decision trees. This greatly reduces the problem of the overfitting and
    instability aspects of a single decision tree. The output, after a few iterations
    using the default parameter, usually delivers great baseline results for many
    different applications.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的集成模型将许多弱学习器结合成一个基于决策树的单一预测器。这大大减少了单个决策树的过拟合和不稳定问题。使用默认参数进行几次迭代后，输出通常为许多不同应用提供很好的基线结果。
- en: In [*Chapter 9*](B17928_09_ePub.xhtml#_idTextAnchor152), *Building ML Models
    Using Azure Machine Learning*, we dedicate a complete section to training a gradient-boosted
    tree ensemble classifier using **LightGBM**, a popular tree ensemble library from
    Microsoft.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第9章*](B17928_09_ePub.xhtml#_idTextAnchor152)“使用Azure机器学习构建ML模型”中，我们专门用一节来介绍如何使用**LightGBM**（来自微软的一个流行的树集成库）训练梯度提升树集成分类器。
- en: To capture the meaning of large amounts of complex training data, we need large
    parametric models. However, training parametric models with many hundreds of millions
    of parameters is no easy task, due to exploding and vanishing gradients, loss
    propagation through such a complex model, numerical instability, and normalization.
    In recent years, a branch of such high-parametric models achieved extremely good
    results through many complex tasks—namely, **deep learning** (**DL**).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉大量复杂训练数据的含义，我们需要大型参数模型。然而，由于梯度爆炸和消失、通过如此复杂的模型传播损失、数值不稳定性和归一化等问题，用数亿个参数训练参数模型并非易事。近年来，这类高参数模型通过许多复杂任务实现了极其好的结果——即**深度学习**（**DL**）。
- en: DL basically spans up a multilayer ANN, where each layer is seen as a certain
    step in the data processing pipeline of the model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）基本上是一个多层人工神经网络（ANN），其中每一层都被视为模型数据处理管道中的某个步骤。
- en: In [*Chapter 10*](B17928_10_ePub.xhtml#_idTextAnchor165), *Training Deep Neural
    Networks on Azure*, and [*Chapter 12*](B17928_12_ePub.xhtml#_idTextAnchor189),
    *Distributed Machine Learning on Azure*, we will delve deeper into how to train
    large and complex DL models on single machines and on a distributed GPU cluster.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第10章*](B17928_10_ePub.xhtml#_idTextAnchor165)“在Azure上训练深度神经网络”和[*第12章*](B17928_12_ePub.xhtml#_idTextAnchor189)“Azure上的分布式机器学习”中，我们将更深入地探讨如何在单机和分布式GPU集群上训练大型和复杂的深度学习模型。
- en: Finally, you might work with a completely different form of data, such as audio
    or text data. In such cases, there are specialized ways to preprocess and score
    this data. One of these fields would be **recommendation engines**, which we will
    discuss thoroughly in [*Chapter 13*](B17928_13_ePub.xhtml#_idTextAnchor202), *Building
    a Recommendation Engine in Azure*.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可能会处理完全不同形式的数据，例如音频或文本数据。在这种情况下，有专门的方法来预处理和评分这些数据。这些领域之一是**推荐引擎**，我们将在[*第13章*](B17928_13_ePub.xhtml#_idTextAnchor202)中详细讨论，*在Azure中构建推荐引擎*。
- en: Choosing a loss function and an optimizer
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择损失函数和优化器
- en: As we discussed in the previous section, there are many metrics to choose from,
    depending on the type of training and model you want to use. After looking at
    the relationship between the feature and target dimensions, as well as the separability
    of the data, you should continue to evaluate which loss function and optimizer
    you will use to train your model.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中讨论的，根据你想要使用的训练类型和模型，有许多指标可以选择。在查看特征和目标维度的关系以及数据的可分性之后，你应该继续评估你将使用哪个损失函数和优化器来训练你的模型。
- en: Many ML practitioners don't value the importance of a proper error metric highly
    enough and just use what is easy, such as accuracy and RMSE. This choice is critical.
    Furthermore, it is useful to understand the baseline performance and the model's
    robustness to noise. The first can be achieved by computing the error metric using
    only the target variable with the highest occurrence as a prediction. This will
    be your baseline performance. The second can be done by modifying the random seed
    of your ML model and observing the changes to the error metric. This will show
    you which decimal place you can trust the error metric to.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习从业者并没有高度重视适当错误指标的重要性，只是使用简单易行的指标，例如准确率和RMSE。这个选择至关重要。此外，了解基线性能和模型对噪声的鲁棒性是有用的。第一种可以通过仅使用出现频率最高的目标变量作为预测来计算错误指标来实现。这将是你基线性能。第二种可以通过修改你的机器学习模型的随机种子并观察错误指标的变化来实现。这将显示你可以信任错误指标的哪个小数位。
- en: Keep in mind that it is prudent to evaluate the chosen error metric and any
    additional metric you desire after training runs, and experiment whether others
    might be more beneficial.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在训练运行后，评估所选的错误指标以及任何其他您希望评估的指标是谨慎的，并尝试实验其他指标是否可能更有益。
- en: As for the optimizer, it highly depends on the model you chose as to which options
    you have in this regard. Just remember the optimizer is how we get to the target,
    and the target is defined by the loss function.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 至于优化器，它高度依赖于你选择的模型，这决定了你在这方面有哪些选项。只需记住，优化器是我们达到目标的方式，而目标由损失函数定义。
- en: Splitting the dataset
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集划分
- en: 'Once you have selected an ML model, a loss function, and an optimizer, you
    need to think about splitting your dataset for training. Optimally, the data should
    be split into three disjointed sets: a training, a validation, and a test dataset.
    We use multiple sets to ensure that the model generalizes well on unseen data
    and that the reported error metric can be trusted. Hence, you can see that dividing
    the data into representative sets is a task that should be performed as an analytical
    process. These sets are defined as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你选择了机器学习模型、损失函数和优化器，你需要考虑将你的数据集划分为训练集。理想情况下，数据应该分为三个互斥的集合：训练集、验证集和测试集。我们使用多个集合来确保模型在未见过的数据上具有良好的泛化能力，并且报告的错误指标是可以信赖的。因此，你可以看到将数据划分为代表性集合是一个应该作为分析过程执行的任务。这些集合如下定义：
- en: '**Training dataset**: The subset of data used to fit/train the model.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据集**：用于拟合/训练模型的子数据集。'
- en: '**Validation dataset**: The subset of data used to provide an evaluation during
    training to tune hyperparameters. The algorithm sees this data during training,
    but never learns from it. Therefore, it has an indirect influence on the model.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证数据集**：用于在训练过程中提供评估以调整超参数的子数据集。算法在训练过程中看到这些数据，但从未从中学习。因此，它对模型有间接影响。'
- en: '**Test dataset**: The subset of data used to run an unbiased evaluation of
    the trained model after training.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试数据集**：用于在训练后对训练好的模型进行无偏评估的子数据集。'
- en: 'If training data leaks into the validation or testing set, you risk overfitting
    the model and skewing the validation and testing results. Overfitting is a problem
    that you must handle besides underfitting the model. Both are defined as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练数据泄露到验证集或测试集中，你可能会使模型过拟合，并扭曲验证和测试结果。除了欠拟合模型外，过拟合也是一个你必须处理的问题。两者如下定义：
- en: Underfitting versus Overfitting
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合与过拟合
- en: An underfitted model performs purely on the data. The reasons for that are often
    that the model is too simplistic to understand the relationship between the features
    and the target variables, or that your initial data is lacking useful features.
    An overfitted model performs perfectly on the training dataset and purely on any
    other data. The reason for that is that it basically memorized the training data
    and is unable to generalize.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 一个欠拟合的模型仅对数据进行操作。这种情况的原因通常是因为模型过于简单，无法理解特征与目标变量之间的关系，或者是因为你的初始数据缺乏有用的特征。一个过拟合的模型在训练数据集上表现完美，在任意其他数据上也表现完美。这种情况的原因是它基本上记住了训练数据，无法进行泛化。
- en: There are different discussions on what the size of these splits should be and
    many different further techniques to choose samples for each category, such as
    stratified splitting (sampling based on class distributions), temporal splitting,
    and group-based splitting. We will take a deeper look at these in [*Chapter 9*](B17928_09_ePub.xhtml#_idTextAnchor152),
    *Building ML* *Models Using Azure Machine Learning*.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些分割的大小有不同的讨论，以及许多不同的进一步技术来为每个类别选择样本，例如分层分割（基于类别分布的采样）、时间分割和基于组的分割。我们将在[*第9章*](B17928_09_ePub.xhtml#_idTextAnchor152)中更深入地探讨这些内容，*使用Azure机器学习构建机器学习模型*。
- en: Running the model training
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行模型训练
- en: In most cases, you will not build an ANN structure and an optimizer from scratch.
    You will use ready-made ML libraries, such as **scikit-learn**, **TensorFlow**,
    or **PyTorch**. Most of these frameworks and libraries are written in Python,
    which should therefore be the language of choice for your ML projects.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，你不会从头开始构建人工神经网络（ANN）结构和优化器。你将使用现成的机器学习库，如**scikit-learn**、**TensorFlow**或**PyTorch**。这些框架和库大多是用Python编写的，因此Python应该是你的机器学习项目的首选语言。
- en: 'When writing your code for model training, it is a good idea to logically divide
    the required code into two files, as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写模型训练的代码时，将所需的代码逻辑上分为两个文件是一个好主意，如下所示：
- en: '**Authoring script (authoring environment)**: The script that defines the environment
    (libraries, training location, and so on) in which the ML training will take place
    and the one triggering the execution script'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**脚本编写（脚本环境）**：定义机器学习训练将进行的（库、训练位置等）环境以及触发执行脚本的脚本'
- en: '**Execution script (execution environment)**: The script that only contains
    the actual ML training'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行脚本（执行环境）**：仅包含实际机器学习训练的脚本'
- en: By splitting your code in this way, you avoid updating the actual training script
    when your target environment changes. This will make code versioning and MLOps
    much cleaner.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以这种方式拆分你的代码，你可以在目标环境发生变化时避免更新实际的训练脚本。这将使代码版本控制和MLOps变得更加干净。
- en: 'To understand what types of class methods we might encounter in an ML library,
    let''s have a look at a short code snippet from TensorFlow here:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解在机器学习库中我们可能会遇到哪些类型的分类方法，让我们来看看TensorFlow中的一个简短代码片段：
- en: '[PRE0]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Looking at this code, we see that we are using a model called `Sequential` that
    is a basic ANN defined by a sequential set of layers with one input and one output.
    We see in the model creation step that there are layers defined and some omitted
    other settings. In addition, in the `compile()` method, we define an optimizer,
    a loss function, and some additional metrics we are interested in. Finally, we
    see a method called `fit()` running on the training dataset and a method called
    `evaluate()` running on the test dataset. Now, what do these methods do exactly?
    Before we get to that, let's first define something.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看这段代码，我们看到我们正在使用一个名为`Sequential`的模型，这是一个由一系列层定义的基本人工神经网络（ANN），这些层有一个输入和一个输出。在模型创建步骤中，我们看到定义了层，并省略了一些其他设置。此外，在`compile()`方法中，我们定义了一个优化器、一个损失函数以及一些我们感兴趣的额外指标。最后，我们看到一个名为`fit()`的方法正在训练数据集上运行，一个名为`evaluate()`的方法正在测试数据集上运行。现在，这些方法究竟做了什么？在我们深入探讨之前，让我们首先定义一些东西。
- en: Hyperparameters versus Parameters of a Model
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的超参数与参数
- en: There are two kinds of settings that are adjusted during model training. Settings
    such as the weights and the bias in an ANN are referred to as the parameters.
    They are changed during the training phase. Other settings—such as the activation
    functions and the number of layers in an ANN, the data split, the learning rate,
    or the chosen optimizer—are referred to as hyperparameters. Those are the meta
    settings we adjust before a training run.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练过程中调整的有两种类型的设置。在人工神经网络（ANN）中，权重和偏差等设置被称为参数。它们在训练阶段被改变。其他设置——如ANN中的激活函数和层数、数据分割、学习率或选择的优化器——被称为超参数。这些是我们训练运行之前调整的元设置。
- en: 'Having this out of the way, let''s define the typical methods you will encounter,
    as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面没有问题之后，让我们定义你将遇到的典型方法，如下所示：
- en: '`Sequential` class), in a special function such as `compile()`, or they are
    part of the training method we discuss next.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (`Sequential`类)，在特殊函数如`compile()`中，或者它们是我们接下来讨论的训练方法的一部分。
- en: '`fit()` or `train()`, this is the main method that trains the parameter of
    the model based on the training dataset, the loss function, and the optimizer.
    These methods do not return any type of value—they just update the model object
    and its parameters.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit()`或`train()`，这是基于训练数据集、损失函数和优化器训练模型参数的主要方法。这些方法不返回任何类型的值——它们只是更新模型对象及其参数。'
- en: '`evaluate()`, `transform()`, `score()`, or `predict()`. In most cases, these
    return some form of result, as they are typically running the test dataset against
    the trained model.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluate()`、`transform()`、`score()`或`predict()`。在大多数情况下，这些函数会返回某种形式的结果，因为它们通常是在将测试数据集与训练模型进行对比时运行的。'
- en: This is the typical structure of methods you will encounter for a model in an
    ML library. Now that we have a good idea of how to set up our coding environment
    and use available ML libraries, let's look at how to tune the model after our
    initial training.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你在机器学习库中遇到的一个模型的典型方法结构。现在我们已经对如何设置我们的编码环境和使用可用的机器学习库有了很好的了解，让我们看看如何在初始训练后调整模型。
- en: Tuning the model
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整模型
- en: After we have trained a simple ensemble model that performs reasonably better
    than the baseline model and achieves acceptable performance according to the expected
    performance estimated during data preparation, we can progress with optimization.
    This is a point we really want to emphasize. It's strongly discouraged to begin
    model optimization and stacking when a simple ensemble technique fails to deliver
    useful results. If this is the case, it would be much better to take a step back
    and dive deeper into data analysis and feature engineering.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练了一个简单的集成模型，该模型的表现比基线模型好得多，并且根据数据准备期间估计的预期性能达到了可接受的水平之后，我们可以进行优化。这是一个我们真的想强调的点。强烈建议不要在简单的集成技术无法提供有用结果时开始模型优化和堆叠。如果这种情况发生，退一步深入数据分析和特征工程会更好。
- en: Common ML optimization techniques—such as hyperparameter optimization, model
    stacking, and even **automated machine learning** (**AutoML**)—help you get the
    last 10% of performance boost out of your model.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的机器学习优化技术——如超参数优化、模型堆叠，甚至**自动化机器学习**（**AutoML**）——可以帮助你从模型中获得最后的10%性能提升。
- en: Hyperparameter optimization concentrates on changing the initial settings of
    the model training to improve its final performance. Similarly, model stacking
    is a very common technique used to improve prediction performance by putting a
    combination of multiple *different* model types into a single stacked model. Hence,
    the output of each model is fed into a meta-model, which itself is trained through
    cross-validation and hyperparameter tuning. By combining significantly different
    models into a single stacked model, you can always outperform a single model.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数优化专注于改变模型训练的初始设置以提高其最终性能。同样，模型堆叠是一种非常常见的用于通过将多种*不同*模型类型组合成一个单一堆叠模型来提高预测性能的技术。因此，每个模型的输出都输入到一个元模型中，该元模型本身通过交叉验证和超参数调整进行训练。通过将显著不同的模型组合成一个单一堆叠模型，你总是可以超越单个模型。
- en: 'If you decide to use any of those optimization techniques, it is advised to
    perform them in parallel and fully automated on a distributed cluster. After seeing
    too many ML practitioners manually parametrizing, tuning, and stacking models
    together, we want to raise this important message: *optimizing ML models is boring*.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定使用这些优化技术中的任何一种，建议你在分布式集群上并行和完全自动化地执行它们。在看到太多机器学习从业者手动参数化、调整和堆叠模型之后，我们想提出这个重要信息：*优化机器学习模型是无聊的*。
- en: It should rarely be done manually as it is much faster to perform it automatically
    as an end-to-end optimization process. Most of your time and effort should go
    into experimentation, data preparation, and feature engineering—that is, everything
    that cannot be easily automated and optimized using raw compute power. We will
    delve deeper into the topic of model tuning in [*Chapter 11*](B17928_11_ePub.xhtml#_idTextAnchor178),
    *Hyperparameter Tuning and Automated Machine Learning*.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这很少需要手动完成，因为作为端到端优化过程自动执行要快得多。你大部分的时间和精力应该投入到实验、数据准备和特征工程中——也就是说，所有这些都不能通过原始计算能力轻松自动化和优化的事情。我们将在[*第11章*](B17928_11_ePub.xhtml#_idTextAnchor178)“超参数调整和自动化机器学习”中更深入地探讨模型调整的主题。
- en: This concludes all important topics to know about model training. Next, we will
    have a look at options for the deployment of ML models.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了关于模型训练的所有重要主题。接下来，我们将探讨机器学习模型部署的选项。
- en: Deploying models
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型部署
- en: Once you have trained and optimized an ML model, it is ready for deployment.
    This step is typically referred to as **inferencing** or **scoring** a model.
    Many data science teams, in practice, stop here and move the model to production
    as a Docker image, often embedded in a **REpresentational State Transfer** (**REST**)
    **API** using Flask or similar frameworks. However, as you can imagine, this is
    not always the best solution, depending on your requirements. An ML or data engineer's
    responsibility doesn't stop here.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练和优化了机器学习模型，它就准备好部署了。这一步通常被称为**推理**或**评分**模型。实际上，许多数据科学团队在这里停止，并将模型作为Docker镜像部署到生产环境中，通常嵌入到**表示状态转移**（**REST**）**API**中，使用Flask或类似框架。然而，正如你可以想象的那样，这并不总是最佳解决方案，这取决于你的需求。机器学习或数据工程师的责任并不止于此。
- en: The deployment and operation of an ML pipeline can be best seen when testing
    the model on live data in production. A test is done to collect insights and data
    to continuously improve the model. Hence, collecting model performance over time
    is an essential step to guaranteeing and improving the performance of the model.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中对模型进行实时数据测试时，最能体现机器学习管道的部署和操作。进行测试是为了收集见解和数据，以持续改进模型。因此，随着时间的推移收集模型性能是一个保证和提升模型性能的重要步骤。
- en: 'In general, we differentiate two main architectures for ML-scoring pipelines,
    as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们将机器学习评分管道分为两种主要架构，如下所示：
- en: '**Batch scoring using pipelines**: An offline process where you evaluate an
    ML model against a batch of data. The result of this scoring technique is usually
    not time-critical, and the data to be scored is usually larger than the model.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理评分使用管道**：这是一个离线过程，其中你将机器学习模型与一批数据评估。这种评分技术的结果通常不是时间敏感的，要评分的数据通常比模型大。'
- en: '**Real-time scoring using a container-based web service endpoint**: This refers
    to a technique where we score single data inputs. This is very common in stream
    processing, where single events are scored in real time. It''s obvious that this
    task is highly time-critical, and the execution is blocked until the resulting
    score is computed.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于容器化Web服务端点的实时评分**：这指的是一种评分单个数据输入的技术。这在流处理中非常常见，其中单个事件会实时评分。显然，这项任务对时间非常敏感，执行会一直阻塞，直到计算出的评分结果出来。'
- en: We will discuss these two architectures in more detail in [*Chapter 14*](B17928_14_ePub.xhtml#_idTextAnchor217),
    *Model Deployments, Endpoints, and Operations*. There, we will also investigate
    an efficient way of collecting runtimes, latency, and other operational metrics,
    as well as model performance.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第14章*](B17928_14_ePub.xhtml#_idTextAnchor217)“模型部署、端点和操作”中更详细地讨论这两种架构。在那里，我们还将研究收集运行时间、延迟和其他操作指标以及模型性能的有效方法。
- en: The model files we create, and the previously mentioned options, are typically
    defined by a standard hardware architecture. As mentioned, we probably create
    a Docker image that is deployed to a **virtual machine** (**VM**) or a web service.
    What if we want to deploy our model to a highly specialized hardware environment,
    such as a GPU or a **field-programmable gate array** (**FPGA**)?
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的模型文件和之前提到的选项通常由标准硬件架构定义。正如之前提到的，我们可能创建一个Docker镜像，将其部署到**虚拟机**（**VM**）或Web服务中。如果我们想将模型部署到高度专业化的硬件环境中，比如GPU或**现场可编程门阵列**（**FPGA**）呢？
- en: To explore this further, we will dive deeper into alternative deployment targets
    and methods in [*Chapter 15*](B17928_15_ePub.xhtml#_idTextAnchor238), *Model Interoperability,
    Hardware Optimization, and Integrations*. There, we will have a look at a framework
    called **Open Neural Network eXchange** (**ONNX**) that allows us to convert our
    model into a standardized model format to be deployed to virtually any environment.
    Additionally, we have a look at FPGAs and why they might be a good deployment
    target for ML, and finally, we will explore other Azure services such as **Azure
    IoT Edge** and **Power BI** for integration.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探讨这个问题，我们将深入探讨[第15章](B17928_15_ePub.xhtml#_idTextAnchor238)中提到的替代部署目标和方法的替代方案，*模型互操作性、硬件优化和集成*。在那里，我们将研究一个名为**Open
    Neural Network eXchange**（**ONNX**）的框架，它允许我们将我们的模型转换为标准化的模型格式，以便部署到几乎任何环境中。此外，我们还将探讨FPGA及其为何可能成为机器学习的良好部署目标，最后，我们将探索其他Azure服务，如**Azure
    IoT Edge**和**Power BI**，用于集成。
- en: This step wraps up the end-to-end process for a single ML model. Next, we will
    see a short overview of how to make such ML projects operational in an enterprise-grade
    environment using MLOps.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤封装了单个机器学习模型的端到端过程。接下来，我们将简要概述如何使用MLOps在企业级环境中使此类机器学习项目投入运行。
- en: Developing and operating enterprise-grade ML solutions
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发和运营企业级机器学习解决方案
- en: To operationalize ML projects requires the use of automated pipelines and **development-operations**
    (**DevOps**) methodologies such as **continuous integration** (**CI**) and **continuous
    delivery**/**continuous deployment** (**CD**). These combined are typically referred
    to as MLOps.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使机器学习项目投入运行，需要使用自动化管道和**开发运维**（**DevOps**）方法，如**持续集成**（**CI**）和**持续交付**/**持续部署**（**CD**）。这些通常统称为MLOps。
- en: 'When looking at the steps we performed in an ML project, we can see that there
    are typically two major operations happening—the training of a model and the deployment
    of a model. As these can happen independently of one another, it is worthwhile
    defining two different automated pipelines, as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看我们在机器学习项目中执行的步骤时，我们可以看到通常有两个主要操作发生——模型的训练和模型的部署。由于这些操作可以独立进行，因此定义两个不同的自动化管道是值得的，如下所示：
- en: '**Training pipeline**: This includes loading datasets (possibly even including
    an ETL pipeline), transformation, model training, and registering final models.
    This pipeline could be triggered by changes in the dataset or possible detected
    data drift in a deployed model.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练管道**：这包括加载数据集（可能包括ETL管道）、转换、模型训练和注册最终模型。此管道可以由数据集的变化或已部署模型中检测到的可能数据漂移触发。'
- en: '**Deployment pipeline**: This includes loading of models from the registry,
    creating and deploying Docker images, creating and deploying operational scripts,
    and the final deployment of the model to the target. This pipeline could be triggered
    by new versions of an ML model.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署管道**：这包括从注册表中加载模型、创建和部署Docker镜像、创建和部署操作脚本，以及将模型最终部署到目标位置。此管道可以由机器学习模型的新版本触发。'
- en: We will have a deep dive into ML pipelining with Azure Machine Learning in [*Chapter
    8*](B17928_08_ePub.xhtml#_idTextAnchor135), *Azure Machine Learning Pipelines*.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第8章](B17928_08_ePub.xhtml#_idTextAnchor135)中深入探讨使用Azure Machine Learning的机器学习管道，*Azure
    Machine Learning Pipelines*。
- en: 'Having these pipelines, we can then turn our eye on **Azure DevOps** besides
    other tooling. With that, we can build a life cycle for our ML projects defined
    by the following parts:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在拥有这些管道之后，我们还可以关注**Azure DevOps**以及其他工具。有了这些，我们可以为我们的机器学习项目构建以下部分定义的生命周期：
- en: '**Creating or retraining a model**: Here, we use training pipelines to create
    or retrain our model while version-controlling the pipelines and the code.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建或重新训练模型**：在此，我们使用训练管道创建或重新训练我们的模型，同时控制管道和代码的版本。'
- en: '**Deploying the model and creating scoring files and dependencies**: Here,
    we use a deployment pipeline to deploy a specific model version while version-controlling
    the pipeline and the code.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署模型并创建评分文件和依赖项**：在此，我们使用部署管道部署特定的模型版本，同时控制管道和代码的版本。'
- en: '**Creating an audit trail**: Through CI/CD pipelines and version control, we
    create an audit trail for all assets ensuring integrity and compliance.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建审计跟踪**：通过CI/CD管道和版本控制，我们为所有资产创建审计跟踪，确保完整性和合规性。'
- en: '**Monitoring model in production**: We monitor the performance and possible
    data drift, which might automatically trigger retraining of the model.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在生产中监控模型**：我们监控性能和可能的数据漂移，这可能会自动触发模型的重新训练。'
- en: We will discuss these topics and others in more detail in [*Chapter 16*](B17928_16_ePub.xhtml#_idTextAnchor252),
    *Bringing Models into Production with MLOps*.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第16章*](B17928_16_ePub.xhtml#_idTextAnchor252)“使用MLOps将模型投入生产”中更详细地讨论这些主题和其他内容。
- en: This concludes our discussion on the end-to-end ML process and this chapter.
    If you hadn't already, you should now have a good understanding of ML and what
    to expect in the rest of the book.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对端到端机器学习过程和本章的讨论。如果你之前还没有，你现在应该对机器学习以及本书剩余部分的内容有一个很好的理解。
- en: Summary
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned in which situations we should use ML and where it
    is coming from, we understood basic concepts of statistics and the mathematical
    knowledge we require for ML, and we discovered the steps we need to go through
    to create a performing ML model. In addition, we had a first glimpse at what is
    required to operationalize ML projects. This should give a base idea of what ML
    is about and what we will dive into in this book.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了在哪些情况下我们应该使用机器学习以及它的来源，我们了解了统计学的基本概念以及我们进行机器学习所需的数学知识，我们还发现了创建一个表现良好的机器学习模型所需的步骤。此外，我们还对将机器学习项目投入运营所需的内容有了初步的了解。这应该为我们提供一个关于机器学习是什么以及我们将在这本书中深入研究什么的基本概念。
- en: As this book not only covers ML but also the cloud platform Azure, in the next
    two chapters, we will go deeper into a topic that we have not covered so far—we
    will speak about tooling for ML. Therefore, in the next chapter, we will discover
    what Azure has to offer in the form of tools and services for ML, and in the third
    chapter, we will use the most useful tool to run our first hands-on experimentation
    with ML on Azure.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书不仅涵盖了机器学习（ML），还包括了云平台Azure，在接下来的两章中，我们将深入探讨我们之前未曾涉及的主题——我们将讨论机器学习的工具。因此，在下一章中，我们将了解Azure为机器学习提供的工具和服务，而在第三章中，我们将使用最有用的工具在我们的Azure上进行第一次实际的机器学习实验。
