- en: '*Chapter 1*: Understanding the End-to-End Machine Learning Process'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第一章*：理解端到端机器学习流程'
- en: Welcome to the second edition of *Mastering Azure Machine Learning*. In this
    first chapter, we want to give you an understanding of what kinds of problems
    require the use of **machine learning** (**ML**), how the full ML process unfolds,
    and what knowledge is required to navigate this vast terrain. You can view it
    as an introduction to ML and an overview of the book itself, where for most topics
    we will provide you with a reference to upcoming chapters so that you can easily
    find your way around the book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到《精通Azure机器学习》的第二版。在本章的第一章，我们希望让你了解哪些类型的问题需要使用**机器学习**（**ML**），完整的机器学习过程是如何展开的，以及在这个广阔领域导航所需的哪些知识。你可以将其视为机器学习的介绍和本书的概述，其中对于大多数主题，我们将提供对后续章节的参考，以便你能够轻松地在书中找到自己的位置。
- en: In the first section, we will ask ourselves what ML is, when we should use it,
    and where it comes from. In addition, we will reflect on how ML is just another
    form of programming.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们将问自己什么是机器学习，我们应该在何时使用它，以及它从何而来。此外，我们将反思机器学习只是编程的另一种形式。
- en: In the second section, we will lay the mathematical groundwork you require to
    process data, and we will understand that the data you work with probably cannot
    be fully trusted. Further, we will look at different classes of ML algorithms,
    how they are defined, and how we can define the performance of a trained model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们将奠定你处理数据所需的数学基础，并且我们将理解你工作的数据可能无法完全信赖。此外，我们将探讨不同类别的机器学习算法，它们的定义，以及我们如何定义训练模型的性能。
- en: Finally, in the third section, we will have a look at the end-to-end process
    of an ML project. We will understand where to get data from, how to preprocess
    data, how to choose a fitting model, and how to deploy this model into production
    environments. This will also get us into the topic of **ML operations**, known
    as **MLOps**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第三部分，我们将探讨机器学习项目的端到端流程。我们将了解从哪里获取数据，如何预处理数据，如何选择合适的模型，以及如何将此模型部署到生产环境中。这也会让我们接触到**机器学习操作**的话题，通常称为**MLOps**。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Grasping the idea behind ML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习背后的理念
- en: Understanding the mathematical basis for statistical analysis and ML modeling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解统计分析和机器学习建模的数学基础
- en: Discovering the end-to-end ML process
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现端到端机器学习流程
- en: Grasping the idea behind ML
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习背后的理念
- en: The terms **artificial intelligence** (**AI**) and—partially—**ML** are omnipresent
    in today's world. However, a lot of what is found under the term *AI* is often
    nothing more than a containerized ML solution, and to make matters worse, ML is
    sometimes unnecessarily used to solve something extremely simple.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 术语**人工智能**（**AI**）和部分地**机器学习**（**ML**）在当今世界无处不在。然而，在**AI**这个术语下找到的很多东西往往不过是一个容器化的机器学习解决方案，而且更糟糕的是，机器学习有时被不必要地用于解决极其简单的问题。
- en: Therefore, in this first section, let's understand the class of problems ML
    tries to solve, in which scenarios to use ML, and when not to use it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本节的第一部分，让我们了解机器学习试图解决的问题类别，在哪些场景下使用机器学习，以及在何时不应使用它。
- en: Problems and scenarios requiring ML
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要机器学习的问题和场景
- en: 'If you look for a definition of ML, you will often find a description such
    as this: *It is the study of self-improving machine algorithms using data*. ML
    is basically described as an algorithm we are trying to evolve, which in turn
    can be seen as one complex mathematical function.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你寻找机器学习的定义，你通常会找到一个这样的描述：*它是使用数据研究自我改进的机器算法的学科*。机器学习基本上被描述为我们试图进化的算法，这反过来又可以被视为一个复杂的数学函数。
- en: Any computer process today follows the simple structure of the **input-process-output
    (IPO) model**. We define allowed inputs, we define a process working with those
    inputs, and we define an output through the type of results the process will show
    us. A simple example would be a word processing application, where every keystroke
    will result in a letter shown as the output on the screen. A completely different
    process might run in parallel to that one, having a time-based trigger to store
    the text file periodically to a hard disk.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的任何计算机过程都遵循简单的**输入-处理-输出（IPO）模型**结构。我们定义允许的输入，我们定义一个处理这些输入的过程，并通过该过程将显示的结果类型定义输出。一个简单的例子是一个文字处理应用程序，其中每个按键都会在屏幕上显示为输出字母。可能并行运行的一个完全不同的过程可能有一个基于时间的触发器，定期将文本文件存储到硬盘上。
- en: All these processes or algorithms have one thing in common—they were manually
    written by someone using a **high-level programming language**. It is clear which
    actions need to be done when someone presses a letter in a word processing application.
    Therefore, we can easily build a process in which we implement which input values
    should create which output values.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些过程或算法有一个共同点——它们都是某人使用**高级编程语言**手动编写的。当有人在文字处理应用程序中按下一个字母时，需要执行哪些操作是显而易见的。因此，我们可以轻松构建一个过程，其中我们实现哪些输入值应该创建哪些输出值。
- en: 'Now, let''s look at a more complex problem. Imagine we have a picture of a
    dog and want an application to just say: *This is a dog*. This sounds simple enough,
    as we know the input *picture of a dog* and the output value *dog*. Unfortunately,
    our brain (our own machine) is far superior to the machines we built, especially
    when it comes to pattern recognition. For a computer, a picture is just a square
    of ![](img/B17928_Formula_1.01.png) pixels, each containing three color channels
    defined by an 8-bit or 10-bit value. Therefore, an image is just a bunch of pixels
    made up of vectors for the computer, so in essence, a lot of numbers.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个更复杂的问题。想象一下，我们有一张狗的图片，并希望一个应用程序能够说：*这是一只狗*。这听起来很简单，因为我们知道输入*狗的图片*和输出值*狗*。不幸的是，我们的大脑（我们自己的机器）比我们构建的机器优越得多，尤其是在模式识别方面。对于计算机来说，一张图片只是一个由
    ![](img/B17928_Formula_1.01.png) 像素组成的正方形，每个像素包含由8位或10位值定义的三个颜色通道。因此，对于计算机来说，图像只是一堆由向量组成的像素，本质上是一堆数字。
- en: We could manually start writing an algorithm that maybe clusters groups of pixels,
    looks for edges and points of interest, and eventually, with a lot of effort,
    we might succeed in having an algorithm that finds dogs in pictures. That is when
    we get a picture of a cat.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以手动开始编写一个算法，可能聚类像素组，寻找边缘和感兴趣点，最终，经过大量努力，我们可能成功拥有一个在图片中找到狗的算法。但那时我们得到了一张猫的图片。
- en: 'It should be clear to you by now that we might run into a problem. Therefore,
    let''s define one problem that ML solves, as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该已经清楚我们可能会遇到问题。因此，让我们定义一个机器学习解决的问题，如下：
- en: '*Building the desired algorithm for a required solution programmatically is
    either extremely time-consuming, completely unfeasible, or impossible.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*以编程方式构建所需解决方案的期望算法要么非常耗时，要么完全不切实际，或者根本不可能。*'
- en: Taking this description, we can surely define good scenarios to use ML, be it
    finding objects in images and videos or understanding voices and extracting their
    intent from audio files. We will further understand what building ML solutions
    entails throughout this chapter (and the rest of the book, for that matter), but
    to make a simple statement, let's just acknowledge that building an ML model is
    also a time-consuming matter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个描述，我们可以确定使用机器学习的良好场景，无论是寻找图像和视频中的对象，还是理解声音并从音频文件中提取其意图。我们将在本章（以及本书的其余部分）中进一步了解构建机器学习解决方案涉及的内容，但简单来说，让我们承认构建机器学习模型也是一个耗时的事情。
- en: In that vein, it should be of utmost importance to avoid ML if we have the chance
    to do so. This might be an obvious statement, but as we (the authors) can attest,
    it is not for a lot of people. We have seen projects realized with ML where the
    output could be defined with a simple combination of `if` statements given some
    input vectors. In such scenarios, a solution could be obtained with a couple of
    hundred lines of code. Instead, months of training and testing an ML algorithm
    occurred, costing a lot of time and resources.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，如果我们有机会避免使用机器学习，那么这应该是最重要的。这或许是一个显而易见的陈述，但正如我们（作者）可以证明的那样，对很多人来说并非如此。我们见过一些使用机器学习实现的项目，如果给定一些输入向量，输出可以通过简单的`if`语句组合来定义。在这种情况下，可以通过几百行代码获得解决方案。相反，对机器学习算法进行数月的训练和测试，耗费了大量时间和资源。
- en: An example of this would be a company wanting to predict fraud (stolen money)
    committed by their own employees in a retail store. You might have heard that
    predicting fraud is a typical scenario for ML. Here, it was *not necessary* to
    use ML, as the company already knew the influencing factors (length of time the
    cashier was open, error codes on return receipts, and so on) and therefore wanted
    to be alerted when certain combinations of these factors occurred. As they knew
    the factors already, they could have just written the code and be done with it.
    But what does this scenario tell us about ML?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一家公司可能想要预测其零售店员工犯下的欺诈（被盗资金）。你可能听说过预测欺诈是机器学习的典型场景。在这里，使用机器学习并不是必要的，因为公司已经知道影响因素（收银员开放的时间长度、退货收据上的错误代码等），因此希望在发生某些因素组合时得到警报。既然他们已经知道这些因素，他们可以直接编写代码并完成。但这个场景告诉我们关于机器学习什么？
- en: 'So far, we have looked at ML as a solution to solve a problem that, in essence,
    is too hard to code. Looking at the preceding scenario, you might understand another
    aspect or another class of problems that ML can solve. Therefore, let''s add a
    second problem description, as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将机器学习视为解决一个本质上难以编码的问题的解决方案。从前面提到的场景来看，你可能理解了机器学习可以解决的另一个方面或另一类问题。因此，让我们添加第二个问题描述，如下：
- en: '*Building the desired algorithm for a required solution is not feasible, as
    the influencing factors for the outcome of the desired outputs are only partially
    known or completely unknown.*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*构建一个所需解决方案的期望算法是不切实际的，因为影响所需输出结果的因素只有部分已知或完全未知。*'
- en: Looking at this problem, you might now understand why ML relies so heavily on
    the field of statistics as, through the application of statistics, we can learn
    how data points influence one another, and therefore we might be able to solve
    such a problem. At the same time, we can build an algorithm that can find and
    predict the desired outcome.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 看到这个问题，你现在可能已经理解为什么机器学习如此依赖于统计学领域，因为通过应用统计学，我们可以了解数据点是如何相互影响的，因此我们可能能够解决这样的问题。同时，我们可以构建一个能够找到并预测所需结果的算法。
- en: In the previously mentioned scenario for detecting fraud, it might be prudent
    to still use ML, as it may be able to find a combination of influencing factors
    no one has thought about. But if this is not your set goal—as it was not in this
    case—you should not use ML for something that is easily written in code.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前提到的检测欺诈的场景中，仍然使用机器学习可能是谨慎的，因为它可能能够找到没有人考虑过的影响因素的组合。但如果这不是你的目标——正如在这个案例中那样——你不应该将机器学习用于那些可以轻易用代码编写的事情。
- en: Now that we have discussed some of the problems solved by ML and have had a
    look at some scenarios for ML, let's have a look at how ML came to be.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了一些机器学习解决的问题，并查看了一些机器学习的场景，让我们来看看机器学习是如何产生的。
- en: The history of ML
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习的历史
- en: To understand ML as a whole, we must first understand where it comes from. Therefore,
    let's delve into the history of ML. As with all events in history, different currents
    are happening simultaneously, adding pieces to the whole picture. We'll now look
    at a few important pillars that birthed the idea of ML as we know it today.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要全面理解机器学习，我们首先必须了解它的来源。因此，让我们深入了解机器学习的历史。与历史上的所有事件一样，不同的潮流同时发生，为整个画面增添了碎片。现在，我们将查看几个重要的支柱，这些支柱孕育了我们今天所知道的机器学习的理念。
- en: Learnings from neuroscience
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经科学的学习
- en: 'A neuropsychologist named Donald O. Hebb published a book titled *The Organization
    of Behavior* in 1949\. In this book, he described his theory of how **neurons**
    (neural cells) in our brain function, and how they contribute to what we understand
    as *learning*. This theory is known as **Hebbian learning**, and it makes the
    following proposition:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一位名叫唐纳德·O·赫布（Donald O. Hebb）的神经心理学家于1949年出版了一本名为《行为组织》（The Organization of Behavior）的书。在这本书中，他描述了他关于我们大脑中的**神经元**（神经细胞）如何工作以及它们如何贡献于我们所理解的**学习**的理论。这个理论被称为**赫布学习**（Hebbian
    learning），并提出了以下命题：
- en: When an axon of cell A is near enough to excite cell B and repeatedly or persistently
    takes part in firing it, some growth process or metabolic change takes place in
    one or both cells such that A's efficiency, as one of the cells firing B, is increased.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当细胞A的轴突足够接近以兴奋细胞B，并且反复或持续地参与其放电时，一个或两个细胞中会发生一些生长过程或代谢变化，使得A作为放电B的细胞之一，其效率得到提高。
- en: This basically describes that there is a process where one cell excites another
    repeatedly (the initiating cell) and maybe even the receiving cell is changed
    through a hidden process. This process is what we call learning.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上描述了一个过程，其中一个细胞反复激发另一个细胞（启动细胞），甚至接收细胞可能通过一个隐藏的过程被改变。这个过程就是我们所说的学习。
- en: 'To understand this a bit more visually, let''s have a look at the biological
    structure of a neuron, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地理解这一点，让我们看一下神经元的生物结构，如下所示：
- en: '![Figure 1.1 – Neuron in a biological neural network ](img/B17928_01_01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – 生物神经网络中的神经元](img/B17928_01_01.jpg)'
- en: Figure 1.1 – Neuron in a biological neural network
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 生物神经网络中的神经元
- en: What is visualized here? Firstly, on the left, we see the main body of the cell
    and its nucleus. The body receives input signals through dendrites that are connected
    to other neurons. In addition, there is a larger exit perturbing from the body
    called the axon, which connects the main body through a chain of Schwann cells
    to the so-called axon terminal, which in turn connects again to other neurons.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可视化的是什么？首先，在左边，我们看到细胞的主体及其细胞核。主体通过与其它神经元相连的树突接收输入信号。此外，还有一个从主体伸出的大轴突，它通过一系列施万细胞将主体与所谓的轴突末端连接起来，而轴突末端反过来又连接到其他神经元。
- en: Looking at this structure with some creativity, it certainly resembles what
    a function or an algorithm might be. We have input signals coming from external
    neurons, we have some hidden process happening with these signals, and we have
    an output in the form of an axon terminal that connects the results to other neurons,
    and therefore other processes again.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 用一些创意来看这个结构，它确实类似于一个函数或算法。我们有来自外部神经元的输入信号，我们有这些信号的一些隐藏过程，我们有一个以轴突末端形式存在的输出，它将结果连接到其他神经元，从而再次连接到其他过程。
- en: It would take another decade again for someone to realize this connection.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 又过了十年，才有人意识到这个联系。
- en: Learnings from computer science
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算机科学的学习经验
- en: It is hard to talk about the history of ML in the context of computer science
    without mentioning one of the fathers of modern machines, Alan Turing. In a paper
    called *Computing Machinery and Intelligence* published in 1950, Turing defines
    a test called the **Imitation Game** (later called the **Turing test**) to evaluate
    whether a machine shows human behavior indistinguishable from a human. There are
    multiple iterations and variants of the test, but in essence, the idea is that
    a person would at no point in a conversation get the feeling they are not speaking
    with a human.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学背景下谈论机器学习的历史，如果不提及现代机器之父之一艾伦·图灵，那就很难。在1950年发表的一篇名为《计算机与智能》的论文中，图灵定义了一个被称为**模仿游戏**（后来称为**图灵测试**）的测试，用以评估机器是否表现出与人类无法区分的行为。这个测试有多个迭代和变体，但本质上，其想法是，在对话中，一个人在任何时候都不会感觉到他们不是在与人类交谈。
- en: Certainly, this test is flawed, as there are ways to give relatively intelligent
    answers to questions while not being intelligent at all. If you want to learn
    more about this, have a look at **ELIZA** built by Joseph Weizenbaum, which passed
    the Turing test.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个测试是有缺陷的，因为有一些方法可以在不真正智能的情况下给出相对智能的答案。如果你想了解更多关于这个的信息，可以看看约瑟夫·魏森鲍姆建造的**ELIZA**，它通过了图灵测试。
- en: Nevertheless, this paper triggered one of the first discussions on what AI could
    be and what it means that a machine can learn.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这篇论文引发了关于人工智能可能是什么以及机器学习意味着什么的第一次讨论。
- en: Living in these exciting times, Arthur Samuel, a researcher working at **International
    Business Machines Corporation** (**IBM**) at that time, started developing a computer
    program that could make the right decisions in a game of checkers. In each move,
    he let the program evaluate a scoring function that tried to measure the chances
    of winning for each available move. Limited by the available resources at the
    time, it was not feasible to calculate all possible combinations of moves all
    the way to the end of the game.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些激动人心的时代生活，亚瑟·塞缪尔，当时在**国际商业机器公司**（**IBM**）工作的研究人员，开始开发一个能够在国际象棋游戏中做出正确决策的计算机程序。在每一步中，他让程序评估一个评分函数，试图衡量每个可用步骤的获胜机会。由于当时可用的资源有限，计算所有可能的移动组合直到游戏结束是不切实际的。
- en: This first step led to the definition of the so-called **minimax algorithm**
    and its accompanying **search tree**, which can commonly be used for any two-player
    adversarial game. Later, the **alpha-beta pruning** algorithm was added to automatically
    trim the tree from decisions that did not lead to better results than the ones
    already evaluated.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这第一步导致了所谓的**最小-最大算法**及其伴随的**搜索树**的定义，这通常可以用于任何两人对抗游戏。后来，添加了**alpha-beta剪枝**算法，以自动从那些没有比已评估的结果更好的决策中剪枝。
- en: 'We are talking about Arthur Samuel, as it was he who coined the name *machine
    learning*, defining it as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在谈论Arthur Samuel，因为正是他提出了“机器学习”这个名称，并将其定义为如下：
- en: The field of study that gives computers the ability to learn without being explicitly
    programmed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 研究领域赋予计算机在不被明确编程的情况下学习的能力。
- en: Combining these first ideas of building an evaluation function for training
    a machine and the research done by Donald O. Hebb in neuroscience, Frank Rosenblatt,
    a researcher at the Cornell Aeronautical Laboratory, invented a new linear classifier
    that he called a **perceptron**. Even though his progress in building this perceptron
    into hardware was relatively short-lived and would not live up to its potential,
    its original definition is nowadays the basis for every neuron in an **artificial
    neural network** (**ANN**).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结合为训练机器构建评估函数的第一个想法和Donald O. Hebb在神经科学领域的研究，Cornell Aeronautical Laboratory的研究员Frank
    Rosenblatt发明了一种新的线性分类器，他称之为**感知器**。尽管他在将这个感知器构建成硬件方面的进展相对短暂，并且没有达到其潜力，但它的原始定义如今是**人工神经网络**（**ANN**）中每个神经元的基石。
- en: Therefore, let's now dive deeper into understanding how ANNs work and what we
    can deduce about the inner workings of an ML algorithm from them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在让我们更深入地了解ANN是如何工作的，以及我们可以从它们中推断出关于ML算法内部工作原理的什么。
- en: Understanding the inner workings of ML through the example of ANNs
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过ANN的例子理解ML的内部工作原理
- en: 'ANNs, as we know them today, are defined by the following two major components,
    one of which we learned about already:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天所知的ANN由以下两个主要组件定义，其中一个我们已经了解过：
- en: '**The neural network**: The base structure of the system. A perceptron is basically
    an NN with only one neuron. By now, this structure comes in multiple facets, often
    involving hidden layers of hundreds of neurons, in the case of **deep neural networks**
    (**DNNs**).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络**：系统的基本结构。感知器基本上是一个只有一个神经元的NN。到目前为止，这种结构已经以多种形式出现，通常涉及数百个神经元的隐藏层，在**深度神经网络**（**DNNs**）的情况下。'
- en: '**The backpropagation function**: A rule for the system to learn and evolve.
    An idea thought of in the 1970s came into appreciation through a paper called
    *Learning Representations by Back-Propagating Errors* by *D. Rumelhart*, *Geoffrey
    E. Hinton*, *Ronald J. Williams* in 1986.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播函数**：系统学习和进化的规则。20世纪70年代的一个想法，通过1986年由D. Rumelhart、Geoffrey E. Hinton和Ronald
    J. Williams发表的名为《通过反向传播错误学习表示》的论文而受到重视。'
- en: To understand these two components and how they work in tandem with each other,
    let's have a deeper look at both.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这两个组件以及它们如何协同工作，让我们更深入地了解它们。
- en: The neural network
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'First, let''s understand how a single neuron operates, which is very close
    to the idea of a perceptron defined by Rosenblatt. The following diagram shows
    the inner workings of such an artificial neuron:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解单个神经元是如何工作的，这非常接近Rosenblatt定义的感知器理念。以下图表显示了这种人工神经元的内部工作原理：
- en: '![Figure 1.2 – Neuron in an ANN ](img/B17928_01_02.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – ANN中的神经元](img/B17928_01_02.jpg)'
- en: Figure 1.2 – Neuron in an ANN
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – ANN中的神经元
- en: We can clearly see the similarities to a real neuron. We get inputs from the
    connected neurons called ![](img/B17928_Formula_1.02.png). Each of those inputs
    is weighted with a corresponding weight ![](img/B17928_Formula_1.03.png), and
    then, in the neuron itself, they are all summed up, including a **bias** ![](img/B17928_Formula_1.04.png).
    This is often referred to as the **net input function**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到它与真实神经元的相似之处。我们从称为 ![](img/B17928_Formula_1.02.png) 的连接神经元那里获得输入。每个输入都对应一个权重
    ![](img/B17928_Formula_1.03.png)，然后在神经元本身中，它们都被加起来，包括一个**偏差** ![](img/B17928_Formula_1.04.png)。这通常被称为**净输入函数**。
- en: 'As the final operation, a so-called **activation function** ![](img/B17928_Formula_1.05.png)
    is applied to this net input that decides how the output signal of the neuron
    should look. This function must be continuous and differentiable and should typically
    create results in the range of [0:1] or [-1:1] to keep results scaled. In addition,
    this function could be linear or non-linear in nature, even though using a linear
    activation function has its downfalls, as described next:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最终操作，一个所谓的**激活函数** ![](img/B17928_Formula_1.05.png) 应用于这个网络输入，决定神经元的输出信号应该如何看起来。这个函数必须是连续且可微分的，并且通常应该在[0:1]或[-1:1]的范围内创建结果，以保持结果缩放。此外，这个函数可以是线性的或非线性的，尽管使用线性激活函数有其缺点，如以下所述：
- en: You cannot learn a non-linear relationship presented in your data through a
    system of linear functions.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您无法通过线性函数系统学习数据中呈现的非线性关系。
- en: A multilayered network made up of nodes with only linear activation functions
    can be broken down to just one layer of nodes with one linear activation function,
    making the network obsolete.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由只有线性激活函数的节点组成的分层网络可以被简化为只有一个线性激活函数的一层节点，从而使网络变得过时。
- en: You cannot use a linear activation function with backpropagation, as this requires
    calculating the derivative of this function, which we will discuss next.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不能使用线性激活函数进行反向传播，因为这需要计算该函数的导数，我们将在下一节讨论。
- en: 'Commonly used activation functions are **sigmoid**, **hyperbolic tangent**
    (**tanh**), **rectified linear unit** (**ReLU**), and **softmax**. Keeping this
    in mind, let''s have a look at how we connect neurons together to achieve an ANN.
    A whole network is typically defined by three types of layers, as outlined here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的激活函数包括**sigmoid**、**双曲正切**（**tanh**）、**整流线性单元**（**ReLU**）和**softmax**。牢记这一点，让我们来看看我们如何连接神经元以实现人工神经网络（ANN）。一个整个网络通常由三种类型的层定义，如下所述：
- en: '**Input layer**: Consists of neurons accepting singular input signals (not
    a weighted sum) to the network. Their weights might be constant or randomized
    depending on the application.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：由接受网络单个输入信号的神经元组成（不是加权求和）。它们的权重可能根据应用是常数或随机化的。'
- en: '**Hidden layer**: Consists of the types of neurons we described before. They
    are defined by an activation function and given weights to the weighted sum of
    the input signals. In DNNs, these layers typically represent specific transformation
    steps.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：由我们之前描述的神经元类型组成。它们由一个激活函数定义，并给输入信号的加权求和分配权重。在深度神经网络（DNNs）中，这些层通常代表特定的转换步骤。'
- en: '**Output layer**: Consists of neurons performing the final transformation of
    the data. They can behave like neurons in hidden layers, but they do not have
    to.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：由执行数据最终转换的神经元组成。它们可以像隐藏层中的神经元一样行为，但不必如此。'
- en: 'These together result in a typical ANN, as shown in the following diagram:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些共同构成了典型的ANN，如下面的图所示：
- en: '![Figure 1.3 – ANN with one hidden layer ](img/B17928_01_03.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3 – 具有一个隐藏层的人工神经网络](img/B17928_01_03.jpg)'
- en: Figure 1.3 – ANN with one hidden layer
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 具有一个隐藏层的人工神经网络
- en: 'With this, we build a generic structure that can receive some input, realize
    some form of mathematical function through different layers of weights and activation
    functions, and in the end, hopefully show the correct output. This process of
    pushing information through the network from inputs to outputs is typically referred
    to as **forward propagation**. This, of course, only shows us what is happening
    with an input that passes through the network. The following question remains:
    *How does it learn the desired function in the first place?* The next section
    will answer this question.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们构建了一个通用的结构，它可以接收一些输入，通过不同层的权重和激活函数实现某种形式的数学函数，最终，希望显示正确的输出。这个过程通常被称为**前向传播**。当然，这只能显示通过网络的输入所发生的事情。以下问题仍然存在：*它最初是如何学习所需函数的？*下一节将回答这个问题。
- en: The backpropagation function
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播函数
- en: 'The question that should have popped up in your mind by now is: *How do we
    define the correct output?* To have a way to change the behavior of the network,
    which mostly boils down to changing the values of the weights in the system, don''t
    we need a way to quantize the error the system made?'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该已经想到一个问题：*我们如何定义正确的输出？*为了有一种方式来改变网络的行为，这主要归结为改变系统中权重的值，我们不需要一种量化系统所犯错误的方法吗？
- en: Therefore, we need a function describing the error or loss, referred to as a
    **loss function** or **error function**. You might have even heard another name—a
    **cost function**. Let's define them next.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要一个描述错误或损失的函数，称为 **损失函数** 或 **误差函数**。你可能甚至听说过另一个名字——**代价函数**。让我们接下来定义它们。
- en: Loss Function versus Cost Function
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数与代价函数
- en: A loss function (error function) computes the error for a single training example.
    A cost function, on the other hand, averages all loss function results for the
    entire training dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数（误差函数）计算单个训练示例的误差。另一方面，代价函数平均整个训练数据集的所有损失函数结果。
- en: This is the correct definition for those terms, but they are often used interchangeably.
    Just keep in mind that we are using some form of metric to measure the error we
    made or the distance we have from the correct results.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这些术语的正确定义，但它们通常可以互换使用。只需记住，我们正在使用某种形式的度量来衡量我们犯的错误或与正确结果的距离。
- en: In classic backpropagation and other ML scenarios, the **mean squared error**
    (**MSE**) between the correct ![](img/B17928_Formula_1.06.png) and the computed
    ![](img/B17928_Formula_1.07.png) is used to define the error or loss of the operation.
    The obvious target is to now minimize this error. Therefore, the actual task to
    perform is to find the total minimum of this function in *n*-dimensional space.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的反向传播和其他机器学习场景中，正确 ![](img/B17928_Formula_1.06.png) 和计算得到的 ![](img/B17928_Formula_1.07.png)
    之间的 **均方误差** (**MSE**) 用于定义操作的错误或损失。显然的目标是现在最小化这个错误。因此，实际要执行的任务是在 *n*-维空间中找到这个函数的总最小值。
- en: To do this, we use something that is often referred to as an **optimizer**,
    defined next.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们使用通常被称为 **优化器** 的东西，定义如下。
- en: Optimizer (Objective Function)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器（目标函数）
- en: An optimizer is a function that implements a specific way to reach the objective
    of minimizing the cost function.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是一个实现达到最小化代价函数目标特定方式的函数。
- en: 'One such optimizer is an iterative process called **gradient descent**. Its
    idea is visualized in the following screenshot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种优化器是一个称为 **梯度下降** 的迭代过程。其思想在以下屏幕截图中进行可视化：
- en: '![Figure 1.4 – Gradient descent with loss function influenced by only one input
    (left: finding global minimum, right: stuck in local minimum) ](img/B17928_01_04.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – 仅受一个输入影响的梯度下降损失函数（左：寻找全局最小值，右：陷入局部最小值）](img/B17928_01_04.jpg)'
- en: 'Figure 1.4 – Gradient descent with loss function influenced by only one input
    (left: finding global minimum, right: stuck in local minimum)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 仅受一个输入影响的梯度下降损失函数（左：寻找全局最小值，右：陷入局部最小值）
- en: In gradient descent, we try to navigate an *n*-dimensional loss function by
    taking reasonably large enough steps, often defined by a *learning rate*, with
    the goal to find the global minimum, while avoiding getting stuck in a local minimum.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降中，我们通过采取合理足够大的步长（通常由 **学习率** 定义），试图导航 *n*-维损失函数，目标是找到全局最小值，同时避免陷入局部最小值。
- en: 'Keeping this in mind and without going into too much detail, let''s finish
    this thought by going through the steps the backpropagation algorithm performs
    on the neural network. These are set out here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这一点，不深入细节，让我们通过回顾反向传播算法在神经网络中执行的步骤来完成这个想法。这些步骤如下：
- en: Pass a pair ![](img/B17928_Formula_1.08.png) through the network (forward propagation).
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一对 ![](img/B17928_Formula_1.08.png) 通过网络（前向传播）。
- en: Compute the loss between the expected ![](img/B17928_Formula_1.09.png) and the
    computed ![](img/B17928_Formula_1.10.png).
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算预期 ![](img/B17928_Formula_1.09.png) 和计算得到的 ![](img/B17928_Formula_1.10.png)
    之间的损失。
- en: Compute all derivatives for all functions and weights throughout the layers
    using a mathematical chain rule.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数学链式法则计算所有函数和权重在所有层中的所有导数。
- en: Update all weights beginning from the back of the network to the front, with
    slightly changed weights defined by the optimizer.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从网络的后面开始更新所有权重到前面，使用优化器定义的略微改变的权重。
- en: Repeat until convergence is achieved (the weights are not receiving any meaningful
    updates anymore).
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行，直到达到收敛（权重不再接收任何有意义的更新）。
- en: This is, in a nutshell, how an ANN learns. Be aware that it is vital to constantly
    change the pairs in *Step 1*, as otherwise, you might push the network too far
    into memorizing these couple of pairs you constantly showed it. We will discuss
    the phenomenon of **overfitting** and **underfitting** later in this chapter.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这就是人工神经网络（ANN）是如何学习的。请注意，在第一步中不断改变对是至关重要的，否则，你可能会使网络过度记住你不断展示给它的这些几个对。我们将在本章后面讨论**过拟合**和**欠拟合**的现象。
- en: As a final step in this section, let's now bring together what we have learned
    so far about ML and what this means for building software solutions in the future.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本节的最后一步，现在让我们将到目前为止关于机器学习所学到的东西以及这对未来构建软件解决方案意味着什么结合起来。
- en: ML and Software 2.0
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习和软件2.0
- en: What we learned so far is that ML seems to be defined by a base structure with
    various knobs and levers (settings and values) that can be changed. In the case
    of ANNs, that would be the structure of the network itself and the weights, bias,
    and activation function we can set in some regard.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所学的似乎表明机器学习是由一个具有各种旋钮和杠杆（设置和值）的基础结构定义的，这些旋钮和杠杆可以改变。在人工神经网络（ANN）的情况下，这将是网络本身的结构以及我们可以设置的一些权重、偏置和激活函数。
- en: Accompanying this base structure is some sort of rule or function as to how
    these knobs and levers should be transformed through a learning process. In the
    case of ANNs, this is defined through the backpropagation function, which combines
    a loss function with an optimizer and some math.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与这个基础结构相伴的是某种规则或函数，用于在学习过程中如何将这些旋钮和杠杆转换。在人工神经网络（ANN）的情况下，这是通过反向传播函数定义的，该函数结合了损失函数、优化器和一些数学。
- en: In 2017, Andrej Karpathy, the **chief technical officer** (**CTO**) of Tesla's
    AI division, proposed that the aforementioned idea could be just another way of
    programming, which he called **Software 2.0** ([https://karpathy.medium.com/software-2-0-a64152b37c35](https://karpathy.medium.com/software-2-0-a64152b37c35)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，特斯拉人工智能部门的首席技术官（CTO）安德烈·卡帕西（Andrej Karpathy）提出，上述想法可能是编程的另一种方式，他称之为**软件2.0**（[https://karpathy.medium.com/software-2-0-a64152b37c35](https://karpathy.medium.com/software-2-0-a64152b37c35)）。
- en: Up to this point, writing software was about explaining to the machine precisely
    what it must do and what outcome it must produce through defining specific commands
    it had to follow. In this classical software development paradigm, we define algorithms
    by their code and let data run through it, typically written in a reasonably readable
    language.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，编写软件是关于通过定义它必须遵循的特定命令，向机器精确地解释它必须做什么以及它必须产生什么结果。在这种经典的软件开发范例中，我们通过代码定义算法，让数据通过它运行，通常是用一种相对可读的语言编写的。
- en: Instead of doing that, another idea could be to define a program we build by
    a base structure, a way to evolve this structure, and the type of data it must
    process. In this case, we get something very human-unfriendly to understand (an
    ANN with weights, for example), but it might be much better to understand for
    a machine.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是这样做，另一个想法可能是定义一个由基础结构、进化这种结构的方式以及它必须处理的数据类型组成的程序。在这种情况下，我们得到的是对人类非常不友好的东西（例如具有权重的ANN），但对于机器来说可能更容易理解。
- en: So, we leave you at the end of this section with the thought that Andrej wanted
    to convey. Perhaps ML is just another form of programming machines.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在本节的结尾留下安德烈想要传达的思想。也许机器学习只是编程机器的另一种形式。
- en: Keeping all this in mind, let's now talk about math.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在记住所有这些的同时，现在让我们谈谈数学。
- en: Understanding the mathematical basis for statistical analysis and ML modeling
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解统计分析和机器学习建模的数学基础
- en: Looking at what we have learned so far, it becomes abundantly clear that ML
    requires an ample understanding of mathematics. We already came across multiple
    mathematical functions we have to handle. Think about the activation function
    of neurons and the optimizer and loss functions for training. On top of that,
    we have not talked about the second aspect of our new programming paradigm—the
    data!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 看看我们到目前为止学到的东西，很明显，机器学习需要充分理解数学。我们已经遇到了多个我们必须处理的数学函数。想想神经元的激活函数和训练的优化器以及损失函数。除此之外，我们还没有谈到我们新编程范式的第二个方面——数据！
- en: 'To choose the right ML algorithm and derive a good metric for a loss function,
    we have to take apart the data points we work with. In addition, we need to bring
    in the data points in relation to the domain we are working with. Therefore, when
    defining the role of a data scientist, you will often find a visual like this
    one:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择正确的机器学习算法并为损失函数推导出良好的指标，我们必须分析我们所处理的数据点。此外，我们还需要将数据点与我们所工作的领域联系起来。因此，在定义数据科学家的角色时，你经常会看到这样的视觉元素：
- en: '![Figure 1.5 – Requirements for data scientists ](img/B17928_01_05.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – 数据科学家所需条件](img/B17928_01_05.jpg)'
- en: Figure 1.5 – Requirements for data scientists
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 数据科学家所需条件
- en: In this section, we will concentrate on what is referred to in *Figure 1.5*
    as *statistical research*. We will understand why we need statistics and what
    base information we can derive from a given dataset, learn what bias is and ways
    to avoid that, mathematically classify possible ML algorithms, and finally, discuss
    how we choose useful metrics to define the performance of our trained models.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将专注于*图1.5*中提到的*统计研究*。我们将了解为什么我们需要统计学，以及我们可以从给定数据集中推导出哪些基本信息，学习什么是偏差以及如何避免它，从数学上对可能的机器学习算法进行分类，最后讨论我们如何选择有用的指标来定义我们训练模型的性能。
- en: The case for statistics in ML
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习中的统计学案例
- en: 'As we have seen, we require statistics to clean and analyze our given data.
    Therefore, let''s start by asking: *What do we understand from the term "statistics"?*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们需要统计学来清理和分析我们的给定数据。因此，让我们先问一下：*我们从“统计学”这个术语中理解了什么？*
- en: '*Statistics is the science of collecting and analyzing a representative sample
    made up of a large quantity of numerical data with the purpose of inferring the
    statistical distribution of the underlying population.*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*统计学是收集和分析大量数值数据构成的代表性样本的科学，目的是推断底层人群的统计分布。*'
- en: A typical example of something such as this would be the prediction for the
    results of an election you see during the campaign or shortly after voting booths
    close. At those points in time, we do not know the precise result of the full
    **population** but we can acquire a **sample**, sometimes referred to as an **observation**.
    We get that by asking people for responses through a questionnaire. Then, based
    on this subset, we make a sound prediction for the full population by applying
    statistical methods.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的一个典型例子就是在竞选期间或投票站关闭后不久预测选举结果。在那个时间点，我们不知道整个**人口**的精确结果，但我们可以获取一个**样本**，有时也称为**观察值**。我们通过让人们填写问卷来获取这些信息。然后，基于这个子集，我们通过应用统计方法对整个群体做出合理的预测。
- en: 'We learned that in ML, we are trying to let the machine figure out a mathematical
    function that fits our problem, such as this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在机器学习中，我们试图让机器找出适合我们问题的数学函数，例如这个：
- en: '![](img/B17928_Formula_1.11.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17928_Formula_1.11.png)'
- en: Thinking back to our ANN, ![](img/B17928_Formula_1.12.png) would be an input
    vector and ![](img/B17928_Formula_1.13.png) would be the resulting output vector.
    In ML jargon, they are known under a different name, as seen next.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们的人工神经网络（ANN），![](img/B17928_Formula_1.12.png) 将是一个输入向量，而 ![](img/B17928_Formula_1.13.png)
    将是产生的输出向量。在机器学习的术语中，它们有另一个名称，如下所示。
- en: Features and Labels
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 特征和标签
- en: One element of the input vector *x* is called a feature; the full output vector
    is called the label. Often, we only deal with a **one-dimensional** label.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量 *x* 的一个元素被称为特征；完整的输出向量被称为标签。通常，我们只处理一个**一维**标签。
- en: Now, to bring this together, when training an ML model, we typically only have
    a sample of the given world, and as with any other time you are dealing with only
    a sample or subset of reality, you want to pick highly representative features
    and samples of the underlying population.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了将这一点结合起来，在训练机器学习模型时，我们通常只有给定世界的样本，并且正如你在处理现实世界中的任何其他样本或子集时一样，你希望选择高度代表性的特征和底层人群的样本。
- en: So, what does this mean? Let's think of an example. Imagine you want to train
    a small little robot car to be able to automatically drive through a tunnel. First,
    we need to think about what our features and labels in this scenario are. As features,
    we probably need something that measures the distance from the edges of the car
    to the tunnel in each direction, as we probably do not want to drive into the
    sides of the tunnel. Let's assume we have some infrared sensors attached to the
    front, the sides, and the back of the vehicle. Then, the output of our program
    would probably control the steering and the speed of the vehicle, which would
    be our labels.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这究竟意味着什么？让我们来想一个例子。想象一下，你想要训练一辆小型机器人汽车能够自动通过隧道。首先，我们需要考虑在这个场景中我们的特征和标签是什么。作为特征，我们可能需要一些能够测量汽车每个方向上与隧道边缘距离的东西，因为我们可能不希望汽车撞到隧道的侧面。假设我们在车辆的前方、侧面和后方都安装了一些红外传感器。那么，我们程序的输出可能将控制车辆的转向和速度，这些就是我们的标签。
- en: Given that, as a next step, we should think of a whole bunch of scenarios in
    which the vehicle could find itself. This might be a simple scenario of the vehicle
    sitting straight-facing in the tunnel, or it could be a bad scenario where the
    vehicle is nearly stuck in a corner and the tunnel is going left or right from
    that point on. In all these cases, we read out the values of our infrared sensors
    and then do the more complicated tasks of making an educated guess as to how the
    steering has to be changed and how the motor has to operate. Eventually, we end
    up with a bunch of example situations and corresponding actions to take, which
    would be our training dataset. This can then be used to train an ANN so that the
    small car can learn how to follow a tunnel.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 既然如此，作为下一步，我们应该考虑车辆可能遇到的各种场景。这可能是一个简单的场景，车辆直直地停在隧道中，或者它可能是一个糟糕的场景，车辆几乎卡在角落里，从那个点开始，隧道向左或向右延伸。在所有这些情况下，我们读取红外传感器的值，然后进行更复杂的任务，即做出有根据的猜测，关于转向应该如何改变，以及电机应该如何运行。最终，我们得到一系列示例情况和相应的行动，这些将成为我们的训练数据集。然后，我们可以使用这个数据集来训练一个人工神经网络（ANN），这样小汽车就可以学会如何跟随隧道行驶。
- en: If you ever get the opportunity, try to perform this training. If you pick very
    good examples, you will understand the full power of ML, as you will most likely
    see something exciting, which I can attest to. In my setup, even though we never
    had a sample where we would instruct the vehicle to drive backward, the optimal
    function the machine trained had values where the vehicle learned to do exactly
    that.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有机会，尝试进行这项训练。如果你挑选了非常好的例子，你将理解机器学习的全部力量，因为你很可能会看到一些令人兴奋的东西，我可以证实这一点。在我的设置中，尽管我们从未有过一个样本，我们会指示车辆倒车，但机器训练出的最优函数的值表明车辆学会了这样做。
- en: In an example such as that, we would do everything from scratch and hopefully
    take representative samples by ourselves. In most cases you will encounter, the
    dataset already exists, and you need to figure out whether it is representative
    or whether we need to introduce additional data to achieve an optimal training
    result.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的例子中，我们会从头开始做所有的事情，并希望我们自己能够抽取有代表性的样本。在大多数情况下，你将遇到的，数据集已经存在，你需要弄清楚它是否具有代表性，或者我们需要引入额外的数据来实现最佳的训练结果。
- en: Therefore, let's have a look at some statistical properties you should familiarize
    yourself with.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们看看一些你应该熟悉的统计特性。
- en: Basics of statistics
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计学基础
- en: We now understand that we need to be able to analyze the statistical properties
    of single features, derive their distribution, and analyze their relationship
    with other features and labels in the dataset.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们明白，我们需要能够分析单个特征的统计特性，推导出它们的分布，并分析它们与数据集中其他特征和标签的关系。
- en: Let's start with the properties of single features and their distribution. All
    the following operations require numerical data. This means that if you work with
    categorical data or something such as media files, you need to transform them
    into some form of numerical representation to get such results.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从单个特征的性质及其分布开始。以下所有操作都需要数值数据。这意味着如果你处理的是分类数据或类似媒体文件这样的东西，你需要将它们转换成某种数值表示形式，才能得到这样的结果。
- en: 'The following screenshot shows the main statistical properties you are after,
    their importance, and how you can calculate them:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了您所追求的主要统计特性、它们的重要性以及如何计算它们：
- en: '![Figure 1.6 – List of major statistical properties ](img/B17928_01_06.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图1.6 – 主要统计特性列表](img/B17928_01_06.jpg)'
- en: Figure 1.6 – List of major statistical properties
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: From here onward, we can make the reasonable assumption that the underlying
    stochastic process follows a **normal distribution**. Be aware that this must
    not be the case, and therefore you should make yourself comfortable with other
    distributions (see [https://www.itl.nist.gov/div898/handbook/eda/section3/eda36.htm](https://www.itl.nist.gov/div898/handbook/eda/section3/eda36.htm)).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows a visual representation of a standard normal
    distribution:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Standard normal distribution and its properties ](img/B17928_01_07.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Standard normal distribution and its properties
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the strength of this normal distribution is that, based on the mean ![](img/B17928_Formula_1.14.png)
    and standard deviation ![](img/B17928_Formula_1.15.png), we can make assumptions
    for the probabilities of samples to be in a certain range. As shown in *Figure
    1.7*, there is a probability of around **68.27%** for a value to have a distance
    from the mean of 1![](img/B17928_Formula_1.16.png), **95.45%** for a distance
    of ![](img/B17928_Formula_1.17.png), and **99.73%** for a distance of ![](img/B17928_Formula_1.18.png).
    Based on this, we can ask questions such as this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '*How probable is it to find a value with a distance of 5*![](img/B17928_Formula_1.19.png)
    *from the mean?*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Through questions such as this, we can start assessing whether what we see in
    our data is a statistical anomaly of the distribution, is a value that is simply
    false, or whether our suspected distribution is incorrect. This is done through
    a process called **hypothesis testing**, defined next.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis Testing (Definition)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: This is a method of testing if the so-called null hypothesis ![](img/B17928_Formula_1.20.png)
    is false, typically referring to the current suspected distribution. It means
    that the unlikely observation we encounter is pure chance. This hypothesis is
    rejected in favor of an alternative hypothesis ![](img/B17928_Formula_1.21.png),
    if the probability falls below a predefined significance level (typically higher
    than ![](img/B17928_Formula_1.22.png)/lower than 5%). The alternative hypothesis
    thus presumes that the observation we have is due to a real effect that is not
    taken into account in the initial distribution.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: We will not go into further details on how to perform this test properly, but
    we urge you to familiarize yourself with this process thoroughly.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'What we will talk about is the types of errors you can make in this process,
    as shown in the following screenshot:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Type I and Type II errors ](img/B17928_01_08.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – Type I and Type II errors
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the errors you see in *Figure 1.8* as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '**Type I error**: This denotes that we reject the hypothesis ![](img/B17928_Formula_1.23.png)
    and the underlying distribution, even though it is correct. This is also referred
    to as a **false-positive** result or an **alpha error**.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type II error**: This denotes that we do not reject the hypothesis ![](img/B17928_Formula_1.24.png)
    and the underlying distribution, even though ![](img/B17928_Formula_1.25.png)
    is correct. This error is also referred to as a **false-negative** result or a
    **beta** **error**.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二类错误**：这表示我们没有拒绝假设 ![](img/B17928_Formula_1.24.png) 和其背后的分布，尽管 ![](img/B17928_Formula_1.25.png)
    是正确的。这种错误也被称为**假阴性**结果或**贝塔** **错误**。'
- en: You might have heard the term *false positive* before. Often, it comes up when
    you take a medical test. A false positive would denote that you have a positive
    result from a test, even though you do not have the disease you are testing for.
    As a medical test is also a **stochastic process**, as with nearly everything
    else in our world, the term is correctly used in this scenario.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能之前听说过**假阳性**这个术语。它通常出现在你进行医学检查时。假阳性表示你在测试中得到了阳性结果，尽管你并没有你正在测试的疾病。由于医学检查也是一个**随机过程**，就像我们世界中的几乎所有其他事物一样，这个术语在这个场景中被正确使用。
- en: At the end of this section, when we talk about errors and metrics in ML model
    training, we will come back to these definitions. As a final step, let's discuss
    relationships among features and between features and labels. Such a relationship
    is referred to as a **correlation**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的最后，当我们谈到机器学习模型训练中的错误和度量时，我们将回到这些定义。作为最后一步，让我们讨论特征之间的关系以及特征和标签之间的关系。这种关系被称为**相关性**。
- en: 'There are multiple ways to calculate a correlation between two vectors ![](img/B17928_Formula_1.26.png)
    and ![](img/B17928_Formula_1.27.png), but what they all have in common is that
    their results will fall in the range of [-1,1]. The result of this operation can
    be broadly defined by the following three categories:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以计算两个向量 ![](img/B17928_Formula_1.26.png) 和 ![](img/B17928_Formula_1.27.png)
    之间的相关性，但它们共同的特点是，它们的结果将落在 [-1,1] 的范围内。这个操作的结果可以大致分为以下三个类别：
- en: '**Negatively correlated**: The result leans toward -1\. When the value of vector
    ![](img/B17928_Formula_1.28.png) rises, the values of vector ![](img/B17928_Formula_1.29.png)
    fall and vice versa.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负相关**：结果偏向于 -1。当向量 ![](img/B17928_Formula_1.28.png) 的值上升时，向量 ![](img/B17928_Formula_1.29.png)
    的值下降，反之亦然。'
- en: '**Uncorrelated**: The result leans toward 0\. There is no real interaction
    between vectors ![](img/B17928_Formula_1.30.png) and ![](img/B17928_Formula_1.31.png).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不相关**：结果偏向于 0。向量 ![](img/B17928_Formula_1.30.png) 和 ![](img/B17928_Formula_1.31.png)
    之间没有真正的交互作用。'
- en: '**Positively correlated**: The result leans toward 1\. When the value of vector
    ![](img/B17928_Formula_1.32.png) rises, the values of vector ![](img/B17928_Formula_1.33.png)
    rise and vice versa.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正相关**：结果偏向于 1。当向量 ![](img/B17928_Formula_1.32.png) 的值上升时，向量 ![](img/B17928_Formula_1.33.png)
    的值也上升，反之亦然。'
- en: Through this, we can get an idea of relationships between data points, but please
    be aware of the differences between causation and correlation, as outlined next.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以了解数据点之间的关系，但请注意因果和相关性之间的区别，如下所述。
- en: Causation versus Correlation
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因果关系与相关性
- en: Even if two vectors are correlated with each other, it does not mean one of
    them is the cause of the other one—it simply means that one of them influences
    the other one. It is not causation as we probably don't see the full picture and
    every single influencing factor.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 即使两个向量相互关联，这并不意味着其中一个向量是另一个的原因——它仅仅意味着其中一个影响了另一个。这不是因果关系，因为我们可能看不到完整的图景和每一个影响因素。
- en: The mathematical theory we discussed so far should give you a good basis to
    build upon. In the next section, we will have a quick look at what kinds of errors
    we can make when taking samples, typically referred to as the bias in the data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止讨论的数学理论应该为你提供一个良好的基础来构建。在下一节中，我们将快速查看在采样过程中可能犯的错误类型，通常被称为数据的偏差。
- en: Understanding bias
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解偏差
- en: At any stage of taking samples and when working with data, it is easily possible
    to introduce what is called **bias**. Typically, this influences the sampling
    quality and therefore has a big impact on any ML model we would like to fit to
    the data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何采样阶段和数据处理过程中，很容易引入所谓的**偏差**。通常，这会影响采样质量，因此对我们想要拟合数据的任何机器学习模型都有重大影响。
- en: 'One example would be the *causation versus correlation* we just discussed.
    Seeing causation where none exists can have consequences in terms of the way you
    continue processing the data points. Other prominent biases that influence data
    are shown next:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子就是我们刚才讨论的*因果关系与相关性*。在没有因果关系的地方看到因果关系，可能会对你的数据处理方式产生后果。接下来展示的是影响数据的其他显著偏差：
- en: '**Selection bias**: This bias happens when samples are taken that are not representative
    of the real-life distribution of data. This is the case when randomization is
    not properly done or when only a certain subgroup is selected for a study—for
    example, when a questionnaire about city planning is only given out to people
    in half of the neighborhoods of the city.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择偏差**：这种偏差发生在样本不是真实数据分布的代表性样本时。这是在随机化没有正确进行或只选择某个特定子群体进行研究时的情况——例如，当关于城市规划的调查问卷只发放给城市一半地区的居民时。'
- en: '**Funding bias**: This bias should be very well known and happens when a study
    or data project is funded by a sponsor and the results will therefore have a tendency
    toward the interests of the funding party.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资助偏差**：这种偏差应该非常为人所知，发生在一项研究或数据项目由赞助商资助，因此结果将倾向于资助方的利益。'
- en: '**Reporting bias**: This bias happens when only a selection of outcomes is
    represented in a dataset due to the fact that it is the tendency of people to
    underreport certain outcomes. Examples of this are given here: when you report
    bad weather events but not when there is sunshine; when you write negative reviews
    for a product but not positive reviews; when you only know about results written
    in your own language or from your own region but not from others.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**报告偏差**：这种偏差发生在由于人们倾向于低估某些结果，导致数据集中只代表了一部分结果。这里给出了一些例子：当你报告恶劣天气事件但不报告晴天时；当你为产品写负面评论但不写正面评论时；当你只了解用你自己的语言或来自你自己的地区的结果，但不了解其他地区的结果时。'
- en: '**Observer bias/confirmation bias**: This bias happens when someone favors
    results that confirm or support their own beliefs and values. Typically, this
    results in ignoring contrary information, not following the agreed guideline,
    or using ambiguous studies that support the existing preconceived opinion. The
    dangerous part here is that this can happen unconsciously.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察者偏差/确认偏差**：这种偏差发生在某人倾向于支持或证实他们自己的信念和价值观的结果。通常，这会导致忽视相反的信息，不遵循已同意的指南，或使用支持现有先入为主的观点的模糊研究。这里危险的部分是，这可能是无意识的。'
- en: '**Exclusion bias**: This bias happens when you remove data points during preprocessing
    that you consider irrelevant but are not. This includes removing null values,
    outliers, or other special data points. The removal might result in the loss of
    accuracy concerning the underlying real-life distribution.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排除偏差**：这种偏差发生在预处理过程中，你移除了你认为无关但实际并非无关的数据点。这包括移除空值、异常值或其他特殊数据点。这种移除可能会导致关于潜在现实分布的准确度下降。'
- en: '**Automation bias**: This bias happens when you favor results generated from
    automated systems over information taken from humans, even if they are correct.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化偏差**：这种偏差发生在你更倾向于来自自动化系统的结果，而不是来自人类的信息，即使它们是正确的。'
- en: '**Overgeneralization bias**: This bias happens when you project a property
    of your dataset toward the whole population. An example would be that you would
    assume that all cats have gray fur because in the large dataset you have, this
    is true.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过度泛化偏差**：这种偏差发生在你将数据集的一个属性投射到整个群体上。一个例子是，你会假设所有猫都有灰色毛发，因为在你的大型数据集中，这是真的。'
- en: '**Group attribution bias**: This bias happens when stereotypes are added as
    attributes to a whole group because of the actions of a few individuals within
    that group.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**群体归因偏差**：这种偏差发生在由于该群体中少数个体的行为，而将刻板印象作为属性添加到整个群体中。'
- en: '**Survivorship bias**: This bias happens when you focus on successful examples
    while completely ignoring failures. An example would be that you study the competition
    of your company while ignoring all companies that failed, merged, or went bankrupt.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幸存者偏差**：这种偏差发生在你专注于成功的例子，而完全忽略失败的情况。一个例子是，你在研究你公司的竞争时，却忽略了所有失败、合并或破产的公司。'
- en: This list should give you a good understanding of problems that may arise when
    gathering and processing data. We can only urge you to read further into this
    topic while following these next guidelines.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表应该能让你对在收集和处理数据时可能出现的各种问题有一个很好的理解。我们只能敦促你进一步阅读这个主题，同时遵循以下指南。
- en: Guidance for Handling Bias in Data
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据偏差的指导
- en: When using existing datasets, figure out the circumstances in which they were
    obtained to be able to judge their quality. When processing data either alone
    or in a team, define clear guidelines on how you define data and how you handle
    certain situations, and always reflect whether you are making assumptions based
    on your own predispositions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用现有数据集时，找出它们被获取的情境，以便能够判断它们的质量。在单独或团队处理数据时，明确界定如何定义数据和如何处理特定情况，并始终反思你是否是在根据自己的先入之见做出假设。
- en: To solidify your understanding that things are—most of the time—not as they
    seem, have a look at what is referred to as **Simpson's paradox** and the corresponding
    **University of California** (**UC**) Berkeley case ([http://corysimon.github.io/articles/simpsons-paradox/](http://corysimon.github.io/articles/simpsons-paradox/)).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固你对事物——大多数时候——并不像它们看起来那样的理解，看看被称为**辛普森悖论**及其相应的**加州大学伯克利分校**（**UC**）案例（[http://corysimon.github.io/articles/simpsons-paradox/](http://corysimon.github.io/articles/simpsons-paradox/)）。
- en: Now that we have a good understanding of what to look out for when working with
    data, let's come back to the basics of ML.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地理解了在处理数据时需要注意的事项，让我们回到机器学习的基本概念。
- en: Classifying ML algorithms
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习算法的分类
- en: 'In the first section of this chapter, we got a glimpse into ANNs. These are
    special in the sense that they can be used in a so-called supervised or unsupervised
    training setup. To understand what is meant by this, let''s define the current
    three major types of ML algorithms, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一节中，我们简要了解了人工神经网络（ANNs）。它们在以下方面很特别，即它们可以用于所谓的监督或无监督训练设置。为了理解这里的含义，让我们定义当前三种主要的机器学习算法类型，如下所示：
- en: '**Supervised learning**: In supervised learning, models are trained with a
    so-called labeled dataset. That means besides knowing the input for the required
    algorithm, we also know the required output. This type of learning is split into
    two groups of problems—namely, **classification problems** and **regression problems**.
    Classification works with discrete results, where the output is a class or group,
    while regression works with continuous results, where the output would be a certain
    value. Examples of classification would be identifying fraud in money transactions
    or doing object detection in images. Examples of regression would be forecasting
    prices for houses or the stock market or predicting population growth. It is important
    to understand that this type of learning *requires* labels, which often results
    in the tedious task of labeling the whole dataset.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：在监督学习中，模型是用所谓的标记数据集训练的。这意味着除了知道所需算法的输入外，我们还知道所需的输出。这种学习分为两个问题组——即**分类问题**和**回归问题**。分类使用离散结果，输出是一个类别或组，而回归使用连续结果，输出将是某个特定值。分类的例子包括在货币交易中识别欺诈或对图像进行目标检测。回归的例子包括预测房价或股市或预测人口增长。重要的是要理解这种学习**需要**标签，这通常会导致标记整个数据集的繁琐任务。'
- en: '**Unsupervised learning**: In unsupervised learning, models are trained on
    unlabeled data. This is basically self-organized learning to find patterns in
    data, referred to as **clustering**. Examples of this would be the filtering of
    spam emails in an inbox or the recommendation of movies or clothing a person might
    like to watch or purchase. Often, the learning algorithms are used in a real-time
    scenario where the data needs to be processed directly. The beauty of this type
    of learning is that we do not have to label the dataset.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：在无监督学习中，模型是在未标记的数据上训练的。这基本上是一种自我组织的学习，用于在数据中寻找模式，被称为**聚类**。这种方法的例子包括在收件箱中过滤垃圾邮件或推荐某人可能喜欢观看或购买的电影或服装。通常，学习算法在需要直接处理数据的实时场景中使用。这种类型学习的优点是我们不需要标记数据集。'
- en: '**Reinforcement learning**: In reinforcement learning, algorithms learn by
    reacting to a given environment on their own. The idea of this comes from how
    we as humans learn as we grow up. We did a certain action, and the outcome of
    that action was either good or bad or somewhere in between. We then either receive
    some sort of reward or we don''t. Another similar example would be the way you
    would train a dog to behave. Technically, this is realized through a so-called
    *agent* that is guided by a *policy map*, deciding the probability to take actions
    when in a specific state. For the environment itself, we define a so-called *state-value
    function* that returns the *value* of being in a specific state. Good examples
    of this type of learning are training navigation control for a robot or an AI
    opponent for a game.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram provides an overview of the discussed ML types and the
    corresponding algorithms that are utilized in those areas:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Types of ML algorithms ](img/B17928_01_09.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – Types of ML algorithms
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: A detailed overview of many of the prominent ML algorithms can be found on the
    *scikit-learn* web page ([https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)),
    which is one of the major Python libraries for ML.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an idea of the types of training we can perform, let's have
    a short look at what types of results we get from a training run and how to interpret
    them.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing errors and the quality of results of model training
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in the first section of this chapter, we require a loss function
    that we can minimize to optimize our training results. Typically, this is defined
    through what is referred to in mathematics as a metric. We need to differentiate
    at this point between metrics that are used to define a loss function and therefore
    used in an optimizer to train the model, and metrics that can be calculated to
    give additional hints toward the performance of the trained model. We will have
    a look at both kinds in this section.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen when looking at types of ML algorithms, we might work with an
    output represented by continuous data (regression), or we might work with an output
    represented by discrete data (classification).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The most prominent loss functions used in regression are **MSE** and **root
    MSE** (**RMSE**). Imagine you try to determine a fitted line for a bunch of samples
    in linear regression. The distance between the line and the sample point in **two-dimensional**
    (**2D**) space is your error. To calculate the RMSE for all data points, you would
    take the expected values ![](img/B17928_Formula_1.34.png) and the predicted values
    ![](img/B17928_Formula_1.35.png) and calculate the following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17928_Formula_1.36.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: For classifications, this gets a little bit trickier. In most cases, the model
    can predict the correct class or cannot, making it a binary result. Further, we
    might have a binary classification problem (1 or 0—yes or no), or a multi-class
    problem (cat, dog, horse, and so on).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'For both classification problems, there is a prominent loss function used called
    **cross-entropy loss**. To solve the problem of having a binary result, this loss
    function requires a model that outputs a probability ![](img/B17928_Formula_1.37.png)
    between 0 and 1 for a given data point ![](img/B17928_Formula_1.38.png) and a
    suggested prediction ![](img/B17928_Formula_1.39.png). For a binary classification
    model, it is calculated as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17928_Formula_1.40.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: 'For multi-class classification, we sum up this error for all classes ![](img/B17928_Formula_1.41.png),
    as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17928_Formula_1.42.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: If you want to look further into this topic, consider other useful loss functions
    for regression, such as the **absolute error** loss and the **Huber loss** functions
    (used in **support vector machines**, or **SVMs**), useful loss functions for
    binary classification, such as the **hinge loss** function, and useful loss functions
    for multi-class classification, such as the **Kullback-Leibler divergence** (**KL-divergence**)
    function. The last one can also be used in RL as a metric to monitor the policy
    function during training.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Everything we have discussed so far requires something we can put into a mathematical
    formula. Imagine working with text files to build a model for **natural language
    processing** (**NLP**). In such a case, we do not have a useful mathematical representation
    for text besides something such as **Unicode**. We will learn in [*Chapter 7*](B17928_07_ePub.xhtml#_idTextAnchor112),
    *Advanced Feature Extraction with NLP*, how to represent it in a useful, vectorized
    manner. Having vectors, we can use a different kind of metric to calculate how
    similar vectors are, called the **cosine similarity** metric, which we will discuss
    in [*Chapter 6*](B17928_06_ePub.xhtml#_idTextAnchor102), *Feature Engineering
    and Labeling*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed how to calculate loss functions for a couple of scenarios,
    but how can we define the performance of our model overall?
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: For regression models, our loss function was defined over the whole corpus of
    our training set. The error of a single observation or prediction would be ![](img/B17928_Formula_1.43.png).
    Therefore, RMSE is already a cost function and can be used by an optimizer to
    improve the model performance, so we can use it to judge the performance of the
    model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: For classification models, this gets a little bit more interesting. Cross-entropy
    can be used with an optimizer to train the model and can be used to judge the
    model, but besides that, we can define an additional metric to look out for.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Something obvious would be what is referred to as the **accuracy** of a model,
    calculated as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17928_Formula_1.44.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Now, this looks about right. We just say that the quality of our model is the
    percentage of how often we guessed correctly, and the reality is that a lot of
    people agree with this statement. Remember when we defined **false positives**
    and **false negatives**? These now come into play. Let's look at an example.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a test that checks for a contagious virus. *Figure 1.10* shows the
    results for 100 people being tested for this virus, including the correctness
    of the results:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – Test results for a group of 100 people ](img/B17928_01_10.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 – Test results for a group of 100 people
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what would be the accuracy of this test given these results? Let''s define
    it again using the values for true positive (![](img/B17928_Formula_1.45.png)),
    false positive (![](img/B17928_Formula_1.46.png)), false negative (![](img/B17928_Formula_1.47.png)),
    and true negative (![](img/B17928_Formula_1.48.png)) and calculate the results
    for our example, as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17928_Formula_1.49.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: This sounds like a good test. It gives accurate results in 92% of cases, but
    perhaps you see the problem here. Accuracy sees everything equally. Our test misclassifies
    someone having the virus eight times as someone being virus-free, which might
    have dire ramifications. That means it might be useful having performance metrics
    that put more emphasis on false-positive or false-negative outcomes. Therefore,
    let's define two additional metrics to calculate.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one we call **precision**, a value that defines how many positive
    identifications were correct. The formula is shown here:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17928_Formula_1.50.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: In our example, only in two out of three cases are we correct when we declare
    someone to be infected. A model with a precision value of 1 would have no false-positive
    results.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'The second one we call **recall**, a value that defines how many positive results
    we identify correctly. The formula is shown here:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17928_Formula_1.51.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: This means in our example, we correctly identify 20% of all infected patients,
    which is a bad result. A model with a recall value of 1 would have no false-negative
    results.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate our test or classification correctly, we need to evaluate accuracy,
    precision, and recall. Be aware that, as mentioned when we talked about hypothesis
    testing, precision and recall can work against each other. Therefore, you often
    have to decide whether you prefer to be precise when saying "*You have the virus*"
    or whether you prefer to find everyone who has the virus. You might now understand
    why such tests are often designed toward recall.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: With this, we conclude the section on the mathematical basis required to get
    better at building ML models and working with data. Based on what we have learned
    so far, you should take the next point with you.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Never just use methods from ML libraries for data analysis and modeling; understand
    them mathematically.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will guide you through the structure of the end-to-end
    ML process and the structure of this book.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the end-to-end ML process
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have finally arrived at the main topic of this chapter. After reviewing the
    past and understanding the purpose of ML and how it takes its roots in mathematical
    data analysis, let's now get a clear picture of which steps need to be taken to
    create a high-quality ML model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an overview of the (sometimes recursive) steps
    from data to model to deployed model:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – End-to-end ML process ](img/B17928_01_11.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – End-to-end ML process
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at this flow, we can define the following distinct steps to take:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Excavating data and sources
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preparing and cleaning data
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining labels and engineering features
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training models
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying models
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These show the steps for running one single ML project. When you deal with a
    lot of projects and data, it becomes increasingly important to adopt some form
    of automation and operationalization, which is typically referred to as MLOps.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will give an overview of each of these steps, including
    MLOps and its importance, and explain in which chapters we will delve deeper into
    the corresponding topic. Before we start going through those steps, reflect on
    the following question:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '*As a percentage, how much time would you put aside for each of those steps?*'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'After you are done, have a look at the following screenshot, which shows you
    the typical time investment required for those tasks:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – ML time invested ](img/B17928_01_12.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – ML time invested
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Was your guess reasonably close to this? You might be surprised that only 20%
    of the time, you will work on something that has to do with the actual training
    and deployment of ML models. Therefore, you should take the next point to heart.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: In an ML project, you should spend most of your time taking apart your datasets
    and finding other useful data sources.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Failure to do so will have ramifications on the quality of your model and its
    performance. Now, having said that, let's go through the steps one by one, starting
    with where to source your data from.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Excavating data and sources
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you start an ML project, you probably have some outcome in mind, and often,
    you have some form of existing dataset you or your company wants to start with.
    This is where you start familiarizing yourself with the given data, understanding
    what you have and what is missing by doing analysis, which we will come back to
    in the following steps.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'At some point, you might realize that you are missing additional—but crucial—data
    points to increase the quality of your results. This highly depends on what you
    are missing—whether it is something you or your company can obtain or whether
    you need to find it somewhere else. To give you some ideas, let''s have a look
    at the following options to acquire additional data and what you should be aware
    of:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '**In-house data sources**: If you are running this project in or with a company,
    the first point to look is internally. Advantages of this are that it is free
    of charge, it is often standardized, and you should be able to find a person that
    knows this data and how it was obtained. Depending on the project, it might also
    be the only place you can acquire the required data. Disadvantages of this option
    are that you might not find what you are looking for, that the data is poorly
    documented, and that the quality might be in question due to bias in the data.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open data sources**: Another option is to use freely available datasets.
    Advantages of those are that they are typically gigantic in size (**terabytes**
    (**TB**) of data), they cover different time periods, and they are typically well
    structured and documented. Disadvantages are that some data fields might be hard
    to understand (and the creator is not available), the quality might also vary
    due to bias in the data, and often when used, they require you to publish your
    results. Examples of this would be the **National Oceanic and Atmospheric Administration**
    (**NOAA**) ([https://www.ncei.noaa.gov/weather-climate-links](https://www.ncei.noaa.gov/weather-climate-links))
    and the **European Union** (**EU**) Open Data Portal ([https://data.europa.eu/en](https://data.europa.eu/en)),
    among many others.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data seller (data as a service, or DaaS)**: A final option would be to buy
    data from a data seller, either by purchasing an existing dataset or by requesting
    the creation of one. Advantages of this option are that it saves you time, it
    can give you access to an individualized dataset, and you might even get access
    to preprocessed data. Disadvantages are that this is expensive, you still need
    to do all the other following steps to make this data useful, and there might
    be questions concerning privacy and ethics.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have a good idea of where to get data initially or additionally,
    let''s look at the next step: preparing and cleaning the data.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Preparing and cleaning data
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As alluded to before, descriptive data exploration is without a doubt one of
    the most important steps in an ML project. If you want to clean data and build
    derived features or select an ML algorithm to predict a target variable in your
    dataset, then you need to understand your data first. Your data will define many
    of the necessary cleaning and preprocessing steps. It will define which algorithms
    you can choose, and it will ultimately define the performance of your predictive
    model.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The exploration should be done as a structured analytical process rather than
    a set of experimental tasks. Therefore, we will go through a checklist of data
    exploration tasks that you can perform as an initial step in every ML project,
    before starting any data cleaning, preprocessing, **feature engineering**, or
    model selection. By applying these steps, you will be able to understand the data
    and gain knowledge about the required preprocessing tasks.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Along with that, it will give you a good estimate of what kinds of difficulties
    you can expect in your prediction task, which is essential for judging the required
    algorithms and validation strategies. You will also gain an insight into which
    possible feature engineering methods could apply to your dataset and have a better
    understanding of how to select a good loss function.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at the required steps.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Storing and preparing data
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Your data might come in a variety of different formats. You might work with
    tabular data stored in a **comma-separated values** (**CSV**) file; you might
    have images stored as **Joint Photographic Experts Group** (**JPEG**) or **Portable
    Network Graphics** (**PNG**) files, text stored in a **JavaScript Object Notation**
    (**JSON**) file, or audio files in **MP3** or **M4V** format. CSV can be a good
    format as it is human-readable and can be parsed efficiently. You can open and
    browse it using any text editor.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: If you work on your own, you might just store this raw data in a folder on your
    system, but when you are working with a cloud infrastructure or even just a company
    infrastructure in general, you might need some form of cloud storage. Certainly,
    you can just upload your raw data by hand to such storage, but often, the data
    you work with is coming from a live system and needs to be extracted from there.
    This means it might be worthwhile having a look at so-called **extract-transform-load**
    (**ETL**) tools that can automate this process and bring the required raw data
    into cloud storage.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: After all of the preprocessing steps are done, you will have some form of layered
    data in your storage, from raw to cleaned to labeled to processed datasets.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: We will dive deeper into this topic in [*Chapter 4*](B17928_04_ePub.xhtml#_idTextAnchor071),
    *Ingesting Data and Managing Datasets*. For now, just understand that we will
    automate this process of making data available for processing.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning data
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this step, we have a look at inconsistency and structural errors in the data
    itself. This step is often required for tabular data and sometimes text files,
    but not so much for image or audio files. For the latter, we might be able to
    crop images and change their brightness or contrast, but it might be required
    to go back to the source to create better-quality samples. The same goes for audio
    files.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'For tabular datasets, we have much more options for processing. Let''s go through
    what to look out for, as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '**Duplicates**: Through mistakes in copying data or due to a combination of
    different data sources, you might find duplicate samples. Typically, copies can
    be deleted. Just make sure that these are not two different samples that look
    the same.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Irrelevant information**: In most cases, you will have datasets with a lot
    of different features, some of which will be completely unnecessary for your project.
    The obvious ones you should just remove in the beginning; others you will be able
    to remove later after analyzing the data further.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`US` and `United States`) or simply typos. These should be standardized or
    cleaned up. A good way to do this is by visualizing all available values of a
    feature.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomalies (outliers)**: This refers to very unlikely values for which you
    need to decide whether they are errors or actually true. This is typically done
    after analyzing the data when you know the distribution of a feature.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NA` or `NaN`. There are different ways to rectify this besides deleting entire
    samples. It is also prudent to wait until you have more insight from analyzing
    the data, as you might see better ways to replace them.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After this step, we can start analyzing the cleaned version of our dataset further.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing data
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this step, we apply our understanding of statistics to get some insights
    into our features and labels. This includes calculating statistical properties
    for each feature, visualizing them, finding correlated features, and measuring
    something that is called **feature importance**, which calculates the impact of
    a feature on the label, also referred to as the **target variable**.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Through these methods, we get ideas about relationships among features and between
    features and targets, which can help us to make a decision. In this decision-making
    process, we also start adding something vitally important—our **domain knowledge**.
    If you do not know what the data represents, you will have a hard time pruning
    it and choosing optimal features and samples for training.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot more techniques that can be applied in this step, including
    something called **dimensional reduction**. If you have thousands of features
    (a numerical representation of an image, for example), it gets very complicated
    for humans and even for ML processes to understand relationships. In such cases,
    it might be useful to map this high-dimensional sample to a two-dimensional or
    three-dimensional representation in the form of a vector. Through this, we can
    easily find similarities in different samples.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: We will dive deeper into the topics of cleaning and analyzing data in [*Chapter
    5*](B17928_05_ePub.xhtml#_idTextAnchor085), *Performing Data Analysis and Visualization*.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Having done all these steps, we will have a good understanding of the data we
    have at hand, and we might already know what we are missing. As the final step
    in preprocessing our data, we will have a look at creating and transforming features,
    typically referred to as **feature engineering**, and creating labels when missing.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Defining labels and engineering features
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the second part of the preprocessing of data, we will discuss the labeling
    of data and the actions we can perform on features. To perform these steps, we
    need the knowledge obtained through the exploratory steps we've discussed so far.
    Let's start by looking at labeling data.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Labeling
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s start with a bummer: this process is very tedious. Labeling, also called
    **annotation**, is the least exciting part of an ML project yet one of the most
    important tasks in the whole process. The goal is to feed high-quality training
    data into the ML algorithms.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: While proper labels greatly help to improve prediction performance, the labeling
    process will also help you to study the dataset in greater detail. Let me clarify
    that labeling data requires deep insight and understanding of the context of the
    dataset and the prediction process, which you should have acquired at this point.
    If we were, for example, aiming to predict breast cancer using **computerized
    tomography** (**CT**) scans, we would also need to understand how breast cancer
    can be detected in CT images to label the data.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Mislabeling the training data has a couple of consequences, such as **label
    noise**, which you want to avoid as it will affect the performance of every downstream
    process in the ML pipeline. In some cases, your labeling methodology is dependent
    on the chosen ML approach for a prediction problem. A good example is the difference
    between object detection and segmentation, both of which require completely differently
    labeled data.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: There are some techniques and tooling available to speed up the labeling process
    that make use of the fact that we can use ML algorithms not only for the desired
    project but also to learn how to label our data. Such models start proposing labels
    during your manual annotation of the dataset.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a nutshell, in this step, we will start transforming the features or adding
    new features. Obviously, we are not doing such actions on a whim, but rather due
    to the knowledge we gathered in the previous steps. We might have understood,
    for example, that the full date and time are far too precise, and we need just
    the day of the week or the month. Whatever it might be, we will try to shape and
    extract what we need.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, we will perform one of the following actions:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature creation**: Create new features from a given set of features or from
    additional information sources.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature transformation**: Transform single features to make them useful and
    stable for the utilized ML algorithm.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction**: Create derived features from the original data.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection**: Choose the most prominent and predictive features.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will dive deeper into labeling and the multitude of methods to apply to our
    features in [*Chapter 6*](B17928_06_ePub.xhtml#_idTextAnchor102), *Feature Engineering
    and Labeling*. In addition, we will have a detailed look at a more complex example
    of feature engineering when working with text data in an NLP project. You will
    find this in [*Chapter 7*](B17928_07_ePub.xhtml#_idTextAnchor112), *Advanced Feature
    Extraction with NLP*.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: We conclude this step by reiterating how important the whole preprocessing data
    steps are and how much influence they have on the next step, where we will discuss
    model training. Further, we remember that we might need to come back to this after
    model training in case of lackluster performance of our model.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Training models
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We finally reached the point where we can bring ML algorithms into play. As
    with data experimentation and preprocessing, training an ML model is an analytical,
    step-by-step process. Each step involves a thought process that evaluates the
    pros and cons of each algorithm according to the results of the experimentation
    phase. As in every other scientific process, it is recommended that you come up
    with a hypothesis first and verify whether this hypothesis is true afterward.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the steps that define the process of training an ML model, as
    follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '**Define your ML task**: First, we need to define the ML task we are facing,
    which most of the time is defined by the business decision behind your use case.
    Depending on the amount of labeled data, you can choose between unsupervised and
    supervised learning methods, as well as many other subcategories.'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pick a suitable model**: Pick a suitable model for the chosen ML task. This
    might be a logistical regression, a gradient-boosted ensemble tree, or a DNN,
    just to name a few popular ML model choices. The choice is mainly dependent on
    the training (or production) infrastructure (such as Python, R, Julia, C, and
    so on) and the shape and type of the data.'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pick or implement a loss function and an optimizer**: During the data experimentation
    phase, you should have already come up with a strategy on how to test your model
    performance. Hence, you should have picked a data split, loss function, and optimizer
    already. If you have not done so, you should at this point evaluate what you want
    to measure and optimize.'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pick a dataset split**: Splitting your data into different sets—namely, training,
    validation, and test sets—gives you additional insights into the performance of
    your training and optimization process and helps you to avoid overfitting your
    model to your training data.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train a simple model using cross-validation**: When all the preceding choices
    are made, you can go ahead and train your ML model. Optimally, this is done as
    cross-validation on a training and validation set, without leaking training data
    into validation. After training a baseline model, it''s time to interpret the
    error metric of the validation runs. Does it make sense? Is it as high or low
    as expected? Is it (hopefully) better than random and better than always predicting
    the most popular target?'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tune the model**: Finally, you can either tune the outcome of the model by
    working with the so-called hyperparameters of a model, do model stacking or other
    advanced methods, or you might have to go back to the initial data and work on
    that before training the model again.'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are the base steps we perform when training our model. In the following
    section, we will give some more insights into the aforementioned steps, starting
    with how to choose a model.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a model
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to choosing a good model for your data, it is recommended that
    you favor simple traditional models before going toward the more complex options.
    An example would be ensemble models, such as **gradient-boosted tree ensembles**,
    when training data is limited. These models perform well on a broad set of input
    values (ordinal, nominal, and numeric) as well as training efficiently, and they
    are understandable.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based ensemble models combine many weak learners into a single predictor
    based on decision trees. This greatly reduces the problem of the overfitting and
    instability aspects of a single decision tree. The output, after a few iterations
    using the default parameter, usually delivers great baseline results for many
    different applications.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 9*](B17928_09_ePub.xhtml#_idTextAnchor152), *Building ML Models
    Using Azure Machine Learning*, we dedicate a complete section to training a gradient-boosted
    tree ensemble classifier using **LightGBM**, a popular tree ensemble library from
    Microsoft.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: To capture the meaning of large amounts of complex training data, we need large
    parametric models. However, training parametric models with many hundreds of millions
    of parameters is no easy task, due to exploding and vanishing gradients, loss
    propagation through such a complex model, numerical instability, and normalization.
    In recent years, a branch of such high-parametric models achieved extremely good
    results through many complex tasks—namely, **deep learning** (**DL**).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: DL basically spans up a multilayer ANN, where each layer is seen as a certain
    step in the data processing pipeline of the model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B17928_10_ePub.xhtml#_idTextAnchor165), *Training Deep Neural
    Networks on Azure*, and [*Chapter 12*](B17928_12_ePub.xhtml#_idTextAnchor189),
    *Distributed Machine Learning on Azure*, we will delve deeper into how to train
    large and complex DL models on single machines and on a distributed GPU cluster.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you might work with a completely different form of data, such as audio
    or text data. In such cases, there are specialized ways to preprocess and score
    this data. One of these fields would be **recommendation engines**, which we will
    discuss thoroughly in [*Chapter 13*](B17928_13_ePub.xhtml#_idTextAnchor202), *Building
    a Recommendation Engine in Azure*.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a loss function and an optimizer
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in the previous section, there are many metrics to choose from,
    depending on the type of training and model you want to use. After looking at
    the relationship between the feature and target dimensions, as well as the separability
    of the data, you should continue to evaluate which loss function and optimizer
    you will use to train your model.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Many ML practitioners don't value the importance of a proper error metric highly
    enough and just use what is easy, such as accuracy and RMSE. This choice is critical.
    Furthermore, it is useful to understand the baseline performance and the model's
    robustness to noise. The first can be achieved by computing the error metric using
    only the target variable with the highest occurrence as a prediction. This will
    be your baseline performance. The second can be done by modifying the random seed
    of your ML model and observing the changes to the error metric. This will show
    you which decimal place you can trust the error metric to.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that it is prudent to evaluate the chosen error metric and any
    additional metric you desire after training runs, and experiment whether others
    might be more beneficial.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: As for the optimizer, it highly depends on the model you chose as to which options
    you have in this regard. Just remember the optimizer is how we get to the target,
    and the target is defined by the loss function.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the dataset
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once you have selected an ML model, a loss function, and an optimizer, you
    need to think about splitting your dataset for training. Optimally, the data should
    be split into three disjointed sets: a training, a validation, and a test dataset.
    We use multiple sets to ensure that the model generalizes well on unseen data
    and that the reported error metric can be trusted. Hence, you can see that dividing
    the data into representative sets is a task that should be performed as an analytical
    process. These sets are defined as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '**Training dataset**: The subset of data used to fit/train the model.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation dataset**: The subset of data used to provide an evaluation during
    training to tune hyperparameters. The algorithm sees this data during training,
    but never learns from it. Therefore, it has an indirect influence on the model.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test dataset**: The subset of data used to run an unbiased evaluation of
    the trained model after training.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If training data leaks into the validation or testing set, you risk overfitting
    the model and skewing the validation and testing results. Overfitting is a problem
    that you must handle besides underfitting the model. Both are defined as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting versus Overfitting
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: An underfitted model performs purely on the data. The reasons for that are often
    that the model is too simplistic to understand the relationship between the features
    and the target variables, or that your initial data is lacking useful features.
    An overfitted model performs perfectly on the training dataset and purely on any
    other data. The reason for that is that it basically memorized the training data
    and is unable to generalize.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: There are different discussions on what the size of these splits should be and
    many different further techniques to choose samples for each category, such as
    stratified splitting (sampling based on class distributions), temporal splitting,
    and group-based splitting. We will take a deeper look at these in [*Chapter 9*](B17928_09_ePub.xhtml#_idTextAnchor152),
    *Building ML* *Models Using Azure Machine Learning*.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Running the model training
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In most cases, you will not build an ANN structure and an optimizer from scratch.
    You will use ready-made ML libraries, such as **scikit-learn**, **TensorFlow**,
    or **PyTorch**. Most of these frameworks and libraries are written in Python,
    which should therefore be the language of choice for your ML projects.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'When writing your code for model training, it is a good idea to logically divide
    the required code into two files, as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '**Authoring script (authoring environment)**: The script that defines the environment
    (libraries, training location, and so on) in which the ML training will take place
    and the one triggering the execution script'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execution script (execution environment)**: The script that only contains
    the actual ML training'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By splitting your code in this way, you avoid updating the actual training script
    when your target environment changes. This will make code versioning and MLOps
    much cleaner.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what types of class methods we might encounter in an ML library,
    let''s have a look at a short code snippet from TensorFlow here:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Looking at this code, we see that we are using a model called `Sequential` that
    is a basic ANN defined by a sequential set of layers with one input and one output.
    We see in the model creation step that there are layers defined and some omitted
    other settings. In addition, in the `compile()` method, we define an optimizer,
    a loss function, and some additional metrics we are interested in. Finally, we
    see a method called `fit()` running on the training dataset and a method called
    `evaluate()` running on the test dataset. Now, what do these methods do exactly?
    Before we get to that, let's first define something.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters versus Parameters of a Model
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: There are two kinds of settings that are adjusted during model training. Settings
    such as the weights and the bias in an ANN are referred to as the parameters.
    They are changed during the training phase. Other settings—such as the activation
    functions and the number of layers in an ANN, the data split, the learning rate,
    or the chosen optimizer—are referred to as hyperparameters. Those are the meta
    settings we adjust before a training run.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'Having this out of the way, let''s define the typical methods you will encounter,
    as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '`Sequential` class), in a special function such as `compile()`, or they are
    part of the training method we discuss next.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fit()` or `train()`, this is the main method that trains the parameter of
    the model based on the training dataset, the loss function, and the optimizer.
    These methods do not return any type of value—they just update the model object
    and its parameters.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`evaluate()`, `transform()`, `score()`, or `predict()`. In most cases, these
    return some form of result, as they are typically running the test dataset against
    the trained model.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the typical structure of methods you will encounter for a model in an
    ML library. Now that we have a good idea of how to set up our coding environment
    and use available ML libraries, let's look at how to tune the model after our
    initial training.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the model
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After we have trained a simple ensemble model that performs reasonably better
    than the baseline model and achieves acceptable performance according to the expected
    performance estimated during data preparation, we can progress with optimization.
    This is a point we really want to emphasize. It's strongly discouraged to begin
    model optimization and stacking when a simple ensemble technique fails to deliver
    useful results. If this is the case, it would be much better to take a step back
    and dive deeper into data analysis and feature engineering.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Common ML optimization techniques—such as hyperparameter optimization, model
    stacking, and even **automated machine learning** (**AutoML**)—help you get the
    last 10% of performance boost out of your model.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization concentrates on changing the initial settings of
    the model training to improve its final performance. Similarly, model stacking
    is a very common technique used to improve prediction performance by putting a
    combination of multiple *different* model types into a single stacked model. Hence,
    the output of each model is fed into a meta-model, which itself is trained through
    cross-validation and hyperparameter tuning. By combining significantly different
    models into a single stacked model, you can always outperform a single model.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'If you decide to use any of those optimization techniques, it is advised to
    perform them in parallel and fully automated on a distributed cluster. After seeing
    too many ML practitioners manually parametrizing, tuning, and stacking models
    together, we want to raise this important message: *optimizing ML models is boring*.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: It should rarely be done manually as it is much faster to perform it automatically
    as an end-to-end optimization process. Most of your time and effort should go
    into experimentation, data preparation, and feature engineering—that is, everything
    that cannot be easily automated and optimized using raw compute power. We will
    delve deeper into the topic of model tuning in [*Chapter 11*](B17928_11_ePub.xhtml#_idTextAnchor178),
    *Hyperparameter Tuning and Automated Machine Learning*.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: This concludes all important topics to know about model training. Next, we will
    have a look at options for the deployment of ML models.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Deploying models
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have trained and optimized an ML model, it is ready for deployment.
    This step is typically referred to as **inferencing** or **scoring** a model.
    Many data science teams, in practice, stop here and move the model to production
    as a Docker image, often embedded in a **REpresentational State Transfer** (**REST**)
    **API** using Flask or similar frameworks. However, as you can imagine, this is
    not always the best solution, depending on your requirements. An ML or data engineer's
    responsibility doesn't stop here.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: The deployment and operation of an ML pipeline can be best seen when testing
    the model on live data in production. A test is done to collect insights and data
    to continuously improve the model. Hence, collecting model performance over time
    is an essential step to guaranteeing and improving the performance of the model.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, we differentiate two main architectures for ML-scoring pipelines,
    as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch scoring using pipelines**: An offline process where you evaluate an
    ML model against a batch of data. The result of this scoring technique is usually
    not time-critical, and the data to be scored is usually larger than the model.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time scoring using a container-based web service endpoint**: This refers
    to a technique where we score single data inputs. This is very common in stream
    processing, where single events are scored in real time. It''s obvious that this
    task is highly time-critical, and the execution is blocked until the resulting
    score is computed.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss these two architectures in more detail in [*Chapter 14*](B17928_14_ePub.xhtml#_idTextAnchor217),
    *Model Deployments, Endpoints, and Operations*. There, we will also investigate
    an efficient way of collecting runtimes, latency, and other operational metrics,
    as well as model performance.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: The model files we create, and the previously mentioned options, are typically
    defined by a standard hardware architecture. As mentioned, we probably create
    a Docker image that is deployed to a **virtual machine** (**VM**) or a web service.
    What if we want to deploy our model to a highly specialized hardware environment,
    such as a GPU or a **field-programmable gate array** (**FPGA**)?
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: To explore this further, we will dive deeper into alternative deployment targets
    and methods in [*Chapter 15*](B17928_15_ePub.xhtml#_idTextAnchor238), *Model Interoperability,
    Hardware Optimization, and Integrations*. There, we will have a look at a framework
    called **Open Neural Network eXchange** (**ONNX**) that allows us to convert our
    model into a standardized model format to be deployed to virtually any environment.
    Additionally, we have a look at FPGAs and why they might be a good deployment
    target for ML, and finally, we will explore other Azure services such as **Azure
    IoT Edge** and **Power BI** for integration.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: This step wraps up the end-to-end process for a single ML model. Next, we will
    see a short overview of how to make such ML projects operational in an enterprise-grade
    environment using MLOps.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Developing and operating enterprise-grade ML solutions
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To operationalize ML projects requires the use of automated pipelines and **development-operations**
    (**DevOps**) methodologies such as **continuous integration** (**CI**) and **continuous
    delivery**/**continuous deployment** (**CD**). These combined are typically referred
    to as MLOps.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'When looking at the steps we performed in an ML project, we can see that there
    are typically two major operations happening—the training of a model and the deployment
    of a model. As these can happen independently of one another, it is worthwhile
    defining two different automated pipelines, as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '**Training pipeline**: This includes loading datasets (possibly even including
    an ETL pipeline), transformation, model training, and registering final models.
    This pipeline could be triggered by changes in the dataset or possible detected
    data drift in a deployed model.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment pipeline**: This includes loading of models from the registry,
    creating and deploying Docker images, creating and deploying operational scripts,
    and the final deployment of the model to the target. This pipeline could be triggered
    by new versions of an ML model.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will have a deep dive into ML pipelining with Azure Machine Learning in [*Chapter
    8*](B17928_08_ePub.xhtml#_idTextAnchor135), *Azure Machine Learning Pipelines*.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'Having these pipelines, we can then turn our eye on **Azure DevOps** besides
    other tooling. With that, we can build a life cycle for our ML projects defined
    by the following parts:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating or retraining a model**: Here, we use training pipelines to create
    or retrain our model while version-controlling the pipelines and the code.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deploying the model and creating scoring files and dependencies**: Here,
    we use a deployment pipeline to deploy a specific model version while version-controlling
    the pipeline and the code.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creating an audit trail**: Through CI/CD pipelines and version control, we
    create an audit trail for all assets ensuring integrity and compliance.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring model in production**: We monitor the performance and possible
    data drift, which might automatically trigger retraining of the model.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss these topics and others in more detail in [*Chapter 16*](B17928_16_ePub.xhtml#_idTextAnchor252),
    *Bringing Models into Production with MLOps*.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion on the end-to-end ML process and this chapter.
    If you hadn't already, you should now have a good understanding of ML and what
    to expect in the rest of the book.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned in which situations we should use ML and where it
    is coming from, we understood basic concepts of statistics and the mathematical
    knowledge we require for ML, and we discovered the steps we need to go through
    to create a performing ML model. In addition, we had a first glimpse at what is
    required to operationalize ML projects. This should give a base idea of what ML
    is about and what we will dive into in this book.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: As this book not only covers ML but also the cloud platform Azure, in the next
    two chapters, we will go deeper into a topic that we have not covered so far—we
    will speak about tooling for ML. Therefore, in the next chapter, we will discover
    what Azure has to offer in the form of tools and services for ML, and in the third
    chapter, we will use the most useful tool to run our first hands-on experimentation
    with ML on Azure.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
