["```py\nfrom sklearn.datasets import make_classification\n\n>>> nb_samples = 500\n>>> X, Y = make_classification(n_samples=nb_samples, n_features=3, n_informative=3, n_redundant=0, n_classes=3, n_clusters_per_class=1)\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n>>> dt = DecisionTreeClassifier()\n>>> print(cross_val_score(dt, X, Y, scoring='accuracy', cv=10).mean())\n0.970\n```", "```py\nfrom sklearn.tree import export_graphviz\n\n>>> dt.fit(X, Y)\n>>> with open('dt.dot', 'w') as df:\n df = export_graphviz(dt, out_file=df, \n feature_names=['A','B','C'], \n class_names=['C1', 'C2', 'C3'])\n```", "```py\n>>> <Graphviz Home>bindot -Tpdf dt.dot -o dt.pdf\n```", "```py\n>>> dt.feature_importances_\narray([ 0.12066952,  0.12532507,  0.0577379 ,  0.14402762,  0.14382398,\n 0.12418921,  0.14638565,  0.13784106])\n\n>>> np.argsort(dt.feature_importances_)\narray([2, 0, 5, 1, 7, 4, 3, 6], dtype=int64)\n```", "```py\n>>> cross_val_score(DecisionTreeClassifier(), X, Y, scoring='accuracy', cv=10).mean()\n0.77308070807080698\n\n>>> cross_val_score(DecisionTreeClassifier(max_features='auto'), X, Y, scoring='accuracy', cv=10).mean()\n0.76410071007100711\n\n>>> cross_val_score(DecisionTreeClassifier(min_samples_split=100), X, Y, scoring='accuracy', cv=10).mean()\n0.72999969996999692\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n\n>>> lr = LogisticRegression()\n>>> cross_val_score(lr, X, Y, scoring='accuracy', cv=10).mean()\n0.9053368347338937\n```", "```py\n>>> nb_samples = 1000\n>>> X, Y = make_classification(n_samples=nb_samples, n_features=8, n_informative=6, n_redundant=2,     n_classes=2, n_clusters_per_class=4)\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n { \n 'criterion': ['gini', 'entropy'],\n 'max_features': ['auto', 'log2', None],\n 'min_samples_split': [ 2, 10, 25, 100, 200 ],\n 'max_depth': [5, 10, 15, None]\n }\n]\n\n>>> gs = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_grid,\n scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())\n\n>>> gs.fit(digits.data, digits.target)\nGridSearchCV(cv=10, error_score='raise',\n estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n max_features=None, max_leaf_nodes=None,\n min_impurity_split=1e-07, min_samples_leaf=1,\n min_samples_split=2, min_weight_fraction_leaf=0.0,\n presort=False, random_state=None, splitter='best'),\n fit_params={}, iid=True, n_jobs=8,\n param_grid=[{'max_features': ['auto', 'log2', None], 'min_samples_split': [2, 10, 25, 100, 200], 'criterion': ['gini', 'entropy'], 'max_depth': [5, 10, 15, None]}],\n pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n scoring='accuracy', verbose=0)\n\n>>> gs.best_estimator_\nDecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n max_features=None, max_leaf_nodes=None,\n min_impurity_split=1e-07, min_samples_leaf=1,\n min_samples_split=2, min_weight_fraction_leaf=0.0,\n presort=False, random_state=None, splitter='best')\n\n>>> gs.best_score_\n0.8380634390651085\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\n>>> nb_classifications = 100\n>>> accuracy = []\n\n>>> for i in range(1, nb_classifications):\n a = cross_val_score(RandomForestClassifier(n_estimators=i), digits.data, digits.target,  scoring='accuracy', cv=10).mean()\n rf_accuracy.append(a)\n```", "```py\nfrom sklearn.ensemble import ExtraTreesClassifier\n>>> nb_classifications = 100 \n>>> for i in range(1, nb_classifications):\n a = cross_val_score(ExtraTreesClassifier(n_estimators=i), digits.data, digits.target,  scoring='accuracy', cv=10).mean()\n et_accuracy.append(a)\n```", "```py\n>>> nb_samples = 1000\n>>> X, Y = make_classification(n_samples=nb_samples, n_features=50, n_informative=30, n_redundant=20, n_classes=2, n_clusters_per_class=5)\n```", "```py\nfrom sklearn.ensemble import AdaBoostClassifier\n\n>>> accuracy = []\n\n>>> nb_classifications = 100\n\n>>> for i in range(1, nb_classifications):\n a = cross_val_score(AdaBoostClassifier(n_estimators=i, learning_rate=0.1), digits.data, digits.target, scoring='accuracy', cv=10).mean()\n>>> ab_accuracy.append(a)\n```", "```py\nfrom sklearn.datasets import load_iris\n\n>>> iris = load_iris()\n\n>>> ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n>>> cross_val_score(ada, iris.data, iris.target, scoring='accuracy', cv=10).mean()\n0.94666666666666666\n```", "```py\nfrom sklearn.datasets import make_classification\n\n>>> nb_samples = 500\n\n>>> X, Y = make_classification(n_samples=nb_samples, n_features=4, n_informative=3, n_redundant=1, n_classes=3)\n```", "```py\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\n\n>>> a = []\n>>> max_estimators = 50\n\n>>> for i in range(1, max_estimators):\n>>> score = cross_val_score(GradientBoostingClassifier(n_estimators=i, learning_rate=10.0/float(i)), X, Y, cv=10, scoring='accuracy').mean()\n>>> a.append(score)\n```", "```py\nfrom sklearn.datasets import make_classification\n\n>>> nb_samples = 500\n\n>>> X, Y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, n_classes=2)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n>>> lr = LogisticRegression()\n>>> svc = SVC(kernel='poly', probability=True)\n>>> dt = DecisionTreeClassifier()\n\n>>> classifiers = [('lr', lr),\n ('dt', dt),\n ('svc', svc)]\n\n>>> vc = VotingClassifier(estimators=classifiers, voting='hard')\n```", "```py\nfrom sklearn.model_selection import cross_val_score\n\n>>> a = []\n\n>>> a.append(cross_val_score(lr, X, Y, scoring='accuracy', cv=10).mean())\n>>> a.append(cross_val_score(dt, X, Y, scoring='accuracy', cv=10).mean())\n>>> a.append(cross_val_score(svc, X, Y, scoring='accuracy', cv=10).mean())\n>>> a.append(cross_val_score(vc, X, Y, scoring='accuracy', cv=10).mean())\n\n>>> print(np.array(a))\n[ 0.90182873  0.84990876  0.87386955  0.89982873] \n```", "```py\n>>> weights = [1.5, 0.5, 0.75]\n\n>>> vc = VotingClassifier(estimators=classifiers, weights=weights, voting='soft')\n```", "```py\n>>> print(np.array(a))\n[ 0.90182873  0.85386795  0.87386955  0.89578952]\n```"]