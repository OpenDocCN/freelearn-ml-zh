<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p>Well, now things are getting fun! Our models can now learn more complex functions, and we are now ready for a wonderful tour around the more contemporary and surprisingly effective models</p>
<p>After piling layers of neurons became the most popular solution to improving models, new ideas for richer nodes appeared, starting with models based on human vision. They started as little more than research themes, and after the image datasets and more processing power became available, they allowed researchers to reach almost human accuracy in classification challenges, and we are now ready to leverage that power in our projects. </p>
<p>The topics we will cover in this chapter are as follows:</p>
<ul>
<li>Origins of convolutional neural networks</li>
<li>Simple implementation of discrete convolution</li>
<li>Other operation types: pooling, dropout</li>
<li>Transfer learning</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Origin of convolutional neural networks</h1>
                </header>
            
            <article>
                
<p><strong>Convolutional neural networks</strong> (<strong>CNNs</strong>) have a remote origin. They developed while <strong>multi-layer perceptrons</strong> were perfected, and the first concrete example is the <strong>neocognitron</strong>.</p>
<p>The neocognitron is a <span>hierarchical, multilayered <strong>Artificial Neural Network</strong> (<strong>ANN</strong>), and </span>was introduced in a 1980 paper by Prof. Fukushima and has the following principal features:</p>
<ul>
<li>Self-organizing</li>
<li>Tolerant to shifts and deformation in the input</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="211" width="315" src="assets/c29e144d-13cd-46c1-a2fd-a92cc2b6aa99.png"/></div>
<p>This original idea appeared again in 1986 in the book version of the original backpropagation paper, and was also employed in 1988 for temporal signals in speech recognition.</p>
<p>The design was improved in 1998, with a paper from Ian LeCun<em>,</em> <em>Gradient-Based Learning Aapplied to Document Recognition</em>, presenting the LeNet-5 network, an architecture used to classify handwritten digits. The model showed increased performance compared to other existing models, especially over several variations of SVM, one of the most performant operations in the year of publication.</p>
<p>Then a generalization of that paper came in 2003, with "<em>Hierarchical Neural Networks for Image Interpretation</em>". But in general, almost all kernels followed the original idea, until now.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with convolution</h1>
                </header>
            
            <article>
                
<p>In order to understand convolution, we will start by studying the origin of the convolution operator, and then we will explain how this concept is applied to the information.</p>
<p>Convolution is basically an operation between two functions, continuous or discrete, and in practice, it has the effect of filtering one of them by another.</p>
<p>It has many uses across diverse fields, especially in digital signal processing, where it is the preferred tool for shaping and filtering audio, and images, and it is even used in probabilistic theory, where it represents the sum of two independent random variables.</p>
<p>And what do these filtering capabilities have to do with machine learning? The answer is that with filters, we will be able to build network nodes that can emphasize or hide certain characteristics of our inputs (by the definition of the filters) so we can build automatic custom detectors for all the features, which can be used to detect a determinate pattern. We will cover more of this in the following sections; now, let's review the formal definition of the operation, and a summary of how it is calculated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous convolution</h1>
                </header>
            
            <article>
                
<p>Convolution as an operation was first created during the 18 century by d'Alembert during the initial developments of differential calculus. The common definition of the operation is as follows:</p>
<div style="padding-left: 120px"><img height="57" width="280" src="assets/cfa53da3-92fd-4150-919e-5949eecac3ba.png"/></div>
<p>If we try to describe the steps needed to apply the operation, and how it combines two functions, we can express the mathematical operation involved in the following detail:</p>
<ul>
<li>Flip the signal: This is the (<em>-τ</em>) part of the variable</li>
<li>Shift it: This is given by the <em>t</em> summing factor for <em>g(τ)</em></li>
<li>Multiply it: The product of <em>f</em> and <em><em>g</em></em></li>
<li>Integrate the resulting curve: This is the less intuitive part, because each instantaneous value is the result of an integral</li>
</ul>
<p>In order to understand all the steps involved, let's <span>visually</span><span> </span><span>represent all the steps involved for the calculation of the convolution between two functions,</span> <em>f</em> <span>and</span> <em>g</em><span>, at a determinate point</span> <em>t</em><sub><em>0</em></sub>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="298" width="281" src="assets/1827d5c2-accf-4d2f-93bc-713e522cd3eb.png"/></div>
<p>This intuitive approximation to the rules of the convolution also applies to the discrete field of functions, and it is the real realm in which we will be working. So, let's start by defining it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discrete convolution</h1>
                </header>
            
            <article>
                
<p>Even when useful, the realm in which we work is eminently digital, so we need to translate this operation to the discrete domain. The convolution operation for two discrete functions <em>f</em> and <em>g</em> is the translation of the original integral to an equivalent summation in the following form:</p>
<div style="padding-left: 90px"><img height="54" width="330" src="assets/c2080076-17fb-4e9f-ab49-c12ada720b31.png"/></div>
<p>This original definition can be applied to functions of an arbitrary number of dimensions. In particular, we will work on 2D images, because this is the realm of a large number of applications, which we will describe further in this chapter.</p>
<p>Now it's time to learn the way in which we normally apply the convolution operator, which is through kernels.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kernels and convolutions</h1>
                </header>
            
            <article>
                
<p>When solving practical problems in the discrete domain, we normally have 2D functions of finite dimensions (which could be an image, for example) that we want to filter through another image. The discipline of filter development studies the effects of different kinds of filters when applied via convolution to a variety of classes. The most common types of function applied are of two to five elements per dimension, and of 0 value on the remaining elements. These little matrices, representing filtering functions, are called <strong>kernels</strong>.</p>
<p>The convolution operation starts with the first element of an <em>n</em>-dimensional matrix <span>(usually a 2D matrix representing an image) </span>with all the elements of a kernel, applying the center element of the matrix to the specific value we are multiplying, and applying the remaining factors following the kernel's dimensions. The final result is, in the case of an image, an equivalent image in which certain elements of it (for example, lines and edges) have been highlighted, and others (for example, in the case of blurring) have been hidden.</p>
<p><span>In the following example, you will see how a particular 3 x 3 kernel is applied to a particular image element. This is repeated in a scan pattern to all elements of the matrix:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="186" width="369" src="assets/8732d877-f163-44ff-8548-701da33e19ad.png"/></div>
<p>Kernels also have a couple of extra elements to consider when applying them, specifically stride and padding, which complete the specification for accommodating special application cases. Let's have a look at stride and padding.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stride and padding</h1>
                </header>
            
            <article>
                
<p>When applying the convolution operation, one of the variations that can be applied to the process is to change the displacement units for the kernels. This parameter, which can be specified per dimension, is called <strong>stride</strong>. In the following image, we show a couple of examples of how stride is applied. In the third case, we see an incompatible stride because the kernel can't be applied to the last step. Depending on the library, this type of warning can be dismissed:</p>
<div class="CDPAlignCenter CDPAlign"><img height="235" width="499" class="alignnone size-full wp-image-753 image-border" src="assets/9998091b-8d00-42e9-a034-11f3ebf7f49d.png"/></div>
<p>The other important fact when applying a kernel is that the bigger the kernel, the more units there are on the border of the image/matrix that won't receive an answer because we need to cover the entire kernel. In order to cope with that, the <strong>padding</strong> parameters will add a border of the specified width to the image to allow the kernel to be able to apply evenly to the edge pixels/elements. Here, you have a graphical depiction of the padding parameter:</p>
<div class="CDPAlignCenter CDPAlign"><img height="233" width="198" class="alignnone size-full wp-image-755 image-border" src="assets/e8261782-72b9-4c56-8e97-c62b33b19db0.png"/></div>
<p>After describing the fundamental concepts of convolution, let's implement convolution in a practical example to see it applied to real image and get an intuitive idea of its effects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the 2D discrete convolution operation in an example</h1>
                </header>
            
            <article>
                
<p>In order to understand the mechanism of the discrete convolution operation, let's do a simple intuitive implementation of this concept and apply it to a sample image with different types of kernel. Let's import the required libraries. As we will implement the algorithms in the clearest possible way, we will just use the minimum necessary ones, such as NumPy:</p>
<div class="sourceCode">
<pre class="sourceCode python"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="im">import</span> imageio
<span class="im">import</span> numpy <span class="im">as</span> np</pre></div>
<p>Using the <kbd>imread</kbd> method of the <kbd>imageio</kbd> package, let's read the image (imported as three equal channels, as it is grayscale). We then slice the first channel, convert it to a floating point, and show it using matplotlib:</p>
<div class="sourceCode">
<pre class="sourceCode python">arr <span class="op">=</span> imageio.imread(<span class="st">"b.bmp"</span>) [:,:,<span class="dv">0</span>].astype(np.<span class="bu">float</span>)
plt.imshow(arr, cmap<span class="op">=</span>plt.get_cmap(<span class="st">'binary_r'</span>))
plt.show()</pre></div>
<div class="figure CDPAlignCenter CDPAlign"><img height="246" width="330" src="assets/e65c77d4-672a-4da5-b364-a6c8dd099a5a.png"/></div>
<p>Now it's time to define the kernel convolution operation. As we did previously, we will simplify the operation on a 3 x 3 kernel in order to better understand the border conditions.  <kbd>apply3x3kernel</kbd> will apply the kernel over all the elements of the image, returning a new equivalent image. Note that we are restricting the kernels to 3 x 3 for simplicity, and so the 1 pixel border of the image won't have a new value because we are not taking padding into consideration:</p>
<div class="sourceCode">
<pre class="sourceCode python"><span class="kw">class</span> ConvolutionalOperation:
    <span class="kw">def</span> apply3x3kernel(<span class="va">self</span>, image, kernel):  <span class="co"># Simple 3x3 kernel operation</span>
        newimage<span class="op">=</span>np.array(image)
        <span class="cf">for</span> m <span class="op">in</span> <span class="bu">range</span>(<span class="dv">1</span>,image.shape[<span class="dv">0</span>]<span class="op">-</span><span class="dv">2</span>):
            <span class="cf">for</span> n <span class="op">in</span> <span class="bu">range</span>(<span class="dv">1</span>,image.shape[<span class="dv">1</span>]<span class="op">-</span><span class="dv">2</span>):
                newelement <span class="op">=</span> <span class="dv">0</span>
                <span class="cf">for</span> i <span class="op">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">3</span>):
                    <span class="cf">for</span> j <span class="op">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">3</span>):
                        newelement <span class="op">=</span> newelement <span class="op">+</span> image[m <span class="op">-</span> <span class="dv">1</span> <span class="op">+</span> i][n <span class="op">-</span> <span class="dv">1</span><span class="op">+</span> <br/>                        j]<span class="op">*</span>kernel[i][j]
                newimage[m][n] <span class="op">=</span> newelement
        <span class="cf">return</span> (newimage)</pre></div>
<p class="mce-root">As we saw in the previous sections, the different kernel configurations highlight different elements and properties of the original image, building filters that in conjunction can specialize in very high-level features <span>after many epochs of training</span><span>, such as eyes, ears, and doors. Here, we will generate a dictionary of kernels with a name as the key, and the coefficients of the kernel arranged in a 3 x 3 array. The <kbd>Blur</kbd> filter is equivalent to calculating the average of the 3 x 3 point neighborhood, <kbd>Identity</kbd> simply returns the pixel value as is, <kbd>Laplacian</kbd> is a classic derivative filter that highlights borders, and then the two <kbd>Sobel</kbd> filters will mark </span><span>horizontal edges</span><span> in the first case, and</span><span> </span><span>vertical ones</span><span> in the second case:</span></p>
<div class="sourceCode">
<pre class="sourceCode python">kernels <span class="op">=</span> {<span class="st">"Blur"</span>:[[<span class="dv">1</span>.<span class="op">/</span><span class="dv">16</span>., <span class="dv">1</span>.<span class="op">/</span><span class="dv">8</span>., <span class="dv">1</span>.<span class="op">/</span><span class="dv">16</span>.], [<span class="dv">1</span>.<span class="op">/</span><span class="dv">8</span>., <span class="dv">1</span>.<span class="op">/</span><span class="dv">4</span>., <span class="dv">1</span>.<span class="op">/</span><span class="dv">8</span>.], [<span class="dv">1</span>.<span class="op">/</span><span class="dv">16</span>., <span class="dv">1</span>.<span class="op">/</span><span class="dv">8</span>., <span class="dv">1</span>.<span class="op">/</span><span class="dv">16</span>.]]
           ,<span class="st">"Identity"</span>:[[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>., <span class="dv">1</span>., <span class="dv">0</span>.], [<span class="dv">0</span>., <span class="dv">0</span>., <span class="dv">0</span>.]]
           ,<span class="st">"Laplacian"</span>:[[<span class="dv">1</span>., <span class="dv">2</span>., <span class="dv">1</span>.], [<span class="dv">0</span>., <span class="dv">0</span>., <span class="dv">0</span>.], [<span class="op">-</span><span class="dv">1</span>., <span class="op">-</span><span class="dv">2</span>., <span class="op">-</span><span class="dv">1</span>.]]
           ,<span class="st">"Left Sobel"</span>:[[<span class="dv">1</span>., <span class="dv">0</span>., <span class="op">-</span><span class="dv">1</span>.], [<span class="dv">2</span>., <span class="dv">0</span>., <span class="op">-</span><span class="dv">2</span>.], [<span class="dv">1</span>., <span class="dv">0</span>., <span class="op">-</span><span class="dv">1</span>.]]
           ,<span class="st">"Upper Sobel"</span>:[[<span class="dv">1</span>., <span class="dv">2</span>., <span class="dv">1</span>.], [<span class="dv">0</span>., <span class="dv">0</span>., <span class="dv">0</span>.], [<span class="op">-</span><span class="dv">1</span>., <span class="op">-</span><span class="dv">2</span>., <span class="op">-</span><span class="dv">1</span>.]]}</pre></div>
<p>Let's generate a <kbd>ConvolutionalOperation</kbd> object and generate a comparative kernel graphical chart to see how they compare:</p>
<div class="sourceCode">
<pre>conv <span class="op">=</span> ConvolutionalOperation()
plt.figure(figsize<span class="op">=</span>(<span class="dv">30</span>,<span class="dv">30</span>))
fig, axs <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">30</span>,<span class="dv">30</span>))
j<span class="op">=</span><span class="dv">1</span>
<span class="cf">for</span> key,value <span class="op">in</span> kernels.items():
    axs <span class="op">=</span> fig.add_subplot(<span class="dv">3</span>,<span class="dv">2</span>,j)
    out <span class="op">=</span> conv.apply3x3kernel(arr, value)
    plt.imshow(out, cmap<span class="op">=</span>plt.get_cmap(<span class="st">'binary_r'</span>))
    j<span class="op">=</span>j<span class="dv">+1</span>
plt.show()<br/><br/>&lt;matplotlib.figure.Figure at 0x7fd6a710a208&gt;</pre></div>
<p class="figure"><span><span>In the final image you can clearly see how our kernel has detected several high-detail features on the image—in the first one, you see the unchanged image because we used the unit kernel, then the Laplacian edge finder, the left border detector, the upper border detector, and then the blur operator:</span></span></p>
<div class="caption CDPAlignCenter CDPAlign"><img height="577" width="596" src="assets/fabe713d-8ade-4606-a804-df420bfbfae5.png"/></div>
<p>Having reviewed the main characteristics of the convolution operation for the continuous and discrete fields, we can conclude by saying that, basically, convolution kernels highlight or hide patterns. Depending on the trained or (in our example) manually set parameters, we can begin to discover many elements in the image, such as orientation and edges in different dimensions. We may also cover some unwanted details or outliers by blurring kernels, for example. Additionally, by piling layers of convolutions, we can even highlight higher-order composite elements, such as eyes or ears.</p>
<p><span>This characteristic of convolutional neural networks is their main advantage over previous data-processing techniques: we can determine with great flexibility the primary components of a certain dataset, and represent further samples as a combination of these basic building blocks.</span></p>
<p>Now it's time to look at another type of layer that is commonly used in combination with the former—the pooling layer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Subsampling operation (pooling)</h1>
                </header>
            
            <article>
                
<p>The subsampling operation consists of applying a kernel (of varying dimensions) and reducing the extension of the input dimensions by dividing the image into <em>mxn</em> blocks and taking one element representing that block, thus reducing the image resolution by some determinate factor. In the case of a 2 x 2 kernel, the image size will be reduced by half. The most well-known operations are maximum (max pool), average (avg pool), and minimum (min pool).<br/>
The following image gives you an idea of how to apply a 2 x 2 <kbd>maxpool</kbd> kernel, applied to a one-channel 16 x 16 matrix. It just maintains the maximum value of the internal zone it covers:</p>
<div class="CDPAlignCenter CDPAlign"><img height="141" width="281" src="assets/854251eb-1ea4-4f3a-aa9b-6e3aa8afaf0a.png"/> </div>
<p>Now that we have seen this simple mechanism, let's ask ourselves, what's the main purpose of it? The main purpose of subsampling layers is related to the convolutional layers: to reduce the quantity and complexity of information while retaining the most important information elements. In other word, they build a <em><strong>compact representation</strong></em> of the underlying information.</p>
<p>Now it's time to write a simple pooling operator. It's much easier and more direct to write than a convolutional operator, and in this case we will only be implementing max pooling, which chooses the brightest pixel in the 4 x 4 vicinity and projects it to the final image:</p>
<div class="sourceCode">
<pre class="sourceCode python"><span class="kw">class</span> PoolingOperation:
    <span class="kw">def</span> apply2x2pooling(<span class="va">self</span>, image, stride):  <span class="co"># Simple 2x2 kernel operation</span>
        newimage<span class="op">=</span>np.zeros((<span class="bu">int</span>(image.shape[<span class="dv">0</span>]<span class="op">/</span><span class="dv">2</span>),<span class="bu">int</span>(image.shape[<span class="dv">1</span>]<span class="op">/</span><span class="dv">2</span>)),np.float32)
        <span class="cf">for</span> m <span class="op">in</span> <span class="bu">range</span>(<span class="dv">1</span>,image.shape[<span class="dv">0</span>]<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>):
            <span class="cf">for</span> n <span class="op">in</span> <span class="bu">range</span>(<span class="dv">1</span>,image.shape[<span class="dv">1</span>]<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>):
                newimage[<span class="bu">int</span>(m<span class="op">/</span><span class="dv">2</span>),<span class="bu">int</span>(n<span class="op">/</span><span class="dv">2</span>)] <span class="op">=</span> np.<span class="bu">max</span>(image[m:m<span class="dv">+2</span>,n:n<span class="dv">+2</span>])
        <span class="cf">return</span> (newimage)</pre></div>
<p>Let's apply the newly created pooling operation, and as you can see, the final image resolution is much more blocky, and the details, in general, are brighter:</p>
<div class="sourceCode">
<pre class="sourceCode python">plt.figure(figsize<span class="op">=</span>(<span class="dv">30</span>,<span class="dv">30</span>))
pool<span class="op">=</span>PoolingOperation()
fig, axs <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">10</span>))
axs <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>)
plt.imshow(arr, cmap<span class="op">=</span>plt.get_cmap(<span class="st">'binary_r'</span>))
out<span class="op">=</span>pool.apply2x2pooling(arr,<span class="dv">1</span>)
axs <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)
plt.imshow(out, cmap<span class="op">=</span>plt.get_cmap(<span class="st">'binary_r'</span>))
plt.show()</pre></div>
<p class="figure"><span>Here you can see the differences, even though they are subtle. The final image is of lower precision, and the chosen pixels, being the maximum of the environment, produce a brighter image:</span></p>
<div class="caption CDPAlignCenter CDPAlign"><img height="355" width="714" src="assets/c37f3486-e074-4d1f-96d4-b1adbd85a921.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving efficiency with the dropout operation</h1>
                </header>
            
            <article>
                
<p>As we have observed in the previous chapters, overfitting is a potential problem for every model. This is also the case for neural networks, where data can do very well on the training set but not on the test set, which renders it useless for generalization. </p>
<p><span>For this reason, in 2012, a team led by Geoffrey Hinton published a paper in which the dropout operation was described. Its operation is simple:</span></p>
<ul>
<li>A random number of nodes is chosen (the ratio of the chosen node from the total is a parameter)</li>
<li><span>The values of the chosen weights are reviews to zero,</span> invalidating their previously connected peers at the subsequent layers</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advantages of the dropout layers</h1>
                </header>
            
            <article>
                
<p><span>The main advantage of this method is that it prevents all neurons in a layer from synchronously optimizing their weights. This adaptation, made in random groups, prevents all the neurons from converging to the same goal, thus decorrelating the weights.</span></p>
<p><span>A second property discovered for the application of dropout is that the activations of the hidden units become sparse, which is also a desirable characteristic.</span></p>
<p class="mce-root">In the following diagram, we have a representation of an original, fully-connected multi-layer neural network, and the associated network with dropout:</p>
<div class="CDPAlignCenter CDPAlign">&gt;<img height="181" width="548" src="assets/000dfeda-4caa-4272-b4a2-28903b1d4044.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep neural networks</h1>
                </header>
            
            <article>
                
<p>Now that we have a rich number of layers, it's time to start a tour of how the neural architectures have evolved over time. Starting in 2012, a rapid succession of new and increasingly powerful combinations of layers began, and it has been unstoppable. This new set of architectures adopted the term <strong>deep learning</strong>, and we can approximately define them as complex neural architectures that involve at least three layers. They also tend to include more advanced layers than the <strong>Single Layer Perceptrons</strong>, like convolutional ones.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep convolutional network architectures through time</h1>
                </header>
            
            <article>
                
<p>Deep learning architectures date from 20 years ago and have evolved, guided for the most part by the challenge of solving the human vision problem. Let's have a look at the main deep learning architectures and their principal building blocks, which we can then reuse for our own purposes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Lenet 5</h1>
                </header>
            
            <article>
                
<p>As we saw in the historical introduction of the convolutional neural networks, convolutional layers were discovered during the 1980s. But the available technology wasn't powerful enough to build complex combinations of layers until the end of the 1990s.</p>
<p>Around 1998, in Bell Labs, during research around the decodification of handwritten digits, Ian LeCun formed a new approach—a mix of convolutional, pooling, and fully connected layers<span>—</span>to solve the problem of recognizing handwritten digits.</p>
<p>At this time, SVM and other much more mathematically defined problems were used more or less successfully, and the fundamental paper on CNNs shows that neural networks could perform comparatively well with the then state-of-the-art methods.</p>
<p>In the following diagram, there is a representation of all the layers of this architecture, which received a grayscale 28 x 28 image as input and returned a 10-element vector, with the probability for each character as the output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/564b343b-c5aa-444e-b00d-38f719b0c32a.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Alexnet</h1>
                </header>
            
            <article>
                
<p>After some more years of hiatus (even though Lecun was applying his networks to other tasks, such as face and object recognition), the exponential growth of both available structured data and raw processing power allowed the teams to grow and tune the models to an extent that could have been considered impossible just a few years before.</p>
<p>One of the elements that fostered innovation in the field was the availability of an image recognition benchmark called <strong>Imagenet</strong>, consisting of millions of images of objects organized into categories.</p>
<p>Starting in 2012, the <strong>Large Scale Visual Recognition Challenge (LSVRC)</strong> ran every year and helped researchers to innovate in network configurations, obtaining better and better results <span>every year.</span></p>
<p><strong>Alexnet</strong>, developed by Alex <span>Krizhevsky,</span> was the first deep convolutional network that won this challenge, and set a precedent for years to come. It consisted of a model similar in structure to Lenet-5, but the convolutional layers of which had a depth of hundreds of units, and the total number of parameters was in the tens of millions.</p>
<p>The following challenges saw the appearance of a powerful contender, the <strong>Visual Geometry Group (VGG)</strong> from Oxford University, with its VGG model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The VGG model</h1>
                </header>
            
            <article>
                
<p>The main characteristic of the VGG network architecture is that it reduced the size of the convolutional filters to a simple 3 x 3 matrix and combined them in sequences, which was different to previous contenders, which had large kernel dimensions (up to 11 x 11). <br/>
Paradoxically, the series of small convolutional weights amounted to a really large number of parameters (in the order of many millions), and so it had to be limited by a number of measures.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7aee123c-c553-4558-a73d-a924b6c642a7.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GoogLenet and the Inception model</h1>
                </header>
            
            <article>
                
<p><strong>GoogLenet</strong> was the neural network architecture that won the LSVRC in 2014, and was the first really successful attempt by one of the big IT companies in the series, which has been won mostly by corporations with giant budgets since 2014.</p>
<p>GoogLenet is basically a deep composition of nine chained Inception modules, with little or no modification. Each one of these Inception modules is represented in the following figure, and it's a mix of tiny convolutional blocks, intermixed with a 3 x 3 max pooling node:</p>
<div class="CDPAlignCenter CDPAlign"><img height="284" width="537" src="assets/5c3db9cd-2c4e-47d0-a033-6fd7357c77be.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">Even with such complexity, GoogLenet managed to reduce the required parameter number (11 millon compared to 60 millon), and increased the accuracy (6.7% error compared to 16.4%) compared to Alexnet, which was released just two years previously. Additionally, the reuse of the Inception module allowed agile experimentation. </p>
<p>But it wasn't the last version of this architecture; soon enough, a second version of the Inception module was created, with the following characteristics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Batch-normalized inception V2 and V3</h1>
                </header>
            
            <article>
                
<p><span>In December 2015, with the paper</span> <em>Rethinking the Inception Architecture for Computer Vision</em><span>, Google Research released a new iteration of the Inception architecture. </span></p>
<p><strong>The internal covariance shift problem</strong></p>
<p>One of the main problems of the original GoogLenet was training instability. As we saw earlier, input normalization consisted basically of centering all the input values on zero and dividing its value by the standard deviation in order to get a good baseline for the gradients of the backpropagations.</p>
<p>What occurs during the training of really large datasets is that after a number of training examples, the different value osculations begin to amplify the mean parameter value, like in a resonance phenomenon. This phenomenon is called <strong>covariance shift</strong>.</p>
<p>To mitigate this, the solution was to apply normalization not only to the original input values, but also to the output values at each layer,<span> </span><span>avoiding the instabilities appearing between layers before they begin to drift from the mean.</span></p>
<p>Apart from batch normalization, there were a number of additions proposed incrementally to V2:</p>
<ul>
<li>Reduce the number of convolutions to maximum of 3 x 3</li>
<li>Increase the general depth of the networks</li>
<li>Use the width increase technique on each layer to improve feature combination</li>
<li>Factorization of convolutions</li>
</ul>
<p>Inception V3 basically implements all the proposed innovations on the same architecture, and adds batch normalization to auxiliary classifiers of the networks.</p>
<p>In the following diagram, we represent the new architecture. Note the reduced size of the convolutional units:</p>
<div class="CDPAlignCenter CDPAlign"><img height="218" width="218" src="assets/ca0f4608-6d08-40a2-ab45-364692d956a2.png"/></div>
<p>At the end of 2015, the last fundamental improvement in this series of architectures came from another company, Microsoft, in the form of <strong>ResNets</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Residual Networks (ResNet)</h1>
                </header>
            
            <article>
                
<p>This new architecture appeared in December 2015 (more or less the same time as Inception V3), and it had a simple but novel idea—not only should the output of each constitutional layer be used but the architecture should also combine the output of the layer with the original input.</p>
<p>In the following diagram, we observe a simplified view of one of the ResNet modules. It clearly shows the sum operation at the end of the convolutional series, and a final ReLU operation:</p>
<div class="CDPAlignCenter CDPAlign"><img height="185" width="198" src="assets/b626c89d-0474-4e33-aab3-01fdf6cc9f19.png"/></div>
<p>The convolutional part of the module includes a feature reduction from 256 to 64 values, a 3 x 3 filter layer maintaining the feature numbers, and a feature augmenting the 1 x 1 layer from 64 x 256 values. Originally, it spanned more than 100 layers, but in recent developments, ResNet is also used in a depth of fewer than 30 layers, but with a wider distribution.</p>
<p>Now that we have seen a general overview of the main developments of recent years, let's go directly to the main types of application that researchers have discovered for CNNs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of problem solved by deep layers of CNNs</h1>
                </header>
            
            <article>
                
<p>CNNs have been employed to solve a wide variety of problems in the past. Here is a review of the main problem types, and a short reference to the architecture:</p>
<ul>
<li>Classification</li>
<li>Detection</li>
<li>Segmentation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification</h1>
                </header>
            
            <article>
                
<p>Classification models, as we have seen previously, take an image or other type of input as a parameter and return one array with as many elements as the number of the possible classes, with a corresponding probability for each one.</p>
<p>The normal architecture for this type of solution is a complex combination of convolutional and pooling layers with a logistic layer at the end, showing the probability any of the pretrained classes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detection</h1>
                </header>
            
            <article>
                
<p>Detection adds a level of complexity because it requires guessing the location of one or more elements pertaining to the image, and then trying to classify each of these elements of information.</p>
<p>For this task, a common strategy for the individual localization problem is to combine a classification and regression problem—one (classification) for the class of the object, and the remaining one (regression) for determining the coordinates of the detected object—and then combining the losses into a common one.</p>
<p class="mce-root">For multiple elements, the first step is to determine a number of regions of interest, searching for places in the image that are statistically showing blobs of information belonging to the same object and then only applying the classification algorithms to the detected regions, looking for positive cases with high probabilities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Segmentation</h1>
                </header>
            
            <article>
                
<p>Segmentation adds an additional layer of complexity to the mix because the model has to locate the elements in an image and mark the exact shapes of all the located objects, as in the following illustration:</p>
<div class="CDPAlignCenter CDPAlign"><img height="402" width="605" class="alignnone size-full wp-image-759 image-border" src="assets/9650e822-2c45-41e6-b22f-cf1082db369a.png"/></div>
<p>One of the most common approaches for this task is to implement sequential downsampling and upsampling operations, recovering a high-resolution image with only a certain number of possible outcomes per pixel that mark the class number for that element.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying a deep neural network with Keras</h1>
                </header>
            
            <article>
                
<p>In this exercise, we will generate an instance of the previously described Inception model, provided by the Keras application library. First of all, we will import all the required libraries, including the Keras model handling, the image preprocessing library, the gradient descent used to optimize the variables, and several Inception utilities. Additionally, we will use OpenCV libraries to adjust the new input images, and the common NumPy and matplotlib libraries:</p>
<div class="sourceCode">
<pre><span class="im">from</span> keras.models <span class="im">import</span> Model
<span class="im">from</span> keras.preprocessing <span class="im">import</span> image
<span class="im">from</span> keras.optimizers <span class="im">import</span> SGD
<span class="im">from</span> keras.applications.inception_v3 <span class="im">import</span> InceptionV3, decode_predictions, preprocess_input

<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> cv2<br/><br/>Using TensorFlow backend.</pre></div>
<p>Keras makes it really simple to load a model. You just have to invoke a new instance of the <kbd>InceptionV3</kbd> class, and then we will assign an optimizer based on <strong>stochastic gradient descent</strong>, and the categorical cross-entropy for the loss, which is very suitable for image classification problems:</p>
<div class="sourceCode">
<pre class="sourceCode python">model<span class="op">=</span>InceptionV3()
model.<span class="bu">compile</span>(optimizer<span class="op">=</span>SGD(), loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>)</pre></div>
<p>Now that the model is loaded into memory, it's time to load and adjust the photo using the <kbd>cv</kbd> library, and then we call the preprocess input of the Keras application, which will normalize the values:</p>
<div class="sourceCode">
<pre class="sourceCode python"><span class="co"># resize into VGG16 trained images' format</span>
im <span class="op">=</span> cv2.resize(cv2.imread(<span class="st">'blue_jay.jpg'</span>), (<span class="dv">299</span>, <span class="dv">299</span>))
im <span class="op">=</span> np.expand_dims(im, axis<span class="op">=</span><span class="dv">0</span>)
im <span class="op">=</span> im <span class="op">/</span><span class="dv">255</span>.
im <span class="op">=</span> im <span class="op">-</span> <span class="fl">0.5</span>
im <span class="op">=</span>  im <span class="op">*</span> <span class="dv">2</span>
plt.figure (figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>))
plt.imshow(im[<span class="dv">0</span>], cmap<span class="op">=</span>plt.get_cmap(<span class="st">'binary_r'</span>))
plt.show()</pre></div>
<p class="figure"><span><span>This is what the image looks like after it's normalized—note how our structural understanding of the image has changed, but from the point of view of the model, this is the best way of allowing the model to converge:</span></span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="408" width="414" src="assets/4adcfca4-f4b1-4e89-98ba-bee9896d17da.png"/></div>
<p>Now we will invoke the <kbd>predict</kbd> method of the model, which will show the results of the last layer of the neural network, an array of probabilities for each of the categories. The <kbd>decode_predictions</kbd> method reads a dictionary with all the category numbers as indexes, and the category name as the value, and so it provides the name of the detected item classification, instead of the number:</p>
<div class="sourceCode">
<pre>out <span class="op">=</span> model.predict(im)
<span class="bu">print</span>(<span class="st">'Predicted:'</span>, decode_predictions(out, top<span class="op">=</span><span class="dv">3</span>)[<span class="dv">0</span>])
<span class="bu">print</span> (np.argmax(out))<br/><br/>Predicted: [('n01530575', 'brambling', 0.18225007), ('n01824575', 'coucal', 0.13728797), ('n01560419', 'bulbul', 0.048493069)]
10</pre></div>
<p>As you can see, with this simple approach we have received a very approximate prediction from a list of similar birds. Additional tuning of the input images and the model itself could lead to more precise answers because the blue jay is a category included in the 1,000 possible classes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring a convolutional model with Quiver</h1>
                </header>
            
            <article>
                
<p>In this practical example, we will load one of the models we have previously studied (in this case, <kbd>Vgg19</kbd>) <span>with the help of the Keras library and Quiver</span><span>. Then we will observe the different stages of the architecture, and how the different layers work, with a certain input.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring a convolutional network with Quiver</h1>
                </header>
            
            <article>
                
<p><strong>Quiver</strong> (<a href="https://github.com/keplr-io/quiver" target="_blank">https://github.com/keplr-io/quiver</a>) is a recent and very convenient tool used to explore models with the help of Keras. It creates a server that can be accessed by a contemporary web browser and allows the visualization of a model's structure and the evaluation of input images from the input layers until the final predictions.</p>
<p>With the following code snippet, we will create an instance of the <kbd>VGG16</kbd> model and then we will allow Quiver to read all the images sitting on the current directory and start a web application that will allow us to interact with our model and its parameters:</p>
<pre>from keras.models import Model<br/>from keras.preprocessing import image<br/>from keras.optimizers import SGD<br/>from keras.applications.vgg16 import VGG16<br/>import keras.applications as apps<br/><br/>model=apps.vgg16.VGG16()<br/><br/>from quiver_engine.server import launch<br/>launch(model,input_folder=".") </pre>
<p>The script will then download the <kbd>VGG16</kbd> model weights (you need a good connection because it weighs in the hundreds of megabytes). Then it loads the model in memory and creates a server listening on port 5000.</p>
<div class="packt_infobox">The model weights that the Keras library downloads have been previously trained thoroughly with Imagenet, so it is ready to get very good accuracy on the 1,000 categories in our dataset.</div>
<p>In the following screenshot, we see the first screen we will see after loading the index page of the web application. On the left, an interactive graphical representation of the network architecture is shown. On the center right, you can select one of the pictures in your current directory, and the application will automatically feed it as input, printing the five most likely outcomes of the input.</p>
<p>The screenshot also shows the first network layer, which basically consists of three matrices representing the red, green, and blue components of the original image:</p>
<div class="CDPAlignCenter CDPAlign"><img height="379" width="772" class="alignnone size-full wp-image-779 image-border" src="assets/7f08fd1e-24e3-484d-b270-fcf57f39f584.png"/></div>
<p>Then, as we advance into the model layers, we have the first convolutional layer. Here we can see that this stage highlights mainly high-level features, like the ones we set up with our 3 x 3 filters, such as different types of border, brightness, and contrast:</p>
<div class="CDPAlignCenter CDPAlign"><img height="361" width="793" src="assets/d6515d78-8381-4126-884f-38a674246a5d.png"/></div>
<p>Let's advance a bit more. We now can see an intermediate layer that isn't focused on global features. Instead, we see that it has trained for intermediate features, such as different sets of textures, angles, or sets of features, such as eyes and noses:</p>
<div class="CDPAlignCenter CDPAlign"><img height="389" width="792" src="assets/c7883909-d0bb-40fa-83f0-55f4d1fb386d.png"/></div>
<p>When arriving at the last convolutional layers, really abstract concepts are appearing. This stage shows how incredibly powerful the models we are now training are, because now we are seeing highlighted elements without any useful (for us) meaning. These new abstract categories will lead, after some fully connected layers, to the final solution, which is a 1,000-element array with a float probability value, the probability value for each category in ImageNet:</p>
<div class="CDPAlignCenter CDPAlign"><img height="360" width="791" src="assets/c0ecddc0-353d-4f3b-b020-8a36010b61e2.png"/></div>
<p>We hope you can explore different examples and the layers' outputs, and try to discover how they highlight the different features for different categories of images.</p>
<p>Now it's time to work on a new type of machine learning, which consists of applying previously trained networks to work on new types of problem. This is called <strong>transfer learning</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing transfer learning</h1>
                </header>
            
            <article>
                
<p>In this example, we will implement one of the previously seen examples, replacing the last stages of a pretrained convolutional neural network and training the last stages for a new set of elements, applying it to classification. It has the following advantages:</p>
<ul>
<li>It builds upon models with proved efficiency for image classification tasks</li>
<li>It reduces the training time because we can reuse coefficients with an accuracy that could take weeks of computing power to reach</li>
</ul>
<p>The dataset classes will be two different flower types from the flower17 dataset. It is a 17-category flower dataset with 80 images for each class. The flowers chosen are some common flowers in the UK. The images have large scale, pose, and light variations, and there are also classes with large variations of images within the class and close similarity to other classes. In this case, we will gather the first two classes (daffodil and coltsfoot), and build a classifier on top of the pretrained VGG16 network.</p>
<p>First, we will do image data augmentation, because the quantity of images may not be enough to abstract all the elements of each species. Let's start by importing all the required libraries, including applications, preprocessing, the checkpoint model, and the associated object, to allow us to save the intermediate steps, and the <kbd>cv2</kbd> and <kbd>NumPy</kbd> libraries for image processing and numerical base operations:</p>
<div class="sourceCode">
<pre><span class="im">from</span> keras <span class="im">import</span> applications
<span class="im">from</span> keras.preprocessing.image <span class="im">import</span> ImageDataGenerator
<span class="im">from</span> keras <span class="im">import</span> optimizers
<span class="im">from</span> keras.models <span class="im">import</span> Sequential, Model 
<span class="im">from</span> keras.layers <span class="im">import</span> Dropout, Flatten, Dense, GlobalAveragePooling2D
<span class="im">from</span> keras <span class="im">import</span> backend <span class="im">as</span> k 
<span class="im">from</span> keras.callbacks <span class="im">import</span> ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping
<span class="im">from</span> keras.models <span class="im">import</span> load_model
<span class="im">from</span> keras.applications.vgg16 <span class="im">import</span> VGG16, decode_predictions,preprocess_input
<span class="im">import</span> cv2
<span class="im">import</span> numpy <span class="im">as</span> np<br/><br/>Using TensorFlow backend.</pre></div>
<p>In this section, we will define all the variables affecting the input, data sources, and training parameters:</p>
<div class="sourceCode">
<pre class="sourceCode python">img_width, img_height <span class="op">=</span> <span class="dv">224</span>, <span class="dv">224</span>
train_data_dir <span class="op">=</span> <span class="st">"train"</span>
validation_data_dir <span class="op">=</span> <span class="st">"validation"</span>
nb_train_samples <span class="op">=</span> <span class="dv">300</span>
nb_validation_samples <span class="op">=</span> <span class="dv">100</span> 
batch_size <span class="op">=</span> <span class="dv">16</span>
epochs <span class="op">=</span> <span class="dv">50</span></pre></div>
<p>Now we will invoke the VGG16 pretrained model, not including the top flattening layers:</p>
<div class="sourceCode">
<pre class="sourceCode python">model <span class="op">=</span> applications.VGG16(weights <span class="op">=</span> <span class="st">"imagenet"</span>, include_top<span class="op">=</span><span class="va">False</span>, input_shape <span class="op">=</span> (img_width, img_height, <span class="dv">3</span>))

<span class="co"># Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.</span>
<span class="cf">for</span> layer <span class="op">in</span> model.layers[:<span class="dv">5</span>]:
    layer.trainable <span class="op">=</span> <span class="va">False</span>

<span class="co">#Adding custom Layers </span>
x <span class="op">=</span> model.output
x <span class="op">=</span> Flatten()(x)
x <span class="op">=</span> Dense(<span class="dv">1024</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)
x <span class="op">=</span> Dropout(<span class="fl">0.5</span>)(x)
x <span class="op">=</span> Dense(<span class="dv">1024</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)
predictions <span class="op">=</span> Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)(x)

<span class="co"># creating the final model </span>
model_final <span class="op">=</span> Model(<span class="bu">input</span> <span class="op">=</span> model.<span class="bu">input</span>, output <span class="op">=</span> predictions)</pre></div>
<p>Now it's time to compile the model and create the image data augmentation object for the training and testing dataset:</p>
<div class="sourceCode">
<pre class="sourceCode python"><span class="co"># compile the model </span>
model_final.<span class="bu">compile</span>(loss <span class="op">=</span> <span class="st">"categorical_crossentropy"</span>, optimizer <span class="op">=</span> optimizers.SGD(lr<span class="op">=</span><span class="fl">0.0001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>), metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])

<span class="co"># Initiate the train and test generators with data Augumentation </span>
train_datagen <span class="op">=</span> ImageDataGenerator(
rescale <span class="op">=</span> <span class="dv">1</span>.<span class="op">/</span><span class="dv">255</span>,
horizontal_flip <span class="op">=</span> <span class="va">True</span>,
fill_mode <span class="op">=</span> <span class="st">"nearest"</span>,
zoom_range <span class="op">=</span> <span class="fl">0.3</span>,
width_shift_range <span class="op">=</span> <span class="fl">0.3</span>,
height_shift_range<span class="op">=</span><span class="fl">0.3</span>,
rotation_range<span class="op">=</span><span class="dv">30</span>)

test_datagen <span class="op">=</span> ImageDataGenerator(
rescale <span class="op">=</span> <span class="dv">1</span>.<span class="op">/</span><span class="dv">255</span>,
horizontal_flip <span class="op">=</span> <span class="va">True</span>,
fill_mode <span class="op">=</span> <span class="st">"nearest"</span>,
zoom_range <span class="op">=</span> <span class="fl">0.3</span>,
width_shift_range <span class="op">=</span> <span class="fl">0.3</span>,
height_shift_range<span class="op">=</span><span class="fl">0.3</span>,
rotation_range<span class="op">=</span><span class="dv">30</span>)</pre></div>
<p>Now we will generate the new augmented data:</p>
<div class="sourceCode">
<pre>train_generator <span class="op">=</span> train_datagen.flow_from_directory(
train_data_dir,
target_size <span class="op">=</span> (img_height, img_width),
batch_size <span class="op">=</span> batch_size, 
class_mode <span class="op">=</span> <span class="st">"categorical"</span>)

validation_generator <span class="op">=</span> test_datagen.flow_from_directory(
validation_data_dir,
target_size <span class="op">=</span> (img_height, img_width),
class_mode <span class="op">=</span> <span class="st">"categorical"</span>)

<span class="co"># Save the model according to the conditions  </span>
checkpoint <span class="op">=</span> ModelCheckpoint(<span class="st">"vgg16_1.h5"</span>, monitor<span class="op">=</span><span class="st">'val_acc'</span>, verbose<span class="op">=</span><span class="dv">1</span>, save_best_only<span class="op">=</span><span class="va">True</span>, save_weights_only<span class="op">=</span><span class="va">False</span>, mode<span class="op">=</span><span class="st">'auto'</span>, period<span class="op">=</span><span class="dv">1</span>)
early <span class="op">=</span> EarlyStopping(monitor<span class="op">=</span><span class="st">'val_acc'</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span><span class="dv">10</span>, verbose<span class="op">=</span><span class="dv">1</span>, mode<span class="op">=</span><span class="st">'auto'</span>)<br/><br/>Found 120 images belonging to 2 classes.
Found 40 images belonging to 2 classes.</pre></div>
<p>It's time to fit the new final layers for the model:</p>
<div class="sourceCode">
<pre>model_final.fit_generator(
train_generator,
samples_per_epoch <span class="op">=</span> nb_train_samples,
nb_epoch <span class="op">=</span> epochs,
validation_data <span class="op">=</span> validation_generator,
nb_val_samples <span class="op">=</span> nb_validation_samples,
callbacks <span class="op">=</span> [checkpoint, early])<br/><br/>Epoch 1/50
288/300 [===========================&gt;..] - ETA: 2s - loss: 0.7809 - acc: 0.5000

/usr/local/lib/python3.5/dist-packages/Keras-1.2.2-py3.5.egg/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.
  warnings.warn('Epoch comprised more than '


Epoch 00000: val_acc improved from -inf to 0.63393, saving model to vgg16_1.h5
304/300 [==============================] - 59s - loss: 0.7802 - acc: 0.4934 - val_loss: 0.6314 - val_acc: 0.6339
Epoch 2/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.6133 - acc: 0.6385Epoch 00001: val_acc improved from 0.63393 to 0.80833, saving model to vgg16_1.h5
312/300 [===============================] - 45s - loss: 0.6114 - acc: 0.6378 - val_loss: 0.5351 - val_acc: 0.8083
Epoch 3/50
288/300 [===========================&gt;..] - ETA: 0s - loss: 0.4862 - acc: 0.7986Epoch 00002: val_acc improved from 0.80833 to 0.85833, saving model to vgg16_1.h5
304/300 [==============================] - 50s - loss: 0.4825 - acc: 0.8059 - val_loss: 0.4359 - val_acc: 0.8583
Epoch 4/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.3524 - acc: 0.8581Epoch 00003: val_acc improved from 0.85833 to 0.86667, saving model to vgg16_1.h5
312/300 [===============================] - 48s - loss: 0.3523 - acc: 0.8590 - val_loss: 0.3194 - val_acc: 0.8667
Epoch 5/50
288/300 [===========================&gt;..] - ETA: 0s - loss: 0.2056 - acc: 0.9549Epoch 00004: val_acc improved from 0.86667 to 0.89167, saving model to vgg16_1.h5
304/300 [==============================] - 45s - loss: 0.2014 - acc: 0.9539 - val_loss: 0.2488 - val_acc: 0.8917
Epoch 6/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.1832 - acc: 0.9561Epoch 00005: val_acc did not improve
312/300 [===============================] - 17s - loss: 0.1821 - acc: 0.9551 - val_loss: 0.2537 - val_acc: 0.8917
Epoch 7/50
288/300 [===========================&gt;..] - ETA: 0s - loss: 0.0853 - acc: 0.9792Epoch 00006: val_acc improved from 0.89167 to 0.94167, saving model to vgg16_1.h5
304/300 [==============================] - 48s - loss: 0.0840 - acc: 0.9803 - val_loss: 0.1537 - val_acc: 0.9417
Epoch 8/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.0776 - acc: 0.9764Epoch 00007: val_acc did not improve
312/300 [===============================] - 17s - loss: 0.0770 - acc: 0.9776 - val_loss: 0.1354 - val_acc: 0.9417
Epoch 9/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.0751 - acc: 0.9865Epoch 00008: val_acc did not improve
312/300 [===============================] - 17s - loss: 0.0719 - acc: 0.9872 - val_loss: 0.1565 - val_acc: 0.9250
Epoch 10/50
288/300 [===========================&gt;..] - ETA: 0s - loss: 0.0465 - acc: 0.9931Epoch 00009: val_acc did not improve
304/300 [==============================] - 16s - loss: 0.0484 - acc: 0.9901 - val_loss: 0.2148 - val_acc: 0.9167
Epoch 11/50
296/300 [============================&gt;.] - ETA: 0s - loss: 0.0602 - acc: 0.9764Epoch 00010: val_acc did not improve
312/300 [===============================] - 17s - loss: 0.0634 - acc: 0.9744 - val_loss: 0.1759 - val_acc: 0.9333
Epoch 12/50
288/300 [===========================&gt;..] - ETA: 0s - loss: 0.0305 - acc: 0.9931</pre></div>
<p>Now let's try this with a daffodil image. Let's test the output of the classifier, which should output an array close to <kbd>[1.,0.]</kbd>, indicating that the probability for the first option is very high:</p>
<div class="sourceCode">
<pre>im <span class="op">=</span> cv2.resize(cv2.imread(<span class="st">'test/gaff2.jpg'</span>), (img_width, img_height))
im <span class="op">=</span> np.expand_dims(im, axis<span class="op">=</span><span class="dv">0</span>).astype(np.float32)
im<span class="op">=</span>preprocess_input(im)


out <span class="op">=</span> model_final.predict(im)

<span class="bu">print</span> (out)
<span class="bu">print</span> (np.argmax(out))<br/><br/>[[  1.00000000e+00   1.35796010e-13]]
0</pre></div>
<p>So, we have a very definitive answer for this kind of flower. You can play with new images and test the model with clipped or distorted images, even with related classes, to test the level of accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Fukushima, Kunihiko, and Sei Miyake, <em>Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition.</em> </span>Competition and cooperation in neural nets<span>. Springer, Berlin, Heidelberg, 1982. 267-285.</span></li>
<li><span>LeCun, Yann, et al. <em>Gradient-based learning applied to document recognition.</em> </span>Proceedings of the IEEE<span><span> 86.11 (1998): 2278-2324.</span></span></li>
<li><span>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton, <em>ImageNet Classification with Deep Convolutional Neural Networks.</em> </span>Advances in neural information processing systems<span>. 2012.</span></li>
<li><span>Hinton, Geoffrey E., et al, <em>Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.</em> </span>arXiv preprint arXiv:1207.0580<span> (2012).</span></li>
<li><span>Simonyan, Karen, and Andrew Zisserman, <em>Very Deep Convolutional Networks for Large-Scale Image Recognition</em>. </span>arXiv preprint arXiv:1409.1556 <span>(2014).</span></li>
<li><span>Srivastava, Nitish, et al. <em>Dropout: A Simple Way to Prevent Neural Networks from Overfitting.</em> </span>Journal of machine learning research<span>15.1 (2014): 1929-1958.</span></li>
<li><span>Szegedy, Christian, et al, <em>Rethinking the Inception Architecture for Computer Vision.</em> </span>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition<span>. 2016.</span></li>
<li><span>He, Kaiming, et al, <em>Deep Residual Learning for Image Recognition.</em> </span>Proceedings of the IEEE conference on computer vision and pattern recognition<span>. 2016.</span></li>
<li><span>Chollet, François, <em>Xception: Deep Learning with Depthwise Separable Convolutions.</em> </span>arXiv preprint arXiv:1610.02357<span> (2016).</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter provides important insights into one of the technologies responsible for the amazing new applications you see in the media every day. Also, with the practical example provided, you will even be able to create new customized solutions.</p>
<p>As our models won't be enough to solve very complex problems, in the following chapter, our scope will expand even more, adding the important dimension of time to the set of elements included in our generalization.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>