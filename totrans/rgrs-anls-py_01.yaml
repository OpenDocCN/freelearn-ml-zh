- en: Chapter 1. Regression – The Workhorse of Data Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to this presentation on the workhorse of data science, linear regression,
    and its related family of linear models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowadays, interconnectivity and data explosion are realities that open a world
    of new opportunities for every business that can read and interpret data in real
    time. Everything is facilitating the production and diffusion of data: the omnipresent
    Internet diffused both at home and at work, an army of electronic devices in the
    pockets of large portions of the population, and the pervasive presence of software
    producing data about every process and event. So much data is generated daily
    that humans cannot deal with it because of its volume, velocity, and variety.
    Thus, machine learning and AI are on the rise.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Coming from a long and glorious past in the field of statistics and econometrics,
    linear regression, and its derived methods, can provide you with a simple, reliable,
    and effective tool to learn from data and act on it. If carefully trained with
    the right data, linear methods can compete well against the most complex and fresh
    AI technologies, offering you unbeatable ease of implementation and scalability
    for increasingly large problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explain:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Why linear models can be helpful as models to be evaluated in a data science
    pipeline or as a shortcut for the immediate development of a scalable minimum
    viable product
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some quick indications for installing Python and setting it up for data science
    tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The necessary modules for implementing linear models in Python
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression analysis and data science
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you are a developer hastily working on a very cool application that
    is going to serve thousands of customers using your company's website everyday.
    Using the available information about customers in your data warehouse, your application
    is expected to promptly provide a pretty smart and not-so-obvious answer. The
    answer unfortunately cannot easily be programmatically predefined, and thus will
    require you to adopt a *learning-from-data* approach, typical of data science
    or predictive analytics.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'In this day and age, such applications are quite frequently found assisting
    numerous successful ventures on the Web, for instance:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In the advertising business, an application delivering targeted advertisements
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In e-commerce, a batch application filtering customers to make more relevant
    commercial offers or an online app recommending products to buy on the basis of
    ephemeral data such as navigation records
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the credit or insurance business, an application selecting whether to proceed
    with online inquiries from users, basing its judgment on their credit rating and
    past relationship with the company
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are numerous other possible examples, given the constantly growing number
    of use cases about machine learning applied to business problems. The core idea
    of all these applications is that you don't need to program how your application
    should behave, but you just set some desired behaviors by providing useful examples.
    The application will learn by itself what to do in any circumstance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: After you are clear about the purpose of your application and decide to use
    the learning-from-data approach, you are confident that you don't have to reinvent
    the wheel. Therefore, you jump into reading tutorials and documentation about
    data science and machine learning solutions applied to problems similar to yours
    (they could be papers, online blogs, or books talking about data science, machine
    learning, statistical learning, and predictive analytics).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: After reading a few pages, you will surely be exposed to the wonders of many
    complex machine learning algorithms you likely have never heard of before.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: However, you start being puzzled. It isn't simply because of the underlying
    complex mathematics; it is mostly because of the large amount of possible solutions
    based on very different techniques. You also often notice the complete lack of
    any discussion about how to deploy such algorithms in a production environment
    and whether they would scale up to real-time server requests.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you are completely unsure where should you start. This is when
    this book will come to your rescue.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Let's start from the beginning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the promise of data science
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a more interconnected world and the growing availability of data, data
    science has become quite a hot topic in recent years.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'In the past, analytical solutions had strong constrains: the availability of
    data. Useful data was generally scarce and always costly to obtain and store.
    Given the current data explosion, now abundant and cheaper information at hand
    makes learning from data a reality, thus opening the doors to a wide range of
    predictive applications that were simply impractical before.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: In addition, being in an interconnected world, most of your customers are now
    reachable (and susceptible of being influenced) through the Internet or through
    mobile devices. This simply means that being smart in developing automated solutions
    based on data and its predictive powers can directly and almost instantaneously
    affect how your business works and performs. Being able to reach your customers
    instantly everywhere, 24 hours a day, 365 days a year, enables your company to
    turn data into profits, if you know the right things to be done. In the 21st century,
    *data is the new oil of the digital economy,* as a memorable and still undisputed
    article on Wired stated not too long ago ([http://www.wired.com/insights/2014/07/data-new-oil-digital-economy/](http://www.wired.com/insights/2014/07/data-new-oil-digital-economy/)).However,
    as with oil, data has to be extracted, refined, and distributed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Being at the intersection of substantive expertise (knowing how to do business
    and make profits), machine learning (learning from data), and hacking skills (integrating
    various systems and data sources), data science promises to find the mix of tools
    to leverage your available data and turn it into profits.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: However, there's another side to the coin.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The challenge
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unfortunately, there are quite a few challenging issues in applying data science
    to a business problem:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Being able to process unstructured data or data that has been modeled for completely
    different purposes
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figuring out how to extract such data from heterogeneous sources and integrate
    it in a timely manner
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning (from data) some effective general rules allowing you to correctly
    predict your problem
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding what has been learned and being able to effectively communicate
    your solution to a non-technical managerial audience
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling to real-time predictions given big data inputs
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two points are mainly problems that require data manipulation skills,
    but from the third point onwards, we really need a data science approach to solve
    the problem.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'The data science approach, based on machine learning, requires careful testing
    of different algorithms, estimating their predictive capabilities with respect
    to the problem, and finally selecting the best one to implement. This is exactly
    what the science in *data science* means: coming up with various different hypotheses
    and experimenting with them to find the one that best fits the problem and allows
    generalization of the results.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there is no white unicorn in data science; there is no single
    hypothesis that can successfully fit all the available problems. In other words,
    we say that there is *no free lunch* (the name of a famous theorem from the optimization
    domain), meaning that there are no algorithms or procedures in data science that
    can always assure you the best results; each algorithm can be less or more successful,
    depending on the problem.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Data comes in all shapes and forms and reflects the complexity of the world
    we live in. The existing algorithms should have certain sophistication in order
    to deal with the complexity of the world, but don't forget that they are just
    models. Models are nothing but simplifications and approximations of the system
    of rules and laws we want to successfully represent and replicate for predictive
    reasons since *you can control only what you can measure*, as Lord Kelvin said.
    An approximation should be evaluated based on its effectiveness, and the efficacy
    of learning algorithms applied to real problems is dictated by so many factors
    (type of problem, data quality, data quantity, and so on) that you really cannot
    tell in advance what will work and what won't. Under such premises, you always
    want to test the simpler solutions first, and follow the principle of *Occam's
    razor* as much as possible, favoring simpler models against more complex ones
    when their performances are comparable.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, even when the situation allows the introduction of more complex and
    more performant models, other factors may still favor the adoption of simpler
    yet less performant solutions. In fact, the best model is not always necessarily
    the most performant one. Depending on the problem and the context of application,
    issues such as ease of implementation in production systems, scalability to growing
    volumes of data, and performance in live settings, may deeply redefine how important
    the role of predictive performance is in the choice of the best solution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: In such situations, it is still advisable to use simpler, well-tuned models
    or easily explainable ones, if they provide an acceptable solution to the problem.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The linear models
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In your initial overview of the problem of what machine learning algorithm to
    use, you may have also stumbled upon linear models, namely linear regression and
    logistic regression. They both have been presented as basic tools, building blocks
    of a more sophisticated knowledge that you should achieve before hoping to obtain
    the best results.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Linear models have been known and studied by scholars and practitioners for
    a long time. Before being promptly adopted into data science, linear models were
    always among the basic statistical models to start with in predictive analytics
    and data mining. They also have been a prominent and relevant tool part of the
    body of knowledge of statistics, economics, and many other quantitative subjects.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: By a simple check (via a query from an online bookstore, from a library, or
    just from Google Books—[https://books.google.com/](https://books.google.com/)),
    you will discover there is quite a vast availability of publications about linear
    regression. There is also quite an abundance of publications about logistic regression,
    and about other different variants of the regression algorithm, the so-called
    generalized linear models, adapted in their formulation to face and solve more
    complex problems.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'As practitioners ourselves, we are well aware of the limits of linear models.
    However, we cannot ignore their strong positive key points: simplicity and efficacy.
    We also cannot ignore that linear models are indeed among the most used learning
    algorithms in applied data science, making them real workhorses in data analysis
    (in business as well as in many scientific domains).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Far from being the best tool at hand, they are always a good starting point
    in a data science path of discovery because they don't require hacking with too
    many parameters and they are very fast to train. Thus, linear models can point
    out the predictive power of your data at hand, identify the most important variables,
    and allow you to quickly test useful transformations of your data before applying
    more complex algorithms.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: In the course of this book, you will learn how to build prototypes based on
    linear regression models, keeping your data treatment and handling pipeline prompt
    for possible development reiterations of the initial linear model into more powerful
    and complex ones, such as neural networks or support vector machines.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, you will learn that you maybe don't even need more complex models,
    sometimes. If you are really working with lots of data, after having certain volumes
    of input data feed into a model, using simple or complex algorithms won't matter
    all that much anymore. They will all perform to the best of their capabilities.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The capability of big data to make even simpler models as effective as a complex
    one has been pointed out by a famous paper co-authored by Alon Halevy, Peter Norvig,
    and Fernando Pereira from Google about *The Unreasonable Effectiveness of Data*
    ([http://static.googleusercontent.com/media/research.google.com/it//pubs/archive/35179.pdf](http://static.googleusercontent.com/media/research.google.com/it//pubs/archive/35179.pdf)).
    Before that, the idea was already been known because of a less popular scientific
    paper by Microsoft researchers, Michele Banko and Eric Brill, *Scaling to Very
    Very Large Corpora for Natural Language Disambiguation* ([http://ucrel.lancs.ac.uk/acl/P/P01/P01-1005.pdf](http://ucrel.lancs.ac.uk/acl/P/P01/P01-1005.pdf)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: In simple and short words, the algorithm with more data wins most of the time
    over other algorithms (no matter their complexity); in such a case, it could well
    be a linear model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: However, linear models can be also helpful downstream in the data science process
    and not just upstream. As they are fast to train, they are also fast to be deployed
    and you do not need coding complex algorithms to do so, allowing you to write
    the solution in any script or programming language you like, from SQL to JavaScript,
    from Python to C/C++.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Given their ease of implementation, it is not even unusual that, after building
    complex solutions using neural networks or ensembles, such solutions are reverse-engineered
    to find a way to make them available in production as a linear model and achieve
    a simpler and scalable implementation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: What you are going to find in the book
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following pages, the book will explain algorithms as well as their implementation
    in Python to solve practical real-world problems.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Linear models can be counted among supervised algorithms, which are those algorithms
    that can formulate predictions on numbers and classes if previously given some
    correct examples to learn from. Thanks to a series of examples, you will immediately
    distinguish if a problem could be tractable using this algorithm or not.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Given the statistical origins of the linear models family, we cannot neglect
    starting from a statistical perspective. After contextualizing the usage of linear
    models, we will provide all the essential elements for understanding on what statistical
    basis and for what purpose the algorithm has been created. We will use Python
    to evaluate the statistical outputs of a linear model, providing information about
    the different statistical tests used.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The data science approach is quite practical (to solve a problem for its business
    impact), and many limitations of the statistical versions of linear models actually
    do not apply. However, knowing how the R-squared coefficient works or being able
    to evaluate the residuals of a regression or highlighting the collinearity of
    its predictors, can provide you with more means to obtain good results from your
    work in regression modeling.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Starting from regression models involving a single predictive variable, we will
    move on to consider multiple variables, and from predicting just numbers we will
    progress to estimating the probability of there being a certain class among two
    or many.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: We will particularly emphasize how to prepare data, both the target variable
    (a number or a class) to be predicted and the predictors; variables contributing
    to a correct prediction. No matter what your data is made of, numbers, nouns,
    text, images, or sounds, we will provide you with the method to correctly prepare
    your data and transform it in such a way that your models will perform the best.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: You will also be introduced to the scientific methodology at the very foundations
    of data science, which will help you understand why the data science approach
    is not just simply theoretical but also quite practical, since it allows obtaining
    models that can really work when applied to real-world problems.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The last pages of the book will cover some of the more advanced techniques for
    handling big data and complexity in models. We will also provide you with a few
    examples from relevant business domains and offer plenty of details about how
    to proceed to build a linear model, validate it, and later on implement it into
    a production environment.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Python for data science
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the availability of many useful packages for creating linear models and
    given the fact that it is a programming language quite popular among developers,
    Python is our language of choice for all the code presented in this book.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Created in 1991 as a general-purpose, interpreted, object-oriented language,
    Python has slowly and steadily conquered the scientific community and grown into
    a mature ecosystem of specialized packages for data processing and analysis. It
    allows you to perform uncountable and fast experiments, easy theory development,
    and prompt deployments of scientific applications.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'As a developer, you will find using Python interesting for various reasons:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: It offers a large, mature system of packages for data analysis and machine learning.
    It guarantees that you will get all that you need in the course of a data analysis,
    and sometimes even more.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is very versatile. No matter what your programming background or style is
    (object-oriented or procedural), you will enjoy programming with Python.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you don't know it yet, but you know other languages well such as C/C++ or
    Java, it is very simple to learn and use. After you grasp the basics, there's
    no better way to learn more than by immediately starting to code.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is cross-platform; your solutions will work perfectly and smoothly on Windows,
    Linux, and Mac OS systems. You won't have to worry about portability.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although interpreted, it is undoubtedly fast compared to other mainstream data
    analysis languages such as R and MATLAB (though it is not comparable to C, Java,
    and the newly emerged Julia language).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are packages that allow you to call other platforms, such as R and Julia,
    outsourcing some of the computations to them and improving your script performance.
    Moreover, there are also static compilers such as Cython or just-in-time compilers
    such as PyPy that can transform Python code into C for higher performance.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can work better than other platforms with in-memory data because of its minimal
    memory footprint and excellent memory management. The memory garbage collector
    will often save the day when you load, transform, dice, slice, save, or discard
    data using the various iterations and reiterations of data wrangling.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Python
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a first step, we are going to create a fully working data science environment
    you can use to replicate and test the examples in the book and prototype your
    own models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: No matter in what language you are going to develop your application, Python
    will provide an easy way to access your data, build your model from it, and extract
    the right parameters you need to make predictions in a production environment.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Python is an open source, object-oriented, cross-platform programming language
    that, compared with its direct competitors (for instance, C/C++ and Java), produces
    very concise and very readable code. It allows you to build a working software
    prototype in a very short time, to maintain it easily, and to scale it to larger
    quantities of data. It has become the most used language in the data scientist's
    toolbox because it is a general-purpose language made very flexible thanks to
    a large variety of available packages that can easily and rapidly help you solve
    a wide spectrum of both common and niche problems.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Choosing between Python 2 and Python 3
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before starting, it is important to know that there are two main branches of
    Python: version 2 and 3\. Since many core functionalities have changed, scripts
    built for one versions are often incompatible (they won''t work without raising
    errors and warnings) with the other one. Although the third version is the newest,
    the older one is still the most used version in the scientific area, and the default
    version for many operating systems (mainly for compatibility in upgrades). When
    version 3 was released in 2008, most scientific packages weren''t ready, so the
    scientific community was stuck with the previous version. Fortunately, since then,
    almost all packages have been updated, leaving just a few orphans of Python 3
    compatibility (see [http://py3readiness.org/](http://py3readiness.org/) for a
    compatibility overview).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, which should address a large audience of developers, we agreed
    that it would have been better to work with Python 3 rather than the older version.
    Python 3 is the future of Python; in fact, it is the only version that will be
    further developed and improved by the Python foundation. It will be the default
    version of the future. If you are currently working with version 2 and you prefer
    to keep on working with it, we suggest you to run these following few lines of
    code at the beginning every time you start the interpreter. By doing so, you''ll
    render Python 2 capable of executing most version 3 code with minimal or no problems
    at all (the code will patch just a few basic incompatibilities, after installing
    the future package using the command `pip install future`, and let you safely
    run all the code in this book):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tip
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `from __future__ import` commands should always occur at the beginning of
    your script or you may experience Python reporting an error.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step installation
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have never used Python (but that doesn't mean that you may not already
    have it installed on your machine), you need to first download the installer from
    the main website of the project, [https://www.python.org/downloads/](https://www.python.org/downloads/)
    (remember, we are using version 3), and then install it on your local machine.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: This section provides you with full control over what can be installed on your
    machine. This is very useful when you are going to use Python as both your prototyping
    and production language. Furthermore, it could help you keep track of the versions
    of packages you are using. Anyway, please be warned that a step-by-step installation
    really takes time and effort. Instead, installing a ready-made scientific distribution
    will lessen the burden of installation procedures and may well facilitate initial
    learning because it can save you quite a lot of time, though it will install a
    large number of packages (that for the most part you may never use) on your computer
    all at once. Therefore, if you want to start immediately and don't need to control
    your installation, just skip this part and proceed to the next section about scientific
    distributions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'As Python is a multiplatform programming language, you''ll find installers
    for computers that either run on Windows or Linux/Unix-like operating systems.
    Please remember that some Linux distributions (such as Ubuntu) already have Python
    packed in the repository, which makes the installation process even easier:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Python shell, type `python` in the terminal or click on the Python **IDLE**
    icon. Then, to test the installation, run the following code in the Python interactive
    shell or REPL:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Tip
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Step-by-step installation](img/00002.jpeg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: If a syntax error is raised, it means that you are running Python 2 instead
    of Python 3\. Otherwise, if you don't experience an error and you read that your
    Python version is 3.x (at the time of writing this book, the latest version was
    3.5.0), then congratulations on running the version of Python we elected for this
    book.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: To clarify, when a command is given in the terminal command line, we prefix
    the command with `$>`. Otherwise, if it's for the Python REPL, it's preceded by
    `>>>`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Installing packages
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on your system and past installations, Python may not come bundled
    with all you need, unless you have installed a distribution (which, on the other
    hand, is usually stuffed with much more than you may need).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'To install any packages you need, you can use the commands `pip` or `easy_install`;
    however, `easy_install` is going to be dropped in the future and `pip` has important
    advantages over it. It is preferable to install everything using `pip` because:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: It is the preferred package manager for Python 3 and, starting with Python 2.7.9
    and Python 3.4, it is included by default with the Python binary installers
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides an uninstall functionality
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It rolls back and leaves your system clear if, for whatever reason, the package
    installation fails
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The command `pip` runs on the command line and makes the process of installing,
    upgrading, and removing Python packages simply a breeze.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned, if you''re running at least Python 2.7.9 or Python 3.4 the
    `pip` command should already be there. To verify which tools have been installed
    on your local machine, directly test with the following command if any error is
    raised:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In some Linux and Mac installations, the command is present as `pip3` (more
    likely if you have both Python 2 and 3 on your machine), so, if you received an
    error when looking for `pip`, also try running the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Alternatively, you can also test if the old command `easy_install` is available:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tip
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using `easy_install` in spite of pip's advantages makes sense if you are working
    on Windows because `pip` will not install binary packages (it will try to build
    them); therefore, if you are experiencing unexpected difficulties installing a
    package, `easy_install` can save your day.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: If your test ends with an error, you really need to install `pip` from scratch
    (and in doing so, also `easy_install` at the same time).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `pip`, simply follow the instructions given at [https://pip.pypa.io/en/stable/installing/](https://pip.pypa.io/en/stable/installing/).
    The safest way is to download the `get-pi.py` script from [https://bootstrap.pypa.io/get-pip.py](https://bootstrap.pypa.io/get-pip.py)
    and then run it using the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: By the way, the script will also install the setup tool from [https://pypi.python.org/pypi/setuptools](https://pypi.python.org/pypi/setuptools),
    which contains `easy_install`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative, if you are running a Debian/Ubuntu Unix-like system, then
    a fast shortcut would be to install everything using `apt-get`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After checking this basic requirement, you''re now ready to install all the
    packages you need to run the examples provided in this book. To install a generic
    package, `<pk>`, you just need to run the following command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Alternatively, if you prefer to use `easy_install`, you can also run the following
    command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After that, the `<pk>`package and all its dependencies will be downloaded and
    installed.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: If you are not sure whether a library has been installed or not, just try to
    import a module inside it. If the Python interpreter raises an `Import Error`
    error, it can be concluded that the package has not been installed.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example. This is what happens when the NumPy library has been
    installed:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is what happens if it is not installed:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the latter case, before importing it, you'll need to install it through `pip`
    or `easy_install`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Take care that you don't confuse packages with modules. With `pip`, you install
    a package; in Python, you import a module. Sometimes, the package and the module
    have the same name, but in many cases they don't match. For example, the `sklearn`
    module is included in the package named `Scikit-learn`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Package upgrades
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'More often than not, you will find yourself in a situation where you have to
    upgrade a package because the new version either is required by a dependency or
    has additional features that you would like to use. To do so, first check the
    version of the library you have installed by glancing at the `__version__` attribute,
    as shown in the following example using the NumPy package:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, if you want to update it to a newer release, say the 1.10.1 version, you
    can run the following command from the command line:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Alternatively, but we do not recommend it unless it proves necessary, you can
    also use the following command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, if you are just interested in upgrading it to the latest available
    version, simply run the following command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can alternatively also run the `easy_install` alternative:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Scientific distributions
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you've read so far, creating a working environment is a time-consuming operation
    for a data scientist. You first need to install Python and then, one by one, you
    can install all the libraries that you will need (sometimes, the installation
    procedures may not go as smoothly as you'd hoped for earlier).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: If you want to save time and effort and want to ensure that you have a working
    Python environment that is ready to use, you can just download, install, and use
    a scientific Python distribution. Apart from Python itself, distributions also
    include a variety of preinstalled packages, and sometimes they even have additional
    tools and an IDE set up for your usage. A few of them are very well known among
    data scientists and, in the sections that follow, you will find some of the key
    features for two of these packages that we found most useful and practical.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: To immediately focus on the contents of the book, we suggest that you first
    download and install a scientific distribution, such as Anaconda (which is the
    most complete one around, in our opinion). Then, after practicing the examples
    in the book, we suggest you to decide to fully uninstall the distribution and
    set up Python alone, which can be accompanied by just the packages you need for
    your projects.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Again, if possible, download and install the version containing Python 3.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: The first package that we would recommend you try is Anaconda ([https://www.continuum.io/downloads](https://www.continuum.io/downloads)),
    which is a Python distribution offered by Continuum Analytics that includes nearly
    200 packages, including NumPy, SciPy, Pandas, IPython, Matplotlib, Scikit-learn,
    and Statsmodels. It's a cross-platform distribution that can be installed on machines
    with other existing Python distributions and versions, and its base version is
    free. Additional add-ons that contain advanced features are charged separately.
    Anaconda introduces conda, a binary package manager, as a command-line tool to
    manage your package installations. As stated on its website, Anaconda's goal is
    to provide enterprise-ready Python distribution for large-scale processing, predictive
    analytics, and scientific computing.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: As a second suggestion, if you are working on Windows, WinPython ([http://winpython.sourceforge.net](http://winpython.sourceforge.net))
    could be a quite interesting alternative (sorry, no Linux or MacOS versions).
    WinPython is also a free, open source Python distribution maintained by the community.
    It is designed with scientists in mind, and it includes many essential packages
    such as NumPy, SciPy, Matplotlib, and IPython (the same as Anaconda's). It also
    includes Spyder as an IDE, which can be helpful if you have experience using the
    MATLAB language and interface. A crucial advantage is that it is portable (you
    can put it into any directory, or even on a USB flash drive, without the need
    for any administrative elevation). Using WinPython, you can have different versions
    present on your computer, move a version from a Windows computer to another, and
    you can easily replace an older version with a newer one just by replacing its
    directory. When you run WinPython or its shell, it will automatically set all
    the environment variables necessary for running Python as it were regularly installed
    and registered on your system.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Finally, another good choice for a distribution that works on Windows could
    be Python(x,y). Python(x,y) ([http://python-xy.github.io](http://python-xy.github.io))
    is a free, open source Python distribution maintained by the scientific community.
    It includes a number of packages, such as NumPy, SciPy, NetworkX, IPython, and
    Scikit-learn. It also features Spyder, the interactive development environment
    inspired by the MATLAB IDE.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Jupyter or IPython
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**IPython** was initiated in 2001 as a free project by Fernando Perez. It addressed
    a lack in the Python stack for scientific investigations. The author felt that
    Python lacked a user programming interface that could incorporate the scientific
    approach (mainly meaning experimenting and interactively discovering) in the process
    of software development.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: A scientific approach implies fast experimentation with different hypotheses
    in a reproducible fashion (as do data exploration and analysis tasks in data science),
    and when using IPython you will be able to more naturally implement an explorative,
    iterative, trial-and-error research strategy in your code writing.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, a large part of the IPython project has been moved to a new one called
    **Jupyter** ([http://jupyter.org](http://jupyter.org)):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing Jupyter or IPython](img/00003.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'This new project extends the potential usability of the original IPython interface
    to a wide range of programming languages such as the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: R ([https://github.com/IRkernel/IRkernel](https://github.com/IRkernel/IRkernel))
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia ([https://github.com/JuliaLang/IJulia.jl](https://github.com/JuliaLang/IJulia.jl))
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala ([https://github.com/mattpap/IScala](https://github.com/mattpap/IScala))
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a complete list of available kernels, please visit: [https://github.com/ipython/ipython/wiki/IPython-kernels-for-other-languages](https://github.com/ipython/ipython/wiki/IPython-kernels-for-other-languages).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: You can use the same IPython-like interface and interactive programming style
    no matter what language you are developing in, thanks to the powerful idea of
    kernels, which are programs that run the user's code, as communicated by the frontend
    interface; they then provide feedback on the results of the executed code to the
    interface itself.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: IPython (Python is the zero kernel, the original starting point) can be simply
    described as a tool for interactive tasks operable by a console or by a web-based
    notebook, which offers special commands that help developers to better understand
    and build the code currently being written.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to an IDE interface, which is built around the idea of writing a script,
    running it afterwards, and finally evaluating its results, IPython lets you write
    your code in chunks, run each of them sequentially, and evaluate the results of
    each one separately, examining both textual and graphic outputs. Besides graphical
    integration, it provides further help, thanks to customizable commands, a rich
    history (in the JSON format), and computational parallelism for enhanced performance
    when dealing with heavy numeric computations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'In IPython, you can easily combine code, comments, formulas, charts and interactive
    plots, and rich media such as images and videos, making it a complete scientific
    sketchpad for all your experimentations and their results together. Moreover,
    IPython allows reproducible research, allowing any data analysis and model building
    to be recreated easily under different circumstances:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing Jupyter or IPython](img/00004.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: IPython works on your favorite browser (which could be Explorer, Firefox, or
    Chrome, for instance) and when started presents a cell waiting for code to written
    in. Each block of code enclosed in a cell can be run and its results are reported
    in the space just after the cell. Plots can be represented in the notebook (inline
    plot) or in a separate window. In our example, we decided to plot our chart inline.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Notes can be written easily using the Markdown language, a very easy and accessible
    markup language ([http://daringfireball.net/projects/markdown](http://daringfireball.net/projects/markdown)).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Such an approach is also particularly fruitful for tasks involving developing
    code based on data, since it automatically accomplishes the often-neglected duty
    of documenting and illustrating how data analysis has been done, as well as its
    premises, assumptions, and intermediate/final results. If part of your job is
    also to present your work and attract internal or external stakeholders to the
    project, IPython can really perform the magic of storytelling for you with little
    additional effort. On the web page [https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks](https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks),
    there are many examples, some of which you may find inspiring for your work as
    we did.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Actually, we have to confess that keeping a clean, up-to-date IPython Notebook
    has saved us uncountable times when meetings with managers/stakeholders have suddenly
    popped up, requiring us to hastily present the state of our work.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: As an additional resource, IPython offers you a complete library of many magic
    commands that allow you to execute some useful actions such as measuring the time
    it takes for a command to execute, or creating a text file with the output of
    a cell. We distinguish between line magic and cell magic, depending on whether
    they operate a single line of code or the code contained in an entire cell. For
    instance, the magic command `%timeit` measures the time it takes to execute the
    command on the same line of the line magic, whereas `%%time` is a cell magic that
    measures the execution time of an entire cell.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to explore more about magic commands, just type `%quickref` into
    an IPython cell and run it: a complete guide will appear to illustrate all available
    commands.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, IPython lets you:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: See intermediate (debugging) results for each step of the analysis
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run only some sections (or cells) of the code
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store intermediate results in JSON format and have the ability to version-control
    them
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Present your work (this will be a combination of text, code, and images), share
    it via the IPython Notebook Viewer service ([http://nbviewer.ipython.org/](http://nbviewer.ipython.org/)),
    and easily export it into HTML, PDF, or even slide shows
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IPython is our favored choice throughout this book, and it is used to clearly
    and effectively illustrate operations with scripts and data and their consequent
    results.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a complete treatise on the full range of IPython functionalities, please
    refer to the two Packt Publishing books *IPython Interactive Computing and Visualization
    Cookbook*, *Cyrille Rossant*, *Packt Publishing*, September 25 2014, and *Learning
    IPython for Interactive Computing and Data Visualization*, *Cyrille Rossant*,
    *Packt Publishing*, April 25 2013.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'For our illustrative purposes, just consider that every IPython block of instructions
    has a numbered input statement and an output one, so you will find the code presented
    in this book structured in two blocks, at least when the output is not at all
    trivial; otherwise just expect only the input part:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Please notice that we do not number the inputs or the outputs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Though we strongly recommend using IPython, if you are using a REPL approach
    or an IDE interface, you can use the same instructions and expect identical results
    (but for print formats and extensions of the returned results).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Python packages and functions for linear models
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear models diffuse in many different scientific and business applications
    and can be found, under different functions, in quite a number of different Python
    packages. We have selected a few for use in this book. Among them, Statsmodels
    is our choice for illustrating the statistical properties of models, and Scikit-learn
    is instead the package we recommend for easily and seamlessly preparing data,
    building models, and deploying them. We will present models built with Statsmodels
    exclusively to illustrate the statistical properties of the linear models, resorting
    to Scikit-learn to demonstrate how to approach modeling from a data science point
    of view.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NumPy, which is Travis Oliphant's creation, is at the core of every analytical
    solution in the Python language. It provides the user with multidimensional arrays,
    along with a large set of functions to operate multiple mathematical operations
    on these arrays. Arrays are blocks of data arranged along multiple dimensions
    and that implement mathematical vectors and matrices. Arrays are useful not just
    for storing data, but also for fast matrix operations (vectorization), which are
    indispensable when you wish to solve ad hoc data science problems.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'In the book, we are primarily going to use the module `linalg` from NumPy;
    being a collection of linear algebra functions, it will provide help in explaining
    the nuts and bolts of the algorithm:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Website: [http://www.numpy.org/](http://www.numpy.org/)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Import conventions: `import numpy as np`'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Version at the time of print: `1.9.2`'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suggested install command: `pip install numpy`'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a convention largely adopted by the Python community, when importing NumPy,
    it is suggested that you alias it as `np`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There are importing conventions also for other Python features that we will
    be using in the code presented in this book.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: SciPy
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An original project by Travis Oliphant, Pearu Peterson, and Eric Jones, SciPy
    completes NumPy's functionalities, offering a larger variety of scientific algorithms
    for linear algebra, sparse matrices, signal and image processing, optimization,
    fast Fourier transformation, and much more.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'The `scipy.optimize` package provides several commonly used optimization algorithms,
    used to detail how a linear model can be estimated using different optimization
    approaches:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'Website: [http://www.scipy.org/](http://www.scipy.org/)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Import conventions: `import scipy as sp`'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Version at time of print: `0.16.0`'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suggested install command: `pip install scipy`'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statsmodels
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previously part of Scikit, Statsmodels has been thought to be a complement to
    SciPy statistical functions. It features generalized linear models, discrete choice
    models, time series analysis, and a series of descriptive statistics as well as
    parametric and nonparametric tests.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'In Statsmodels, we will use the `statsmodels.api` and `statsmodels.formula.api`
    modules, which provide functions for fitting linear models by providing both input
    matrices and formula''s specifications:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Website: [http:/statsmodels.sourceforge.net/](http://http:/statsmodels.sourceforge.net/)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Import conventions: `import statsmodels.api as sm` and `import statsmodels.formula.api
    as smf`'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Version at the time of print: `0.6.1`'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suggested install command: `pip install statsmodels`'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Started as part of the **SciPy Toolkits** (**SciKits**), Scikit-learn is the
    core of data science operations on Python. It offers all that you may need in
    terms of data preprocessing, supervised and unsupervised learning, model selection,
    validation, and error metrics. Expect us to talk at length about this package
    throughout the book.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn started in 2007 as a Google Summer of Code project by David Cournapeau.
    Since 2013, it has been taken over by the researchers at INRA (French Institute
    for Research in Computer Science and Automation).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn offers modules for data processing (`sklearn.preprocessing`, `sklearn.feature_extraction`),
    model selection, and validation (`sklearn.cross_validation`, `sklearn.grid_search`,
    and `sklearn.metrics`) and a complete set of methods (`sklearn.linear_model`)
    in which the target value, being both a number or a probability, is expected to
    be a linear combination of the input variables:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Website: [http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Import conventions: None; modules are usually imported separately'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Version at the time of print: `0.16.1`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suggested install command: `pip install scikit-learn`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the imported module is named `sklearn`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we glanced at the usefulness of linear models under the data
    science perspective and we introduced some basic concepts of the data science
    approach that will be explained in more detail later and will be applied to linear
    models. We have also provided detailed instructions on how to set up the Python
    environment; these will be used throughout the book to present examples and provide
    useful code snippets for the fast development of machine learning hypotheses.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从数据科学的角度简要探讨了线性模型的有用性，并介绍了一些数据科学方法的基本概念，这些概念将在后续章节中详细解释，并将应用于线性模型。我们还提供了如何设置Python环境的详细说明；这些说明将在整本书中用于展示示例，并提供用于快速开发机器学习假设的有用代码片段。
- en: In the next chapter, we will begin presenting linear regression from its statistical
    foundations. Starting from the idea of correlation, we will build up the simple
    linear regression (using just one predictor) and provide the algorithm's formulations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将从统计基础开始介绍线性回归。从相关性的概念出发，我们将构建简单的线性回归（仅使用一个预测因子）并提供算法的公式。
