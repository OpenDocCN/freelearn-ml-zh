<html><head></head><body>
		<div id="_idContainer082">
			<h1 id="_idParaDest-122"><em class="italic"><a id="_idTextAnchor129"/>Chapter 6</em>: Online Classification</h1>
			<p>In the previous two chapters, you were introduced to some basic notions of classification. You first saw a use case in which online classification models in River were used to build a model that can identify an iris species based on a number of characteristics of a plant. This iris dataset is one of the best-known datasets in the world and is a very common starting point for classification.</p>
			<p>After that, you looked at anomaly detection. We discussed how classification models can be used for anomaly detection for those cases where we can label anomalies as one class and non-anomalies as another class. Specific anomaly detection models are often better at the task, as they strive to understand only the non-anomalies. Classification models will strive to understand each of the classes.</p>
			<p>In this chapter, you'll go much deeper into classification. The chapter will start by posing definitions of what classification is and what it can be used for. You will then see a number of classification models, of which you'll learn the differences between their online and offline counterparts. You will also implement multiple examples in Python using the River package. This will, in the end, result in a model benchmarking study for the use case that will be introduced later on.</p>
			<p>This chapter will cover the following topics:</p>
			<ul>
				<li>Defining classification</li>
				<li>Identifying use cases of classification</li>
				<li>Classification algorithms in River</li>
			</ul>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor130"/>Technical requirements</h1>
			<p>You can find all the code for this book on GitHub at the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python">https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python</a>. If you are not yet familiar with Git and GitHub, the easiest way to download the notebooks and code samples is the following:</p>
			<ol>
				<li>Go to the link of the repository.</li>
				<li>Click the green <strong class="bold">Code</strong> button.</li>
				<li>Select <strong class="bold">Download ZIP</strong>.</li>
			</ol>
			<p>When you download the ZIP file, unzip it in your local environment, and you will be able to access the code through your preferred Python editor.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor131"/>Python environment</h2>
			<p>To follow along with this book, you can download the code in the repository and execute it using your preferred Python editor.</p>
			<p>If you are not yet familiar with Python environments, I would advise you to check out Anaconda (<a href="https://www.anaconda.com/products/individual">https://www.anaconda.com/products/individual</a>), which comes with Jupyter Notebook and JupyterLab, which are both great for executing notebooks. It also comes with Spyder and VSCode for editing scripts and programs.</p>
			<p>If you have difficulty installing Python or the associated programs on your machine, you can check out Google Colab (<a href="https://colab.research.google.com/">https://colab.research.google.com/</a>) or Kaggle Notebooks (<a href="https://www.kaggle.com/code">https://www.kaggle.com/code</a>), which both allow you to run Python code in online notebooks for free, without any setup to do.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor132"/>Defining classification</h1>
			<p>In this chapter, you will discover classification. Classification is a supervised machine learning task in which <a id="_idIndexMarker324"/>a model is constructed that assigns observations to a category.</p>
			<p>The simplest types of classification models that everybody tends to know are decision trees. Let's consider a super simple example of how a decision tree could be used for classification.</p>
			<p>Imagine that we have a dataset in which we have observations about five humans and five animals. The goal is to use this data to build a decision tree that can be used on any new, unseen animal or human.</p>
			<p>The data can be imported as follows:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-1</p>
			<pre class="source-code">import pandas as pd</pre>
			<pre class="source-code"># example to classify human vs animal</pre>
			<pre class="source-code">#dataset with one variable</pre>
			<pre class="source-code">can_speak = [True,True,True,True,True,True,True,False,False,False]</pre>
			<pre class="source-code">has_feathers = [False,False,False,False,False,True,True,False,False,False]</pre>
			<pre class="source-code">is_human = [True,True,True,True,True,False,False,False,False,False]</pre>
			<pre class="source-code">data = pd.DataFrame({'can_speak': can_speak, 'has_feathers': has_feathers, 'is_human': is_human})</pre>
			<pre class="source-code">data</pre>
			<p>The data is <a id="_idIndexMarker325"/>shown in the following figure:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B18335_06_1.jpg" alt="Figure 6.1 – The data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – The data</p>
			<p>Now, to construct the decision tree, you would generally use machine learning, as that is far more efficient <a id="_idIndexMarker326"/>than constructing the tree by hand. Yet, for this example, let's do a simple decision tree that works as the following graph indicates:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B18335_06_2.jpg" alt="Figure 6.2 – The example decision tree&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – The example decision tree</p>
			<p>Of course, this is a model, so it is only a partial representation of the truth. It works quite well for the <a id="_idIndexMarker327"/>current dataset of 10 observations, but with more data points, you would encounter all types of anomalies, so you'd need more variables.</p>
			<p>You could code this model for a <strong class="source-inline">human</strong> versus <strong class="source-inline">not human</strong> classification in Python as follows:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-2</p>
			<pre class="source-code">def self_made_decision_tree(observation):</pre>
			<pre class="source-code">    if observation.can_speak:</pre>
			<pre class="source-code">        if not observation.has_feathers:</pre>
			<pre class="source-code">            return 'human'</pre>
			<pre class="source-code">    </pre>
			<pre class="source-code">    return 'not human'</pre>
			<pre class="source-code">for i,row in data.iterrows():</pre>
			<pre class="source-code">    print(self_made_decision_tree(row))</pre>
			<p>The result is the following:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B18335_06_3.jpg" alt="Figure 6.3 – The predicted outcomes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – The predicted outcomes</p>
			<p>The general idea behind this is that a classification model is any machine learning model that uses the <a id="_idIndexMarker328"/>data to generate decision rules to assign observations to specific classes. In the next section, we'll be going into some use cases of classification to get a better idea of what it can be used for in practice.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor133"/>Identifying use cases of classification</h1>
			<p>The use cases of <a id="_idIndexMarker329"/>classification are huge; it is a very commonly used method in many projects. Still, let's see some examples to get a better idea of the different types of use cases that can benefit from classification methods.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor134"/>Use case 1 – email spam classification</h2>
			<p>The first use case that is generally built on classification is <strong class="bold">spam detection</strong> in email. Spam emails have <a id="_idIndexMarker330"/>been around for a long time. The business model of sending fake emails to generally steal people's money is a big problem, and receiving many spam emails can negatively impact your emailing experience.</p>
			<p>Email service providers have come a long way in detecting spam emails automatically and sending them to your spam/junk box. Nowadays, this is all done automatically and relies heavily on machine learning.</p>
			<p>If you compare this to our super-small classification example, you could imagine that the decision tree (or any other model) can take several information types about every received email and use that to decide whether or not the email should be classified as spam. This has to be done in real time, as nobody wants to wait for a spam detection service to finally send their email through.</p>
			<p>You can read <a id="_idIndexMarker331"/>more about this use case in the following resources:</p>
			<ul>
				<li><a href="https://www.sciencedirect.com/science/article/pii/S2405844018353404">https://www.sciencedirect.com/science/article/pii/S2405844018353404</a></li>
				<li><a href="https://www.enjoyalgorithms.com/blog/email-spam-and-non-spam-filtering-using-machine-learning">https://www.enjoyalgorithms.com/blog/email-spam-and-non-spam-filtering-using-machine-learning</a></li>
			</ul>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor135"/>Use case 2 – face detection in phone camera</h2>
			<p>The second example of classification is face detection when you want to unlock your phone. Your <a id="_idIndexMarker332"/>phone has to make a split-second decision whether the face it's seeing is the face of its owner or not.</p>
			<p>This decision is a classification decision, as it comes down to a yes/no decision: it <em class="italic">is</em> the owner, or it is <em class="italic">not</em> the owner. This decision will generally be made by machine learning, as the rules would be very complex and hard to write down as <strong class="source-inline">if</strong>/<strong class="source-inline">else</strong> statements. Machine learning algorithms are, nowadays, relatively good at such use cases.</p>
			<p>For other more <a id="_idIndexMarker333"/>detailed examples of this use case, you can check out the following links:</p>
			<ul>
				<li><a href="https://www.xfinity.com/hub/mobile/facial-recognition-on-phone">https://www.xfinity.com/hub/mobile/facial-recognition-on-phone</a></li>
				<li><a href="https://www.nytimes.com/wirecutter/blog/how-facial-recognition-works/">https://www.nytimes.com/wirecutter/blog/how-facial-recognition-works/</a></li>
			</ul>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor136"/>Use case 3 – online marketing ad selection</h2>
			<p>A final example to add to the previous two is online marketing ad selection. Many websites <a id="_idIndexMarker334"/>nowadays display personalized ads. This means that you will see an advertisement that matches you as a customer.</p>
			<p>Personalized ad systems do not invent ads though; they have to make a decision and choose between multiple available ads to know which one fits you best. In this way, it is a classification, as it has to decide between multiple choices.</p>
			<p>As you can understand, page loads have to be fast and, therefore, ad selection has to be done in a split second as well. Real-time responses are key for the model to provide any value at all.</p>
			<p>The following <a id="_idIndexMarker335"/>links talk in more depth about this use case:</p>
			<ul>
				<li><a href="https://www.owox.com/blog/articles/machine-learning-in-marketing/">https://www.owox.com/blog/articles/machine-learning-in-marketing/</a></li>
				<li><a href="https://www.ibm.com/watson-advertising/thought-leadership/benefits-of-machine-learning-in-advertising">https://www.ibm.com/watson-advertising/thought-leadership/benefits-of-machine-learning-in-advertising</a></li>
			</ul>
			<p>In the next section, you'll see a more practical side to doing classification, as you will discover several classification algorithms in the River Python library.</p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor137"/>Overview of classification algorithms in River</h1>
			<p>There is a large number of online classification <a id="_idIndexMarker336"/>models available in the River online machine learning package.</p>
			<p>A selection of relevant ones is as follows:</p>
			<ul>
				<li><strong class="source-inline">LogisticRegression</strong></li>
				<li><strong class="source-inline">Perceptron</strong></li>
				<li><strong class="source-inline">AdaptiveRandomForestClassifier</strong></li>
				<li><strong class="source-inline">ALMAClassifier</strong></li>
				<li><strong class="source-inline">PAClassifier</strong></li>
			</ul>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor138"/>Classification algorithm 1 – LogisticRegression</h2>
			<p>Logistic regression is <a id="_idIndexMarker337"/>one of the most basic statistical <a id="_idIndexMarker338"/>classification models. It models a dependent variable (target variable) that has two classes (1 or 0) and can use multiple independent variables to make the prediction.</p>
			<p>The model combines each of the independent variables as log-odds; you can see this as the coefficients in linear regression, except that they are log-odds for each variable. The split in the model is based on the logistic function.</p>
			<p>You can see a simplified schematic of the idea as follows:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B18335_06_4.jpg" alt="Figure 6.4 – The logistic curve&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – The logistic curve</p>
			<h3>Logistic regression in River</h3>
			<p>For online logistic regression, you can use the <strong class="source-inline">LogisticRegression</strong> class in River's <strong class="source-inline">linear_model</strong> section. Let's <a id="_idIndexMarker339"/>now see an example of that:</p>
			<ol>
				<li value="1">First, you can start by making a classification dataset using sklearn's inbuilt <strong class="source-inline">make_blobs</strong> function, which makes classification datasets. You can use the following code for this:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-3</p>
			<p class="source-code">from sklearn.datasets import make_blobs</p>
			<p class="source-code">X,y=make_blobs(shuffle=True,centers=2,n_samples=2000)</p>
			<ol>
				<li value="2">To see what this dataset looks like, it is important to make a plot. You can use the following <strong class="source-inline">matplotlib</strong> code for this:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-4</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">plt.scatter(X[:,0], X[:,1], c=y)</p>
			<p>You should obtain the following plot, or something resembling it:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B18335_06_5.jpg" alt="Figure 6.5 – The data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – The data</p>
			<ol>
				<li value="3">To make sure that your model evaluation will be fair, it is important to make a train-test <a id="_idIndexMarker340"/>split in the data. You can do this with sklearn's <strong class="source-inline">train_test_split</strong>, as shown here:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-5</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</p>
			<ol>
				<li value="4">Let's now move on to the application of the logistic regression model. The following code shows how to fit the model one data point at a time. Note that you should be using a JSON conversion of the input data for <strong class="source-inline">x</strong>, as this is required by River:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-6</p>
			<p class="source-code">!pip install river</p>
			<p class="source-code">from river import linear_model</p>
			<p class="source-code">model=linear_model.LogisticRegression()</p>
			<p class="source-code">for x_i,y_i in zip(X_train,y_train):  </p>
			<p class="source-code">    x_json = {'val1': x_i[0], 'val2': x_i[1]}</p>
			<p class="source-code">    print(x_json, y_i)</p>
			<p class="source-code">    model.learn_one(x_json,y_i)</p>
			<p>The printed <a id="_idIndexMarker341"/>data will look something like this:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B18335_06_6.jpg" alt="Figure 6.6 – The output of Code Block 6-6&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – The output of Code Block 6-6</p>
			<ol>
				<li value="5">You can do predictions one by one as well, or you can use <strong class="source-inline">predict_many</strong> to make all the <a id="_idIndexMarker342"/>predictions on the test set at once. There will not be any difference in the result. In the following code, <strong class="source-inline">predict_many</strong> is used:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-7</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">preds = model.predict_many(pd.DataFrame(X_test,columns=['val1', 'val2']))</p>
			<ol>
				<li value="6">To get a quality metric on this prediction, let's use the accuracy score by <strong class="source-inline">scikit-learn</strong>. As you can see in the following code block, the model has obtained 100% accuracy on the blob data example. It must be stated that this blob data example is a simple prediction task as the data is perfectly separable by a straight line, as can be seen in the plot shown earlier:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-8</p>
			<p class="source-code">from sklearn.metrics import accuracy_score</p>
			<p class="source-code">accuracy_score(y_test, preds)</p>
			<p>This should <a id="_idIndexMarker343"/>result in the following output:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B18335_06_7.jpg" alt="Figure 6.7 – The output of Code Block 6-8&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – The output of Code Block 6-8</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor139"/>Classification algorithm 2 – Perceptron</h2>
			<p>The perceptron is another <a id="_idIndexMarker344"/>algorithm for supervised learning on <a id="_idIndexMarker345"/>classification problems. It takes inputs, multiplies them by weights, and puts the sum of those through an activation function. The output is the resulting classification. The following graph shows an example:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B18335_06_8.jpg" alt="Figure 6.8 – Schematic overview of a perceptron&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Schematic overview of a perceptron</p>
			<h3>Perceptron in River</h3>
			<p>Like logistic regression, the perceptron is a commonly used offline model that has been reworked into <a id="_idIndexMarker346"/>an online model for River. In River, the perceptron has been implemented as a special case of logistic regression.</p>
			<p>You can use the perceptron just like logistic regression. You can use the same code example as in the previous case, as follows:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-9</p>
			<pre class="source-code"># make data</pre>
			<pre class="source-code">from sklearn.datasets import make_blobs</pre>
			<pre class="source-code">X,y=make_blobs(shuffle=True,centers=2,n_samples=2000)</pre>
			<pre class="source-code"># train test split</pre>
			<pre class="source-code">from sklearn.model_selection import train_test_split</pre>
			<pre class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</pre>
			<pre class="source-code"># build the model</pre>
			<pre class="source-code">from river import linear_model</pre>
			<pre class="source-code">model=linear_model.Perceptron()</pre>
			<pre class="source-code"># fit the model</pre>
			<pre class="source-code">for x_i,y_i in zip(X_train,y_train):</pre>
			<pre class="source-code">    x_json = {'val1': x_i[0], 'val2': x_i[1]}</pre>
			<pre class="source-code">    model.learn_one(x_json,y_i)</pre>
			<pre class="source-code">    </pre>
			<pre class="source-code"># predict on the test set</pre>
			<pre class="source-code">import pandas as pd</pre>
			<pre class="source-code">preds = model.predict_many(pd.DataFrame(X_test,columns=['val1', 'val2']))</pre>
			<pre class="source-code"># compute accuracy</pre>
			<pre class="source-code">from sklearn.metrics import accuracy_score</pre>
			<pre class="source-code">accuracy_score(y_test, preds) </pre>
			<p>The result is <strong class="source-inline">1.0</strong>, which is, unsurprisingly, the <a id="_idIndexMarker347"/>same as the logistic regression result.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor140"/>Classification algorithm 3 – AdaptiveRandomForestClassifier</h2>
			<p>In the introduction, you <a id="_idIndexMarker348"/>already saw the general idea <a id="_idIndexMarker349"/>behind a decision tree. Random Forests are an ensemble model that improves decision trees.</p>
			<p>The idea behind Random Forests is that they reduce the error of single decision trees by making a large number of slightly different decision trees. The most common prediction among a large number of decision trees is retained as the final prediction.</p>
			<p>The decision trees are made slightly differently by fitting each of them on a slightly different dataset, which is created by resampling the observations. There is also a subset of variables used for creating the decision tree splits.</p>
			<h3>Random Forest in River</h3>
			<p>For online learning, the data needs to be fitted one by one into the Random Forest, which is not an <a id="_idIndexMarker350"/>easy task. River's implementation is based on the two key elements of Random Forests, which are the resampling and the variable subsets. They have also added drift detection for each single decision tree:</p>
			<ol>
				<li value="1">Let's use an alternative data creation function, which creates data that is harder to separate than the blobs. This function from <strong class="source-inline">sklearn</strong> is called <strong class="source-inline">make_classification</strong>:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-10</p>
			<p class="source-code"># make data</p>
			<p class="source-code">from sklearn.datasets import make_classification</p>
			<p class="source-code">X,y=make_classification(shuffle=True,n_samples=2000)</p>
			<p class="source-code">pd.DataFrame(X).head()</p>
			<p>The data is shown in the following figure:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B18335_06_9.jpg" alt="Figure 6.9 – The new data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9 – The new data</p>
			<ol>
				<li value="2">There is a total of 20 variables generated by default, of which a number are automatically <a id="_idIndexMarker351"/>made more relevant and some are mostly irrelevant. Let's do a train-test split just like before:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-11</p>
			<p class="source-code"># train test split</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</p>
			<ol>
				<li value="3">Using this train-test split, we can move on to building the model:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-12</p>
			<p class="source-code">from river import ensemble</p>
			<p class="source-code">model = ensemble.AdaptiveRandomForestClassifier()</p>
			<p class="source-code"># fit the model</p>
			<p class="source-code">for x_i,y_i in zip(X_train,y_train):</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    model.learn_one(x_json,y_i)</p>
			<ol>
				<li value="4">Now that the model is fit, we can make predictions on the test set. There is no <strong class="source-inline">predict_many</strong> function here, so it is necessary to do a loop with <strong class="source-inline">predict_one</strong> repeatedly:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-13</p>
			<p class="source-code"># predict on the test set</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">preds = []</p>
			<p class="source-code">for x_i in X_test:</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    preds.append(model.predict_one(x_json))</p>
			<ol>
				<li value="5">As a final <a id="_idIndexMarker352"/>step, let's compute the accuracy of this model:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-14</p>
			<p class="source-code"># compute accuracy</p>
			<p class="source-code">from sklearn.metrics import accuracy_score</p>
			<p class="source-code">accuracy_score(y_test, preds)</p>
			<ol>
				<li value="6">The result is <strong class="source-inline">0.86</strong>. Of course, the dataset was more difficult to predict, so that is not to be mistaken for a bad score. As an additional metric, we can look at the classification report for more information:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-15</p>
			<p class="source-code"># classification report</p>
			<p class="source-code">from sklearn.metrics import classification_report</p>
			<p class="source-code">print(classification_report(y_test, preds))</p>
			<p>The result is <a id="_idIndexMarker353"/>shown in the following figure:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B18335_06_10.jpg" alt="Figure 6.10 – The output of Code Block 6-15&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10 – The output of Code Block 6-15</p>
			<p>In this classification report, you see that the precision and recall and the scores for positives and negatives are all relatively equal. This shows that there is no imbalance in the classifier, which is important when relying on the accuracy score.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor141"/>Classification algorithm 4 – ALMAClassifier</h2>
			<p>Now that you have <a id="_idIndexMarker354"/>seen some commonly <a id="_idIndexMarker355"/>used machine learning models for classification in a way adapted to accommodate online learning, it is time to see some more specific models as well. The first of these is the ALMA classifier.</p>
			<p>The <strong class="bold">approximate large margin algorithm</strong> (<strong class="bold">ALMA</strong>) classifier is an incremental implementation of <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>), a commonly used machine learning <a id="_idIndexMarker356"/>model for classification.</p>
			<p>You saw <a id="_idIndexMarker357"/>the adaptation <a id="_idIndexMarker358"/>of SVMs in the previous chapter: a one-class SVM is often used for anomaly detection. For classification, you'd use a regular (two-class) SVM.</p>
			<h3>ALMAClassifier in River</h3>
			<p>Let's see how ALMAClassifier compares <a id="_idIndexMarker359"/>to the adaptive Random Forest, by executing it on the same data:</p>
			<ol>
				<li value="1">We start by applying the same code that we already defined before:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-16</p>
			<p class="source-code"># make data</p>
			<p class="source-code">from sklearn.datasets import make_classification</p>
			<p class="source-code">X,y=make_classification(shuffle=True,n_samples=2000)</p>
			<p class="source-code"># train test split</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</p>
			<p class="source-code">from river import linear_model</p>
			<p class="source-code">model = linear_model.ALMAClassifier()</p>
			<p class="source-code"># fit the model</p>
			<p class="source-code">for x_i,y_i in zip(X_train,y_train):</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    model.learn_one(x_json,y_i) </p>
			<p class="source-code"># predict on the test set</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">preds = []</p>
			<p class="source-code">for x_i in X_test:</p>
			<p class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</p>
			<p class="source-code">    preds.append(model.predict_one(x_json))</p>
			<p class="source-code"># compute accuracy</p>
			<p class="source-code">from sklearn.metrics import accuracy_score</p>
			<p class="source-code">accuracy_score(y_test, preds)</p>
			<ol>
				<li value="2">The result is <strong class="source-inline">0.77</strong>, not as good as the Random Forest. Let's also check the classification <a id="_idIndexMarker360"/>report to see whether anything changed there:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-17</p>
			<p class="source-code"># classification report</p>
			<p class="source-code">from sklearn.metrics import classification_report</p>
			<p class="source-code">print(classification_report(y_test, preds))</p>
			<ol>
				<li value="3">The result is shown in the following figure:</li>
			</ol>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B18335_06_11.jpg" alt="Figure 6.11 – The output of Code Block 6-17&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11 – The output of Code Block 6-17</p>
			<p>There is a little <a id="_idIndexMarker361"/>more variation here, but nothing that seems too shocking. In general, the Random Forest was just better overall for this data.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor142"/>Classification algorithm 5 – PAClassifier</h2>
			<p>The <strong class="bold">passive-aggressive</strong> (<strong class="bold">PA</strong>) classifier is an online machine learning model that is not related to <a id="_idIndexMarker362"/>any existing offline model. It is based on the idea <a id="_idIndexMarker363"/>of updating the model at each step and thereby solving the following problem:</p>
			<p><em class="italic">The update of the classifier is performed by solving a constrained optimization problem: we would like the new classifier to remain as close as possible to the current one while achieving at least a unit margin on the most recent example.</em></p>
			<p>This quote has been taken from the following paper on PA algorithms, which is also an interesting reference for further reading: <a href="https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf</a>.</p>
			<p>The name <em class="italic">passive-aggressive</em> comes from the idea that an algorithm that learns too quickly from each new data point is considered too aggressive. PA is less aggressive.</p>
			<h3>PAClassifier in River</h3>
			<p>Let's see how the PA classifier <a id="_idIndexMarker364"/>performs on the same task as the two previous models:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 6-18</p>
			<pre class="source-code"># make data</pre>
			<pre class="source-code">from sklearn.datasets import make_classification</pre>
			<pre class="source-code">X,y=make_classification(shuffle=True,n_samples=2000)</pre>
			<pre class="source-code"># train test split</pre>
			<pre class="source-code">from sklearn.model_selection import train_test_split</pre>
			<pre class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</pre>
			<pre class="source-code">from river import linear_model</pre>
			<pre class="source-code">model = linear_model.PAClassifier()</pre>
			<pre class="source-code"># fit the model</pre>
			<pre class="source-code">for x_i,y_i in zip(X_train,y_train):</pre>
			<pre class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</pre>
			<pre class="source-code">    model.learn_one(x_json,y_i)</pre>
			<pre class="source-code"># predict on the test set</pre>
			<pre class="source-code">import pandas as pd</pre>
			<pre class="source-code">preds = []</pre>
			<pre class="source-code">for x_i in X_test:</pre>
			<pre class="source-code">    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}</pre>
			<pre class="source-code">    preds.append(model.predict_one(x_json))</pre>
			<pre class="source-code"># compute accuracy</pre>
			<pre class="source-code">from sklearn.metrics import accuracy_score</pre>
			<pre class="source-code">accuracy_score(y_test, preds)</pre>
			<p>The obtained <a id="_idIndexMarker365"/>score is <strong class="source-inline">0.85</strong>. The following section summarizes all the scores that we have obtained.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor143"/>Evaluating benchmark results</h2>
			<p>This leaves us <a id="_idIndexMarker366"/>with the following accuracy scores for the past three models:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B18335_06_Table_01.jpg" alt="Table 6.1 – The table with the results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 6.1 – The table with the results</p>
			<p>The best result was <a id="_idIndexMarker367"/>obtained by AdaptiveRandomForest and PAClassifier came in second place. ALMAClassifier was less performant with a score of <strong class="source-inline">0.77</strong>.</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor144"/>Summary</h1>
			<p>In this chapter, you have first seen a general overview of classification and its use cases. You have understood how it is different from anomaly detection, but how it can sometimes still be applied to anomaly detection use cases.</p>
			<p>You have learned about five models for online classification of which some are mainly adaptations of offline models, and others are specifically designed for working in an online manner. Both types exist, and it is important to have the tools to benchmark model performance before making a choice for a final model.</p>
			<p>The model benchmark that you executed in Python was done in such a way as to find the best model in terms of the accuracy of the model on a test set. You have seen clear differences between the benchmarked models, and this is a great showcase for the importance of model benchmarking.</p>
			<p>In the following chapter, you will do the same type of model benchmarking exercise, but this time, you will be focusing on a regression use case, which has a goal that is fundamentally different from classification. This comes with some changes with respect to measuring errors and benchmarking, but from a high-level perspective, also has a lot in common with the classification benchmarking use case that you worked with in this chapter.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor145"/>Further reading</h1>
			<ul>
				<li><em class="italic">LogisticRegression</em>: <a href="https://riverml.xyz/latest/api/linear-model/LogisticRegression/">https://riverml.xyz/latest/api/linear-model/LogisticRegression/</a></li>
				<li><em class="italic">Perceptron</em>: <a href="https://riverml.xyz/latest/api/linear-model/Perceptron/">https://riverml.xyz/latest/api/linear-model/Perceptron/</a></li>
				<li><em class="italic">AdaptiveRandomForestClassifier</em>: <a href="https://riverml.xyz/latest/api/ensemble/AdaptiveRandomForestClassifier/">https://riverml.xyz/latest/api/ensemble/AdaptiveRandomForestClassifier/</a></li>
				<li><em class="italic">ALMA</em>: <a href="https://riverml.xyz/latest/api/linear-model/ALMAClassifier/">https://riverml.xyz/latest/api/linear-model/ALMAClassifier/</a></li>
				<li><em class="italic">ALMA</em>: <a href="https://www.jmlr.org/papers/volume2/gentile01a/gentile01a.pdf&#13;">https://www.jmlr.org/papers/volume2/gentile01a/gentile01a.pdf</a></li>
				<li><em class="italic">PAClassifier</em>: <a href="https://riverml.xyz/latest/api/linear-model/PAClassifier/&#13;">https://riverml.xyz/latest/api/linear-model/PAClassifier/</a></li>
				<li><em class="italic">PAClassifier</em>: <a href="https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf&#13;">https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf</a></li>
				<li><em class="italic">make_classification</em>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.htm&#13;">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.htm</a></li>
				<li><em class="italic">make_blobs</em>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html</a></li>
			</ul>
		</div>
	</body></html>