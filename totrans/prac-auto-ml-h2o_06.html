<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer224">
<h1 class="chapter-number" id="_idParaDest-97"><a id="_idTextAnchor129"/>6</h1>
<h1 id="_idParaDest-98"><a id="_idTextAnchor130"/>Understanding H2O AutoML Leaderboard and Other Performance Metrics</h1>
<p>When we train ML models, the statistical nuances of different algorithms often make it difficult to compare one model with another model that is trained using a different algorithm. From a professional standpoint, you will eventually need to select the right model to solve your ML problem. So, the question arises: how do you compare two different models solving the same ML problem and decide which one is better?</p>
<p>This is where model performance metrics come in. Model performance metrics are certain numerical metrics that give an accurate measurement of a model’s performance. The performance of a model can mean various things and can also be measured in several ways. The way we evaluate a model, whether it is a classification model or a regression model, only differs by the metrics that we use for that evaluation. You can measure how accurately the model classifies objects by measuring the number of correct and incorrect predictions. You can measure how accurately the model predicted a stock price and note the magnitude of the error between the predicted value and the actual value. You can also compare how the model fairs with outliers in data.</p>
<p>H2O provides plenty of model performance measuring techniques. Most of them are automatically calculated and stored as the model metadata whenever a model is trained. H2O AutoML further automates the selection of models as well. It does so by presenting you with a leaderboard comparing the different performance metrics of the trained models. In this chapter, we will explore the different performance metrics that are used in the AutoML leaderboard, as well as some additional metrics that are important for users to know. </p>
<p>We shall explore these performance metrics according to the following sections:</p>
<ul>
<li>Exploring the H2O AutoML leaderboard performance metrics</li>
<li>Exploring other important performance metrics</li>
</ul>
<p>By the end of this chapter, you should understand how a model’s performance is measured and how we can use these metrics to get an understanding of its prediction behavior.</p>
<p>So, let’s begin by exploring and understanding the H2O AutoML leaderboard performance metrics.</p>
<h1 id="_idParaDest-99"><a id="_idTextAnchor131"/>Exploring the H2O AutoML leaderboard performance metrics</h1>
<p>In <a href="B17298_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with H2O Flow (H2O’s Web UI)</em>, once we trained<a id="_idIndexMarker634"/> the models on a dataset using H2O AutoML, the results of the models were stored in a leaderboard. The leaderboard was a table containing the model IDs and certain metric values for the respective models (<em class="italic">see Figure 2.33</em>).</p>
<p>The leaderboard ranks the models based on a default metric, which is ideally the second column in the table. The ranking metrics depend on what kind of prediction problem the models are trained on. The following list represents the ranking metrics used for the respective ML problems:</p>
<ul>
<li>For binary classification<a id="_idIndexMarker635"/> problems, the ranking metric is <strong class="bold">AUC</strong>.</li>
<li>For multi-classification<a id="_idIndexMarker636"/> problems, the ranking metric is the <strong class="bold">mean per-class error</strong>.</li>
<li>For regression<a id="_idIndexMarker637"/> problems, the ranking metric is <strong class="bold">deviance</strong>.</li>
</ul>
<p>Along with the ranking metrics, the leaderboard also provides some additional performance metrics for a better understanding of the model quality.</p>
<p>Let’s try to understand these performance metrics, starting with the mean squared error.</p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor132"/>Understanding the mean squared error and the root mean squared error</h2>
<p><strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>), also called <strong class="bold">Mean Squared Deviation</strong> (<strong class="bold">MSD</strong>), as the name suggests, is a metric<a id="_idIndexMarker638"/> that measures the mean<a id="_idIndexMarker639"/> of the squares of errors of the predicted<a id="_idIndexMarker640"/> value against the actual value. </p>
<p>Consider the following regression scenario:</p>
<div>
<div class="IMG---Figure" id="_idContainer171">
<img alt="" height="356" src="image/B17298_06_001.jpg" width="1006"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – The MSE in a regression scenario</p>
<p>This is a generic regression scenario where the line of regression passes through the data points plotted on the graph. The trained model makes predictions based on this line of regression. The error values show the difference between the actual value and the predicted value, which lies on the line of regression, as denoted by the red lines. These<a id="_idIndexMarker641"/> errors are also called residuals. When calculating the MSE, we square these errors to remove any negative signs, as we are only concerned with the magnitude of the error and not its direction. The squaring also gives more weight to larger error values. Once all the squared errors have been calculated for all the data points, we calculate the mean, which gives us the final MSE value.</p>
<p>The MSE<a id="_idIndexMarker642"/> is a metric that tells you how close the line of regression is to the data points. Accordingly, the fewer error values the line of regression has against the data points, the lower your MSE value will be. Thus, when comparing the MSE of different models, the model with the lower MSE is ideally the more accurate one.</p>
<p>The mathematical<a id="_idIndexMarker643"/> formula for the MSE is as follows:</p>
<p class="IMG---Figure"><img alt="" height="67" src="image/Formula_B17298_06_001.png" width="822"/></p>
<p>Here, <em class="italic">n</em> would be the number of data points in the dataset.</p>
<p>The <strong class="bold">Root Mean Squared Error</strong> (<strong class="bold">RMSE</strong>), as the name suggests, is the root value<a id="_idIndexMarker644"/> of the MSE. So <a id="_idIndexMarker645"/>accordingly, its mathematical formula is as follows:</p>
<p class="IMG---Figure"><img alt="" height="63" src="image/Formula_B17298_06_002.png" width="928"/></p>
<p>The difference between the MSE and the RMSE is straightforward. While the MSE is measured in the squared units of the response column, the RMSE is measured in the same units as the response column.</p>
<p>For example, if you have a linear regression problem that predicts the price of its stock in terms of dollars, the MSE measures the errors in terms of squared dollars, while RMSE measures the error value as just dollars. Hence, the RMSE is often used over the MSE, as it is slightly easier to interpret the model quality from the RMSE than the MSE.</p>
<p>Congratulations – you are now aware of what MSE and RMSE metrics are and how they can be used to measure the performance of a regression model.</p>
<p>Let’s move on to the next important performance metric, which is the confusion matrix.</p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor133"/>Working with the confusion matrix</h2>
<p>A classification problem<a id="_idIndexMarker646"/> is an ML problem where the ML model tries to classify the data inputs into the pre-specified classes. What makes<a id="_idIndexMarker647"/> the performance measurement of classification models different from regression models is that in classification problems, there is no numeric magnitude of the error between the predicted value and the actual value. The predicted value is either correctly classified into the right class or it is incorrectly classified. To measure model performance for classification problems, data scientists rely on certain performance metrics that are derived from a special type of matrix called a <strong class="bold">confusion matrix</strong>.</p>
<p>A confusion matrix<a id="_idIndexMarker648"/> is a tabular matrix that summarizes the correctness of the prediction results of a classification problem. The matrix presents the count of correct and incorrect predicted values alongside each other, as well as breaking them down by each class. This matrix is called a confusion matrix, as it shows how confused the model is when classifying the values.</p>
<p>Consider the example of the heart disease prediction dataset we used. It is a binary classification problem where we want to predict whether a person with certain health conditions is likely to suffer<a id="_idIndexMarker649"/> from heart disease or not. In this case, the prediction is either <strong class="bold">Yes</strong>, also called a <strong class="bold">positive classification</strong>, meaning the person is likely to suffer from heart disease, or <strong class="bold">No</strong>, also called a <strong class="bold">negative classification</strong>, meaning the person is not likely<a id="_idIndexMarker650"/> to suffer from heart disease.</p>
<p>The confusion matrix of this scenario will be as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer174">
<img alt="" height="256" src="image/B17298_06_002.jpg" width="772"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – A binomial confusion matrix</p>
<p>The rows of the confusion matrix<a id="_idIndexMarker651"/> correspond to the classifications predicted by the model. The columns of the confusion matrix correspond to the actual class values of the model. </p>
<p>In the top-left corner of the matrix, we have <strong class="bold">true positives</strong> – these are the number of <strong class="bold">Yes</strong> actuals that were correctly predicted as Yes. In the top-right corner, we have the <strong class="bold">false positives</strong> – these are the number of Yes actuals that were incorrectly predicted as <strong class="bold">No</strong>. In the bottom-left corner, we have <strong class="bold">false negatives</strong> – these are the number of No actuals that were incorrectly<a id="_idIndexMarker652"/> predicted as Yes values. And finally, we have the <strong class="bold">true negative</strong> – these are the number of No actuals that were correctly predicted as No.</p>
<p>The confusion matrix for a multinomial classification with six possible classes will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer175">
<img alt="" height="565" src="image/B17298_06_003.jpg" width="1370"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – A multinomial confusion matrix</p>
<p>Using the confusion matrix<a id="_idIndexMarker653"/> of two classification models, you can compare the number of true positives and true negatives that were predicted by the individual algorithms and select the one with the greater number of correct predictions as the better model.</p>
<p>Despite being very easy to interpret the prediction quality of a model using the confusion matrix, it is still difficult to compare two or more models solely based on the number of true positives and true negatives. </p>
<p>Consider a scenario where you want to classify some medical records to identify whether the patient has a brain tumor. Let’s assume that a specific model’s confusion matrix has a high number of true positives and true negatives compared to other models and also has a high number of false positives. In this case, the model will incorrectly flag a lot of normal medical records as indicative of a potential brain tumor. This might result in hospitals making incorrect decisions and performing risky surgeries that were never needed. In such a scenario, models with less accuracy but the smallest number of false positives are preferable.</p>
<p>Hence, more sophisticated metrics<a id="_idIndexMarker654"/> are developed on top of the confusion matrix. They are as follows:</p>
<ul>
<li><strong class="bold">Accuracy</strong>: Accuracy is a metric that measures<a id="_idIndexMarker655"/> the number of correctly predicted positive and negative predictions against the total number of predictions made. This is calculated as follows:</li>
</ul>
<p class="IMG---Figure"><img alt="" height="114" src="image/Formula_B17298_06_003.png" width="566"/></p>
<p>Here, the abbreviations <a id="_idIndexMarker656"/>stand for the following:</p>
<ul>
<li><strong class="bold">TP</strong> stands for True Positive.</li>
<li><strong class="bold">TN</strong> stands for True Negative.</li>
<li><strong class="bold">FP</strong> stands for False Positive.</li>
<li><strong class="bold">FN</strong> stands for False Negative.</li>
</ul>
<p>This metric is useful when you want to compare how well a classification model correctly makes predictions, irrespective of whether the prediction value is positive or negative.</p>
<ul>
<li><strong class="bold">Precision</strong>: Precision is a metric that measures the number<a id="_idIndexMarker657"/> of correct positive predictions made compared to the total number of positive predictions made. This is calculated as follows:</li>
</ul>
<p class="IMG---Figure"><img alt="" height="90" src="image/Formula_B17298_06_004.png" width="747"/></p>
<p>This metric is especially useful when measuring the performance of a classification model that is trained on data with a high number of negative results and only a few positive results. Precision is not affected by the imbalance of positive and negative classification values, as it only considers positive values.</p>
<ul>
<li><strong class="bold">Sensitivity or recall</strong>: Sensitivity, also known as recall, is a<a id="_idIndexMarker658"/> probability measurement<a id="_idIndexMarker659"/> for how well a model can predict true positives. Sensitivity is measured by identifying what percentage of predictions were correctly identified as positive in a binomial classification. This is calculated as follows:</li>
</ul>
<p class="IMG---Figure"><img alt="" height="120" src="image/Formula_B17298_06_005.png" width="836"/></p>
<p>If your classification ML problem <a id="_idIndexMarker660"/>aims to accurately identify all the positive predictions, then the sensitivity of the model should be high.</p>
<ul>
<li><strong class="bold">Specificity</strong>: While sensitivity is the probability<a id="_idIndexMarker661"/> measurement of how well a model can predict true positives, specificity is measured by identifying what percentage of predictions were correctly identified as negative in a binomial classification. This is calculated as follows:</li>
</ul>
<p class="IMG---Figure"><img alt="" height="120" src="image/Formula_B17298_06_006.png" width="839"/></p>
<p>If your classification ML problem<a id="_idIndexMarker662"/> aims to accurately identify all the negative predictions, then the specificity of the model should be high.</p>
<p>There is always a trade-off between sensitivity and specificity. A model with high sensitivity will often have very low specificity and vice versa. Thus, the context of the ML problem plays a very important part in deciding whether you want a model with high sensitivity or high specificity to solve the problem.</p>
<p>For multinomial classification, you calculate the sensitivity and specificity for each class type. For sensitivity, your true positives will remain the same, but the false negatives will change depending on the number of incorrect predictions made for that class. Similarly, for specificity, the true negatives will remain the same – however, the false positives will change depending on the number of incorrect predictions made for that class.</p>
<p>Now that you have understood <a id="_idIndexMarker663"/>how a confusion matrix<a id="_idIndexMarker664"/> is used for measuring classification models and how sensitivity and specificity are built on top of it, let’s now move on to the next metric, which is the receiver operating characteristic curve and its area under the curve.</p>
<h2 id="_idParaDest-102">Calculating the receiver operating characteristic and its <a id="_idTextAnchor134"/>area under the curve (ROC-AUC)</h2>
<p>Another good way of comparing<a id="_idIndexMarker665"/> classification models is via a visual<a id="_idIndexMarker666"/> representation of their performance. One of the most widely used visual evaluation metrics is the <strong class="bold">Receiver Operating Characteristic</strong> and its <strong class="bold">Area Under the Curve</strong> (<strong class="bold">ROC-AUC</strong>).</p>
<p>The ROC-AUC metric is split into two concepts:</p>
<ul>
<li><strong class="bold">The ROC curve</strong>: This is the graphical curve plotted on a graph that summarizes the model’s classification ability at various thresholds. The threshold is a classification value that separates the data points into different classes.</li>
<li><strong class="bold">AUC</strong>: This is the area under the ROC curve that helps us compare which classification algorithm performed better depending on whose ROC curve covers the most area.</li>
</ul>
<p>Let’s consider an example to better understand how ROC-AUC can help us compare classification models. Refer to the following sample dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer180">
<img alt="" height="387" src="image/B17298_06_004.jpg" width="887"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – An obesity dataset</p>
<p>This dataset has two columns:</p>
<ul>
<li><strong class="bold">Weight (kgs)</strong>: This is a numerical column that contains the weight of a person in kilograms</li>
<li><strong class="bold">Obese</strong>: This is a categorical column that contains either <strong class="bold">1</strong> or <strong class="bold">0</strong>, where <strong class="bold">1</strong> indicates the person is obese and <strong class="bold">0</strong> indicates that the person is not obese</li>
</ul>
<p>Let’s plot this dataset<a id="_idIndexMarker667"/> onto a graph where <strong class="bold">Weight</strong>, being the independent<a id="_idIndexMarker668"/> variable, is on the <em class="italic">x</em>-axis, and <strong class="bold">Obese</strong>, being the dependent variable, is on the <em class="italic">y</em>-axis. This simple dataset on a graph will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer181">
<img alt="" height="273" src="image/B17298_06_005.jpg" width="684"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – The plotted obesity dataset</p>
<p>Let’s fit a classification line through<a id="_idIndexMarker669"/> this data using one <a id="_idIndexMarker670"/>of the simplest classification algorithms called <strong class="bold">logistic regression</strong>. Logistic regression is an algorithm that predicts<a id="_idIndexMarker671"/> the probability that a given sample of data belongs to a certain class. In our example, the algorithm will predict the probability of whether the person is obese or not depending on their weight.</p>
<p>The logistic regression line will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer182">
<img alt="" height="285" src="image/B17298_06_006.jpg" width="975"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – A plotted obesity dataset with a classification line</p>
<p>Note that since logistic regression predicts the probability that data might belong to a certain class, we have converted the <em class="italic">y</em>-axis into the probability that the person is obese.</p>
<p>During prediction, we will first plot the sample weight data of the person on the <em class="italic">x</em>-axis. We will then find its respective <em class="italic">y</em> value on the line of classification. This value is the probability that the respective person is obese.</p>
<p>Now, to classify whether the person<a id="_idIndexMarker672"/> is obese or not, we will need<a id="_idIndexMarker673"/> to decide what the probability <strong class="bold">cut-off line</strong> that separates obese<a id="_idIndexMarker674"/> and not obese will be. This cut-off line is called the <strong class="bold">threshold</strong>. Any probability value above<a id="_idIndexMarker675"/> the threshold can be categorized as obese and any value below it can be categorized as not obese. The threshold can be any value between 0 and 1:</p>
<div>
<div class="IMG---Figure" id="_idContainer183">
<img alt="" height="312" src="image/B17298_06_007.jpg" width="834"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – An obesity dataset classification with a threshold line</p>
<p>As you can see from the diagram, multiple values are incorrectly classified. This is bound to happen in any classification problem. So, to keep a track of the correct and incorrect classification, we will create a confusion matrix and calculate sensitivity and specificity to evaluate how well the model performs for the selected threshold.</p>
<p>But as mentioned previously, there can be many thresholds for classification. Thresholds with high values will minimize the number of false positives but the trade-off is that classification for that class will become stricter, leading to more false negatives. Similarly, if the threshold value is too low, then we will end up with more False Positives.</p>
<p>Which threshold performs best depends on your ML problem. However, a comparative study of different thresholds is needed to find a suitable value. Since you can create any number of thresholds, you will end up creating plenty of confusion matrices eventually. This is where the ROC-AUC metric comes in.</p>
<p>The ROC-AUC metric<a id="_idIndexMarker676"/> summarizes the performance<a id="_idIndexMarker677"/> of the model at different thresholds and plots them on a graph. In this graph, the <em class="italic">x</em>-axis is the False Positive rate, which is <strong class="bold">1 - specificity</strong>, while the <em class="italic">y</em>-axis is the true positive rate, which is nothing but <strong class="bold">sensitivity</strong>.</p>
<p>Let’s plot the ROC graph for our sample dataset. We will start by using a threshold that classifies all samples as obese. The threshold on the graph will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<img alt="" height="255" src="image/B17298_06_008.jpg" width="505"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – A plotted obesity classification with a very low threshold</p>
<p>We will now need to calculate the sensitivity (and 1 - specificity) values needed to plot our ROC curve, so accordingly, we will need to create a confusion matrix first. The confusion matrix for this threshold will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<img alt="" height="271" src="image/B17298_06_009.jpg" width="1042"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – A confusion matrix with sensitivity and 1 - specificity</p>
<p>Calculating the sensitivity<a id="_idIndexMarker678"/> and 1 - specificity values using the formula<a id="_idIndexMarker679"/> mentioned previously, we get a sensitivity equal to <strong class="bold">1</strong> and a 1 - specificity equal to <strong class="bold">0.5</strong>. Let’s plot this value in the ROC graph. The ROC graph will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer186">
<img alt="" height="258" src="image/B17298_06_010.jpg" width="593"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – The ROC graph</p>
<p>The blue line in the diagram indicates that the sensitivity is equal to the 1 - specificity – in other words, the true positive rate is equal to the False Positive rate. Any ROC points on this line indicate that the model trained using this threshold has an equal likelihood of predicting a correct positive as predicting an incorrect positive. So, to find the best threshold, we aim to find a ROC point that has as high a sensitivity as possible and as low a 1 - specificity as possible. This would indicate that the model has a high likelihood of predicting a correct positive prediction and a much smaller likelihood of predicting an incorrect positive prediction.</p>
<p>Let’s now raise the threshold<a id="_idIndexMarker680"/> and repeat the same process<a id="_idIndexMarker681"/> to calculate the ROC value for this new threshold. Let’s assume this new threshold has a sensitivity of 1 and a 1 - specificity of 0.25. Plotting this value in the ROC graph, we get the following result:</p>
<div>
<div class="IMG---Figure" id="_idContainer187">
<img alt="" height="345" src="image/B17298_06_011.jpg" width="895"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – A ROC graph with the new threshold</p>
<p>The new ROC value<a id="_idIndexMarker682"/> for the new threshold<a id="_idIndexMarker683"/> is on the left side of the blue line and also of the previous ROC point. This indicates that it has a lower false positive rate compared to the previous threshold. Thus, the new threshold is better than the previous one.</p>
<p>Raising the threshold value way too high will make the model predict that all the values are not obese. Basically, it will incorrectly predict all the values as false, increasing the number of false negatives. Based on the sensitivity equation, the higher the number of false negatives, the lower the sensitivity. So, this will eventually lower your sensitivity, reducing the model’s ability to predict the true positives.</p>
<p>We repeat this same process for different threshold values and plot their ROC values on the ROC graph. If we connect all these dots, we get the ROC curve:</p>
<div>
<div class="IMG---Figure" id="_idContainer188">
<img alt="" height="347" src="image/B17298_06_012.jpg" width="978"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – The ROC graph with a ROC curve</p>
<p>Just by looking at the ROC<a id="_idIndexMarker684"/> graph, you can identify which threshold values<a id="_idIndexMarker685"/> are better than the others, and depending on how many false positive predictions your ML problem can tolerate, you can select the ROC point with the right false positive rate as your final threshold value reference. This explains what the ROC curve does.</p>
<p>Now, suppose you have another algorithm trained with different thresholds and you plot its ROC points on this same graph. Assume the plots look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer189">
<img alt="" height="322" src="image/B17298_06_013.jpg" width="881"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – A ROC graph with multiple ROC curves</p>
<p>How would you compare<a id="_idIndexMarker686"/> which algorithm performed better? Which<a id="_idIndexMarker687"/> threshold is the optimum one for that algorithm’s model?</p>
<p>This is where AUC helps us. AUC is nothing but the area under the ROC curve. The whole ROC graph will have a total area of <em class="italic">1</em>. The red line splits the area into half, so ideally, all potentially good algorithms should have an AUC greater than 0.5. The greater the AUC is, the better the algorithm is: </p>
<div>
<div class="IMG---Figure" id="_idContainer190">
<img alt="" height="383" src="image/B17298_06_014.jpg" width="936"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – The AUC of the ROC curve</p>
<p>Just by visualizing this, you can see which algorithm is better from its AUC. Similarly, the AUC values help engineers and scientists identify which algorithm to choose and which threshold to use as the optimal ML model for classification.</p>
<p>Congratulations, you just<a id="_idIndexMarker688"/> understood how the ROC-AUC metric<a id="_idIndexMarker689"/> works and how it can help you compare model performance. Let’s now move on to another similar performance metric called the <strong class="bold">Precision-Recall curve</strong> (<strong class="bold">PR curve</strong>).</p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor135"/>Calculating the precision-recall curve and its area under the curve (AUC-PR)</h2>
<p>With ROC-AUC, despite<a id="_idIndexMarker690"/> being a very good metric to compare models, there<a id="_idIndexMarker691"/> is a minor drawback to relying on it exclusively. In a very imbalanced dataset, where there is a large number of true negative values, the <em class="italic">x</em>-axis of the ROC graph will be very small, as specificity has a true negative value as its denominator. This forces the ROC curve toward the left side of the graph, raising the ROC-AUC value toward 1, which is technically incorrect.</p>
<p>This is where the <strong class="bold">PR curve</strong> proves beneficial. The PR curve<a id="_idIndexMarker692"/> is similar to the ROC curve, the only<a id="_idIndexMarker693"/> difference being that the PR curve<a id="_idIndexMarker694"/> is a function that uses precision on the <em class="italic">y</em>-axis and recall on the <em class="italic">x</em>-axis. Neither precision nor recall uses true negatives in their calculation. Hence, the PR curve and its AUC metric are suitable when there is an imbalance in the classes of the dataset that impacts the true negatives during prediction, or when your ML problem is not concerned with true negatives at all.</p>
<p>Let’s understand the PR curve further using an example. We will use the same sample obesity dataset that we used for understanding the ROC-AUC curve. The process of plotting the records of the dataset on the graph and creating its confusion matrix is the same as for the ROC-AUC curve.</p>
<p>Now, instead of calculating the sensitivity and 1 – specificity from the confusion matrix, this time, we shall calculate the precision and recall values: </p>
<div>
<div class="IMG---Figure" id="_idContainer191">
<img alt="" height="176" src="image/B17298_06_015.jpg" width="689"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.15 – Calculating the precision and recall values</p>
<p>As you can see from the preceding diagram, we got precision values of <strong class="bold">0.625</strong> and recall values of <strong class="bold">1</strong>. Let’s plot these values onto the PR graph as shown in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer192">
<img alt="" height="282" src="image/B17298_06_016.jpg" width="636"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.16 – The PR graph</p>
<p>Similarly, by moving the threshold<a id="_idIndexMarker695"/> line and creating the new confusion<a id="_idIndexMarker696"/> matrix, the precision and recall values will change based on the distribution of the predictions in the confusion matrix. We repeat this same process for different threshold values, calculate the precision and recall values, and then plot them onto the PR graph:</p>
<div>
<div class="IMG---Figure" id="_idContainer193">
<img alt="" height="279" src="image/B17298_06_017.jpg" width="629"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.17 – A PR graph and its PR curve</p>
<p>The blue line that joins all the points is the PR curve. The point that represents a threshold value closest<a id="_idIndexMarker697"/> to the black point, so closest to having a precision<a id="_idIndexMarker698"/> value of 1 and a recall value of 1, is the ideal classifier.</p>
<p>When comparing different algorithm models, you will have multiple PR curves in the PR graph:</p>
<div>
<div class="IMG---Figure" id="_idContainer194">
<img alt="" height="280" src="image/B17298_06_018.jpg" width="630"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.18 – A PR graph with multiple PR curves</p>
<p>The preceding diagram shows you the multiple PR curves that can be plotted on the same graph to give you a better comparative view of the performances of different algorithms. With one glance, you can see that the algorithm represented by the blue line has a threshold value that is closest to the black point and should ideally be the best performing model. </p>
<p>Just as with ROC-AUC, you can also use the AUC-PR to calculate the area under the PR curves to get a better understanding of the performances of different algorithms. Based on this, you know that the algorithm represented by the red PR curve is better than the one with the yellow curve and the algorithm represented by the blue PR curve is better than both the red and yellow curves.</p>
<p>Congratulations! You now understand<a id="_idIndexMarker699"/> the AUC-PR metric in the H2O AutoML leaderboard<a id="_idIndexMarker700"/> and how it can be another good model performance metric that you can refer to when comparing models trained by H2O AutoML.</p>
<p>Let’s now move on to the next performance metric, which is called log loss.</p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor136"/>Working with log loss</h2>
<p><strong class="bold">Log loss</strong> is another important model performance metric<a id="_idIndexMarker701"/> for classification models. It is primarily used to measure the performance of binary classification models.</p>
<p>Log loss<a id="_idIndexMarker702"/> is a way of measuring the performance<a id="_idIndexMarker703"/> of a classification model that outputs classification results in the form of probability values. The probability values can range from <em class="italic">0</em>, which means that the data has zero probability that it belongs to a certain positive class, to <em class="italic">1</em>, which means the data has a 100% chance of belonging to a certain positive class. The log loss value can range from 0 to infinity and the goal of all ML models is to minimize the log loss as much as possible. Any model with a log loss value as close to 0 as possible is regarded as the better performing model.</p>
<p>Log loss calculation is entirely statistical. However, it is important to understand the intuition behind the mathematics to better understand its application when comparing model performances.</p>
<p>Log loss is a metric that measures the divergence of the predicted probability from the actual value. So, if the predicted probability diverges very little from the actual value, then your log loss value will be forgiving – however, if the divergence is greater, the log loss value will be that much more punishing.</p>
<p>Let’s start by understanding what prediction probability is. We shall use the same obesity dataset that we used for the ROC-AUC curve. Assume we ran a classification algorithm that calculated the prediction probability that the person is obese and let’s add those values to a column in the dataset, as seen in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer195">
<img alt="" height="572" src="image/B17298_06_019.jpg" width="1143"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.19 – The obesity dataset with the prediction probabilities added</p>
<p>We will have a certain threshold <a id="_idIndexMarker704"/>value that decides what the prediction<a id="_idIndexMarker705"/> probability value has to be for us to classify the data as obese or not obese. Let’s assume the threshold is 0.5 – in this case, a prediction probability value above 0.5 is classified as obese and anything below it is classified as not obese.</p>
<p>We now calculate the log loss value of each data point. The equation for calculating the log loss of each record is as follows:</p>
<p class="IMG---Figure"><img alt="" height="76" src="image/Formula_B17298_06_007.png" width="989"/></p>
<p>Here, the equation can be broken down as follows:</p>
<ul>
<li><strong class="bold">y</strong> is the actual classification value, that is, <em class="italic">0</em> or <em class="italic">1</em>.</li>
<li><strong class="bold">p</strong> is the prediction probability.</li>
<li><strong class="bold">log</strong> is the natural logarithm of the number.</li>
</ul>
<p>In our example, since we are using the obese class as a reference, we shall set <em class="italic">y</em> to <em class="italic">1</em>. Using this equation, we calculate the log loss value of individual data values as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer197">
<img alt="Figure 6.20 – An obesity dataset with the log loss values per record " height="594" src="image/B17298_06_020.jpg" width="1061"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.20 – An obesity dataset with the log loss values per record</p>
<p>Now, let’s plot these values<a id="_idIndexMarker706"/> into a log loss graph<a id="_idIndexMarker707"/> where we set the log loss values on the <em class="italic">y</em>-axis and the prediction probability on the <em class="italic">x</em>-axis:</p>
<div>
<div class="IMG---Figure" id="_idContainer198">
<img alt="Figure 6.21 – A log loss graph where y = 1 " height="276" src="image/B17298_06_021.jpg" width="725"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.21 – A log loss graph where y = 1</p>
<p>You will notice that the log loss value exponentially rises as the predicted probability diverges away from the actual value. The lesser the divergence, the less the increase in log loss will be. This is what makes log loss a good comparison metric, as it compares not only which model is good or bad but also how good or bad it is.</p>
<p>Similarly, if you wanted<a id="_idIndexMarker708"/> to use the not obese class as a reference<a id="_idIndexMarker709"/> for log loss, then you would inverse the prediction probabilities, calculate the log loss values, and plot the graph, or you could just calculate the log loss value by setting <em class="italic">y</em> to 0 and use the log loss values calculated to plot the log loss graph. This graph will be a mirror image of the previous graph (<em class="italic">see Figure 6.21</em>):</p>
<div>
<div class="IMG---Figure" id="_idContainer199">
<img alt="Figure 6.22 – A log loss graph where y = 0 " height="410" src="image/B17298_06_022.jpg" width="962"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.22 – A log loss graph where y = 0</p>
<p>The log loss value<a id="_idIndexMarker710"/> of the model, also called the <strong class="bold">skill</strong> of the model, is the average<a id="_idIndexMarker711"/> of the log loss values for all the records in the dataset. Accordingly, the equation for the log loss of the model is as follows:</p>
<p class="IMG---Figure"><img alt="" height="151" src="image/Formula_B17298_06_008.png" width="1001"/></p>
<p>Here, the equation can be broken down as follows:</p>
<ul>
<li><strong class="bold">n</strong> is the total number of records in the dataset. </li>
<li><strong class="bold">y</strong> is the actual classification value, that is, <em class="italic">0</em> or <em class="italic">1</em>.</li>
<li><strong class="bold">p</strong> is the prediction probability. </li>
<li><strong class="bold">log</strong> is the natural logarithm of the number.</li>
</ul>
<p>In an ideal world, a model with perfect scoring capabilities and skills is said to have a log loss equal to <em class="italic">0</em>. To correctly apply log loss to compare models, both models must be trained using the same dataset.</p>
<p>Congratulations! We have just covered<a id="_idIndexMarker712"/> how log loss is statistically <a id="_idIndexMarker713"/>calculated. In the next section, we shall explore some other important metrics that are not a part of the H2O AutoML leaderboard but are nonetheless important in terms of understanding model performance.</p>
<h1 id="_idParaDest-105"><a id="_idTextAnchor137"/>Exploring other model performance metrics</h1>
<p>The H2O AutoML leaderboard summarizes<a id="_idIndexMarker714"/> the model performances based on certain commonly used important metrics. However, there are still plenty of performance metrics in the field of ML that describe different skills of the ML model. These skills can often be the deciding factor in what works best for your given ML problem and hence, it is important that we are aware of how we can use these different metrics. H2O also provides us with these metrics values by computing them once training is finished and storing them as the model’s metadata. You can easily access them using built-in functions.</p>
<p>In the following subsections, we shall explore some of the other important model performance metrics, starting with F1.</p>
<h2 id="_idParaDest-106"><a id="_idTextAnchor138"/>Understanding the F1 score performance metric</h2>
<p>Precision and recall, despite being<a id="_idIndexMarker715"/> very good metrics<a id="_idIndexMarker716"/> to measure a classification model’s performance, have a trade-off. Precision and recall cannot both have high values at the same time. If you increase the precision by adjusting your classification threshold, then it impacts your recall, as the number of false negatives might increase, reducing your recall value, and vice versa.</p>
<p>The precision metric works to minimize incorrect predictions, while the recall metric works to find the greatest number of positive predictions. So, technically, we need to find the right balance between these two metrics.</p>
<p>This is where the <strong class="bold">F1 score</strong> performance metric comes into the picture. The F1 score is a metric that tries to maximize both precision and recall at the same time and gives an overall score for the model’s performance. </p>
<p>The F1 score is the harmonic mean<a id="_idIndexMarker717"/> of the precision and recall values. <strong class="bold">The harmonic mean</strong> is just one of the variations for calculating the mean of values. With the harmonic mean, we calculate the reciprocal<a id="_idIndexMarker718"/> of the arithmetic mean of the reciprocals of all the observations. The reason we use a harmonic mean for calculating the F1 score is that using a general arithmetic mean would lead to the equation giving equal importance to all degrees of error. The harmonic mean, on the other hand, punishes high values of error by lowering the F1 score accordingly. This is the reason why the harmonic mean is used to generate the F1 score, as the score value calculated gives a better representation of the model’s performance.</p>
<p>The F1 score ranges from 0 to 1, where 1 indicates that the model has perfect precision and recall values, while 0 indicates that either the precision or recall value is 0.</p>
<p>The equation for calculating the F1 score is as follows:</p>
<p class="IMG---Figure"><img alt="" height="120" src="image/Formula_B17298_06_009.png" width="520"/></p>
<p>Let’s take the example of a confusion matrix as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer202">
<img alt="Figure 6.23 – An example confusion matrix with precision and recall values " height="262" src="image/B17298_06_023.jpg" width="989"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.23 – An example confusion matrix with precision and recall values</p>
<p>Let’s calculate the precision<a id="_idIndexMarker719"/> value for the matrix:</p>
<p class="IMG---Figure"><img alt="" height="94" src="image/Formula_B17298_06_010.png" width="1131"/></p>
<p>Similarly, lets now calculate the recall value of the matrix:</p>
<p class="IMG---Figure"><img alt="" height="123" src="image/Formula_B17298_06_011.png" width="1070"/></p>
<p>Now, plugging the precision<a id="_idIndexMarker720"/> and recall values into the F1 score<a id="_idIndexMarker721"/> equation, we get the following:</p>
<p class="IMG---Figure"><img alt="" height="92" src="image/Formula_B17298_06_012.png" width="1051"/></p>
<p>We got an F1 score of <em class="italic">0.15</em>. You can now compare the performance of another model by similarly calculating its F1 score and comparing it to this one. If the F1 score of the new model is greater than <em class="italic">0.15</em>, then that model is more performant than this one.</p>
<p>The benefit of the F1 score is that when comparing classification models, you don’t need to balance the precision and recall values of multiple models and make a decision based on the comparisons between two contrasting metrics. The F1 score summarizes the optimum values of precision and recall, making it easier to identify which model is better.</p>
<p>Despite being a good metric, the F1 score still has certain drawbacks. Firstly, the F1 score does not consider true negatives when calculating the score. Secondly, the F1 Score does not adequately capture the performance of a multi-class classification problem. You can technically calculate the F1 score for multi-class classification problems using macro-averaging – however, there are better metrics that can be used instead.</p>
<p>Let’s look at one such metric<a id="_idIndexMarker722"/> that overcomes the drawbacks<a id="_idIndexMarker723"/> of the F1 score, which is the <a id="_idTextAnchor139"/>absolute Matthews correlation coefficient.</p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor140"/>Calculating the absolute Matthews correlation coefficient</h2>
<p>Consider an example<a id="_idIndexMarker724"/> where we are trying to predict, based<a id="_idIndexMarker725"/> on a given fruit’s size, whether it is a grape or a watermelon. We have 200 samples, out of which 180 are grapes and 20 are watermelon. Pretty simple, yes – the bigger the size, the more likely it is to be a watermelon, while a smaller size indicates it is a grape. Assume that we trained a classifier taking the grape as the positive class. This classifier was able to classify the fruits as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer206">
<img alt="Figure 6.24 – A fruit classification confusion matrix with the grape as the positive class " height="283" src="image/B17298_06_024.jpg" width="772"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.24 – A fruit classification confusion matrix with the grape as the positive class</p>
<p>Let’s quickly calculate the scalar classification metrics that we previously covered.</p>
<p>The accuracy of the classifier will be as follows:</p>
<p class="IMG---Figure"><img alt="" height="88" src="image/Formula_B17298_06_013.png" width="1005"/></p>
<p>The precision of the classifier<a id="_idIndexMarker726"/> will be as follows:</p>
<p class="IMG---Figure"><img alt="" height="114" src="image/Formula_B17298_06_014.png" width="1131"/></p>
<p>The recall of the classifier will be as follows:</p>
<p class="IMG---Figure"><img alt="" height="130" src="image/Formula_B17298_06_015.png" width="1167"/></p>
<p>The F1 score of the classifier will be as follows:</p>
<p class="IMG---Figure"><img alt="" height="134" src="image/Formula_B17298_06_016.png" width="1069"/></p>
<p>Based on these metric values, it seems<a id="_idIndexMarker727"/> that our classifier is performing<a id="_idIndexMarker728"/> really well when making a prediction for grapes. So, what if we want to predict watermelons instead? Let’s change the positive class to watermelons instead of grapes. The confusion matrix for this scenario will be as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer211">
<img alt="Figure 6.25 – A fruit classification confusion matrix with watermelon as the positive class " height="263" src="image/B17298_06_025.jpg" width="1003"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.25 – A fruit classification confusion matrix with watermelon as the positive class</p>
<p>We’ll quickly calculate the scalar classification metrics that we previously covered.</p>
<p>The accuracy of the classifier will be as follows:</p>
<p class="IMG---Figure"><img alt="" height="114" src="image/Formula_B17298_06_017.png" width="1067"/></p>
<p>The precision of the classifier will be as follows:</p>
<p class="IMG---Figure"><img alt="" height="104" src="image/Formula_B17298_06_018.png" width="1128"/></p>
<p>The recall of the classifier will be as follows:</p>
<p class="IMG---Figure"><img alt="" height="107" src="image/Formula_B17298_06_019.png" width="1120"/></p>
<p>The F1 score of the classifier will be as follows:</p>
<p class="IMG---Figure"><img alt="" height="103" src="image/Formula_B17298_06_020.png" width="1016"/></p>
<p>As we can see from the metric<a id="_idIndexMarker729"/> values, accuracy has remained<a id="_idIndexMarker730"/> the same but precision, recall, and the F1 score have drastically gone down. Accuracy, precision, recall, and the F1 score, despite being very good metrics for measuring classification performance, have some drawbacks when there is a class imbalance in the dataset. In our grape and watermelon dataset, we only had 20 samples of watermelon in the dataset but 180 samples of grape. This imbalance in data can cause asymmetry in the metric calculation, which can be misleading. </p>
<p>Ideally, as data scientists and engineers, it is often advisable to keep the data as symmetrical as possible to keep the measurements of these metrics as relevant as possible. However, in a real-world dataset with millions of records, it will be difficult to maintain this symmetry. So, it would be beneficial to have some sort of metric that treats both the positive and negative class as equal and gives an overall picture of the classification model’s performance.</p>
<p>This is where the <strong class="bold">absolute Matthews Correlation Coefficient</strong> (<strong class="bold">MCC</strong>), also called the <strong class="bold">phi coefficient</strong>, comes into play. The equation<a id="_idIndexMarker731"/> for the MCC is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer216">
<img alt="" height="110" src="image/Formula_B17298_06_021.jpg" width="1402"/>
</div>
</div>
<p>During computation, it treats the actual class and the predicted class as two different variables and identifies the correlation coefficient between them. The correlation coefficient is nothing but a numerical value that represents some statistical relationship between the variables. The higher this correlation coefficient value, the better your classification model is.</p>
<p>The MCC values range from -1 to 1. 1 indicates that the classifier is perfect and will always classify the records correctly. A MCC of 0 indicates that there is no correlation between the classes and the prediction from the model is completely random. -1 indicates that the classifier will always incorrectly classify the records.</p>
<p>A classifier with a MCC value of -1 does not mean the model is bad in any sense. It only indicates that the correlation coefficient between the predicted and the actual class is negative. So, if you just reverse<a id="_idIndexMarker732"/> the predictions of the classifier, you will always get the correct classification<a id="_idIndexMarker733"/> prediction. Also, the MCC is perfectly symmetrical – thus, it treats all the classes equally to provide a metric that considers the overall performance of the model. Switching the positive and negative classes does not affect the MCC value. Thus, if you just take the absolute value of the MCC, it still does not lose the value’s relevance. H2O often uses the absolute value of MCC for an easier understanding of the model’s performance.</p>
<p>Let’s calculate the MCC value of the fruit classification confusion matrix with grape as the positive class:</p>
<p class="IMG---Figure"><img alt="" height="140" src="image/Formula_B17298_06_022.png" width="1277"/></p>
<p>Similarly, let’s calculate the MCC value of the fruit classification confusion matrix with watermelon as the positive class:</p>
<p class="IMG---Figure"><img alt="" height="132" src="image/Formula_B17298_06_023.png" width="1206"/></p>
<p>As you can see, the MCC value remains the same, at <em class="italic">0.277</em>, even if we switch the positive and negative classes. Also, A MCC of <em class="italic">0.277</em> indicates that the predicted class and the actual class are weakly correlated, which is correct considering the classifier was bad at classifying watermelons.</p>
<p>Congratulations, you now<a id="_idIndexMarker734"/> understand another important<a id="_idIndexMarker735"/> metric called the absolute MCC.</p>
<p>Let’s now move to the next performance metric, which is R2.</p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor141"/>Measuring the R2 performance metric</h2>
<p><strong class="bold">R2</strong>, also called the coefficient of determination, is a regression<a id="_idIndexMarker736"/> model performance<a id="_idIndexMarker737"/> metric that aims to explain<a id="_idIndexMarker738"/> the relationship between the dependent variable and the independent variable in terms of how much of a change in the independent variable affects the dependent variable.</p>
<p>The value of R2 ranges from 0 to 1, where 0 indicates that the regression line is not correctly capturing the trend in the data and 1 indicates that the regression line perfectly captures the trend in the data.</p>
<p>Let’s better understand this metric using a graphical example of a dataset. Refer to the below image for the height-to-weight regression graph:</p>
<div>
<div class="IMG---Figure" id="_idContainer219">
<img alt="Figure 6.26 – A height-to-weight regression graph " height="301" src="image/B17298_06_026.jpg" width="774"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.26 – A height-to-weight regression graph</p>
<p>The dataset has two columns:</p>
<ul>
<li><strong class="bold">Height</strong>: This is a numerical column that contains the height of a person in centimeters</li>
<li><strong class="bold">Weight</strong>: This is a numerical column that contains the weight of a person in kilograms</li>
</ul>
<p>Using this dataset, we are trying to predict someone’s weight based on their height.</p>
<p>So firstly, let’s use the average<a id="_idIndexMarker739"/> of all the weights as a general regression line to predict the weights. Technically, it does<a id="_idIndexMarker740"/> make sense, as the majority of people will have an average range of weight for a grown adult body – even though there might be some errors, it is still a plausible way of predicting a person’s weight.</p>
<p>If we plot this dataset on a graph, the mean value used for prediction will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer220">
<img alt="Figure 6.27 – The height-to-weight regression graph with a mean line " height="291" src="image/B17298_06_027.jpg" width="909"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.27 – The height-to-weight regression graph with a mean line</p>
<p>As you can see, there is definitely<a id="_idIndexMarker741"/> some error between the predicted values<a id="_idIndexMarker742"/> of the weight, which is the mean, and the actual values. As mentioned previously, this kind of error is called a residual. Calculating the square<a id="_idIndexMarker743"/> of the residuals gives us the squared error. The sum of these squared errors of all the records gives us the variation around the mean line.</p>
<p>Now let’s perform linear regression and fit a line through the data so that we get another regression line. This regression line should ideally be a better predictor than using the mean value alone. The regression line on the graph should look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer221">
<img alt="Figure 6.28 – The height-to-weight regression dataset with a regression line " height="413" src="image/B17298_06_028.jpg" width="942"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.28 – The height-to-weight regression dataset with a regression line</p>
<p>Let’s calculate the residual square<a id="_idIndexMarker744"/> of errors for this line too – this gives<a id="_idIndexMarker745"/> us the variation around the regression line.</p>
<p>Now, we need to figure out a way to identify which line is better, regression or mean, and by how much. This is where R2 can be used to compare the two regression lines. The equation for calculating R2 is as follows:</p>
<p class="IMG---Figure"><img alt="" height="123" src="image/Formula_B17298_06_024.png" width="1100"/></p>
<p>Let’s assume the sum of squares of residuals around the regression line is <em class="italic">7</em> and the sum of squares of residuals around the mean line is <em class="italic">56</em>. Plugging these values into the R2 equation, we get the following value:</p>
<p class="IMG---Figure"><img alt="" height="105" src="image/Formula_B17298_06_025.png" width="347"/></p>
<p>The value <em class="italic">0.875</em> is a percentage. This value explains that 87.5 percent of the total variation in the values of <em class="italic">y</em> is described by the variations in values of <em class="italic">x</em>. The remaining 12.5 percent may be because of some other factors in the dataset such as muscle mass, fat content, or any other factor. </p>
<p>From an ML perspective, a higher value of R2 indicates that the relationship between the two variables explains the variations in the data and as such, the linear model has captured the pattern of the dataset accurately. A lower R2 value indicates that the linear model has not fully captured the pattern of the dataset and there must be some other factors that contribute to the dataset’s pattern.</p>
<p>This sums up how the R2 metric<a id="_idIndexMarker746"/> can be used to measure<a id="_idIndexMarker747"/> to what degree the linear model is correctly capturing the trends in the data.</p>
<h1 id="_idParaDest-109"><a id="_idTextAnchor142"/>Summary</h1>
<p>In this chapter, we focused on understanding how we can measure the performance of our ML models and how we can choose one model over the other depending on which is more performant. We started by exploring the H2O AutoML leaderboard metrics since they are the most readily available metrics that AutoML provides out of the box. We first covered what the MSE and the RMSE are, what the difference between them is, and how they are calculated. We then covered what a confusion matrix is and how we calculate accuracy, sensitivity, specificity, precision, and recall from the values in the confusion matrix. With our new understanding of sensitivity and specificity, we understood what a ROC curve and its AUC are, and how they can be used to visually measure the performance of different algorithms, as well as the performance of different models of the same algorithms trained on different thresholds. Building on the ROC-AUC metric, we explored the PR curve, its AUC, and how it overcomes the drawbacks faced by the ROC-AUC metric. And finally, within the leaderboard, we understood what log loss is and how we can use it to measure the performance of binary classification models.</p>
<p>We then explored some important metrics outside of the realm of the leaderboard, starting with the F1 score. We understood how the F1 score incorporates both recall and precision into a single metric. We then understood the MCC and how it overcomes the drawbacks of precision, recall, and the F1 score when measured against imbalanced datasets. And finally, we explored the R2 metrics, which explain the relationship between the dependent variable and the independent variable in terms of how much of a change in the independent variable affects the dependent variable.</p>
<p>With this information in mind, we are now capable of correctly measuring and comparing models to find the most performant model to solve our ML problems. In the next chapter, we shall explore more about the various model explainability features that H2O provides, which give advanced details about a model and its features.</p>
</div>
</div></body></html>