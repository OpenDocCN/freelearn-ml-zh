- en: Association Rule Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Association rule learning, or association rule mining, is a relatively modern
    unsupervised learning technique originally used to discover associations between
    purchased items in grocery stores. The goal of association rule mining is to discover
    interesting relationships between sets of items, for instance, discovering that
    shoppers preparing for a hurricane often buy Pop-Tarts along with their bottled
    water, batteries, and flashlights.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml), *Classification
    Algorithms*, we introduced the concept of conditional probability. In this chapter,
    we're going to take the concept a bit further and apply conditional probability
    to association rule learning. Recall that conditional probability asks (and answers)
    the question: given that we know something, what's the probability of something
    else happening? Or, what's the probability that someone will buy Pop-Tarts given
    that they also bought bottled water and batteries? The probability is high, as
    we will shortly see.
  prefs: []
  type: TYPE_NORMAL
- en: In association rule learning, our goal is to look at a database of transactions
    or events and relate the most common subsets to each other through probabilities.
    This may be easier to understand with an example. Imagine you run an e-commerce
    store, and your task is to create a personalized widget on the homepage suggesting
    products to the shopper. You have the full database of their order history available
    to you, and you must use the shopper's browsing history to suggest items that
    have a high probability of being purchased by them.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, there are several ways to solve this problem. There's no reason you
    can't train a neural network on the entire order history of your store to suggest
    new products—except for time and complexity. Training the neural network on millions
    of transactions is both time-consuming and very difficult to inspect and understand
    intuitively. Association rule learning, on the other hand, gives us a simple and
    quick tool that's grounded in basic probability concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say your e-commerce store is a drop-shipping business that sells boutique,
    curated home decorations, and furniture. Your goal is to determine the sets of
    items that are most frequently bought together, for instance: 90% of people who
    bought the recliner chair and end table also bought the ottoman, and 80% of people
    who bought the giant wall clock also bought the drywall-mounting anchor set.'
  prefs: []
  type: TYPE_NORMAL
- en: If you have a fast and efficient way to search through millions of previous
    orders to find these relationships, you can compare the current shopper's browsing
    history to other shoppers' purchase histories and display the items that the shopper
    is most likely to purchase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Association rule learning is not limited to e-commerce. Another obvious application
    is physical stores, such as your local supermarket. If 90% of shoppers who buy
    milk and eggs also buy bread, it might be good to keep the bread close by so your
    shoppers can find it more easily. Alternatively, you might want to put bread on
    the *opposite* side of the store, because you know the shopper is going to have
    to walk through a bunch of aisles and probably pick up some more items along the
    way. How you use this data is up to you, and depends on what you want to optimize
    for: shopper convenience or total shopping basket value.'
  prefs: []
  type: TYPE_NORMAL
- en: At first blush, it seems like this would be an easy algorithm to write—we're
    just counting probabilities, after all. However, with a large database and a large
    number of possible items to select from, it becomes very time-consuming to check
    every combination of items for their frequencies, and therefore we need something
    a little more involved than the brute-force, exhaustive-search approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: Association rule learning from a mathematical perspective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A description of the Apriori algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various applications of association rule learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worked examples of various association rule algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started by looking at association rule learning from a mathematical
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Association rule learning assumes that you have a *transactional database* to
    learn from. This doesn't refer to any specific technology, but rather the concept
    of a database that stores transactions—the database can be an array in memory,
    an Excel file, or a table in your production MySQL or PostgreSQL instance. Since
    association rule learning was developed for products in supermarkets, the original
    transactional database was a list of items bought by each individual shopper on
    a given shopping trip—essentially an archive of receipts from the checkout aisle.
    However, a transactional database can be any list of items or events that occur
    during a single session, whether that session is a shopping trip, a website visit,
    or a trip to a doctor. For the time being, we'll consider the supermarket example.
    We'll discuss other uses of the association rule in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: 'A transactional database is a database where the rows are sessions and the
    columns are *items*. Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Receipt ** | **Eggs** | **Milk** | **Bread** | **Cheese** | **Shampoo**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Yes | Yes | No | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | No | No | Yes | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | No | No | No | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Yes | Yes | Yes | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Yes | Yes | No | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: 'Such a table can be considered a transactional database. Note that we are not
    recording the quantities of each item purchased, just whether or not the item
    was purchased. This is the case for most association rule learning: both the quantities
    and the ordering of items are typically ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the information in the table, we can put together probabilities for
    the occurrences of various events. For instance, the probability that a shopper
    buys `Shampoo`, or *P(E[Shampoo]),* is 20%. The probability that a shopper buys
    both `Cheese` and `Bread` is 40%, since two of the five shoppers bought both `Cheese`
    and `Bread`.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically speaking, *milk* and *bread* is called an **itemset** and is
    typically written as `{milk, bread}`. Itemsets are similar to the concept of the
    probability *events* we introduced in [Chapter 5](8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml), *Classification
    Algorithms*, except that itemsets are specifically used for situations such as
    this, and events are a more general concept in probability.
  prefs: []
  type: TYPE_NORMAL
- en: In association rule learning, the probability that an itemset appears as part
    of a transaction is called the **support** for that itemset. Just a moment ago,
    we mentioned that the probability of someone buying both milk and bread was 40%;
    this is another way of saying that the support for the `{milk, bread}` itemset
    is 40%. Noted mathematically, we can write `supp({milk, bread}) = 40%`.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the support of an itemset does not get us all the way to association
    rule learning, however. We first need to define what an association rule is. An
    association rule has the form X -> Y, where *X* and *Y* are both itemsets. Written
    out fully, an example association rule could be `{eggs, milk} -> {cheese}`, which
    relates the buying of eggs and milk to the buying of cheese. Association rules
    almost always only have a single item on the right-hand side, though the left-hand
    side can have any number of items. The association rule, by itself, tells us nothing
    about the association; we also need to look at various metrics, such as the association's
    *confidence* and *lift*, to understand how strong the association is.
  prefs: []
  type: TYPE_NORMAL
- en: The most important metric to consider for an association rule is its *confidence*,
    which is essentially how often the rule is found to be true. The *confidence*
    also happens to be the conditional probability of *P(E[Y]|E[X])*, or the probability
    that someone buys the items in itemset `Y` given that they bought the items in
    `X`.
  prefs: []
  type: TYPE_NORMAL
- en: Using our knowledge of conditional probability from Chapter 5, *Classification
    Algorithms*, and the new concepts of *support* and *confidence* in association
    rule learning, let's write a few equivalences that will help us solidify these
    mathematical concepts.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's say that the itemset `X` is eggs and milk, or `X = {eggs, milk}`,
    and that `Y = {cheese}`.
  prefs: []
  type: TYPE_NORMAL
- en: The support of `X`, or `supp(X)`, is the same as the probability of finding
    the items in `X` in a transaction, or *P(E[X])*. In this case, eggs and milk appears
    in three out of five transactions, so its support is 60%. Similarly, the support
    of `Y` (just cheese) is 80%.
  prefs: []
  type: TYPE_NORMAL
- en: The confidence of an association rule, `X -> Y`, is defined as `conf(X -> Y)
    = supp(X ∪ Y) / supp(X)`. Another way to say this is that the confidence of a
    rule is the support of all items in the rule divided by the support of the left-hand
    side. The ∪ symbol is used in probability theory to mean *union—*basically a Boolean
    OR operation. The *union* of the `X` and `Y` itemsets is therefore any item that
    appears in either X or Y. In our case, the union is eggs, milk, and cheese.
  prefs: []
  type: TYPE_NORMAL
- en: 'If `supp(X) = P(EX)`, then `supp(X ∪ Y) = P(EX ∩ XY)`. Recall that ∩ is the
    symbol for *intersection*, or essentially a Boolean AND*.* This is one scenario
    in which the semantics of itemsets differ from the semantics of probability events—the
    *union* of two itemsets is related to the *intersection* of two events containing
    those itemsets. Despite the slightly confusing notations, what we''re getting
    at is this: this *confidence* formula is starting to look exactly like the formula
    for conditional probability, once we start translating the association rule notation
    into a standard probability notation.'
  prefs: []
  type: TYPE_NORMAL
- en: Since, in conditional probability, the *P(E[Y] | E[X]) = P(E[X] ∩ E[Y]) / P(E[X])* relation
    defines the conditional probability, and we know that `supp(X ∪ Y) = P(E[X] ∩
    E[Y])`, and we also know that *P(E[X]) = supp(X)*, we find that the confidence
    of an association rule is just its conditional probability.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to our example rule of `{eggs, milk} ⇒ {cheese}`, we find that the
    confidence of this rule is 1.0\. The union of *X* and *Y* (or `{eggs, milk, cheese}`)
    appears in three of the five transactions, and has a support of 0.6\. We divide
    that by the support of the left-hand side, or just `supp ({eggs, milk})`, which
    we also find in three of the five transactions. Dividing 0.6 by 0.6 gives us 1.0,
    which is the highest possible confidence value. Every time a shopper bought eggs
    and milk, they also bought cheese. Or, stated in terms of conditional probability,
    the probability that someone bought cheese given that they bought eggs and milk
    is 100%. Compare that to the probability of someone buying cheese, which is only
    80%. We clearly have a positive relationship between eggs, milk, and cheese.
  prefs: []
  type: TYPE_NORMAL
- en: This coincidental relationship can be further explored with a concept called
    **lift***.* Lift is defined as the support of the combined items, divided by the
    support for the left- and right-hand sides individually (that is, assuming they
    are independent). The formula is `lift(X -> Y) = supp(X ∪ Y) / ( supp(X) * supp(Y)
    )`. This formula essentially measures how dependent or independent `X` and `Y`
    are on each other. If the support of `X` and `Y` together is the same as the support
    of `X` and `Y` separately, the lift of the rule will be 1, and `X` and `Y` can
    be considered completely independent from one another. As the co-dependence of
    the two itemsets increases, the value of *lift* will increase as well. In our
    case, the support for `{eggs, milk, cheese}` is once again 0.6, the support for
    `{eggs, milk}` is 0.6, and the support for `{cheese}` is 0.8\. Combining these
    values with the lift equation gives us `lift(X -> Y) = 0.6 / (0.6 * 0.8) = 1.25`.
    This rule is said to have a lift of 25%, which indicates that there is some dependent
    relationship between `{eggs, milk}` and `{cheese}`.
  prefs: []
  type: TYPE_NORMAL
- en: There are several other metrics that researchers can use when developing association
    rules, though we will not encounter any of these in our examples. There are metrics
    such as *conviction,* *leverage,* and *collective strength*, but for the most
    part, the familiar concepts of support, confidence, and lift will be all you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take one thing away from this section, let it be this: many modern problems
    in computer science and machine learning can be solved with centuries-old probability
    theory. Association rule learning was developed in the 1990s, but the core concepts
    can be traced back hundreds of years. As we saw in [Chapter 5](8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml), *Classification
    Algorithms*, we can use probability theory to develop powerful **machine learning**
    (**ML**) algorithms, and association rule learning is another argument for honing
    your knowledge of probability theory.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now take a look at the challenges of analyzing a transactional database,
    and how an association rule algorithm might work.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithmic perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now come to the much more difficult task of identifying frequent itemsets
    in a database. Once we know which itemsets and associations we want to generate
    rules for, calculating the support and confidence of the rules is quite easy.
    The difficulty, however, lies in automatically discovering the frequent and interesting
    itemsets in a database of millions of transactions among thousands of possible
    items.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that your e-commerce store only carries 100 unique items. Obviously,
    your customers can purchase any number of items during a session. Let's say a
    shopper buys only two items—there are 4,950 different combinations of two items
    from your catalog to consider. But you also must consider shoppers who buy three
    items, of which there are 161,700 combinations to search for. If your product
    catalog contains 1,000 items, there are a whopping 166 million combinations of
    three items that you'd have to consider when searching for frequent itemsets.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, a more evolved algorithm is necessary to search the transactional database
    for frequent itemsets. Note that the frequent itemset search is only half of the
    solution; once you find frequent itemsets, you still must generate association
    rules from them. However, as the search for frequent itemsets is much more difficult
    than generating association rules, the itemset search becomes the key focus for
    most algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we''ll describe one of the original frequent itemset search
    algorithms: the Apriori algorithm. We''re doing this for educational purposes
    only; it''s unlikely you''d ever need to implement your own version of the Apriori
    algorithm, as there are newer and faster frequent itemset search algorithms available.
    However, I think it''s important to study and understand these classic algorithms,
    especially algorithms that tackle a very large search space. Most algorithms that
    search a very large space use some kind of axiomatically or heuristically justified
    trick to drastically reduce the search space, and Apriori is no different.'
  prefs: []
  type: TYPE_NORMAL
- en: The Apriori algorithm begins by scanning the database of transactions and recording
    the support (or frequency) of each individual item. The result of this is a list
    or hash table of items such as eggs = 0.6, milk = 0.6, shampoo = 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to find combinations of two items and determine their support
    (or frequency) in the database. The result of this step would be something like
    `{eggs, milk} = 0.6`, `{eggs, bread} = 0.2`, `{eggs, cheese} = 0.6`, `{eggs, shampoo}
    = 0.0`, and so on. The problem with the brute-force, exhaustive-search approach
    starts at this step. If you have 100 items in your catalog, you need to calculate
    the support for 4,950 pairs. If you have 1,000 items in your catalog, you must
    calculate the support for nearly 500,000 pairs. I don't know how many products
    Amazon ([https://www.amazon.com/](https://www.amazon.com/)) sells (the latest
    report from January 2017 states 368 million), but assuming they now have 400 million
    products, there are 8 x 10^(16) pairs of items to consider (that's eighty million
    billion pairs of items). And that's just *pairs* of items. We'd also need to look
    at every triplet of items, every quadruplet of items, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The clever trick that Apriori uses to reduce the search space is to filter the
    list of unique products by a minimum support level, or minimum frequency of interest.
    If we set the minimum support to 0.25, for instance, we find that `{shampoo}`
    doesn't make the cut, and shampoo can never be part of our frequent itemset analysis
    because it's simply not purchased frequently enough.
  prefs: []
  type: TYPE_NORMAL
- en: If shampoo, by itself, is not purchased frequently enough to be considered frequent,
    it also follows that any pair of items containing shampoo will *also* not be frequent
    enough for consideration. If shampoo appears in 20% of purchases, then the `{eggs,
    shampoo}` pair must appear *less* frequently than (or equal to) 20% of purchases.
    We cannot only eliminate shampoo from our search, we can also eliminate *any*
    set that contains shampoo from consideration. If shampoo by itself is infrequent
    enough that we can ignore it, then `{eggs, shampoo}`, `{bread, shampoo}`, and
    `{eggs, bread, shampoo}` will all also be so infrequent that we can ignore them.
    This cuts down on our search space *drastically*.
  prefs: []
  type: TYPE_NORMAL
- en: We can take this approach a step further as we inspect larger combinations of
    items. In our example, `{eggs}` has a support of 60% and `{bread}` has a support
    of 40%. If we've set our minimum support to 25%, both of these items individually
    make the cut and should be considered in our frequent dataset analysis. However,
    the combination of `{eggs, bread}` has a support of only 20% and can be discarded.
    In the same way we were able to eliminate any combination containing `{shampoo}`
    from our second-degree search, we can now eliminate any combination containing
    `{eggs, bread}` from our third-degree search. Because eggs and bread together
    is rare, any combination of three or more items that contains both eggs and bread
    must also be rare. We can therefore eliminate combinations such as `{eggs, bread,
    cheese}`, `{eggs, bread, milk}`, and `{eggs, bread, shampoo}` from consideration,
    as they all contain the rare combination of `eggs` and `bread`.
  prefs: []
  type: TYPE_NORMAL
- en: While this approach drastically reduces the time required to find frequent itemsets,
    you should use this approach with caution as it's possible to accidentally skip
    over interesting, but somewhat rare, combinations. Most Apriori implementations
    will allow you to set both a minimum support and a minimum confidence for the
    resultant association rules. If you set the minimum support to a high value, your
    search will be faster but you may get more obvious or less interesting results;
    if you set the support lower, you're in danger of waiting for a very long time
    for the search to complete. Typically, association rules are generated after the
    frequent itemsets are found, so any minimum confidence level you set will not
    have an impact on the search time—only the minimum support variable will have
    a significant effect on search time.
  prefs: []
  type: TYPE_NORMAL
- en: It should also be noted that there are more advanced and faster algorithms for
    frequent itemset searches. In particular, we will experiment with the FP-Growth
    algorithm later in this chapter. However, the Apriori algorithm is a great starting
    point for understanding how frequent itemset searches work in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Before we implement libraries, let's take a look at a few situations in which
    association rules might be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Association rule applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The original use of association rule algorithms was for market basket analysis,
    such as the grocery store example we've been using throughout this chapter. This
    is a clear-cut application for association rule mining. Market basket analysis
    can be used both in physical stores and in e-commerce stores, and different models
    can be maintained for different days of the week, seasons, or even for specific
    rare events, such as an upcoming concert or a hurricane.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, in 2004, The New York Times (and others) reported that Walmart used
    association rule mining to figure out how to stock stores in advance of hurricanes.
    Walmart discovered that the association with the highest lift right before a hurricane
    wasn't bottled water or flashlights, but in fact strawberry Pop-Tarts. Another
    association with a high confidence was beer. I'm not too surprised about the beer,
    but the strawberry Pop-Tarts is the type of insight that you can really only get
    from ML!
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you were a data scientist at Walmart back in 2004\. It would be easy
    to look up the individual sales volumes of various products during different time
    periods. It's possible that the strawberry Pop-Tarts, being a small-ticket item,
    only showed a very minor percentage change in the relative sales volume during
    hurricane periods. That's the kind of data point you might naturally ignore as
    being insignificant. Pop-Tarts get a slight bump, so what? But if you were to
    mine the data for frequent itemsets and association rules, you might have found
    that the `{bottled water, batteries} -> {Strawberry Pop-Tarts}` rule appeared
    with an unusually strong confidence, and a lift of around 8.0 (a very high value
    for lift) in the days before a hurricane. Outside of hurricane season, this association
    may have been nonexistent or too weak to make the cut. But when a hurricane is
    about to hit, strawberry Pop-Tarts become a necessary hurricane supply, almost
    certainly due to their long shelf life and their ability to make both children
    and adults happy. Seeing this association, you'd tell stores to stock up on strawberry
    Pop-Tarts and put them right at the front of the store—next to the bottled water
    and batteries—and make a killing on Pop-Tart sales.
  prefs: []
  type: TYPE_NORMAL
- en: While this type of scenario is what association rules were designed for, you
    can apply frequent itemset mining and association rules to any transactional database.
    If you consider a website session to be a transaction, and if you can capture
    actions taken (such as *logged in*, *wishlisted item*, *downloaded case study*)
    as your items, you can apply the same algorithms and association rule mining to
    website visitor behaviors. You can develop association rules, such as `{downloaded
    case study, viewed pricing page} -> {entered credit card}`, to model your visitor
    behavior and optimize the layout and functionality of your site to encourage the
    behavior you want.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that association rules are not just valuable when they're positive.
    They're valuable when they're negative, too. Oftentimes you need cold, hard facts
    to change your mind about a stubborn belief that you previously held. Performing
    association rule mining on a dataset and *not* seeing an association you expected
    to see can be just as powerful as discovering an unexpected association. Seeing
    that the confidence for a rule you intuitively thought to be a strong association
    is actually very low, or below your cut-off, can help you let go of outdated thinking
    that might be holding you or your product back.
  prefs: []
  type: TYPE_NORMAL
- en: There are stories of association rule mining being used in many and varied fields.
    Yes, association rules can be used to maximize Pop-Tarts profits before a hurricane,
    but association rules can also be used to characterize the hurricanes themselves
    in terms of their features and power output. Even though association rule learning
    was developed for market basket analysis, its foundation in conditional probability
    makes it applicable to nearly any statistical system that can be represented by
    items and transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for instance, medical diagnoses. If each diagnosis by a doctor is
    considered a transaction, and every medical condition or environmental factor
    an item, we can apply association rule mining to find surprising associations
    between pre-existing conditions, environmental factors, and new diagnoses. You
    might find that the `{poor air quality, poor diet} -> {asthma}` rule has a high
    confidence or lift, and that can inform the way researchers and doctors treat
    asthma, perhaps by taking a closer look at diet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Association rules can be used in many other fields, such as genetics, bioinformatics,
    and IT security. Because these approaches can be used so broadly, it''s often
    difficult to recognize when association rules should be applied. A good rule of
    thumb to use is this: if your dataset contains transactions, or if you can see
    yourself calculating conditional probabilities for many combinations of events,
    you may want to consider association rule mining.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at a couple of JavaScript libraries for association rule mining.
  prefs: []
  type: TYPE_NORMAL
- en: Example – retail data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we''ll use the Apriori algorithm to analyze a retail dataset.
    Start by creating a new folder for this project called `Ch6-Apriori`, and add
    the following `package.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After adding the `package.json` file, run `yarn install` from the command line
    to install the dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Next, create a `src` directory and download the required data file from this
    book's GitHub repository, `retail-data.json`, into the folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now add an `index.js` file to the `src` folder and add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code imports the data and the Apriori library. It then initializes
    a new Apriori solver with a minimum support of `0.02` (2%) and a minimum rule
    confidence of 90%. We're also only analyzing the first 1,000 receipts in the dataset;
    the Apriori algorithm is a bit slow by nature, so you'll likely want to limit
    the dataset as you experiment initially.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the program with `yarn start` and you should see output similar to the
    following. The output will be longer than what I show here; take a minute to explore
    your own console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: These association rules all have a confidence of 1.0, which means that the right-hand
    side (labeled `rhs`) appeared in a transaction 100% of the times that the left-hand
    side appeared.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll down through the results a little more and you might find this rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This rule essentially tells us that when a shopper buys the babushka and red
    retrospot design hand warmers, they have a 91% likelihood of also buying the bird
    design hand warmer. Have you ever wondered why, when shopping on Amazon, you often
    see suggestions similar to items you've just bought or added to your cart? This
    is why—apparently shoppers buy groups of similar items often enough that the association
    rule passes the various thresholds it needs to pass, despite the fact that the
    *average* shopper has no need for three differently designed hand warmers. But
    catering to the average shopper is not always the goal; you want to cater to the
    shopper who's going to spend more money, and you can find that shopper with statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment a little with the Apriori settings. What happens if you decrease
    the minimum confidence? What happens if you increase the minimum support?
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the minimum support the same while decreasing the minimum confidence
    should give you more association rule results with no real impact on execution
    time. Most of the execution time is spent discovering frequent itemsets, where
    confidence is not yet a defined parameter; confidence only comes into play when
    composing rules, and does not affect individual itemsets.
  prefs: []
  type: TYPE_NORMAL
- en: Raising the minimum support will speed up the algorithm, however, you will find
    that you get less interesting results. As you raise the minimum support, you'll
    find that the left-hand side of the rules become simpler. Where you used to see
    rules with three and four items on the left-hand side, you'll now start seeing
    simpler left-hand itemsets, with only one or maybe two items. Itemsets with more
    than one item naturally tend toward having lower support values, so as you raise
    the minimum support, you will end up with simpler associations.
  prefs: []
  type: TYPE_NORMAL
- en: Lowering the minimum support, on the other hand, will drastically increase execution
    time but also yield more interesting results. Note that it's possible to have
    rules with generally low support but very high confidence; these are rules that
    hold to be true often, but are rare in occurrence. As you lower the minimum support,
    you will find that the new rules that appear are evenly spread across a range
    of confidence values.
  prefs: []
  type: TYPE_NORMAL
- en: Also try increasing the limit given to `receipts.slice`. Not only will the program
    become slower, but you'll also have *fewer* rules in the output if you keep the
    minimum support parameter constant. The reason for this is that the support value
    depends on the size of the dataset. An itemset that appeared in 2% of 1,000 transactions
    *might* only appear in 1% of 2,000 transactions, depending on the distribution
    of items. If you have a very large selection of items, or if your distribution
    of items is exponentially decaying (that is, the *long-tail distribution*), you
    will find that you need to scale the minimum support value as you scale the number
    of items considered.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate this, I started with a minimum support of 0.02, a minimum confidence
    of 0.9, and a limit of 1,000 items from the receipts variable. With these parameters,
    the Apriori algorithm found 67 association rules. When I update the limit from
    1,000 to 2,000, the algorithm finds zero rules. The frequent itemsets in the first
    1,000 transactions are different enough from the itemsets in the second 1,000
    transactions that most itemsets' support values were reduced when I increased
    the limit.
  prefs: []
  type: TYPE_NORMAL
- en: In order to find more results, I must decrease the minimum support. I first
    tried setting a minimum support of 0.01, however, I had to cancel that attempt
    after two hours of waiting for the program to complete. I tried again at 0.015\.
    This time, the program finished in 70 seconds and gave me 12 results. There must
    be some point between 0.010 and 0.015 where the number of itemsets dramatically
    increases—and indeed, the program found 584 rules with a minimum support of 0.0125.
  prefs: []
  type: TYPE_NORMAL
- en: The support of an itemset is simply its frequency among all transactions. We
    can reframe everything related to support in terms of frequency. If we're considering
    2,000 transactions, a support of 0.0125 corresponds to 25 occurrences. Put another
    way, the list of 584 rules I just generated only includes items that were purchased
    at least 25 times in my 2,000-transaction dataset. In order to generate rules
    for products that were only purchased, say, 5 or more times, I'd need to set a
    minimum support of 0.0025—a value I'm pretty sure would set my laptop on fire.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the need for an algorithm more refined than Apriori becomes apparent.
    Unfortunately, the JavaScript ecosystem is still lacking in this department. Another
    popular frequent itemset mining algorithm, ECLAT, seems not to have any JavaScript
    implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another frequent itemset mining algorithm available to us: the FP-Growth
    algorithm. This algorithm should be able to handle our task quite readily, however,
    the library available to us only does the frequent itemset search and does not
    generate association rules. It is much easier to generate association rules once
    the frequent itemsets have been discovered, however, I will leave this exercise
    up to the reader. For now, let''s take a look at the FP-Growth library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `index.js` file, you may comment out the existing lines related to the
    Apriori solver and add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The FP-Growth implementation does not generate association rules, therefore
    the only parameter it takes is the minimum support value. In this example, we
    are not truncating the `receipts` transaction database, since the algorithm should
    be able to handle the larger dataset. The full transaction database has approximately
    26,000 records, so a minimum support of `0.01` corresponds to products that were
    purchased `260` times or more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run `yarn start` from the command line and you should see output similar to
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the value for support is given as an absolute value, that is, the
    number of times the items were found in the database. While these are only frequent
    itemsets and not association rules, they are still useful. If you see a frequent
    itemset similar to the following, you might want to show the user the rose teapot
    if they''re browsing the sugar bowl page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: While I think there is still some work to be done in terms of association rule
    learning in the JavaScript ecosystem, the Apriori and FP-Growth algorithms are
    both available and useful. The Apriori implementation in particular should be
    useful in most real-world use cases, which often contain fewer transactions and
    smaller item catalogs. While the FP-Growth implementation doesn't bother to generate
    association rules, there are still many things you can do by finding sets that
    an item frequently occurs in.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed association rule learning, or the approach of
    finding frequent sets of items in a transactional database and relating them to
    one another via probabilities. We learned that association rule learning was invented
    for market basket analysis but has applications in many fields, since the underlying
    probability theory and the concept of transactional databases are both broadly
    applicable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then discussed the mathematics of association rule learning in depth, and
    explored the canonical algorithmic approach to frequent itemset mining: the Apriori
    algorithm. We looked at other possible applications of association rule learning
    before trying out our own example on a retail dataset.'
  prefs: []
  type: TYPE_NORMAL
