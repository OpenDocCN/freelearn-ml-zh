<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer334">
<h1 class="chapter-number" id="_idParaDest-216"><a id="_idTextAnchor231"/><a id="_idTextAnchor232"/>11</h1>
<h1 id="_idParaDest-217"><a id="_idTextAnchor233"/>Machine Learning Pipelines with SageMaker Pipelines</h1>
<p>In <a href="B18638_10.xhtml#_idTextAnchor215"><em class="italic">Chapter 10</em></a>, <em class="italic">Machine Learning Pipelines with Kubeflow on Amazon EKS</em>, we used <strong class="bold">Kubeflow</strong>, <strong class="bold">Kubernetes</strong>, and <strong class="bold">Amazon EKS</strong> to build and run an end-to-end <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) pipeline. Here, we were able to automate several steps in the ML process inside a running Kubernetes cluster. If you are wondering whether we can also build ML pipelines using the different features and capabilities of <strong class="bold">SageMaker</strong>, then the quick answer to that would be <em class="italic">YES</em>! </p>
<p>In this chapter, we will use <strong class="bold">SageMaker Pipelines</strong> to build and run automated ML workflows. In addition to this, we will demonstrate how we can utilize <strong class="bold">AWS Lambda</strong> functions to deploy trained models to new (or existing) ML inference endpoints during pipeline execution.</p>
<p>That said, in this chapter, we will cover the following topics:</p>
<ul>
<li>Diving deeper into SageMaker Pipelines</li>
<li>Preparing the essential prerequisites</li>
<li>Running our first pipeline with SageMaker Pipelines</li>
<li>Creating Lambda functions for deployment</li>
<li>Testing our ML inference endpoint</li>
<li>Completing the end-to-end ML pipeline</li>
<li>Cleaning up</li>
<li>Recommended strategies and best practices</li>
</ul>
<p>After completing the hands-on solutions in this chapter, we should be equipped with the skills required to build more complex ML pipelines and workflows on AWS using the different capabilities of <strong class="bold">Amazon SageMaker</strong>!</p>
<h1 id="_idParaDest-218"><a id="_idTextAnchor234"/>Technical requirements</h1>
<p>Before we start, it is important that we have the following ready:</p>
<ul>
<li>A web browser (preferably Chrome or Firefox)</li>
<li>Access to the AWS account and the <strong class="bold">SageMaker Studio</strong> domain used in the previous chapters of this book</li>
<li>A text editor (for example, <strong class="bold">VS Code</strong>) on your local machine that we will use for storing and copying string values for later use in this chapter</li>
</ul>
<p>The Jupyter notebooks, source code, and other files used for each chapter are available in the repository at <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS</a>.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">It is recommended that you use an IAM user with limited permissions instead of the root account when running the examples in this book. If you are just starting out with using AWS, you can proceed with using the root account in the meantime.</p>
<h1 id="_idParaDest-219"><a id="_idTextAnchor235"/>Diving deeper into SageMaker Pipelines</h1>
<p>Often, data science teams start by performing ML experiments and deployments manually. Once they <a id="_idIndexMarker1415"/>need to standardize the workflow and enable <strong class="bold">automated model retraining</strong> to refresh the deployed models regularly, these <a id="_idIndexMarker1416"/>teams would then start considering the use of ML pipelines to automate a portion of their work. In <a href="B18638_06.xhtml#_idTextAnchor132"><em class="italic">Chapter 6</em></a>, <em class="italic">SageMaker Training and Debugging Solutions</em>, we learned how to use the <strong class="bold">SageMaker Python SDK</strong> to train an ML model. Generally, training an ML model with the SageMaker Python SDK involves running a few lines of code similar to what we have in the following block of code:</p>
<pre class="source-code">estimator = <strong class="bold">Estimator</strong>(...) 
estimator.<strong class="bold">set_hyperparameters</strong>(...)
estimator.<strong class="bold">fit</strong>(...)</pre>
<p><em class="italic">What if we wanted to prepare an automated ML pipeline and include this as one of the steps?</em> You would be surprised that all we need to do is add a few lines of code to convert this into a step that can be included in a pipeline! To convert this into a step using <strong class="bold">SageMaker Pipelines</strong>, we simply need to initialize a <strong class="source-inline">TrainingStep</strong> object similar to what we have in the following block of code:</p>
<pre class="source-code"><strong class="bold">step_train</strong> = <strong class="bold">TrainingStep</strong>(
    name="TrainModel",
    estimator=<strong class="bold">estimator</strong>,
    inputs=...
)</pre>
<p><em class="italic">Wow! Isn’t that amazing?</em> This would <a id="_idIndexMarker1417"/>mean that existing notebooks using the <strong class="bold">SageMaker Python SDK</strong> for manually training and deploying ML models can easily be converted into using SageMaker Pipelines using a few additional lines of code! <em class="italic">What about the other steps?</em> We have the following classes, as well:</p>
<ul>
<li><strong class="source-inline">ProcessingStep</strong> – This is <a id="_idIndexMarker1418"/>for processing data using <strong class="bold">SageMaker Processing</strong>.</li>
<li><strong class="source-inline">TuningStep</strong> – This is for <a id="_idIndexMarker1419"/>creating a hyperparameter tuning job using the <strong class="bold">Automatic Model Tuning</strong> capability of SageMaker.</li>
<li><strong class="source-inline">ModelStep</strong> – This is <a id="_idIndexMarker1420"/>for creating and registering a SageMaker model to the <strong class="bold">SageMaker Model Registry</strong>.</li>
<li><strong class="source-inline">TransformStep</strong> – This is <a id="_idIndexMarker1421"/>for running inference on a dataset using the <strong class="bold">Batch Transform</strong> capability of SageMaker.</li>
<li><strong class="source-inline">ConditionStep</strong> – This is for the conditional branching support of the execution of pipeline steps.</li>
<li><strong class="source-inline">CallbackStep</strong> – This is for incorporating custom steps not directly available or supported in SageMaker Pipelines.</li>
<li><strong class="source-inline">LambdaStep</strong> – This is <a id="_idIndexMarker1422"/>for running an <strong class="bold">AWS Lambda</strong> function.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Note that this is not an exhaustive list of steps as there are other steps that can be used for more specific <a id="_idIndexMarker1423"/>use cases. You can find the complete list of <strong class="bold">SageMaker Pipeline Steps</strong> at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.xhtml</a>.</p>
<p>In <a href="B18638_04.xhtml#_idTextAnchor079"><em class="italic">Chapter 4</em></a>, <em class="italic">Serverless Data Management on AWS</em>, we stored and queried our data inside a Redshift cluster and in an Athena table. If we need to directly query data from these data sources, we can use <strong class="bold">SageMaker Processing</strong> right away as it supports <a id="_idIndexMarker1424"/>reading directly from <strong class="bold">Amazon Athena</strong> and <strong class="bold">Amazon Redshift</strong> (along with <strong class="bold">Amazon S3</strong>). Inside the SageMaker Processing job, we can perform a variety <a id="_idIndexMarker1425"/>of data preparation and processing steps similar to the transformations <a id="_idIndexMarker1426"/>performed in <a href="B18638_05.xhtml#_idTextAnchor105"><em class="italic">Chapter 5</em></a>, <em class="italic">Pragmatic Data Processing and Analysis</em>. However, this <a id="_idIndexMarker1427"/>time, we will <a id="_idIndexMarker1428"/>be using <strong class="bold">scikit-learn</strong>, <strong class="bold">Apache Spark</strong>, <strong class="bold">Framework Processors</strong> (to run jobs using ML frameworks such as <strong class="bold">Hugging Face</strong>, <strong class="bold">MXNet</strong>, <strong class="bold">PyTorch</strong>, <strong class="bold">TensorFlow</strong>, and <strong class="bold">XGBoost</strong>), or custom processing containers instead to process and <a id="_idIndexMarker1429"/>transform our <a id="_idIndexMarker1430"/>data. Converting this processing job <a id="_idIndexMarker1431"/>into a step that <a id="_idIndexMarker1432"/>is part of an automated <a id="_idIndexMarker1433"/>pipeline is easy, as we just need to <a id="_idIndexMarker1434"/>prepare the corresponding <strong class="source-inline">ProcessingStep</strong> object, which will be added later on to the pipeline. Once the processing job completes, it stores the output files in S3, which can then be picked up and processed by a training job or an automatic model tuning job. If we need to convert this into a step, we can create a corresponding <strong class="source-inline">TrainingStep</strong> object (if we will be running a training job) or a <strong class="source-inline">TuningStep</strong> object (if we will be running an automatic model tuning job), which would then be added later to the pipeline. <em class="italic">What happens after the training (or tuning) job completes?</em> We have the option to store the resulting model inside the <strong class="bold">SageMaker Model Registry</strong> (similar to how we stored ML models in the <em class="italic">Registering models to SageMaker Model Registry</em> section in <a href="B18638_08.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Model Monitoring and Management Solutions</em>). If we want to convert this into a step, we can create the corresponding <strong class="source-inline">ModelStep</strong> object that would then be added later to the pipeline, too. Let’s refer to <em class="italic">Figure 11.1</em> to help us visualize how this all works once we’ve prepared the different steps of the pipeline:</p>
<div>
<div class="IMG---Figure" id="_idContainer315">
<img alt="Figure 11.1 – Using SageMaker Pipelines " height="713" src="image/B18638_11_001.jpg" width="1028"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Using SageMaker Pipelines</p>
<p>In <em class="italic">Figure 11.1</em>, we can see that <strong class="bold">SageMaker Pipelines</strong> provides a bridge to connect and link the different steps that are usually performed separately. Since all the steps in the workflow have <a id="_idIndexMarker1435"/>been connected using this pipeline, all we need to do is trigger a single pipeline run and all the steps will be executed sequentially. Once all the steps have been defined, we can proceed with initializing and configuring the <strong class="source-inline">Pipeline</strong> object, which maps to the ML pipeline definition:</p>
<pre class="source-code"><strong class="bold">pipeline</strong> = <strong class="bold">Pipeline</strong>(
    name=...,
    parameters=...,
    <strong class="bold">steps</strong>=[
        ..., 
        <strong class="bold">step_train</strong>,
        ...
    ],
)
# create (or update) the ML pipeline
<strong class="bold">pipeline.upsert</strong>(...)</pre>
<p>Then, to run the pipeline, all we need to do is call the <strong class="source-inline">start()</strong> method:</p>
<pre class="source-code">execution = <strong class="bold">pipeline.start()</strong></pre>
<p>Once the pipeline starts, we would have to wait for all steps to finish executing (one step at a time) or for <a id="_idIndexMarker1436"/>the pipeline to stop if an error occurs in one of the steps. To debug and troubleshoot running pipelines, we can easily navigate to the <strong class="bold">SageMaker Resources</strong> pane of <strong class="bold">SageMaker Studio</strong> and locate the corresponding pipeline resource. We should see a diagram corresponding to the pipeline execution that is similar to what we have in <em class="italic">Figure 11.2</em>.</p>
<div>
<div class="IMG---Figure" id="_idContainer316">
<img alt="Figure 11.2 – Pipeline execution " height="498" src="image/B18638_11_002.jpg" width="527"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Pipeline execution</p>
<p>Here, we can see that all steps in the pipeline have been completed successfully, and the model we trained has been registered to the SageMaker Model Registry, too. If we wish to run the pipeline again (for example, using a different input dataset), we can simply trigger another pipeline execution and pass a different pipeline parameter value that points to where the new input dataset is stored. <em class="italic">Pretty cool, huh?</em> In addition to this, we can also dive deeper into what’s happening (or what happened) in each of the steps by clicking on the corresponding rounded rectangle of the step we wish to check, and then reviewing the input parameters, the output values, the ML metric values, the hyperparameters used to train the model, and the logs generated during the execution of the step. This allows us to understand what’s happening during the execution of the pipeline and troubleshoot issues when errors are encountered in the middle of a pipeline execution.</p>
<p>So far, we’ve been <a id="_idIndexMarker1437"/>talking about a relatively simple pipeline involving three or four steps executed sequentially. Additionally, <strong class="bold">SageMaker Pipelines</strong> allows us to build more complex ML pipelines that utilize conditional steps similar to what we have in <em class="italic">Figure 11.3</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer317">
<img alt="" height="591" src="image/B18638_11_003.jpg" width="834"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – An ML pipeline with a conditional step</p>
<p>Here, using a <strong class="source-inline">ConditionStep</strong>, the pipeline checks whether an ML inference endpoint exists already (given the endpoint name) and performs one of the following steps depending on the existence of the endpoint:</p>
<ul>
<li><em class="italic">Deploy model to a new endpoint</em> – Using <strong class="source-inline">LambdaStep</strong>, which maps to an <strong class="bold">AWS Lambda</strong> function that deploys the ML model to a new ML inference endpoint if the <a id="_idIndexMarker1438"/>endpoint does not exist yet</li>
<li><em class="italic">Deploy model to an existing endpoint</em> – Using a <strong class="source-inline">LambdaStep</strong>, which maps to an <strong class="bold">AWS Lambda</strong> function that deploys the ML model to an existing ML inference endpoint if the endpoint already exists (with <strong class="bold">zero downtime deployment</strong>)</li>
</ul>
<p><em class="italic">Cool right?</em> <em class="italic">What’s cooler is that this is the pipeline we will build in this chapter!</em>  Building an ML pipeline might seem intimidating at first. However, as long as we iteratively build and test the pipeline and use the right set of tools, we should be able to come up with the ML pipeline we need to automate the manual processes. </p>
<p>Now that we have a better understanding of how <strong class="bold">SageMaker Pipelines</strong> works, let’s proceed with the hands-on portion of this chapter.</p>
<p class="callout-heading">Note</p>
<p class="callout">At this point, you might be wondering why we should use <strong class="bold">SageMaker Pipelines</strong> instead of <strong class="bold">Kubeflow</strong> and <strong class="bold">Kubernetes</strong>. One of the major differences between SageMaker Pipelines and Kubeflow is that the instances used to train ML models in SageMaker automatically <a id="_idIndexMarker1439"/>get deleted after the training step completes. This helps reduce the <a id="_idIndexMarker1440"/>overall cost since these training instances are only expected to run when models need to be trained. On the other hand, the infrastructure required by Kubeflow needs to be up and running before any of the training steps can proceed. Note that this is just one of the differences, and there are other things to consider when choosing the “right” tool for the job. Of course, there are scenarios where a data science team would choose Kubeflow instead since the members are already comfortable with the usage of Kubernetes (or they are running production Kubernetes workloads already). To help you and your team assess these tools properly, I would recommend that, first, you try building sample ML pipelines using both of these options.</p>
<h1 id="_idParaDest-220"><a id="_idTextAnchor236"/>Preparing the essential prerequisites</h1>
<p>In this section, we <a id="_idIndexMarker1441"/>will ensure that the following prerequisites are ready:</p>
<ul>
<li>The SageMaker Studio Domain execution role with the <strong class="source-inline">AWSLambda_FullAccess</strong> AWS managed permission policy attached to it – This will allow the Lambda functions to run without issues in the <em class="italic">Completing the end-to-end ML pipeline</em> section of this chapter.</li>
<li>The IAM role (<strong class="source-inline">pipeline-lambda-role</strong>) – This will be used to run the Lambda functions in the <em class="italic">Creating Lambda Functions for Deployment</em> section of this chapter.</li>
<li>The <strong class="source-inline">processing.py</strong> file – This will be used by the <strong class="bold">SageMaker Processing</strong> job to process the input data and split it into training, validation, and test sets.</li>
<li>The <strong class="source-inline">bookings.all.csv</strong> file – This will be used as the input dataset for the ML pipeline.</li>
</ul>
<p class="callout-heading">Important Note</p>
<p class="callout">In this chapter, we will create and manage our resources in the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region. Make sure that you have set the correct region before proceeding with the next steps.</p>
<p>Preparing these essential prerequisites is critical to ensure that we won’t encounter unexpected blockers while preparing and running the ML pipelines in this chapter. That said, let’s proceed with preparing the prerequisites in the next set of steps:</p>
<ol>
<li>Let’s start by navigating to the <strong class="bold">SageMaker Studio Control Panel</strong> by typing <strong class="source-inline">sagemaker studio</strong> in the search bar of the AWS Management Console, hovering over the search result box for <strong class="bold">Amazon SageMaker</strong>, and then clicking on the <strong class="bold">SageMaker Studio</strong> link under <strong class="bold">Top features</strong>.</li>
<li>On the SageMaker Studio <strong class="bold">Control Panel</strong> page, locate the <strong class="bold">Execution role</strong> section attached to the <strong class="bold">Domain</strong> box (as highlighted in <em class="italic">Figure 11.4</em>):</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer318">
<img alt="Figure 11.4 – Copying the Execution role name " height="546" src="image/B18638_11_004.jpg" width="1002"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Copying the Execution role name</p>
<p class="list-inset">Locate and copy the following values into a text editor on your local machine:</p>
<ul>
<li><strong class="bold">Execution role name</strong> – Copy the role name to a text editor on your local machine (copy the string <em class="italic">after</em> <strong class="source-inline">arn:aws:iam::&lt;ACCOUNT ID&gt;:role/service-role/</strong>). The execution role name might follow the <strong class="source-inline">AmazonSageMaker-ExecutionRole-&lt;DATETIME&gt;</strong> format similar to what we have in <em class="italic">Figure 11.4</em>. Make sure that you exclude <strong class="source-inline">arn:aws:iam::&lt;ACCOUNT ID&gt;:role/service-role/</strong> when copying the execution role name.</li>
<li><strong class="bold">Execution role ARN</strong> – Copy <a id="_idIndexMarker1442"/>the complete execution role ARN to the text editor (copy the entire ARN string, including <strong class="source-inline">arn:aws:iam::&lt;ACCOUNT ID&gt;:role/service-role/</strong>). The execution role ARN should follow the <strong class="source-inline">arn:aws:iam::&lt;ACCOUNT ID&gt;:role/service-role/AmazonSageMaker-ExecutionRole-&lt;DATETIME&gt;</strong> format.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">We will use the Execution role ARN when testing the Lambda functions in the <em class="italic">Creating Lambda functions for deployment</em> section of this chapter.</p>
<ol>
<li value="3">Navigate to the <strong class="bold">Roles</strong> page of the IAM console by typing <strong class="source-inline">iam</strong> into the search bar of the AWS Management Console, hovering over the search result box for <strong class="bold">IAM</strong>, and then clicking on the <strong class="bold">Roles</strong> link under <strong class="bold">Top features</strong>.</li>
<li>On the <strong class="bold">Roles</strong> page, search and locate the execution role by typing the execution role name (which is copied to the text editor on your local machine) into the search box (as highlighted in <em class="italic">Figure 11.5</em>):</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer319">
<img alt="Figure 11.5 – Navigating to the specific role page " height="249" src="image/B18638_11_005.jpg" width="962"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Navigating to the specific role page</p>
<p class="list-inset">This should filter the results and display a single row that is similar to what we have in <em class="italic">Figure 11.5</em>. Click on the link under the <strong class="bold">Role name</strong> column to navigate to the page where we can modify the permissions of the role. </p>
<ol>
<li value="5">Locate the <strong class="bold">Permissions policies</strong> table (inside the <strong class="bold">Permissions</strong> tab), and then click on <strong class="bold">Add permissions</strong> to open a drop-down menu of options. Select <strong class="bold">Attach policies</strong> from the list of available options. This should redirect us to the page <a id="_idIndexMarker1443"/>where we can see the <strong class="bold">Current permissions policies</strong> section and attach additional policies under <strong class="bold">Other permissions policies</strong>.</li>
<li>Locate the <strong class="source-inline">AWSLambda_FullAccess</strong> AWS managed permission policy using the search bar (under <strong class="bold">Other permissions policies</strong>). Toggle on the checkbox to select the row corresponding to the <strong class="source-inline">AWSLambda_FullAccess</strong> policy. After that, click on the <strong class="bold">Attach policies</strong> button.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">You should see the following success notification message after clicking on the <strong class="bold">Attach policies</strong> button, <strong class="bold">Policy was successfully attached to role</strong>.</p>
<ol>
<li value="7">Now, let’s create the IAM role that we will use later when creating the Lambda functions. Navigate to the <strong class="bold">Roles</strong> page (using the sidebar) and then click on the <strong class="bold">Create role</strong> button. </li>
<li>On the <strong class="bold">Select trusted entity</strong> page (<em class="italic">step 1</em>), perform the following steps: <ul><li>Under <strong class="bold">Trusted entity type</strong>, choose <strong class="bold">AWS service</strong> from the list of options available.</li>
<li>Under <strong class="bold">Use case</strong>, select <strong class="bold">Lambda</strong> under <strong class="bold">Common use cases</strong>.</li>
<li>Afterward, click on the <strong class="bold">Next</strong> button. </li>
</ul></li>
<li>In the <strong class="bold">Add permissions</strong> page (<em class="italic">step 2</em>), perform the following steps: <ul><li>Search and select the <strong class="source-inline">AmazonSageMakerFullAccess</strong> policy.</li>
<li>Search <a id="_idIndexMarker1444"/>and select the <strong class="source-inline">AWSLambdaExecute</strong> policy. </li>
<li>After toggling on the radio buttons for the two policies, click on the <strong class="bold">Next</strong> button.</li>
</ul></li>
<li>On the <strong class="bold">Name, review, and create</strong> page (<em class="italic">step 3</em>), perform the following steps:<ul><li>Specify <strong class="source-inline">pipeline-lambda-role</strong> under <strong class="bold">Role name</strong>.</li>
<li>Scroll down to the bottom of the page, and then click on the <strong class="bold">Create role</strong> button.</li>
</ul></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">You should see the following success notification message after clicking on the <strong class="bold">Create role</strong> button: <strong class="bold">Role pipeline-lambda-role created</strong>.</p>
<ol>
<li value="11">Navigate back to the <strong class="bold">SageMaker Studio Control Panel</strong> by typing <strong class="source-inline">sagemaker studio</strong> into the search bar of the AWS Management Console and then clicking on the <strong class="bold">SageMaker Studio</strong> link under <strong class="bold">Top features</strong> (after hovering over the search result box for <strong class="bold">Amazon SageMaker</strong>).</li>
<li>Click on <strong class="bold">Launch app</strong> and then select <strong class="bold">Studio</strong> from the list of drop-down options. </li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This will redirect you to <strong class="bold">SageMaker Studio</strong>. Wait for a few seconds for the interface to load. </p>
<ol>
<li value="13">Now, let’s proceed with creating the <strong class="source-inline">CH11</strong> folder where we will store the files relevant to <a id="_idIndexMarker1445"/>our ML pipeline in this chapter. Right-click on the empty space in the <strong class="bold">File Browser</strong> sidebar pane to open a context menu that is similar to what is shown in <em class="italic">Figure 11.6</em>: </li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer320">
<img alt="Figure 11.6 – Creating a new folder " height="366" src="image/B18638_11_006.jpg" width="833"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Creating a new folder</p>
<p class="list-inset">Select <strong class="bold">New Folder</strong> to create a new folder inside the current directory. Name the folder <strong class="source-inline">CH11</strong>. After that, navigate to the <strong class="source-inline">CH11</strong> directory by double-clicking on the corresponding folder name in the sidebar.</p>
<ol>
<li value="14">Create a new notebook by clicking on the <strong class="bold">File</strong> menu and choosing <strong class="bold">Notebook</strong> from the list of options under the <strong class="bold">New</strong> submenu. This should create a <strong class="source-inline">.ipynb</strong> file inside the <strong class="source-inline">CH11</strong> directory where we can run our Python code.</li>
<li>In the <strong class="bold">Set up notebook environment</strong> window, specify the following configuration values:<ul><li><strong class="bold">Image</strong>: <strong class="source-inline">Data Science</strong> (option found under Sagemaker image)</li>
<li><strong class="bold">Kernel</strong>: <strong class="source-inline">Python 3</strong></li>
<li><strong class="bold">Start-up script</strong>: <strong class="source-inline">No script</strong></li>
</ul></li>
<li>Afterward, click on the <strong class="bold">Select</strong> button.  </li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Wait for the kernel to start. This step could take around 3–5 minutes while an ML instance is being provisioned to run the Jupyter notebook cells. Make sure that you stop this instance after finishing all the hands-on solutions in this chapter (or if you’re not using it). For more information, feel free to check the <em class="italic">Cleaning up</em> section near the end of this chapter.</p>
<ol>
<li value="17">Right-click on the tab name and then select <strong class="bold">Rename Notebook…</strong> from the list of <a id="_idIndexMarker1446"/>options in the context menu. Update the name of the file to <strong class="source-inline">Machine Learning Pipelines with SageMaker Pipelines.ipynb</strong>.</li>
<li>In the first cell of the <strong class="source-inline">Machine Learning Pipelines with SageMaker Pipelines.ipynb</strong> notebook, run the following command:<pre class="source-code"><strong class="bold">!wget -O processing.py https://bit.ly/3QiGDQO</strong></pre></li>
</ol>
<p class="list-inset">This should download a <strong class="source-inline">processing.py</strong> file that does the following: </p>
<ul>
<li>Loads the <strong class="source-inline">dataset.all.csv</strong> file and stores the data inside a DataFrame</li>
<li>Performs the <strong class="bold">train-test split</strong>, which <a id="_idIndexMarker1447"/>would divide the DataFrame into three DataFrames (containing the training, validation, and test sets)</li>
<li>Ensures that the output directories have been created before saving the output CSV files</li>
<li>Saves the DataFrames containing the training, validation, and test sets into their corresponding CSV files inside the output directories</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Feel free to check the contents of the downloaded <strong class="source-inline">processing.py</strong> file. Additionally, you can find a copy of the <strong class="source-inline">processing.py</strong> script file at <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/processing.py">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/processing.py</a>.</p>
<ol>
<li value="19">Next, let’s use the <strong class="source-inline">mkdir</strong> command to create a <strong class="source-inline">tmp</strong> directory if it does not exist yet:<pre class="source-code">!<strong class="bold">mkdir -p tmp</strong></pre></li>
<li>After that, download the <strong class="source-inline">bookings.all.csv</strong> file using the <strong class="source-inline">wget</strong> command:<pre class="source-code">!<strong class="bold">wget -O tmp/bookings.all.csv https://bit.ly/3BUcMK4</strong></pre></li>
</ol>
<p class="list-inset">Here, we download a clean(er) version of the synthetic <strong class="source-inline">bookings.all.csv</strong> file similar to what we used in <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to ML Engineering on AWS</em>. However, this time, multiple data cleaning and transformation steps have been <a id="_idIndexMarker1448"/>applied already to produce a higher quality model.</p>
<ol>
<li value="21">Specify a unique S3 bucket name and prefix. Make sure that you replace the value of <strong class="source-inline">&lt;INSERT S3 BUCKET NAME HERE&gt;</strong> with a unique S3 bucket name before running the following block of code:<pre class="source-code">s3_bucket = '<strong class="bold">&lt;INSERT S3 BUCKET NAME HERE&gt;</strong>'</pre><pre class="source-code">prefix = 'pipeline'</pre></li>
</ol>
<p class="list-inset">You could use one of the S3 buckets created in the previous chapters and update the value of <strong class="source-inline">s3_bucket</strong> with the S3 bucket name. If you are planning to create and use a new S3 bucket, make sure that you update the value of <strong class="source-inline">s3_bucket</strong> with a name for a bucket that does not exist yet. After that, run the following command:</p>
<pre class="list-inset1 source-code">!<strong class="bold">aws s3 mb s3://{s3_bucket}</strong></pre>
<p class="list-inset">Note that this command should only be executed if we are planning to create a new S3 bucket.</p>
<p class="callout-heading">Note</p>
<p class="callout">Copy the S3 bucket name to the text editor on your local machine. We will use this later in the <em class="italic">Testing our ML inference endpoint</em> section of this chapter.</p>
<ol>
<li value="22">Let’s prepare the path where we will upload our CSV file:<pre class="source-code"><strong class="bold">source_path</strong> = f's3://{s3_bucket}/{prefix}' + \</pre><pre class="source-code">               '/source/<strong class="bold">dataset.all.csv</strong>'</pre></li>
<li>Finally, let’s <a id="_idIndexMarker1449"/>upload the <strong class="source-inline">bookings.all.csv</strong> file to the S3 bucket using the <strong class="source-inline">aws s3 cp</strong> command:<pre class="source-code">!aws s3 cp tmp/bookings.all.csv {source_path}</pre></li>
</ol>
<p class="list-inset">Here, the CSV file gets renamed to <strong class="source-inline">dataset.all.csv</strong> file upon uploading it to the S3 bucket (since we specified this in the <strong class="source-inline">source_path</strong> variable).</p>
<p>With the prerequisites ready, we can now proceed with running our first pipeline!</p>
<h1 id="_idParaDest-221"><a id="_idTextAnchor237"/>Running our first pipeline with SageMaker Pipelines</h1>
<p>In <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to ML Engineering on AWS</em>, we installed and used <strong class="bold">AutoGluon</strong> to train multiple ML models (with <strong class="bold">AutoML</strong>) inside an AWS Cloud9 environment. In <a id="_idIndexMarker1450"/>addition to this, we performed the <a id="_idIndexMarker1451"/>different steps of the ML process manually using a variety of tools and libraries. In this chapter, we will convert these manually executed steps into an automated pipeline so that all we need to do is provide an input dataset and the ML pipeline will do the rest of the work for us (and store the trained model in a model registry). </p>
<p class="callout-heading">Note</p>
<p class="callout">Instead of preparing a custom Docker container image to use AutoGluon for training ML models, we will use the built-in <strong class="bold">AutoGluon-Tabular</strong> algorithm instead. With a built-in algorithm available <a id="_idIndexMarker1452"/>for use, all we need to worry about would be the hyperparameter values and the additional configuration parameters we will use to configure the training job.</p>
<p>That said, this section is divided into two parts: </p>
<ul>
<li><em class="italic">Defining and preparing our first ML pipeline</em> – This is where we will define and prepare a pipeline with the following steps:<ul><li><strong class="source-inline">PrepareData</strong> – This utilizes a <strong class="bold">SageMaker Processing</strong> job to process the input dataset and splits it into training, validation, and test sets.</li>
<li><strong class="source-inline">TrainModel</strong> – This utilizes the <strong class="bold">AutoGluon-Tabular</strong> built-in algorithm to train a classification model.</li>
<li><strong class="source-inline">RegisterModel</strong> – This registers the trained ML model to the <strong class="bold">SageMaker Model Registry</strong>.</li>
</ul></li>
<li><em class="italic">Running our first ML pipeline</em> – This is where we will use the <strong class="source-inline">start()</strong> method to execute our pipeline.</li>
</ul>
<p>With these in mind, let’s start by preparing our ML pipeline. </p>
<h2 id="_idParaDest-222"><a id="_idTextAnchor238"/>Defining and preparing our first ML pipeline</h2>
<p>The first pipeline we will prepare would be a relatively simple pipeline with three steps—including <a id="_idIndexMarker1453"/>the data preparation step, the model training step, and the <a id="_idIndexMarker1454"/>model registration step. To help us visualize what our first ML pipeline using <strong class="bold">SageMaker Pipelines</strong> will look like, let’s quickly check <em class="italic">Figure 11.7</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer321">
<img alt="Figure 11.7 – Our first ML pipeline using SageMaker Pipelines " height="213" src="image/B18638_11_007.jpg" width="1182"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Our first ML pipeline using SageMaker Pipelines</p>
<p>Here, we can see that our pipeline accepts an input dataset and splits this dataset into training, validation, and test sets. Then, the training and validation sets are used to train an ML model, which then gets registered to the <strong class="bold">SageMaker Model Registry</strong>.</p>
<p>Now that we have a good idea of what our pipeline will look like, let’s run the following blocks of code in our <strong class="source-inline">Machine Learning Pipelines with SageMaker Pipelines.ipynb</strong> Jupyter notebook in the next set of steps:</p>
<ol>
<li value="1">Let’s start by importing the building blocks from <strong class="source-inline">boto3</strong> and <strong class="source-inline">sagemaker</strong>:<pre class="source-code">import <strong class="bold">boto3</strong></pre><pre class="source-code">import <strong class="bold">sagemaker</strong></pre><pre class="source-code">from sagemaker import <strong class="bold">get_execution_role</strong></pre><pre class="source-code">from sagemaker.sklearn.processing import (</pre><pre class="source-code">    <strong class="bold">SKLearnProcessor</strong></pre><pre class="source-code">)</pre><pre class="source-code">from sagemaker.workflow.steps import (</pre><pre class="source-code">    <strong class="bold">ProcessingStep</strong>, </pre><pre class="source-code">    <strong class="bold">TrainingStep</strong></pre><pre class="source-code">)</pre><pre class="source-code">from sagemaker.workflow.step_collections import (</pre><pre class="source-code">    <strong class="bold">RegisterModel</strong></pre><pre class="source-code">)</pre><pre class="source-code">from sagemaker.processing import (</pre><pre class="source-code">    <strong class="bold">ProcessingInput</strong>, </pre><pre class="source-code">    <strong class="bold">ProcessingOutput</strong></pre><pre class="source-code">)</pre><pre class="source-code">from sagemaker.workflow.parameters import (</pre><pre class="source-code">    <strong class="bold">ParameterString</strong></pre><pre class="source-code">)</pre><pre class="source-code">from sagemaker.inputs import <strong class="bold">TrainingInput</strong></pre><pre class="source-code">from sagemaker.estimator import <strong class="bold">Estimator</strong></pre><pre class="source-code">from sagemaker.workflow.pipeline import <strong class="bold">Pipeline</strong></pre></li>
<li>Store <a id="_idIndexMarker1455"/>the SageMaker <a id="_idIndexMarker1456"/>execution role ARN inside the <strong class="source-inline">role</strong> variable:<pre class="source-code">role = <strong class="bold">get_execution_role()</strong></pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">The <strong class="source-inline">get_execution_role()</strong> function should return the ARN of the IAM role we modified in the <em class="italic">Preparing the essential prerequisites</em> section of this chapter.</p>
<ol>
<li value="3">Additionally, let’s prepare the SageMaker <strong class="source-inline">Session</strong> object:<pre class="source-code">session = <strong class="bold">sagemaker.Session()</strong></pre></li>
<li>Let’s initialize a <strong class="source-inline">ParameterString</strong> object that maps to the <strong class="source-inline">Pipeline</strong> parameter pointing to where the input dataset is stored:<pre class="source-code"><strong class="bold">input_data</strong> = <strong class="bold">ParameterString</strong>(</pre><pre class="source-code">    name="RawData",</pre><pre class="source-code">    default_value=source_path, </pre><pre class="source-code">)</pre></li>
<li>Let’s <a id="_idIndexMarker1457"/>prepare the <strong class="source-inline">ProcessingInput</strong> object that contains the <a id="_idIndexMarker1458"/>configuration of the input source of the <strong class="bold">SageMaker Processing</strong> job. After that, let’s initialize the <strong class="source-inline">ProcessingOutput</strong> object that maps to the configuration for the output results of the <strong class="bold">SageMaker Processing</strong> job:<pre class="source-code"><strong class="bold">input_raw</strong> = <strong class="bold">ProcessingInput</strong>(</pre><pre class="source-code">    source=input_data,</pre><pre class="source-code">    destination='/opt/ml/processing/input/'</pre><pre class="source-code">)</pre><pre class="source-code"><strong class="bold">output_split</strong> = <strong class="bold">ProcessingOutput</strong>(</pre><pre class="source-code">    output_name="split",</pre><pre class="source-code">    source='/opt/ml/processing/output/', </pre><pre class="source-code">    destination=f's3://{s3_bucket}/{prefix}/output/'</pre><pre class="source-code">)</pre></li>
<li>Let’s initialize the <strong class="source-inline">SKLearnProcessor</strong> object along with the corresponding <strong class="source-inline">ProcessingStep</strong> object:<pre class="source-code"><strong class="bold">processor</strong> = <strong class="bold">SKLearnProcessor</strong>(</pre><pre class="source-code">    framework_version='0.20.0',</pre><pre class="source-code">    role=role,</pre><pre class="source-code">    instance_count=1,</pre><pre class="source-code">    instance_type='ml.m5.large'</pre><pre class="source-code">)</pre><pre class="source-code"><strong class="bold">step_process</strong> = <strong class="bold">ProcessingStep</strong>(</pre><pre class="source-code">    name="PrepareData",  </pre><pre class="source-code">    processor=<strong class="bold">processor</strong>,</pre><pre class="source-code">    inputs=[<strong class="bold">input_raw</strong>],</pre><pre class="source-code">    outputs=[<strong class="bold">output_split</strong>],</pre><pre class="source-code">    code="<strong class="bold">processing.py</strong>",</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">To help us visualize <a id="_idIndexMarker1459"/>how we configured the <strong class="source-inline">ProcessingStep</strong> object, let’s <a id="_idIndexMarker1460"/>quickly check <em class="italic">Figure 11.8</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer322">
<img alt="Figure 11.8 – Configuring and preparing the ProcessingStep " height="651" src="image/B18638_11_008.jpg" width="1281"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Configuring and preparing the ProcessingStep</p>
<p class="list-inset">Here, we initialized the <strong class="source-inline">ProcessingStep</strong> object using the configured <strong class="source-inline">SKLearnProcessor</strong> object along with the parameter values for the <strong class="source-inline">inputs</strong>, <strong class="source-inline">outputs</strong>, and <strong class="source-inline">code</strong> parameters.</p>
<ol>
<li value="7">Next, let’s prepare the <strong class="source-inline">model_path</strong> variable to point to where the model will be uploaded after the SageMaker training job has finished (when the ML pipeline is executed during a later step):<pre class="source-code"><strong class="bold">model_path</strong> = f"s3://{s3_bucket}/{prefix}/model/"</pre></li>
<li>Additionally, let’s <a id="_idIndexMarker1461"/>prepare the <strong class="source-inline">model_id</strong> variable to store the <a id="_idIndexMarker1462"/>ID of the ML model we’ll use:<pre class="source-code"><strong class="bold">model_id</strong> = "<strong class="bold">autogluon-classification-ensemble</strong>"</pre></li>
<li>Let’s specify the region we are using inside <strong class="source-inline">region_name</strong>:<pre class="source-code"><strong class="bold">region_name</strong> = "us-west-2"</pre></li>
<li>Use <strong class="source-inline">image_uris.retrieve()</strong> to get the ECR container image URI of our training image:<pre class="source-code">from sagemaker import image_uris</pre><pre class="source-code"><strong class="bold">train_image_uri</strong> = <strong class="bold">image_uris.retrieve</strong>(</pre><pre class="source-code">    region=region_name,</pre><pre class="source-code">    framework=None,</pre><pre class="source-code">    model_id=<strong class="bold">model_id</strong>,</pre><pre class="source-code">    model_version="*",</pre><pre class="source-code">    image_scope="<strong class="bold">training</strong>",</pre><pre class="source-code">    instance_type="ml.m5.xlarge",</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">If you are wondering what the value of <strong class="source-inline">train_image_uri</strong> is, it should have a string value equal (or similar to): <strong class="source-inline">'763104351884.dkr.ecr.us-west-2.amazonaws.com/autogluon-training:0.4.0-cpu-py38'.</strong></p>
<ol>
<li value="11">Use <strong class="source-inline">script_uris.retrieve()</strong> to get the script S3 URI associated with the model (given the values of <strong class="source-inline">model_id</strong>, <strong class="source-inline">model_version</strong>, and <strong class="source-inline">script_scope</strong>):<pre class="source-code">from sagemaker import script_uris</pre><pre class="source-code"><strong class="bold">train_source_uri</strong> = <strong class="bold">script_uris.retrieve</strong>(</pre><pre class="source-code">    model_id=<strong class="bold">model_id</strong>, </pre><pre class="source-code">    model_version="*", </pre><pre class="source-code">    script_scope="<strong class="bold">training</strong>"</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">Note that <strong class="source-inline">train_source_uri</strong> should have a string value equal (or similar) to <strong class="source-inline">'s3://jumpstart-cache-prod-us-west-2/source-directory-tarballs/autogluon/transfer_learning/classification/v1.0.1/sourcedir.tar.gz'</strong>.</p>
<p class="callout-heading">Note</p>
<p class="callout">What’s inside this <strong class="source-inline">sourcedir.tar.gz</strong> file? If the <strong class="source-inline">script_scope</strong> value used when calling <strong class="source-inline">script_uris.retrieve()</strong> is <strong class="source-inline">"training"</strong>, the <strong class="source-inline">sourcedir.tar.gz</strong> file should contain code that uses <strong class="source-inline">autogluon.tabular.TabularPredictor</strong> when training the ML model. Note that the contents of <strong class="source-inline">sourcedir.tar.gz</strong> change depending on the arguments specified when calling <strong class="source-inline">script_uris.retrieve()</strong>.</p>
<ol>
<li value="12">Use <strong class="source-inline">model_uris.retrieve()</strong> to <a id="_idIndexMarker1463"/>get the model artifact S3 URI <a id="_idIndexMarker1464"/>associated with the model (given the values of <strong class="source-inline">model_id</strong>, <strong class="source-inline">model_version</strong>, and <strong class="source-inline">model_scope</strong>):<pre class="source-code">from sagemaker import model_uris</pre><pre class="source-code"><strong class="bold">train_model_uri</strong> = <strong class="bold">model_uris.retrieve</strong>(</pre><pre class="source-code">    model_id=<strong class="bold">model_id</strong>, </pre><pre class="source-code">    model_version="*", </pre><pre class="source-code">    model_scope="<strong class="bold">training</strong>"</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">Note that <strong class="source-inline">train_model_uri</strong> should have a string value equal (or similar) to <strong class="source-inline">'s3://jumpstart-cache-prod-us-west-2/autogluon-training/train-autogluon-classification-ensemble.tar.gz'</strong>.</p>
<ol>
<li value="13">With the <a id="_idIndexMarker1465"/>values for <strong class="source-inline">train_image_uri</strong>, <strong class="source-inline">train_source_uri</strong>, <strong class="source-inline">train_model_uri</strong>, and <strong class="source-inline">model_path</strong> ready, we can now <a id="_idIndexMarker1466"/>initialize the <strong class="source-inline">Estimator</strong> object:<pre class="source-code">from sagemaker.estimator import Estimator</pre><pre class="source-code">estimator = <strong class="bold">Estimator</strong>(</pre><pre class="source-code">    image_uri=<strong class="bold">train_image_uri</strong>,</pre><pre class="source-code">    source_dir=<strong class="bold">train_source_uri</strong>,</pre><pre class="source-code">    model_uri=<strong class="bold">train_model_uri</strong>,</pre><pre class="source-code">    entry_point="transfer_learning.py",</pre><pre class="source-code">    instance_count=1,</pre><pre class="source-code">    instance_type="ml.m5.xlarge",</pre><pre class="source-code">    max_run=900,</pre><pre class="source-code">    output_path=<strong class="bold">model_path</strong>,</pre><pre class="source-code">    session=session,</pre><pre class="source-code">    role=role</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">Here, the <strong class="source-inline">entry_point</strong> value points to the <strong class="source-inline">transfer_learning.py</strong> script file stored inside <strong class="source-inline">sourcedir.tar.gz</strong> containing the relevant scripts for training the model.</p>
<ol>
<li value="14">Next, let’s use the <strong class="source-inline">retrieve_default()</strong> function to retrieve the default set of <a id="_idIndexMarker1467"/>hyperparameters for our <strong class="bold">AutoGluon</strong> classification model:<pre class="source-code">from sagemaker.hyperparameters import <strong class="bold">retrieve_default</strong></pre><pre class="source-code">hyperparameters = <strong class="bold">retrieve_default</strong>(</pre><pre class="source-code">    model_id=model_id, </pre><pre class="source-code">    model_version="*"</pre><pre class="source-code">)</pre><pre class="source-code">hyperparameters["verbosity"] = "3"</pre><pre class="source-code"><strong class="bold">estimator.set_hyperparameters(**hyperparameters)</strong> </pre></li>
<li>Prepare <a id="_idIndexMarker1468"/>the <strong class="source-inline">TrainingStep</strong> object that uses the <strong class="source-inline">Estimator</strong> object <a id="_idIndexMarker1469"/>as one of the parameter values during initialization: <pre class="source-code"><strong class="bold">s3_data</strong> = <strong class="bold">step_process</strong>         \</pre><pre class="source-code">    .properties                \</pre><pre class="source-code">    .ProcessingOutputConfig    \</pre><pre class="source-code">    .Outputs["split"]          \</pre><pre class="source-code">    .S3Output.<strong class="bold">S3Uri</strong>            \</pre><pre class="source-code"><strong class="bold">step_train</strong> = <strong class="bold">TrainingStep</strong>(</pre><pre class="source-code">    name="<strong class="bold">TrainModel</strong>",</pre><pre class="source-code">    estimator=estimator,</pre><pre class="source-code">    inputs={</pre><pre class="source-code">        "training": TrainingInput(</pre><pre class="source-code">            s3_data=<strong class="bold">s3_data</strong>,</pre><pre class="source-code">        )</pre><pre class="source-code">    },</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">Here, <strong class="source-inline">s3_data</strong> contains a <strong class="source-inline">Properties</strong> object that points to the path where the output files of the <strong class="bold">SageMaker Processing</strong> job (from the previous step of the pipeline) will be stored when the ML pipeline runs. If we inspect <strong class="source-inline">s3_data</strong> using <strong class="source-inline">s3_data.__dict__</strong>, we should get a dictionary similar to the following:</p>
<pre class="list-inset1 source-code">{'<strong class="bold">step_name</strong>': 'PrepareData',
 '<strong class="bold">path</strong>':  "ProcessingOutputConfig.Outputs['split']
           .S3Output.S3Uri",
 '<strong class="bold">_shape_names</strong>': ['S3Uri'],
 '__str__': 'S3Uri'} </pre>
<p class="list-inset">To help <a id="_idIndexMarker1470"/>us visualize how we configured the <strong class="source-inline">TrainingStep</strong> object, let’s <a id="_idIndexMarker1471"/>quickly check <em class="italic">Figure 11.9</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer323">
<img alt="Figure 11.9 – Configuring and preparing the TrainingStep object " height="649" src="image/B18638_11_009.jpg" width="1438"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – Configuring and preparing the TrainingStep object</p>
<p class="list-inset">Here, we initialize the <strong class="source-inline">TrainingStep</strong> object using the configured <strong class="source-inline">Estimator</strong> object along with the parameter values for the <strong class="source-inline">name</strong> and <strong class="source-inline">inputs</strong> parameters.</p>
<ol>
<li value="16">Now, let’s <a id="_idIndexMarker1472"/>use <strong class="source-inline">image_uris.retrieve()</strong> and <strong class="source-inline">script_uris.retrieve()</strong> to retrieve the container image URI and script URI for the <a id="_idIndexMarker1473"/>deployment of AutoGluon classification models:<pre class="source-code"><strong class="bold">deploy_image_uri</strong> = <strong class="bold">image_uris.retrieve</strong>(</pre><pre class="source-code">    region=region_name,</pre><pre class="source-code">    framework=None,</pre><pre class="source-code">    image_scope="inference",</pre><pre class="source-code">    model_id=model_id,</pre><pre class="source-code">    model_version="*",</pre><pre class="source-code">    instance_type="ml.m5.xlarge",</pre><pre class="source-code">)</pre><pre class="source-code"><strong class="bold">deploy_source_uri</strong> = <strong class="bold">script_uris.retrieve</strong>(</pre><pre class="source-code">    model_id=model_id, </pre><pre class="source-code">    model_version="*", </pre><pre class="source-code">    script_scope="<strong class="bold">inference</strong>"</pre><pre class="source-code">)</pre></li>
<li>Use the <strong class="source-inline">aws s3 cp</strong> command to download the <strong class="source-inline">sourcedir.tar.gz</strong> file to the <strong class="source-inline">tmp</strong> directory:<pre class="source-code">!<strong class="bold">aws s3 cp</strong> {deploy_source_uri} tmp/sourcedir.tar.gz</pre></li>
<li>Next, upload the <strong class="source-inline">sourcedir.tar.gz</strong> file from the <strong class="source-inline">tmp</strong> directory to your S3 bucket:<pre class="source-code"><strong class="bold">updated_source_uri</strong> = f's3://{s3_bucket}/{prefix}' + \</pre><pre class="source-code">                      '/sourcedir/sourcedir.tar.gz'</pre><pre class="source-code">!<strong class="bold">aws s3 cp</strong> tmp/sourcedir.tar.gz {<strong class="bold">updated_source_uri</strong>}</pre></li>
<li>Let’s define the <strong class="source-inline">random_string()</strong> function:<pre class="source-code">import uuid</pre><pre class="source-code">def <strong class="bold">random_string()</strong>:</pre><pre class="source-code">    return uuid.uuid4().hex.upper()[0:6]</pre></li>
</ol>
<p class="list-inset">This function should return a random alphanumeric string (with 6 characters).</p>
<ol>
<li value="20">With the <a id="_idIndexMarker1474"/>values for <strong class="source-inline">deploy_image_uri</strong>, <strong class="source-inline">updated_source_uri</strong>, and <strong class="source-inline">model_data</strong> ready, we <a id="_idIndexMarker1475"/>can now initialize the <strong class="source-inline">Model</strong> object:<pre class="source-code">from sagemaker.model import Model</pre><pre class="source-code">from sagemaker.workflow.pipeline_context import \</pre><pre class="source-code">    PipelineSession</pre><pre class="source-code">pipeline_session = PipelineSession()</pre><pre class="source-code"><strong class="bold">model_data</strong> = <strong class="bold">step_train</strong>    \</pre><pre class="source-code">    .properties            \</pre><pre class="source-code">    .<strong class="bold">ModelArtifacts</strong>        \</pre><pre class="source-code">    .<strong class="bold">S3ModelArtifacts</strong>      \</pre><pre class="source-code">model = <strong class="bold">Model</strong>(image_uri=<strong class="bold">deploy_image_uri</strong>, </pre><pre class="source-code">              source_dir=<strong class="bold">updated_source_uri</strong>,</pre><pre class="source-code">              model_data=<strong class="bold">model_data</strong>,</pre><pre class="source-code">              role=role,</pre><pre class="source-code">              entry_point=<strong class="bold">"inference.py"</strong>,</pre><pre class="source-code">              sagemaker_session=pipeline_session,</pre><pre class="source-code">              name=<strong class="bold">random_string()</strong>)</pre></li>
</ol>
<p class="list-inset">Here, we use the <strong class="source-inline">random_string()</strong> function that we defined in the previous step for the name identifier of the <strong class="source-inline">Model</strong> object.</p>
<ol>
<li value="21">Next, let’s prepare the <strong class="source-inline">ModelStep</strong> object that uses the output of <strong class="source-inline">model.register()</strong> during initialization:<pre class="source-code">from sagemaker.workflow.model_step import ModelStep</pre><pre class="source-code">model_package_group_name = "<strong class="bold">AutoGluonModelGroup</strong>"</pre><pre class="source-code"><strong class="bold">register_args</strong> = model.<strong class="bold">register</strong>(</pre><pre class="source-code">    content_types=["text/csv"],</pre><pre class="source-code">    response_types=["application/json"],</pre><pre class="source-code">    inference_instances=["ml.m5.xlarge"],</pre><pre class="source-code">    transform_instances=["ml.m5.xlarge"],</pre><pre class="source-code">    model_package_group_name=model_package_group_name,</pre><pre class="source-code">    approval_status="Approved",</pre><pre class="source-code">)</pre><pre class="source-code">step_model_create = <strong class="bold">ModelStep</strong>(</pre><pre class="source-code">    name="CreateModel",</pre><pre class="source-code">    step_args=<strong class="bold">register_args</strong></pre><pre class="source-code">)</pre></li>
<li>Now, let’s <a id="_idIndexMarker1476"/>initialize the <strong class="source-inline">Pipeline</strong> object using the different step <a id="_idIndexMarker1477"/>objects we prepared in the previous steps:<pre class="source-code">pipeline_name = f"PARTIAL-PIPELINE"</pre><pre class="source-code">partial_pipeline = <strong class="bold">Pipeline</strong>(</pre><pre class="source-code">    name=pipeline_name,</pre><pre class="source-code">    parameters=[</pre><pre class="source-code">        <strong class="bold">input_data</strong></pre><pre class="source-code">    ],</pre><pre class="source-code">    steps=[</pre><pre class="source-code">        <strong class="bold">step_process</strong>, </pre><pre class="source-code">        <strong class="bold">step_train</strong>,</pre><pre class="source-code">        <strong class="bold">step_model_create</strong>,</pre><pre class="source-code">    ],</pre><pre class="source-code">)</pre></li>
<li>Finally, let’s <a id="_idIndexMarker1478"/>use the <strong class="source-inline">upsert()</strong> method to create our ML <a id="_idIndexMarker1479"/>pipeline:<pre class="source-code"><strong class="bold">partial_pipeline.upsert(role_arn=role)</strong></pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Note that the <strong class="source-inline">upsert()</strong> method can be used to update an existing ML pipeline, too.</p>
<p>Now that our initial pipeline is ready, we can proceed with running the ML pipeline!</p>
<h2 id="_idParaDest-223"><a id="_idTextAnchor239"/>Running our first ML pipeline</h2>
<p>Once the <strong class="source-inline">Pipeline</strong> object <a id="_idIndexMarker1480"/>has been initialized and created, we can run it right away using the <strong class="source-inline">start()</strong> method, which is similar to what we have in the following line of code:</p>
<pre class="source-code">execution = <strong class="bold">partial_pipeline.start()</strong></pre>
<p>If we wish to override the default parameters of the pipeline inputs (for example, the input data used), we can specify parameter values when calling the <strong class="source-inline">start()</strong> method similar to what we have in the following block of code: </p>
<pre class="source-code">execution = <strong class="bold">partial_pipeline.start</strong>(
    parameters=dict(
        RawData="<strong class="bold">&lt;INSERT NEW SOURCE PATH&gt;</strong>",
    )
)</pre>
<p>Once the pipeline execution starts, we can then use <strong class="source-inline">execution.wait()</strong> to wait for the pipeline to finish running.</p>
<p>With this in mind, let’s run our ML pipeline in the next set of steps:</p>
<ol>
<li value="1">With everything ready, let’s run the (partial) ML pipeline using the <strong class="source-inline">start()</strong> method:<pre class="source-code">execution = <strong class="bold">partial_pipeline.start()</strong></pre><pre class="source-code">execution.<strong class="bold">describe()</strong></pre></li>
<li>Let’s use the <strong class="source-inline">wait()</strong> method to wait for the pipeline to complete before proceeding:<pre class="source-code">execution.<strong class="bold">wait()</strong></pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This should take around 10–15 minutes to complete. Feel free to grab a cup of coffee or tea while waiting!</p>
<ol>
<li value="3">Run the following <a id="_idIndexMarker1481"/>block of code to get the resulting model package ARN:<pre class="source-code">steps = execution.<strong class="bold">list_steps()</strong></pre><pre class="source-code">steps[0]['Metadata']['RegisterModel']['Arn']</pre></li>
</ol>
<p class="list-inset">This should yield an ARN with a format similar to <strong class="source-inline">arn:aws:sagemaker:us-west-2:&lt;ACCOUNT ID&gt;:model-package/autogluonmodelgroup/1</strong>. Copy this value into your text editor. We will use this model package ARN later when testing our Lambda functions in the <em class="italic">Creating Lambda functions for deployment</em> section of this chapter.</p>
<ol>
<li value="4">Locate and click on the triangle icon (<strong class="bold">SageMaker resources</strong>) near the bottom of the left-hand sidebar of SageMaker Studio (as highlighted in <em class="italic">Figure 11.10</em>): </li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer324">
<img alt="Figure 11.10 – Opening the SageMaker resources pane  " height="453" src="image/B18638_11_010.jpg" width="729"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – Opening the SageMaker resources pane </p>
<p class="list-inset">This should open the <strong class="bold">SageMaker resources</strong> pane where we can view and inspect a variety of SageMaker resources.</p>
<ol>
<li value="5">Select <strong class="bold">Pipelines</strong> from the list of options available in the drop-down menu in the <strong class="bold">SageMaker resources</strong> pane. </li>
<li>After that, double-click on the row that maps to the <strong class="source-inline">PARTIAL-PIPELINE</strong> pipeline we just <a id="_idIndexMarker1482"/>created. After that, double-click on the row that maps to the pipeline execution we triggered after calling <strong class="source-inline">partial_pipeline.start()</strong>.</li>
<li>Once the execution has finished, you should see a graph that is similar to what is shown in <em class="italic">Figure 11.11</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer325">
<img alt="Figure 11.11 – Completed pipeline execution " height="572" src="image/B18638_11_011.jpg" width="863"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Completed pipeline execution</p>
<p class="list-inset">Feel free to <a id="_idIndexMarker1483"/>click on the rounded rectangles to check the following details of each of the steps:</p>
<ul>
<li><strong class="bold">Input</strong> – The input files, parameters, and configuration</li>
<li><strong class="bold">Output</strong> – The output files and metrics (if any)</li>
<li><strong class="bold">Logs</strong> – The generated logs</li>
<li><strong class="bold">Information</strong> – Any additional information/metadata</li>
</ul>
<ol>
<li value="8">Navigate back to the tab corresponding to the <strong class="bold">Machine Learning Pipelines with SageMaker Pipelines.ipynb</strong> notebook.</li>
<li>Let’s review the steps executed during the (partial) pipeline run using the <strong class="source-inline">list_steps()</strong> method:<pre class="source-code">execution.<strong class="bold">list_steps()</strong></pre></li>
</ol>
<p class="list-inset">This should return a list of dictionaries that map to the executed steps of the pipeline.</p>
<p>We are not yet done! At this point, we have only finished half of our ML pipeline. Make sure that you <a id="_idIndexMarker1484"/>do not turn off the running apps and instances in SageMaker Studio, as we will be running more blocks of code inside the <strong class="source-inline">Machine Learning Pipelines with SageMaker Pipelines.ipynb</strong> notebook later to complete our pipeline. </p>
<p class="callout-heading">Note</p>
<p class="callout">If you need to take a break, you may turn off the running instances and apps (to manage costs), and then run all the cells again in the <strong class="source-inline">Machine Learning Pipelines with SageMaker Pipelines.ipynb</strong> notebook before working on the <em class="italic">Completing the end-to-end ML pipeline</em> section of this chapter.</p>
<h1 id="_idParaDest-224"><a id="_idTextAnchor240"/>Creating Lambda functions for deployment</h1>
<p>Our second (and more complete pipeline) will require a few additional resources to help us deploy our ML model. In this <a id="_idIndexMarker1485"/>section, we will create the following Lambda functions:</p>
<ul>
<li><strong class="source-inline">check-if-endpoint-exists</strong> – This is a Lambda function that accepts the name of the ML inference endpoint as input and returns <strong class="source-inline">True</strong> if the endpoint exists already. </li>
<li><strong class="source-inline">deploy-model-to-new-endpoint</strong> – This is a Lambda function that accepts the model package ARN as input (along with the role and the endpoint name) and deploys the model into a new inference endpoint</li>
<li><strong class="source-inline">deploy-model-to-existing-endpoint</strong> – This is a Lambda function that accepts the model package ARN as input (along with the role and the endpoint name) and deploys the model into an existing inference endpoint (by updating the deployed model inside the ML instance)</li>
</ul>
<p>We will use these functions later in the <em class="italic">Completing the end-to-end ML pipeline</em> section to deploy the ML model we will register in the SageMaker Model Registry (using <strong class="source-inline">ModelStep</strong>).</p>
<h2 id="_idParaDest-225"><a id="_idTextAnchor241"/>Preparing the Lambda function for deploying a model to a new endpoint</h2>
<p>The first <strong class="bold">AWS Lambda</strong> function <a id="_idIndexMarker1486"/>we will create will be configured and programmed to deploy a <a id="_idIndexMarker1487"/>model to a new endpoint. To help us visualize how our function will work, let’s quickly check <em class="italic">Figure 11.12</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer326">
<img alt="Figure 11.12 – Deploying a model to a new endpoint " height="634" src="image/B18638_11_012.jpg" width="1031"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – Deploying a model to a new endpoint</p>
<p>This function will accept the following input parameters: an IAM role, the endpoint name, and the model package ARN. After receiving these input parameters, the function will create the corresponding set of resources needed to deploy the model (from the model package) to a new ML inference endpoint.</p>
<p>In the next set of steps, we will create a Lambda function that we will use to deploy an ML model to a new inference endpoint:</p>
<ol>
<li value="1">Navigate to the <strong class="bold">Lambda Management console</strong> by typing <strong class="source-inline">lambda</strong> in the search bar of the AWS Management Console, and then clicking on the <strong class="bold">Lambda</strong> link from the list of search results.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">In this chapter, we will create and manage our resources in the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region. Make sure that you have set the correct region before proceeding with the next steps.</p>
<ol>
<li value="2">Locate <a id="_idIndexMarker1488"/>and click on the <strong class="bold">Create function</strong> button (located in the upper-left corner <a id="_idIndexMarker1489"/>of the <strong class="bold">Functions</strong> page). In the <strong class="bold">Create function</strong> page, select <strong class="bold">Author from scratch</strong> when choosing an option to create our function. Additionally, specify the following configuration under <strong class="bold">Basic information</strong>:<ul><li><strong class="bold">Function name</strong>: <strong class="source-inline">deploy-model-to-new-endpoint</strong></li>
<li><strong class="bold">Runtime</strong>: <strong class="source-inline">Python 3.9</strong></li>
<li><strong class="bold">Permissions</strong> &gt; <strong class="bold">Change default execution role</strong></li>
<li><strong class="bold">Execution role:</strong> <strong class="source-inline">Use an existing role</strong> </li>
<li><strong class="bold">Existing role</strong>: <strong class="source-inline">pipeline-lambda-role</strong></li>
</ul></li>
<li>Scroll down to the bottom of the page and then click on the <strong class="bold">Create function</strong> button.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">You should see the following success notification after clicking on the <strong class="bold">Create function</strong> button: <strong class="bold">Successfully created the function deploy-model-to-new-endpoint</strong>.<strong class="bold"> </strong>You can now change its code and configuration. To invoke your function with a test event, choose <strong class="bold">Test</strong>.</p>
<ol>
<li value="4">Navigate to the <strong class="bold">Configuration</strong> tab. Under <strong class="bold">General configuration</strong>, click on the <strong class="bold">Edit</strong> button. This should redirect you to the <strong class="bold">Edit basic settings</strong> page. Specify the following configuration values on the <strong class="bold">Edit basic settings</strong> page:<ul><li><strong class="bold">Memory</strong>: <strong class="source-inline">1024</strong> MB</li>
<li><strong class="bold">Timeout</strong>: <strong class="source-inline">15</strong> min <strong class="source-inline">0</strong> sec</li>
</ul></li>
</ol>
<p class="list-inset">Afterward, click on the <strong class="bold">Save</strong> button.</p>
<ol>
<li value="5">Open the following link in another browser tab: <a href="https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter11/utils.py">https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter11/utils.py</a>. Copy the contents of the page into your clipboard using <em class="italic">Ctrl</em> + <em class="italic">A</em> and then <em class="italic">Ctrl</em> + <em class="italic">C</em> (or, alternatively, <em class="italic">CMD</em> + <em class="italic">A</em> and then <em class="italic">CMD</em> + <em class="italic">C</em> if you are using a Mac).</li>
<li>Back in <a id="_idIndexMarker1490"/>the browser tab showing the Lambda console, navigate to the <strong class="bold">Code</strong> tab. Under <strong class="bold">Code source</strong>, open the <strong class="bold">File</strong> menu and then select <strong class="bold">New File</strong>. This will <a id="_idIndexMarker1491"/>open a new tab named <strong class="source-inline">Untitled1</strong>.</li>
<li>In the new tab (containing no code), paste the code copied to the clipboard. Open the <strong class="bold">File</strong> menu and then select <strong class="bold">Save</strong> from the list of options. Specify <strong class="source-inline">utils.py</strong> as the <strong class="bold">Filename</strong> field value, and then click on <strong class="bold">Save</strong>.</li>
<li>Navigate to the tab where we can modify the code inside <strong class="source-inline">lambda_function.py</strong>. Delete the boilerplate code currently stored inside <strong class="source-inline">lambda_function.py</strong> before proceeding.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Type (or copy) the code blocks in the succeeding set of steps inside <strong class="source-inline">lambda_function.py</strong>. You can find a copy of the code for the Lambda function at <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/deploy-model-to-new-endpoint.py">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/deploy-model-to-new-endpoint.py</a>.</p>
<ol>
<li value="9">In the <strong class="source-inline">lambda_function.py</strong> file, import the functions we will need for deploying a trained ML model to a new ML inference endpoint:<pre class="source-code">import json</pre><pre class="source-code">from utils import (</pre><pre class="source-code">    <strong class="bold">create_model</strong>, </pre><pre class="source-code">    <strong class="bold">create_endpoint_config</strong>, </pre><pre class="source-code">    <strong class="bold">create_endpoint</strong>, </pre><pre class="source-code">    <strong class="bold">random_string</strong>,</pre><pre class="source-code">    <strong class="bold">block</strong></pre><pre class="source-code">)</pre></li>
<li>Now, let’s <a id="_idIndexMarker1492"/>define the <strong class="source-inline">lambda_handler()</strong> function:<pre class="source-code">def <strong class="bold">lambda_handler</strong>(event, context):</pre><pre class="source-code">    role = event['role']</pre><pre class="source-code">    endpoint_name = event['endpoint_name']</pre><pre class="source-code">    package_arn = event['package_arn']</pre><pre class="source-code">    </pre><pre class="source-code">    model_name = 'model-' + random_string()</pre><pre class="source-code">    </pre><pre class="source-code">    with block('CREATE MODEL'):</pre><pre class="source-code">        <strong class="bold">create_model</strong>(</pre><pre class="source-code">            model_name=model_name,</pre><pre class="source-code">            package_arn=package_arn,</pre><pre class="source-code">            role=role</pre><pre class="source-code">        )</pre><pre class="source-code">    </pre><pre class="source-code">    with block('CREATE ENDPOINT CONFIG'):</pre><pre class="source-code">        endpoint_config_name = <strong class="bold">create_endpoint_config</strong>(</pre><pre class="source-code">            model_name</pre><pre class="source-code">        )</pre><pre class="source-code">    </pre><pre class="source-code">    with block('CREATE ENDPOINT'):</pre><pre class="source-code">        <strong class="bold">create_endpoint</strong>(</pre><pre class="source-code">            endpoint_name=endpoint_name, </pre><pre class="source-code">            endpoint_config_name=endpoint_config_name</pre><pre class="source-code">        )</pre><pre class="source-code">        </pre><pre class="source-code">    return {</pre><pre class="source-code">        'statusCode': 200,</pre><pre class="source-code">        'body': json.dumps(event),</pre><pre class="source-code">        'model': model_name</pre><pre class="source-code">    }</pre></li>
<li>Click on the <strong class="bold">Deploy</strong> button.</li>
<li>Click on the <strong class="bold">Test</strong> button.</li>
<li>In <a id="_idIndexMarker1493"/>the <strong class="bold">Configure test event</strong> pop-up window, specify <strong class="source-inline">test</strong> under <strong class="bold">Event name</strong>, and then <a id="_idIndexMarker1494"/>specify the following JSON value under <strong class="bold">Event JSON</strong>:<pre class="source-code">{</pre><pre class="source-code">  "role": "<strong class="bold">&lt;INSERT SAGEMAKER EXECUTION ROLE ARN&gt;</strong>",</pre><pre class="source-code">  "endpoint_name": "AutoGluonEndpoint",</pre><pre class="source-code">  "package_arn": "<strong class="bold">&lt;INSERT MODEL PACKAGE ARN&gt;</strong>"</pre><pre class="source-code">}</pre></li>
</ol>
<p class="list-inset">Make sure that you replace the following values:</p>
<ul>
<li><strong class="source-inline">&lt;INSERT SAGEMAKER EXECUTION ROLE ARN&gt;</strong> – Replace this placeholder value with the <strong class="bold">Execution Role ARN</strong> copied to your text editor in <a id="_idIndexMarker1495"/>the <em class="italic">Preparing the essential prerequisites</em> section of this chapter. The Execution Role ARN should follow a format similar to <strong class="source-inline">arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole-20220000T000000</strong>.</li>
<li><strong class="source-inline">&lt;INSERT MODEL PACKAGE ARN&gt;</strong> – Replace this placeholder value with the <strong class="bold">model package ARN</strong> copied to <a id="_idIndexMarker1496"/>your text editor in the <em class="italic">Running our first pipeline with SageMaker Pipelines</em> section of this chapter. The model package ARN should follow a format similar to <strong class="source-inline">arn:aws:sagemaker:us-west-2:1234567890:model-package/autogluonmodelgroup/1</strong>.</li>
</ul>
<ol>
<li value="14">Copy <a id="_idIndexMarker1497"/>this test event JSON value to the text editor on your local machine. We will <a id="_idIndexMarker1498"/>use this test event JSON again later when testing our <strong class="source-inline">deploy-model-to-existing-endpoint</strong> Lambda function.</li>
<li>Afterward, click on the <strong class="bold">Save</strong> button.</li>
<li>With everything ready, let’s click on the <strong class="bold">Test</strong> button. This should open a new tab that should show the execution results after a few minutes.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This step might take 5–15 minutes to complete. Feel free to grab a cup of coffee or tea! </p>
<ol>
<li value="17">While waiting, scroll up and locate the <strong class="bold">Function overview</strong> pane. Copy the <strong class="bold">Function ARN</strong> value to your text editor. We will use this <strong class="bold">Function ARN</strong> value later in the <em class="italic">Completing the end-to-end ML pipeline</em> section of this chapter.</li>
</ol>
<p>Once the <strong class="source-inline">deploy-model-to-new-endpoint</strong> Lambda function has finished running, we <a id="_idIndexMarker1499"/>should have our ML model deployed already in an ML inference endpoint. Note that <a id="_idIndexMarker1500"/>we are just testing the Lambda function, and we will delete the ML inference endpoint (launched by the <strong class="source-inline">deploy-model-to-new-endpoint</strong> Lambda function) in a later step before running the complete ML pipeline.</p>
<h2 id="_idParaDest-226"><a id="_idTextAnchor242"/>Preparing the Lambda function for checking whether an endpoint exists</h2>
<p>The second <strong class="bold">AWS Lambda</strong> function we will create will be configured and programmed to check whether <a id="_idIndexMarker1501"/>an endpoint exists already (given the endpoint name). To help us visualize how <a id="_idIndexMarker1502"/>our function will work, let’s quickly check <em class="italic">Figure 11.13</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer327">
<img alt="Figure 11.13 – Check whether an endpoint exists already " height="562" src="image/B18638_11_013.jpg" width="1062"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – Check whether an endpoint exists already</p>
<p>This function will accept one input parameter—the name of the ML inference endpoint. After receiving the input parameter, the function will use the <strong class="source-inline">boto3</strong> library to list all running endpoints in the region and check whether the name of one of these endpoints matches the input parameter value.</p>
<p>In the <a id="_idIndexMarker1503"/>next set of <a id="_idIndexMarker1504"/>steps, we will create a Lambda function that we will use to check whether an ML inference endpoint exists already:</p>
<ol>
<li value="1">Open a new browser tab and navigate to the <strong class="bold">Functions</strong> page of the Lambda Management console.</li>
<li>Locate and click on the <strong class="bold">Create function</strong> button (located in the upper-left corner of the <strong class="bold">Functions</strong> page), and then specify the following configuration values:<ul><li><strong class="bold">Author from scratch</strong></li>
<li><strong class="bold">Function name</strong>: <strong class="source-inline">check-if-endpoint-exists</strong></li>
<li><strong class="bold">Runtime</strong>: <strong class="source-inline">Python 3.9</strong></li>
<li><strong class="bold">Permissions</strong> &gt; <strong class="bold">Change default execution role</strong></li>
<li><strong class="bold">Execution role:</strong> <strong class="source-inline">Use an existing role</strong> </li>
<li><strong class="bold">Existing role</strong>: <strong class="source-inline">pipeline-lambda-role</strong></li>
</ul></li>
<li>Scroll down to the bottom of the page, and then click on the <strong class="bold">Create function</strong> button.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Type (or copy) the code blocks into the succeeding set of steps inside <strong class="source-inline">lambda_function.py</strong>. You can find a copy of the code for the Lambda function here at <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/check-if-endpoint-exists.py">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/check-if-endpoint-exists.py</a>.</p>
<ol>
<li value="4">In the <strong class="source-inline">lambda_function.py</strong> file, import <strong class="source-inline">boto3</strong> and initialize the client for the SageMaker service:<pre class="source-code">import boto3</pre><pre class="source-code"><strong class="bold">sm_client = boto3.client('sagemaker')</strong></pre></li>
<li>Next, let’s define the <strong class="source-inline">endpoint_exists()</strong> function:<pre class="source-code">def <strong class="bold">endpoint_exists</strong>(endpoint_name):</pre><pre class="source-code">    response = <strong class="bold">sm_client.list_endpoints</strong>(</pre><pre class="source-code">        NameContains=endpoint_name</pre><pre class="source-code">    )</pre><pre class="source-code">    </pre><pre class="source-code">    results = list(</pre><pre class="source-code">        filter(</pre><pre class="source-code">            lambda x: \</pre><pre class="source-code">            x['EndpointName'] == endpoint_name, </pre><pre class="source-code">            </pre><pre class="source-code">            response['Endpoints']</pre><pre class="source-code">        )</pre><pre class="source-code">    )</pre><pre class="source-code">    </pre><pre class="source-code">    return len(results) &gt; 0</pre></li>
<li>Now, let’s <a id="_idIndexMarker1505"/>define the <strong class="source-inline">lambda_handler()</strong> function that makes use of the <strong class="source-inline">endpoint_exists()</strong> function to check whether an ML inference endpoint exists <a id="_idIndexMarker1506"/>or not (given the endpoint name):<pre class="source-code">def <strong class="bold">lambda_handler</strong>(event, context):</pre><pre class="source-code">    endpoint_name = event['endpoint_name']</pre><pre class="source-code">    </pre><pre class="source-code">    return {</pre><pre class="source-code">        'endpoint_exists': <strong class="bold">endpoint_exists</strong>(</pre><pre class="source-code">            endpoint_name=endpoint_name</pre><pre class="source-code">        )</pre><pre class="source-code">    }</pre></li>
<li>Click on the <strong class="bold">Deploy</strong> button.</li>
<li>Click on the <strong class="bold">Test</strong> button. In the <strong class="bold">Configure test event</strong> pop-up window, specify <strong class="source-inline">test</strong> under <strong class="bold">Event name</strong> and then specify the following JSON value under <strong class="bold">Event JSON</strong>:<pre class="source-code">{</pre><pre class="source-code">  "endpoint_name": "<strong class="bold">AutoGluonEndpoint</strong>"</pre><pre class="source-code">}</pre></li>
<li>Afterward, click on the <strong class="bold">Save</strong> button.</li>
<li>With <a id="_idIndexMarker1507"/>everything ready, let’s click on the <strong class="bold">Test</strong> button. This should open a new <a id="_idIndexMarker1508"/>tab that will show the execution results after a few seconds. We should get the following response value after testing the Lambda function:<pre class="source-code">{</pre><pre class="source-code">  "endpoint_exists": <strong class="bold">true</strong></pre><pre class="source-code">}</pre></li>
<li>Finally, scroll up and locate the <strong class="bold">Function overview</strong> pane. Copy the <strong class="bold">Function ARN</strong> value to your text editor. We will use this <strong class="bold">Function ARN</strong> value later in the <em class="italic">Completing the end-to-end ML pipeline</em> section of this chapter.</li>
</ol>
<p>Now that we <a id="_idIndexMarker1509"/>have finished preparing and testing the <strong class="source-inline">check-if-endpoint-exists</strong> Lambda <a id="_idIndexMarker1510"/>function, we can proceed with creating the last Lambda function (<strong class="source-inline">deploy-model-to-existing-endpoint</strong>).</p>
<h2 id="_idParaDest-227"><a id="_idTextAnchor243"/>Preparing the Lambda function for deploying a model to an existing endpoint</h2>
<p>The third <strong class="bold">AWS Lambda</strong> function we will create will be configured and programmed to deploy <a id="_idIndexMarker1511"/>a model to an existing endpoint. To help us visualize how our function will work, let’s <a id="_idIndexMarker1512"/>quickly check <em class="italic">Figure 11.14</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer328">
<img alt="Figure 11.14 – Deploying a model to an existing endpoint " height="634" src="image/B18638_11_014.jpg" width="1025"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – Deploying a model to an existing endpoint</p>
<p>This function will accept three input parameters—an IAM role, the endpoint name, and the model package ARN. After receiving these input parameters, the function will perform the necessary steps to update the model deployed in an existing ML inference endpoint with the model from the model package provided.</p>
<p>In the next set of steps, we will create a Lambda function that we will use to deploy an ML model to an existing inference endpoint:</p>
<ol>
<li value="1">Open a new browser tab and navigate to the <strong class="bold">Functions</strong> page of the Lambda Management console.</li>
<li>Locate and click on the <strong class="bold">Create function</strong> button (located in the upper-left corner of the <strong class="bold">Functions</strong> page), and then specify the following configuration values:<ul><li><strong class="bold">Author from scratch</strong></li>
<li><strong class="bold">Function name</strong>: <strong class="source-inline">deploy-model-to-existing-endpoint</strong></li>
<li><strong class="bold">Runtime</strong>: <strong class="source-inline">Python 3.9</strong></li>
<li><strong class="bold">Permissions</strong> &gt; <strong class="bold">Change default execution role</strong></li>
<li><strong class="bold">Execution role:</strong> <strong class="source-inline">Use an existing role</strong> </li>
<li><strong class="bold">Existing role</strong>: <strong class="source-inline">pipeline-lambda-role</strong></li>
</ul></li>
<li>Scroll down to the bottom of the page and then click on the <strong class="bold">Create function</strong> button.</li>
<li>Navigate <a id="_idIndexMarker1513"/>to the <strong class="bold">Configuration</strong> tab. Under <strong class="bold">General configuration</strong>, click on the <strong class="bold">Edit</strong> button. This should redirect you to the <strong class="bold">Edit basic settings</strong> page. Specify <a id="_idIndexMarker1514"/>the following configuration values in the <strong class="bold">Edit basic settings</strong> page:<ul><li><strong class="bold">Memory</strong>: <strong class="source-inline">1024</strong> MB</li>
<li><strong class="bold">Timeout</strong>: <strong class="source-inline">15</strong> min <strong class="source-inline">0</strong> sec</li>
</ul></li>
<li>Afterward, click on the <strong class="bold">Save</strong> button.</li>
<li>Open the following link in another browser tab: <a href="https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter11/utils.py">https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter11/utils.py</a>. Copy the contents of the page into your clipboard using <em class="italic">Ctrl</em> + <em class="italic">A</em> and then <em class="italic">Ctrl</em> + <em class="italic">C</em> (or, alternatively, <em class="italic">CMD</em> + <em class="italic">A</em> and then <em class="italic">CMD</em> + <em class="italic">C</em> if you are using a Mac).</li>
<li>Back in the browser tab showing the Lambda console, navigate to the <strong class="bold">Code</strong> tab. Under <strong class="bold">Code source</strong>, open the <strong class="bold">File</strong> menu and then select <strong class="bold">New File</strong>. This will open a new tab named <strong class="source-inline">Untitled1</strong>. In the new tab (containing no code), paste the code copied to the clipboard. </li>
<li>Open the <strong class="bold">File</strong> menu and then select <strong class="bold">Save</strong> from the list of options. Specify <strong class="source-inline">utils.py</strong> as the <strong class="bold">Filename</strong> field value and then click on <strong class="bold">Save</strong>.</li>
<li>Navigate <a id="_idIndexMarker1515"/>to the tab where we can modify the code inside <strong class="source-inline">lambda_function.py</strong>. Delete <a id="_idIndexMarker1516"/>the boilerplate code currently stored inside <strong class="source-inline">lambda_function.py</strong> before proceeding.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Type (or copy) the code blocks in the succeeding set of steps inside <strong class="source-inline">lambda_function.py</strong>. You can find a copy of the code for the Lambda function at <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/deploy-model-to-existing-endpoint.py">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/deploy-model-to-existing-endpoint.py</a>.</p>
<ol>
<li value="10">In the <strong class="source-inline">lambda_function.py</strong> file, import the functions we will need to update the deployed model of an existing endpoint:<pre class="source-code">import json</pre><pre class="source-code">from utils import (</pre><pre class="source-code">    <strong class="bold">create_model</strong>, </pre><pre class="source-code">    <strong class="bold">create_endpoint_config</strong>, </pre><pre class="source-code">    <strong class="bold">update_endpoint</strong>, </pre><pre class="source-code">    random_string,</pre><pre class="source-code">    block</pre><pre class="source-code">)</pre></li>
<li>Now, let’s <a id="_idIndexMarker1517"/>define the <strong class="source-inline">lambda_handler()</strong> function using the following <a id="_idIndexMarker1518"/>block of code:<pre class="source-code">def <strong class="bold">lambda_handler</strong>(event, context):</pre><pre class="source-code">    role = event['role']</pre><pre class="source-code">    endpoint_name = event['endpoint_name']</pre><pre class="source-code">    package_arn = event['package_arn']</pre><pre class="source-code">    </pre><pre class="source-code">    model_name = 'model-' + random_string()</pre><pre class="source-code">    </pre><pre class="source-code">    with block('CREATE MODEL'):</pre><pre class="source-code">        <strong class="bold">create_model</strong>(</pre><pre class="source-code">            model_name=model_name,</pre><pre class="source-code">            package_arn=package_arn,</pre><pre class="source-code">            role=role</pre><pre class="source-code">        )</pre><pre class="source-code">    </pre><pre class="source-code">    with block('CREATE ENDPOINT CONFIG'):</pre><pre class="source-code">        endpoint_config_name = <strong class="bold">create_endpoint_config</strong>(</pre><pre class="source-code">            model_name</pre><pre class="source-code">        )</pre><pre class="source-code">    </pre><pre class="source-code">    with block('UPDATE ENDPOINT'):</pre><pre class="source-code">        <strong class="bold">update_endpoint</strong>(</pre><pre class="source-code">            endpoint_name=endpoint_name, </pre><pre class="source-code">            endpoint_config_name=endpoint_config_name</pre><pre class="source-code">        )</pre><pre class="source-code">        </pre><pre class="source-code">    return {</pre><pre class="source-code">        'statusCode': 200,</pre><pre class="source-code">        'body': json.dumps(event),</pre><pre class="source-code">        'model': model_name</pre><pre class="source-code">    } </pre></li>
<li>Click on the <strong class="bold">Deploy</strong> button.</li>
<li>Click on <a id="_idIndexMarker1519"/>the <strong class="bold">Test</strong> button. In the <strong class="bold">Configure test event</strong> pop-up window, specify <strong class="source-inline">test</strong> under <strong class="bold">Event name</strong> and <a id="_idIndexMarker1520"/>then specify the following JSON value under <strong class="bold">Event JSON</strong>:<pre class="source-code">{</pre><pre class="source-code">  "role": "<strong class="bold">&lt;INSERT SAGEMAKER EXECUTION ROLE ARN&gt;</strong>",</pre><pre class="source-code">  "endpoint_name": "AutoGluonEndpoint",</pre><pre class="source-code">  "package_arn": "<strong class="bold">&lt;INSERT MODEL PACKAGE ARN&gt;</strong>"</pre><pre class="source-code">}</pre></li>
</ol>
<p class="list-inset">Make sure that you replace the following values:</p>
<ul>
<li><strong class="source-inline">&lt;INSERT SAGEMAKER EXECUTION ROLE ARN&gt;</strong> – Replace this placeholder value with the <strong class="bold">Execution Role ARN</strong> copied to your text editor in the <em class="italic">Preparing the essential prerequisites</em> section of this chapter.</li>
<li><strong class="source-inline">&lt;INSERT MODEL PACKAGE ARN&gt;</strong> – Replace this placeholder value with the <strong class="bold">model package ARN</strong> copied to your text editor in the <em class="italic">Running our first pipeline with SageMaker Pipelines</em> section of this chapter.</li>
</ul>
<p class="list-inset">Additionally, you can use the same test event JSON value that we copied to our text editor while testing our <strong class="source-inline">deploy-model-to-new-endpoint</strong> Lambda function.</p>
<ol>
<li value="14">Afterward, click on the <strong class="bold">Save</strong> button.</li>
<li>With <a id="_idIndexMarker1521"/>everything ready, let’s click on the <strong class="bold">Test</strong> button. This should open a new tab <a id="_idIndexMarker1522"/>that should show the execution results after a few minutes.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This step may take 5–15 minutes to complete. Feel free to grab a cup of coffee or tea! </p>
<ol>
<li value="16">While waiting, scroll up and locate the <strong class="bold">Function overview</strong> pane. Copy the <strong class="bold">Function ARN</strong> value to your text editor. We will use this <strong class="bold">Function ARN</strong> value later in the <em class="italic">Completing the end-to-end ML pipeline</em> section of this chapter.</li>
</ol>
<p>With all the Lambda functions ready, we can now proceed with testing our ML inference endpoint (before completing the end-to-end ML pipeline).</p>
<p class="callout-heading">Note</p>
<p class="callout">At this point, we should have 3 x <strong class="bold">Function ARN</strong> values in our text editor. This includes the ARNs for the <strong class="source-inline">check-if-endpoint-exists</strong> Lambda function, the <strong class="source-inline">deploy-model-to-new-endpoint</strong> Lambda function, and the <strong class="source-inline">deploy-model-to-existing-endpoint</strong> Lambda function. We will use these ARN values later in the <em class="italic">Completing the end-to-end ML pipeline</em> section of this chapter.</p>
<h1 id="_idParaDest-228"><a id="_idTextAnchor244"/>Testing our ML inference endpoint</h1>
<p>Of course, we need to <a id="_idIndexMarker1523"/>check whether the ML inference endpoint is working! In the next set of steps, we will download and run a Jupyter notebook (named <strong class="source-inline">Test Endpoint and then Delete.ipynb</strong>) that tests our ML inference endpoint using the test dataset:</p>
<ol>
<li value="1">Let’s begin by opening the following link in another browser tab: <a href="https://bit.ly/3xyVAXz">https://bit.ly/3xyVAXz</a><a href="https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter11/Test%20Endpoint%20then%20Delete.ipynb%20"/></li>
<li>Right-click on any part of the page to open a context menu, and then choose <strong class="bold">Save as...</strong> from the list of available options. Save the file as <strong class="source-inline">Test Endpoint then Delete.ipynb</strong>, and then download it to the <strong class="source-inline">Downloads</strong> folder (or similar) on your local machine.</li>
<li>Navigate back to your <strong class="bold">SageMaker Studio</strong> environment. In the <strong class="bold">File Tree</strong> (located on the left-hand side of the SageMaker Studio environment), make sure that you are in the <strong class="source-inline">CH11</strong> folder similar to what we have in <em class="italic">Figure 11.15</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer329">
<img alt="Figure 11.15 – Uploading the test endpoint and then the Delete.ipynb file " height="275" src="image/B18638_11_015.jpg" width="619"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – Uploading the test endpoint and then the Delete.ipynb file</p>
<ol>
<li value="4">Click on the <a id="_idIndexMarker1524"/>upload button (as highlighted in <em class="italic">Figure 11.15</em>), and then select the <strong class="source-inline">Test Endpoint then Delete.ipynb</strong> file that we downloaded in an earlier step.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This should upload the <strong class="source-inline">Test Endpoint then Delete.ipynb</strong> notebook file from your local machine to the SageMaker Studio environment (in the <strong class="source-inline">CH11</strong> folder).</p>
<ol>
<li value="5">Double-click on the <strong class="source-inline">Test Endpoint then Delete.ipynb</strong> file in the <strong class="bold">File tree</strong> to open the notebook in the <strong class="bold">Main work area</strong> (which contains tabs of the open notebooks, files, and terminals).</li>
<li>Update the first cell with the name of the S3 bucket used in the <strong class="source-inline">Machine Learning Pipelines with SageMaker Pipelines.ipynb</strong> notebook:<pre class="source-code">s3_bucket = '<strong class="bold">&lt;INSERT S3 BUCKET HERE&gt;</strong>'</pre></li>
</ol>
<p class="list-inset">Make sure to replace <strong class="source-inline">&lt;INSERT S3 BUCKET HERE&gt;</strong> with the S3 bucket name we copied to our text editor earlier in the <em class="italic">Preparing the essential prerequisites</em> section of this chapter.</p>
<ol>
<li value="7">Open the <strong class="bold">Run</strong> menu and select <strong class="bold">Run All Cells</strong> to execute all the blocks of code in the <strong class="source-inline">Test Endpoint then Delete.ipynb</strong> notebook.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">It should take around 1–2 minutes to run all the cells in the Jupyter notebook. Feel free to grab a cup of coffee or tea while waiting!</p>
<ol>
<li value="8">Once all the cells in the <strong class="source-inline">Test Endpoint then Delete.ipynb</strong> notebook have been <a id="_idIndexMarker1525"/>executed, locate the cell containing the following block of code (along with the returned output):<pre class="source-code">from sklearn.metrics import accuracy_score</pre><pre class="source-code"><strong class="bold">accuracy_score</strong>(actual_list, predicted_list)</pre></li>
</ol>
<p class="list-inset">Verify that the model got an accuracy score equal to or close to <strong class="source-inline">0.88</strong> (or 88%).</p>
<p>At this point, the ML inference endpoint should be in a deleted state since the <strong class="source-inline">Test</strong><strong class="bold"> </strong><strong class="source-inline">Endpoint then Delete.ipynb</strong> Jupyter notebook also runs the <strong class="source-inline">predictor.delete_endpoint()</strong> line after computing for the ML model metrics.</p>
<h1 id="_idParaDest-229"><a id="_idTextAnchor245"/>Completing the end-to-end ML pipeline</h1>
<p>In this section, we <a id="_idIndexMarker1526"/>will build on top of the (partial) pipeline we prepared in the <em class="italic">Running our first pipeline with SageMaker Pipelines</em> section of this chapter. In addition to the steps and resources used to build our partial pipeline, we will also utilize the Lambda functions we created (in the <em class="italic">Creating Lambda functions for deployment</em> section) to complete our ML pipeline. </p>
<h2 id="_idParaDest-230"><a id="_idTextAnchor246"/>Defining and preparing the complete ML pipeline</h2>
<p>The second <a id="_idIndexMarker1527"/>pipeline we will prepare would be slightly longer than <a id="_idIndexMarker1528"/>the first pipeline. To help us visualize how our second ML pipeline using <strong class="bold">SageMaker Pipelines</strong> will look like, let’s quickly check <em class="italic">Figure 11.16</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer330">
<img alt="Figure 11.16 – Our second ML pipeline using SageMaker Pipelines " height="268" src="image/B18638_11_016.jpg" width="1128"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16 – Our second ML pipeline using SageMaker Pipelines</p>
<p>Here, we can see that our pipeline accepts two input parameters—the input dataset and the endpoint name. When the pipeline runs, the input dataset is first split into training, validation, and test sets. The training and validation sets are then used to train an ML model, which then gets registered to the <strong class="bold">SageMaker Model Registry</strong>. After that, the pipeline checks whether an ML inference endpoint with the provided endpoint name exists already. If the <a id="_idIndexMarker1529"/>endpoint does not exist yet, the model is <a id="_idIndexMarker1530"/>deployed to a new endpoint. Otherwise, the model of an existing endpoint (with the provided endpoint name) is updated using the model trained during the pipeline execution.</p>
<p>In the next set of steps, we will create a new ML pipeline using the steps and resources configured in the <strong class="source-inline">Machine Learning Pipelines with SageMaker Pipelines.ipynb</strong> notebook:</p>
<ol>
<li value="1">Navigate back to the tab corresponding to the <strong class="source-inline">Machine Learning Pipelines with SageMaker Pipelines.ipynb</strong> notebook. </li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">We will run the blocks of code in the succeeding set of steps inside the <strong class="source-inline">Machine Learning Pipelines with SageMaker Pipelines.ipynb</strong> notebook (after the existing set of cells). If you turned off the kernel and/or the SageMaker Studio instance after running the commands in the <em class="italic">Running our first pipeline with SageMaker Pipelines</em> section, make sure that you run all the cells again (and wait for the pipeline to finish running) by selecting <strong class="bold">Run All Cells</strong> from the list of options under the <strong class="bold">Run</strong> menu. </p>
<ol>
<li value="2">Let’s initialize the <strong class="source-inline">ParameterString</strong> object that maps to the <strong class="source-inline">Pipeline</strong> parameter for the name of the ML inference endpoint (which will be created or updated after the ML pipeline has finished running):<pre class="source-code"><strong class="bold">input_endpoint_name</strong> = ParameterString(</pre><pre class="source-code">    name="<strong class="bold">EndpointName</strong>",</pre><pre class="source-code">    default_value=f'<strong class="bold">AutoGluonEndpoint</strong>', </pre><pre class="source-code">)</pre></li>
<li>Next, let’s import <a id="_idIndexMarker1531"/>the classes we will need to <a id="_idIndexMarker1532"/>complete the end-to-end ML pipeline:<pre class="source-code">from sagemaker.workflow.lambda_step import (</pre><pre class="source-code">    <strong class="bold">LambdaStep</strong>, </pre><pre class="source-code">    <strong class="bold">LambdaOutput</strong>, </pre><pre class="source-code">    <strong class="bold">LambdaOutputTypeEnum</strong></pre><pre class="source-code">)</pre><pre class="source-code">from sagemaker.lambda_helper import (</pre><pre class="source-code">    <strong class="bold">Lambda</strong></pre><pre class="source-code">)</pre><pre class="source-code">from sagemaker.workflow.conditions import (</pre><pre class="source-code">    <strong class="bold">ConditionEquals</strong></pre><pre class="source-code">)</pre><pre class="source-code">from sagemaker.workflow.condition_step import (</pre><pre class="source-code">    <strong class="bold">ConditionStep</strong>, </pre><pre class="source-code">    <strong class="bold">JsonGet</strong></pre><pre class="source-code">)</pre></li>
<li>Prepare the <strong class="source-inline">LambdaOutput</strong> object that will map (later) to the output of a <strong class="source-inline">LambdaStep</strong> object:<pre class="source-code"><strong class="bold">output_endpoint_exists</strong> = <strong class="bold">LambdaOutput</strong>(</pre><pre class="source-code">    output_name="<strong class="bold">endpoint_exists</strong>", </pre><pre class="source-code">    output_type=LambdaOutputTypeEnum.Boolean</pre><pre class="source-code">)</pre></li>
<li>Initialize <a id="_idIndexMarker1533"/>the <strong class="source-inline">LambdaStep</strong> object, which maps to <a id="_idIndexMarker1534"/>the Lambda function that checks whether a specified ML inference endpoint exists already (given the endpoint name):<pre class="source-code"><strong class="bold">package_arn</strong> = <strong class="bold">step_model_create</strong> \</pre><pre class="source-code">    .properties.<strong class="bold">ModelPackageArn</strong></pre><pre class="source-code"><strong class="bold">endpoint_exists_lambda</strong> = <strong class="bold">LambdaStep</strong>(</pre><pre class="source-code">    name="<strong class="bold">CheckIfEndpointExists</strong>",</pre><pre class="source-code">    lambda_func=Lambda(</pre><pre class="source-code">        function_arn="<strong class="bold">&lt;INSERT FUNCTION ARN&gt;</strong>"</pre><pre class="source-code">    ),</pre><pre class="source-code">    inputs={</pre><pre class="source-code">        "endpoint_name": input_endpoint_name,</pre><pre class="source-code">        "package_arn": package_arn</pre><pre class="source-code">    },</pre><pre class="source-code">    outputs=[output_endpoint_exists]</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">Make sure to replace <strong class="source-inline">&lt;INSERT FUNCTION ARN&gt;</strong> with the ARN of the <strong class="source-inline">check-if-endpoint-exists</strong> Lambda function we copied into our text editor. It should have a format that is similar to <strong class="source-inline">arn:aws:lambda:us-west-2:&lt;ACCOUNT ID&gt;:function:check-if-endpoint-exists</strong>.</p>
<ol>
<li value="6">Next, initialize <a id="_idIndexMarker1535"/>the <strong class="source-inline">LambdaStep</strong> object, which <a id="_idIndexMarker1536"/>maps to the Lambda function that deploys the trained ML model to an existing ML inference endpoint:<pre class="source-code"><strong class="bold">step_lambda_deploy_to_existing_endpoint</strong> = <strong class="bold">LambdaStep</strong>(</pre><pre class="source-code">    name="<strong class="bold">DeployToExistingEndpoint</strong>",</pre><pre class="source-code">    lambda_func=Lambda(</pre><pre class="source-code">        function_arn="<strong class="bold">&lt;INSERT FUNCTION ARN&gt;</strong>"</pre><pre class="source-code">    ),</pre><pre class="source-code">    inputs={</pre><pre class="source-code">        "role": role,</pre><pre class="source-code">        "endpoint_name": input_endpoint_name,</pre><pre class="source-code">        "package_arn": package_arn</pre><pre class="source-code">    },</pre><pre class="source-code">    outputs=[]</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">Make sure that you replace <strong class="source-inline">&lt;INSERT FUNCTION ARN&gt;</strong> with the ARN of the <strong class="source-inline">deploy-model-to-existing-endpoint</strong> Lambda function we copied into our text editor. It should have a format similar to <strong class="source-inline">arn:aws:lambda:us-west-2:&lt;ACCOUNT ID&gt;:function:</strong> <strong class="source-inline">deploy-model-to-existing-endpoint</strong>.</p>
<ol>
<li value="7">After that, initialize the <strong class="source-inline">LambdaStep</strong> object, which maps to the Lambda function that deploys the trained ML model to a new ML inference endpoint:<pre class="source-code"><strong class="bold">step_lambda_deploy_to_new_endpoint</strong> = <strong class="bold">LambdaStep</strong>(</pre><pre class="source-code">    name="<strong class="bold">DeployToNewEndpoint</strong>",</pre><pre class="source-code">    lambda_func=Lambda(</pre><pre class="source-code">        function_arn="<strong class="bold">&lt;INSERT FUNCTION ARN&gt;</strong>"</pre><pre class="source-code">    ),</pre><pre class="source-code">    inputs={</pre><pre class="source-code">        "role": role,</pre><pre class="source-code">        "endpoint_name": input_endpoint_name,</pre><pre class="source-code">        "package_arn": package_arn</pre><pre class="source-code">    },</pre><pre class="source-code">    outputs=[]</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">Make sure that you replace <strong class="source-inline">&lt;INSERT FUNCTION ARN&gt;</strong> with the ARN of the <strong class="source-inline">deploy-model-to-new-endpoint</strong> Lambda function we copied into our text editor. It should have a format that is similar to <strong class="source-inline">arn:aws:lambda:us-west-2:&lt;ACCOUNT ID&gt;:function: deploy-model-to-new-endpoint</strong>.</p>
<ol>
<li value="8">With the <a id="_idIndexMarker1537"/>three <strong class="source-inline">LambdaStep</strong> objects ready, let’s <a id="_idIndexMarker1538"/>prepare the <strong class="source-inline">ConditionStep</strong> object, which checks whether an endpoint exists already (using the output of the <strong class="source-inline">endpoint_exists_lambda</strong> <strong class="source-inline">LambdaStep</strong> object):<pre class="source-code"><strong class="bold">left</strong> = <strong class="bold">endpoint_exists_lambda</strong> \</pre><pre class="source-code">    .properties               \</pre><pre class="source-code">    .<strong class="bold">Outputs['endpoint_exists']</strong></pre><pre class="source-code"><strong class="bold">cond_equals</strong> = <strong class="bold">ConditionEquals</strong>(</pre><pre class="source-code">    left=<strong class="bold">left</strong>,</pre><pre class="source-code">    right=<strong class="bold">True</strong></pre><pre class="source-code">)</pre><pre class="source-code"><strong class="bold">if_steps</strong> = [<strong class="bold">step_lambda_deploy_to_existing_endpoint</strong>]</pre><pre class="source-code"><strong class="bold">else_steps</strong> = [<strong class="bold">step_lambda_deploy_to_new_endpoint</strong>]</pre><pre class="source-code"><strong class="bold">step_endpoint_exists_condition</strong> = <strong class="bold">ConditionStep</strong>(</pre><pre class="source-code">    name="<strong class="bold">EndpointExists</strong>",</pre><pre class="source-code">    conditions=[<strong class="bold">cond_equals</strong>],</pre><pre class="source-code">    if_steps=if_steps,</pre><pre class="source-code">    else_steps=else_steps</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">This step tells the ML pipeline to do the following:</p>
<ul>
<li>Deploy the <a id="_idIndexMarker1539"/>model to a new endpoint if the <a id="_idIndexMarker1540"/>endpoint does not exist yet.</li>
<li>Deploy the model to an existing endpoint if the endpoint exists already.</li>
</ul>
<p class="list-inset">To help us visualize how we configured the <strong class="source-inline">ConditionStep</strong> object, let’s quickly check <em class="italic">Figure 11.17</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer331">
<img alt="Figure 11.17 – Configuring and preparing the ConditionStep object " height="615" src="image/B18638_11_017.jpg" width="1072"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.17 – Configuring and preparing the ConditionStep object</p>
<p class="list-inset">Here, we can see that the <strong class="source-inline">ConditionStep</strong> object is initialized with several parameters—<strong class="source-inline">conditions</strong>, <strong class="source-inline">if_steps</strong>, and <strong class="source-inline">else_steps</strong> (in addition to <strong class="source-inline">name</strong> of the endpoint). If <strong class="source-inline">EndpointExists</strong> <strong class="source-inline">LambdaStep</strong> returns <strong class="source-inline">True</strong>, then <strong class="source-inline">DeployToExistingEndpoint</strong> <strong class="source-inline">LambdaStep</strong> is executed. Otherwise, <strong class="source-inline">DeployToNewEndpoint</strong> <strong class="source-inline">LambdaStep</strong> is executed instead.</p>
<ol>
<li value="9">With all of the <a id="_idIndexMarker1541"/>steps ready, let’s initialize a <a id="_idIndexMarker1542"/>new <strong class="source-inline">Pipeline</strong> object using the different step objects we prepared:<pre class="source-code">pipeline_name = f"COMPLETE-PIPELINE"</pre><pre class="source-code">complete_pipeline = <strong class="bold">Pipeline</strong>(</pre><pre class="source-code">    name=<strong class="bold">pipeline_name</strong>,</pre><pre class="source-code">    parameters=[</pre><pre class="source-code">        <strong class="bold">input_data</strong>,</pre><pre class="source-code">        <strong class="bold">input_endpoint_name</strong></pre><pre class="source-code">    ],</pre><pre class="source-code">    steps=[</pre><pre class="source-code">        <strong class="bold">step_process</strong>, </pre><pre class="source-code">        <strong class="bold">step_train</strong>,</pre><pre class="source-code">        <strong class="bold">step_model_create</strong>,</pre><pre class="source-code">        <strong class="bold">endpoint_exists_lambda</strong>, </pre><pre class="source-code">        <strong class="bold">step_endpoint_exists_condition</strong></pre><pre class="source-code">    ],</pre><pre class="source-code">)</pre><pre class="source-code"><strong class="bold">complete_pipeline.upsert(role_arn=role)</strong></pre></li>
</ol>
<p>Note that this pipeline is <a id="_idIndexMarker1543"/>different and separate from the (partial) pipeline <a id="_idIndexMarker1544"/>we prepared in the <em class="italic">Running our first pipeline with SageMaker Pipelines</em> section of this chapter. We should see that this pipeline has a few more additional steps once we run it in the next section.</p>
<h2 id="_idParaDest-231"><a id="_idTextAnchor247"/>Running the complete ML pipeline</h2>
<p>With everything ready, we can now run our end-to-end ML pipeline. Compared to the (partial) pipeline we <a id="_idIndexMarker1545"/>executed in the <em class="italic">Running our first pipeline with SageMaker Pipelines</em> section of this chapter, our (complete) pipeline allows us to specify an optional name of the ML inference endpoint (<em class="italic">Note: Do not run the following block of code</em>):</p>
<pre class="source-code">execution = <strong class="bold">complete_pipeline.start</strong>(
    parameters=dict(
        EndpointName="<strong class="bold">&lt;INSERT NEW ENDPOINT NAME&gt;</strong>",
    )
)</pre>
<p>If the endpoint name is not specified, the pipeline proceeds with using the default endpoint name value (that is, <strong class="source-inline">AutoGluonEndpoint</strong>) during pipeline execution.</p>
<p>In the next set of steps, we will run our pipeline, wait for it to deploy a trained ML model to a new inference endpoint, and then test the deployed model using the test dataset:</p>
<ol>
<li value="1">Continuing <a id="_idIndexMarker1546"/>where we left off after running the last block of code in the <strong class="source-inline">Machine Learning Pipelines with SageMaker Pipelines.ipynb</strong> notebook, let’s run the end-to-end ML pipeline using the following block of code:<pre class="source-code">execution = <strong class="bold">complete_pipeline.start()</strong></pre><pre class="source-code">execution.<strong class="bold">describe()</strong></pre></li>
<li>Next, let’s use the <strong class="source-inline">wait()</strong> method to wait for the entire pipeline to complete:<pre class="source-code">execution.<strong class="bold">wait()</strong></pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">The pipeline execution should take around 15–30 minutes to complete. Feel free to grab a cup of coffee or tea while waiting!</p>
<ol>
<li value="3">While waiting, locate and click on the triangle icon (<strong class="bold">SageMaker resources</strong>) near the bottom of the left-hand sidebar of SageMaker Studio. This should open the <strong class="bold">SageMaker resources</strong> pane where we can view and inspect a variety of SageMaker resources.</li>
<li>Select <strong class="bold">Pipelines</strong> from the list of options available in the drop-down menu of the <strong class="bold">SageMaker resources</strong> pane. </li>
<li>After that, double-click on the row that maps to the <strong class="source-inline">COMPLETE-PIPELINE</strong> pipeline we just created. After that double-click on the row that maps to the pipeline execution we triggered. You should see a graph similar to what is shown in <em class="italic">Figure 11.18</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer332">
<img alt="Figure 11.18 – The ML pipeline is currently running the TrainModel step " height="612" src="image/B18638_11_018.jpg" width="678"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18 – The ML pipeline is currently running the TrainModel step</p>
<p class="list-inset">Here, we can see that the <strong class="bold">PrepareData</strong> step has just been completed, and the ML pipeline is currently running the <strong class="bold">TrainModel</strong> step. As you can see, the <strong class="source-inline">COMPLETE-PIPELINE</strong> pipeline has more steps compared to the <strong class="source-inline">PARTIAL-PIPELINE</strong> pipeline we executed in the <em class="italic">Running our first pipeline with SageMaker Pipelines</em> section of this chapter.</p>
<ol>
<li value="6">After a few <a id="_idIndexMarker1547"/>minutes, the graph should have more steps completed similar to what we have in <em class="italic">Figure 11.19</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer333">
<img alt="Figure 11.19 – The ML pipeline proceeds with running the DeployToNewEndpoint step  " height="1137" src="image/B18638_11_019.jpg" width="1388"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.19 – The ML pipeline proceeds with running the DeployToNewEndpoint step </p>
<p class="list-inset">Here, we can see that since the ML endpoint does not exist yet (since we deleted it earlier <a id="_idIndexMarker1548"/>while running the <strong class="source-inline">Test Endpoint then Delete.ipynb</strong> notebook), the ML pipeline proceeded with running the <strong class="bold">DeployToNewEndpoint</strong> step. Note that for succeeding runs, if the ML endpoint exists already, the <strong class="bold">DeployToExistingEndpoint</strong> step should run instead.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Make sure that the execution role (attached to the <strong class="bold">SageMaker Domain</strong>) has the <strong class="source-inline">AWSLambda_FullAccess</strong> permission policy attached if you encounter the following error while running the Lambda functions: <strong class="bold">ClientError: User: &lt;ARN&gt; is not authorized to perform: lambda:InvokeFunction on resource: &lt;arn&gt; because no identity-based policy allows the lambda:InvokeFunction action</strong>. Feel free to check the <em class="italic">Preparing the essential prerequisites</em> section of this chapter for step-by-step instructions on how to update the permissions of the execution role.</p>
<ol>
<li value="7">Wait for the pipeline execution to finish. Once the pipeline has finished running, our AutoGluon model should be deployed inside an ML inference endpoint (named <strong class="source-inline">AutoGluonEndpoint</strong>).</li>
<li>Navigate back to the tab corresponding to the <strong class="source-inline">Test Endpoint then Delete.ipynb</strong> notebook. Open the <strong class="bold">Run</strong> menu, and then select <strong class="bold">Run All Cells</strong> to execute all the blocks of code in our <strong class="source-inline">Test Endpoint then Delete.ipynb</strong> notebook. Note that running all the cells in the notebook would also delete the <a id="_idIndexMarker1549"/>existing ML inference endpoint (named <strong class="source-inline">AutoGluonEndpoint</strong>) after all cells have finished running.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">It should take 1–2 minutes to run all the cells in the Jupyter notebook. Feel free to grab a cup of coffee or tea while waiting! </p>
<ol>
<li value="9">Once all the cells in the <strong class="source-inline">Test Endpoint then Delete.ipynb</strong> notebook have been executed, locate the cell containing the following block of code (along with the output returned):<pre class="source-code">from sklearn.metrics import accuracy_score</pre><pre class="source-code"><strong class="bold">accuracy_score</strong>(actual_list, predicted_list)</pre></li>
</ol>
<p class="list-inset">Verify that our model obtained an accuracy score equal to or close to <strong class="source-inline">0.88</strong> (or 88%). Note that this should be similar to what we obtained earlier in the <em class="italic">Testing our ML inference endpoint</em> section of this chapter.</p>
<p><em class="italic">What can we do with this pipeline?</em> With this pipeline, by specifying different endpoint names for each pipeline run, we would be able to train and deploy a model to multiple endpoints. This should help us handle scenarios where we would need to manage dedicated ML inference endpoints for different environments (such as the <strong class="source-inline">production</strong> and <strong class="source-inline">staging</strong> environments). For example, we can have two running ML inference endpoints at the same time—<strong class="source-inline">AutoGluonEndpoint-production</strong> and <strong class="source-inline">AutoGluonEndpoint-staging</strong>. If we wish to generate a new model from a new dataset, we can trigger a pipeline run and specify the endpoint name for the <strong class="source-inline">staging</strong> environment instead of the <strong class="source-inline">production</strong> environment. This will help us test and verify the <a id="_idIndexMarker1550"/>quality of the new model deployed in the <strong class="source-inline">staging</strong> environment and ensure that the <strong class="source-inline">production</strong> environment is always in a stable state. Once we need to update the <strong class="source-inline">production</strong> environment, we can simply trigger another pipeline run and specify the endpoint name associated with the <strong class="source-inline">production</strong> environment when training and deploying the new model.</p>
<p class="callout-heading">Note</p>
<p class="callout">There are several ways to manage these types of deployments, and this is one of the options available for ML engineers and data scientists. </p>
<p>That’s pretty much it! Congratulations on being able to complete a relatively more complex ML pipeline! We were able to accomplish a lot in this chapter, and we should be ready to design and build our own custom pipelines. </p>
<h1 id="_idParaDest-232"><a id="_idTextAnchor248"/>Cleaning up</h1>
<p>Now that we have completed working on the hands-on solutions of this chapter, it is time we clean up <a id="_idIndexMarker1551"/>and turn off the resources we will no longer use. In the next set of steps, we will locate and turn off any remaining running instances in <strong class="bold">SageMaker Studio</strong>:</p>
<ol>
<li value="1">Make sure to check and delete all running inference endpoints under <strong class="bold">SageMaker resources</strong> (if any). To check whether there are running inference endpoints, click on the <strong class="bold">SageMaker resources</strong> icon and then select <strong class="bold">Endpoints</strong> from the list of options in the drop-down menu.</li>
<li>Open the <strong class="bold">File</strong> menu and select <strong class="bold">Shut down</strong> from the list of available options. This should turn off all running instances inside SageMaker Studio.</li>
</ol>
<p>It is important to note that this cleanup operation needs to be performed after using <strong class="bold">SageMaker Studio</strong>. These resources are not turned off automatically by SageMaker even during periods <a id="_idIndexMarker1552"/>of inactivity. Make sure to review whether all delete operations have succeeded before proceeding to the next section.</p>
<p class="callout-heading">Note</p>
<p class="callout">Feel free to clean up and delete all the other resources in the AWS account (for example, the Cloud9 environment and the VPCs and Lambda functions we created), too.</p>
<h1 id="_idParaDest-233"><a id="_idTextAnchor249"/>Recommended strategies and best practices</h1>
<p>Before we end <a id="_idIndexMarker1553"/>this chapter (and this book), let’s quickly discuss some of the recommended strategies and best practices when using SageMaker Pipelines to prepare automated ML workflows. <em class="italic">What improvements can we make to the initial version of our pipeline?</em> Here are some of the possible upgrades we can implement to make our setup more scalable, more secure, and more capable of handling different types of ML and ML engineering requirements:</p>
<ul>
<li>Configure and set up <strong class="bold">autoscaling</strong> (automatic scaling) of the ML inference endpoint upon <a id="_idIndexMarker1554"/>creation to dynamically adjust the number of resources used to handle the incoming traffic (of ML inference requests).</li>
<li>Allow ML models <a id="_idIndexMarker1555"/>to also be deployed in <strong class="bold">serverless</strong> and <strong class="bold">asynchronous</strong> endpoints (depending on the value of an additional <a id="_idIndexMarker1556"/>pipeline input parameter) to help provide additional model deployment options for a variety of use cases.</li>
<li>Add an additional step (or steps) in the pipeline that automatically evaluates the trained ML model using the test set and rejects the deployment of the model if the target metric value falls below a specified threshold score.</li>
<li>Add an additional <a id="_idIndexMarker1557"/>step in the pipeline that uses <strong class="bold">SageMaker Clarify</strong> to check for biases and drifts.</li>
<li>Trigger a pipeline <a id="_idIndexMarker1558"/>execution once an event happens through <strong class="bold">Amazon EventBridge</strong> (such as a file being uploaded in an Amazon S3 bucket).</li>
<li>Cache specific pipeline steps to speed up repeated pipeline executions.</li>
<li>Utilize <strong class="bold">Retry policies</strong> to automatically retry specific pipeline steps when exceptions and errors occur during pipeline executions.</li>
<li>Use <strong class="bold">SageMaker Pipelines</strong> with <strong class="bold">SageMaker Projects</strong> for building complete ML <a id="_idIndexMarker1559"/>workflows, which <a id="_idIndexMarker1560"/>may involve CI/CD capabilities (using AWS services such as <strong class="bold">AWS CodeCommit</strong> and <strong class="bold">AWS CodePipeline</strong>).</li>
<li>Update the IAM roles used in this chapter with a more restrictive set of permissions to improve the security of the setup.</li>
<li>To manage <a id="_idIndexMarker1561"/>the long-term costs of running SageMaker resources, we can utilize the <strong class="bold">Machine Learning Savings Plans</strong>, which involves reducing the overall cost of running resources after making a long-term commitment (for example, a 1-year or 3-year commitment)</li>
</ul>
<p>There’s more we can add to this list, but these should do for now! Make sure that you review and check the recommended solutions and strategies shared in <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>, <em class="italic">Security, Governance, and Compliance Strategies</em>, too.</p>
<h1 id="_idParaDest-234"><a id="_idTextAnchor250"/>Summary</h1>
<p>In this chapter, we used <strong class="bold">SageMaker Pipelines</strong> to build end-to-end automated ML pipelines. We started by preparing a relatively simple pipeline with three steps—including the data preparation step, the model training step, and the model registration step. After preparing and defining the pipeline, we proceeded with triggering a pipeline execution that registered a newly trained model to the <strong class="bold">SageMaker Model Registry</strong> after the pipeline execution finished running.</p>
<p>Then, we prepared three AWS Lambda functions that would be used for the model deployment steps of the second ML pipeline. After preparing the Lambda functions, we proceeded with completing the end-to-end ML pipeline by adding a few additional steps to deploy the model to a new or existing ML inference endpoint. Finally, we discussed relevant best practices and strategies to secure, scale, and manage ML pipelines using the technology stack we used in this chapter.</p>
<p>You’ve finally reached the end of this book! Congratulations on completing all the chapters including the hands-on examples and solutions discussed in this book. It has been an amazing journey from start to finish, and it would be great if you can share this journey with others, too.</p>
<h1 id="_idParaDest-235"><a id="_idTextAnchor251"/>Further reading</h1>
<p>At this point, you might want to dive deeper into the relevant subtopics discussed by checking the references listed in the <em class="italic">Further reading</em> section of each of the previous chapters. In addition to these, you can check the following resources, too:</p>
<ul>
<li><em class="italic">Amazon SageMaker Model Building Pipelines – Pipeline Steps</em> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.xhtml</a>)</li>
<li><em class="italic">Boto3 – SageMaker Client</em> (<a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.xhtml">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.xhtml</a>)</li>
<li><em class="italic">Amazon SageMaker – AutoGluon-Tabular Algorithm</em> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.xhtml</a>)</li>
<li><em class="italic">Automate MLOps with SageMaker Projects</em> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.xhtml</a>)</li>
<li><em class="italic">Machine Learning Savings Plans</em> (<a href="https://aws.amazon.com/savingsplans/ml-pricing/">https://aws.amazon.com/savingsplans/ml-pricing/</a>)</li>
<li><em class="italic">SageMaker – Amazon EventBridge Integration</em> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.xhtml</a>)</li>
</ul>
</div>
</div>
</body></html>