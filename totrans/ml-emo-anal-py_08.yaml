- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Neural Networks and Deep Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络和深度神经网络
- en: In [*Chapter 7*](B18714_07.xhtml#_idTextAnchor144), *Support Vector Machines*,
    we saw that **support vector machines** (**SVMs**) can be used to classify tweets
    for emotions based on the words that they contain. Is there anything else that
    we can use that simply looks at the words that are present? In this chapter, we
    will consider the use of neural networks for this purpose. Neural networks are
    a way of carrying out computations by assigning weights to a network of nodes
    and propagating an initial set of values through the network until the output
    nodes are reached. The values of the output nodes will then be a representation
    of the result of the computation. When neural networks were introduced in the
    1940s, they were intended as a model of the way that the human brain carries out
    computations (Hebb, 1949) (McCulloch & Pitts, 1943). This kind of network is no
    longer taken seriously as a model of the human brain, but the results that can
    sometimes be achieved this way can be very impressive, particularly when the relationship
    between the inputs and outputs is hard to determine. A typical neural network
    has an **input layer** of nodes, a set of **hidden layers**, and an **output layer**,
    with connections usually linking nodes in one layer with nodes in either the same
    layer or the next.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B18714_07.xhtml#_idTextAnchor144)中，*支持向量机*，我们了解到**支持向量机**（**SVMs**）可以根据包含的单词对推文进行情感分类。我们还能使用什么仅仅查看现有单词的方法吗？在本章中，我们将考虑使用神经网络来实现这一目的。神经网络是一种通过为节点网络分配权重并传播一组初始值直到达到输出节点来执行计算的方法。输出节点的值将代表计算的结果。当神经网络在20世纪40年代被引入时，它们被设想为人类大脑执行计算的方式的模型（Hebb，1949年）（McCulloch
    & Pitts，1943年）。这种网络不再被认真视为人类大脑的模型，但有时通过这种方式可以实现的结果可以非常令人印象深刻，尤其是在输入和输出之间的关系难以确定时。一个典型的神经网络有一个**输入层**的节点、一组**隐藏层**和一个**输出层**，通常通过连接同一层或下一层的节点来实现。
- en: We will start by looking at the use of simple neural networks with no hidden
    layers, and we will investigate the effect of varying several relevant parameters.
    As with the algorithms from *Chapters 6* and *7*, the standard application of
    neural networks aims to produce a single value for each input set of features;
    however, as with the Naïve Bayes algorithm from [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134),
    *Naive Bayes* it does this by assigning a score to each potential output label,
    and hence we can easily adapt it to the case where a tweet can have any number
    of labels. By the end of this chapter, you’ll have a clear understanding of how
    neural networks carry out computations and how adding hidden layers to a network
    allows it to compute functions that cannot be computed with a single hidden layer.
    You will also understand how they can be used to assign labels to tweets in our
    datasets.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先探讨没有隐藏层的简单神经网络的用途，并研究几个相关参数变化的影响。与第6章和第7章中的算法一样，神经网络的标准应用旨在为每个输入特征集产生一个单一值；然而，正如第6章中提到的朴素贝叶斯算法[*第6章*](B18714_06.xhtml#_idTextAnchor134)，*朴素贝叶斯*通过为每个潜在的输出标签分配一个分数来实现这一点，因此我们可以轻松地将其适应于一条推文可以具有任意数量标签的情况。到本章结束时，你将清楚地了解神经网络如何执行计算，以及添加隐藏层如何使网络能够计算单隐藏层无法计算的功能。你还将了解它们如何用于为我们数据集中的推文分配标签。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Single-layer neural networks and their use as classifiers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单层神经网络及其作为分类器的应用
- en: Multi-layer neural networks and their use as classifiers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层神经网络及其作为分类器的应用
- en: Single-layer neural networks
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单层神经网络
- en: 'A neural network, in general, consists of a set of nodes, organized in layers,
    with connections between them. A **simple neural network** (**SNN**) simply has
    an input layer that corresponds to the features that the classification is to
    be based on, and an output layer that corresponds to the possible outcomes. In
    the simplest case, where we just want to know whether something belongs to a specified
    category, there will be just one output node, but in our case, where we have multiple
    possible outcomes, we will have multiple output nodes. An SNN looks something
    like the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个神经网络由一组节点组成，这些节点按层组织，它们之间有连接。一个**简单神经网络**（**SNN**）仅仅有一个与分类所依据的特征相对应的输入层，以及一个与可能结果相对应的输出层。在最简单的情况下，如果我们只想知道某物是否属于指定的类别，将只有一个输出节点，但在我们的情况下，由于有多个可能的结果，我们将有多个输出节点。一个SNN看起来可能如下所示：
- en: '![Figure 8.1 – A single-layer neural network where the inputs are words and
    the outputs are emotions](img/B18714_08_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 单层神经网络，其中输入是单词，输出是情感](img/B18714_08_01.jpg)'
- en: Figure 8.1 – A single-layer neural network where the inputs are words and the
    outputs are emotions
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 单层神经网络，其中输入是单词，输出是情感
- en: The links between nodes each have a weight and every node that’s not in the
    input layer has a bias. The weights and the bias are essentially the same as the
    weights and the constant term in the ![<mml:math  ><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mi>*</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mi>*</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>n</mml:mi><mml:mi>*</mml:mi><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/18.png)
    equation, which we used to define the separating hyperplane in [*Chapter 7*](B18714_07.xhtml#_idTextAnchor144),
    *Support* *Vector Machines.*
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 节点之间的链接每个都有一个权重，并且不在输入层的每个节点都有一个偏差。权重和偏差本质上与![<mml:math  ><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mi>*</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mi>*</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>n</mml:mi><mml:mi>*</mml:mi><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/18.png)方程中的权重和常数项相同，该方程我们用来定义[*第7章*](B18714_07.xhtml#_idTextAnchor144)中的*支持向量机*的分隔超平面。
- en: 'Applying such a network to a tweet that needs to be classified is very simple:
    just multiply the weights associated with each word in the tweet (each **active
    input node**), take the sum of those, and add it to the bias for the connected
    node: if this is positive, then set it as the activation for the connected node;
    otherwise, set the activation to 0\. Training a network of this kind is more challenging.
    The basic idea is that you look at the output nodes. If an output node is doing
    what the training data says it should, then there is nothing to be done (after
    all, if all the output nodes did what they should on all the training data, then
    the classifier would be trained as well as possible). If it is not, then there
    must be something wrong with the connections leading into it. There are two possibilities:
    the node is on when it should be off, or it is off when it should be on. Suppose
    it is on when it should be off. The only reason for it to be on is if the sum
    of the weights on the links from active nodes that lead into it is greater than
    its threshold, so to stop it from turning on, the weights on those links should
    all be decreased slightly.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将此类网络应用于需要分类的推文非常简单：只需将推文中每个单词（每个**活动输入节点**）相关的权重相乘，计算这些权重的总和，并将其加到连接节点的偏差上：如果是正的，则将其设置为连接节点的激活；否则，将激活设置为0。训练此类网络更具挑战性。基本思想是查看输出节点。如果一个输出节点正在执行训练数据所说的操作，那么就没有什么需要做的（毕竟，如果所有输出节点在所有训练数据上都按预期执行，那么分类器就会训练得尽可能好）。如果不是这样，那么进入它的连接中肯定有问题。有两种可能性：节点在应该关闭时打开，或者在应该打开时关闭。假设它在应该关闭时打开。它打开的唯一原因可能是从导致它的活动节点到它的链接上的权重总和大于其阈值，因此为了阻止它打开，这些链接上的权重都应该稍微减小。
- en: Similarly, if a node is off when it should be on, then the weights on active
    nodes that lead into it should be increased slightly. Note that in both cases,
    it is the links from active nodes that are adjusted – inactive nodes cannot contribute
    to turning a node that they are connected to on, so changing the weights on the
    links from them has no effect. Exactly how much the weights should be increased
    or decreased, and when this should happen, has substantial effects on the accuracy
    of the results and the time taken to get them. If you change the weights too much,
    the process may fail to converge or it may converge on a sub-optimal configuration;
    if you change them too little, then convergence can take a very long time. This
    process of gradually changing the weights to drive the network to reproduce the
    training data is known as **gradient descent**, reflecting the fact that the aim
    is to move the network downhill in the space of weights and thresholds to obtain
    the minimum overall error.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果一个节点在应该开启时关闭，那么进入该节点的活动节点的权重应该略微增加。请注意，在这两种情况下，调整的是来自活动节点的链接——非活动节点不能帮助它们连接的节点开启，因此改变它们链接上的权重没有任何效果。权重应该增加或减少多少，以及何时进行这种变化，对结果的准确性和获得结果所需的时间有重大影响。如果你改变权重太多，这个过程可能无法收敛，或者可能收敛到一个次优配置；如果你改变得太少，那么收敛可能需要非常长的时间。这个过程通过逐渐改变权重来驱动网络重现训练数据，被称为**梯度下降**，反映了目标是在权重和阈值的空间中将网络向下移动以获得最小的总体误差。
- en: In the original presentations of neural networks, this process was **back-propagated**
    through the network so that the weights on connections leading into the layer
    before the output layer were also adjusted per their overall contribution to the
    output, and the weights on the layer before that, and so on until the input layer
    was reached (Rumelhart et al., 1986). Doing this could be very slow with networks
    with many hidden layers, with very small changes – sometimes vanishingly small
    changes – happening in the early layers. The use of neural networks was therefore
    restricted to quite shallow networks until it was realized that you could train
    a network with *N* hidden layers by training one with N-1 layers and adding another
    layer and fine-tuning the resulting network (Hinton et al., 2006). This meant
    that you could train a network with, say, three hidden layers by training one
    with no hidden layers and then adding a new layer just before the output layer
    and retraining this network (which has one hidden layer), then adding another
    new layer just before the output layers and retraining this network (which has
    two hidden layers), and then adding another new layer, retraining with this one
    (which has three hidden layers). This strategy makes it feasible to train complex
    networks based on the assumption that the generalizations captured by the early
    layers are robust so that errors in later ones will not have a major effect on
    the early ones.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络最初的演示中，这个过程是通过网络**反向传播**的，以便在输出层之前的层的连接上的权重也根据它们对输出的总体贡献进行调整，然后是之前的层，以此类推，直到达到输入层（Rumelhart
    等人，1986年）。对于具有许多隐藏层的网络，这样做可能会非常慢，早期层的变化非常小——有时几乎可以忽略不计。因此，神经网络的用途因此被限制在相当浅的网络，直到人们意识到可以通过训练一个具有
    N-1 层的网络，然后添加另一个层并微调得到的网络来训练一个具有 *N* 个隐藏层的网络（Hinton 等人，2006年）。这意味着你可以通过训练一个没有隐藏层的网络，然后在其输出层之前添加一个新层并重新训练这个网络（它有一个隐藏层），然后在其输出层之前添加另一个新层并重新训练这个网络（它有两个隐藏层），然后添加另一个新层，使用这个新层重新训练（它有三个隐藏层）。这种策略使得基于早期层捕获的泛化假设稳健，从而后来的错误不会对早期层产生重大影响，从而能够训练复杂的网络成为可能。
- en: There are numerous strategies for implementing the training algorithm (there
    is only one way to do the actual application once a network has been trained),
    and numerous implementations of the training algorithm and the machinery for applying
    a trained network to a task. For very large datasets, using an implementation
    that can run in parallel can be a good idea, but the sparse nature of our data
    means that the `sklearn.neural_network.MLPClassifier` implementation runs in a
    reasonable amount of time. We will not try out every possible combination of features
    for every dataset. As with SVMs (but more so), there are countless settings and
    parameters to play with, and it is easy to get diverted into trying variations
    in the hope of getting a few percentage points of improvement. We will look at
    the effect of some of the more significant choices, but we will concentrate mainly
    on considering the way that features are used rather than on the minutiae of the
    training regime. We will start by considering an SNN – that is, a neural network
    with just an input layer and an output layer, as in *Figure 8**.1*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 实现训练算法（一旦网络训练完成，实际应用只有一种方式）的策略有很多，同样，训练算法和将训练好的网络应用于任务的机制也有很多种。对于非常大的数据集，使用可以并行运行的实现可能是个好主意，但我们的数据稀疏性意味着`sklearn.neural_network.MLPClassifier`的实现可以在合理的时间内运行。我们不会尝试每个数据集上所有可能的特征组合。与SVMs（尤其是）一样，有无数个设置和参数可以调整，很容易陷入尝试各种变体以期望获得几个百分点的改进的希望中。我们将研究一些更显著选择的影响，但我们将主要关注考虑特征的使用方式，而不是训练过程的细节。我们将从考虑一个SNN（即只有一个输入层和一个输出层的神经网络，如*图8**.1*所示）开始。
- en: 'An SNN is just a `DNNCLASSIFIER` as a subclass of `SVMCLASSIFIER` (they have
    several shared properties, which we can exploit by doing this). If we specify
    that the default is that `DNNCLASSIFIER` should have no hidden layers, then we
    have a constructor for SNNs. The initialization code follows the same pattern
    as for the other `SKLEARNCLASSIFIER` (including `SVMCLASSIFIER`) – use `readTrainingData`
    to read the training data and put it into the standard format and then invoke
    the `sklearn` implementation of the classification algorithm and fit it to the
    training data:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SNN只是一个作为`SVMCLASSIFIER`子类的`DNNCLASSIFIER`（它们有几个共享属性，我们可以通过这样做来利用）。如果我们指定默认情况下`DNNCLASSIFIER`应该没有隐藏层，那么我们就有了SNN的构造函数。初始化代码遵循与其他`SKLEARNCLASSIFIER`（包括`SVMCLASSIFIER`）相同的模式——使用`readTrainingData`读取训练数据并将其放入标准格式，然后调用`sklearn`的分类算法实现并将其拟合到训练数据：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We are using the `sklearn.neural_network.MLPClassifier` package from `sklearn`.
    This package takes a large number of parameters that control the shape of the
    network and the ways that the weights are calculated and used. As ever, we are
    not going to carry out experiments to see how varying these parameters affects
    performance on our tasks. Our goal is to see how well the basic algorithm works
    for us, so we will largely use the default values for these parameters. Once we
    have ascertained how well the algorithm works in general, it may be worth tweaking
    the parameters, but since, as with all these algorithms, the performance depends
    to a large extent on the nature of the dataset, this is something that can be
    left for later.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用来自`sklearn`的`sklearn.neural_network.MLPClassifier`包。这个包接受大量参数，这些参数控制网络的形状以及权重计算和使用的方式。一如既往，我们不会进行实验来观察这些参数的变化如何影响我们任务上的性能。我们的目标是看看基本算法对我们来说效果如何，所以我们将主要使用这些参数的默认值。一旦我们确定了算法在一般情况下效果如何，可能值得调整参数，但由于所有这些算法的性能在很大程度上取决于数据集的性质，这可以留到以后再考虑。
- en: 'As with all the classifiers so far, the constructor trains the model: with
    the sklearn-based ones, this always involves using `readTrainingData` to convert
    the data into its standard form, making a model of the specified type, and calling
    `self.clsf.fit(self.matrix, self.values)` to train it. Applying the trained model
    involves applying the `applyToTweets` method, which is inherited from the abstract
    `BASECLASSIFIER` class from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment
    Lexicons and* *Vector-Space Models*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与迄今为止的所有分类器一样，构造函数训练模型：基于sklearn的这些分类器，这总是涉及使用`readTrainingData`将数据转换为标准形式，创建指定类型的模型，并调用`self.clsf.fit(self.matrix,
    self.values)`来训练它。应用训练好的模型涉及应用从抽象`BASECLASSIFIER`类继承的`applyToTweets`方法，如[*第5章*](B18714_05.xhtml#_idTextAnchor116)中所述，*情感词典和*
    *向量空间模型*。
- en: 'Trying it out on our datasets, we get the following. (The results for CARER
    were obtained by training on 70K of the full 440K available. Training neural networks
    is considerably slower than other algorithms. We will look at the relationships
    between training size, accuracy, and time later, but for now, just note that the
    accuracy on the CARER dataset seems to have started to level off after about 70K,
    so we can use that for comparison with the other algorithms):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集上尝试，我们得到以下结果。（CARER的结果是通过在440K中的70K上训练获得的。训练神经网络比其他算法慢得多。我们将在稍后研究训练大小、准确性和时间之间的关系，但到目前为止，只需注意CARER数据集上的准确性似乎在70K左右开始趋于平稳，因此我们可以用它与其他算法进行比较）：
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确度** | **召回率** | **微观F1** | **宏观F1** | **Jaccard** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SEM4-EN | 0.902 | 0.902 | 0.902 | 0.902 | 0.822 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-EN | 0.902 | 0.902 | 0.902 | 0.902 | 0.822 |'
- en: '| SEM11-EN | 0.648 | 0.275 | 0.386 | 0.388 | 0.239 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.648 | 0.275 | 0.386 | 0.388 | 0.239 |'
- en: '| **WASSA-EN** | **0.837** | **0.837** | **0.837** | **0.837** | **0.720**
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **WASSA-EN** | **0.837** | **0.837** | **0.837** | **0.837** | **0.720**
    |'
- en: '| **CARER-EN** | **0.901** | **0.901** | **0.901** | **0.901** | **0.820**
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **CARER-EN** | **0.901** | **0.901** | **0.901** | **0.901** | **0.820**
    |'
- en: '| IMDB-EN | 0.885 | 0.885 | 0.885 | 0.885 | 0.793 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-EN | 0.885 | 0.885 | 0.885 | 0.885 | 0.793 |'
- en: '| SEM4-AR | 0.670 | 0.670 | 0.670 | 0.670 | 0.504 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-AR | 0.670 | 0.670 | 0.670 | 0.670 | 0.504 |'
- en: '| SEM11-AR | 0.596 | 0.260 | 0.362 | 0.370 | 0.221 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.596 | 0.260 | 0.362 | 0.370 | 0.221 |'
- en: '| KWT.M-AR | 0.035 | 0.126 | 0.055 | 0.034 | 0.028 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.035 | 0.126 | 0.055 | 0.034 | 0.028 |'
- en: '| SEM4-ES | 0.541 | 0.472 | 0.504 | 0.409 | 0.337 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-ES | 0.541 | 0.472 | 0.504 | 0.409 | 0.337 |'
- en: '| SEM11-ES | 0.484 | 0.290 | 0.362 | 0.361 | 0.221 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.484 | 0.290 | 0.362 | 0.361 | 0.221 |'
- en: Figure 8.2 – Simple neural network applied to the standard datasets
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 应用于标准数据集的简单神经网络
- en: The scores in the preceding table for the two big datasets are the best so far,
    but the others are all slightly worse than what we achieved using Naïve Bayes
    and SVMs. The obvious way to improve the performance of this algorithm is to use
    a DNN. DNNs have been shown to have better performance than SNNs on many tasks,
    and it is reasonable to expect that they will help here. There are, however, a
    huge number of options to choose from when you start using networks with hidden
    layers, and it is worth looking at what the non-hidden layer version is doing
    with the data it was supplied with before trying to add hidden layers. Do we want
    one hidden layer that is half the size of the input layer? Do we want 50 hidden
    layers, each of which is of size 15? Given that training a neural network with
    hidden layers can be very slow, it is a good idea to think about what we want
    the hidden layers to do before we start doing any experiments.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前表中，对于两个大型数据集的分数是目前为止最好的，但其他都略差于我们使用朴素贝叶斯和SVMs所达到的效果。提高此算法性能的明显方法是使用深度神经网络。深度神经网络已经在许多任务上显示出比简单神经网络（SNNs）更好的性能，因此有理由期待它们在这里也会有所帮助。然而，当你开始使用具有隐藏层的网络时，有大量的选项可以选择，在尝试添加隐藏层之前，查看非隐藏层版本如何处理所提供的数据是值得的。我们是否想要一个大小是输入层一半的隐藏层？我们是否想要50个大小为15的隐藏层？鉴于训练具有隐藏层的神经网络可能非常缓慢，在我们开始进行任何实验之前，考虑隐藏层想要做什么是一个好主意。
- en: 'We will start by looking at the effects of varying parameters, such as the
    size of the training data, the number of input features, and the number of iterations.
    Training even a no-hidden- layers neural network can be quite slow (see *Figure
    8**.3*), and it is worth looking at how changes to the training data affect the
    time for training and the accuracy: if we find that there is a way of reducing
    training time while maintaining a reasonable level of performance, then it may
    be worth using that rather than the full unrestricted training set.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先研究参数变化的影响，例如训练数据的大小、输入特征的数量和迭代次数。即使是没有任何隐藏层的神经网络训练也可能相当缓慢（参见*图8.3*），研究训练数据的变化如何影响训练时间和准确性是值得的：如果我们发现有一种方法可以在保持合理性能水平的同时减少训练时间，那么使用这种方法而不是完整的无限制训练集可能是有价值的。
- en: 'There are three obvious things we can look at:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察三个明显的事情：
- en: How does the accuracy and training time vary with the size of the training set?
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性和训练时间如何随着训练集的大小变化？
- en: How does the accuracy and training time vary with the number of input features
    (that is, words)?
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性和训练时间如何随着输入特征数量（即单词）的变化而变化？
- en: How does the accuracy and training time vary with the number of iterations?
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确度和训练时间是如何随着迭代次数变化的？
- en: 'We will start by looking at how accuracy (reported as Jaccard score) and training
    time vary with the size of the training set. The following graph plots these for
    the CARER dataset (which is the largest of our datasets) with the other factors
    held constant (only use the 10K most frequent words, do at most 1K iterations):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从观察准确度（以Jaccard分数报告）和训练时间如何随着训练集大小变化开始。以下图表展示了CARER数据集（我们数据集中最大的一个）的这些数据，其他因素保持不变（仅使用最频繁的10K个单词，最多进行1K次迭代）：
- en: '![Figure 8.3 – Jaccard score and training time in seconds versus training size
    with the CARER-EN dataset](img/B18714_08_03.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – Jaccard分数和训练时间（以秒为单位）与CARER-EN数据集的训练大小对比](img/B18714_08_03.jpg)'
- en: Figure 8.3 – Jaccard score and training time in seconds versus training size
    with the CARER-EN dataset
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – Jaccard分数和训练时间（以秒为单位）与CARER-EN数据集的训练大小对比
- en: It is clear that the Jaccard score levels off after about 40K tweets, while
    the training time seems to be on an upward trend. It is not easy to fit a curve
    to the Jaccard plot – a polynomial one will inevitably begin to trend downwards,
    and a logarithmic one will inevitably increase to above 1 at some point – however,
    a simple inspection should give you a reasonable idea of the point at which adding
    extra data will stop producing useful increases in performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，Jaccard分数在大约40K条推文后趋于平稳，而训练时间似乎呈上升趋势。将曲线拟合到Jaccard图上并不容易——多项式曲线不可避免地会开始下降趋势，而对数曲线则不可避免地会在某个点增加到1以上——然而，简单的检查应该能给你一个合理的想法，即添加额外数据将停止产生性能的有用提升。
- en: 'The next thing to vary is the size of the dictionary. Since the input layer
    consists of the words that appear in the tweets, removing infrequent words may
    speed things up without having too much effect on accuracy:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要变化的是字典的大小。由于输入层由推文中出现的单词组成，移除不频繁出现的单词可能会加快速度，而对准确度的影响不大：
- en: '![Figure 8.4 – Jaccard score and training time in seconds versus dictionary
    size with the CARER-EN dataset](img/B18714_08_04.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – Jaccard分数和训练时间（以秒为单位）与CARER-EN数据集的字典大小对比](img/B18714_08_04.jpg)'
- en: Figure 8.4 – Jaccard score and training time in seconds versus dictionary size
    with the CARER-EN dataset
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – Jaccard分数和训练时间（以秒为单位）与CARER-EN数据集的字典大小对比
- en: The CARER-EN dataset contains 16.7K words, but the Jaccard score flattens out
    at somewhere between 1K and 2K. Since training time does increase more or less
    linearly as the number of input features increases, it is worth checking for the
    point at which adding new words has little effect on the accuracy.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: CARER-EN数据集包含16.7K个单词，但Jaccard分数在大约1K到2K之间趋于平稳。由于训练时间随着输入特征数量的增加而或多或少呈线性增加，因此检查添加新单词对准确度影响甚微的点是有意义的。
- en: 'The third thing that we can vary is the number of iterations. Neural network
    training involves making a series of adjustments to the weights and thresholds
    until no further improvement is achievable. The more iterations that we carry
    out, the longer training takes, but the accuracy tends to start to flatten out
    before the best possible result is found. The following chart shows how the training
    time and the Jaccard score vary as the number of iterations increases for SEM4-EN.
    There were no further improvements after 1,800 iterations for this dataset, so
    we stopped the plot at this point. Unsurprisingly, the training time goes up linearly
    with the number of iterations, whereas the Jaccard score starts to level off at
    around 1,400 iterations:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以变化的第三件事是迭代次数。神经网络训练涉及对权重和阈值进行一系列调整，直到无法实现进一步的改进。我们进行的迭代越多，训练时间越长，但准确度往往会在找到最佳结果之前开始趋于平稳。以下图表显示了SEM4-EN数据集随着迭代次数增加，训练时间和Jaccard分数的变化。在1,800次迭代后，该数据集没有进一步的改进，所以我们在这个点停止了绘图。不出所料，训练时间与迭代次数呈线性增长，而Jaccard分数在大约1,400次迭代时开始趋于平稳：
- en: '![Figure 8.5 – Jaccard score and training time versus iterations with the SEM4-EN
    dataset](img/B18714_08_05.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – Jaccard分数和训练时间与迭代次数对比（SEM4-EN数据集）](img/B18714_08_05.jpg)'
- en: Figure 8.5 – Jaccard score and training time versus iterations with the SEM4-EN
    dataset
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – Jaccard分数和训练时间与迭代次数对比（SEM4-EN数据集）
- en: Varying the size of the training data, the number of input features, and the
    number of iterations affects the scores and the training time. While you are developing
    a model and are trying out different combinations of parameters, settings, and
    preprocessing steps, it is certainly worth doing some preliminary investigations
    to find a set of values for these factors at which the Jaccard score appears to
    be leveling off. But in the end, you just have to grit your teeth and train the
    model using a large amount of training data, a large dictionary, and a large number
    of iterations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 调整训练数据的大小、输入特征的数量和迭代次数会影响分数和训练时间。当你在开发模型并尝试不同的参数组合、设置和预处理步骤时，进行一些初步调查以找到使Jaccard分数似乎趋于平稳的这些因素的值无疑是值得的。但最终，你只能咬紧牙关，使用大量的训练数据、大型字典和大量的迭代来训练模型。
- en: Multi-layer neural networks
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层神经网络
- en: We have seen that if we are prepared to wait, using an SNN can produce, at least
    in some cases, better results than any of the previous algorithms. For a lot of
    problems, adding extra hidden layers can produce better results than networks
    with just an input layer and an output layer. Will this help with our current
    task?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，如果我们愿意等待，使用SNN至少在某些情况下可以产生比之前任何算法更好的结果。对于许多问题，添加额外的隐藏层可以比只有输入层和输出层的网络产生更好的结果。这能帮助我们当前的任务吗？
- en: 'SNNs compute very similar information to that calculated by Naïve Bayes and
    SVMs. The links between input and output nodes carry information about how strongly
    the input nodes (that is, words) are correlated to the output nodes (that is,
    emotions) and how the biases roughly carry information about how likely the given
    output is. The following tables show the links between several common words and
    emotions after training on the CARER dataset:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SNNs计算的信息与朴素贝叶斯和SVMs计算的信息非常相似。输入节点和输出节点之间的链接携带了关于输入节点（即词汇）与输出节点（即情感）之间相关性有多强的信息，以及偏差大致携带了关于给定输出可能性有多大的信息。以下表格显示了在CARER数据集上训练后，几个常见词汇与情感之间的链接：
- en: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | **愤怒** | **恐惧** | **快乐** | **爱** | **悲伤** | **惊讶** |'
- en: '| the | -0.036 | -0.065 | 0.031 | 0.046 | -0.015 | 0.036 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 这 | -0.036 | -0.065 | 0.031 | 0.046 | -0.015 | 0.036 |'
- en: '| sorrow | 0.002 | -0.028 | -0.098 | 0.098 | 0.063 | 0.020 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 悲伤 | 0.002 | -0.028 | -0.098 | 0.098 | 0.063 | 0.020 |'
- en: '| scared | -0.356 | 1.792 | -0.683 | -0.283 | -0.562 | 0.057 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 害怕 | -0.356 | 1.792 | -0.683 | -0.283 | -0.562 | 0.057 |'
- en: '| happy | -0.090 | -0.161 | 0.936 | -0.332 | -0.191 | -0.156 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 快乐 | -0.090 | -0.161 | 0.936 | -0.332 | -0.191 | -0.156 |'
- en: '| disgusting | 0.048 | -0.014 | -0.031 | -0.045 | 0.020 | -0.000 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 讨厌 | 0.048 | -0.014 | -0.031 | -0.045 | 0.020 | -0.000 |'
- en: '| and | -0.001 | -0.033 | 0.014 | 0.015 | -0.031 | 0.022 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 和 | -0.001 | -0.033 | 0.014 | 0.015 | -0.031 | 0.022 |'
- en: '| adoring | -0.054 | -0.034 | -0.110 | 0.218 | -0.085 | 0.007 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 爱慕 | -0.054 | -0.034 | -0.110 | 0.218 | -0.085 | 0.007 |'
- en: '| irritated | 1.727 | -0.249 | -0.558 | -0.183 | -0.621 | -0.124 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 激怒 | 1.727 | -0.249 | -0.558 | -0.183 | -0.621 | -0.124 |'
- en: '| kisses | -0.004 | -0.041 | -0.041 | 0.120 | -0.038 | -0.001 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 吻 | -0.004 | -0.041 | -0.041 | 0.120 | -0.038 | -0.001 |'
- en: Figure 8.6 – Links between words and emotions in the CARER-EN dataset
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – CARER-EN数据集中词汇与情感之间的联系
- en: 'The following table displays the words with the strongest and weakest connections
    to the emotions:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了与情感联系最强和最弱的词汇：
- en: '| anger | offended, greedy, rushed, resentful, selfish ... passionate, supporting,
    strange, amazing, weird |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 愤怒 | 愤怒、贪婪、匆忙、怨恨、自私 ... 热情、支持、奇怪、惊人、怪异 |'
- en: '| fear | unsure, reluctant, shaky, insecure, vulnerable ... shocked, supporting,
    sweet, stressed |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 恐惧 | 不确定、犹豫、颤抖、不安全、脆弱 ... 惊吓、支持、甜蜜、紧张 |'
- en: '| joy | smug, sincere, invigorated, joyful, positive ... helpless, agitated,
    weird, strange, overwhelmed |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 快乐 | 自满、真诚、振奋、快乐、积极 ... 无助、焦虑、怪异、奇怪、压倒 |'
- en: '| love | horny, sympathetic, gentle, naughty, liked ... amazing, overwhelmed,
    hated, strange, weird |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 爱 | 淫荡、同情、温柔、淘气、喜欢 ... 惊人、压倒、讨厌、奇怪、怪异 |'
- en: '| sadness | burdened, homesick, disturbed, rotten, guilty ... sweet, agitated,
    weird, strange |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 悲伤 | 负担、思乡、不安、腐烂、内疚 ... 甜蜜、焦虑、怪异、奇怪 |'
- en: '| surprise | impressed, shocked, amazed, surprised, curious ... feelings, don,
    very, being, or |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 惊讶 | 印象深刻、震惊、惊讶、好奇、好奇 ... 感觉、做、非常、存在、或 |'
- en: Figure 8.7 – Strongest and weakest words for each emotion in the CARER-EN dataset
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – CARER-EN数据集中每种情感的强度和最弱词汇
- en: 'Given a set of input words (a tweet!), the neural network calculates the sum
    of the links from those words to each emotion and compares it with the threshold
    (different implementations of neural networks carry out slightly different calculations:
    the one used here, **rectified linear activation** (Fukushima, 1969), calculates
    the weighted sum of the inputs and the bias but then sets it to zero if it is
    negative). This is very similar to what all the other algorithms do – SVMs also
    calculate a weighted sum of the inputs but do not reset negative outcomes to zero;
    the lexicon-based algorithms also just calculate a weighted sum but since none
    of the weights are negative, the total cannot be less than zero, so there is no
    need to reset them. Naïve Bayes combines the conditional probabilities of the
    various observed events to produce an overall probability. What they all have
    in common is that a single word *always* makes the same contribution to the total.
    This may not always be true:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组输入词（一条推文！），神经网络计算这些词到每个情绪的链接总和，并将其与阈值进行比较（不同神经网络实现执行的计算略有不同：这里使用的，**修正线性激活**（Fukushima，1969）计算输入和偏差的加权和，但如果是负数则将其设置为零）。这与所有其他算法所做的是非常相似的——SVMs
    也计算输入的加权和，但不会将负结果重置为零；基于词典的算法也只计算加权和，但由于没有负权重，总和不可能小于零，因此没有必要重置它们。朴素贝叶斯结合各种观察事件的条件概率来产生一个整体概率。它们共同的特点是单个词
    *总是* 对总和做出相同的贡献。这并不总是正确的：
- en: 'There are words whose sole job is to change the meaning of other words. Consider
    the word *happy*. This word is linked to **joy** rather than to any of the other
    emotions:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些词的唯一任务就是改变其他词的含义。考虑一下 *happy* 这个词。这个词与 **喜悦** 相关联，而不是与其他任何情绪相关：
- en: '|  | **anger** | **fear** | **joy** | **sadness** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | **愤怒** | **恐惧** | **喜悦** | **悲伤** |'
- en: '| happy | -0.077 | -0.159 | 0.320 | -0.048 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| happy | -0.077 | -0.159 | 0.320 | -0.048 |'
- en: Figure 8.8 – Links between happy and the four emotions from SEM4-EN
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – SEM4-EN中“喜悦”与四种情绪之间的链接
- en: 'Tweets where *happy* appears next to *not*, however, do not express joy:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当 *happy* 与 *not* 同时出现在推文中时，并不表达喜悦：
- en: '*This is kind of screwed up , but my brother is about to join the police academy
    . . . . and I ‘ m not happy about . And I ‘m not the* *only one.*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*这有点糟糕，但我的兄弟即将加入警察学院……而我并不高兴。而且我并不是* *唯一一个*.*'
- en: '*Yay bmth canceled Melbourne show fanpoxytastic just lost a days pay and hotel
    fees not happy atm # sad #* *angry*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*Yay bmth canceled Melbourne show fanpoxytastic just lost a days pay and hotel
    fees not happy atm # sad #* *angry*'
- en: '*I was just put on hold for 20 minutes till I hung up . # not happy # terribleservice
    # unhappy @ virginmedia I should have stayed . . .*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*我刚才被电话保持通话20分钟直到我挂断。# 不高兴 # 糟糕的服务 # 不高兴 @ virginmedia 我应该留下来……*'
- en: The presence of *not* changes the meaning of these tweets so that they express
    something other than joy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*not* 的存在并不改变这些推文的含义，以至于它们表达的不是喜悦。'
- en: Not all words that affect the meanings of other words are as easy to identify
    as *not*, particularly in informal texts where abbreviations such as *don’t* and
    *can’t* are very common, but there are certainly others that do something like
    this. It is also important to note that the word whose meaning is being affected
    by the modifying term may not be adjacent to it.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有影响其他词含义的词都像 *not* 那样容易识别，尤其是在非正式文本中，缩写词如 *don’t* 和 *can’t* 非常常见，但肯定还有其他词做类似的事情。还应注意，被修饰词的含义可能不会与修饰词相邻。
- en: 'Some words form compounds that express meanings that are not straightforwardly
    related to the meanings of the individual words in isolation. We saw this with
    Chinese compounds earlier, but even English words can do this. Using pointwise
    mutual information to find compounds (as in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*,
    Sentiment Lexicons and Vector Space Models*), we find that *supporting cast* and
    *sweet potatoes* occur more often in the CARER-EN dataset than you would expect
    given the distributions of the individual words – that is, these terms may be
    viewed as compounds. The weights for the individual words are given in the following
    table, with *supporting* and *sweet* both having strong links to **love** and
    slightly weaker links to **joy**. Neither of the compound words would be expected
    to have these links – there is nothing particularly lovely or joyous about sweet
    potatoes! It is not possible to capture the fact that these words make a different
    contribution to the overall emotional charge of the texts containing them when
    they co-occur with *cast* and *potatoes* using an SNN or indeed any of the earlier
    algorithms:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些单词可以形成复合词，这些复合词表达的意义与单个单词孤立时的意义并不直接相关。我们之前在中文复合词中看到了这一点，但英语单词也可以这样做。使用点互信息来寻找复合词（如[*第5章*](B18714_05.xhtml#_idTextAnchor116)*，情感词典和向量空间模型*），我们发现
    *supporting cast* 和 *sweet potatoes* 在 CARER-EN 数据集中比根据单个单词的分布预期出现的频率要高得多——也就是说，这些术语可以被视为复合词。以下表格给出了单个单词的权重，其中
    *supporting* 和 *sweet* 都与 **爱** 有强烈的联系，与 **喜悦** 的联系则稍微弱一些。复合词本身并不预期会有这些联系——土豆本身并没有什么特别可爱或喜悦的地方！使用SNN或任何早期算法都无法捕捉到这些单词与
    *cast* 和 *potatoes* 共现时对包含它们的文本整体情感电荷的不同贡献：
- en: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | **愤怒** | **恐惧** | **喜悦** | **爱** | **悲伤** | **惊讶** |'
- en: '| supporting | -0.183 | -0.154 | 0.220 | 0.515 | -0.319 | -0.043 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 支持性 | -0.183 | -0.154 | 0.220 | 0.515 | -0.319 | -0.043 |'
- en: '| cast | -0.015 | 0.017 | -0.012 | 0.003 | 0.006 | -0.009 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| cast | -0.015 | 0.017 | -0.012 | 0.003 | 0.006 | -0.009 |'
- en: '| sweet | -0.177 | -0.187 | 0.207 | 0.553 | -0.371 | -0.079 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 甜味 | -0.177 | -0.187 | 0.207 | 0.553 | -0.371 | -0.079 |'
- en: '| potatoes | -0.009 | 0.003 | 0.004 | 0.003 | -0.020 | -0.019 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 土豆 | -0.009 | 0.003 | 0.004 | 0.003 | -0.020 | -0.019 |'
- en: Figure 8.8 – Weights for individual words that can occur as compounds
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 可以作为复合词出现的单个单词的权重
- en: 'Some words are simply ambiguous, with one interpretation carrying one emotional
    charge and another carrying a different one (or none). It is extremely difficult
    to detect that a word is ambiguous simply by looking at texts that contain it,
    and even more difficult to detect how many interpretations it has, and even if
    you do know how many interpretations a word has, you still have to decide which
    one is intended in a given text. So, inferring what emotional charge each interpretation
    has and then deciding which interpretation is intended is more or less impossible.
    However, in some cases, we can see, as with the cases of compounds previously,
    that two words are cooccurring unexpectedly often, and in such cases, we can be
    reasonably sure that the same interpretations are intended in each case. *Feel
    like* and *looks like*, for instance, occur more often in the SEM4-EN data than
    they should: both of these could be ambiguous, with the different meanings carrying
    different emotional charges. But it seems very likely that in each occurrence
    of *feel like* the same interpretations of *feel* and *like* are intended – as
    it happens, the interpretation of *like* in these phrases is not the one that
    is closely linked to **love**.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一些单词纯粹是模糊的，一个解释带有一种情感电荷，另一个解释则带有不同的情感电荷（或没有）。仅通过查看包含该单词的文本，很难检测到一个单词是否模糊，更不用说检测它有多少种解释了，即使你知道一个单词有多少种解释，你仍然必须决定在给定的文本中哪种解释是意图的。因此，推断每种解释的情感电荷并决定哪种解释是意图的几乎是不可行的。然而，在某些情况下，我们可以看到，就像之前的复合词案例一样，两个单词意外地经常共现，在这种情况下，我们可以合理地确信在每个情况下都意图了相同的解释。"Feel
    like" 和 "looks like" 例如，在 SEM4-EN 数据中比预期的出现频率要高：这两个词都可能具有歧义，不同的含义带有不同的情感电荷。但似乎非常可能，在每个
    "feel like" 的出现中，"feel" 和 "like" 的相同解释被意图——实际上，这些短语中 "like" 的解释并不是与 **爱** 紧密相关的那个。 '
- en: All the algorithms that we have seen so far, including SNNs, treat the contributions
    made by individual words atomistically – they all compute a score for each word
    for each emotion, and they then combine the scores using some fairly simple arithmetical
    calculation. Therefore, they *cannot* be sensitive to the issues raised here.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止看到的所有算法，包括SNN，都是原子性地处理单个单词的贡献的——它们都为每个单词和每种情绪计算一个分数，然后使用一些相当简单的算术计算来组合这些分数。因此，它们**不能**对这里提出的问题敏感。
- en: Adding extra layers to our neural network will enable us to handle these phenomena.
    The simplest demonstration of how adding layers will allow a neural network to
    compute something that cannot be dealt with by an SNN involves the XOR function,
    where we have two inputs and we want to get a response if one, but not both, of
    the inputs is on.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的神经网络中添加额外的层将使我们能够处理这些现象。添加层如何使神经网络能够计算SNN无法处理的内容的最简单示例是XOR函数，其中我们有两个输入，我们希望如果其中一个输入而不是两个都处于激活状态时，得到一个响应。
- en: 'This cannot be done with an SNN. We will explore the reasons for this and the
    way that DNNs overcome this limitation by considering a set of made-up tweets
    consisting solely of the words *love*, *like*, *hate*, and *shock* and the emotions
    *anger*, *joy*, and *surprise*, as shown in *Figure 8**.9*:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这不能用SNN来完成。我们将通过考虑一组由**爱**、**喜欢**、**仇恨**和**震惊**以及**愤怒**、**快乐**和**惊讶**组成的虚构推文来探讨这一原因，以及DNN如何通过考虑这些推文来克服这一限制，如图
    8.9 所示：
- en: '| **ID** | **tweet** | **joy** | **anger** | **surprise** |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **ID** | **tweet** | **joy** | **anger** | **surprise** |'
- en: '| 1 | love | 1 | 0 | 0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 1 | love | 1 | 0 | 0 |'
- en: '| 2 | like | 1 | 0 | 0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 2 | like | 1 | 0 | 0 |'
- en: '| 3 | love like | 1 | 0 | 0 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 3 | love like | 1 | 0 | 0 |'
- en: '| 4 | hate | 0 | 1 | 0 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 4 | hate | 0 | 1 | 0 |'
- en: '| 5 | shock | 0 | 0 | 1 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 5 | shock | 0 | 0 | 1 |'
- en: '| 6 | love | 1 | 0 | 0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 6 | love | 1 | 0 | 0 |'
- en: '| 7 | like | 1 | 0 | 0 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 7 | like | 1 | 0 | 0 |'
- en: '| 8 | love like | 1 | 1 | 0 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 8 | love like | 1 | 1 | 0 |'
- en: '| 9 | hate | 0 | 1 | 0 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 9 | hate | 0 | 1 | 0 |'
- en: '| 10 | shock | 0 | 0 | 1 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 10 | shock | 0 | 0 | 1 |'
- en: Figure 8.9 – Straightforward training data
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 直接的训练数据
- en: 'If we train an SNN on this data, we will get the following network:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在这些数据上训练一个SNN，我们将得到以下网络：
- en: '![Figure 8.10 – An SNN for surprise, anger, and joy, with straightforward training
    data](img/B18714_08_10.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.10 – 用于惊讶、愤怒和快乐的 SNN，以及直接的训练数据](img/B18714_08_10.jpg)'
- en: Figure 8.10 – An SNN for surprise, anger, and joy, with straightforward training
    data
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 用于惊讶、愤怒和快乐的 SNN，以及直接的训练数据
- en: 'The strongest link from *hate* is to **anger**, the strongest link from *shock*
    is to **surprise**, and the strongest links from *love* and *like* are to **joy**.
    So, if a tweet consists of one of these words, it will trigger the appropriate
    emotion. If a tweet contains both *love* and *like*, it will also trigger **joy**,
    but the training data says nothing about what should happen if a tweet consists
    of, for instance, *shock* and *like* or *shock* and *hate*. Looking at the network,
    we can see that *hate* votes quite strongly for **anger** and *shock* votes by
    about the same amount for **surprise**, but that *shock* votes much more strongly
    *against* **anger** than *hate* does against **surprise**. So, overall, *shock*
    and *hate* vote for **surprise**. There is nothing meaningful going on here: the
    network is initialized with random values, and these spill over into random decisions
    about configurations of features that have not been seen in the training data.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从**仇恨**到**愤怒**的关联最强，从**震惊**到**惊讶**的关联最强，而从**爱**和**喜欢**到**快乐**的关联最强。因此，如果一条推文包含这些词中的任何一个，它将触发相应的情绪。如果一条推文同时包含**爱**和**喜欢**，它也会触发**快乐**，但训练数据并没有说明如果一条推文包含例如**震惊**和**喜欢**或**震惊**和**仇恨**，会发生什么。观察网络，我们可以看到**仇恨**对**愤怒**的投票相当强烈，而**震惊**对**惊讶**的投票大约相同，但**震惊**对**愤怒**的投票远比对**惊讶**的投票要强。因此，总的来说，**震惊**和**仇恨**投票给**惊讶**。这里并没有什么有意义的事情发生：网络是用随机值初始化的，这些随机值溢出到训练数据中未见过的特征配置的随机决策中。
- en: 'As noted previously, our SNN carries out essentially the same operations as
    an SVM: if the weights on the connections between a set of input nodes, ![<mml:math  ><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/19.png),
    and an output node, ![<mml:math  ><mml:mi>O</mml:mi></mml:math>](img/20.png),
    are ![<mml:math  ><mml:mi>w</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/21.png)![<mml:math  ><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/22.png)
    and the bias for the output node is ![<mml:math  ><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/23.png),
    then if the input values for ![<mml:math  ><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/24.png)
    are v(N 1), ..., v(N k), then the excitation of the output node is determined
    by ![<mml:math  ><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/25.png)![<mml:math  ><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/26.png)
    – the excitation of the output node will be ![<mml:math  ><mml:mn>0</mml:mn></mml:math>](img/27.png)
    if this sum is negative and on to some degree if it is positive. ![<mml:math  ><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/28.png)
    determines a hyperplane that divides points into two classes, ![<mml:math  ><mml:mi>O</mml:mi></mml:math>](img/29.png)
    or ![<mml:math  ><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/30.png),
    just like the coefficients in SVMs.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的SNN执行的操作本质上与SVM相同：如果一组输入节点![<mml:math  ><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/19.png)与输出节点![<mml:math  ><mml:mi>O</mml:mi></mml:math>](img/20.png)之间的权重![<mml:math  ><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/21.png)![<mml:math  ><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/22.png)以及输出节点的偏置![<mml:math  ><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/23.png)，那么如果![<mml:math  ><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/24.png)的输入值为v(N 1),
    ..., v(N k)，那么输出节点的激发由![<mml:math  ><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/25.png)![<mml:math  ><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/26.png)决定，如果这个和是负的，输出节点的激发将是![<mml:math  ><mml:mn>0</mml:mn></mml:math>](img/27.png)，如果是正的，则在一定程度上是正的。![<mml:math  ><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/28.png)确定了一个将点分为两类![<mml:math  ><mml:mi>O</mml:mi></mml:math>](img/29.png)或![<mml:math  ><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/30.png)的超平面，就像SVM中的系数一样。
- en: 'But that means that an SNN cannot classify if the classes are *not* linearly
    separable. The classic example here is the XOR function – that is, examples where
    each of two features in isolation denotes a specific class but the two together
    do not – that is, *XOR(0, 0)=0*, *XOR(0, 1)=1*, *XOR(1, 0)=1*, and *XOR(1, 1)=0*.
    It is easy enough to draw this function and show that it looks as though no line
    separates the 0 and 1 cases. In *Figure 8**.11*, the red circles (at (0, 0) and
    (1, 1)) represent the cases where XOR is 0, and the blue diamonds (at (1, 0) and
    (0, 1)) represent where XOR is 1:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但这意味着，如果类别不是线性可分的，SNN 就无法进行分类。这里的经典例子是 XOR 函数——也就是说，每个特征单独表示一个特定的类别，但两个特征组合在一起则不表示——即
    *XOR(0, 0)=0*，*XOR(0, 1)=1*，*XOR(1, 0)=1*，和 *XOR(1, 1)=0*。画出这个函数并展示它看起来似乎没有线能分开
    0 和 1 的情况是很容易的。在 *图 8**.11 中，红色圆圈（位于 (0, 0) 和 (1, 1)）代表 XOR 为 0 的情况，而蓝色钻石（位于 (1,
    0) 和 (0, 1)）代表 XOR 为 1 的情况：
- en: '![Figure 8.11 – XOR – the blue diamonds and red circles cannot be separated
    by a straight line](img/B18714_08_11.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – XOR – 蓝色钻石和红色圆圈无法用直线分开](img/B18714_08_11.jpg)'
- en: Figure 8.11 – XOR – the blue diamonds and red circles cannot be separated by
    a straight line
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – XOR – 蓝色钻石和红色圆圈无法用直线分开
- en: It looks as though it would be impossible to draw a line that divides the blue
    diamonds and red circles – that is, these two classes look as though they are
    not linearly separable.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来似乎不可能画出一条线来将蓝色钻石和红色圆圈分开——也就是说，这两个类别看起来似乎不是线性可分的。
- en: For formal proof of this, assume that there is such a line. It will have an
    equation such as ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/31.png)
    where, for any point that is above the line, ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/32.png)
    , and for any point below the line, ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/33.png)
    (if B is positive and vice versa if B is negative).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个结论的正式证明，假设存在这样一条线。它将有一个类似于 ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/31.png)
    的方程，其中，对于任何位于线上的点，![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/32.png)
    ，而对于任何位于线下的点，![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/33.png)
    （如果 B 是正数，反之亦然）。
- en: Given our four points, let’s assume that the red circles are both below the
    line and the blue diamonds are above it and B is positive. Then, for the red circle
    at (0, 0), we would have ![<mml:math  ><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/34.png),
    since putting 0 for each of *x* and *y* would give us ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/35.png)),
    which makes C<0\. Similarly, for the red circle at (1, 1), substituting 1 for
    each of *x* and *y* would give us ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/36.png),
    for the blue diamond at (1, 0) substituting 1 for *x* and 0 for *y* would give
    us ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/37.png),
    and for the blue diamond at (1, 0), we would get ![<mml:math  ><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/38.png).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的四个点，假设红色圆圈都在线以下，蓝色菱形都在线以上，且B为正值。那么，对于位于(0, 0)的红色圆圈，我们会有![<mml:math  ><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/34.png)，因为将*x*和*y*都设为0会得到![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/35.png))，这使得C<0。同样地，对于位于(1,
    1)的红色圆圈，将*x*和*y*都替换为1会得到![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/36.png)，对于位于(1,
    0)的蓝色菱形，将*x*替换为1，将*y*替换为0会得到![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/37.png)，而对于位于(1,
    0)的蓝色菱形，我们会得到![<mml:math  ><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/38.png)。
- en: From ![<mml:math  ><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/39.png)
    and ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/40.png),
    we get ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/41.png),
    and likewise from ![<mml:math  ><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/42.png)
    and ![<mml:math  ><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/43.png),
    we get ![<mml:math  ><mml:mi>B</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/44.png).
    But then ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:math>](img/45.png),
    so since ![<mml:math  ><mml:mfenced separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/46.png),
    then ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/47.png),
    contradicting the observation that ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/48.png)
    for the point (1, 1). We do have to consider the possibility that the blue diamonds
    are both above the line and the red circles are below, or that B is negative,
    but an exactly parallel argument rules these out in the same way – there is no
    way to draw a straight line that separates the blue diamonds and red circles.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从 ![<mml:math  ><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:mn></mml:math>](img/39.png)
    和 ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/40.png)，我们得到
    ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/41.png)，同样从
    ![<mml:math  ><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:mn></mml:math>](img/42.png)
    和 ![<mml:math  ><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/43.png)，我们得到
    ![<mml:math  ><mml:mi>B</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/44.png)。但随后
    ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:math>](img/45.png)，因此由于
    ![<mml:math  ><mml:mfenced separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/46.png)，那么
    ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/47.png)，这与观察到的点
    (1, 1) 处 ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/48.png)
    相矛盾。我们确实必须考虑蓝色菱形都在线上方，而红色圆圈都在下方的可能性，或者 B 是负数的可能性，但一个完全相同的论证以同样的方式排除了这些可能性——无法画出一条直线来区分蓝色菱形和红色圆圈。
- en: 'What does this mean for our task? Suppose we adjust our made-up data as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们任务意味着什么？假设我们调整我们的虚构数据如下：
- en: '| **ID** | **tweet** | **joy** | **anger** | **Surprise** |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **ID** | **tweet** | **joy** | **anger** | **Surprise** |'
- en: '| 1 | love | 1 | 0 | 0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 1 | love | 1 | 0 | 0 |'
- en: '| 2 | like | 1 | 0 | 0 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 2 | like | 1 | 0 | 0 |'
- en: '| 3 | love like | 0 | 1 | 0 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 3 | love like | 0 | 1 | 0 |'
- en: '| 4 | hate | 0 | 1 | 0 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 4 | hate | 0 | 1 | 0 |'
- en: '| 5 | shock | 0 | 0 | 1 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 5 | shock | 0 | 0 | 1 |'
- en: '| 6 | love | 1 | 0 | 0 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 6 | love | 1 | 0 | 0 |'
- en: '| 7 | like | 1 | 0 | 0 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 7 | like | 1 | 0 | 0 |'
- en: '| 8 | love like | 0 | 1 | 0 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 8 | love like | 0 | 1 | 0 |'
- en: '| 9 | hate | 0 | 1 | 0 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 9 | hate | 0 | 1 | 0 |'
- en: Figure 8.12 – Difficult training data
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – 难以训练的数据
- en: The only change we have made is that we have made the tweets that contain *love*
    and *like* express **anger** rather than **joy**. This is much like the situation
    with XOR previously, where two features express one emotion when they appear in
    isolation and a different one when they appear together. There is no exact parallel
    to the (0, 0) point from XOR, but in the cases where neither was present, then
    the target is either **anger** (if the tweet was just the word *hate*) or **surprise**
    (if the tweet was just the word *shock*) – that is, when neither *love* nor *like*
    is present, then the tweets do not express **joy**.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们唯一做的改变是，我们让包含*爱*和*喜欢*的推文表达**愤怒**而不是**快乐**。这与之前XOR的情况非常相似，其中两个特征在单独出现时表达一种情绪，而在一起出现时表达另一种情绪。没有与XOR的(0,
    0)点完全相同的情况，但在两种特征都不存在的情况下，目标要么是**愤怒**（如果推文只是单词*恨*），要么是**惊讶**（如果推文只是单词*震惊*）——也就是说，当*爱*和*喜欢*都不存在时，推文不表达**快乐**。
- en: When we train on this data, we will find that the SNN cannot be relied on to
    find weights that assign the right emotions. Sometimes it does, sometimes not.
    The problem is not that there is no set of weights that will assign the correct
    labels. Over a run of 10 folds of 90% training/10% testing, weights that split
    the data correctly were found in two cases, but in eight cases, the classifier
    assigned the wrong emotion to tweets containing both *like* and *love*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在这种数据上训练时，我们会发现SNN不能被依赖来找到分配正确情绪的权重。有时它做到了，有时没有。问题不在于没有一组权重可以分配正确的标签。在90%训练/10%测试的10次折叠运行中，在两种情况下找到了正确分割数据的权重，但在另外八次中，分类器将*喜欢*和*爱*都包含的推文分配了错误的情绪。
- en: 'In the incorrectly trained network shown here, the scores for joy, anger, and
    surprise for tweets containing both these words were 0.35, -0.25, and -3.99, with
    joy the clear winner. The data *is* linearly separable since the correctly trained
    classifier does separate the data into the right classes by using hyperplanes
    defined by the connection weights and biases; however, the gradient descent process
    can easily get stuck in local minima, doing the best it can with the single-word
    tweets but unable to find the correct weights for the compound ones:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里显示的错误训练的网络中，包含这两个单词的推文的快乐、愤怒和惊讶的分数分别为0.35、-0.25和-3.99，快乐是明显的赢家。数据*是*线性可分的，因为正确训练的分类器通过使用由连接权重和偏差定义的超平面将数据正确地分为正确的类别；然而，梯度下降过程很容易陷入局部最小值，在单词推文上做到最好，但无法找到复合词的正确权重：
- en: '![Figure 8.13 – Correctly trained network](img/B18714_08_13.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图8.13 – 正确训练的网络](img/B18714_08_13.jpg)'
- en: Figure 8.13 – Correctly trained network
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – 正确训练的网络
- en: '![Figure 8.14 – Incorrectly trained SNN](img/B18714_08_14.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图8.14 – 错误训练的SNN](img/B18714_08_14.jpg)'
- en: Figure 8.14 – Incorrectly trained SNN
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 – 错误训练的SNN
- en: 'Therefore, we have two kinds of problems:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有两种类型的问题：
- en: If the data is not linearly separable, then no SNN can classify it correctly
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据不可线性分离，那么没有任何SNN可以正确分类它
- en: Even if it can be divided by a set of hyperplanes, SNNs can easily get stuck
    in local minima, doing the best they can with most of the data but unable to find
    the right weights for cases where words have a different effect when they occur
    together from the effect they would have in isolation
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使它可以被一组超平面分割，SNN也容易陷入局部最小值，在大多数数据上做到最好，但无法找到当单词在一起时与单独出现时不同的效果的权重
- en: 'We can solve the first problem by adding extra layers. In order, for instance,
    to calculate XOR, you need a node in the network that is turned on when both the
    input nodes are on and has a negative link to the output. A simple feedforward
    network should have the nodes in the input layer connected to nodes in the first
    hidden layer and then nodes in the first hidden layer connected to nodes in the
    second hidden layer and so on until the nodes in the last hidden layer are connected
    to nodes in the output layer. You need at least one hidden layer with at least
    three nodes in it. However, as we have seen, networks can quite easily get stuck
    in local minima, and the smallest configuration that can be reliably trained to
    recognize XOR has a single hidden layer with five nodes:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加额外的层来解决第一个问题。例如，为了计算异或（XOR），网络中需要一个在两个输入节点都开启时开启，并且对输出节点有负链接的节点。一个简单的前馈网络应该将输入层的节点连接到第一隐藏层的节点，然后第一隐藏层的节点连接到第二隐藏层的节点，依此类推，直到最后一隐藏层的节点连接到输出层的节点。你需要至少一个包含至少三个节点的隐藏层。然而，正如我们所看到的，网络很容易陷入局部最小值，而可以可靠训练以识别异或的最小配置只有一个隐藏层，包含五个节点：
- en: '![Figure 8.15 – DNN trained to classify tweets with either happy or like, but
    not both, as joy](img/B18714_08_15.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图8.15 – 训练用于将只包含快乐或喜欢但不包含两者的推文分类为喜悦的深度神经网络](img/B18714_08_15.jpg)'
- en: Figure 8.15 – DNN trained to classify tweets with either happy or like, but
    not both, as joy
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – 训练用于将只包含快乐或喜欢但不包含两者的推文分类为喜悦的深度神经网络
- en: If *like* is on but *happy* is off, then the second hidden node will be on with
    a score of 1.22 (i.e. -0.766+1.988), which will propagate through to the output
    node as -2.96 (1.22*-2.46). This will then be added to the bias of the output
    node to produce -2.15\. If *happy* is on but *like* is not, then the second and
    fourth hidden nodes will be on, with scores of 0.76 and 1.23, which will propagate
    through to the output node as -0.766*0.76 (for the second hidden node) + -1.309*1.23,
    which when added to the bias for the output node becomes -2.66\. If both the input
    nodes are on, then none of the hidden nodes will be, so the score at the output
    node is just its own bias – that is, 0.81\. For networks with just one output,
    the standard logistic function used for interpreting the final score treats negative
    numbers as being on and positive ones as off, so this network classifies tweets
    that contain just *like* or *happy* as expressing joy and ones that contain both
    as not expressing it.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*喜欢*是开启的但*快乐*是关闭的，那么第二个隐藏节点将开启，得分为1.22（即-0.766+1.988），这将传播到输出节点，得分为-2.96（1.22*-2.46）。然后，这将添加到输出节点的偏差中，产生-2.15。如果*快乐*是开启的但*喜欢*不是，那么第二个和第四个隐藏节点将开启，得分分别为0.76和1.23，这将传播到输出节点，第二个隐藏节点为-0.766*0.76（对于第二个隐藏节点）加上-1.309*1.23，当加上输出节点的偏差时，变为-2.66。如果两个输入节点都开启，那么没有隐藏节点会开启，因此输出节点的得分只是其自身的偏差，即0.81。对于只有一个输出的网络，用于解释最终得分的标准逻辑函数将负数视为开启，正数视为关闭，因此这个网络将只包含*喜欢*或*快乐*的推文分类为表达喜悦，而包含两者的推文则不表达喜悦。
- en: 'Adding hidden units will let the network recognize significant combinations
    of input features as being non-compositional, in that the effect of the combination
    is not just the cumulative effect of the features themselves. We can also see
    that if you do not have enough hidden features, then the training process can
    get stuck in local minima – although you *can* compute XOR using just three features
    in the hidden layer, it is very difficult to train such a network for this task
    (see (Minsky & Papert, 1969) for further discussion of this issue). This is not
    just a matter of not having enough data, or of not allowing the network to train
    for long enough. Networks with hidden layers with three nodes converge very quickly
    (after about 10 or 12 epochs) and ones with four just take a few hundred. We can
    also add extra layers – networks with two hidden layers with four and three nodes
    each can also solve this problem and typically converge slightly more quickly
    than ones with one hidden layer with five nodes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 添加隐藏单元将使网络能够识别输入特征的显著组合为非组合性，即组合的效果不仅仅是特征本身的累积效果。我们还可以看到，如果你没有足够的隐藏特征，那么训练过程可能会陷入局部最小值——尽管你**可以**仅使用隐藏层中的三个特征来计算XOR，但训练这样一个网络来完成这项任务是非常困难的（参见(Minsky
    & Papert, 1969)对该问题的进一步讨论）。这不仅仅是没有足够的数据，或者没有允许网络训练足够长的时间的问题。具有三个节点的隐藏层的网络会非常快地收敛（大约在10或12个epoch之后），而具有四个节点的网络只需要几百个epoch。我们还可以添加额外的层——具有两个隐藏层，每个层有四个和三个节点的网络也可以解决这个问题，并且通常比具有一个隐藏层和五个节点的网络收敛得更快：
- en: '![Figure 8.16 – Network with two hidden layers for solving XOR](img/B18714_08_16.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图8.16 – 解决XOR问题的具有两个隐藏层的网络](img/B18714_08_16.jpg)'
- en: Figure 8.16 – Network with two hidden layers for solving XOR
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 – 解决XOR问题的具有两个隐藏层的网络
- en: The problem is that setting the initial weights and biases at random in small
    networks nearly always leaves you in an area of the search space where you will
    end up in a local minimum. Using larger networks, on the other hand, nearly always
    produces networks that can solve the problem since there will be nodes that are
    in the right part of the search space that can be given increasing significance,
    but they take much longer to train. So, the key task is to find the appropriate
    number of hidden units.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，在小网络中随机设置初始权重和偏差几乎总是让你处于搜索空间的一个区域，最终会陷入局部最小值。另一方面，使用较大的网络，几乎总是会产生能够解决问题的新网络，因为将会有节点位于搜索空间的正确部分，可以赋予它们越来越重要的意义，但它们需要更长的时间来训练。因此，关键任务是找到合适的隐藏单元数量。
- en: The role of a hidden unit is to find words that feed differently into the output
    nodes when they occur in isolation and when they occur in combinations with other
    words. The two key parameters here seem likely to be the number of input features
    (that is, the number of distinct words in the data) and the number of output classes
    (that is, the number of emotions). If there are more words in the lexicon, then
    there are more possible combinations of words, which might mean that having more
    words requires more hidden nodes. If there are more output classes, then there
    are more places where having combinations of words might be helpful.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏单元的作用是在孤立状态下和与其他单词组合时，找到向输出节点输入不同单词的单词。这里的关键参数似乎很可能是输入特征的数量（即数据中的不同单词数量）和输出类别的数量（即情绪的数量）。如果词典中有更多的单词，那么就有更多的单词组合可能，这意味着可能需要更多的隐藏节点。如果输出类别更多，那么就有更多的地方可能需要单词组合来发挥作用。
- en: 'Given that there are many thousands of words in the lexicons for our datasets,
    but only four to eleven emotions, it seems sensible to start by investigating
    the effect of relating the number of hidden nodes to the number of emotions. *Figure
    8**.17* shows what happens when we have a hidden layer with 0.5 times as many
    nodes as there are emotions, 1 times as many, or 1.5 times as many:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的数据集的词典中有成千上万的单词，但只有四到十一个情绪，似乎有道理首先研究将隐藏节点数与情绪数相关联的影响。*图8.17*显示了当我们有一个隐藏层，其节点数是情绪数的0.5倍、1倍或1.5倍时会发生什么：
- en: '![Figure 8.17 – Jaccard score versus the number of hidden nodes = F*number
    of emotions, F from 0.5 to 5](img/B18714_08_17.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图8.17 – Jaccard分数与隐藏节点数的关系 = F*情绪数，F从0.5到5](img/B18714_08_17.jpg)'
- en: Figure 8.17 – Jaccard score versus the number of hidden nodes = F*number of
    emotions, F from 0.5 to 5
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 – Jaccard分数与隐藏节点数的关系 = F*情绪数，F从0.5到5
- en: 'For the three datasets for which we obtained quite good results with an SNN,
    the effect of adding a hidden layer with a moderate number of nodes is substantial.
    The original scores have been repeated here for ease of reference:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们使用SNN获得相当好结果的三个数据集，添加一个具有适度节点数量的隐藏层的效果是显著的。为了便于参考，这里重复了原始分数：
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确率** | **召回率** | **微观F1** | **宏观F1** | **Jaccard** |'
- en: '| SEM4-EN | 0.902 | 0.902 | 0.902 | 0.902 | 0.822 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-EN | 0.902 | 0.902 | 0.902 | 0.902 | 0.822 |'
- en: '| SEM11-EN | 0.648 | 0.275 | 0.386 | 0.388 | 0.239 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.648 | 0.275 | 0.386 | 0.388 | 0.239 |'
- en: '| WASSA-EN | 0.837 | 0.837 | 0.837 | 0.837 | 0.720 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| WASSA-EN | 0.837 | 0.837 | 0.837 | 0.837 | 0.720 |'
- en: '| CARER-EN | 0.901 | 0.901 | 0.901 | 0.901 | 0.820 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| CARER-EN | 0.901 | 0.901 | 0.901 | 0.901 | 0.820 |'
- en: Figure 8.18 – Simple neural network applied to the standard English datasets
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 – 应用于标准英语数据集的简单神经网络
- en: The original Jaccard score for CARER-EN was 0.77, which is equivalent to around
    0.87 accuracy; when we add a hidden layer with half as many nodes as the number
    of emotions in CARER (that is, with just three nodes in the hidden layer since
    CARER has six emotions), we get a better score (Jaccard 0.79, accuracy 0.89) than
    in the original, and then as we increase the number of hidden nodes to 6, 9, 12,
    we get a very gradual improvement, up to the point where the score seems to have
    flattened out and maybe even started to overtrain.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: CARER-EN的原始Jaccard分数为0.77，相当于大约0.87的准确率；当我们添加一个节点数量是CARER中情感数量一半的隐藏层（即，由于CARER有六个情感，隐藏层中只有三个节点）时，我们得到了比原始分数更好的分数（Jaccard
    0.79，准确率0.89），然后当我们增加隐藏节点的数量到6、9、12时，我们得到了非常渐进的改进，直到分数似乎已经趋于平稳，甚至可能开始过拟合。
- en: 'A similar, but more marked, pattern occurs with SEM4-EN and WASSA-EN. In these
    cases, the score starts fairly low when we only have half as many nodes in the
    hidden layer as we have emotions (that is, just two for both of these), but then
    leaps up considerably higher than the original scores as soon as we have as many
    nodes in the hidden layer as we have emotions and then flattens out at around
    Jaccard 0.875 (accuracy 0.93) for SEM4-EN and Jaccard 0.81 (accuracy 0.9) for
    WASSA-EN. In general, it looks as though adding a hidden layer with a modest number
    of nodes produces some improvement over the basic neural network with no hidden
    units, but experiments with more hidden layers or a single hidden layer with more
    nodes suggest that these improvements are fairly limited. This is likely to be
    because hidden layers look for non-compositional combinations of words. There
    are two possible reasons why this has limited effects:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 与SEM4-EN和WASSA-EN类似，但更为明显的是，当隐藏层中的节点数量只有情感数量的一半时（即，这两个都是两个节点），分数开始相当低，但一旦隐藏层中的节点数量与情感数量相同，分数就会显著提高，并且对于SEM4-EN在Jaccard
    0.875（准确率0.93）左右趋于平稳，对于WASSA-EN在Jaccard 0.81（准确率0.9）左右趋于平稳。总的来说，看起来添加一个具有适度节点数量的隐藏层可以在没有隐藏单元的基本神经网络之上产生一些改进，但更多隐藏层或单个具有更多节点的隐藏层的实验表明，这些改进相当有限。这很可能是由于隐藏层寻找非组合性的单词组合。这种有限效果可能有两个原因：
- en: There simply are not all that many cases of words whose emotional weight changes
    when they co-occur with specific partners
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单来说，并不是所有单词的情感权重在它们与特定伙伴共现时都会发生变化
- en: Where there are such combinations, their frequency in the data is not enough
    to override their normal interpretation
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这些组合存在的情况下，它们在数据中的频率不足以改变它们的正常解释
- en: It may be that using much more training data makes using networks with multiple
    or large hidden layers more effective, but with modest-sized datasets, doing so
    has comparatively little effect.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是使用更多的训练数据使得使用具有多个或大型隐藏层的网络更加有效，但在数据集规模较小的情况下，这样做的影响相对较小。
- en: Summary
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at using neural networks for our task of identifying
    the emotions expressed in informal communications such as tweets. We examined
    the way that the lexicon for the datasets is used as the nodes in the input layer
    and looked at how the weights associated with individual words reflect the emotional
    significance of those words. We considered simple neural networks with no hidden
    layers and also slightly deeper ones with a single hidden layer with slightly
    more nodes than the set of output nodes – the performance of the neural network
    flattened out once the hidden layer contained 1.5 to 2 times as many nodes as
    the output layer, so there seemed little point.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用神经网络来识别非正式通讯（如推文）中表达的情感的任务。我们研究了数据集的词汇表如何作为输入层的节点，并考察了与单个单词相关的权重如何反映这些单词的情感意义。我们考虑了没有隐藏层的简单神经网络，以及具有一个隐藏层且节点数量略多于输出节点集的稍微深一点的神经网络——一旦隐藏层包含的节点数量是输出层的1.5到2倍，神经网络的性能就会趋于平稳，因此似乎没有必要做得更复杂。
- en: 'The highest-scoring algorithms for the various datasets are now as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于各种数据集，得分最高的算法如下：
- en: '|  | **LEX (****unstemmed)** | **LEX (****stemmed)** | **CP (****stemmed)**
    | **NB (****multi)** | **SVM (****single)** | **MULTI-SVM** | **SNN (****single)**
    | **DNN** |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | **LEX (****未分词)** | **LEX (****分词)** | **CP (****分词)** | **NB (****多标签)**
    | **SVM (****单标签)** | **MULTI-SVM** | **SNN (****单标签)** | **DNN** |'
- en: '| SEM4-EN | 0.503 | 0.497 | 0.593 | 0.778 | 0.845 |  | 0.829 | *******0.847***
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-EN | 0.503 | 0.497 | 0.593 | 0.778 | 0.845 |  | 0.829 | *******0.847***
    |'
- en: '| SEM11-EN | 0.347 | 0.348 | 0.353 | 0.267 | 0.224 | *******0.385*** | 0.242
    | 0.246 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.347 | 0.348 | 0.353 | 0.267 | 0.224 | *******0.385*** | 0.242
    | 0.246 |'
- en: '| WASSA-EN | 0.445 | 0.437 | 0.505 | 0.707 | *******0.770*** |  | 0.737 | 0.752
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| WASSA-EN | 0.445 | 0.437 | 0.505 | 0.707 | *******0.770*** |  | 0.737 | 0.752
    |'
- en: '| CARER-EN | 0.350 | 0.350 | 0.395 | 0.774 | 0.770 |  | *******0.820*** | 0.804
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| CARER-EN | 0.350 | 0.350 | 0.395 | 0.774 | 0.770 |  | *******0.820*** | 0.804
    |'
- en: '| IMDB-EN | 0.722 | 0.667 | 0.722 | 0.740 | 0.736 |  | *******0.793*** | *******0.793***
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-EN | 0.722 | 0.667 | 0.722 | 0.740 | 0.736 |  | *******0.793*** | *******0.793***
    |'
- en: '| SEM4-AR | 0.506 | 0.509 | 0.513 | *******0.532*** | 0.514 |  | 0.504 | 0.444
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-AR | 0.506 | 0.509 | 0.513 | *******0.532*** | 0.514 |  | 0.504 | 0.444
    |'
- en: '| SEM11-AR | 0.378 | *******0.386*** | 0.382 | 0.274 | 0.216 | 0.340 | 0.221
    | 0.207 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.378 | *******0.386*** | 0.382 | 0.274 | 0.216 | 0.340 | 0.221
    | 0.207 |'
- en: '| KWT.M-AR | *******0.687*** | 0.663 | 0.666 | 0.507 | 0.631 | 0.341 | 0.028
    | 0.026 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | *******0.687*** | 0.663 | 0.666 | 0.507 | 0.631 | 0.341 | 0.028
    | 0.026 |'
- en: '| SEM4-ES | *******0.425*** | 0.420 | 0.177 | 0.331 | 0.412 |  | 0.337 | 0.343
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-ES | *******0.425*** | 0.420 | 0.177 | 0.331 | 0.412 |  | 0.337 | 0.343
    |'
- en: '| SEM11-ES | 0.269 | 0.271 | *******0.278*** | 0.255 | 0.226 | 0.268 | 0.221
    | 0.222 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.269 | 0.271 | *******0.278*** | 0.255 | 0.226 | 0.268 | 0.221
    | 0.222 |'
- en: Figure 8.19 – Scores for the best algorithms so far
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 – 目前最佳算法的得分
- en: 'Neural networks produced the best results in 4 of the 10 datasets, but the
    simple lexical algorithms are still the best for the multi-label datasets. The
    general lesson remains the same as it was at the end of [*Chapter 7*](B18714_07.xhtml#_idTextAnchor144),
    *Support Vector Machines* – you shouldn’t just accept that there is a single best
    classification algorithm: do experiments, try out variations, and see for yourself
    what works best with your data.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在10个数据集中，神经网络在4个数据集上产生了最佳结果，但对于多标签数据集，简单的词汇算法仍然是最好的。一般的教训与第7章末尾的“支持向量机”相同，即你不应该仅仅接受存在一个最佳分类算法：进行实验，尝试不同的变体，并亲自看看什么最适合你的数据。
- en: References
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于本章涉及的主题，请参阅以下资源：
- en: Fukushima, K. (1969). *Visual Feature Extraction by a Multilayered Network of
    Analog Threshold Elements*. IEEE Transactions on Systems Science and Cybernetics,
    5(4), 322–333\. [https://doi.org/10.1109/TSSC.1969.300225](https://doi.org/10.1109/TSSC.1969.300225).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冈山，K. (1969). *通过多层模拟阈值元件网络进行视觉特征提取*. IEEE系统科学与控制论杂志，5(4)，322–333\. [https://doi.org/10.1109/TSSC.1969.300225](https://doi.org/10.1109/TSSC.1969.300225).
- en: 'Hebb, D. O. (1949). *The organization of behavior: A neuropsychological* *theory*.
    Wiley.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赫布，D. O. (1949). *行为组织：一种神经心理理论*. 威廉姆斯出版社。
- en: Hinton, G. E., Osindero, S., & Teh, Y.-W. (2006). *A Fast Learning Algorithm
    for Deep Belief Nets*. Neural Comput., 18(7), 1527–1554\. [https://doi.org/10.1162/neco.2006.18.7.1527](https://doi.org/10.1162/neco.2006.18.7.1527).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辛顿，G. E.，奥斯因德罗，S.，与 Teh，Y.-W. (2006). *深度信念网的快速学习算法*. 神经计算，18(7)，1527–1554\.
    [https://doi.org/10.1162/neco.2006.18.7.1527](https://doi.org/10.1162/neco.2006.18.7.1527).
- en: McCulloch, W. S., & Pitts, W. (1943). *A logical calculus of the ideas immanent
    in nervous activity*. The Bulletin of Mathematical Biophysics, 5(4), 115–133\.
    [https://doi.org/10.1007/BF02478259](https://doi.org/10.1007/BF02478259).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCulloch, W. S., & Pitts, W. (1943). *神经活动中内在思想的逻辑演算*. 数学生物物理学通报, 5(4), 115–133\.
    [https://doi.org/10.1007/BF02478259](https://doi.org/10.1007/BF02478259).
- en: Minsky, M., & Papert, S. (1969). *Perceptrons*. MIT Press.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minsky, M., & Papert, S. (1969). *感知器*. 麻省理工学院出版社.
- en: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). *Learning representations
    by back-propagating errors*. Nature, 323(6088), 533–536\. [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). *通过反向传播错误学习表示*. 自然,
    323(6088), 533–536\. [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0).
