- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural Networks and Deep Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B18714_07.xhtml#_idTextAnchor144), *Support Vector Machines*,
    we saw that **support vector machines** (**SVMs**) can be used to classify tweets
    for emotions based on the words that they contain. Is there anything else that
    we can use that simply looks at the words that are present? In this chapter, we
    will consider the use of neural networks for this purpose. Neural networks are
    a way of carrying out computations by assigning weights to a network of nodes
    and propagating an initial set of values through the network until the output
    nodes are reached. The values of the output nodes will then be a representation
    of the result of the computation. When neural networks were introduced in the
    1940s, they were intended as a model of the way that the human brain carries out
    computations (Hebb, 1949) (McCulloch & Pitts, 1943). This kind of network is no
    longer taken seriously as a model of the human brain, but the results that can
    sometimes be achieved this way can be very impressive, particularly when the relationship
    between the inputs and outputs is hard to determine. A typical neural network
    has an **input layer** of nodes, a set of **hidden layers**, and an **output layer**,
    with connections usually linking nodes in one layer with nodes in either the same
    layer or the next.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by looking at the use of simple neural networks with no hidden
    layers, and we will investigate the effect of varying several relevant parameters.
    As with the algorithms from *Chapters 6* and *7*, the standard application of
    neural networks aims to produce a single value for each input set of features;
    however, as with the Naïve Bayes algorithm from [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134),
    *Naive Bayes* it does this by assigning a score to each potential output label,
    and hence we can easily adapt it to the case where a tweet can have any number
    of labels. By the end of this chapter, you’ll have a clear understanding of how
    neural networks carry out computations and how adding hidden layers to a network
    allows it to compute functions that cannot be computed with a single hidden layer.
    You will also understand how they can be used to assign labels to tweets in our
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Single-layer neural networks and their use as classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-layer neural networks and their use as classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single-layer neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A neural network, in general, consists of a set of nodes, organized in layers,
    with connections between them. A **simple neural network** (**SNN**) simply has
    an input layer that corresponds to the features that the classification is to
    be based on, and an output layer that corresponds to the possible outcomes. In
    the simplest case, where we just want to know whether something belongs to a specified
    category, there will be just one output node, but in our case, where we have multiple
    possible outcomes, we will have multiple output nodes. An SNN looks something
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – A single-layer neural network where the inputs are words and
    the outputs are emotions](img/B18714_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – A single-layer neural network where the inputs are words and the
    outputs are emotions
  prefs: []
  type: TYPE_NORMAL
- en: The links between nodes each have a weight and every node that’s not in the
    input layer has a bias. The weights and the bias are essentially the same as the
    weights and the constant term in the ![<mml:math  ><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mi>*</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mi>*</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>n</mml:mi><mml:mi>*</mml:mi><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/18.png)
    equation, which we used to define the separating hyperplane in [*Chapter 7*](B18714_07.xhtml#_idTextAnchor144),
    *Support* *Vector Machines.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying such a network to a tweet that needs to be classified is very simple:
    just multiply the weights associated with each word in the tweet (each **active
    input node**), take the sum of those, and add it to the bias for the connected
    node: if this is positive, then set it as the activation for the connected node;
    otherwise, set the activation to 0\. Training a network of this kind is more challenging.
    The basic idea is that you look at the output nodes. If an output node is doing
    what the training data says it should, then there is nothing to be done (after
    all, if all the output nodes did what they should on all the training data, then
    the classifier would be trained as well as possible). If it is not, then there
    must be something wrong with the connections leading into it. There are two possibilities:
    the node is on when it should be off, or it is off when it should be on. Suppose
    it is on when it should be off. The only reason for it to be on is if the sum
    of the weights on the links from active nodes that lead into it is greater than
    its threshold, so to stop it from turning on, the weights on those links should
    all be decreased slightly.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if a node is off when it should be on, then the weights on active
    nodes that lead into it should be increased slightly. Note that in both cases,
    it is the links from active nodes that are adjusted – inactive nodes cannot contribute
    to turning a node that they are connected to on, so changing the weights on the
    links from them has no effect. Exactly how much the weights should be increased
    or decreased, and when this should happen, has substantial effects on the accuracy
    of the results and the time taken to get them. If you change the weights too much,
    the process may fail to converge or it may converge on a sub-optimal configuration;
    if you change them too little, then convergence can take a very long time. This
    process of gradually changing the weights to drive the network to reproduce the
    training data is known as **gradient descent**, reflecting the fact that the aim
    is to move the network downhill in the space of weights and thresholds to obtain
    the minimum overall error.
  prefs: []
  type: TYPE_NORMAL
- en: In the original presentations of neural networks, this process was **back-propagated**
    through the network so that the weights on connections leading into the layer
    before the output layer were also adjusted per their overall contribution to the
    output, and the weights on the layer before that, and so on until the input layer
    was reached (Rumelhart et al., 1986). Doing this could be very slow with networks
    with many hidden layers, with very small changes – sometimes vanishingly small
    changes – happening in the early layers. The use of neural networks was therefore
    restricted to quite shallow networks until it was realized that you could train
    a network with *N* hidden layers by training one with N-1 layers and adding another
    layer and fine-tuning the resulting network (Hinton et al., 2006). This meant
    that you could train a network with, say, three hidden layers by training one
    with no hidden layers and then adding a new layer just before the output layer
    and retraining this network (which has one hidden layer), then adding another
    new layer just before the output layers and retraining this network (which has
    two hidden layers), and then adding another new layer, retraining with this one
    (which has three hidden layers). This strategy makes it feasible to train complex
    networks based on the assumption that the generalizations captured by the early
    layers are robust so that errors in later ones will not have a major effect on
    the early ones.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous strategies for implementing the training algorithm (there
    is only one way to do the actual application once a network has been trained),
    and numerous implementations of the training algorithm and the machinery for applying
    a trained network to a task. For very large datasets, using an implementation
    that can run in parallel can be a good idea, but the sparse nature of our data
    means that the `sklearn.neural_network.MLPClassifier` implementation runs in a
    reasonable amount of time. We will not try out every possible combination of features
    for every dataset. As with SVMs (but more so), there are countless settings and
    parameters to play with, and it is easy to get diverted into trying variations
    in the hope of getting a few percentage points of improvement. We will look at
    the effect of some of the more significant choices, but we will concentrate mainly
    on considering the way that features are used rather than on the minutiae of the
    training regime. We will start by considering an SNN – that is, a neural network
    with just an input layer and an output layer, as in *Figure 8**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'An SNN is just a `DNNCLASSIFIER` as a subclass of `SVMCLASSIFIER` (they have
    several shared properties, which we can exploit by doing this). If we specify
    that the default is that `DNNCLASSIFIER` should have no hidden layers, then we
    have a constructor for SNNs. The initialization code follows the same pattern
    as for the other `SKLEARNCLASSIFIER` (including `SVMCLASSIFIER`) – use `readTrainingData`
    to read the training data and put it into the standard format and then invoke
    the `sklearn` implementation of the classification algorithm and fit it to the
    training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We are using the `sklearn.neural_network.MLPClassifier` package from `sklearn`.
    This package takes a large number of parameters that control the shape of the
    network and the ways that the weights are calculated and used. As ever, we are
    not going to carry out experiments to see how varying these parameters affects
    performance on our tasks. Our goal is to see how well the basic algorithm works
    for us, so we will largely use the default values for these parameters. Once we
    have ascertained how well the algorithm works in general, it may be worth tweaking
    the parameters, but since, as with all these algorithms, the performance depends
    to a large extent on the nature of the dataset, this is something that can be
    left for later.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with all the classifiers so far, the constructor trains the model: with
    the sklearn-based ones, this always involves using `readTrainingData` to convert
    the data into its standard form, making a model of the specified type, and calling
    `self.clsf.fit(self.matrix, self.values)` to train it. Applying the trained model
    involves applying the `applyToTweets` method, which is inherited from the abstract
    `BASECLASSIFIER` class from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment
    Lexicons and* *Vector-Space Models*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Trying it out on our datasets, we get the following. (The results for CARER
    were obtained by training on 70K of the full 440K available. Training neural networks
    is considerably slower than other algorithms. We will look at the relationships
    between training size, accuracy, and time later, but for now, just note that the
    accuracy on the CARER dataset seems to have started to level off after about 70K,
    so we can use that for comparison with the other algorithms):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-EN | 0.902 | 0.902 | 0.902 | 0.902 | 0.822 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.648 | 0.275 | 0.386 | 0.388 | 0.239 |'
  prefs: []
  type: TYPE_TB
- en: '| **WASSA-EN** | **0.837** | **0.837** | **0.837** | **0.837** | **0.720**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **CARER-EN** | **0.901** | **0.901** | **0.901** | **0.901** | **0.820**
    |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB-EN | 0.885 | 0.885 | 0.885 | 0.885 | 0.793 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-AR | 0.670 | 0.670 | 0.670 | 0.670 | 0.504 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.596 | 0.260 | 0.362 | 0.370 | 0.221 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.035 | 0.126 | 0.055 | 0.034 | 0.028 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-ES | 0.541 | 0.472 | 0.504 | 0.409 | 0.337 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.484 | 0.290 | 0.362 | 0.361 | 0.221 |'
  prefs: []
  type: TYPE_TB
- en: Figure 8.2 – Simple neural network applied to the standard datasets
  prefs: []
  type: TYPE_NORMAL
- en: The scores in the preceding table for the two big datasets are the best so far,
    but the others are all slightly worse than what we achieved using Naïve Bayes
    and SVMs. The obvious way to improve the performance of this algorithm is to use
    a DNN. DNNs have been shown to have better performance than SNNs on many tasks,
    and it is reasonable to expect that they will help here. There are, however, a
    huge number of options to choose from when you start using networks with hidden
    layers, and it is worth looking at what the non-hidden layer version is doing
    with the data it was supplied with before trying to add hidden layers. Do we want
    one hidden layer that is half the size of the input layer? Do we want 50 hidden
    layers, each of which is of size 15? Given that training a neural network with
    hidden layers can be very slow, it is a good idea to think about what we want
    the hidden layers to do before we start doing any experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by looking at the effects of varying parameters, such as the
    size of the training data, the number of input features, and the number of iterations.
    Training even a no-hidden- layers neural network can be quite slow (see *Figure
    8**.3*), and it is worth looking at how changes to the training data affect the
    time for training and the accuracy: if we find that there is a way of reducing
    training time while maintaining a reasonable level of performance, then it may
    be worth using that rather than the full unrestricted training set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three obvious things we can look at:'
  prefs: []
  type: TYPE_NORMAL
- en: How does the accuracy and training time vary with the size of the training set?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the accuracy and training time vary with the number of input features
    (that is, words)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the accuracy and training time vary with the number of iterations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will start by looking at how accuracy (reported as Jaccard score) and training
    time vary with the size of the training set. The following graph plots these for
    the CARER dataset (which is the largest of our datasets) with the other factors
    held constant (only use the 10K most frequent words, do at most 1K iterations):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Jaccard score and training time in seconds versus training size
    with the CARER-EN dataset](img/B18714_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Jaccard score and training time in seconds versus training size
    with the CARER-EN dataset
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that the Jaccard score levels off after about 40K tweets, while
    the training time seems to be on an upward trend. It is not easy to fit a curve
    to the Jaccard plot – a polynomial one will inevitably begin to trend downwards,
    and a logarithmic one will inevitably increase to above 1 at some point – however,
    a simple inspection should give you a reasonable idea of the point at which adding
    extra data will stop producing useful increases in performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing to vary is the size of the dictionary. Since the input layer
    consists of the words that appear in the tweets, removing infrequent words may
    speed things up without having too much effect on accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Jaccard score and training time in seconds versus dictionary
    size with the CARER-EN dataset](img/B18714_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Jaccard score and training time in seconds versus dictionary size
    with the CARER-EN dataset
  prefs: []
  type: TYPE_NORMAL
- en: The CARER-EN dataset contains 16.7K words, but the Jaccard score flattens out
    at somewhere between 1K and 2K. Since training time does increase more or less
    linearly as the number of input features increases, it is worth checking for the
    point at which adding new words has little effect on the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third thing that we can vary is the number of iterations. Neural network
    training involves making a series of adjustments to the weights and thresholds
    until no further improvement is achievable. The more iterations that we carry
    out, the longer training takes, but the accuracy tends to start to flatten out
    before the best possible result is found. The following chart shows how the training
    time and the Jaccard score vary as the number of iterations increases for SEM4-EN.
    There were no further improvements after 1,800 iterations for this dataset, so
    we stopped the plot at this point. Unsurprisingly, the training time goes up linearly
    with the number of iterations, whereas the Jaccard score starts to level off at
    around 1,400 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Jaccard score and training time versus iterations with the SEM4-EN
    dataset](img/B18714_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Jaccard score and training time versus iterations with the SEM4-EN
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: Varying the size of the training data, the number of input features, and the
    number of iterations affects the scores and the training time. While you are developing
    a model and are trying out different combinations of parameters, settings, and
    preprocessing steps, it is certainly worth doing some preliminary investigations
    to find a set of values for these factors at which the Jaccard score appears to
    be leveling off. But in the end, you just have to grit your teeth and train the
    model using a large amount of training data, a large dictionary, and a large number
    of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen that if we are prepared to wait, using an SNN can produce, at least
    in some cases, better results than any of the previous algorithms. For a lot of
    problems, adding extra hidden layers can produce better results than networks
    with just an input layer and an output layer. Will this help with our current
    task?
  prefs: []
  type: TYPE_NORMAL
- en: 'SNNs compute very similar information to that calculated by Naïve Bayes and
    SVMs. The links between input and output nodes carry information about how strongly
    the input nodes (that is, words) are correlated to the output nodes (that is,
    emotions) and how the biases roughly carry information about how likely the given
    output is. The following tables show the links between several common words and
    emotions after training on the CARER dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |'
  prefs: []
  type: TYPE_TB
- en: '| the | -0.036 | -0.065 | 0.031 | 0.046 | -0.015 | 0.036 |'
  prefs: []
  type: TYPE_TB
- en: '| sorrow | 0.002 | -0.028 | -0.098 | 0.098 | 0.063 | 0.020 |'
  prefs: []
  type: TYPE_TB
- en: '| scared | -0.356 | 1.792 | -0.683 | -0.283 | -0.562 | 0.057 |'
  prefs: []
  type: TYPE_TB
- en: '| happy | -0.090 | -0.161 | 0.936 | -0.332 | -0.191 | -0.156 |'
  prefs: []
  type: TYPE_TB
- en: '| disgusting | 0.048 | -0.014 | -0.031 | -0.045 | 0.020 | -0.000 |'
  prefs: []
  type: TYPE_TB
- en: '| and | -0.001 | -0.033 | 0.014 | 0.015 | -0.031 | 0.022 |'
  prefs: []
  type: TYPE_TB
- en: '| adoring | -0.054 | -0.034 | -0.110 | 0.218 | -0.085 | 0.007 |'
  prefs: []
  type: TYPE_TB
- en: '| irritated | 1.727 | -0.249 | -0.558 | -0.183 | -0.621 | -0.124 |'
  prefs: []
  type: TYPE_TB
- en: '| kisses | -0.004 | -0.041 | -0.041 | 0.120 | -0.038 | -0.001 |'
  prefs: []
  type: TYPE_TB
- en: Figure 8.6 – Links between words and emotions in the CARER-EN dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table displays the words with the strongest and weakest connections
    to the emotions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| anger | offended, greedy, rushed, resentful, selfish ... passionate, supporting,
    strange, amazing, weird |'
  prefs: []
  type: TYPE_TB
- en: '| fear | unsure, reluctant, shaky, insecure, vulnerable ... shocked, supporting,
    sweet, stressed |'
  prefs: []
  type: TYPE_TB
- en: '| joy | smug, sincere, invigorated, joyful, positive ... helpless, agitated,
    weird, strange, overwhelmed |'
  prefs: []
  type: TYPE_TB
- en: '| love | horny, sympathetic, gentle, naughty, liked ... amazing, overwhelmed,
    hated, strange, weird |'
  prefs: []
  type: TYPE_TB
- en: '| sadness | burdened, homesick, disturbed, rotten, guilty ... sweet, agitated,
    weird, strange |'
  prefs: []
  type: TYPE_TB
- en: '| surprise | impressed, shocked, amazed, surprised, curious ... feelings, don,
    very, being, or |'
  prefs: []
  type: TYPE_TB
- en: Figure 8.7 – Strongest and weakest words for each emotion in the CARER-EN dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a set of input words (a tweet!), the neural network calculates the sum
    of the links from those words to each emotion and compares it with the threshold
    (different implementations of neural networks carry out slightly different calculations:
    the one used here, **rectified linear activation** (Fukushima, 1969), calculates
    the weighted sum of the inputs and the bias but then sets it to zero if it is
    negative). This is very similar to what all the other algorithms do – SVMs also
    calculate a weighted sum of the inputs but do not reset negative outcomes to zero;
    the lexicon-based algorithms also just calculate a weighted sum but since none
    of the weights are negative, the total cannot be less than zero, so there is no
    need to reset them. Naïve Bayes combines the conditional probabilities of the
    various observed events to produce an overall probability. What they all have
    in common is that a single word *always* makes the same contribution to the total.
    This may not always be true:'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are words whose sole job is to change the meaning of other words. Consider
    the word *happy*. This word is linked to **joy** rather than to any of the other
    emotions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  | **anger** | **fear** | **joy** | **sadness** |'
  prefs: []
  type: TYPE_TB
- en: '| happy | -0.077 | -0.159 | 0.320 | -0.048 |'
  prefs: []
  type: TYPE_TB
- en: Figure 8.8 – Links between happy and the four emotions from SEM4-EN
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweets where *happy* appears next to *not*, however, do not express joy:'
  prefs: []
  type: TYPE_NORMAL
- en: '*This is kind of screwed up , but my brother is about to join the police academy
    . . . . and I ‘ m not happy about . And I ‘m not the* *only one.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Yay bmth canceled Melbourne show fanpoxytastic just lost a days pay and hotel
    fees not happy atm # sad #* *angry*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I was just put on hold for 20 minutes till I hung up . # not happy # terribleservice
    # unhappy @ virginmedia I should have stayed . . .*'
  prefs: []
  type: TYPE_NORMAL
- en: The presence of *not* changes the meaning of these tweets so that they express
    something other than joy.
  prefs: []
  type: TYPE_NORMAL
- en: Not all words that affect the meanings of other words are as easy to identify
    as *not*, particularly in informal texts where abbreviations such as *don’t* and
    *can’t* are very common, but there are certainly others that do something like
    this. It is also important to note that the word whose meaning is being affected
    by the modifying term may not be adjacent to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some words form compounds that express meanings that are not straightforwardly
    related to the meanings of the individual words in isolation. We saw this with
    Chinese compounds earlier, but even English words can do this. Using pointwise
    mutual information to find compounds (as in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*,
    Sentiment Lexicons and Vector Space Models*), we find that *supporting cast* and
    *sweet potatoes* occur more often in the CARER-EN dataset than you would expect
    given the distributions of the individual words – that is, these terms may be
    viewed as compounds. The weights for the individual words are given in the following
    table, with *supporting* and *sweet* both having strong links to **love** and
    slightly weaker links to **joy**. Neither of the compound words would be expected
    to have these links – there is nothing particularly lovely or joyous about sweet
    potatoes! It is not possible to capture the fact that these words make a different
    contribution to the overall emotional charge of the texts containing them when
    they co-occur with *cast* and *potatoes* using an SNN or indeed any of the earlier
    algorithms:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |'
  prefs: []
  type: TYPE_TB
- en: '| supporting | -0.183 | -0.154 | 0.220 | 0.515 | -0.319 | -0.043 |'
  prefs: []
  type: TYPE_TB
- en: '| cast | -0.015 | 0.017 | -0.012 | 0.003 | 0.006 | -0.009 |'
  prefs: []
  type: TYPE_TB
- en: '| sweet | -0.177 | -0.187 | 0.207 | 0.553 | -0.371 | -0.079 |'
  prefs: []
  type: TYPE_TB
- en: '| potatoes | -0.009 | 0.003 | 0.004 | 0.003 | -0.020 | -0.019 |'
  prefs: []
  type: TYPE_TB
- en: Figure 8.8 – Weights for individual words that can occur as compounds
  prefs: []
  type: TYPE_NORMAL
- en: 'Some words are simply ambiguous, with one interpretation carrying one emotional
    charge and another carrying a different one (or none). It is extremely difficult
    to detect that a word is ambiguous simply by looking at texts that contain it,
    and even more difficult to detect how many interpretations it has, and even if
    you do know how many interpretations a word has, you still have to decide which
    one is intended in a given text. So, inferring what emotional charge each interpretation
    has and then deciding which interpretation is intended is more or less impossible.
    However, in some cases, we can see, as with the cases of compounds previously,
    that two words are cooccurring unexpectedly often, and in such cases, we can be
    reasonably sure that the same interpretations are intended in each case. *Feel
    like* and *looks like*, for instance, occur more often in the SEM4-EN data than
    they should: both of these could be ambiguous, with the different meanings carrying
    different emotional charges. But it seems very likely that in each occurrence
    of *feel like* the same interpretations of *feel* and *like* are intended – as
    it happens, the interpretation of *like* in these phrases is not the one that
    is closely linked to **love**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the algorithms that we have seen so far, including SNNs, treat the contributions
    made by individual words atomistically – they all compute a score for each word
    for each emotion, and they then combine the scores using some fairly simple arithmetical
    calculation. Therefore, they *cannot* be sensitive to the issues raised here.
  prefs: []
  type: TYPE_NORMAL
- en: Adding extra layers to our neural network will enable us to handle these phenomena.
    The simplest demonstration of how adding layers will allow a neural network to
    compute something that cannot be dealt with by an SNN involves the XOR function,
    where we have two inputs and we want to get a response if one, but not both, of
    the inputs is on.
  prefs: []
  type: TYPE_NORMAL
- en: 'This cannot be done with an SNN. We will explore the reasons for this and the
    way that DNNs overcome this limitation by considering a set of made-up tweets
    consisting solely of the words *love*, *like*, *hate*, and *shock* and the emotions
    *anger*, *joy*, and *surprise*, as shown in *Figure 8**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **tweet** | **joy** | **anger** | **surprise** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | love | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | like | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | love like | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | hate | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | shock | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | love | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | like | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | love like | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | hate | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | shock | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Figure 8.9 – Straightforward training data
  prefs: []
  type: TYPE_NORMAL
- en: 'If we train an SNN on this data, we will get the following network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – An SNN for surprise, anger, and joy, with straightforward training
    data](img/B18714_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – An SNN for surprise, anger, and joy, with straightforward training
    data
  prefs: []
  type: TYPE_NORMAL
- en: 'The strongest link from *hate* is to **anger**, the strongest link from *shock*
    is to **surprise**, and the strongest links from *love* and *like* are to **joy**.
    So, if a tweet consists of one of these words, it will trigger the appropriate
    emotion. If a tweet contains both *love* and *like*, it will also trigger **joy**,
    but the training data says nothing about what should happen if a tweet consists
    of, for instance, *shock* and *like* or *shock* and *hate*. Looking at the network,
    we can see that *hate* votes quite strongly for **anger** and *shock* votes by
    about the same amount for **surprise**, but that *shock* votes much more strongly
    *against* **anger** than *hate* does against **surprise**. So, overall, *shock*
    and *hate* vote for **surprise**. There is nothing meaningful going on here: the
    network is initialized with random values, and these spill over into random decisions
    about configurations of features that have not been seen in the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As noted previously, our SNN carries out essentially the same operations as
    an SVM: if the weights on the connections between a set of input nodes, ![<mml:math  ><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/19.png),
    and an output node, ![<mml:math  ><mml:mi>O</mml:mi></mml:math>](img/20.png),
    are ![<mml:math  ><mml:mi>w</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/21.png)![<mml:math  ><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/22.png)
    and the bias for the output node is ![<mml:math  ><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/23.png),
    then if the input values for ![<mml:math  ><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/24.png)
    are v(N 1), ..., v(N k), then the excitation of the output node is determined
    by ![<mml:math  ><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/25.png)![<mml:math  ><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/26.png)
    – the excitation of the output node will be ![<mml:math  ><mml:mn>0</mml:mn></mml:math>](img/27.png)
    if this sum is negative and on to some degree if it is positive. ![<mml:math  ><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/28.png)
    determines a hyperplane that divides points into two classes, ![<mml:math  ><mml:mi>O</mml:mi></mml:math>](img/29.png)
    or ![<mml:math  ><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/30.png),
    just like the coefficients in SVMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But that means that an SNN cannot classify if the classes are *not* linearly
    separable. The classic example here is the XOR function – that is, examples where
    each of two features in isolation denotes a specific class but the two together
    do not – that is, *XOR(0, 0)=0*, *XOR(0, 1)=1*, *XOR(1, 0)=1*, and *XOR(1, 1)=0*.
    It is easy enough to draw this function and show that it looks as though no line
    separates the 0 and 1 cases. In *Figure 8**.11*, the red circles (at (0, 0) and
    (1, 1)) represent the cases where XOR is 0, and the blue diamonds (at (1, 0) and
    (0, 1)) represent where XOR is 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – XOR – the blue diamonds and red circles cannot be separated
    by a straight line](img/B18714_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – XOR – the blue diamonds and red circles cannot be separated by
    a straight line
  prefs: []
  type: TYPE_NORMAL
- en: It looks as though it would be impossible to draw a line that divides the blue
    diamonds and red circles – that is, these two classes look as though they are
    not linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: For formal proof of this, assume that there is such a line. It will have an
    equation such as ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/31.png)
    where, for any point that is above the line, ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/32.png)
    , and for any point below the line, ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/33.png)
    (if B is positive and vice versa if B is negative).
  prefs: []
  type: TYPE_NORMAL
- en: Given our four points, let’s assume that the red circles are both below the
    line and the blue diamonds are above it and B is positive. Then, for the red circle
    at (0, 0), we would have ![<mml:math  ><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/34.png),
    since putting 0 for each of *x* and *y* would give us ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>×</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/35.png)),
    which makes C<0\. Similarly, for the red circle at (1, 1), substituting 1 for
    each of *x* and *y* would give us ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/36.png),
    for the blue diamond at (1, 0) substituting 1 for *x* and 0 for *y* would give
    us ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/37.png),
    and for the blue diamond at (1, 0), we would get ![<mml:math  ><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/38.png).
  prefs: []
  type: TYPE_NORMAL
- en: From ![<mml:math  ><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/39.png)
    and ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/40.png),
    we get ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/41.png),
    and likewise from ![<mml:math  ><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/42.png)
    and ![<mml:math  ><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/43.png),
    we get ![<mml:math  ><mml:mi>B</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/44.png).
    But then ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:math>](img/45.png),
    so since ![<mml:math  ><mml:mfenced separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/46.png),
    then ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:math>](img/47.png),
    contradicting the observation that ![<mml:math  ><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo><</mml:mo><mml:mn>0</mml:mn></mml:math>](img/48.png)
    for the point (1, 1). We do have to consider the possibility that the blue diamonds
    are both above the line and the red circles are below, or that B is negative,
    but an exactly parallel argument rules these out in the same way – there is no
    way to draw a straight line that separates the blue diamonds and red circles.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does this mean for our task? Suppose we adjust our made-up data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **tweet** | **joy** | **anger** | **Surprise** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | love | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | like | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | love like | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | hate | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | shock | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | love | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | like | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | love like | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | hate | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Figure 8.12 – Difficult training data
  prefs: []
  type: TYPE_NORMAL
- en: The only change we have made is that we have made the tweets that contain *love*
    and *like* express **anger** rather than **joy**. This is much like the situation
    with XOR previously, where two features express one emotion when they appear in
    isolation and a different one when they appear together. There is no exact parallel
    to the (0, 0) point from XOR, but in the cases where neither was present, then
    the target is either **anger** (if the tweet was just the word *hate*) or **surprise**
    (if the tweet was just the word *shock*) – that is, when neither *love* nor *like*
    is present, then the tweets do not express **joy**.
  prefs: []
  type: TYPE_NORMAL
- en: When we train on this data, we will find that the SNN cannot be relied on to
    find weights that assign the right emotions. Sometimes it does, sometimes not.
    The problem is not that there is no set of weights that will assign the correct
    labels. Over a run of 10 folds of 90% training/10% testing, weights that split
    the data correctly were found in two cases, but in eight cases, the classifier
    assigned the wrong emotion to tweets containing both *like* and *love*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the incorrectly trained network shown here, the scores for joy, anger, and
    surprise for tweets containing both these words were 0.35, -0.25, and -3.99, with
    joy the clear winner. The data *is* linearly separable since the correctly trained
    classifier does separate the data into the right classes by using hyperplanes
    defined by the connection weights and biases; however, the gradient descent process
    can easily get stuck in local minima, doing the best it can with the single-word
    tweets but unable to find the correct weights for the compound ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Correctly trained network](img/B18714_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Correctly trained network
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Incorrectly trained SNN](img/B18714_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Incorrectly trained SNN
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we have two kinds of problems:'
  prefs: []
  type: TYPE_NORMAL
- en: If the data is not linearly separable, then no SNN can classify it correctly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if it can be divided by a set of hyperplanes, SNNs can easily get stuck
    in local minima, doing the best they can with most of the data but unable to find
    the right weights for cases where words have a different effect when they occur
    together from the effect they would have in isolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can solve the first problem by adding extra layers. In order, for instance,
    to calculate XOR, you need a node in the network that is turned on when both the
    input nodes are on and has a negative link to the output. A simple feedforward
    network should have the nodes in the input layer connected to nodes in the first
    hidden layer and then nodes in the first hidden layer connected to nodes in the
    second hidden layer and so on until the nodes in the last hidden layer are connected
    to nodes in the output layer. You need at least one hidden layer with at least
    three nodes in it. However, as we have seen, networks can quite easily get stuck
    in local minima, and the smallest configuration that can be reliably trained to
    recognize XOR has a single hidden layer with five nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – DNN trained to classify tweets with either happy or like, but
    not both, as joy](img/B18714_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – DNN trained to classify tweets with either happy or like, but
    not both, as joy
  prefs: []
  type: TYPE_NORMAL
- en: If *like* is on but *happy* is off, then the second hidden node will be on with
    a score of 1.22 (i.e. -0.766+1.988), which will propagate through to the output
    node as -2.96 (1.22*-2.46). This will then be added to the bias of the output
    node to produce -2.15\. If *happy* is on but *like* is not, then the second and
    fourth hidden nodes will be on, with scores of 0.76 and 1.23, which will propagate
    through to the output node as -0.766*0.76 (for the second hidden node) + -1.309*1.23,
    which when added to the bias for the output node becomes -2.66\. If both the input
    nodes are on, then none of the hidden nodes will be, so the score at the output
    node is just its own bias – that is, 0.81\. For networks with just one output,
    the standard logistic function used for interpreting the final score treats negative
    numbers as being on and positive ones as off, so this network classifies tweets
    that contain just *like* or *happy* as expressing joy and ones that contain both
    as not expressing it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding hidden units will let the network recognize significant combinations
    of input features as being non-compositional, in that the effect of the combination
    is not just the cumulative effect of the features themselves. We can also see
    that if you do not have enough hidden features, then the training process can
    get stuck in local minima – although you *can* compute XOR using just three features
    in the hidden layer, it is very difficult to train such a network for this task
    (see (Minsky & Papert, 1969) for further discussion of this issue). This is not
    just a matter of not having enough data, or of not allowing the network to train
    for long enough. Networks with hidden layers with three nodes converge very quickly
    (after about 10 or 12 epochs) and ones with four just take a few hundred. We can
    also add extra layers – networks with two hidden layers with four and three nodes
    each can also solve this problem and typically converge slightly more quickly
    than ones with one hidden layer with five nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Network with two hidden layers for solving XOR](img/B18714_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Network with two hidden layers for solving XOR
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that setting the initial weights and biases at random in small
    networks nearly always leaves you in an area of the search space where you will
    end up in a local minimum. Using larger networks, on the other hand, nearly always
    produces networks that can solve the problem since there will be nodes that are
    in the right part of the search space that can be given increasing significance,
    but they take much longer to train. So, the key task is to find the appropriate
    number of hidden units.
  prefs: []
  type: TYPE_NORMAL
- en: The role of a hidden unit is to find words that feed differently into the output
    nodes when they occur in isolation and when they occur in combinations with other
    words. The two key parameters here seem likely to be the number of input features
    (that is, the number of distinct words in the data) and the number of output classes
    (that is, the number of emotions). If there are more words in the lexicon, then
    there are more possible combinations of words, which might mean that having more
    words requires more hidden nodes. If there are more output classes, then there
    are more places where having combinations of words might be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that there are many thousands of words in the lexicons for our datasets,
    but only four to eleven emotions, it seems sensible to start by investigating
    the effect of relating the number of hidden nodes to the number of emotions. *Figure
    8**.17* shows what happens when we have a hidden layer with 0.5 times as many
    nodes as there are emotions, 1 times as many, or 1.5 times as many:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Jaccard score versus the number of hidden nodes = F*number
    of emotions, F from 0.5 to 5](img/B18714_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Jaccard score versus the number of hidden nodes = F*number of
    emotions, F from 0.5 to 5
  prefs: []
  type: TYPE_NORMAL
- en: 'For the three datasets for which we obtained quite good results with an SNN,
    the effect of adding a hidden layer with a moderate number of nodes is substantial.
    The original scores have been repeated here for ease of reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-EN | 0.902 | 0.902 | 0.902 | 0.902 | 0.822 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.648 | 0.275 | 0.386 | 0.388 | 0.239 |'
  prefs: []
  type: TYPE_TB
- en: '| WASSA-EN | 0.837 | 0.837 | 0.837 | 0.837 | 0.720 |'
  prefs: []
  type: TYPE_TB
- en: '| CARER-EN | 0.901 | 0.901 | 0.901 | 0.901 | 0.820 |'
  prefs: []
  type: TYPE_TB
- en: Figure 8.18 – Simple neural network applied to the standard English datasets
  prefs: []
  type: TYPE_NORMAL
- en: The original Jaccard score for CARER-EN was 0.77, which is equivalent to around
    0.87 accuracy; when we add a hidden layer with half as many nodes as the number
    of emotions in CARER (that is, with just three nodes in the hidden layer since
    CARER has six emotions), we get a better score (Jaccard 0.79, accuracy 0.89) than
    in the original, and then as we increase the number of hidden nodes to 6, 9, 12,
    we get a very gradual improvement, up to the point where the score seems to have
    flattened out and maybe even started to overtrain.
  prefs: []
  type: TYPE_NORMAL
- en: 'A similar, but more marked, pattern occurs with SEM4-EN and WASSA-EN. In these
    cases, the score starts fairly low when we only have half as many nodes in the
    hidden layer as we have emotions (that is, just two for both of these), but then
    leaps up considerably higher than the original scores as soon as we have as many
    nodes in the hidden layer as we have emotions and then flattens out at around
    Jaccard 0.875 (accuracy 0.93) for SEM4-EN and Jaccard 0.81 (accuracy 0.9) for
    WASSA-EN. In general, it looks as though adding a hidden layer with a modest number
    of nodes produces some improvement over the basic neural network with no hidden
    units, but experiments with more hidden layers or a single hidden layer with more
    nodes suggest that these improvements are fairly limited. This is likely to be
    because hidden layers look for non-compositional combinations of words. There
    are two possible reasons why this has limited effects:'
  prefs: []
  type: TYPE_NORMAL
- en: There simply are not all that many cases of words whose emotional weight changes
    when they co-occur with specific partners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where there are such combinations, their frequency in the data is not enough
    to override their normal interpretation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may be that using much more training data makes using networks with multiple
    or large hidden layers more effective, but with modest-sized datasets, doing so
    has comparatively little effect.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at using neural networks for our task of identifying
    the emotions expressed in informal communications such as tweets. We examined
    the way that the lexicon for the datasets is used as the nodes in the input layer
    and looked at how the weights associated with individual words reflect the emotional
    significance of those words. We considered simple neural networks with no hidden
    layers and also slightly deeper ones with a single hidden layer with slightly
    more nodes than the set of output nodes – the performance of the neural network
    flattened out once the hidden layer contained 1.5 to 2 times as many nodes as
    the output layer, so there seemed little point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The highest-scoring algorithms for the various datasets are now as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX (****unstemmed)** | **LEX (****stemmed)** | **CP (****stemmed)**
    | **NB (****multi)** | **SVM (****single)** | **MULTI-SVM** | **SNN (****single)**
    | **DNN** |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-EN | 0.503 | 0.497 | 0.593 | 0.778 | 0.845 |  | 0.829 | *******0.847***
    |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.347 | 0.348 | 0.353 | 0.267 | 0.224 | *******0.385*** | 0.242
    | 0.246 |'
  prefs: []
  type: TYPE_TB
- en: '| WASSA-EN | 0.445 | 0.437 | 0.505 | 0.707 | *******0.770*** |  | 0.737 | 0.752
    |'
  prefs: []
  type: TYPE_TB
- en: '| CARER-EN | 0.350 | 0.350 | 0.395 | 0.774 | 0.770 |  | *******0.820*** | 0.804
    |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB-EN | 0.722 | 0.667 | 0.722 | 0.740 | 0.736 |  | *******0.793*** | *******0.793***
    |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-AR | 0.506 | 0.509 | 0.513 | *******0.532*** | 0.514 |  | 0.504 | 0.444
    |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.378 | *******0.386*** | 0.382 | 0.274 | 0.216 | 0.340 | 0.221
    | 0.207 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | *******0.687*** | 0.663 | 0.666 | 0.507 | 0.631 | 0.341 | 0.028
    | 0.026 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-ES | *******0.425*** | 0.420 | 0.177 | 0.331 | 0.412 |  | 0.337 | 0.343
    |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.269 | 0.271 | *******0.278*** | 0.255 | 0.226 | 0.268 | 0.221
    | 0.222 |'
  prefs: []
  type: TYPE_TB
- en: Figure 8.19 – Scores for the best algorithms so far
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks produced the best results in 4 of the 10 datasets, but the
    simple lexical algorithms are still the best for the multi-label datasets. The
    general lesson remains the same as it was at the end of [*Chapter 7*](B18714_07.xhtml#_idTextAnchor144),
    *Support Vector Machines* – you shouldn’t just accept that there is a single best
    classification algorithm: do experiments, try out variations, and see for yourself
    what works best with your data.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Fukushima, K. (1969). *Visual Feature Extraction by a Multilayered Network of
    Analog Threshold Elements*. IEEE Transactions on Systems Science and Cybernetics,
    5(4), 322–333\. [https://doi.org/10.1109/TSSC.1969.300225](https://doi.org/10.1109/TSSC.1969.300225).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hebb, D. O. (1949). *The organization of behavior: A neuropsychological* *theory*.
    Wiley.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton, G. E., Osindero, S., & Teh, Y.-W. (2006). *A Fast Learning Algorithm
    for Deep Belief Nets*. Neural Comput., 18(7), 1527–1554\. [https://doi.org/10.1162/neco.2006.18.7.1527](https://doi.org/10.1162/neco.2006.18.7.1527).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCulloch, W. S., & Pitts, W. (1943). *A logical calculus of the ideas immanent
    in nervous activity*. The Bulletin of Mathematical Biophysics, 5(4), 115–133\.
    [https://doi.org/10.1007/BF02478259](https://doi.org/10.1007/BF02478259).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minsky, M., & Papert, S. (1969). *Perceptrons*. MIT Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). *Learning representations
    by back-propagating errors*. Nature, 323(6088), 533–536\. [https://doi.org/10.1038/323533a0](https://doi.org/10.1038/323533a0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
