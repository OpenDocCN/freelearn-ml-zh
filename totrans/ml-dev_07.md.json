["```py\nUnfold the network to contain k instances of the cell\nWhile (error < Îµ or iteration>max):\n    x = zeros(sequence_legth)\n    for t in range (0, n-sequence_length)  # initialize the weights \n        copy sequence_length input values into the input x\n        p = (forward-propagate the inputs over the whole unfolded network)\n        e = y[t+k] - p;           # calculate error as target - prediction\n        Back-propagate the error e, back across the whole unfolded network\n        Sum the weight changes in the k model instances together.\n            Update all the weights in f and g.\n            x = f(x, a[t]);    # compute new input for the next time-step\n```", "```py\n%matplotlib inline\n%config InlineBackend.figure_formats = {'png', 'retina'}\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\nfrom keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Dropout\n\nUsing TensorFlow backend.\n```", "```py\ndf = pd.read_csv(\"data/elec_load.csv\", error_bad_lines=False)\nplt.subplot()\nplot_test, = plt.plot(df.values[:1500], label='Load')\nplt.legend(handles=[plot_test])\n```", "```py\nprint(df.describe())\narray=(df.values - 145.33) /338.21\nplt.subplot()\nplot_test, = plt.plot(array[:1500], label='Normalized Load')\nplt.legend(handles=[plot_test])\n\n                Load\ncount  140256.000000\nmean      145.332503\nstd        48.477976\nmin         0.000000\n25%       106.850998\n50%       151.428571\n75%       177.557604\nmax       338.218126\n```", "```py\nlistX = []\nlisty = []\nX={}\ny={}\n\nfor i in range(0,len(array)-6):\n    listX.append(array[i:i+5].reshape([5,1]))\n    listy.append(array[i+6])\n\narrayX=np.array(listX)\narrayy=np.array(listy)\n\nX['train']=arrayX[0:13000]\nX['test']=arrayX[13000:14000]\n\ny['train']=arrayy[0:13000]\ny['test']=arrayy[13000:14000]\n```", "```py\n#Build the model\nmodel = Sequential()\n\nmodel.add(LSTM( units=50, input_shape=(None, 1), return_sequences=True))\n\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM( units=200, input_shape=(None, 100), return_sequences=False))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=1))\nmodel.add(Activation(\"linear\"))\n\nmodel.compile(loss=\"mse\", optimizer=\"rmsprop\")  \n```", "```py\n#Fit the model to the data\n\nmodel.fit(X['train'], y['train'], batch_size=512, epochs=10, validation_split=0.08)\n\nTrain on 11960 samples, validate on 1040 samples\nEpoch 1/10\n11960/11960 [==============================] - 41s - loss: 0.0035 - val_loss: 0.0022\nEpoch 2/10\n11960/11960 [==============================] - 61s - loss: 0.0020 - val_loss: 0.0020\nEpoch 3/10\n11960/11960 [==============================] - 45s - loss: 0.0019 - val_loss: 0.0018\nEpoch 4/10\n11960/11960 [==============================] - 29s - loss: 0.0017 - val_loss: 0.0020\nEpoch 5/10\n11960/11960 [==============================] - 30s - loss: 0.0016 - val_loss: 0.0015\nEpoch 6/10\n11960/11960 [==============================] - 28s - loss: 0.0015 - val_loss: 0.0013\nEpoch 7/10\n11960/11960 [==============================] - 43s - loss: 0.0014 - val_loss: 0.0012\nEpoch 8/10\n11960/11960 [==============================] - 37s - loss: 0.0013 - val_loss: 0.0013\nEpoch 9/10\n11960/11960 [==============================] - 31s - loss: 0.0013 - val_loss: 0.0012\nEpoch 10/10\n11960/11960 [==============================] - 25s - loss: 0.0012 - val_loss: 0.0011\n\n<keras.callbacks.History at 0x7fa435512588>\n```", "```py\n# Rescale the test dataset and predicted data\n\ntest_results = model.predict( X['test'])\n\ntest_results = test_results * 338.21 + 145.33\ny['test'] = y['test'] * 338.21 + 145.33\n\nplt.figure(figsize=(10,15))\nplot_predicted, = plt.plot(test_results, label='predicted')\n\nplot_test, = plt.plot(y['test']  , label='test');\nplt.legend(handles=[plot_predicted, plot_test]);\n```"]