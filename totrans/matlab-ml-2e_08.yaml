- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MATLAB for Image Processing and Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computer vision is a discipline that explores methods for processing, analyzing,
    and comprehending visual data. In the realm of image content analysis, a multitude
    of computer vision algorithms is employed to develop insights into the objects
    depicted in the image. Encompassing diverse facets of image analysis, computer
    vision addresses tasks such as object recognition, shape analysis, pose estimation,
    3D modeling, visual search, and more. While humans excel at identifying and recognizing
    objects in their surroundings, the objective of computer vision is to faithfully
    replicate the capabilities of the **human visual system** (**HVS**) using computational
    methods. In this chapter, we will understand the basic concepts of computer vision
    and how to implement a model for object recognition using MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing image processing and computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring MATLAB tools for computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a MATLAB model for object recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and fine-tuning pretrained deep learning models in MATLAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpreting and explaining machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce basic machine learning concepts. To understand
    these topics, a basic knowledge of algebra and mathematical modeling is needed.
    You will also require a working knowledge of MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: To work with the MATLAB code in this chapter, you’ll need the files available
    on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing image processing and computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Through the five senses, humans gather information from the external world and
    process it, making decisions to carry out the actions that shape their daily lives.
    One of the most intriguing challenges in computer science is replicating this
    sequence of events, identifying and harnessing new sources of information. The
    ability to acquire and interpret information by simulating the human sensory system
    is called machine perception, and it is fundamental in the field of artificial
    intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to interpret and acquire information from the external world is made
    possible through techniques such as encoding and information processing. Through
    digital image encoding techniques, it is possible to represent what humans can
    perceive in the form of bits. Depending on the methodologies used, it is possible
    to select the quantity and quality of the information to be represented. Through
    processing methods, on the other hand, it is possible to interpret the information
    contained in images and attempt to replicate the mechanisms of human decision-making.
    One such human ability is the capacity to recognize, through sight, what types
    of objects are present within a scene, thereby identifying the distinctive characteristics
    of each object. The highest level of information that can be extracted from images
    is done through identifying and recognizing individual objects within a scene.
    This information can be used to categorize and group images based on the objects
    they contain.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding image processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For humans, a fundamental sense is vision. Through digital images, it is possible
    to represent what the HVS captures in an instant numerically. An image is a 2D
    representation of visual perception; it is perceived in the form of electromagnetic
    waves that enter the eye and impact the retina. The elements that make up the
    retina capture information, such as luminance and spectral characteristics. These
    are transformed into nerve signals to be sent, through the optic nerve, to the
    brain structures responsible for visual interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the digital domain, images are commonly represented as an ordered set of
    points and pixels, arranged in rows and columns. This mode of representation is
    called raster or bitmap. Essentially, it involves 2D sampling of a continuous
    signal in two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Image representation as a sequence of pixels. Each pixel has
    a value from 0 (black) to 255 (white)](img/B21156_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Image representation as a sequence of pixels. Each pixel has a
    value from 0 (black) to 255 (white)
  prefs: []
  type: TYPE_NORMAL
- en: The simplest technique for representation is the use of grayscale. In this type
    of representation, pixels contain the amount of luminance. Luminance is a fundamental
    quantity in the visual field and represents the amount of light that reaches the
    observer’s eye. Pixel values range from absence (black) to the maximum level of
    light (white), while intermediate states are perceived as varying shades of gray.
  prefs: []
  type: TYPE_NORMAL
- en: In computer graphics, the pixel is the smallest conventional unit of a digital
    image’s surface. The more pixels an image has, the more information it contains,
    and consequently, our ability to notice details within it increases. The amount
    of information can be indicated by measuring resolution, either in absolute terms
    (pixels) or concerning physical measurements (**dots per** **inch** (**dpi**)).
  prefs: []
  type: TYPE_NORMAL
- en: 'When deciding to change the resolution of an image, two situations can arise:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Constant pixel size**: The number of pixels that make up the image is reduced,
    resulting in the image shrinking in size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increase in pixel size**: The pixels (dpi) are reduced in size, while the
    overall dimensions of the image remain constant, causing the individual pixel
    size to increase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An important aspect of digital images is how to represent the information contained
    in their pixels. In the case of grayscale, it represents luminance, which needs
    to be quantized by representing it in a finite number of bits. The greater the
    number of bits, the lower the quantization noise. Using *b* bits, there are 2b
    possible values. Typically, 8 bits is the most common value, allowing for a total
    of 256 levels of luminance to be represented. It has been demonstrated that 8
    bits provide an acceptable representation of grayscale gradients in most applications.
    This quantity effectively adapts to the HVS’s ability to distinguish different
    luminance levels in the image.
  prefs: []
  type: TYPE_NORMAL
- en: This holds for grayscale images that contain only luminance information for
    each pixel. Introducing color increases the complexity of representation because
    it requires the use of a model to represent it. This model must be able to capture
    the chromatic information significant to the HVS and translate it into numbers.
    The goal is to obtain a vector of numbers that “summarizes” the frequencies contained
    in the electromagnetic wave for each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most commonly used model in the digital domain is undoubtedly RGB. It is
    based on the combination of three chromatic components with different intensities:
    red, green, and blue.'
  prefs: []
  type: TYPE_NORMAL
- en: These components roughly correspond to the three types of cones in the human
    retina. Therefore, it is not necessary to represent all the color information
    that exists in the real world, but only the information to which the HVS is sensitive.
    The information that’s carried by this electromagnetic wave corresponds to the
    light that hits the organs inside the retina; hence, the color corresponds to
    the spectrum of the electromagnetic signal. Therefore, to represent the signal,
    only the three components related to the red, green, and blue colors, known as
    primary colors, which coincide with the luminance in three different frequency
    bands, are required. The RGB model represents how much energy is present in the
    spectral bands corresponding to the primary colors and provides this information
    in the form of three distinct values or components.
  prefs: []
  type: TYPE_NORMAL
- en: Once you understand how it’s possible to represent a digital image numerically,
    it’s necessary to know how to process it to facilitate its representation and
    extract relevant information. Image processing techniques can leverage digital
    transformation algorithms that modify the pixels of the original image, resulting
    in a new one. However, they also include techniques that extract numerical or
    tabular values from the image, representing a particular feature of it. Depending
    on their complexity, these techniques can be placed into different categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest processing methods are those that pertain to the transformation
    of individual pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Grayscale conversion**: This is a type of transformation that allows you
    to transition from the RGB model to grayscale. There is a linear relationship
    between luminance and the three chromatic components of the RGB model. This conversion
    allows us to have simpler pixels to manage and process in subsequent operations.
    Each pixel will have only one value corresponding to luminance rather than three
    values, one for each chromatic component (R, G, B) (*Figure 8**.2*):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "![Figure 8.2 – Grayscale conversion of the Flavian \uFEFFamphitheatre](img/B21156_08_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Grayscale conversion of the Flavian amphitheatre
  prefs: []
  type: TYPE_NORMAL
- en: '**Thresholding**: This is a very useful transformation in the image segmentation
    phase, where the goal is to isolate an object of interest from the background.
    The idea is to set pixels above a certain threshold value equal to the maximum
    luminance intensity value, while pixels below the threshold are set to the minimum
    intensity value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aliasing**: This is an effect that makes two different signals that are indistinguishable
    during sampling. Aliasing occurs when sampling or interpolation produces a lower
    resolution in the image, distorting the output compared to the original signal.
    Anti-aliasing filters can be used to correct this problem. In the case of a digital
    image, aliasing manifests as a moiré effect or a wavy pattern.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, there are the direct comparison and feature extraction methods. The first
    group of techniques is used to compare two images pixel by pixel, obtaining a
    value that measures the discrepancy between them. The second, on the other hand,
    allows for creating a summary of the initial image and using a smaller dataset
    that still adequately describes the original set. Let’s see something:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct comparison methods**: One of the most interesting pieces of information
    that can be derived from images is their degree of similarity. The human brain
    can process the information contained in visual perceptions that arrive through
    the retina. There are dedicated neurons for interpreting visual information (shape,
    color, motion, space, lines, and so on). Recognition, for example, of faces and
    objects, occurs only after this information is extracted and memory is accessed.
    One type of recognition methodology is the direct approach. This method of comparing
    images is also called brute force as it involves comparing each pixel in the two
    images. These methodologies apply algebraic formulas and calculate a discrepancy
    index that indicates the degree of similarity between two images. If we have two
    images called *image1* and *image2*, we can calculate the discrepancy index using
    the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: discrepancy _ index = 1 − sum(sum(abs(image1 − image2))) / sum(sum(image1))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Mean squared error** (**MSE**): This is a value that indicates, in an absolute
    sense, how similar two images are. This index compares the images pixel by pixel
    and represents the average discrepancy of these values. The closer the MSE value
    is to 0, the greater the similarity between the analyzed images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structural similarity index measure** (**SSIM**): Another method of direct
    comparison is SSIM. The difference compared to MSE is that the latter makes estimates
    using absolute errors. SSIM, on the other hand, is a perception-based model that
    considers image degradation as a change in the perception of structural information.
    Instead of performing a pixel-by-pixel comparison, the algorithm divides the image
    into grids of N x N pixels. Within each grid, an average value of the pixels is
    calculated, allowing for relationships between neighboring pixels to be considered
    rather than the absolute value of a single pixel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Direct comparison methodologies come with a significant cost associated with
    the amount of data to process as they consider all the information contained in
    the image. Through some techniques, it’s possible to reduce dimensionality. When
    there’s an excessive amount of data to process and the possibility of redundancy,
    a transformation can be applied, adopting a reduced representation of the data.
    This reduced representation is nothing but a set of features. The process that
    transforms the input data into the feature set is called **feature extraction**.
  prefs: []
  type: TYPE_NORMAL
- en: The chosen characteristics encompass pertinent details from the input data,
    enabling the intended task to be accomplished by utilizing this condensed representation
    rather than the entirety of the original data. This method minimizes the cost
    and resources needed for an accurate depiction of a vast dataset. When tackling
    intricate data analysis, a primary hurdle involves diminishing the number of variables
    in play. Analyzing a large number of variables usually translates into high memory
    usage and computational power requirements. Moreover, when applying classification
    algorithms, there is a risk of **overfitting**. In this case, the model adapts
    too closely to the dataset that was used for learning, failing to generalize and
    thus losing effectiveness. Feature extraction is a broad term that’s used to describe
    methods of creating variable combinations aimed at addressing these issues while
    maintaining sufficient accuracy in depicting the data. Now that we’ve introduced
    the most widespread image processing methodologies, we can focus on how to extract
    knowledge from images.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining computer vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computer vision is an interdisciplinary field of computer science and artificial
    intelligence that deals with the development of algorithms, models, and computer
    systems capable of interpreting, understanding, and analyzing visual information
    from images or videos. The main goal of computer vision is to replicate some of
    the capabilities of the HVS, allowing computers to perceive and understand the
    world around them through visual data.
  prefs: []
  type: TYPE_NORMAL
- en: This field of study focuses on a wide range of tasks, including object recognition,
    motion detection, pattern recognition, information extraction from images, image
    segmentation, 3D reconstruction, and much more. Computer vision has applications
    in a wide range of industries, including medicine, automotive, security, manufacturing,
    retail, entertainment, and robotics, to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: Recent developments in computer vision have been driven by the use of deep neural
    networks, in particular **convolutional neural networks** (**CNNs**), which have
    delivered exceptional results in many visual recognition tasks. These advances
    are fundamentally changing the way machines can interact with the visual world,
    opening up new possibilities in areas such as autonomous driving, medical diagnosis,
    augmented reality, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: 'This task is the most complex in terms of abstraction. The most common use
    of this category is for object recognition. Conceptually, the process can be divided
    into two steps. The first step involves defining an object of interest according
    to a model using feature extraction techniques. In the second step, the object
    is searched for within an image. These types of transformations require the use
    of machine learning and data mining algorithms that, through a dataset, allow
    a model to be constructed for the object to be searched for. Subsequently, it
    can be determined whether there are pixels within the image that match the previously
    created model. There are different approaches to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shape matching**: This is an approach that involves searching for a silhouette.
    This method allows you to measure the similarity between shapes and identify correspondences
    between points that belong to the contour of the object being searched for. The
    basic idea is to select *n* points on the contour of the silhouette. For each
    point, the n - 1 vectors connecting it to all the others are considered. A set
    of all these vectors forms a complex descriptor of the silhouette localized at
    that point. This set of vectors is obtained through a shape extraction process,
    which is part of feature extraction. The idea is to obtain a descriptor using
    these vectors and use it to identify a similar shape within other images and perform
    classification:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Shape matching](img/B21156_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Shape matching
  prefs: []
  type: TYPE_NORMAL
- en: '**Pattern matching**: This is a technique that involves identifying a specific
    sequence or regularity of data (referred to as a pattern) within a large dataset.
    The field of pattern recognition involves automatically searching for regularities
    within data using computer algorithms and patterns to perform actions such as
    classifying data into different categories. In digital images, the identification
    process involves preparing a pattern and corresponding to a set of pixels that
    describes a specific object of interest or a part of it. Then, a pixel classification
    process is performed within the image to determine whether a group of pixels comparable
    to the pattern being searched for exists or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature-based object recognition**: Through this technique, it is possible
    to create descriptors that represent the typical characteristics of an object.
    Each object has unique features that describe it. If we can identify all these
    features within a dataset, we can assume that the object is present. For example,
    a human face can be modeled based on specific anatomical features such as the
    placement of eyes, nostrils, angles formed by the lips, and so on. The combination
    of these anatomical elements and the vectors that connect them forms a patch model.
    This model represents the ordered set of elements that are sufficient to accurately
    describe an object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s interesting to understand how object recognition technology within images
    has evolved and the impact it has had on society. One of the most fascinating
    cases that leverages image processing techniques is facial recognition. Until
    not long ago, this technology was commonly seen as something straight out of science
    fiction. However, in the last decade, it has not only become a reality but also
    become widely used.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve analyzed the basic concepts connected to image processing and
    computer vision, let’s analyze the necessary tools to address these problems in
    MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring MATLAB tools for computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computer vision encompasses the development of algorithms, techniques, and systems
    that allow computers to acquire, process, analyze, and make decisions based on
    visual data from images and videos. The primary goal of computer vision is to
    enable machines to perform tasks that typically require human visual perception
    and comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision can automate various tasks that would be time-consuming or even
    impossible for humans to perform consistently and at scale. This includes tasks
    such as object detection, image classification, and tracking. When trained and
    configured properly, computer vision algorithms can achieve high levels of accuracy
    in tasks such as image recognition and segmentation. They don’t suffer from fatigue
    or distraction, leading to consistent results. These algorithms can process images
    and videos in real time or near real time, making them suitable for applications
    that require rapid analysis and decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision can also be applied to a wide range of industries and applications,
    from healthcare and automotive to agriculture and manufacturing. It can adapt
    to various domains with appropriate training. These systems can scale easily to
    handle large volumes of data and images, making them suitable for big data applications.
    In applications such as medical imaging, computer vision provides a non-invasive
    way to diagnose and monitor conditions without the need for invasive procedures.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, these algorithms heavily rely on large datasets for training. Insufficient
    or biased training data can lead to poor performance and inaccurate results. Developing
    and fine-tuning computer vision models can be complex and time-consuming. It often
    requires expertise in machine learning, deep learning, and image processing. Deep
    learning models used in computer vision can be computationally intensive and require
    powerful hardware, such as GPUs, for training and inference. Deep learning models,
    especially CNNs, are often perceived as black boxes, posing challenges in interpreting
    the rationale behind their specific decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision systems may struggle to perform well under adverse conditions,
    such as poor lighting, occlusions, or variations in camera angles. The use of
    computer vision in surveillance and facial recognition has raised concerns about
    privacy and potential misuse. These algorithms may adopt biases embedded in their
    training data, resulting in unjust or discriminatory outcomes. Ensuring fairness
    and mitigating bias remains a persistent challenge. While computer vision can
    identify and classify objects, it often lacks a deep understanding of the context
    in which those objects appear, which can limit its usefulness in some scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, computer vision is a powerful and rapidly evolving field with the potential
    to transform many industries. However, it’s essential to be aware of its limitations
    and challenges and to apply it responsibly and ethically, addressing issues related
    to data quality, privacy, and bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'MATLAB provides a variety of tools and functions for computer vision tasks.
    These tools can be found in Computer Vision Toolbox, Image Processing Toolbox,
    and other related toolboxes. Here are some of the key MATLAB tools and functionalities
    for computer vision:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computer Vision Toolbox**: This toolbox is specifically designed for computer
    vision tasks. It includes a wide range of functions and algorithms for image processing,
    feature extraction, object detection and recognition, 3D vision, camera calibration,
    and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image Processing Toolbox**: While not exclusively for computer vision, this
    toolbox is often used in conjunction with Computer Vision Toolbox. It provides
    fundamental image processing functions such as filtering, morphological operations,
    and image enhancement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Camera calibration**: MATLAB offers tools for camera calibration, which is
    essential for mapping 2D image points to 3D world coordinates. This is crucial
    for tasks such as 3D reconstruction and object tracking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object detection and recognition**: MATLAB provides functions and pretrained
    models for object detection and recognition. You can use popular deep learning
    models such as YOLO, SSD, and Faster R-CNN for these tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction**: MATLAB supports feature extraction techniques such
    as **Scale-Invariant Feature Transform** (**SIFT**), **Speeded-Up Robust Features**
    (**SURF**), and **Histogram of Oriented Gradients** (**HOG**) for object detection
    and matching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stereo vision**: MATLAB supports stereo vision techniques for depth estimation
    and 3D reconstruction from stereo camera setups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Motion analysis**: You can perform motion analysis tasks such as optical
    flow estimation, tracking, and motion segmentation using MATLAB functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning and deep learning**: MATLAB integrates with various machine
    learning and deep learning frameworks, making it suitable for training custom
    models for computer vision tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic segmentation**: MATLAB includes tools for semantic segmentation,
    which is the process of labeling each pixel in an image with the class it belongs
    to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Point cloud processing**: For 3D point cloud data, MATLAB provides tools
    for visualization, manipulation, and analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apps**: MATLAB offers interactive apps for tasks such as image labeling,
    camera calibration, and object training, which simplify the development workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel computing**: MATLAB supports parallel computing, allowing you to
    speed up computationally intensive computer vision tasks by leveraging multiple
    CPU cores or GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools and functions make MATLAB a powerful environment for developing
    and prototyping computer vision applications, whether you’re working on image
    analysis, object detection, 3D reconstruction, or any other related tasks. Now,
    let’s learn how to recognize an object using MATLAB and CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Building a MATLAB model for object recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An enduring challenge within the realm of computer vision involves ascertaining
    the presence of specific objects (object recognition) or activities within an
    image. For objects under predefined conditions, such as the identification of
    specific geometric shapes such as polyhedral or the recognition of faces and handwritten
    characters, this problem can be tackled effectively and without significant hurdles.
    However, the complexity escalates when dealing with arbitrary objects in unrestricted
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Object recognition entails the capacity to detect a particular object within
    a series of images or videos. Human beings possess the remarkable ability to identify
    various objects in images effortlessly, even when the objects’ appearances may
    vary. Moreover, objects can be recognized even when they are partially obscured
    from view. However, this remains a formidable challenge for computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Each object within an image exhibits a multitude of intriguing characteristics
    that can be extracted to construct a comprehensive description of the object.
    This description serves to identify the object when seeking it within a test image
    containing multiple objects. Crucially, the set of characteristics that are extracted
    from the reference image must be resilient to variations in image scale, disturbances,
    lighting conditions, and geometric distortions to ensure dependable recognition.
    CNNs excel in this endeavor, offering algorithms with exceptional object recognition
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing handwriting recognition (HWR)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HWR encompasses the computer’s capability to receive and comprehend handwritten
    input, transforming it into readable text. This input can originate from various
    sources, including paper documents, photographs, and touchscreens. Detection of
    written text can be accomplished through optical scanning, which involves **optical
    character recognition** (**OCR**), or through intelligent word recognition techniques.
  prefs: []
  type: TYPE_NORMAL
- en: We have long been acutely aware of the challenge of automating HWR to facilitate
    smoother interactions between humans and machines. In recent years, this challenge
    has witnessed intriguing advancements and increasingly efficient solutions, primarily
    fueled by substantial economic interest and the growing computational capabilities
    of modern computers. Notably, certain countries, such as Japan and various other
    Asian nations, have made significant investments in research and financial resources
    to pioneer cutting-edge OCR technologies.
  prefs: []
  type: TYPE_NORMAL
- en: The rationale behind the enthusiasm of these countries in this research domain
    is quite evident. Their goal is to develop devices capable of interpreting the
    intricate ideograms that characterize their respective cultures, thereby enhancing
    the ease of interaction with machines. Given that there are currently no input
    devices, such as keyboards, capable of representing thousands of characters, the
    focus is on acquiring this information directly from handwritten scripts through
    digitized scanning.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, even in Western countries, substantial attention has been dedicated
    to the field of optical HWR. There exist numerous applications that stand to benefit
    from automated text interpretation. Consider, for instance, the automatic parsing
    of preprinted templates or the recognition of addresses and postal codes on envelopes,
    which are just a few instances where OCR technology proves invaluable.
  prefs: []
  type: TYPE_NORMAL
- en: HWR is accomplished using diverse techniques that typically involve OCR. Nevertheless,
    a comprehensive script recognition system goes beyond OCR and encompasses tasks
    such as formatting, accurate character segmentation, and identifying the most
    probable words.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how HWR can be approached using machine learning methodologies,
    we will use a very popular dataset that’s largely used in the community to address
    this type of topic. This is the **Modified National Institute of Standards and
    Technology** (**MNIST**), a large database of handwritten digits. This dataset
    consists of 70,000 data examples, which is a subset of a larger dataset maintained
    by NIST. These examples represent digits and are in a format of 28 x 28-pixel
    resolution, organized as a matrix with 70,000 rows and 785 columns. In each row,
    there are 784 columns corresponding to pixel values from the 28 x 28 matrix, and
    one column containing the actual digit label. These digits have been size-normalized
    and positioned at the center of a fixed-size image.
  prefs: []
  type: TYPE_NORMAL
- en: The digit images within the MNIST dataset were originally chosen and processed
    by Chris Burges and Corinna Cortes, who employed bounding box normalization and
    centering techniques. Yann LeCun’s version of the dataset, on the other hand,
    utilizes centering based on the center of mass within a larger window.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is already available in the MATLAB environment, in a short version
    with only 10,000 images evenly distributed over the 10 digits (0-9). Let’s get
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import the dataset into the MATLAB workspace. The dataset is
    available in the standard MATLAB installation with Deep Learning Toolbox. Under
    our MATLAB installation folder, we will find the `toolbox\nnet\nndemos\nndatasets\DigitDataset`
    path. The `DigitDataset` folder contains 10 subfolders, each of which contains
    1,000 images of a single digit; each folder is named after the digit it contains.
    To find the path in an automated way, we can use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `imageDatastore()` function generates a data store by incorporating the
    dataset’s path for a specified collection of image data. We passed three arguments:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`FolderPath`: The path of the folder containing the images'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IncludeSubfolders`: The possibility to include all the subfolders in the main
    folder'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LabelSource`: We used the folder names to label the data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An `ImageDatastore` object will be created with some properties. These properties
    delineate the characteristics of the data and provide instructions on how to retrieve
    data from the data store. When creating the data store object, you have the option
    to set these properties using name-value pairs as arguments. If you wish to inspect
    or adjust a property after the object’s creation, you can do so using the dot
    notation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s look at some of these properties:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Files`: This is a cell array that stores the file paths of all the images
    in the data store. You can access this property to get a list of file paths.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Labels`: This is an array or cell array that associates labels or categories
    with each image in the data store. This property is often used for image classification
    tasks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReadSize`: This specifies the number of images to read at once during data
    iteration. This can impact memory usage and performance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can display a selection of images that have been loaded via a random
    selection process:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we have used the `randperm()` function, which generates a row vector
    comprising nine distinct random integers chosen from a range of 1 to 10,000\.
    Each number that was generated was used as an index to identify an image file
    path stored in the `Data.Files` property. *Figure 8**.4* shows a collage of nine
    images extracted from the database:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.4 – The MNIST dataset](img/B21156_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – The MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can check the distribution of the images over the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With this, we have proof that the images are evenly distributed across the 10
    digits (0-9).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start building the machine learning algorithm, it’s imperative to
    partition the available data into two distinct subsets. The first subset will
    serve as the training data, while the second will be earmarked for algorithm validation.
    Data partitioning plays a pivotal role in machine learning and data analysis as
    it involves segregating a dataset into multiple subsets for training, validation,
    and model testing. In our case, we have 10,000 samples, each containing 1,000
    images for a specific digit. Our chosen approach is to split the data into a 70%
    portion for training and a 30% portion for validation. This split rate is used
    because most of the data must be used for training the network. Accordingly, we
    will employ 7,000 samples for training and reserve the remainder for validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To achieve this, we utilized the `splitEachLabel()` function. This function
    effectively separates the image files within the `Data` dataset into two separate
    data stores: `TrainDat` and `ValDat`. The `TrainDat` data store consists of an
    initial portion of each category determined by `SplitRate`, while the `ValDat`
    data store contains the remaining images from each class. `SplitRate` can take
    the form of a fractional value ranging from 0 to 1, indicating the proportion
    of images allocated to `TrainDat`, or it can be an integer denoting the exact
    count of images assigned to `TrainDat` for each class.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s begin constructing our convolutional network. As expected, a CNN consists
    of a sequence of interconnected layers. To commence, you’ll need to employ a layer
    so that you can import your input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `layers` variable is an array that contains the list of layers for our
    CNN, defining the architecture of the neural network used for deep learning. To
    initiate this architecture, we begin with `imageInputLayer`. This layer functions
    as an input for images, accepting 2D image data into the neural network and performing
    data normalization. Additionally, this layer specifies the unmodifiable `InputSize`
    attribute. This attribute contains the height, width, and number of channels,
    respectively. In this case, we are working with grayscale images with a height
    of 28 and a width of 28\. Following the input layer, we establish the initial
    block of three consecutive layers:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Downsampling is accomplished via a 2D max pooling layer, which divides the input
    into rectangular pooling regions and subsequently identifies the maximum value
    within each of these regions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will introduce a second set of layers, akin to the first set, with
    adjusted parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we add a third block of layers, to finish the architecture:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In an FC layer, the input undergoes multiplication with a weight matrix and
    is subsequently adjusted by a bias vector. The “parameter” in this context specifies
    the desired output size, which, in our case, is 10 since we are classifying `10`
    distinct digits. This layer type mirrors the layer configuration commonly found
    in a traditional `softmaxLayer` is a specialized layer that’s employed within
    neural networks, designed specifically to implement the `softmax` function on
    its input. The `softmax` function finds extensive use in classification tasks
    as it transforms raw scores or logits into a probability distribution spanning
    multiple classes. Typically, this layer serves as the concluding component in
    a neural network designed for multi-class classification. It transforms the network’s
    output values into probabilities that collectively sum to `1`, simplifying the
    interpretation of the model’s predictions.   classificationLayer];
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The classification layer calculates cross-entropy loss for both regular and
    weighted classification tasks that pertain to distinct and mutually exclusive
    classes. It automatically deduces the number of classes by inspecting the dimensions
    of the output originating from the preceding layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before training the CNN, we need to configure the settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To learn more about the various training options, please read [*Chapter 6*](B21156_06.xhtml#_idTextAnchor124),
    *Deep Learning and Convolutional* *Neural Networks*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It’s time to train the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following plot was printed on the screen (*Figure 8**.5*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Training process of the CNN for handwritten digit recognition](img/B21156_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Training process of the CNN for handwritten digit recognition
  prefs: []
  type: TYPE_NORMAL
- en: This plot will undergo continuous updates as the training progresses, enabling
    us to monitor how the algorithm adapts the weights to achieve convergence. We
    can see the results with an accuracy of 84.1%, which indicates a good result.
  prefs: []
  type: TYPE_NORMAL
- en: It is natural to ask whether it is possible to improve the performance of the
    handwritten digit recognition model. In the next section, we will see how to improve
    the accuracy of the model using transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Training and fine-tuning pretrained deep learning models in MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transfer learning is a machine learning approach wherein a model created for
    a particular task is repurposed as the initial foundation for a model addressing
    a second task. This technique entails leveraging knowledge acquired from one problem
    and applying it to a distinct yet related problem. Transfer learning is particularly
    useful in deep learning and neural networks, where pretrained models can be fine-tuned
    or used as feature extractors for new tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In pretrained models, you start with a pretrained model that has been trained
    on a large dataset for a specific task, such as image classification, natural
    language processing, or speech recognition. These pretrained models are often
    complex neural networks with many layers. In many cases, you can use the layers
    of the pretrained model as feature extractors. You remove the final classification
    layer(s) and use the activations from the earlier layers as features for your
    new task. This is especially common in computer vision tasks. Optionally, you
    can fine-tune the pretrained model on your specific task by training it further
    with your own dataset. This involves updating the weights of some or all layers
    while keeping the knowledge you’ve learned from the original task.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning can significantly reduce the amount of data and time required
    to train a model for a new task, especially when you have a limited dataset. Pretrained
    models have already learned useful features from large and diverse datasets, which
    can be valuable for related tasks. It can help improve model performance when
    you have limited computational resources or limited labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is commonly used in various fields, including computer vision,
    natural language processing, and audio processing, and has been a key technique
    in advancing the state of the art in machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the ResNet pretrained network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ResNet stands for Residual Network, a profound CNN architecture introduced by
    Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in their 2015 paper titled
    *Deep Residual Learning for Image Recognition*. This groundbreaking architecture
    has had a substantial impact on the domains of computer vision and deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: The key innovation in ResNet is the use of residual blocks. In traditional deep
    neural networks, as the network becomes deeper, it becomes increasingly difficult
    to train. This is because of the vanishing gradient problem, where gradients become
    extremely small as they are propagated back through the network during training.
    As a result, deep networks tend to suffer from degradation in performance as their
    depth increases.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet addresses this problem by introducing residual blocks, which contain
    skip or shortcut connections that allow the gradient to flow more easily through
    the network. These shortcut connections bypass one or more layers, making it easier
    to train very deep networks. The skip connections essentially learn the residual
    (the difference) between the output and input of a layer, hence the name “residual
    network.”
  prefs: []
  type: TYPE_NORMAL
- en: 'We can summarize the following key features of ResNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep architecture**: ResNet can be very deep, with hundreds or even thousands
    of layers, thanks to the use of residual blocks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Skip connections**: The skip connections allow gradients to propagate effectively,
    mitigating the vanishing gradient problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High accuracy**: ResNet achieved state-of-the-art performance on various
    image classification tasks, including the ImageNet Large Scale Visual Recognition
    Challenge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transfer learning**: Pretrained ResNet models are widely used as feature
    extractors or starting points for various computer vision tasks through transfer
    learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Architectural variations**: There are several ResNet architectures with different
    depths, such as ResNet-18, ResNet-34, ResNet-50, and deeper variants.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet has become a foundational architecture in deep learning, and its principles
    of skip connections and residual learning have influenced the design of many subsequent
    neural network architectures. It has been applied not only to image classification
    but also to various other computer vision tasks, including object detection, semantic
    segmentation, and more.
  prefs: []
  type: TYPE_NORMAL
- en: The MATLAB Deep Network Designer app
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MATLAB Deep Network Designer app is a **graphical user interface** (**GUI**)
    tool provided by MATLAB for designing, training, and analyzing deep neural networks.
    It’s part of the MATLAB Deep Learning Toolbox, which offers a comprehensive set
    of tools and functions for working with ANN and deep learning. You can visually
    design neural network architectures by adding layers, connecting them, and specifying
    their properties and parameters. This allows you to create custom network architectures
    easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'The app provides a library of predefined layers that you can drag and drop
    into your network design. These layers include common types such as convolutional
    layers, FC layers, and more. The Deep Network Designer app simplifies the process
    of designing and training deep neural networks, making it more accessible for
    users who may not be familiar with deep learning concepts and programming. It’s
    a valuable tool for researchers, engineers, and data scientists working on machine
    learning and deep learning projects using MATLAB. Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Deep Network Designer app, simply click on the **Deep Learning**
    section of the **Apps** tab at the top of the MATLAB interface. In the **Deep
    Learning** section, you will find the **Deep Network Designer** icon. Click on
    this icon to open the **Deep Network Designer** app. Alternatively, you can open
    the Deep Network Designer app by entering the following command in the MATLAB
    command window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command will launch the app, allowing you to create, design, and train
    deep neural networks using a GUI. You need to have the MATLAB Deep Learning Toolbox
    installed to use the Deep Network Designer app. If it’s not already installed,
    you may need to install it separately from your MATLAB installation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following window will open (*Figure 8**.6*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Deep Network Designer Start Page](img/B21156_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Deep Network Designer Start Page
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t find the network, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the image pretrained network, we will find ResNet-18\. ResNet-18 consists
    of 18 layers, which include convolutional layers, residual blocks, and FC layers.
    It’s considered relatively shallow compared to deeper variants such as ResNet-50
    or ResNet-101\. Like all ResNet architectures, ResNet-18 employs residual blocks.
    These blocks contain skip connections (or shortcut connections) that allow gradients
    to flow more effectively during training, addressing the vanishing gradient problem.
    ResNet-18 has been widely adopted in the deep learning community due to its balance
    between model complexity and performance. It’s often used in tasks such as image
    classification, object detection, and feature extraction in various computer vision
    applications. Its architectural principles, such as residual learning, have influenced
    the design of many subsequent neural network architectures. To import ResNet-18,
    it is necessary to install the relative toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: 'After importing it, we can import the dataset that we’ll train the network
    on. To do that, we can move to the **Data** tab of the app. Upon clicking the
    **Import Data** icon, the following window will open (*Figure 8**.7*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.7 – The Import Image Data window](img/B21156_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – The Import Image Data window
  prefs: []
  type: TYPE_NORMAL
- en: 'Under **Data source**, we can select **ImageDatastore in workspace** to select
    the data that’s already been imported into the MATLAB workspace, as indicated
    in the previous section. The MNIST dataset will be imported, as shown in the *Figure
    8**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – The MNIST dataset imported into the Deep Network Designer app](img/B21156_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – The MNIST dataset imported into the Deep Network Designer app
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 8**.8*, we can see that a good distribution of the images is regularly
    present in the 10 classes, with 700 images for each class. We will use 70% of
    the data for training and the rest for validation.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can set the pretrained network (ResNet-18) by moving to the
    **Designer** tab. We’ll use ResNet-18 for another type of image (an RGB image
    whose size is 227 x 227 x 3). To do this, we have to change the first layer, which
    defines the size of the input data. Click on the first layer (**ImageInput**)
    and then click on the **Canc** button to remove this layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, we can click on the **ImageInputLayer** icon in **Layer Library**
    to the left of the tab, at which point we have to connect this layer with the
    next layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, we have to change the `28,28,1` in the **Properties** window to
    the right of the **Designer** tab (*Figure 8**.9*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After that, we have to change the first convolutional layer. First, we must
    remove that and then drop a `Convolution2Dlayer` and connect it to another layer.
    Then, we have to set `3,3` and `64` in the **Properties** window to the right
    of the **Designer** tab (*Figure 8**.9*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Modifying the first two layers of ResNet-18 to adapt it to the
    new input data](img/B21156_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Modifying the first two layers of ResNet-18 to adapt it to the
    new input data
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have to move on to the final part of ResNet-18, which involves setting
    the classification option. To do that, we have to replace the FC layer and the
    classification layer, as shown in *Figure 8**.10*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Modifying the final layers of ResNet-18 to adapt it to the
    new classification](img/B21156_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Modifying the final layers of ResNet-18 to adapt it to the new
    classification
  prefs: []
  type: TYPE_NORMAL
- en: To check that the layer has been modified correctly, we can test it by clicking
    on the **Analyze** icon at the top of the **Designer** tab.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are ready to train the model, we can move to the **Training** tab
    of the Deep Network Designer app. We can check the training option by clicking
    on the **Training option** icon at the top of the **Training** tab. After that,
    we can push the **Train** button at the top of the tab. The training process will
    start; we will check its progress in the **Training Progress** window (*Figure
    8**.11*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.11 – The Training Progress window](img/B21156_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – The Training Progress window
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the training process, we will be able to verify the performance
    of the model by reading the accuracy value that was obtained in the validation
    procedure. As we can see, we obtained an accuracy of 90.27%.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s try to collect some useful information on how to interpret the results
    that have been obtained from a model based on machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting and explaining machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interpreting and explaining machine learning models is essential for understanding
    their predictions and making them more transparent and trustworthy, especially
    in applications where interpretability is critical. This is an ongoing process
    that requires collaboration between data scientists, domain experts, and stakeholders.
    The choice of interpretation techniques depends on the model type, problem domain,
    and level of transparency required for the application. It’s important to strike
    a balance between model complexity and interpretability, depending on the specific
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding saliency maps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Saliency maps are a visualization technique that’s used in computer vision and
    deep learning to understand and interpret neural network predictions, particularly
    in image classification and object recognition tasks. Saliency maps help identify
    which regions of an input image or feature map are most relevant to a model’s
    prediction. They are especially useful for gaining insights into why a neural
    network is making a particular decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Saliency maps are generated using gradient-based methods, typically backpropagation.
    The idea is to compute the gradients of the model’s output concerning the input
    image’s pixels. By calculating these gradients, you can identify which pixels
    in the input image have the most significant impact on the model’s prediction.
    In other words, saliency maps highlight the regions that the model pays attention
    to when making a decision. Saliency maps are usually visualized as heatmaps overlaid
    on the original input image. In a heatmap, the intensity of color corresponds
    to the importance of each pixel. High-intensity areas indicate regions that strongly
    influence the model’s output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – A saliency map as a heatmap](img/B21156_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – A saliency map as a heatmap
  prefs: []
  type: TYPE_NORMAL
- en: Saliency maps provide interpretability to neural network predictions. By examining
    the saliency map, you can see which parts of the image resemble specific features
    and contribute to the decision. These maps can be used for model debugging and
    improvement. If the model’s predictions appear incorrect, examining the saliency
    map can reveal whether the model is focusing on the right or wrong features. There
    are variations of saliency maps, such as class-specific saliency maps (highlighting
    features specific to a particular class) and gradient-based approaches such as
    guided backpropagation and SmoothGrad, which improve the interpretability of saliency
    maps.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that saliency maps provide insights into a model’s behavior
    but do not necessarily explain why a neural network made a particular decision
    in human-understandable terms. They are just one tool in the interpretability
    toolbox and are often used in conjunction with other techniques for a more comprehensive
    understanding of model decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding feature importance scores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature importance scores are a set of metrics or values that indicate the relative
    importance of different input features (also known as variables or attributes)
    in a machine learning model’s prediction. These scores help data scientists and
    analysts understand which features have the most significant influence on the
    model’s output. Feature importance scores are particularly valuable for feature
    selection, model interpretation, and debugging.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some common methods and techniques for calculating feature importance
    scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gini importance**: In decision trees and random forests, Gini importance
    measures how often a feature is used for splitting data across the tree nodes.
    Higher values indicate more important features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean decrease in impurity**: Similar to Gini importance, this metric calculates
    how much the impurity (or impurity reduction) decreases when a particular feature
    is used for splitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coefficient magnitude**: In linear models, the magnitude (absolute value)
    of the coefficients represents the feature’s importance. Larger coefficients indicate
    greater importance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Permutation feature importance**: This method involves randomly permuting
    the values of a single feature while keeping other features constant and measuring
    how much the model’s performance decreases. A significant drop in performance
    indicates an important feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recursive feature elimination** (**RFE**): RFE is an iterative method that
    starts with all features and gradually removes the least important ones based
    on a model’s performance. The order in which features are removed indicates their
    importance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SHapley Additive exPlanations** (**SHAP**): SHAP values provide a unified
    measure of feature importance by considering all possible feature combinations.
    They can be applied to various models, including deep learning models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of feature importance calculation method depends on the machine learning
    algorithm used, the dataset, and the problem at hand. Different algorithms may
    provide different rankings of feature importance, so it’s essential to consider
    multiple methods and use domain knowledge to interpret the results effectively.
    Feature importance scores help identify relevant features, reduce dimensionality,
    and improve model interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering gradient-based attribution methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gradient-based attribution methods, also known as gradient-based attribution
    techniques, are approaches that are used to understand and attribute the contributions
    of individual features or input elements to the output of a machine learning model.
    These methods rely on gradients, which represent the sensitivity of the model’s
    output to changes in input features. Here are some gradient-based attribution
    methods that are commonly used in machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient saliency**: Saliency maps emphasize the most pertinent areas in
    an input image that influence a model’s prediction. These maps are created by
    calculating the gradient of the model’s output concerning the input image pixels.
    Regions with high gradients correspond to areas of significant relevance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrated gradients**: Integrated gradients assigns attribution scores to
    each input feature by computing the cumulative integral of the gradient concerning
    the input along a path from a reference input (usually all zeros) to the actual
    input. This method provides a more comprehensive understanding of feature importance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guided backpropagation**: Guided backpropagation is a modified backpropagation
    algorithm that retains only positive gradients during backpropagation. This helps
    highlight the positive contributions of input features to predictions and suppresses
    negative contributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SmoothGrad**: SmoothGrad reduces noise in saliency maps by averaging gradients
    across multiple perturbed versions of the input and then visualizing the smoothed
    gradient values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer-wise relevance propagation** (**LRP**): LRP is an attribution method
    that assigns relevance scores to each neuron in the network’s hidden layers and
    propagates them backward to the input features. It provides fine-grained feature
    relevance information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deconvolutional networks** (**DeconvNets**): DeconvNets are designed to reverse
    the effects of convolutional layers in a neural network. They help visualize feature
    maps at different layers of the network to understand what each layer learns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient Class Activation Mapping** (**GradientCAM**): GradientCAM combines
    gradient information with class activation mapping techniques to highlight regions
    in an input image that are important for a specific class prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-based attribution methods are valuable for model interpretability and
    debugging. They help identify which features or parts of the input are influential
    in driving the model’s decisions. Choosing the right attribution method depends
    on the model architecture, dataset, and specific goals of interpretation. These
    methods provide insights into model behavior and can help build trust in AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we understood the basic concepts surrounding computer vision
    and how to implement a model for object recognition using MATLAB. We started by
    introducing image processing and computer vision. We learned how tools are available
    to process images and how computer vision is used for object recognition, motion
    detection, and pattern recognition. Then, we explored MATLAB tools for computer
    vision, and how the capabilities and functions provided by MATLAB create a robust
    environment for the development and prototyping of computer vision applications.
    Whether your focus is on tasks such as image analysis, object detection, 3D reconstruction,
    or any related application, MATLAB offers the necessary tools and features to
    support your work effectively.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we learned how to build a MATLAB model for object recognition by
    using a CNN and the MNIST dataset. We understood how to import image data into
    a MATLAB workspace and how to use images to train a CNN. Then, we learned how
    to use pretrained deep learning models in MATLAB to improve the performance of
    the object recognition model. Finally, we introduced some tools for interpreting
    and explaining deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into fundamental concepts surrounding sequential
    data and explore the process of constructing a model to capture patterns within
    time series or any general sequence. We will learn the basic concepts of time
    series data, how to extract statistics from sequential data, and how to implement
    a model to predict the stock market data. Finally, we will understand oversampling,
    undersampling, and cost-sensitive learning.
  prefs: []
  type: TYPE_NORMAL
