- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: MATLAB for Image Processing and Computer Vision
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MATLAB图像处理和计算机视觉
- en: Computer vision is a discipline that explores methods for processing, analyzing,
    and comprehending visual data. In the realm of image content analysis, a multitude
    of computer vision algorithms is employed to develop insights into the objects
    depicted in the image. Encompassing diverse facets of image analysis, computer
    vision addresses tasks such as object recognition, shape analysis, pose estimation,
    3D modeling, visual search, and more. While humans excel at identifying and recognizing
    objects in their surroundings, the objective of computer vision is to faithfully
    replicate the capabilities of the **human visual system** (**HVS**) using computational
    methods. In this chapter, we will understand the basic concepts of computer vision
    and how to implement a model for object recognition using MATLAB.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是一个探索处理、分析和理解视觉数据方法的学科。在图像内容分析领域，众多计算机视觉算法被用于深入理解图像中描绘的对象。涵盖图像分析的各个方面，计算机视觉处理诸如对象识别、形状分析、姿态估计、3D建模、视觉搜索等任务。虽然人类在识别和识别周围环境中的对象方面表现出色，但计算机视觉的目标是使用计算方法忠实复制**人类视觉系统**（**HVS**）的能力。在本章中，我们将了解计算机视觉的基本概念以及如何使用MATLAB实现对象识别模型。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Introducing image processing and computer vision
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍图像处理和计算机视觉
- en: Exploring MATLAB tools for computer vision
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索MATLAB计算机视觉工具
- en: Building a MATLAB model for object recognition
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建MATLAB对象识别模型
- en: Training and fine-tuning pretrained deep learning models in MATLAB
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MATLAB中训练和微调预训练的深度学习模型
- en: Interpreting and explaining machine learning models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释和说明机器学习模型
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will introduce basic machine learning concepts. To understand
    these topics, a basic knowledge of algebra and mathematical modeling is needed.
    You will also require a working knowledge of MATLAB.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍基本的机器学习概念。为了理解这些主题，需要具备代数和数学建模的基本知识。您还需要具备MATLAB的实际操作能力。
- en: To work with the MATLAB code in this chapter, you’ll need the files available
    on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用本章中的MATLAB代码，您需要GitHub上提供的文件，网址为[https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)。
- en: Introducing image processing and computer vision
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍图像处理和计算机视觉
- en: Through the five senses, humans gather information from the external world and
    process it, making decisions to carry out the actions that shape their daily lives.
    One of the most intriguing challenges in computer science is replicating this
    sequence of events, identifying and harnessing new sources of information. The
    ability to acquire and interpret information by simulating the human sensory system
    is called machine perception, and it is fundamental in the field of artificial
    intelligence.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过五种感官，人类从外部世界收集信息并处理它，做出决策以执行塑造他们日常生活的行动。计算机科学中最引人入胜的挑战之一是复制这一系列事件，识别和利用新的信息来源。通过模拟人类感官系统来获取和解释信息的能力被称为机器感知，这是人工智能领域的基本内容。
- en: Being able to interpret and acquire information from the external world is made
    possible through techniques such as encoding and information processing. Through
    digital image encoding techniques, it is possible to represent what humans can
    perceive in the form of bits. Depending on the methodologies used, it is possible
    to select the quantity and quality of the information to be represented. Through
    processing methods, on the other hand, it is possible to interpret the information
    contained in images and attempt to replicate the mechanisms of human decision-making.
    One such human ability is the capacity to recognize, through sight, what types
    of objects are present within a scene, thereby identifying the distinctive characteristics
    of each object. The highest level of information that can be extracted from images
    is done through identifying and recognizing individual objects within a scene.
    This information can be used to categorize and group images based on the objects
    they contain.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过编码和信息处理等技术，能够从外部世界解释和获取信息成为可能。通过数字图像编码技术，可以将人类可以感知的内容以比特的形式表示出来。根据使用的方法，可以选择要表示的信息的数量和质量。另一方面，通过处理方法，可以解释图像中包含的信息，并尝试复制人类决策机制。其中一种人类能力是通过视觉识别场景中存在的物体类型，从而识别每个物体的独特特征。从图像中可以提取的最高水平信息是通过识别和识别场景中的单个物体来完成的。这些信息可以用来根据它们包含的物体对图像进行分类和分组。
- en: Understanding image processing
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解图像处理
- en: For humans, a fundamental sense is vision. Through digital images, it is possible
    to represent what the HVS captures in an instant numerically. An image is a 2D
    representation of visual perception; it is perceived in the form of electromagnetic
    waves that enter the eye and impact the retina. The elements that make up the
    retina capture information, such as luminance and spectral characteristics. These
    are transformed into nerve signals to be sent, through the optic nerve, to the
    brain structures responsible for visual interpretation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类来说，一个基本的感觉是视觉。通过数字图像，可以以数值形式表示HVS在瞬间捕获的内容。图像是视觉感知的二维表示；它以电磁波的形式被感知，这些电磁波进入眼睛并影响视网膜。视网膜的组成元素捕获信息，如亮度和光谱特性。这些被转换成神经信号，通过视神经发送到负责视觉解释的大脑结构。
- en: 'In the digital domain, images are commonly represented as an ordered set of
    points and pixels, arranged in rows and columns. This mode of representation is
    called raster or bitmap. Essentially, it involves 2D sampling of a continuous
    signal in two dimensions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在数字领域，图像通常被表示为按顺序排列的点集和像素，这些点集和像素以行和列的形式排列。这种表示方式被称为光栅或位图。本质上，它涉及在两个维度上对连续信号进行二维采样：
- en: '![Figure 8.1 – Image representation as a sequence of pixels. Each pixel has
    a value from 0 (black) to 255 (white)](img/B21156_08_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 以像素序列表示的图像表示。每个像素的值从0（黑色）到255（白色）](img/B21156_08_01.jpg)'
- en: Figure 8.1 – Image representation as a sequence of pixels. Each pixel has a
    value from 0 (black) to 255 (white)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 以像素序列表示的图像表示。每个像素的值从0（黑色）到255（白色）
- en: The simplest technique for representation is the use of grayscale. In this type
    of representation, pixels contain the amount of luminance. Luminance is a fundamental
    quantity in the visual field and represents the amount of light that reaches the
    observer’s eye. Pixel values range from absence (black) to the maximum level of
    light (white), while intermediate states are perceived as varying shades of gray.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 表示的最简单技术是使用灰度。在这种类型的表示中，像素包含亮度值。亮度是视觉场中的基本量，表示到达观察者眼睛的光量。像素值从无（黑色）到最大光亮度（白色），而中间状态则被感知为不同灰度的阴影。
- en: In computer graphics, the pixel is the smallest conventional unit of a digital
    image’s surface. The more pixels an image has, the more information it contains,
    and consequently, our ability to notice details within it increases. The amount
    of information can be indicated by measuring resolution, either in absolute terms
    (pixels) or concerning physical measurements (**dots per** **inch** (**dpi**)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机图形学中，像素是数字图像表面的最小传统单位。图像中像素的数量越多，它包含的信息就越多，因此我们注意到其中的细节的能力也增加。信息量可以通过测量分辨率来表示，无论是以绝对值（像素）还是关于物理测量（每英寸**点数**（dpi））来表示。
- en: 'When deciding to change the resolution of an image, two situations can arise:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当决定改变图像的分辨率时，可能出现两种情况：
- en: '**Constant pixel size**: The number of pixels that make up the image is reduced,
    resulting in the image shrinking in size'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**像素尺寸不变**：组成图像的像素数量减少，导致图像尺寸缩小。'
- en: '**Increase in pixel size**: The pixels (dpi) are reduced in size, while the
    overall dimensions of the image remain constant, causing the individual pixel
    size to increase'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**像素尺寸增加**：像素（dpi）的尺寸减小，而图像的整体尺寸保持不变，导致单个像素尺寸增加。'
- en: An important aspect of digital images is how to represent the information contained
    in their pixels. In the case of grayscale, it represents luminance, which needs
    to be quantized by representing it in a finite number of bits. The greater the
    number of bits, the lower the quantization noise. Using *b* bits, there are 2b
    possible values. Typically, 8 bits is the most common value, allowing for a total
    of 256 levels of luminance to be represented. It has been demonstrated that 8
    bits provide an acceptable representation of grayscale gradients in most applications.
    This quantity effectively adapts to the HVS’s ability to distinguish different
    luminance levels in the image.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数字图像的一个重要方面是如何表示其像素中包含的信息。在灰度图的情况下，它表示亮度，需要通过在有限数量的比特中表示来进行量化。比特数越多，量化噪声越低。使用*b*比特，有2b种可能的值。通常，8比特是最常见的值，允许表示总共256级的亮度。已经证明，8比特在大多数应用中可以提供可接受的灰度梯度表示。这个数量有效地适应了人视觉系统区分图像中不同亮度级别的能力。
- en: This holds for grayscale images that contain only luminance information for
    each pixel. Introducing color increases the complexity of representation because
    it requires the use of a model to represent it. This model must be able to capture
    the chromatic information significant to the HVS and translate it into numbers.
    The goal is to obtain a vector of numbers that “summarizes” the frequencies contained
    in the electromagnetic wave for each pixel.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这也适用于只包含每个像素亮度信息的灰度图像。引入颜色增加了表示的复杂性，因为它需要使用一个模型来表示它。这个模型必须能够捕捉对人视觉系统有意义的颜色信息并将其转换为数字。目标是获得一个数字向量，它“总结”了每个像素中电磁波包含的频率。
- en: 'The most commonly used model in the digital domain is undoubtedly RGB. It is
    based on the combination of three chromatic components with different intensities:
    red, green, and blue.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在数字领域，最常用的模型无疑是RGB。它基于三种不同强度的颜色成分的组合：红色、绿色和蓝色。
- en: These components roughly correspond to the three types of cones in the human
    retina. Therefore, it is not necessary to represent all the color information
    that exists in the real world, but only the information to which the HVS is sensitive.
    The information that’s carried by this electromagnetic wave corresponds to the
    light that hits the organs inside the retina; hence, the color corresponds to
    the spectrum of the electromagnetic signal. Therefore, to represent the signal,
    only the three components related to the red, green, and blue colors, known as
    primary colors, which coincide with the luminance in three different frequency
    bands, are required. The RGB model represents how much energy is present in the
    spectral bands corresponding to the primary colors and provides this information
    in the form of three distinct values or components.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件大致对应于人视网膜中的三种锥体细胞。因此，没有必要表示现实世界中存在的所有颜色信息，而只需表示人视觉系统敏感的信息。这种电磁波携带的信息对应于击中视网膜内部器官的光线；因此，颜色对应于电磁信号的频谱。因此，为了表示信号，只需要与红色、绿色和蓝色三种颜色相关的三个成分，这三种颜色被称为原色，它们与三个不同的频率带中的亮度相对应。RGB模型表示与原色对应的频谱带中存在的能量量，并以三个不同的值或成分的形式提供这些信息。
- en: Once you understand how it’s possible to represent a digital image numerically,
    it’s necessary to know how to process it to facilitate its representation and
    extract relevant information. Image processing techniques can leverage digital
    transformation algorithms that modify the pixels of the original image, resulting
    in a new one. However, they also include techniques that extract numerical or
    tabular values from the image, representing a particular feature of it. Depending
    on their complexity, these techniques can be placed into different categories.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了如何将数字图像以数值形式表示，就必须知道如何处理它，以便于其表示和提取相关信息。图像处理技术可以利用数字转换算法来修改原始图像的像素，从而生成一个新的图像。然而，它们还包括从图像中提取数值或表格值的技术，代表图像的特定特征。根据它们的复杂性，这些技术可以被归入不同的类别。
- en: 'The simplest processing methods are those that pertain to the transformation
    of individual pixels:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的处理方法是与单个像素的转换相关：
- en: '**Grayscale conversion**: This is a type of transformation that allows you
    to transition from the RGB model to grayscale. There is a linear relationship
    between luminance and the three chromatic components of the RGB model. This conversion
    allows us to have simpler pixels to manage and process in subsequent operations.
    Each pixel will have only one value corresponding to luminance rather than three
    values, one for each chromatic component (R, G, B) (*Figure 8**.2*):'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灰度转换**：这是一种允许你从RGB模型过渡到灰度的转换类型。亮度与RGB模型的三个色度成分之间存在线性关系。这种转换使我们能够在后续操作中管理并处理更简单的像素。每个像素将只有一个值，对应于亮度，而不是三个值，每个色度成分（R、G、B）一个值（*图8**.2*）：'
- en: "![Figure 8.2 – Grayscale conversion of the Flavian \uFEFFamphitheatre](img/B21156_08_02.jpg)"
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 弗拉维安竞技场的灰度转换](img/B21156_08_02.jpg)'
- en: Figure 8.2 – Grayscale conversion of the Flavian amphitheatre
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 弗拉维安竞技场的灰度转换
- en: '**Thresholding**: This is a very useful transformation in the image segmentation
    phase, where the goal is to isolate an object of interest from the background.
    The idea is to set pixels above a certain threshold value equal to the maximum
    luminance intensity value, while pixels below the threshold are set to the minimum
    intensity value.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阈值化**：这是一种在图像分割阶段非常有用的转换，其目标是隔离感兴趣的对象与背景。想法是将高于某个阈值值的像素设置为最大亮度强度值，而将低于阈值的像素设置为最小强度值。'
- en: '**Aliasing**: This is an effect that makes two different signals that are indistinguishable
    during sampling. Aliasing occurs when sampling or interpolation produces a lower
    resolution in the image, distorting the output compared to the original signal.
    Anti-aliasing filters can be used to correct this problem. In the case of a digital
    image, aliasing manifests as a moiré effect or a wavy pattern.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混叠**：这是一种在采样期间使两个不同的信号在采样时无法区分的效果。混叠发生在采样或插值产生图像的较低分辨率时，与原始信号相比，输出被扭曲。可以使用抗混叠滤波器来纠正这个问题。在数字图像的情况下，混叠表现为摩尔纹或波浪状图案。'
- en: 'Then, there are the direct comparison and feature extraction methods. The first
    group of techniques is used to compare two images pixel by pixel, obtaining a
    value that measures the discrepancy between them. The second, on the other hand,
    allows for creating a summary of the initial image and using a smaller dataset
    that still adequately describes the original set. Let’s see something:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，还有直接比较和特征提取方法。第一组技术用于逐像素比较两个图像，获得一个衡量它们之间差异的值。另一方面，第二组技术允许创建初始图像的摘要，并使用一个更小的数据集，该数据集仍然可以充分描述原始集。让我们看看一些例子：
- en: '**Direct comparison methods**: One of the most interesting pieces of information
    that can be derived from images is their degree of similarity. The human brain
    can process the information contained in visual perceptions that arrive through
    the retina. There are dedicated neurons for interpreting visual information (shape,
    color, motion, space, lines, and so on). Recognition, for example, of faces and
    objects, occurs only after this information is extracted and memory is accessed.
    One type of recognition methodology is the direct approach. This method of comparing
    images is also called brute force as it involves comparing each pixel in the two
    images. These methodologies apply algebraic formulas and calculate a discrepancy
    index that indicates the degree of similarity between two images. If we have two
    images called *image1* and *image2*, we can calculate the discrepancy index using
    the following formula:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接比较方法**：从图像中可以得出许多有趣的信息之一是它们的相似度。人脑可以处理通过视网膜到达的视觉感知中的信息。有专门的神经元用于解释视觉信息（形状、颜色、运动、空间、线条等）。例如，识别面部和物体只有在提取这些信息并访问记忆之后才会发生。一种识别方法是直接方法。这种比较图像的方法也称为暴力方法，因为它涉及比较两个图像中的每个像素。这些方法应用代数公式并计算一个差异度指数，该指数表示两个图像之间的相似度。如果我们有两个图像称为*image1*和*image2*，我们可以使用以下公式计算差异度指数：'
- en: discrepancy _ index = 1 − sum(sum(abs(image1 − image2))) / sum(sum(image1))
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 差异度指数 = 1 − sum(sum(abs(image1 − image2))) / sum(sum(image1))
- en: '**Mean squared error** (**MSE**): This is a value that indicates, in an absolute
    sense, how similar two images are. This index compares the images pixel by pixel
    and represents the average discrepancy of these values. The closer the MSE value
    is to 0, the greater the similarity between the analyzed images.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）：这是一个表示两个图像在绝对意义上相似程度的值。该指数逐像素比较图像，并代表这些值的平均差异。MSE值越接近0，分析图像之间的相似度就越大。'
- en: '**Structural similarity index measure** (**SSIM**): Another method of direct
    comparison is SSIM. The difference compared to MSE is that the latter makes estimates
    using absolute errors. SSIM, on the other hand, is a perception-based model that
    considers image degradation as a change in the perception of structural information.
    Instead of performing a pixel-by-pixel comparison, the algorithm divides the image
    into grids of N x N pixels. Within each grid, an average value of the pixels is
    calculated, allowing for relationships between neighboring pixels to be considered
    rather than the absolute value of a single pixel.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构相似性指数度量**（**SSIM**）：另一种直接比较的方法是SSIM。与MSE相比，后者使用绝对误差进行估计。而SSIM则是一个基于感知的模型，它将图像退化视为结构信息感知的变化。该算法不是逐像素比较图像，而是将图像划分为N
    x N像素的网格。在每个网格内，计算像素的平均值，从而考虑相邻像素之间的关系，而不是单个像素的绝对值。'
- en: Direct comparison methodologies come with a significant cost associated with
    the amount of data to process as they consider all the information contained in
    the image. Through some techniques, it’s possible to reduce dimensionality. When
    there’s an excessive amount of data to process and the possibility of redundancy,
    a transformation can be applied, adopting a reduced representation of the data.
    This reduced representation is nothing but a set of features. The process that
    transforms the input data into the feature set is called **feature extraction**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 直接比较方法伴随着与处理数据量相关的显著成本，因为它们考虑了图像中包含的所有信息。通过一些技术，可以降低维度。当处理的数据量过多且存在冗余的可能性时，可以应用一种转换，采用数据的简化表示。这种简化表示不过是特征集。将输入数据转换为特征集的过程称为**特征提取**。
- en: The chosen characteristics encompass pertinent details from the input data,
    enabling the intended task to be accomplished by utilizing this condensed representation
    rather than the entirety of the original data. This method minimizes the cost
    and resources needed for an accurate depiction of a vast dataset. When tackling
    intricate data analysis, a primary hurdle involves diminishing the number of variables
    in play. Analyzing a large number of variables usually translates into high memory
    usage and computational power requirements. Moreover, when applying classification
    algorithms, there is a risk of **overfitting**. In this case, the model adapts
    too closely to the dataset that was used for learning, failing to generalize and
    thus losing effectiveness. Feature extraction is a broad term that’s used to describe
    methods of creating variable combinations aimed at addressing these issues while
    maintaining sufficient accuracy in depicting the data. Now that we’ve introduced
    the most widespread image processing methodologies, we can focus on how to extract
    knowledge from images.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所选特征涵盖了输入数据的相关细节，通过利用这种浓缩的表示方法而不是原始数据的全部，使得预期的任务得以完成。这种方法最小化了准确描述大量数据集所需的成本和资源。在处理复杂的数据分析时，一个主要挑战是减少在游戏中变量的数量。分析大量变量通常意味着高内存使用和计算能力需求。此外，在应用分类算法时，存在**过拟合**的风险。在这种情况下，模型过于紧密地适应用于学习的数据集，无法泛化，从而失去有效性。特征提取是一个广泛的概念，用于描述创建变量组合的方法，旨在解决这些问题，同时保持对数据描述的足够准确性。现在我们已经介绍了最广泛使用的图像处理方法，我们可以专注于如何从图像中提取知识。
- en: Explaining computer vision
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释计算机视觉
- en: Computer vision is an interdisciplinary field of computer science and artificial
    intelligence that deals with the development of algorithms, models, and computer
    systems capable of interpreting, understanding, and analyzing visual information
    from images or videos. The main goal of computer vision is to replicate some of
    the capabilities of the HVS, allowing computers to perceive and understand the
    world around them through visual data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是计算机科学和人工智能的一个跨学科领域，涉及开发算法、模型和计算机系统，这些系统能够从图像或视频中解释、理解和分析视觉信息。计算机视觉的主要目标是复制人眼视觉系统（HVS）的一些能力，使计算机能够通过视觉数据感知和理解周围的世界。
- en: This field of study focuses on a wide range of tasks, including object recognition,
    motion detection, pattern recognition, information extraction from images, image
    segmentation, 3D reconstruction, and much more. Computer vision has applications
    in a wide range of industries, including medicine, automotive, security, manufacturing,
    retail, entertainment, and robotics, to name a few.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究领域关注广泛的任务，包括物体识别、运动检测、模式识别、从图像中提取信息、图像分割、3D重建等。计算机视觉在众多行业中都有应用，包括医疗、汽车、安全、制造、零售、娱乐和机器人技术，仅举几例。
- en: Recent developments in computer vision have been driven by the use of deep neural
    networks, in particular **convolutional neural networks** (**CNNs**), which have
    delivered exceptional results in many visual recognition tasks. These advances
    are fundamentally changing the way machines can interact with the visual world,
    opening up new possibilities in areas such as autonomous driving, medical diagnosis,
    augmented reality, and much more.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的最近发展是由深度神经网络的运用驱动的，特别是**卷积神经网络**（**CNNs**），它们在许多视觉识别任务中取得了卓越的结果。这些进步从根本上改变了机器与视觉世界互动的方式，为自动驾驶、医疗诊断、增强现实等领域开辟了新的可能性。
- en: 'This task is the most complex in terms of abstraction. The most common use
    of this category is for object recognition. Conceptually, the process can be divided
    into two steps. The first step involves defining an object of interest according
    to a model using feature extraction techniques. In the second step, the object
    is searched for within an image. These types of transformations require the use
    of machine learning and data mining algorithms that, through a dataset, allow
    a model to be constructed for the object to be searched for. Subsequently, it
    can be determined whether there are pixels within the image that match the previously
    created model. There are different approaches to this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在抽象方面，这项任务是最复杂的。这个类别最常见的用途是物体识别。从概念上讲，这个过程可以分为两个步骤。第一步涉及使用特征提取技术根据模型定义一个感兴趣的物体。在第二步中，在图像中搜索该物体。这类变换需要使用机器学习和数据挖掘算法，通过数据集，允许为要搜索的物体构建一个模型。随后，可以确定图像中是否有像素与先前创建的模型匹配。对此有不同方法：
- en: '**Shape matching**: This is an approach that involves searching for a silhouette.
    This method allows you to measure the similarity between shapes and identify correspondences
    between points that belong to the contour of the object being searched for. The
    basic idea is to select *n* points on the contour of the silhouette. For each
    point, the n - 1 vectors connecting it to all the others are considered. A set
    of all these vectors forms a complex descriptor of the silhouette localized at
    that point. This set of vectors is obtained through a shape extraction process,
    which is part of feature extraction. The idea is to obtain a descriptor using
    these vectors and use it to identify a similar shape within other images and perform
    classification:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形状匹配**：这是一种涉及搜索轮廓的方法。这种方法允许你测量形状之间的相似性，并识别属于正在搜索的物体轮廓的点之间的对应关系。基本思想是在轮廓上选择
    *n* 个点。对于每个点，考虑连接它到所有其他点的 n - 1 个向量。所有这些向量的集合形成了一个在该点局部化的轮廓的复杂描述符。这个向量集是通过形状提取过程获得的，它是特征提取的一部分。想法是使用这些向量获得一个描述符，并使用它来识别其他图像中的相似形状并执行分类：'
- en: '![Figure 8.3 – Shape matching](img/B21156_08_03.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 形状匹配](img/B21156_08_03.jpg)'
- en: Figure 8.3 – Shape matching
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 形状匹配
- en: '**Pattern matching**: This is a technique that involves identifying a specific
    sequence or regularity of data (referred to as a pattern) within a large dataset.
    The field of pattern recognition involves automatically searching for regularities
    within data using computer algorithms and patterns to perform actions such as
    classifying data into different categories. In digital images, the identification
    process involves preparing a pattern and corresponding to a set of pixels that
    describes a specific object of interest or a part of it. Then, a pixel classification
    process is performed within the image to determine whether a group of pixels comparable
    to the pattern being searched for exists or not.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式匹配**：这是一种涉及在大型数据集中识别特定序列或数据（称为模式）的规律性的技术。模式识别领域涉及使用计算机算法和模式自动在数据中搜索规律性，以执行将数据分类到不同类别等操作。在数字图像中，识别过程涉及准备一个模式和对应于描述特定感兴趣物体或其一部分的一组像素。然后，在图像内执行像素分类过程，以确定是否存在与正在搜索的模式相媲美的像素组。'
- en: '**Feature-based object recognition**: Through this technique, it is possible
    to create descriptors that represent the typical characteristics of an object.
    Each object has unique features that describe it. If we can identify all these
    features within a dataset, we can assume that the object is present. For example,
    a human face can be modeled based on specific anatomical features such as the
    placement of eyes, nostrils, angles formed by the lips, and so on. The combination
    of these anatomical elements and the vectors that connect them forms a patch model.
    This model represents the ordered set of elements that are sufficient to accurately
    describe an object.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于特征的物体识别**：通过这项技术，可以创建代表物体典型特征的描述符。每个物体都有描述其自身的独特特征。如果我们能在数据集中识别出所有这些特征，我们可以假设该物体存在。例如，人脸可以根据特定的解剖特征进行建模，如眼睛的位置、鼻孔、嘴唇形成的角度等。这些解剖元素及其连接的向量的组合形成了一个补丁模型。这个模型代表了一组有序的元素，这些元素足以准确描述一个物体。'
- en: It’s interesting to understand how object recognition technology within images
    has evolved and the impact it has had on society. One of the most fascinating
    cases that leverages image processing techniques is facial recognition. Until
    not long ago, this technology was commonly seen as something straight out of science
    fiction. However, in the last decade, it has not only become a reality but also
    become widely used.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 了解图像中对象识别技术是如何演变以及它对社会产生的影响是非常有趣的。其中最引人入胜的案例之一就是利用图像处理技术的面部识别。直到不久前，这项技术还常被视为科幻小说中的东西。然而，在过去的十年里，它不仅成为现实，而且被广泛使用。
- en: Now that we’ve analyzed the basic concepts connected to image processing and
    computer vision, let’s analyze the necessary tools to address these problems in
    MATLAB.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经分析了与图像处理和计算机视觉相关的基本概念，让我们分析解决这些问题的MATLAB所需工具。
- en: Exploring MATLAB tools for computer vision
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索MATLAB计算机视觉工具
- en: Computer vision encompasses the development of algorithms, techniques, and systems
    that allow computers to acquire, process, analyze, and make decisions based on
    visual data from images and videos. The primary goal of computer vision is to
    enable machines to perform tasks that typically require human visual perception
    and comprehension.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉包括开发算法、技术和系统，使计算机能够从图像和视频中获取、处理、分析和基于视觉数据做出决策。计算机视觉的主要目标是使机器能够执行通常需要人类视觉感知和理解的任务。
- en: Computer vision can automate various tasks that would be time-consuming or even
    impossible for humans to perform consistently and at scale. This includes tasks
    such as object detection, image classification, and tracking. When trained and
    configured properly, computer vision algorithms can achieve high levels of accuracy
    in tasks such as image recognition and segmentation. They don’t suffer from fatigue
    or distraction, leading to consistent results. These algorithms can process images
    and videos in real time or near real time, making them suitable for applications
    that require rapid analysis and decision-making.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉可以自动化各种任务，这些任务对于人类来说可能耗时或甚至无法持续和大规模地执行。这包括对象检测、图像分类和跟踪等任务。当经过适当训练和配置时，计算机视觉算法可以在图像识别和分割等任务中实现高水平的准确性。它们不会受到疲劳或分心的困扰，从而产生一致的结果。这些算法可以实时或接近实时地处理图像和视频，使其适用于需要快速分析和决策的应用。
- en: Computer vision can also be applied to a wide range of industries and applications,
    from healthcare and automotive to agriculture and manufacturing. It can adapt
    to various domains with appropriate training. These systems can scale easily to
    handle large volumes of data and images, making them suitable for big data applications.
    In applications such as medical imaging, computer vision provides a non-invasive
    way to diagnose and monitor conditions without the need for invasive procedures.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉也可以应用于广泛的行业和应用，从医疗保健和汽车行业到农业和制造业。它可以通过适当的训练适应各种领域。这些系统可以轻松扩展以处理大量数据和图像，使其适合大数据应用。在医疗成像等应用中，计算机视觉提供了一种非侵入性的诊断和监测条件的方法，无需进行侵入性程序。
- en: In contrast, these algorithms heavily rely on large datasets for training. Insufficient
    or biased training data can lead to poor performance and inaccurate results. Developing
    and fine-tuning computer vision models can be complex and time-consuming. It often
    requires expertise in machine learning, deep learning, and image processing. Deep
    learning models used in computer vision can be computationally intensive and require
    powerful hardware, such as GPUs, for training and inference. Deep learning models,
    especially CNNs, are often perceived as black boxes, posing challenges in interpreting
    the rationale behind their specific decisions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，这些算法在训练时严重依赖于大量数据集。训练数据不足或存在偏差可能导致性能不佳和结果不准确。开发和微调计算机视觉模型可能很复杂且耗时。这通常需要机器学习、深度学习和图像处理方面的专业知识。在计算机视觉中使用的深度学习模型可能计算密集，需要强大的硬件，如GPU，进行训练和推理。深度学习模型，尤其是卷积神经网络（CNNs），通常被视为黑盒，这给解释其特定决策背后的理由带来了挑战。
- en: Computer vision systems may struggle to perform well under adverse conditions,
    such as poor lighting, occlusions, or variations in camera angles. The use of
    computer vision in surveillance and facial recognition has raised concerns about
    privacy and potential misuse. These algorithms may adopt biases embedded in their
    training data, resulting in unjust or discriminatory outcomes. Ensuring fairness
    and mitigating bias remains a persistent challenge. While computer vision can
    identify and classify objects, it often lacks a deep understanding of the context
    in which those objects appear, which can limit its usefulness in some scenarios.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉系统可能在恶劣条件下难以表现良好，例如光线不足、遮挡或相机角度变化。在监控和面部识别中应用计算机视觉引发了关于隐私和潜在滥用的担忧。这些算法可能采用其训练数据中嵌入的偏见，导致不公正或歧视性的结果。确保公平性和减轻偏见仍然是一个持续的挑战。虽然计算机视觉可以识别和分类对象，但它通常缺乏对那些对象出现背景的深入理解，这可能在某些场景中限制其有用性。
- en: Overall, computer vision is a powerful and rapidly evolving field with the potential
    to transform many industries. However, it’s essential to be aware of its limitations
    and challenges and to apply it responsibly and ethically, addressing issues related
    to data quality, privacy, and bias.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，计算机视觉是一个强大且快速发展的领域，具有改变许多行业的潜力。然而，了解其局限性和挑战，并负责任、道德地应用它，解决与数据质量、隐私和偏见相关的问题，是至关重要的。
- en: 'MATLAB provides a variety of tools and functions for computer vision tasks.
    These tools can be found in Computer Vision Toolbox, Image Processing Toolbox,
    and other related toolboxes. Here are some of the key MATLAB tools and functionalities
    for computer vision:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: MATLAB为计算机视觉任务提供了各种工具和函数。这些工具可以在计算机视觉工具箱、图像处理工具箱和其他相关工具箱中找到。以下是MATLAB在计算机视觉中的关键工具和功能：
- en: '**Computer Vision Toolbox**: This toolbox is specifically designed for computer
    vision tasks. It includes a wide range of functions and algorithms for image processing,
    feature extraction, object detection and recognition, 3D vision, camera calibration,
    and more.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算机视觉工具箱**：这个工具箱专门为计算机视觉任务设计。它包括广泛的图像处理、特征提取、目标检测和识别、3D视觉、相机标定等功能和算法。'
- en: '**Image Processing Toolbox**: While not exclusively for computer vision, this
    toolbox is often used in conjunction with Computer Vision Toolbox. It provides
    fundamental image processing functions such as filtering, morphological operations,
    and image enhancement.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像处理工具箱**：虽然这个工具箱不仅仅用于计算机视觉，但它通常与计算机视觉工具箱一起使用。它提供了基本的图像处理函数，如滤波、形态学操作和图像增强。'
- en: '**Camera calibration**: MATLAB offers tools for camera calibration, which is
    essential for mapping 2D image points to 3D world coordinates. This is crucial
    for tasks such as 3D reconstruction and object tracking.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相机标定**：MATLAB提供相机标定工具，这对于将2D图像点映射到3D世界坐标至关重要。这对于3D重建和目标跟踪等任务至关重要。'
- en: '**Object detection and recognition**: MATLAB provides functions and pretrained
    models for object detection and recognition. You can use popular deep learning
    models such as YOLO, SSD, and Faster R-CNN for these tasks.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标检测和识别**：MATLAB提供用于目标检测和识别的函数和预训练模型。您可以使用YOLO、SSD和Faster R-CNN等流行的深度学习模型来完成这些任务。'
- en: '**Feature extraction**: MATLAB supports feature extraction techniques such
    as **Scale-Invariant Feature Transform** (**SIFT**), **Speeded-Up Robust Features**
    (**SURF**), and **Histogram of Oriented Gradients** (**HOG**) for object detection
    and matching.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：MATLAB支持特征提取技术，如**尺度不变特征变换**（**SIFT**）、**加速鲁棒特征**（**SURF**）和**方向梯度直方图**（**HOG**），用于目标检测和匹配。'
- en: '**Stereo vision**: MATLAB supports stereo vision techniques for depth estimation
    and 3D reconstruction from stereo camera setups.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**立体视觉**：MATLAB支持立体视觉技术，用于从立体相机设置中进行深度估计和3D重建。'
- en: '**Motion analysis**: You can perform motion analysis tasks such as optical
    flow estimation, tracking, and motion segmentation using MATLAB functions.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运动分析**：您可以使用MATLAB函数执行运动分析任务，例如光流估计、跟踪和运动分割。'
- en: '**Machine learning and deep learning**: MATLAB integrates with various machine
    learning and deep learning frameworks, making it suitable for training custom
    models for computer vision tasks.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习和深度学习**：MATLAB与各种机器学习和深度学习框架集成，使其适合为计算机视觉任务训练自定义模型。'
- en: '**Semantic segmentation**: MATLAB includes tools for semantic segmentation,
    which is the process of labeling each pixel in an image with the class it belongs
    to.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义分割**：MATLAB 包含用于语义分割的工具，这是一个将图像中的每个像素标记为其所属类别的过程。'
- en: '**Point cloud processing**: For 3D point cloud data, MATLAB provides tools
    for visualization, manipulation, and analysis.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点云处理**：对于 3D 点云数据，MATLAB 提供了用于可视化、操作和分析的工具。'
- en: '**Apps**: MATLAB offers interactive apps for tasks such as image labeling,
    camera calibration, and object training, which simplify the development workflow.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用**：MATLAB 提供了用于图像标记、相机校准和对象训练等任务的交互式应用，这些应用简化了开发工作流程。'
- en: '**Parallel computing**: MATLAB supports parallel computing, allowing you to
    speed up computationally intensive computer vision tasks by leveraging multiple
    CPU cores or GPUs.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行计算**：MATLAB 支持并行计算，允许您通过利用多个 CPU 核心或 GPU 来加速计算密集型计算机视觉任务。'
- en: These tools and functions make MATLAB a powerful environment for developing
    and prototyping computer vision applications, whether you’re working on image
    analysis, object detection, 3D reconstruction, or any other related tasks. Now,
    let’s learn how to recognize an object using MATLAB and CNN.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具和函数使 MATLAB 成为开发原型计算机视觉应用的强大环境，无论您是在进行图像分析、对象检测、3D 重建还是其他相关任务。现在，让我们学习如何使用
    MATLAB 和 CNN 识别对象。
- en: Building a MATLAB model for object recognition
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建用于物体识别的 MATLAB 模型
- en: An enduring challenge within the realm of computer vision involves ascertaining
    the presence of specific objects (object recognition) or activities within an
    image. For objects under predefined conditions, such as the identification of
    specific geometric shapes such as polyhedral or the recognition of faces and handwritten
    characters, this problem can be tackled effectively and without significant hurdles.
    However, the complexity escalates when dealing with arbitrary objects in unrestricted
    scenarios.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉领域，一个持续的挑战是在图像中确定特定对象（物体识别）或活动的存在。对于在预定义条件下（如识别特定的几何形状，如多面体或识别面部和手写字符）的对象，这个问题可以有效地解决，且没有重大障碍。然而，当处理不受限制场景中的任意对象时，复杂性会急剧增加。
- en: Object recognition entails the capacity to detect a particular object within
    a series of images or videos. Human beings possess the remarkable ability to identify
    various objects in images effortlessly, even when the objects’ appearances may
    vary. Moreover, objects can be recognized even when they are partially obscured
    from view. However, this remains a formidable challenge for computer vision.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 物体识别涉及在一系列图像或视频中检测特定对象的能力。人类具有在图像中轻松识别各种对象的能力，即使这些对象的形状可能有所不同。此外，即使对象部分被遮挡，也可以识别对象。然而，这仍然是计算机视觉中的一个巨大挑战。
- en: Each object within an image exhibits a multitude of intriguing characteristics
    that can be extracted to construct a comprehensive description of the object.
    This description serves to identify the object when seeking it within a test image
    containing multiple objects. Crucially, the set of characteristics that are extracted
    from the reference image must be resilient to variations in image scale, disturbances,
    lighting conditions, and geometric distortions to ensure dependable recognition.
    CNNs excel in this endeavor, offering algorithms with exceptional object recognition
    performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的每个对象都表现出多种引人入胜的特征，这些特征可以提取出来，构建一个关于该对象的全面描述。这个描述有助于在包含多个对象的测试图像中寻找对象时识别该对象。关键的是，从参考图像中提取的特征必须对图像尺度、干扰、光照条件和几何畸变的变化具有鲁棒性，以确保可靠的识别。卷积神经网络（CNN）在这方面表现出色，提供了具有卓越物体识别性能的算法。
- en: Introducing handwriting recognition (HWR)
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍手写识别（HWR）
- en: HWR encompasses the computer’s capability to receive and comprehend handwritten
    input, transforming it into readable text. This input can originate from various
    sources, including paper documents, photographs, and touchscreens. Detection of
    written text can be accomplished through optical scanning, which involves **optical
    character recognition** (**OCR**), or through intelligent word recognition techniques.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 手写识别（HWR）包括计算机接收和理解手写输入的能力，将其转换为可读文本。这种输入可以来自各种来源，包括纸质文件、照片和触摸屏。通过光学扫描可以实现文本检测，这涉及**光学字符识别**（**OCR**），或通过智能文字识别技术。
- en: We have long been acutely aware of the challenge of automating HWR to facilitate
    smoother interactions between humans and machines. In recent years, this challenge
    has witnessed intriguing advancements and increasingly efficient solutions, primarily
    fueled by substantial economic interest and the growing computational capabilities
    of modern computers. Notably, certain countries, such as Japan and various other
    Asian nations, have made significant investments in research and financial resources
    to pioneer cutting-edge OCR technologies.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们长期以来一直清醒地意识到自动化HWR以促进人与机器之间更顺畅互动的挑战。近年来，这一挑战见证了引人入胜的进步和越来越有效的解决方案，这主要得益于巨大的经济利益和现代计算机计算能力的增长。值得注意的是，某些国家，如日本和亚洲的其他国家，在研究和财务资源方面进行了重大投资，以开创尖端OCR技术。
- en: The rationale behind the enthusiasm of these countries in this research domain
    is quite evident. Their goal is to develop devices capable of interpreting the
    intricate ideograms that characterize their respective cultures, thereby enhancing
    the ease of interaction with machines. Given that there are currently no input
    devices, such as keyboards, capable of representing thousands of characters, the
    focus is on acquiring this information directly from handwritten scripts through
    digitized scanning.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些国家在这个研究领域热情洋溢背后的原因非常明显。他们的目标是开发能够解读各自文化中特有的复杂象形文字的设备，从而提高与机器互动的便捷性。鉴于目前尚无能够表示数千个字符的输入设备，如键盘，因此重点在于直接从手写文稿通过数字化扫描获取这些信息。
- en: Nevertheless, even in Western countries, substantial attention has been dedicated
    to the field of optical HWR. There exist numerous applications that stand to benefit
    from automated text interpretation. Consider, for instance, the automatic parsing
    of preprinted templates or the recognition of addresses and postal codes on envelopes,
    which are just a few instances where OCR technology proves invaluable.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，即使在西方国家，也投入了大量精力研究光学HWR领域。存在许多可以从自动文本识别中受益的应用程序。例如，自动解析预印模板或识别信封上的地址和邮政编码，这只是OCR技术在几个实例中证明其价值的地方。
- en: HWR is accomplished using diverse techniques that typically involve OCR. Nevertheless,
    a comprehensive script recognition system goes beyond OCR and encompasses tasks
    such as formatting, accurate character segmentation, and identifying the most
    probable words.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 手写识别（HWR）是通过多种技术实现的，通常涉及OCR技术。然而，一个全面的脚本识别系统不仅超越了OCR，还包括格式化、精确字符分割和识别最可能的单词等任务。
- en: To better understand how HWR can be approached using machine learning methodologies,
    we will use a very popular dataset that’s largely used in the community to address
    this type of topic. This is the **Modified National Institute of Standards and
    Technology** (**MNIST**), a large database of handwritten digits. This dataset
    consists of 70,000 data examples, which is a subset of a larger dataset maintained
    by NIST. These examples represent digits and are in a format of 28 x 28-pixel
    resolution, organized as a matrix with 70,000 rows and 785 columns. In each row,
    there are 784 columns corresponding to pixel values from the 28 x 28 matrix, and
    one column containing the actual digit label. These digits have been size-normalized
    and positioned at the center of a fixed-size image.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解如何使用机器学习方法来处理HWR，我们将使用一个在社区中广泛使用的非常流行的数据集来解决这个问题。这是**修改后的国家标准与技术研究院**（**MNIST**）数据集，一个包含大量手写数字的大型数据库。这个数据集包含70,000个数据示例，这是NIST维护的更大数据集的一个子集。这些示例代表数字，以28
    x 28像素分辨率的格式呈现，组织成一个有70,000行和785列的矩阵。在每一行中，有784列对应于28 x 28矩阵中的像素值，还有一列包含实际的数字标签。这些数字已经被尺寸归一化，并放置在固定大小图像的中心。
- en: The digit images within the MNIST dataset were originally chosen and processed
    by Chris Burges and Corinna Cortes, who employed bounding box normalization and
    centering techniques. Yann LeCun’s version of the dataset, on the other hand,
    utilizes centering based on the center of mass within a larger window.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集中的数字图像最初是由Chris Burges和Corinna Cortes选择的，他们采用了边界框归一化和居中技术。另一方面，Yann
    LeCun的数据集版本使用的是基于更大窗口内质心的居中技术。
- en: 'This dataset is already available in the MATLAB environment, in a short version
    with only 10,000 images evenly distributed over the 10 digits (0-9). Let’s get
    started:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集已经在MATLAB环境中可用，是一个简短版本，只有10,000张图像，均匀分布在10个数字（0-9）上。让我们开始吧：
- en: 'First, we will import the dataset into the MATLAB workspace. The dataset is
    available in the standard MATLAB installation with Deep Learning Toolbox. Under
    our MATLAB installation folder, we will find the `toolbox\nnet\nndemos\nndatasets\DigitDataset`
    path. The `DigitDataset` folder contains 10 subfolders, each of which contains
    1,000 images of a single digit; each folder is named after the digit it contains.
    To find the path in an automated way, we can use the following command:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将数据集导入MATLAB工作空间。该数据集包含在带有深度学习工具箱的标准MATLAB安装中。在我们的MATLAB安装文件夹下，我们将找到`toolbox\nnet\nndemos\nndatasets\DigitDataset`路径。`DigitDataset`文件夹包含10个子文件夹，每个子文件夹包含单个数字的1,000张图像；每个文件夹都是以它包含的数字命名的。为了以自动化的方式找到路径，我们可以使用以下命令：
- en: '[PRE0]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `imageDatastore()` function generates a data store by incorporating the
    dataset’s path for a specified collection of image data. We passed three arguments:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`imageDatastore()`函数通过包含数据集路径来生成一个数据存储，用于指定的图像数据集合。我们传递了三个参数：'
- en: '`FolderPath`: The path of the folder containing the images'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FolderPath`：包含图像的文件夹路径'
- en: '`IncludeSubfolders`: The possibility to include all the subfolders in the main
    folder'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IncludeSubfolders`：包括主文件夹中所有子文件夹的可能性'
- en: '`LabelSource`: We used the folder names to label the data'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LabelSource`：我们使用文件夹名称来标记数据'
- en: An `ImageDatastore` object will be created with some properties. These properties
    delineate the characteristics of the data and provide instructions on how to retrieve
    data from the data store. When creating the data store object, you have the option
    to set these properties using name-value pairs as arguments. If you wish to inspect
    or adjust a property after the object’s creation, you can do so using the dot
    notation.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将创建一个具有一些属性的`ImageDatastore`对象。这些属性描述了数据的特征，并提供了从数据存储中检索数据的说明。在创建数据存储对象时，您可以使用名称值对作为参数来设置这些属性。如果您希望在对象创建后检查或调整属性，可以使用点符号来完成。
- en: 'Let’s look at some of these properties:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看这些属性中的一些：
- en: '`Files`: This is a cell array that stores the file paths of all the images
    in the data store. You can access this property to get a list of file paths.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Files`：这是一个单元数组，用于存储数据存储中所有图像的文件路径。您可以通过访问此属性来获取文件路径列表。'
- en: '`Labels`: This is an array or cell array that associates labels or categories
    with each image in the data store. This property is often used for image classification
    tasks.'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Labels`：这是一个数组或单元数组，将标签或类别与数据存储中的每个图像关联起来。此属性通常用于图像分类任务。'
- en: '`ReadSize`: This specifies the number of images to read at once during data
    iteration. This can impact memory usage and performance.'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadSize`：这指定了在数据迭代过程中一次读取的图像数量。这可能会影响内存使用和性能。'
- en: 'Now, we can display a selection of images that have been loaded via a random
    selection process:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们可以通过随机选择过程显示一些已加载的图像：
- en: '[PRE1]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we have used the `randperm()` function, which generates a row vector
    comprising nine distinct random integers chosen from a range of 1 to 10,000\.
    Each number that was generated was used as an index to identify an image file
    path stored in the `Data.Files` property. *Figure 8**.4* shows a collage of nine
    images extracted from the database:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`randperm()`函数，该函数生成一个包含从1到10,000范围内选择的九个不同随机整数的行向量。生成的每个数字都被用作索引，以识别存储在`Data.Files`属性中的图像文件路径。*图8**.4*显示了从数据库中提取的九张图像的拼贴：
- en: '![Figure 8.4 – The MNIST dataset](img/B21156_08_04.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – MNIST数据集](img/B21156_08_04.jpg)'
- en: Figure 8.4 – The MNIST dataset
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – MNIST数据集
- en: 'Now, we can check the distribution of the images over the classes:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以检查图像在类别上的分布：
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following results are returned:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With this, we have proof that the images are evenly distributed across the 10
    digits (0-9).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们有证据表明图像在10个数字（0-9）之间均匀分布。
- en: 'Before we start building the machine learning algorithm, it’s imperative to
    partition the available data into two distinct subsets. The first subset will
    serve as the training data, while the second will be earmarked for algorithm validation.
    Data partitioning plays a pivotal role in machine learning and data analysis as
    it involves segregating a dataset into multiple subsets for training, validation,
    and model testing. In our case, we have 10,000 samples, each containing 1,000
    images for a specific digit. Our chosen approach is to split the data into a 70%
    portion for training and a 30% portion for validation. This split rate is used
    because most of the data must be used for training the network. Accordingly, we
    will employ 7,000 samples for training and reserve the remainder for validation:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们开始构建机器学习算法之前，将可用的数据分成两个不同的子集至关重要。第一个子集将作为训练数据，而第二个子集将用于算法验证。数据分区在机器学习和数据分析中起着关键作用，因为它涉及将数据集分割成多个子集进行训练、验证和模型测试。在我们的案例中，我们有
    10,000 个样本，每个样本包含 1,000 张特定数字的图像。我们选择的方法是将数据分成 70% 用于训练和 30% 用于验证的部分。这个分割率被使用是因为大部分数据必须用于训练网络。因此，我们将使用
    7,000 个样本进行训练，并将剩余的部分保留用于验证：
- en: '[PRE4]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To achieve this, we utilized the `splitEachLabel()` function. This function
    effectively separates the image files within the `Data` dataset into two separate
    data stores: `TrainDat` and `ValDat`. The `TrainDat` data store consists of an
    initial portion of each category determined by `SplitRate`, while the `ValDat`
    data store contains the remaining images from each class. `SplitRate` can take
    the form of a fractional value ranging from 0 to 1, indicating the proportion
    of images allocated to `TrainDat`, or it can be an integer denoting the exact
    count of images assigned to `TrainDat` for each class.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们使用了 `splitEachLabel()` 函数。这个函数有效地将 `Data` 数据集中的图像文件分成两个独立的数据存储：`TrainDat`
    和 `ValDat`。`TrainDat` 数据存储包含每个类别由 `SplitRate` 确定的初始部分，而 `ValDat` 数据存储包含每个类别的剩余图像。`SplitRate`
    可以是 0 到 1 之间的分数值，表示分配给 `TrainDat` 的图像比例，或者它可以是一个整数，表示每个类别分配给 `TrainDat` 的确切图像数量。
- en: 'Let’s begin constructing our convolutional network. As expected, a CNN consists
    of a sequence of interconnected layers. To commence, you’ll need to employ a layer
    so that you can import your input data:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始构建我们的卷积网络。正如预期的那样，CNN 由一系列相互连接的层组成。首先，你需要使用一个层来导入你的输入数据：
- en: '[PRE5]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `layers` variable is an array that contains the list of layers for our
    CNN, defining the architecture of the neural network used for deep learning. To
    initiate this architecture, we begin with `imageInputLayer`. This layer functions
    as an input for images, accepting 2D image data into the neural network and performing
    data normalization. Additionally, this layer specifies the unmodifiable `InputSize`
    attribute. This attribute contains the height, width, and number of channels,
    respectively. In this case, we are working with grayscale images with a height
    of 28 and a width of 28\. Following the input layer, we establish the initial
    block of three consecutive layers:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`layers` 变量是一个数组，包含我们 CNN 的层列表，定义了用于深度学习的神经网络架构。为了启动这个架构，我们首先使用 `imageInputLayer`。这个层作为图像的输入，将
    2D 图像数据输入到神经网络中，并执行数据归一化。此外，这个层还指定了不可修改的 `InputSize` 属性。该属性分别包含高度、宽度和通道数。在这种情况下，我们正在处理高度和宽度均为
    28 的灰度图像。在输入层之后，我们建立了三个连续层的初始块：'
- en: '[PRE6]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Downsampling is accomplished via a 2D max pooling layer, which divides the input
    into rectangular pooling regions and subsequently identifies the maximum value
    within each of these regions.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过 2D 最大池化层实现下采样，该层将输入分成矩形池化区域，并随后在每个这些区域中识别最大值。
- en: 'Next, we will introduce a second set of layers, akin to the first set, with
    adjusted parameters:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍第二组层，类似于第一组，但参数已调整：
- en: '[PRE7]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we add a third block of layers, to finish the architecture:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们添加第三个层块，以完成架构：
- en: '[PRE8]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In an FC layer, the input undergoes multiplication with a weight matrix and
    is subsequently adjusted by a bias vector. The “parameter” in this context specifies
    the desired output size, which, in our case, is 10 since we are classifying `10`
    distinct digits. This layer type mirrors the layer configuration commonly found
    in a traditional `softmaxLayer` is a specialized layer that’s employed within
    neural networks, designed specifically to implement the `softmax` function on
    its input. The `softmax` function finds extensive use in classification tasks
    as it transforms raw scores or logits into a probability distribution spanning
    multiple classes. Typically, this layer serves as the concluding component in
    a neural network designed for multi-class classification. It transforms the network’s
    output values into probabilities that collectively sum to `1`, simplifying the
    interpretation of the model’s predictions.   classificationLayer];
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在一个全连接层（FC layer）中，输入与权重矩阵相乘，随后由偏置向量调整。在此上下文中，“参数”指定了期望的输出大小，在我们的案例中，由于我们要对`10`个不同的数字进行分类，所以是10。这种类型的层与在传统`softmaxLayer`中常见的层配置相类似。`softmaxLayer`是一个专门用于神经网络中的层，旨在对其输入实现`softmax`函数。`softmax`函数在分类任务中得到了广泛的应用，因为它将原始分数或logits转换为跨越多个类别的概率分布。通常，此层作为设计用于多类分类的神经网络中的最后一个组件。它将网络的输出值转换为概率，这些概率的总和为`1`，简化了对模型预测的解释。   classificationLayer];
- en: The classification layer calculates cross-entropy loss for both regular and
    weighted classification tasks that pertain to distinct and mutually exclusive
    classes. It automatically deduces the number of classes by inspecting the dimensions
    of the output originating from the preceding layer.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类层计算与不同且互斥类别相关的常规和加权分类任务的交叉熵损失。它通过检查前一层的输出维度来自动推断类别的数量。
- en: 'Before training the CNN, we need to configure the settings:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练CNN之前，我们需要配置设置：
- en: '[PRE9]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To learn more about the various training options, please read [*Chapter 6*](B21156_06.xhtml#_idTextAnchor124),
    *Deep Learning and Convolutional* *Neural Networks*.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解更多关于各种训练选项的信息，请阅读[*第6章*](B21156_06.xhtml#_idTextAnchor124)，*深度学习和卷积神经网络*。
- en: 'It’s time to train the network:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是时候训练网络了：
- en: '[PRE10]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following plot was printed on the screen (*Figure 8**.5*):'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下图在屏幕上打印出来（*图8**.5*）：
- en: '![Figure 8.5 – Training process of the CNN for handwritten digit recognition](img/B21156_08_05.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 手写数字识别CNN的训练过程](img/B21156_08_05.jpg)'
- en: Figure 8.5 – Training process of the CNN for handwritten digit recognition
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 手写数字识别CNN的训练过程
- en: This plot will undergo continuous updates as the training progresses, enabling
    us to monitor how the algorithm adapts the weights to achieve convergence. We
    can see the results with an accuracy of 84.1%, which indicates a good result.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的进行，此图将不断更新，使我们能够监控算法如何调整权重以实现收敛。我们可以看到准确率为84.1%的结果，这表明效果良好。
- en: It is natural to ask whether it is possible to improve the performance of the
    handwritten digit recognition model. In the next section, we will see how to improve
    the accuracy of the model using transfer learning.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地会问，是否有可能提高手写数字识别模型的性能。在下一节中，我们将看到如何使用迁移学习来提高模型的准确率。
- en: Training and fine-tuning pretrained deep learning models in MATLAB
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MATLAB中训练和微调预训练的深度学习模型
- en: Transfer learning is a machine learning approach wherein a model created for
    a particular task is repurposed as the initial foundation for a model addressing
    a second task. This technique entails leveraging knowledge acquired from one problem
    and applying it to a distinct yet related problem. Transfer learning is particularly
    useful in deep learning and neural networks, where pretrained models can be fine-tuned
    or used as feature extractors for new tasks.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一种机器学习方法，其中为特定任务创建的模型被重新用于作为解决第二个任务的模型的初始基础。这种技术涉及利用从一个问题中获得的知识并将其应用于一个不同但相关的问题。迁移学习在深度学习和神经网络中特别有用，其中预训练模型可以被微调或用作新任务的特征提取器。
- en: In pretrained models, you start with a pretrained model that has been trained
    on a large dataset for a specific task, such as image classification, natural
    language processing, or speech recognition. These pretrained models are often
    complex neural networks with many layers. In many cases, you can use the layers
    of the pretrained model as feature extractors. You remove the final classification
    layer(s) and use the activations from the earlier layers as features for your
    new task. This is especially common in computer vision tasks. Optionally, you
    can fine-tune the pretrained model on your specific task by training it further
    with your own dataset. This involves updating the weights of some or all layers
    while keeping the knowledge you’ve learned from the original task.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练模型中，你从一个已经针对特定任务（如图像分类、自然语言处理或语音识别）在大数据集上训练过的预训练模型开始。这些预训练模型通常是具有许多层的复杂神经网络。在许多情况下，你可以使用预训练模型的层作为特征提取器。你移除最终的分类层（们），并使用早期层的激活作为新任务的特性。这在计算机视觉任务中尤为常见。你可以选择性地通过使用自己的数据集进一步训练预训练模型来微调预训练模型，以针对特定任务进行训练。这涉及到更新一些或所有层的权重，同时保留从原始任务中学到的知识。
- en: Transfer learning can significantly reduce the amount of data and time required
    to train a model for a new task, especially when you have a limited dataset. Pretrained
    models have already learned useful features from large and diverse datasets, which
    can be valuable for related tasks. It can help improve model performance when
    you have limited computational resources or limited labeled data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习可以显著减少训练新任务模型所需的数据量和时间，尤其是在数据集有限的情况下。预训练模型已经从大型和多样化的数据集中学习到了有用的特征，这对于相关任务非常有价值。它可以帮助提高模型性能，当你拥有有限的计算资源或有限的标记数据时。
- en: Transfer learning is commonly used in various fields, including computer vision,
    natural language processing, and audio processing, and has been a key technique
    in advancing the state of the art in machine learning applications.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习在各个领域都得到了广泛应用，包括计算机视觉、自然语言处理和音频处理，并且在推进机器学习应用领域的技术前沿中发挥了关键作用。
- en: Introducing the ResNet pretrained network
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入ResNet预训练网络
- en: ResNet stands for Residual Network, a profound CNN architecture introduced by
    Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in their 2015 paper titled
    *Deep Residual Learning for Image Recognition*. This groundbreaking architecture
    has had a substantial impact on the domains of computer vision and deep learning.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet代表残差网络，这是一种由Kaiming He、Xiangyu Zhang、Shaoqing Ren和Jian Sun在2015年发表的论文《用于图像识别的深度残差学习》中引入的深刻CNN架构。这一开创性的架构对计算机视觉和深度学习领域产生了重大影响。
- en: The key innovation in ResNet is the use of residual blocks. In traditional deep
    neural networks, as the network becomes deeper, it becomes increasingly difficult
    to train. This is because of the vanishing gradient problem, where gradients become
    extremely small as they are propagated back through the network during training.
    As a result, deep networks tend to suffer from degradation in performance as their
    depth increases.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet的关键创新在于使用了残差块。在传统的深度神经网络中，随着网络的加深，训练变得越来越困难。这是因为梯度消失问题，在训练过程中，随着梯度通过网络反向传播，其值变得极其小。因此，随着深度的增加，深度网络往往会遭受性能下降的问题。
- en: ResNet addresses this problem by introducing residual blocks, which contain
    skip or shortcut connections that allow the gradient to flow more easily through
    the network. These shortcut connections bypass one or more layers, making it easier
    to train very deep networks. The skip connections essentially learn the residual
    (the difference) between the output and input of a layer, hence the name “residual
    network.”
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet通过引入残差块来解决这一问题，这些残差块包含跳过或快捷连接，允许梯度更容易地通过网络流动。这些快捷连接绕过一层或多层，使得训练非常深的网络变得更容易。跳过连接本质上学习了层输出和输入之间的残差（差异），因此得名“残差网络”。
- en: 'We can summarize the following key features of ResNet:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结ResNet的以下关键特性：
- en: '**Deep architecture**: ResNet can be very deep, with hundreds or even thousands
    of layers, thanks to the use of residual blocks.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度架构**：由于使用了残差块，ResNet可以非常深，拥有数百甚至数千层。'
- en: '**Skip connections**: The skip connections allow gradients to propagate effectively,
    mitigating the vanishing gradient problem.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跳过连接**：跳过连接允许梯度有效地传播，减轻了梯度消失问题。'
- en: '**High accuracy**: ResNet achieved state-of-the-art performance on various
    image classification tasks, including the ImageNet Large Scale Visual Recognition
    Challenge.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高精度**：ResNet 在各种图像分类任务上实现了最先进的性能，包括 ImageNet 大规模视觉识别挑战赛。'
- en: '**Transfer learning**: Pretrained ResNet models are widely used as feature
    extractors or starting points for various computer vision tasks through transfer
    learning.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迁移学习**：预训练的 ResNet 模型通过迁移学习广泛用作特征提取器或各种计算机视觉任务的起点。'
- en: '**Architectural variations**: There are several ResNet architectures with different
    depths, such as ResNet-18, ResNet-34, ResNet-50, and deeper variants.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构变体**：有几个不同深度的 ResNet 架构，例如 ResNet-18、ResNet-34、ResNet-50 以及更深的变体。'
- en: ResNet has become a foundational architecture in deep learning, and its principles
    of skip connections and residual learning have influenced the design of many subsequent
    neural network architectures. It has been applied not only to image classification
    but also to various other computer vision tasks, including object detection, semantic
    segmentation, and more.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 已成为深度学习的基础架构，其跳跃连接和残差学习的原理影响了众多后续神经网络架构的设计。它不仅应用于图像分类，还应用于各种其他计算机视觉任务，包括目标检测、语义分割等。
- en: The MATLAB Deep Network Designer app
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MATLAB 深度网络设计器应用程序
- en: The MATLAB Deep Network Designer app is a **graphical user interface** (**GUI**)
    tool provided by MATLAB for designing, training, and analyzing deep neural networks.
    It’s part of the MATLAB Deep Learning Toolbox, which offers a comprehensive set
    of tools and functions for working with ANN and deep learning. You can visually
    design neural network architectures by adding layers, connecting them, and specifying
    their properties and parameters. This allows you to create custom network architectures
    easily.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: MATLAB 深度网络设计器应用程序是 MATLAB 提供的用于设计、训练和分析深度神经网络的**图形用户界面**（**GUI**）工具。它是 MATLAB
    深度学习工具箱的一部分，该工具箱提供了一套全面的工具和函数，用于处理人工神经网络和深度学习。您可以通过添加层、连接它们并指定它们的属性和参数来直观地设计神经网络架构。这使得您能够轻松创建自定义网络架构。
- en: 'The app provides a library of predefined layers that you can drag and drop
    into your network design. These layers include common types such as convolutional
    layers, FC layers, and more. The Deep Network Designer app simplifies the process
    of designing and training deep neural networks, making it more accessible for
    users who may not be familiar with deep learning concepts and programming. It’s
    a valuable tool for researchers, engineers, and data scientists working on machine
    learning and deep learning projects using MATLAB. Let’s take a closer look:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序提供了一系列预定义层，您可以将它们拖放到网络设计中。这些层包括常见的类型，如卷积层、全连接层等。深度网络设计器应用程序简化了设计和训练深度神经网络的过程，使得对于可能不熟悉深度学习概念和编程的用户来说更加易于使用。它是研究人员、工程师和数据科学家在
    MATLAB 上进行机器学习和深度学习项目时的宝贵工具。让我们更深入地了解一下：
- en: 'To open the Deep Network Designer app, simply click on the **Deep Learning**
    section of the **Apps** tab at the top of the MATLAB interface. In the **Deep
    Learning** section, you will find the **Deep Network Designer** icon. Click on
    this icon to open the **Deep Network Designer** app. Alternatively, you can open
    the Deep Network Designer app by entering the following command in the MATLAB
    command window:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要打开深度网络设计器应用程序，只需单击 MATLAB 界面顶部的**应用**标签页中的**深度学习**部分。在**深度学习**部分，您将找到**深度网络设计器**图标。单击此图标即可打开**深度网络设计器**应用程序。或者，您可以在
    MATLAB 命令窗口中输入以下命令来打开深度网络设计器应用程序：
- en: '[PRE11]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This command will launch the app, allowing you to create, design, and train
    deep neural networks using a GUI. You need to have the MATLAB Deep Learning Toolbox
    installed to use the Deep Network Designer app. If it’s not already installed,
    you may need to install it separately from your MATLAB installation.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此命令将启动应用程序，允许您使用 GUI 创建、设计和训练深度神经网络。您需要安装 MATLAB 深度学习工具箱才能使用深度网络设计器应用程序。如果尚未安装，您可能需要从
    MATLAB 安装中单独安装它。
- en: 'The following window will open (*Figure 8**.6*):'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将打开以下窗口（*图 8**.6*）：
- en: '![Figure 8.6 – Deep Network Designer Start Page](img/B21156_08_06.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 深度网络设计器起始页面](img/B21156_08_06.jpg)'
- en: Figure 8.6 – Deep Network Designer Start Page
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 深度网络设计器起始页面
- en: 'If you don’t find the network, you can use the following command:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您找不到网络，可以使用以下命令：
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the image pretrained network, we will find ResNet-18\. ResNet-18 consists
    of 18 layers, which include convolutional layers, residual blocks, and FC layers.
    It’s considered relatively shallow compared to deeper variants such as ResNet-50
    or ResNet-101\. Like all ResNet architectures, ResNet-18 employs residual blocks.
    These blocks contain skip connections (or shortcut connections) that allow gradients
    to flow more effectively during training, addressing the vanishing gradient problem.
    ResNet-18 has been widely adopted in the deep learning community due to its balance
    between model complexity and performance. It’s often used in tasks such as image
    classification, object detection, and feature extraction in various computer vision
    applications. Its architectural principles, such as residual learning, have influenced
    the design of many subsequent neural network architectures. To import ResNet-18,
    it is necessary to install the relative toolbox.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练的网络图像中，我们将找到ResNet-18。ResNet-18由18层组成，包括卷积层、残差块和全连接层。与更深层的变体（如ResNet-50或ResNet-101）相比，它被认为相对较浅。像所有ResNet架构一样，ResNet-18使用残差块。这些块包含跳跃连接（或快捷连接），允许在训练期间更有效地流动梯度，解决梯度消失问题。由于其在模型复杂性和性能之间的平衡，ResNet-18在深度学习社区中得到广泛应用。它常用于图像分类、目标检测以及各种计算机视觉应用中的特征提取。其架构原则，如残差学习，影响了后续许多神经网络架构的设计。要导入ResNet-18，需要安装相应的工具箱。
- en: 'After importing it, we can import the dataset that we’ll train the network
    on. To do that, we can move to the **Data** tab of the app. Upon clicking the
    **Import Data** icon, the following window will open (*Figure 8**.7*):'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入后，我们可以导入我们将训练网络的数据库。为此，我们可以切换到应用程序的**数据**标签。点击**导入数据**图标后，将打开以下窗口（*图8**.7*）：
- en: '![Figure 8.7 – The Import Image Data window](img/B21156_08_07.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – 导入图像数据窗口](img/B21156_08_07.jpg)'
- en: Figure 8.7 – The Import Image Data window
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 导入图像数据窗口
- en: 'Under **Data source**, we can select **ImageDatastore in workspace** to select
    the data that’s already been imported into the MATLAB workspace, as indicated
    in the previous section. The MNIST dataset will be imported, as shown in the *Figure
    8**.8*:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在**数据源**下，我们可以选择**工作空间中的ImageDatastore**来选择已经导入到MATLAB工作空间中的数据，如前所述。MNIST数据集将按*图8**.8*所示导入：
- en: '![Figure 8.8 – The MNIST dataset imported into the Deep Network Designer app](img/B21156_08_08.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 导入到深度网络设计器应用程序中的MNIST数据集](img/B21156_08_08.jpg)'
- en: Figure 8.8 – The MNIST dataset imported into the Deep Network Designer app
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 导入到深度网络设计器应用程序中的MNIST数据集
- en: In *Figure 8**.8*, we can see that a good distribution of the images is regularly
    present in the 10 classes, with 700 images for each class. We will use 70% of
    the data for training and the rest for validation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图8**.8*中，我们可以看到图像在10个类别中分布良好，每个类别有700个图像。我们将使用70%的数据进行训练，其余的用于验证。
- en: At this point, we can set the pretrained network (ResNet-18) by moving to the
    **Designer** tab. We’ll use ResNet-18 for another type of image (an RGB image
    whose size is 227 x 227 x 3). To do this, we have to change the first layer, which
    defines the size of the input data. Click on the first layer (**ImageInput**)
    and then click on the **Canc** button to remove this layer.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以通过切换到“**设计器**”标签来设置预训练网络（ResNet-18）。我们将使用ResNet-18处理另一种类型的图像（一个大小为227
    x 227 x 3的RGB图像）。为此，我们必须更改定义输入数据大小的第一层。点击第一层（**ImageInput**），然后点击**Canc**按钮来删除此层。
- en: After that, we can click on the **ImageInputLayer** icon in **Layer Library**
    to the left of the tab, at which point we have to connect this layer with the
    next layer.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们可以点击标签左侧的**ImageInputLayer**图标在**层库**中，此时我们必须将此层与下一层连接。
- en: After that, we have to change the `28,28,1` in the **Properties** window to
    the right of the **Designer** tab (*Figure 8**.9*).
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们必须将位于“**设计器**”标签右侧的“**属性**”窗口中的`28,28,1`更改为（*图8**.9*）。
- en: 'After that, we have to change the first convolutional layer. First, we must
    remove that and then drop a `Convolution2Dlayer` and connect it to another layer.
    Then, we have to set `3,3` and `64` in the **Properties** window to the right
    of the **Designer** tab (*Figure 8**.9*):'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们必须更改第一层卷积。首先，我们必须删除它，然后放下一个`Convolution2Dlayer`并将其连接到另一层。然后，我们必须在“**设计器**”标签右侧的“**属性**”窗口中设置`3,3`和`64`（*图8**.9*）：
- en: '![Figure 8.9 – Modifying the first two layers of ResNet-18 to adapt it to the
    new input data](img/B21156_08_09.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9 – 修改ResNet-18的前两层以适应新的输入数据](img/B21156_08_09.jpg)'
- en: Figure 8.9 – Modifying the first two layers of ResNet-18 to adapt it to the
    new input data
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – 修改ResNet-18的前两层以适应新的输入数据
- en: 'Now, we have to move on to the final part of ResNet-18, which involves setting
    the classification option. To do that, we have to replace the FC layer and the
    classification layer, as shown in *Figure 8**.10*:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须继续到ResNet-18的最后一部分，这涉及到设置分类选项。为了做到这一点，我们必须替换FC层和分类层，如图*图8**.10*所示：
- en: '![Figure 8.10 – Modifying the final layers of ResNet-18 to adapt it to the
    new classification](img/B21156_08_10.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10 – 修改ResNet-18的最后一层以适应新的分类](img/B21156_08_10.jpg)'
- en: Figure 8.10 – Modifying the final layers of ResNet-18 to adapt it to the new
    classification
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – 修改ResNet-18的最后一层以适应新的分类
- en: To check that the layer has been modified correctly, we can test it by clicking
    on the **Analyze** icon at the top of the **Designer** tab.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查层是否已正确修改，我们可以通过点击**设计师**标签页顶部的**分析**图标来测试它。
- en: 'Now that we are ready to train the model, we can move to the **Training** tab
    of the Deep Network Designer app. We can check the training option by clicking
    on the **Training option** icon at the top of the **Training** tab. After that,
    we can push the **Train** button at the top of the tab. The training process will
    start; we will check its progress in the **Training Progress** window (*Figure
    8**.11*):'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好训练模型，我们可以转到Deep Network Designer应用程序的**训练**标签页。我们可以通过点击**训练**标签页顶部的**训练选项**图标来检查训练选项。之后，我们可以在标签页顶部点击**训练**按钮。训练过程将开始；我们将在**训练进度**窗口中检查其进度（*图8**.11*）：
- en: '![Figure 8.11 – The Training Progress window](img/B21156_08_11.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图8.11 – 训练进度窗口](img/B21156_08_11.jpg)'
- en: Figure 8.11 – The Training Progress window
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 – 训练进度窗口
- en: At the end of the training process, we will be able to verify the performance
    of the model by reading the accuracy value that was obtained in the validation
    procedure. As we can see, we obtained an accuracy of 90.27%.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程结束时，我们将能够通过读取验证过程中获得的确切值来验证模型的性能。正如我们所看到的，我们获得了90.27%的准确率。
- en: Now, let’s try to collect some useful information on how to interpret the results
    that have been obtained from a model based on machine learning.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试收集一些关于如何根据机器学习模型获得的结果进行解释的有用信息。
- en: Interpreting and explaining machine learning models
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释和说明机器学习模型
- en: Interpreting and explaining machine learning models is essential for understanding
    their predictions and making them more transparent and trustworthy, especially
    in applications where interpretability is critical. This is an ongoing process
    that requires collaboration between data scientists, domain experts, and stakeholders.
    The choice of interpretation techniques depends on the model type, problem domain,
    and level of transparency required for the application. It’s important to strike
    a balance between model complexity and interpretability, depending on the specific
    use case.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 解释和说明机器学习模型对于理解它们的预测、使它们更加透明和可信至关重要，尤其是在解释性至关重要的应用中。这是一个需要数据科学家、领域专家和利益相关者之间协作的持续过程。解释技术选择取决于模型类型、问题域和应用程序所需的透明度水平。根据具体用例，在模型复杂性和可解释性之间取得平衡很重要。
- en: Understanding saliency maps
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解显著性图
- en: Saliency maps are a visualization technique that’s used in computer vision and
    deep learning to understand and interpret neural network predictions, particularly
    in image classification and object recognition tasks. Saliency maps help identify
    which regions of an input image or feature map are most relevant to a model’s
    prediction. They are especially useful for gaining insights into why a neural
    network is making a particular decision.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性图是一种可视化技术，用于计算机视觉和深度学习，以理解和解释神经网络预测，特别是在图像分类和对象识别任务中。显著性图有助于识别输入图像或特征图中哪些区域与模型的预测最相关。它们特别有助于深入了解神经网络为何做出特定决策。
- en: 'Saliency maps are generated using gradient-based methods, typically backpropagation.
    The idea is to compute the gradients of the model’s output concerning the input
    image’s pixels. By calculating these gradients, you can identify which pixels
    in the input image have the most significant impact on the model’s prediction.
    In other words, saliency maps highlight the regions that the model pays attention
    to when making a decision. Saliency maps are usually visualized as heatmaps overlaid
    on the original input image. In a heatmap, the intensity of color corresponds
    to the importance of each pixel. High-intensity areas indicate regions that strongly
    influence the model’s output:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性图是通过基于梯度的方法生成的，通常是反向传播。其思路是计算模型输出相对于输入图像像素的梯度。通过计算这些梯度，您可以确定输入图像中哪些像素对模型的预测影响最大。换句话说，显著性图突出了模型在做出决策时关注的区域。显著性图通常以热图的形式叠加在原始输入图像上。在热图中，颜色的强度对应于每个像素的重要性。高强度的区域表示对模型输出有强烈影响的区域：
- en: '![Figure 8.12 – A saliency map as a heatmap](img/B21156_08_12.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图8.12 – 显著性图作为热图](img/B21156_08_12.jpg)'
- en: Figure 8.12 – A saliency map as a heatmap
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – 显著性图作为热图
- en: Saliency maps provide interpretability to neural network predictions. By examining
    the saliency map, you can see which parts of the image resemble specific features
    and contribute to the decision. These maps can be used for model debugging and
    improvement. If the model’s predictions appear incorrect, examining the saliency
    map can reveal whether the model is focusing on the right or wrong features. There
    are variations of saliency maps, such as class-specific saliency maps (highlighting
    features specific to a particular class) and gradient-based approaches such as
    guided backpropagation and SmoothGrad, which improve the interpretability of saliency
    maps.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性图提供了对神经网络预测的解释性。通过检查显著性图，您可以查看图像的哪些部分与特定特征相似并有助于决策。这些图可用于模型调试和改进。如果模型的预测看起来不正确，检查显著性图可以揭示模型是否关注正确的或错误的功能。显著性图有几种变体，例如特定类别的显著性图（突出显示特定类别的特定特征）和基于梯度的方法，如引导反向传播和SmoothGrad，这些方法提高了显著性图的可解释性。
- en: It’s important to note that saliency maps provide insights into a model’s behavior
    but do not necessarily explain why a neural network made a particular decision
    in human-understandable terms. They are just one tool in the interpretability
    toolbox and are often used in conjunction with other techniques for a more comprehensive
    understanding of model decisions.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，显著性图提供了对模型行为的洞察，但并不一定以人类可理解的方式解释为什么神经网络做出了特定的决策。它们只是解释工具箱中的一个工具，通常与其他技术结合使用，以更全面地理解模型决策。
- en: Understanding feature importance scores
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解特征重要性分数
- en: Feature importance scores are a set of metrics or values that indicate the relative
    importance of different input features (also known as variables or attributes)
    in a machine learning model’s prediction. These scores help data scientists and
    analysts understand which features have the most significant influence on the
    model’s output. Feature importance scores are particularly valuable for feature
    selection, model interpretation, and debugging.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性分数是一组指标或值，表示不同输入特征（也称为变量或属性）在机器学习模型预测中的相对重要性。这些分数有助于数据科学家和分析人员了解哪些特征对模型输出的影响最大。特征重要性分数对于特征选择、模型解释和调试特别有价值。
- en: 'There are some common methods and techniques for calculating feature importance
    scores:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一些常见的方法和技术用于计算特征重要性分数：
- en: '**Gini importance**: In decision trees and random forests, Gini importance
    measures how often a feature is used for splitting data across the tree nodes.
    Higher values indicate more important features.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gini重要性**：在决策树和随机森林中，Gini重要性衡量特征在树节点上分割数据时的使用频率。较高的值表示更重要的特征。'
- en: '**Mean decrease in impurity**: Similar to Gini importance, this metric calculates
    how much the impurity (or impurity reduction) decreases when a particular feature
    is used for splitting.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**杂质减少的平均值**：与Gini重要性相似，此指标计算在特定特征用于分割时，杂质（或杂质减少）减少的程度。'
- en: '**Coefficient magnitude**: In linear models, the magnitude (absolute value)
    of the coefficients represents the feature’s importance. Larger coefficients indicate
    greater importance.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系数幅度**：在线性模型中，系数的幅度（绝对值）表示特征的重要性。较大的系数表示更大的重要性。'
- en: '**Permutation feature importance**: This method involves randomly permuting
    the values of a single feature while keeping other features constant and measuring
    how much the model’s performance decreases. A significant drop in performance
    indicates an important feature.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**置换特征重要性**：这种方法涉及随机置换单个特征的价值，同时保持其他特征不变，并测量模型性能下降的程度。性能的显著下降表明这是一个重要的特征。'
- en: '**Recursive feature elimination** (**RFE**): RFE is an iterative method that
    starts with all features and gradually removes the least important ones based
    on a model’s performance. The order in which features are removed indicates their
    importance.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归特征消除** (**RFE**)：RFE是一种迭代方法，从所有特征开始，根据模型性能逐渐移除最不重要的特征。特征被移除的顺序表明了它们的重要性。'
- en: '**SHapley Additive exPlanations** (**SHAP**): SHAP values provide a unified
    measure of feature importance by considering all possible feature combinations.
    They can be applied to various models, including deep learning models.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SHapley Additive exPlanations** (**SHAP**)：SHAP值通过考虑所有可能的特征组合，提供了一个统一的特征重要性度量。它们可以应用于各种模型，包括深度学习模型。'
- en: The choice of feature importance calculation method depends on the machine learning
    algorithm used, the dataset, and the problem at hand. Different algorithms may
    provide different rankings of feature importance, so it’s essential to consider
    multiple methods and use domain knowledge to interpret the results effectively.
    Feature importance scores help identify relevant features, reduce dimensionality,
    and improve model interpretability.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性计算方法的选择取决于所使用的机器学习算法、数据集和实际问题。不同的算法可能提供不同的特征重要性排名，因此考虑多种方法并使用领域知识来有效解释结果至关重要。特征重要性分数有助于识别相关特征、降低维度并提高模型的可解释性。
- en: Discovering gradient-based attribution methods
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现基于梯度的属性方法
- en: 'Gradient-based attribution methods, also known as gradient-based attribution
    techniques, are approaches that are used to understand and attribute the contributions
    of individual features or input elements to the output of a machine learning model.
    These methods rely on gradients, which represent the sensitivity of the model’s
    output to changes in input features. Here are some gradient-based attribution
    methods that are commonly used in machine learning:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的属性方法，也称为基于梯度的属性技术，是用于理解和归因于机器学习模型输出中单个特征或输入元素贡献的方法。这些方法依赖于梯度，梯度表示模型输出对输入特征变化的敏感性。以下是一些在机器学习中常用到的基于梯度的属性方法：
- en: '**Gradient saliency**: Saliency maps emphasize the most pertinent areas in
    an input image that influence a model’s prediction. These maps are created by
    calculating the gradient of the model’s output concerning the input image pixels.
    Regions with high gradients correspond to areas of significant relevance.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度显著性**：显著性图强调影响模型预测的输入图像中最相关的区域。这些图是通过计算模型输出关于输入图像像素的梯度来创建的。梯度高的区域对应于具有显著相关性的区域。'
- en: '**Integrated gradients**: Integrated gradients assigns attribution scores to
    each input feature by computing the cumulative integral of the gradient concerning
    the input along a path from a reference input (usually all zeros) to the actual
    input. This method provides a more comprehensive understanding of feature importance.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成梯度**：集成梯度通过计算从参考输入（通常是全零）到实际输入的路径上关于输入的累积积分，为每个输入特征分配属性分数。这种方法提供了对特征重要性的更全面的理解。'
- en: '**Guided backpropagation**: Guided backpropagation is a modified backpropagation
    algorithm that retains only positive gradients during backpropagation. This helps
    highlight the positive contributions of input features to predictions and suppresses
    negative contributions.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引导反向传播**：引导反向传播是一种修改后的反向传播算法，在反向传播过程中仅保留正梯度。这有助于突出输入特征对预测的积极贡献，并抑制消极贡献。'
- en: '**SmoothGrad**: SmoothGrad reduces noise in saliency maps by averaging gradients
    across multiple perturbed versions of the input and then visualizing the smoothed
    gradient values.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SmoothGrad**：SmoothGrad通过在多个扰动的输入版本上平均梯度并可视化平滑后的梯度值，减少了显著性图中的噪声。'
- en: '**Layer-wise relevance propagation** (**LRP**): LRP is an attribution method
    that assigns relevance scores to each neuron in the network’s hidden layers and
    propagates them backward to the input features. It provides fine-grained feature
    relevance information.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层级相关性传播**（**LRP**）：LRP是一种归因方法，它为网络隐藏层中的每个神经元分配相关性分数，并将它们反向传播到输入特征。它提供了细粒度的特征相关性信息。'
- en: '**Deconvolutional networks** (**DeconvNets**): DeconvNets are designed to reverse
    the effects of convolutional layers in a neural network. They help visualize feature
    maps at different layers of the network to understand what each layer learns.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反卷积网络**（**DeconvNets**）：DeconvNets旨在逆转神经网络中卷积层的效果。它们有助于可视化网络不同层的特征图，以了解每个层学到了什么。'
- en: '**Gradient Class Activation Mapping** (**GradientCAM**): GradientCAM combines
    gradient information with class activation mapping techniques to highlight regions
    in an input image that are important for a specific class prediction.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度类激活映射**（**GradientCAM**）：GradientCAM结合了梯度信息与类激活映射技术，以突出显示对特定类别预测重要的输入图像区域。'
- en: Gradient-based attribution methods are valuable for model interpretability and
    debugging. They help identify which features or parts of the input are influential
    in driving the model’s decisions. Choosing the right attribution method depends
    on the model architecture, dataset, and specific goals of interpretation. These
    methods provide insights into model behavior and can help build trust in AI systems.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的归因方法对于模型可解释性和调试非常有价值。它们有助于识别哪些特征或输入的部分对驱动模型决策有影响。选择正确的归因方法取决于模型架构、数据集和解释的具体目标。这些方法提供了对模型行为的洞察，并有助于在AI系统中建立信任。
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we understood the basic concepts surrounding computer vision
    and how to implement a model for object recognition using MATLAB. We started by
    introducing image processing and computer vision. We learned how tools are available
    to process images and how computer vision is used for object recognition, motion
    detection, and pattern recognition. Then, we explored MATLAB tools for computer
    vision, and how the capabilities and functions provided by MATLAB create a robust
    environment for the development and prototyping of computer vision applications.
    Whether your focus is on tasks such as image analysis, object detection, 3D reconstruction,
    or any related application, MATLAB offers the necessary tools and features to
    support your work effectively.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了计算机视觉的基本概念以及如何使用MATLAB实现对象识别模型。我们首先介绍了图像处理和计算机视觉。我们学习了有哪些工具可以处理图像，以及计算机视觉如何用于对象识别、运动检测和模式识别。然后，我们探讨了MATLAB的计算机视觉工具，以及MATLAB提供的功能和功能如何为计算机视觉应用的开发和原型设计创建一个强大的环境。无论你的重点是图像分析、对象检测、3D重建或任何相关应用，MATLAB都提供了必要的工具和功能来有效地支持你的工作。
- en: After that, we learned how to build a MATLAB model for object recognition by
    using a CNN and the MNIST dataset. We understood how to import image data into
    a MATLAB workspace and how to use images to train a CNN. Then, we learned how
    to use pretrained deep learning models in MATLAB to improve the performance of
    the object recognition model. Finally, we introduced some tools for interpreting
    and explaining deep learning models.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们学习了如何使用CNN和MNIST数据集在MATLAB中构建对象识别模型。我们了解了如何将图像数据导入MATLAB工作空间，以及如何使用图像来训练CNN。然后，我们学习了如何在MATLAB中使用预训练的深度学习模型来提高对象识别模型的表现。最后，我们介绍了一些用于解释和说明深度学习模型的工具。
- en: In the next chapter, we will delve into fundamental concepts surrounding sequential
    data and explore the process of constructing a model to capture patterns within
    time series or any general sequence. We will learn the basic concepts of time
    series data, how to extract statistics from sequential data, and how to implement
    a model to predict the stock market data. Finally, we will understand oversampling,
    undersampling, and cost-sensitive learning.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨与序列数据相关的根本概念，并探讨构建模型以捕捉时间序列或任何一般序列中模式的过程。我们将学习时间序列数据的基本概念，如何从序列数据中提取统计信息，以及如何实现预测股票市场数据的模型。最后，我们将了解过采样、欠采样和成本敏感学习。
