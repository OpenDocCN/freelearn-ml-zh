- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Productionizing ML on Databricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Production is 80% of the work.”
  prefs: []
  type: TYPE_NORMAL
- en: — Matei Zaharia
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve refined your model and have satisfactory results, you are ready
    to put it into production. We’ve now entered the field of **machine learning operations**
    (**MLOps**)! Unfortunately, this is where many data scientists and ML engineers
    get stuck, and it’s common for companies to struggle here. Implementing models
    in production is much more complex than running models ad hoc because MLOps requires
    distinct tools and skill sets and sometimes, entirely new teams. MLOps is an essential
    part of the data science process because the actual value of a model is often
    only realized post-deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can think of MLOps as combining **DevOps**, **DataOps**, and **ModelOps**.
    MLOps is often divided into two parts: inner and outer loops. The inner loop covers
    the data science work and includes tracking various stages of the model development
    and experimentation process. The outer loop encompasses methods to orchestrate
    your data science project throughout its life cycle, from testing to staging and
    ultimately into production.'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the path from model development to production doesn’t have to depend
    entirely on another team and tool stack when using the Databricks **Data Intelligence**
    (**DI**) platform. Productionizing an ML model using Databricks products makes
    the journey more straightforward and cohesive by incorporating functionality such
    as the **Unity Catalog Registry** (**UC Registry**), Databricks workflows, **Databricks
    Asset Bundles** (**DABs**), and model serving capabilities. This chapter will
    cover the tools and practices for taking your models from development to production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what you will learn as part of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the MLOps inner loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the MLOps outer loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying your model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the MLOps inner loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Databricks, the MLOps inner loop uses a variety of tools within the DI platform
    that we’ve already touched upon throughout this book, such as MLflow, Feature
    Engineering with Unity Catalog, and Delta. This chapter will highlight how you
    can leverage them together to facilitate MLOps from one place. MLOps is covered
    in even more depth by Databricks’ ebook, *The Big Book of MLOps*, which we highly
    recommend if you wish to learn more about the guiding principles and design decisions
    when architecting your own MLOps solution. We use `mlflow.log_input()` method.
    The upstream sources of the feature tables we create are automatically tracked
    by UC lineage.
  prefs: []
  type: TYPE_NORMAL
- en: Registering a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s jump into model registries and their role in productionalization. A model
    registry is a centralized model store that helps manage the entire model life
    cycle, including versioning or aliasing, CI/CD integration, webhooks, and notifications.
    In Databricks, the UC Registry extends the governance features of Unity Catalog
    to the model registries, including centralized access control, auditing, lineage,
    and model discovery across workspaces. The UC Registry is a centralized repository
    for your models and their chronological lineage. This means that the experiment
    and experiment run that created each respective model are linked to the respective
    code and data source. Once you are satisfied with your ML model, the first thing
    to do is register it in Databricks under your central model registry. A registered
    model is a logical grouping of a model’s version history. The UC Registry tracks
    different model versions, including each version’s training data, hyperparameters,
    and evaluation metrics. A model rarely, if ever, has only one version. In addition
    to experimentation of model types and hyperparameter values, there are also cases
    where we want to experiment with different versions of a model. We can refer to
    these different model versions using model aliases. For example, deploying different
    versions simultaneously can be helpful for A/B testing; we’ll cover this in more
    detail in the *Model inference* section. With the UC Registry, you can easily
    create and manage multiple model aliases, making it easier to track changes, compare
    performance, and revert to earlier versions if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The UC Registry provides a collaborative model development environment, enabling
    teams to share and review models. This collaboration is also tracked, allowing
    multiple teams or leads to stay abreast of the model development process. Team
    or project leads can also require documentation before allowing a model to continue
    through the life cycle. We find it easier to add snippets of documentation throughout
    the project rather than trying to remember everything and take the time to write
    it all down afterward.
  prefs: []
  type: TYPE_NORMAL
- en: The UC Registry allows you to tag models and model versions. In the Streaming
    Transactions project, we use tags to track the validation status of a model. The
    approval process can be automated or may require human interaction. An approval
    process can ensure that models are high quality and meet business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.1* shows a screenshot of the UC Model Registry. Note that there
    are built-in locations for tags, aliases, and comments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – The UC Model Registry UI shows each model version](img/B16865_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – The UC Model Registry UI shows each model version
  prefs: []
  type: TYPE_NORMAL
- en: The UC Registry is tightly integrated with the DI platform. This provides a
    single technical stack – a unified environment for moving a model through experimentation
    to deployment. The seamless integration lets you leverage other Databricks components
    such as Databricks Notebooks, Databricks Jobs, Databricks Lakehouse Monitoring,
    and Databricks Model Serving.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s move on to product features that support the outer loop process.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the MLOps outer loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ML life cycle looks different for different use cases. However, the set
    of tools available in the Databricks platform makes it possible to automate as
    you like and supports your MLOps. The outer loop connects the inner loop products
    with the help of Workflows, Databricks Terraform Provider, REST API, DABs, and
    more. We covered automating the tracking process through MLflow Tracking and the
    UC Registry. The UC Registry is tightly integrated with the Model Serving feature
    and has a robust API that can easily be integrated into the automation process
    using webhooks. Each of these features can play a role in automating the ML life
    cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Databricks Workflows is a flexible orchestration tool for productionizing and
    automating ML projects. Workflows help the ML life cycle by providing a unified
    way to chain together all aspects of ML, from data preparation to model deployment.
    With Databricks Workflows, you can designate dependencies between tasks to ensure
    tasks are completed in the required order. These dependencies are visualized with
    arrows connecting the tasks, as shown in *Figure 7**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.2 – A Databricks workflow with five tasks, with the feature engineering\
    \ task having two\uFEFF dependencies](img/B16865_07_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – A Databricks workflow with five tasks, with the feature engineering
    task having two dependencies
  prefs: []
  type: TYPE_NORMAL
- en: 'Tasks in a workflow are not limited to notebooks. In [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123),
    we prepared a DLT pipeline to prepare data in the Bronze layer, and DLT pipelines
    can be a workflow component. Additional objects, such as JAR files, Python scripts,
    and SQL, can be tasks within a workflow, as shown in *Figure 7**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Examples of objects that can be used as tasks within a workflow](img/B16865_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Examples of objects that can be used as tasks within a workflow
  prefs: []
  type: TYPE_NORMAL
- en: Workflows are robust and play a key role in automating work within Databricks.
    DABs are another tool for productionization in Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: DABs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DABs are a way to bring uniformity and standardization to the deployment approach
    for all data products built on the Databricks platform. They are an **Infrastructure
    as Code** (**IaC**) approach to managing your projects, allowing developers to
    outline a project’s infrastructure and resources using a YAML configuration file.
    DABs are especially useful for managing complex projects that involve a lot of
    collaborators and require automation. Where **continuous integration and continuous
    deployment** (**CI/CD**) is necessary, DABs are a wonderful way to manage ML pipeline
    resources across environments and to help your team follow best practices throughout
    the development and productionalization processes.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, DABs are collections of Databricks artifacts (including jobs,
    DLT pipelines, and ML models) and assets (for example, notebooks, SQL queries,
    and dashboards). These bundles are managed through YAML templates that are created
    and maintained alongside source code. You can build DAB YAML files manually or
    use templates to automate. You can also build custom templates for more complex
    processing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Using DABs requires the Databricks CLI. We discussed installing the CLI in [*Chapter
    2*](B16865_02.xhtml#_idTextAnchor073) if you want to review it again. Also, DABs
    are fairly new and not incorporated into the projects. However, great resources
    are listed in *Further reading* that cover this new product feature in depth.
  prefs: []
  type: TYPE_NORMAL
- en: REST API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Everything you can do in the UI can be accomplished via the API as well. The
    UI is great for exploring product features and building workflows for the first
    time. For example, we automated the process after building out our AutoML experiment
    in the UI. Additionally, we saw how secrets are completely contained in the API
    and not available via the UI. As we’ll see in the next section, it is possible
    to deploy your models via the API.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying your model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying a model can be done in many ways, depending on the use case and data
    availability. For example, deployment may look like packaging a model in a container
    and deploying it on an endpoint or model that runs daily in a production workflow
    to provide predictions in tables that can be consumed by applications. Databricks
    has product features to pave the way to production for all inference types.
  prefs: []
  type: TYPE_NORMAL
- en: Model Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve walked through the methods and tools that help you set up your model
    in production, and finally, you have a model ready for inference! But one key
    question you should consider as part of this process is how your model should
    be used. Do you need the results once a day? Is the model powering an application
    that requires real-time results? Your model’s purpose will help you decide the
    type of deployment you need. You’ve seen the words “batch” and “streaming” a few
    times in this chapter already, so let’s quickly define what those mean in the
    context of model inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch inference**: Batch inference (also known as offline inference) refers
    to a job that generates predictions on a group (or “batch”) of data all at once.
    Batch jobs are scheduled to run on a specified cadence, such as once a day. This
    type of inference is best when there are no/low-latency requirements and allows
    you to take advantage of scalable compute resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming inference**: Streaming inference (also known as online inference)
    refers to a job that generates predictions on data as it is streamed. This is
    possible in the Databricks platform via Delta Live Tables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time inference**: Real-time inference (also known as model serving)
    refers to the process of exposing your models as REST API endpoints. This enables
    universally available, low-latency predictions and is especially useful when deploying
    models that generate predictions required by real-time applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these options are available via the Databricks UI. Let’s use the Favorita
    Store example again. Perhaps we’ve met with our beverages business team, and they
    would like to see weekly forecasts to help decide how much of each product they
    should purchase. We will opt for batch inference since we only need to produce
    an updated forecast once a week. It just takes a few clicks to set up a model
    for batch processing. Follow the *Applying our learning* section of the Favorita
    Sales dataset for detailed instructions on deploying your model for batch inference.
  prefs: []
  type: TYPE_NORMAL
- en: Model serving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s dive deeper into real-time inference or **model serving**. The process
    of model serving can be complex and costly, involving additional tools and systems
    to achieve real-time needs. Fortunately, deploying a registered model as an endpoint
    only takes a single click from the Databricks platform! Because model serving
    is tightly integrated with MLflow, the path from development to production is
    much faster. Using a model registered in the MLflow Model Registry, Databricks
    automatically prepares a production-ready container and deploys the container
    to serverless compute.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s also easy to deploy models via API, as shown here. A forecasting problem
    doesn’t make much sense as a model serving use case, so let’s consider instead
    the MIC problem we’ve been working on throughout this book. We will use model
    serving to serve our classification model in real time. The following example
    code snippet creates an endpoint that serves a version of a model named `asl_model`:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.4 – Deploying a mo\uFEFFdel through a serving endpoint](img/B16865_07_04.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Deploying a model through a serving endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'Model serving lets you build workflows around model endpoints, which provides
    plenty of flexibility in terms of model deployment. For example, an organization
    may want to A/B test two or more models. Model serving makes it easy to deploy
    multiple models behind the same endpoint and distribute traffic among them. In
    another pattern, you can deploy the same model behind multiple endpoints. The
    following is an example of the UI and the analysis you can perform when A/B testing
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Model serving gives us the flexibility to A/B test models deployed
    behind the same endpoint](img/B16865_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Model serving gives us the flexibility to A/B test models deployed
    behind the same endpoint
  prefs: []
  type: TYPE_NORMAL
- en: Get ready to follow along in your own Databricks workspace as we review the
    [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325) code project by project.
  prefs: []
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s use what we have learned to productionalize our models.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the technical requirements needed to complete the hands-on examples
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: On-demand features require the use of DBR ML 13.1 or higher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG and CV parts require DBR ML 14.2 and higher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python UDFs are created and governed in UC; hence, Unity Catalog must be enabled
    for the workspace – no shared clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Streaming Transactions project uses `scikit-learn==1.4.0rc1`. The notebooks
    that need it install it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Streaming Transactions project, again, performs better with parallel compute.
    We’ll use the multi-node cluster from [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244).
    See *Figure 7**.6* for the multi-node CPU configuration:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "![Figure 7.6 – Multi-node CPU c\uFEFFluster configuration (on AWS)](img/B16865_07_06.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Multi-node CPU cluster configuration (on AWS)
  prefs: []
  type: TYPE_NORMAL
- en: Project – Favorita Sales forecasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed using managed MLflow and the UC Model Registry
    to register and prepare models for deployment. We’ll start by walking through
    the UI, so please open the following notebooks and a tab within the Experiments
    UI page:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH7-01-Registering` `the Model`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-02-Batch Inference`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a reminder, in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297), we ran experiments
    to find a baseline model, and you can review those experiments from the Experiments
    UI page in Databricks. To follow along in your workspace, please open the Experiments
    UI page from [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297), as shown in *Figure
    7**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.7 – The Experiments UI page for \uFEFFexploring the experiment runs](img/B16865_07_07.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – The Experiments UI page for exploring the experiment runs
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of the project, we will move forward as though the best baseline
    model is the model we want in production. To productionalize the model, follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the AutoML experiment page, click on the best run (the run at the top when
    sorting by the evaluation metric in descending order). This will open the run
    details page. As shown in *Figure 7**.8*, there are four tabs – **Overview**,
    **Model metrics**, **System metrics**, and **Artifacts**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Exploring the model details page](img/B16865_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Exploring the model details page
  prefs: []
  type: TYPE_NORMAL
- en: 'Click `CH7-01-Registering the Model` notebook. When running the notebook in
    your workspace, you must update the `runs:/` URL in the notebook before executing
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure \uFEFF7.9 – Registering a new model](img/B16865_07_09.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Registering a new model
  prefs: []
  type: TYPE_NORMAL
- en: While still in your notebook, execute the final cell a second time. This creates
    a second version of the same model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the `favorita_forecasting` database in Unity Catalog. Selecting
    the **Models** tab opens a new UI, as shown in *Figure 7**.10*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Viewing the model in Unity Catalog](img/B16865_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Viewing the model in Unity Catalog
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the forecasting model. You’ll notice that we have two versions of the
    model. Add an alias to each: **champion** and **challenger** are common to indicate
    the current best model and the newer version being considered to replace the current
    best model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Assigning aliases to identify model status](img/B16865_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Assigning aliases to identify model status
  prefs: []
  type: TYPE_NORMAL
- en: Now, it’s time to think about how we want to deploy this model. Since this is
    a sales forecasting use case that’s predicting 10+ days in advance, real-time
    inference doesn’t make the most sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `CH7-02-Batch Inference` to run inference on the test set. Notice that
    in *Figure 7**.12*, we define `model_uri` using the alias rather than the model
    version number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Batch forecasting on the test set using the Champion model](img/B16865_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Batch forecasting on the test set using the Champion model
  prefs: []
  type: TYPE_NORMAL
- en: This inference code will always run inference on the data provided using the
    Champion model. If we later determine that an updated version is better, we can
    change the model alias and run the correct model without making any code changes
    to the inference code.
  prefs: []
  type: TYPE_NORMAL
- en: The next practical step is to set up a workflow on a schedule. Please refer
    to the streaming project to see this demonstrated. This wraps up the end of this
    project. In [*Chapter 8*](B16865_08.xhtml#_idTextAnchor384), we will use the Favorita
    Sales data to show how easily a SQLbot can be created using the Foundational Model
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Project – Streaming Transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have much to do to wrap up the Streaming Transactions project! We will build
    our model, wrap it, validate it, and implement batch inference. To accomplish
    this, we’ll begin ingesting the labels into a different table. This allows us
    to set up the inference table and merge the actual labels after the predictions
    happen. We will create two workflows: *Production Streaming Transactions* and
    *Production Batch Inference and* *Model Retraining*.'
  prefs: []
  type: TYPE_NORMAL
- en: As with any project, we must refine the previously written code as we work toward
    production. The notebooks that look familiar from earlier in this book may have
    some minor updates, but most of the code remains unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we jump in, let’s remember where we are and where we are going by taking
    a quick look at the project pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – The project pipeline for the Production Streaming Transactions
    project](img/B16865_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – The project pipeline for the Production Streaming Transactions
    project
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow along in your workspace, please open the following notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH7-01-Generating Records`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-02-Auto Loader`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-03-Feature` `Engineering Streams`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-04-Update Maximum Price` `Feature Table`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-05-Wrapping and Logging the` `Baseline Model`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-06-Wrapping and Logging the` `Production Model`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-07-Model Validation`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-08-Batch Inference`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-09-Production` `Batch Inference`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-10-Auto` `Label Loader`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlia_utils/transactions_funcs.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`production_streams.yaml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_retraining_n_inference_workflow.yaml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We recommend using a multi-node CPU cluster for this project. The first four
    notebooks (`CH7-01` through `CH7-04`) are nearly identical to their previous versions
    in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297), but the [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325)
    versions all point to the production rather than the development catalog. Table
    names are parameterized in widgets so that they can be set in workflows. Here
    is a list of the essential notebook-specific changes that have been made to the
    first four notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH7-01-Generating Records`: Labels and transactions are now being written
    to separate folders. The data generation components have also been moved to the
    `transactions_funcs` file in the `utils` folder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-02-Auto Loader`: The label column is no longer being added to the transactions
    table. For production, we ensure that the table being written to has `delta.enableChangeDataFeed
    = true` before the stream starts. If the table property is set after the stream
    starts, the stream is interrupted and will require a restart. Lastly, if the table
    property is never set, Lakehouse Monitoring is negatively impacted and will not
    support continuous monitoring.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-03-Feature Engineering Streams`: Similar to the `CH7-02-Pipeline Auto
    Loader` notebook, table properties are set before any data is written.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-04-Update Maximum Price Feature Table`: The code is cleaned up for a step
    toward production. Specifically, the feature table is updated rather than created
    once it exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You will need transaction data in the production catalog to create the model.
    Note the new setup variable, `$env=prod`. The pipeline workflow notebooks – that
    is, `CH7-01`, `CH7-02`, and `CH7-03` – are ready to be added to a workflow in
    the jobs section of Databricks. Start by clicking the `production_streams.yaml`,
    to guide you. Note that in the YAML file and *Figure 7**.14*, the tasks do not
    depend on one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – The task DAG (left) and some settings (right) for the Production
    Streaming Transactions job](img/B16865_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – The task DAG (left) and some settings (right) for the Production
    Streaming Transactions job
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 7**.14*, you can use the compute cluster shown in *Figure
    7**.6* for the job and the other notebooks. Also, we chose to add parameters at
    the job level rather than the individual tasks. You can now easily generate the
    data needed to build the model.
  prefs: []
  type: TYPE_NORMAL
- en: We will use `training_set`, which we created in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297),
    to train an updated version of the model. Recall that logging a model in Unity
    Catalog packages the feature metadata with the model. Hence, at inference time,
    it automatically looks up features from feature tables provided in the specified
    training set. We added this spec to the `CH7-05-Wrapping and Logging the Baseline
    Model` and `CH7-06-Wrapping and Logging the Production Model` notebooks. We will
    not go through these notebooks separately. The difference between them is that
    the baseline model is trained on data in the development catalog. Then, the model
    is re-trained in production using production data from the inference table.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that we don’t have an inference table yet. Don’t worry – it’s coming!
    Going back to `training_set`, being able to match up the features is only useful
    if we have feature values. We use the time boundaries of our feature table to
    ensure that the raw transactions being used for training have feature values.
    Also, we require the label column to be present, as shown in *Figure 7**.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Defining the feature lookups for the inference model](img/B16865_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Defining the feature lookups for the inference model
  prefs: []
  type: TYPE_NORMAL
- en: 'The transactions shown in *Figure 7**.15* come from the inference table, `packaged_transaction_model_predictions`,
    which was created in `CH7-08-Batch Inference`. The baseline model does something
    similar with the `raw_transactions` table. The baseline model also sets the model
    description, as shown in *Figure 7**.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Setting the model description](img/B16865_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Setting the model description
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to focus on the model. Let’s walk through the rest of the
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the model registry to Unity Catalog with `mlflow.set_registry_uri("databricks-uc")`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `pip` to save a `requirements.txt` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a PyFunc wrapper for `TransactionModelWrapper`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Most of the code for `TransactionModelWrapper` should look familiar to those
    who have created a model using Sklearn. The initialization function, `__init__(self,
    model, X, y, numeric_columns, cat_columns)`, accepts a model and DataFrames. The
    data preprocessing for training and inference data is standardized within the
    wrapper. `TransactionModelWrapper` consists of four methods: `init`, `predict`,
    `preprocess_data`, and `fit`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The initialization method does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Splits the data into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializes and fits `OneHotEncoder` for the categorical feature columns provided.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializes and fits `StandardScaler` for the numerical feature columns provided.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applies the `preprocess_data` method to the training and test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defines an `evaluation` method to calculate the log loss on the *X* and *y*
    sets provided.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invokes the `evaluation` method on preprocessed test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defines and invokes the `_model_signature` method to easily provide the signature
    when logging the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `predict` method calls the `preprocess_data` (*Figure 7**.17)* method on
    the input DataFrame before performing and returning the prediction. This method
    is used to process the training data and the inference data, ensuring identical
    preprocessing for predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – The preprocessing method for the model](img/B16865_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – The preprocessing method for the model
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 7**.17*, the fitted numeric scaler and one-hot encoder are
    passed as input. This protects against skew between the training and inference
    features. Notice how `TransactionTimestamp` is dropped. This is done because after
    the features from the feature tables are present, we no longer need a timestamp.
    The input DataFrame can be a Spark or pandas DataFrame. This is why we need different
    syntax to drop the timestamp column.
  prefs: []
  type: TYPE_NORMAL
- en: In the following command cell, you customize `mlflow.autolog` and start the
    MLflow experiment for training, testing, wrapping, and logging the model. You
    will use `mlflow.evaluate` to handle the evaluation process. The logging process
    is easy – you call `log_model` with the model name, wrapped model, `pyfunc` flavor,
    and previously created training set. This process also registers the model in
    Unity Catalog. The last thing in this notebook is a quick test that’s performed
    on the predict function showing how to pass in the Spark context with the input
    data. You are now ready to validate the model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, focus on the `CH7-07-Model Validation` notebook, which checks that the
    input model has the correct metadata so that it’s ready for production. This notebook
    can be used to test any model. Ideally, you will add numerous checks, including
    the ability to predict and possibly test the accuracy of specific slices of data.
    For example, you could check the model performance on each product or geography.
    You can pass those columns with tags when slices need testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Collect the model details, as shown in *Figure 7**.18*. Notice the use of the
    `util` function, which is imported in a previous cell from `mlia_utils.mlflow_funcs`
    `import get_latest_model_version`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Using the mlfclient and util functions to access the model
    details](img/B16865_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Using the mlfclient and util functions to access the model details
  prefs: []
  type: TYPE_NORMAL
- en: 'Each time you train and log the model, a new model version is created. Using
    tags, you can indicate which model version(s) must be tested and validated before
    deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Assertions to ensure the model needs to be tested](img/B16865_07_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Assertions to ensure the model needs to be tested
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 7**.20*, you need an informative model description for
    all production models. We recommend that you include information about the use
    case the model is used for. Metadata hygiene is becoming increasingly important
    as companies want to leverage generative AI on internal data. This is because
    LLMs use the metadata fields to find relevant information in the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Validating that a model description is present](img/B16865_07_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Validating that a model description is present
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to what’s shown in *Figure 7**.20*, the notebook checks for tags. These
    are examples to get you started. This is an ideal section to expand on the current
    code and continue adding validation results and update tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Processing the results of the validation tests](img/B16865_07_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Processing the results of the validation tests
  prefs: []
  type: TYPE_NORMAL
- en: 'With a tested model, you can progress to inference; see the `CH7-08-Batch Inference`
    notebook. Review the examples of performing batch inference on a DataFrame (*Figure
    7**.22*) and a JSON data string:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22 – Loading and scoring the model by performing batch inference
    on a DataFrame](img/B16865_07_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.22 – Loading and scoring the model by performing batch inference on
    a DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, look at the code in `CH7-09-Production Batch Inference`. The substantial
    changes include `scoring_df`, which is the DataFrame we apply our model to for
    predictions. Notice that in *Figure 7**.23*, the `min_time` and `max_time` variables
    provide boundaries for the transactions, ensuring the batch feature values have
    been calculated. Additionally, the inference table provides a boundary that prevents
    duplicate prediction calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23 – The scoring_df query’s configuration](img/B16865_07_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.23 – The scoring_df query’s configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'The inference table in `CH7-08` needs to be updated to fit the requirements
    for the inference table monitoring provided by Lakehouse Monitoring. This means
    adding the `model_version` and `actual_label` columns. The `actual_label` column
    is set to `NULL` so that it is clear the value has not been updated yet; see *Figure
    7**.24*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – The addition of the model version and actual label columns](img/B16865_07_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.24 – The addition of the model version and actual label columns
  prefs: []
  type: TYPE_NORMAL
- en: 'These two additional columns for the inference table are requirements for Lakehouse
    Monitoring. The `InferenceLog` monitor comes with autogenerated dashboards. However,
    you need to populate the table. Begin by creating a bronze table for the transaction
    labels. The Auto Loader is back again, focusing on labels; see `CH7-10-Auto Label
    Loader` and *Figure 7**.25*. In the notebook, the `transaction_labels` table was
    created; this is similar to the code from [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123).
    In *Figure 7**.25*, you can use the CDF and CDC Delta features to update the new
    inference table with the ground truth label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25 – Merging transaction labels into the inference table](img/B16865_07_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.25 – Merging transaction labels into the inference table
  prefs: []
  type: TYPE_NORMAL
- en: 'You now have a bronze table and the ability to merge new labels into the inference
    table. However, the inference table is still empty. So, let’s create a workflow
    job, as shown in *Figure 7**.26*, to generate predictions every 15 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26 – The DAG for the inference (upper) and model retraining (lower)
    job](img/B16865_07_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.26 – The DAG for the inference (upper) and model retraining (lower)
    job
  prefs: []
  type: TYPE_NORMAL
- en: Walk through the two workflow paths, starting with the upper path. The batch
    features are updated, thus providing feature data for inference. The data is ready
    for predictions, and the inference task can begin.
  prefs: []
  type: TYPE_NORMAL
- en: 'As its first task, the lower path updates labels. It adds the latest data to
    the `transaction_labels` table and merges all new labels that match previous predictions
    into the inference table. Skip forward beyond the first iteration and the inference
    table contains not only previous predictions but also the labels for those predictions.
    Model training is performed using the updated table containing the features. Retraining
    the model only occurs if there is data in the inference table, as shown in *Figure
    7**.27*. The retraining process is, of course, followed by validation when needed.
    The validation notebook exits when it detects that the latest version of the model
    has already been tested:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.27 – The retraining notebook exits if there is no data to retrain
    on](img/B16865_07_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.27 – The retraining notebook exits if there is no data to retrain on
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the workflow job shown in *Figure 7**.29*. You can reference the configuration
    for the job in the `model_retraining_n_inference_workflow.yaml` file. The workflow
    automatically provides the lineage of all upstream and downstream tables. You
    can see these in *Figure 7**.26*. This saves us time on documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.28 – The workflow table lineage](img/B16865_07_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.28 – The workflow table lineage
  prefs: []
  type: TYPE_NORMAL
- en: 'All that is left is to run both workflows simultaneously. After letting both
    run for a bit (don’t forget to schedule the Production Batch Inference and Model
    Retraining workflow), you should have a screen that looks similar to what’s shown
    in *Figure 7**.29*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.29 –  The historical view of successful job runs](img/B16865_07_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.29 – The historical view of successful job runs
  prefs: []
  type: TYPE_NORMAL
- en: You now have all of the pieces to productionize this project.
  prefs: []
  type: TYPE_NORMAL
- en: Project – multilabel image classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We currently have a working image classification model that we trained and
    evaluated in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297). Now, let’s add some
    infrastructure around our code to serve our model and make it available to downstream
    applications and, ultimately, our end users. To follow along in your workspace,
    please open the following notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Ch7-01-Create_Final_Wrapper_Production`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ch7-02-Serve_In_Production`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ll start by creating our model class wrapper. This wrapper includes two
    functions, `feature_extractor` and `predict`. The `feature_extractor` function
    is required because otherwise, our fine-tuned model won’t contain the same preprocessing
    step that’s used during fine-tuning and would, therefore, not be consistent during
    serving. Of course, you can simply serve your original model if you do not need
    to make any custom modifications to your model and only need the raw format outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.30 – Creating the model class wrapper](img/B16865_07_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.30 – Creating the model class wrapper
  prefs: []
  type: TYPE_NORMAL
- en: The `feature_extractor` function, which transforms an image into the format
    required by the served model, is the same code we used to score the model in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297). Let’s dive into the prediction part; it’s
    similar to what we created in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297)
    to score our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `predict` function is similar to the one we used to score our model in
    [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297)using `pandas_udf`. Note that we
    are not only returning the predicted label but also a label corresponding to it
    in a dictionary format (this isn’t required, but we wanted to show the output
    format’s flexibility):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.31 – Including the feature_extractor and predict functions in the
    model class wrapper](img/B16865_07_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.31 – Including the feature_extractor and predict functions in the model
    class wrapper
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready to wrap our fine-tuned model from [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180)
    into the wrapper. To do this, we must load the artifact from MLflow and pass it
    to the pre-created `CVModelWrapper` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.32 – Loading our model from the existing MLflow experiment](img/B16865_07_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.32 – Loading our model from the existing MLflow experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test whether our wrapper is functioning as expected. To do so, we must
    encode a few images (as the model serving accepts strings and cannot accept images
    and convert them yet) and save them into a pandas DataFrame. Then, we must use
    our model wrapper to get a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.33 – Using our model wrapper to create predictions on a few images](img/B16865_07_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.33 – Using our model wrapper to create predictions on a few images
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will use MLflow to log and serve the model via Databricks Model Serving:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.34 – Logging and running models using MLflow](img/B16865_07_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.34 – Logging and running models using MLflow
  prefs: []
  type: TYPE_NORMAL
- en: 'During the production phase, you will usually operate on the aliases and the
    latest model version, so here, we’ll simulate setting the alias, Champion, to
    the best-performing model and getting the latest model version to be deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.35 – Loading our champion model](img/B16865_07_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.35 – Loading our champion model
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must create our model serving endpoint using the Databricks SDK. You
    could also create your endpoint using the UI. If you decide to use the SDK, you
    must create a configuration file for your endpoint. The following example is for
    a CPU container with a small workload size. If you are unfamiliar with this option,
    please check out *Create model serving endpoints* in the *Further* *reading* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.36 – Setting config input for the endpoint to serve our model](img/B16865_07_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.36 – Setting config input for the endpoint to serve our model
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the settings have been provided, you are ready to deploy or update your
    endpoint if it already exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.37 – Deploying a new endpoint if it does not exist or updating the
    existing one](img/B16865_07_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.37 – Deploying a new endpoint if it does not exist or updating the
    existing one
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind that if your endpoint does not exist, it will take a while to
    deploy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.38 – UI example while the GPU endpoint is deploying/updating](img/B16865_07_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.38 – UI example while the GPU endpoint is deploying/updating
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can score our model. Again, for simplicity and reusability, we are
    covering the serving call into a `score_model` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.39 – Defining a model scoring function](img/B16865_07_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.39 – Defining a model scoring function
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we must score our model using our model scoring function, as shown
    in *Figure 7**.40*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.40 – Scoring our model](img/B16865_07_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.40 – Scoring our model
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of scoring under the UI page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.41 – Scoring our model under the UI page of the Databricks Model
    Serving](img/B16865_07_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.41 – Scoring our model under the UI page of the Databricks Model Serving
  prefs: []
  type: TYPE_NORMAL
- en: Now that our model is ready for production, we can train and create it, designate
    our champion model, serve it on an endpoint, and score. The next steps might include
    setting up a monitor to track image pixel distributions, the number of images,
    label distributions, and the distribution of the response. We will talk more about
    model monitoring in [*Chapter 8*](B16865_08.xhtml#_idTextAnchor384).
  prefs: []
  type: TYPE_NORMAL
- en: Project – retrieval augmented generation chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we completed our chatbot and tested it out in a notebook.
    It might be tempting to jump to the final deployment step immediately, but that
    would involve skipping a critical step in the process – evaluation! Evaluating
    projects for correctness before providing them to the end users should always
    be part of the development process. However, it can be especially tricky to build
    automated evaluation solutions for newer technologies and techniques, such as
    when working with LLMs. This is a continually developing area of research, and
    we recommend reading Databricks’ recommendations on *Best Practices for LLM Evaluation*
    for more information (linked in *Further reading*). We’ll walk through the MLflow
    evaluation capability to evaluate the RAG chatbot we’ve built.
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow along in your workspace, please open the following notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH7-01-GetData`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH7-02-Evaluating_ChatBot`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the ground truth labels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll start in the first notebook, `CH7-01-GetData`. To evaluate our model,
    we must have ground truth labels – that is, the correct answers. This generally
    involves human effort to write out some typical questions you expect users to
    ask your chatbot and the answers you expect your chatbot to respond with. To simplify
    this process, we created a file containing 35 sample questions and the corresponding
    human-generated answers for this project, saved to `Questions_Evaluation.csv`.
    Let’s load this file and examine the question and answer pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.42 – Reading our pre-created evaluation set](img/B16865_07_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.42 – Reading our pre-created evaluation set
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at some of the records to get a sense of the questions a user might
    ask and the expected answers. You can also add your own examples to augment the
    evaluation data further:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.43 – Question-answer pair examples](img/B16865_07_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.43 – Question-answer pair examples
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s save these examples to a Delta table named `evaluation_table`. That way,
    if a new `Question_Evaluation.csv` file with different examples is uploaded, you
    can append the new examples to the existing table without risking losing the original
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.44 – Creating evaluation_table to store the question/answer pairs](img/B16865_07_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.44 – Creating evaluation_table to store the question/answer pairs
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon saving the table, we are now ready to evaluate our model:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.45 – Saving  thequestions_evaluation.csv da\uFEFFta to a Delta table](img/B16865_07_45.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.45 – Saving thequestions_evaluation.csv data to a Delta table
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the evaluation workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’re now ready to open the second notebook, `CH7-02-Evaluating_ChatBot`, and
    evaluate our chatbot against the ground truth labels we saved to `evaluation_table`.
    Let’s briefly discuss how we will compare our model’s outputs to the human-generated
    answers. While there is plenty of ongoing research in the realm of automated LLM
    evaluation methods, we will focus on one technique in particular: *LLM-as-a-judge*.
    This method brings in a second LLM to evaluate the performance of the first, automating
    the tedious task of comparing the generated answer to the true answer, something
    a human would traditionally have to do. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use an LLM as a judge, we must load the original model we created in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.46 – Loading mlaction_chatbot_model as rag_model](img/B16865_07_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.46 – Loading mlaction_chatbot_model as rag_model
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must run a quick test to verify that our RAG model works as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.47 – Verifying our loaded model works as expected](img/B16865_07_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.47 – Verifying our loaded model works as expected
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must use our RAG chatbot to generate answers for all of the example
    questions we stored in `evaluation_table`. These responses are what we will compare
    against the ground truth answers. We’ll build a `pandas_udf` function to make
    this part run faster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.48 – Creating a function to receive a question and return an answer
    using our RAG model](img/B16865_07_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.48 – Creating a function to receive a question and return an answer
    using our RAG model
  prefs: []
  type: TYPE_NORMAL
- en: 'We used Llama-2-70b for our judge, but you could use GPT-4 or any other LLM
    you prefer (though we cannot guarantee satisfactory results!). Our code leverages
    the Databricks Foundational Model API, which we also used in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)
    when creating the embeddings of ArXiv article text chunks. As a reminder, the
    Databricks Foundation Model APIs provide direct access to state-of-the-art open
    models from a serving endpoint, allowing you to incorporate high-quality generative
    AI models into your application without the need to maintain your model deployment.
    We call the Llama-2-70b endpoint in *Figure 7**.49*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.49 – Testing the Foundation Model endpoint for Llama-2-70b](img/B16865_07_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.49 – Testing the Foundation Model endpoint for Llama-2-70b
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must build a DataFrame from the `evaluation_table` questions and answers.
    If you have added many more question/answer examples to this dataset, you may
    want to downsample the number of questions and speed up the prediction process.
    Then, we must call our UDF, `predict_answer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.50 – Building a DataFrame with questions and ground truth answers,
    and adding RAG chatbot answers](img/B16865_07_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.50 – Building a DataFrame with questions and ground truth answers,
    and adding RAG chatbot answers
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our DataFrame with the chatbot’s responses to each question,
    we must save this to a Delta table. We’ll continue to reference the DataFrame
    throughout the rest of this code, but this way, we won’t have to generate the
    chatbot’s responses again if we want to query this data in the future:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.51 – Writing our evaluation DataFrame to a new table](img/B16865_07_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.51 – Writing our evaluation DataFrame to a new table
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned at the beginning of this section, we are using MLflow’s Evaluate
    capability to facilitate our model evaluation. Before we evaluate our RAG chatbot,
    let’s load the methods and verify how MLflow Evaluate works by default. First,
    we must load the “answer correctness” metric, which we will use as-is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.52 – Viewing the answer correctness metrics](img/B16865_07_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.52 – Viewing the answer correctness metrics
  prefs: []
  type: TYPE_NORMAL
- en: 'The out-of-the-box metrics are good, but professionalism is also an important
    criterion for our use case, so we’ll create a custom professionalism metric. *Figure
    7**.53* shows how to use the `make_genai_metric()` function to build out our professionalism
    evaluation metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.53 – Adding a custom professionalism metric](img/B16865_07_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.53 – Adding a custom professionalism metric
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from `grading_prompt`, we’ve designed this metric to give a
    score between one and five, where a score of one identifies text as casual and
    a score of five evaluates text as noticeably formal. This is a powerful tool to
    evaluate your model based on criteria that are important to your business use
    case. You can modify the template according to your needs. We must also add examples
    of the metric, as defined in *Figure 7**.54*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.54 – Adding an example for the custom professionalism metric](img/B16865_07_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.54 – Adding an example for the custom professionalism metric
  prefs: []
  type: TYPE_NORMAL
- en: 'With our professionalism metric, let’s run the model evaluation using MLfLow.
    To run an evaluation, we can call `mlflow.evaluate()` against the pandas DataFrame
    containing the questions, ground truth answers, and chatbot-generated answers.
    We’ll include the answer correctness and professionalism metrics as extra metrics.
    The following code will calculate many other metrics in addition to the two we
    specified, such as token count, toxicity, and Automated Readability Index grade
    (the approximate grade level required to comprehend the text):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 7.55 – Running an MLflow experiment with m\uFEFFlflow.evaluate()](img/B16865_07_55.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.55 – Running an MLflow experiment with mlflow.evaluate()
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve run the experiment, the metrics can be accessed in our notebook and
    via the UI so that we can easily see how our chatbot is performing in terms of
    accuracy and professionalism.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the chatbot’s responses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let’s look at the MLfLow UI in Databricks to compare the results between
    the human-generated and chatbot-generated responses. To do so, navigate to the
    `CH7-02-Evaluating Chatbot`). Then, navigate to the `answers` match what we would
    expect to see, but it is also particularly useful when you want to test and compare
    outputs across different models or chunking strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.56 – Using the Evaluation view in Databricks MLflow](img/B16865_07_56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.56 – Using the Evaluation view in Databricks MLflow
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at a few examples from the evaluation dataset. We’ll see that our
    model, according to a quick human assessment, is performing reasonably well. Of
    course, this form of evaluation isn’t scalable, so let’s dig into the other metrics
    as well. We’ll use the common visualization library, `plotly`, to take a closer
    look at our model results. First, we’ll look at the distribution of token counts
    in our chatbot’s responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.57 – Plotting token counts from our chatbot’s responses](img/B16865_07_57.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.57 – Plotting token counts from our chatbot’s responses
  prefs: []
  type: TYPE_NORMAL
- en: 'While interesting, what we care about here are the two metrics we discussed
    earlier: correctness and professionalism. Let’s take a look at the distribution
    of the correctness scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.58 – Plotting the correctness score distribution](img/B16865_07_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.58 – Plotting the correctness score distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also view the professionalism score distribution. Our distribution is
    threes and fours, which means the tone is most often “borderline professional.”
    This is how we defined it in our custom metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.59 – Plotting the professionalism score distribution](img/B16865_07_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.59 – Plotting the professionalism score distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, our model is looking good! If we’re satisfied with the accuracy and
    professionalism scores, we can mark this model as ready for production by giving
    it a production alias:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.60 – Aliasing our model to show it is production-ready](img/B16865_07_60.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.60 – Aliasing our model to show it is production-ready
  prefs: []
  type: TYPE_NORMAL
- en: With that, we evaluated our chatbot by creating predictions from our question-and-answer
    dataset, created a custom evaluation metrics to evaluate the professionalism of
    each response, and visualized information about our model outputs. In the last
    chapter, we will demonstrate how to build a Gradio app so that you can bring your
    RAG chatbot to your end users!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing models in production can be challenging, but with tools designed
    to support productionizing models and automating the MLOps process, it is much
    easier. In this chapter, we looked at using the UC Model Registry to manage the
    life cycle of an ML model. We highlighted MLflow and how it can be used to create
    reproducible, modularized data science workflows that automatically track parameters
    and performance metrics. We also discussed techniques for calculating features
    at the time of inference. To make the end-to-end MLOps process more manageable,
    we showed how to use workflows and webhooks to automate the ML life cycle. We
    also showed how to serve models and make inferences using MLflow and the Databricks
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: In the last chapter, *Monitoring, Evaluating, and More*, we will look at monitoring
    our data and ML models within the Databricks Lakehouse so that you can get the
    most value from your data.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s test ourselves on what we’ve learned by going through the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Can more than one model be in production simultaneously? When would you want
    to use two models in production?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What component of MLflow could you use to route approvals?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you use an MLflow API to serve your model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After putting some thought into the preceding questions, compare your answers
    to ours:'
  prefs: []
  type: TYPE_NORMAL
- en: Yes, having multiple models in production simultaneously is possible, and this
    is appropriate for use cases such as comparing models in a challenger/champion
    test or running A/B tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The UC Model Registry can be used to route approvals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Model Serving API within MLFlow can be used for model serving.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discussed tools and technologies to help productionize ML. Please
    take a look at these resources to dive deeper into the areas that interest you
    most – and help you get more of your ML projects into production!
  prefs: []
  type: TYPE_NORMAL
- en: '*Best Practices for Using Structured Streaming in Production - The Databricks*
    *Blog*: [https://www.databricks.com/blog/streaming-production-collected-best-practices](https://www.databricks.com/blog/streaming-production-collected-best-practices)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The big book of machine learning use* *cases*: [https://www.databricks.com/resources/ebook/big-book-of-machine-learning-use-cases](https://www.databricks.com/resources/ebook/big-book-of-machine-learning-use-cases)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks Model* *Serving*: [https://www.databricks.com/blog/2023/03/07/announcing-general-availability-databricks-model-serving.html](https://www.databricks.com/blog/2023/03/07/announcing-general-availability-databricks-model-serving.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Create and manage serving endpoints using* *Mlflow*: [https://docs.databricks.com/en/machine-learning/model-serving/create-serving-endpoints-mlflow.html](https://docs.databricks.com/en/machine-learning/model-serving/create-serving-endpoints-mlflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model Evaluation in* *MLFLow*: [https://www.databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html](https://www.databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The big book of* *MLOps*: [https://www.databricks.com/resources/ebook/the-big-book-of-mlops](https://www.databricks.com/resources/ebook/the-big-book-of-mlops)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks Asset Bundles - Programmatically define, deploy, and run Databricks
    jobs, Delta Live Tables pipelines, and MLOps Stacks using CI/CD best practices
    and* *workflows*: [https://docs.databricks.com/en/dev-tools/bundles/index.html](https://docs.databricks.com/en/dev-tools/bundles/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model Registry Webhooks*: `MLflow Model Registry Webhooks` `on Databricks`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Webhooks*: [https://docs.databricks.com/en/mlflow/model-registry-webhooks.html](https://docs.databricks.com/en/mlflow/model-registry-webhooks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CI/CD workflows with Git and Databricks Repos - Use GitHub and Databricks
    Repos for source control and CI/CD* *workflows*: [https://docs.databricks.com/en/repos/ci-cd-techniques-with-repos.html](https://docs.databricks.com/en/repos/ci-cd-techniques-with-repos.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Continuous integration and delivery using GitHub Actions - Build a CI/CD workflow
    on GitHub that uses GitHub Actions developed for* *Databricks*: [https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-github.html](https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-github.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CI/CD with Jenkins on Databricks – Develop a CI/CD pipeline for Databricks
    that uses* *Jenkins*: [https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-jenkins.html](https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-jenkins.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Orchestrate Databricks jobs with Apache Airflow – Manage and schedule a data
    pipeline that uses Apache* *Airflow*: [https://docs.databricks.com/en/workflows/jobs/how-to/use-airflow-with-jobs.html](https://docs.databricks.com/en/workflows/jobs/how-to/use-airflow-with-jobs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Service principals for CI/CD –* *Use service principals instead of users with
    CI/CD* *systems*: [https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-sp.html](https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-sp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DFE* *client*: [https://docs.databricks.com/en/machine-learning/feature-store/python-api.html#use-the-clients-for-unit-testing](https://docs.databricks.com/en/machine-learning/feature-store/python-api.html#use-the-clients-for-unit-testing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unity*: [https://docs.databricks.com/en/udf/unity-catalog.html](https://docs.databricks.com/en/udf/unity-catalog.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Best Practices for LLM Evaluation of RAG Applications, Part* *1*: [https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Best Practices for LLM Evaluation of RAG Applications, Part* *2*: [https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part](https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Create model serving* *endpoints*: [https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html](https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
