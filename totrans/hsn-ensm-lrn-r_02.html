<html><head></head><body>
<div class="book" title="Chapter&#xA0;2.&#xA0;Bootstrapping" id="H5A41-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02" class="calibre1"/>Chapter 2. Bootstrapping</h1></div></div></div><p class="calibre7">As seen in the previous chapter, statistical inference is enhanced to a very large extent with the use of computational power. We also looked at the process of permutation tests, wherein the same test is applied multiple times for the resamples of the given data under the (null) hypothesis. The rationale behind resampling methods is also similar; we believe that if the sample is truly random and the observations are generated from the same identical distribution, we have a valid reason to resample the same set of observations with replacements. This is because any observation might as well occur multiple times rather than as a single instance.</p><p class="calibre7">This chapter will begin with a formal definition of resampling, followed by a look at the jackknife technique. This will be applied to multiple, albeit relatively easier, problems, and we will look at the definition of the pseudovalues first. The bootstrap method, invented by Efron, is probably the most useful resampling method. We will study this concept thoroughly and vary the applications from simple cases to regression models.</p><p class="calibre7">In this chapter, we will cover the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre8">The jackknife technique</strong></span>: Our first resampling method that enables bias reduction</li><li class="listitem"><span class="strong"><strong class="calibre8">Bootstrap</strong></span>: A statistical method and generalization of the jackknife method</li><li class="listitem"><span class="strong"><strong class="calibre8">The boot package</strong></span>: The main R package for bootstrap methods</li><li class="listitem"><span class="strong"><strong class="calibre8">Bootstrap and testing hypothesis</strong></span>: Using the bootstrap method for hypothesis testing</li><li class="listitem"><span class="strong"><strong class="calibre8">Bootstrapping regression models</strong></span>: Applying the bootstrap method to the general regression model</li><li class="listitem"><span class="strong"><strong class="calibre8">Bootstrapping survival models</strong></span>: Applying the bootstrap method for the survival data</li><li class="listitem"><span class="strong"><strong class="calibre8">Bootstrapping time series models</strong></span>: The bootstrap method for the time series data – observations are dependent here</li></ul></div></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Bootstrapping" id="H5A41-2006c10fab20488594398dc4871637ee">
<div class="book" title="Technical requirements"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch02lvl1sec16" class="calibre1"/>Technical requirements</h1></div></div></div><p class="calibre7">We will be using the following libraries in the chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="literal">ACSWR</code></li><li class="listitem"><code class="literal">boot</code></li><li class="listitem"><code class="literal">car</code></li><li class="listitem"><code class="literal">gee</code></li><li class="listitem"><code class="literal">mvtnorm</code></li><li class="listitem"><code class="literal">pseudo</code></li><li class="listitem"><code class="literal">RSADBE</code></li><li class="listitem"><code class="literal">survival</code></li></ul></div></div></div>

<div class="book" title="The jackknife technique"><div class="book" id="I3QM2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec17" class="calibre1"/>The jackknife technique</h1></div></div></div><p class="calibre7">Quenouille (1949) invented the jackknife technique. The purpose of this was to reduce bias by looking at multiple samples of data in a methodical way. The name<a id="id89" class="calibre1"/> jackknife seems to have been coined by the well-known statistician John W. Tukey. Due mainly to the lack of computational power, the advances and utility of the jackknife method were restricted. Efron invented the bootstrap method in 1979 (see the following section for its applications) and established the connection with the jackknife method. In fact, these two methods have a lot in common and are generally put under the umbrella of <span class="strong"><em class="calibre9">resampling methods</em></span>.</p><p class="calibre7">Suppose that we draw a random sample <span class="strong"><img src="../images/00044.jpeg" alt="The jackknife technique" class="calibre15"/></span>of size <span class="strong"><em class="calibre9">n</em></span> from a probability distribution <span class="strong"><em class="calibre9">F</em></span>, and we denote by <span class="strong"><img src="../images/00045.jpeg" alt="The jackknife technique" class="calibre15"/></span> the parameter of interest. Let <span class="strong"><img src="../images/00046.jpeg" alt="The jackknife technique" class="calibre15"/></span> be an estimator of <span class="strong"><img src="../images/00047.jpeg" alt="The jackknife technique" class="calibre15"/></span>, and here we don't have the probability distribution of <span class="strong"><img src="../images/00048.jpeg" alt="The jackknife technique" class="calibre15"/></span> for a given <span class="strong"><img src="../images/00049.jpeg" alt="The jackknife technique" class="calibre15"/></span>. Resampling methods will help in carrying out statistical inference when the probability distribution is unknown. A formal definition of the concept is in order.</p><p class="calibre7">Definition: <span class="strong"><em class="calibre9">Resampling methods</em></span> are ways of estimating the bias and variance of the estimator <span class="strong"><img src="../images/00050.jpeg" alt="The jackknife technique" class="calibre15"/></span> that uses the values of <span class="strong"><img src="../images/00051.jpeg" alt="The jackknife technique" class="calibre15"/></span> based on subsamples from the available observations <span class="strong"><img src="../images/00052.jpeg" alt="The jackknife technique" class="calibre15"/></span>.</p><p class="calibre7">The <span class="strong"><strong class="calibre8">jackknife technique</strong></span> is <a id="id90" class="calibre1"/>a resampling method, and we will lay down its general procedure in the ensuing discussion. As stated previously, <span class="strong"><img src="../images/00053.jpeg" alt="The jackknife technique" class="calibre15"/></span> is an estimator of <span class="strong"><img src="../images/00054.jpeg" alt="The jackknife technique" class="calibre15"/></span>. For simplicity, we define the vector of the given observations by <span class="strong"><img src="../images/00055.jpeg" alt="The jackknife technique" class="calibre15"/></span>. The important quantity in setting up this procedure is the <span class="strong"><em class="calibre9">pseudovalue</em></span>, and we will define this mathematically next.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Definition</strong></span>: Let <span class="strong"><img src="../images/00056.jpeg" alt="The jackknife technique" class="calibre15"/></span>, that is, <span class="strong"><img src="../images/00057.jpeg" alt="The jackknife technique" class="calibre15"/></span> is the vector <span class="strong"><img src="../images/00058.jpeg" alt="The jackknife technique" class="calibre15"/></span> without the <span class="strong"><em class="calibre9">i-th</em></span> observation. The <span class="strong"><em class="calibre9">i-th pseudovalue</em></span> of <span class="strong"><img src="../images/00059.jpeg" alt="The jackknife technique" class="calibre15"/></span> is then defined as follows:</p><div class="mediaobject"><img src="../images/00060.jpeg" alt="The jackknife technique" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">It can be mathematically demonstrated that the <span class="strong"><em class="calibre9">pseudovalue</em></span> is equivalent to the following:</p><div class="mediaobject"><img src="../images/00061.jpeg" alt="The jackknife technique" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Thus, the <span class="strong"><em class="calibre9">pseudovalue</em></span> is seen<a id="id91" class="calibre1"/> as the bias-corrected version of <span class="strong"><img src="../images/00062.jpeg" alt="The jackknife technique" class="calibre15"/></span>. The pseudovalues defined here are also referred to as <span class="strong"><em class="calibre9">delete-one</em></span> jackknife. The jackknife method treats the pseudovalues as independent observations with mean <span class="strong"><img src="../images/00063.jpeg" alt="The jackknife technique" class="calibre15"/></span>, and then applies the <span class="strong"><em class="calibre9">central limit theorem</em></span> for carrying out the statistical inference. The mean and (sampling) variance of the pseudovalues is given as follows:</p><div class="mediaobject"><img src="../images/00064.jpeg" alt="The jackknife technique" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00065.jpeg" alt="The jackknife technique" class="calibre10"/></div><p class="calibre11"> </p></div>

<div class="book" title="The jackknife technique">
<div class="book" title="The jackknife method for mean and variance"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec23" class="calibre1"/>The jackknife method for mean and variance</h2></div></div></div><p class="calibre7">Suppose the probability distribution<a id="id92" class="calibre1"/> is unknown, and the histogram and other visualization techniques suggest that the assumption of normal distribution is not appropriate. However, we don't have rich information either to formulate a reasonable probability model for the problem at hand. Here, we can put the jackknife technique to good use.</p><p class="calibre7">We define mean and variance estimators as follows:</p><div class="mediaobject"><img src="../images/00066.jpeg" alt="The jackknife method for mean and variance" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00067.jpeg" alt="The jackknife method for mean and variance" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The pseudovalues associated with <span class="strong"><img src="../images/00068.jpeg" alt="The jackknife method for mean and variance" class="calibre15"/></span> and <span class="strong"><img src="../images/00069.jpeg" alt="The jackknife method for mean and variance" class="calibre15"/></span> are respectively given in the following expressions:</p><div class="mediaobject"><img src="../images/00070.jpeg" alt="The jackknife method for mean and variance" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00071.jpeg" alt="The jackknife method for mean and variance" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The mean of <span class="strong"><img src="../images/00072.jpeg" alt="The jackknife method for mean and variance" class="calibre15"/></span> will be the<a id="id93" class="calibre1"/> sample mean, and the mean of <span class="strong"><img src="../images/00073.jpeg" alt="The jackknife method for mean and variance" class="calibre15"/></span> will be sampling variance. However, the application of the jackknife method lies in the details. Based on the estimated mean alone, we would not be able to infer about the population mean, and based on the sample variance, we would not be able to exact inference about the population variance. To see what is happening with these formulas of pseudovalues and how their variances will be useful, we will set up an elegant R program next.</p><p class="calibre7">We will simulate <span class="strong"><em class="calibre9">n = 1000</em></span> observations from the Weibull distribution with some scale and shape parameters. In the standard literature, we will be able to find the estimates of these two parameters. However, a practitioner is seldom interested in these parameters and would prefer to infer about the mean and variance of the lifetimes. The density function is a complex form. Furthermore, the theoretical mean and variance of a Weibull random variable in terms of the scale and shape parameter is easily found to be too complex, and the expressions involving Gamma integrals do not help the case any further. If the reader tries to search for the string <span class="strong"><em class="calibre9">statistical inference for the mean of Weibull distribution</em></span> in a search engine, the results will not be satisfactory, and it won't be easy to proceed any further, except for individuals who are mathematically adept. In this complex scenario, we will look at how the jackknife method saves the day for us.</p><p class="calibre7">A note is in order before we proceed. The reader might wonder, <span class="strong"><em class="calibre9">who cares about Weibull distribution in this era of brawny super-computational machines?</em></span> However, any reliability engineer will vouch for the usefulness of lifetime distributions, and Weibull is an important member of this class. The second point might be that the normal approximation will hold well for large samples. However, when we<a id="id94" class="calibre1"/> have moderate samples to carry out the inference, the normal approximation for a highly skewed distribution such as Weibull might lose out on the power and confidence of the tests. Besides, the question is that if we firmly believe that the underlying distribution is Weibull (without parameters) it remains a monumental mathematical task to obtain the exact distributions of the mean and variance of the Weibull distribution.</p><p class="calibre7">The R program will implement the jackknife technique for the mean and variance for given raw data:</p><div class="informalexample"><pre class="programlisting">&gt; # Simulating observations from Weibull distribution
&gt; set.seed(123)
&gt; sr &lt;- rweibull(1000,0.5,15)
&gt; mean(sr); sd(sr); var(sr)
[1] 30.41584
[1] 69.35311
[1] 4809.854</pre></div><p class="calibre7">As mentioned in earlier simulation scenarios, we plant the seed for the sake of reproducible results. The <code class="literal">rweibull</code> function helps to enact the task of simulating observations from the Weibull distribution. We calculate the mean, standard deviation, and variance of the sample. Next, we define the <code class="literal">pv_mean</code> function that will enable computation of <code class="literal">pseudovalues</code> of mean and variance:</p><div class="informalexample"><pre class="programlisting">&gt; # Calculating the pseudovalues for the mean
&gt; pv_mean &lt;- NULL; n &lt;- length(sr)
&gt; for(i in 1:n)
+   pv_mean[i] &lt;- sum(sr)- (n-1)*mean(sr[-i])
&gt; head(sr,20)
 [1]  23.29756524   0.84873231  11.99112962   0.23216910   0.05650965
 [6] 143.11046494   6.11445277   0.19432310   5.31450418   9.21784734
[11]   0.02920662   9.38819985   2.27263386   4.66225355  77.54961762
[16]   0.16712791  29.48688494 150.60696742  18.64782005   0.03252283
&gt; head(pv_mean,20)
 [1]  23.29756524   0.84873231  11.99112962   0.23216910   0.05650965
 [6] 143.11046494   6.11445277   0.19432310   5.31450418   9.21784734
[11]   0.02920662   9.38819985   2.27263386   4.66225355  77.54961762
[16]   0.16712791  29.48688494 150.60696742  18.64782005   0.03252283
&gt; mean(pv_mean); sd(pv_mean)
[1] 30.41584
[1] 69.35311</pre></div><p class="calibre7">Note that the values and <code class="literal">pseudovalues</code> of the mean and the value of the observation are the same for all observations. In fact, this is anticipated, as the statistic we are looking at is<a id="id95" class="calibre1"/> the mean, which is simply the average. Removing the average of other observations from that should return the value. Consequently, the mean of the <code class="literal">pseudovalues</code> and the sample mean would be the same too. However, that does not imply that the efforts are futile. We will continue with the computations for the variance term as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # Calculating the pseudovalues for the variance
&gt; pv_var &lt;- NULL
&gt; pseudo_var &lt;- function(x,i){
+   n = length(x)
+   psv &lt;- (n/(n-2))*(x[i]-mean(x))^2-(1/(n-1)*(n-2))*sum(x-mean(x))^2
+   return(psv)
+ }
&gt; pv_var &lt;- NULL
&gt; for(i in 1:n)
+   pv_var[i] &lt;- pseudo_var(sr,i)
&gt; head(pv_var)
[1]    50.77137   875.96574   340.15022   912.87970   923.53596 12725.52973
&gt; var(sr); mean(pv_var)
[1] 4809.854
[1] 4814.673
&gt; sd(pv_var)
[1] 35838.59</pre></div><p class="calibre7">Now, there is no counterpart to the <code class="literal">pseudovalue</code> of the observation in the actual data. Here, the mean of the <code class="literal">pseudovalues</code> will approximately equal the sample variance. This is the standard deviation <code class="literal">sd(pv_var)</code> which will help in carrying out the inference related to the variance or standard deviation.</p><p class="calibre7">We have seen how the<a id="id96" class="calibre1"/> jackknife is useful in inferring the mean and variance. In the next part of this section, we will see how the <code class="literal">pseudovalues</code> can be useful in solving problems in the context of a survival regression problem.</p></div></div>

<div class="book" title="The jackknife technique">
<div class="book" title="Pseudovalues method for survival data"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec24" class="calibre1"/>Pseudovalues method for survival data</h2></div></div></div><p class="calibre7">The primary biliary cirrhosis data, <code class="literal">pbc</code>, was introduced in <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, in <span class="strong"><em class="calibre9">Section 2</em></span>, and we made a note that the data is special in that it is <a id="id97" class="calibre1"/>survival data and the variable of interest time is subject to censoring, which complicates further analysis. Specialized methods for dealing with survival data will be dealt with in <a class="calibre1" title="Chapter 10. Ensembling Survival Models" href="part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee">Chapter 10</a>, <span class="strong"><em class="calibre9">Ensembling Survival Models</em></span>. The specialized methods include the hazards regression, and the impact of covariates is measured on the hazard rate and not on the lifetime. It has been observed that practitioners find these concepts a tad difficult, and hence we will briefly discuss an alternative approach based on the pseudovalues.</p><p class="calibre7">Andersen and Klein have effectively used the notion of pseudovalues for various problems in a series of papers:</p><div class="informalexample"><pre class="programlisting">&gt; library(survival)
&gt; library(pseudo)
&gt; library(gee)
&gt; data(pbc)
&gt; time_pseudo &lt;- pseudomean(time=pbc$time,event=pbc$status==2)
&gt; pbc_gee &lt;- gee(time_pseudo ~ trt + age + sex + ascites + hepato +
+                  spiders + edema + bili + chol + albumin + copper + 
+                  alk.phos + ast + trig + platelet + protime + stage,
+                id=1:nrow(pbc), family="gaussian",
+                data=pbc)
Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27
running glm to get initial regression estimate
 (Intercept)          trt          age         sexf      ascites       hepato 
5901.1046673  115.5247130  -23.6893551  233.0351191 -251.2292823  -63.1776549 
     spiders        edema         bili         chol      albumin       copper 
-264.2063329 -441.2298926  -67.7863015   -0.5739644  438.5953357   -2.3704801 
    alk.phos          ast         trig     platelet      protime        stage 
  -0.0619931   -1.1273468    0.2317984   -0.4243154 -160.6784722 -292.9838866 
&gt; summary(pbc_gee)

 GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA
 gee S-function, version 4.13 modified 98/01/27 (1998) 

Model:
 Link:                      Identity 
 Variance to Mean Relation: Gaussian 
 Correlation Structure:     Independent 

Call:
gee(formula = time_pseudo ~ trt + age + sex + ascites + hepato + 
    spiders + edema + bili + chol + albumin + copper + alk.phos + 
    ast + trig + platelet + protime + stage, id = 1:nrow(pbc), 
    data = pbc, family = "gaussian")

Summary of Residuals:
       Min         1Q     Median         3Q        Max 
-3515.1303  -792.8410   112.1563   783.9519  3565.1490 


Coefficients:
                Estimate   Naive S.E.    Naive z  Robust S.E.   Robust z
(Intercept) 5901.1046673 1.524661e+03  3.8704367 1.470722e+03  4.0123856
trt          115.5247130 1.616239e+02  0.7147750 1.581686e+02  0.7303895
age          -23.6893551 8.507630e+00 -2.7844835 8.204491e+00 -2.8873643
sexf         233.0351191 2.701785e+02  0.8625227 3.215865e+02  0.7246421
ascites     -251.2292823 4.365874e+02 -0.5754387 5.133867e+02 -0.4893568
hepato       -63.1776549 1.884840e+02 -0.3351885 1.786614e+02 -0.3536166
spiders     -264.2063329 1.986929e+02 -1.3297220 2.045738e+02 -1.2914962
edema       -441.2298926 4.155360e+02 -1.0618331 4.850261e+02 -0.9097034
bili         -67.7863015 2.651543e+01 -2.5564852 2.009844e+01 -3.3727151
chol          -0.5739644 4.117889e-01 -1.3938317 3.929789e-01 -1.4605475
albumin      438.5953357 2.321347e+02  1.8894000 2.156405e+02  2.0339196
copper        -2.3704801 1.120153e+00 -2.1162114 1.102365e+00 -2.1503594
alk.phos      -0.0619931 3.932052e-02 -1.5766092 4.571919e-02 -1.3559535
ast           -1.1273468 1.640940e+00 -0.6870130 1.797116e+00 -0.6273089
trig           0.2317984 1.416552e+00  0.1636356 1.375674e+00  0.1684980
platelet      -0.4243154 9.348907e-01 -0.4538663 9.106646e-01 -0.4659403
protime     -160.6784722 9.139593e+01 -1.7580484 9.254740e+01 -1.7361749
stage       -292.9838866 1.137951e+02 -2.5746618 1.025891e+02 -2.8558966

Estimated Scale Parameter:  1675818
Number of Iterations:  1
Working Correlation
     [,1]
[1,]    1</pre></div></div></div>

<div class="book" title="Bootstrap &#x2013; a statistical method"><div class="book" id="J2B82-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec18" class="calibre1"/>Bootstrap – a statistical method</h1></div></div></div><p class="calibre7">In this section, we will explore <a id="id98" class="calibre1"/>complex statistical functional. What is the statistical distribution of the correlation between two random variables? If normality assumption does not hold for the multivariate data, then what is an alternative way to obtain the standard error and confidence interval? Efron (1979) invented the <span class="strong"><em class="calibre9">bootstrap technique</em></span>, which provides the solutions that enable statistical inference related to complex statistical functionals. In <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, the permutation test, which repeatedly draws samples of the given sample and carries out the test for each of the resamples, was introduced. In theory, the permutation test requires <span class="strong"><img src="../images/00074.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> number of resamples, where <span class="strong"><em class="calibre9">m </em></span>and <span class="strong"><em class="calibre9">n </em></span>are the number of observations in the two samples, though one does take their foot off the pedal after having enough resamples. The bootstrap method works in a similar way and is an important resampling method.</p><p class="calibre7">Let <span class="strong"><img src="../images/00075.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> be an independent random sample from a probability distribution F, the parameter of interest be <span class="strong"><img src="../images/00076.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>, and an estimator of the parameter be denoted by <span class="strong"><img src="../images/00077.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>. If the probability distribution of <span class="strong"><img src="../images/00077.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> for the <span class="strong"><img src="../images/00078.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> parameter is either unknown or intractable, then the statistical inference about the parameter can't be carried out. Consequently, we need a generic technique that will aid in the inference.</p><p class="calibre7">Efron's method unfolds as follows. The estimate provided by <span class="strong"><img src="../images/00077.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> is a single value. Given the data and based on the assumption that we have an IID sample, the bootstrap method explores the possibility that any observed value is as likely as any other observed value. Thus, a random sample of size <span class="strong"><em class="calibre9">n</em></span> drawn <span class="strong"><em class="calibre9">with replacement</em></span> is intuitively expected to carry the same information as the actual sample, and we can obtain an estimate of the <span class="strong"><img src="../images/00079.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> parameter based on this sample. This step can then be repeated a large number of times, and we<a id="id99" class="calibre1"/> will produce a varied number of estimates of the parameter. Using this distribution of estimates, statistical inference can then be performed. A formal description of this method is in order.</p><p class="calibre7">Draw a random sample with replacement of size <span class="strong"><em class="calibre9">n </em></span>from <span class="strong"><img src="../images/00080.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> and denote it by <span class="strong"><img src="../images/00081.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>. The sample <span class="strong"><img src="../images/00082.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> is referred to as the <span class="strong"><em class="calibre9">first bootstrap sample</em></span>. Compute the estimate <span class="strong"><img src="../images/00083.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> for this sample <span class="strong"><img src="../images/00084.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> and denote it by <span class="strong"><img src="../images/00085.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>. Repeat the steps a large number of times and obtain <span class="strong"><img src="../images/00086.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>. The inference for <span class="strong"><img src="../images/00087.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> can then be based on the bootstrap estimates <span class="strong"><img src="../images/00088.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>. We can put this description in the form of an algorithm:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">
Given data <span class="strong"><img src="../images/00089.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>, the parameter of interest <span class="strong"><img src="../images/00090.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>, calculate the estimate <span class="strong"><img src="../images/00091.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>.
</li><li class="listitem" value="2">
Draw a bootstrap sample of size <span class="strong"><em class="calibre9">n</em></span> with replacement from <span class="strong"><img src="../images/00092.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>, and denote it by <span class="strong"><img src="../images/00093.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>.
</li><li class="listitem" value="3">
Calculate the statistic <span class="strong"><img src="../images/00094.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> for the bootstrap sample.
</li><li class="listitem" value="4">
Repeat <span class="strong"><em class="calibre9">Repeat Steps 2 and 3 'B – 1'</em></span> number of times to produce <span class="strong"><img src="../images/00095.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>.
</li><li class="listitem" value="5">
Use <span class="strong"><img src="../images/00096.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> to carry out the statistical inference related to <span class="strong"><img src="../images/00097.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>.
</li></ol><div class="calibre13"/></div><p class="calibre7">What does <span class="strong"><em class="calibre9">Step 5</em></span> convey here? We have each <a id="id100" class="calibre1"/>of the <span class="strong"><img src="../images/00098.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> values estimate the parameter of interest, <span class="strong"><em class="calibre9">B</em></span> estimates to be precise. Since the estimate (based on the sample) is <span class="strong"><img src="../images/00099.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>, we (intuitively) expect that the average of the bootstrap estimates <span class="strong"><img src="../images/00100.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> will be very close to <span class="strong"><img src="../images/00101.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>, hence the variance of the bootstrap estimates also gives a 'good' measure of the variance of the estimator too. The bootstrap mean and standard deviation are then computed as the following:</p><div class="mediaobject"><img src="../images/00102.jpeg" alt="Bootstrap – a statistical method" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00103.jpeg" alt="Bootstrap – a statistical method" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Using <span class="strong"><img src="../images/00104.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span> and <span class="strong"><img src="../images/00105.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>, we can carry out inference for <span class="strong"><img src="../images/00106.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>.</p><p class="calibre7">It should be noted that the<a id="id101" class="calibre1"/> bootstrap method is a very generic algorithm, and the execution of this is illustrated to address occasions when faced with certain interesting problems.</p><p class="calibre7">The idea of sampling with replacement needs to be elucidated here. For the purpose of simplicity, assume that we have only five observations, for example <span class="strong"><img src="../images/00107.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>. Now, when we draw the first bootstrap sample with a replacement size of 5, we might get the labels <span class="strong"><em class="calibre9">2</em></span>, <span class="strong"><em class="calibre9">4</em></span>, <span class="strong"><em class="calibre9">4</em></span>, <span class="strong"><em class="calibre9">1</em></span>, <span class="strong"><em class="calibre9">3</em></span>. This means that, from the original sample, we select <span class="strong"><img src="../images/00108.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>, and consequently the observations labeled <span class="strong"><em class="calibre9">2</em></span>, <span class="strong"><em class="calibre9">1</em></span>, and <span class="strong"><em class="calibre9">3</em></span> are selected once and <span class="strong"><em class="calibre9">4</em></span> is selected twice. This is the same as <span class="strong"><img src="../images/00109.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>. In the bootstrap notation, it would be <span class="strong"><img src="../images/00110.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>. The second bootstrap might be <span class="strong"><img src="../images/00111.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>, and the third might be <span class="strong"><img src="../images/00112.jpeg" alt="Bootstrap – a statistical method" class="calibre15"/></span>, and so on.</p><p class="calibre7">Next, we will illustrate this<a id="id102" class="calibre1"/> technique and clarify its implementation. The bootstrap method will be applied to two problems:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Standard error of correlation coefficient </li><li class="listitem">Eigenvalue of the covariance/correlation matrix</li></ul></div></div>

<div class="book" title="Bootstrap &#x2013; a statistical method">
<div class="book" title="The standard error of correlation coefficient"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec25" class="calibre1"/>The standard error of correlation coefficient</h2></div></div></div><p class="calibre7">Consider a hypothetical scenario where we<a id="id103" class="calibre1"/> are trying to study the relationship between two variables. The historical information, intuition, and scatterplot all align, showing that there is a linear relationship between the two variables, and the only problem is that the histogram of each of the two variables suggests a shape that is anything but a bell shape. In other words, the assumption of normal distribution looks very unlikely, and since it fails in the univariate cases (of each variable), the analyst is skeptical about the joint bivariate normality holding true for the two variables.</p><p class="calibre7">Let <span class="strong"><img src="../images/00113.jpeg" alt="The standard error of correlation coefficient" class="calibre15"/></span> be <span class="strong"><em class="calibre9">n</em></span> pairs of observations. The sample correlation coefficient is easily calculated using the following formula:</p><div class="mediaobject"><img src="../images/00114.jpeg" alt="The standard error of correlation coefficient" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Here we have <span class="strong"><img src="../images/00115.jpeg" alt="The standard error of correlation coefficient" class="calibre15"/></span>. An estimate of any parameter is no good if we can't carry out the relevant statistical inference. A confidence interval<a id="id104" class="calibre1"/> suffices with the relevant results to perform the statistical inference. We will now learn how the bootstrap method helps us to do that. We will use the vector notation to maintain consistency, and toward this define <span class="strong"><img src="../images/00116.jpeg" alt="The standard error of correlation coefficient" class="calibre15"/></span>, that is, <span class="strong"><img src="../images/00117.jpeg" alt="The standard error of correlation coefficient" class="calibre15"/></span> is a vector now. The first bootstrap sample is obtained by selecting n pairs of observations randomly and with replacement, and we will denote the first bootstrap sample by <span class="strong"><img src="../images/00118.jpeg" alt="The standard error of correlation coefficient" class="calibre15"/></span>. Now, using the bootstrap sample, we will estimate the correlation coefficient by <span class="strong"><img src="../images/00119.jpeg" alt="The standard error of correlation coefficient" class="calibre15"/></span>. Repeating the process of obtaining the bootstrap samples <span class="strong"><em class="calibre9">B – 1</em></span> more times, we will compute the correlation coefficients <span class="strong"><img src="../images/00120.jpeg" alt="The standard error of correlation coefficient" class="calibre15"/></span>.</p><p class="calibre7">The law school data is used here, drawn from Table 3.1 of Efron and Tibshirani (1990, p.19). In this study, fifteen schools are randomly selected from a pool of 82 law schools. Two variables measured for the schools include the average score for the class on a national law test (LSAT) and the average undergraduate grade point (GPA). We will first import the data from the CSV file, display it, and then visualize the histograms of the two variables along with the scatterplot:</p><div class="informalexample"><pre class="programlisting">&gt; LS &lt;- read.csv("../Data/Law_School.csv",header=TRUE)
&gt; LS
   School LSAT  GPA
1       1  576 3.39
2       2  635 3.30
3       3  558 2.81

13     13  545 2.76
14     14  572 2.88
15     15  594 2.96
&gt; windows(height=100,width=100)
&gt; layout(matrix(c(1,2,3,3),byrow=TRUE, nrow=2))
&gt; hist(LS$LSAT,xlab="LSAT",main="Histogram of LSAT")
&gt; hist(LS$GPA,xlab="GPA",main="Histogram of GPA")
&gt; plot(LS[,2:3],main="Scatter plot between LSAT and GPA")</pre></div><p class="calibre7">We will look at the code first. The <code class="literal">read.csv</code> file helps in importing the dataset from the chapter's <code class="literal">Data</code> folder, as unzipped from the code bundle and stored in the <code class="literal">LS</code> object. The <code class="literal">LS</code> is then displayed in the console. Here, we give the first and last three observations. The <code class="literal">windows</code> function creates a new graphical device with specified <code class="literal">height</code> and <code class="literal">weight</code>. Note that this function will only work on the Windows OS. Next, we specify the <code class="literal">layout</code> for the graphical device. To confirm that it's working, running the line <code class="literal">matrix(c(1,2,3,3),byrow=TRUE, nrow=2)</code> in R terminal gives the following result: </p><div class="informalexample"><pre class="programlisting">&gt; matrix(c(1,2,3,3),byrow=TRUE, nrow=2)
     [,1] [,2]
[1,]    1    2
[2,]    3    3</pre></div><p class="calibre7">This means that the first graphical output, as a<a id="id105" class="calibre1"/> consequence of running any code that results in a graph, is displayed in region 1 (upper left) of the device. The second graphical output is displayed in the right upper part, while the third will be spread across the lower part. It is a convenient manipulation for visual displays. The histogram of the two variables does not suggest normal distribution, though it may be argued that the number of observations is much less; fifteen, in this case. However, the scatter plot suggests that as <code class="literal">LSAT</code> increases, the <code class="literal">GPA</code> does too. Consequently, the correlation coefficient is a meaningful measure of the<a id="id106" class="calibre1"/> linear relationship between the two variables. However, the normal distribution assumption is not suitable here, or at least we need more observations that are not available as of now, and hence it remains a challenge to carry out the statistical inference. To overcome this, we will use the bootstrap technique. Take a look at the following figure:</p><div class="mediaobject"><img src="../images/00121.jpeg" alt="The standard error of correlation coefficient" class="calibre10"/><div class="caption"><p class="calibre14">Figure 1: LSAT and GPA variables visualization</p></div></div><p class="calibre11"> </p><p class="calibre7">Imitating Efron and Tibshirani's illustration, we<a id="id107" class="calibre1"/> fix the number of bootstrap samples at 3200, and we will be interested when the number of bootstrap samples is at 25, 50, 100, 200, 400, 800, 1600, and 3200. The R program approach is as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">After finding the number of observations, fixing the number of bootstrap samples, and bootstrap samples of interest, we will initialize the bootstrap mean and standard vector.</li><li class="listitem">For the purpose of replicating the results, we will fix the initial seed at 54321. The seed will be stepped up by an increment of 1 for obtaining the bootstrap sample, which will ensure that all the bootstrap samples are different.</li><li class="listitem">The value of the correlation coefficient is computed for each of the bootstrap samples, and thus we will have B = 3200 correlation coefficients.</li><li class="listitem">The mean and standard deviation of the correlation coefficients up to the desired number of bootstrap samples is calculated.</li><li class="listitem">For comparisons with Efron and Tibshirani (1990, p.50), the results are reported for bootstrap samples 25, 50, 100, 200, 400, 800, 1600, and 3200.</li></ul></div><p class="calibre7">The R program along with the output is given next:</p><div class="informalexample"><pre class="programlisting">&gt; n &lt;- nrow(LS)
&gt; B &lt;- 3200
&gt; TB &lt;- c(25,50,100,200,400,800,1600,3200)
&gt; seB &lt;- NULL
&gt; tcorr &lt;- NULL
&gt; myseed &lt;- 54321
&gt; for(i in 1:B){
+   myseed &lt;- myseed+1
+   set.seed(myseed)
+   tcorr[i] &lt;- as.numeric(cor(LS[sample(1:n,n,replace=TRUE),2:3])[1,2])
+ }
&gt; for(j in 1:length(TB)) seB[j] &lt;- sd(tcorr[1:TB[j]])
&gt; round(seB,3)
[1] 0.141 0.124 0.115 0.135 0.133 0.132 0.133 0.131
&gt; for(j in 2:B){
+   corrB[j] &lt;- mean(tcorr[1:j])
+   seB[j] &lt;- sd(tcorr[1:j])
+   }
&gt; round(corrB[TB],3)
[1] 0.775 0.787 0.793 0.777 0.782 0.773 0.771 0.772
&gt; round(seB[TB],3)
[1] 0.141 0.124 0.115 0.135 0.133 0.132 0.133 0.131
&gt; plot.ts(seB,xlab="Number of Bootstrap Samples",
+         ylab="Bootstrap Standard Error of Correlation")</pre></div><p class="calibre7">The time<a id="id108" class="calibre1"/> series plot is displayed as follows:</p><div class="mediaobject"><img src="../images/00122.jpeg" alt="The standard error of correlation coefficient" class="calibre10"/><div class="caption"><p class="calibre14">Figure 2: The nonparametric bootstrap standard error for correlation coefficient</p></div></div><p class="calibre11"> </p><p class="calibre7">The standard error of the<a id="id109" class="calibre1"/> correlation coefficient can be seen to stabilize at around <code class="literal">0.13</code>. Finally, to carry out the statistical inference, we can use the bootstrap confidence intervals. A <code class="literal">naïve</code> method is to simply obtain the 95% coverage of the correlation coefficient estimates in the bootstrap samples. This is easily achieved in the software. We will use the quantile function to achieve the result, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; for(i in 1:length(TB)) print(quantile(tcorr[1:TB[i]],c(0.025,0.975)))
     2.5%     97.5% 
0.5225951 0.9481351 
     2.5%     97.5% 
0.5205679 0.9399541 
     2.5%     97.5% 
0.5429510 0.9513826 
     2.5%     97.5% 
0.4354776 0.9588759 
     2.5%     97.5% 
0.4662406 0.9668964 
     2.5%     97.5% 
0.4787843 0.9667736 
     2.5%     97.5% 
0.4614067 0.9621344 
     2.5%     97.5% 
0.4609731 0.9606689 </pre></div><p class="calibre7">We have learned how to carry out statistical inference using the bootstrap technique.</p><p class="calibre7">One of the main reasons for carrying out the bootstrap method here is that we can't assume bivariate normal distribution for the LSAT and GPA variables. Now, if we are told that the distribution of LSAT and GPA historically follows bivariate normal distribution, then technically the probability distribution of the sample correlation coefficient <span class="strong"><img src="../images/00123.jpeg" alt="The standard error of correlation coefficient" class="calibre15"/></span> can be derived. However, as a practitioner, suppose that you are unable to derive the probability distribution of the sample correlation coefficient. How do you then carry out the statistical inference? Using the same technique as discussed here might seem tempting. We will continue to explore this matter in the next subsection.</p></div></div>

<div class="book" title="Bootstrap &#x2013; a statistical method">
<div class="book" title="The parametric bootstrap"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec26" class="calibre1"/>The parametric bootstrap</h2></div></div></div><p class="calibre7">As mentioned in the previous subsection, it <a id="id110" class="calibre1"/>might be tempting to carry out the usual <span class="strong"><em class="calibre9">nonparametric</em></span> bootstrap method. However, nonparametric methods are traditionally known to be somewhat inefficient compared with the parametric methods. We will now look at a mixture of both of these methods.</p><p class="calibre7">We have seen that the bootstrap method relies heavily on the resamples. The bootstrap samples and the consequent estimates are then expected to meet the true underlying probability distributions. However, we might occasionally know more about the shape of the underlying probability distributions, except for a few parameters. The approach for mixing up these two methods will require a modification. The parametric bootstrap method is set up and run as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">
Let <span class="strong"><img src="../images/00124.jpeg" alt="The parametric bootstrap" class="calibre15"/></span> be the IID sample from <span class="strong"><img src="../images/00125.jpeg" alt="The parametric bootstrap" class="calibre15"/></span>, and let <span class="strong"><img src="../images/00126.jpeg" alt="The parametric bootstrap" class="calibre15"/></span> denote an estimator of the parameter based on an appropriate method, say maximum likelihood estimation or method of moments, for example.
</li><li class="listitem">
Simulate the first bootstrap sample <span class="strong"><img src="../images/00127.jpeg" alt="The parametric bootstrap" class="calibre15"/></span> of size <span class="strong"><em class="calibre9">n</em></span> from <span class="strong"><img src="../images/00128.jpeg" alt="The parametric bootstrap" class="calibre15"/></span>, and obtain the first bootstrap estimate <span class="strong"><img src="../images/00129.jpeg" alt="The parametric bootstrap" class="calibre15"/></span> based on <span class="strong"><img src="../images/00130.jpeg" alt="The parametric bootstrap" class="calibre15"/></span> using the same estimation technique as used in the previous step.
</li><li class="listitem">
Repeat the previous step <span class="strong"><em class="calibre9">B – 1</em></span> number of times to obtain <span class="strong"><img src="../images/00131.jpeg" alt="The parametric bootstrap" class="calibre15"/></span> respectively, based on the bootstrap samples <span class="strong"><img src="../images/00132.jpeg" alt="The parametric bootstrap" class="calibre15"/></span>.
</li><li class="listitem">
Carry out the inference based on the <span class="strong"><em class="calibre9">B</em></span> bootstrap estimates <span class="strong"><img src="../images/00133.jpeg" alt="The parametric bootstrap" class="calibre15"/></span>.
</li></ul></div><p class="calibre7">The parametric bootstrap technique will be illustrated using the earlier example of <code class="literal">LSAT</code> and <code class="literal">GPA</code> variables. For the bivariate normal distribution, the mean vector is an estimator of the<a id="id111" class="calibre1"/> population mean, and it enjoys statistical properties as being the unbiased estimator and MLE. Similarly, the sample variance-covariance matrix also gives an important estimate of the population variance-covariance matrix. The <code class="literal">colMeans</code> is applied on the data frame to obtain the vector mean and the <code class="literal">var</code> function to compute the sample variance-covariance matrix. The R code block easily follows:</p><div class="informalexample"><pre class="programlisting">&gt; LS_mean &lt;- colMeans(LS[,2:3])
&gt; LS_var&lt;- var(LS[,2:3])
&gt; LS_mean; LS_var
      LSAT        GPA 
600.266667   3.094667 
            LSAT       GPA
LSAT 1746.780952 7.9015238
GPA     7.901524 0.0592981</pre></div><p class="calibre7">Thus, we have the mean and variance-covariance matrix estimators. We now look at the parametric bootstrap computations. Now, using the <code class="literal">rmvnorm</code> function from the <code class="literal">mvtnorm</code> package, we are able to simulate observations from a multivariate (bivariate) normal distribution. With the (parametric) bootstrap<a id="id112" class="calibre1"/> sample available, the rest of the program and conclusion is similar. The complete R program with the resulting diagram is as follows:</p><div class="informalexample"><pre class="programlisting">&gt; TB &lt;- c(25,50,100,200,400,800,1600,3200)
&gt; ptcorr &lt;- NULL
&gt; ptcorrB &lt;- NULL
&gt; pseB &lt;- NULL
&gt; myseed &lt;- 54321
&gt; for(i in 1:B){
+   myseed &lt;- myseed+1
+   set.seed(myseed)
+   temp &lt;- rmvnorm(n,LS_mean,LS_var)
+   ptcorr[i] &lt;- as.numeric(cor(temp)[1,2])
+ }
&gt; for(j in 2:B){
+   ptcorrB[j] &lt;- mean(ptcorr[1:j])
+   pseB[j] &lt;- sd(ptcorr[1:j])
+ }
&gt; round(ptcorrB[TB],3)
[1] 0.760 0.782 0.772 0.761 0.766 0.763 0.762 0.766
&gt; round(pseB[TB],3)
[1] 0.129 0.114 0.109 0.129 0.118 0.117 0.120 0.120
&gt; windows(height=100,width=100)
&gt; plot.ts(pseB,xlab="Number of Bootstrap Samples",
+         ylab="Parametric Bootstrap Standard Error of Correlation")
&gt; for(i in 1:length(TB)) print(quantile(ptcorr[1:TB[i]],c(0.025,0.975)))
     2.5%     97.5% 
0.4360780 0.9048064 
     2.5%     97.5% 
0.5439972 0.9211768 
     2.5%     97.5% 
0.5346929 0.9200953 
     2.5%     97.5% 
0.4229031 0.9179324 
     2.5%     97.5% 
0.4650078 0.9194452 
     2.5%     97.5% 
0.4747372 0.9214653 
     2.5%     97.5% 
0.4650078 0.9245066 
     2.5%     97.5% 
0.4662502 0.9241084 </pre></div><p class="calibre7">The difference between parametric and<a id="id113" class="calibre1"/> nonparametric bootstrap can easily be seen. The confidence intervals are very short, and the standard error decreases to zero as the number of bootstrap samples increases. In spite of the advantage, we generally need bootstrap methods when the parametric methods fail. Take a look at the following figure:</p><div class="mediaobject"><img src="../images/00134.jpeg" alt="The parametric bootstrap" class="calibre10"/><div class="caption"><p class="calibre14">Figure 3: The parametric bootstrap standard error for correlation coefficient</p></div></div><p class="calibre11"> </p><p class="calibre7">Next, we will consider a slightly complex problem<a id="id114" class="calibre1"/> for the application of the bootstrap method.</p></div></div>

<div class="book" title="Bootstrap &#x2013; a statistical method">
<div class="book" title="Eigen values"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch02lvl2sec27" class="calibre1"/>Eigen values</h2></div></div></div><p class="calibre7">Multivariate statistics is the arm of Statistics which deals with a vector of random variables. In the previous example, we have bivariate data, where LSAT and GPA <a id="id115" class="calibre1"/>scores are obtained for fifteen schools. Now we will consider another example, where we have more than two variables; namely we have five observations here. The description and bootstrap technique-related details are drawn from <a class="calibre1" title="Chapter 7. The General Ensemble Technique" href="part0051_split_000.html#1GKCM1-2006c10fab20488594398dc4871637ee">Chapter 7</a>, <span class="strong"><em class="calibre9">The General Ensemble Technique</em></span>, of Efron and Tibshirani (1990). The chapter discusses the score data from the classic multivariate book by Mardia, Kent, and Bibby (1979).</p><p class="calibre7">A quick brief of notation is as follows. We will denote the vector of random variables by <span class="strong"><img src="../images/00135.jpeg" alt="Eigen values" class="calibre15"/></span>, and for the <span class="strong"><em class="calibre9">ith</em></span> observation, the vector will be <span class="strong"><img src="../images/00136.jpeg" alt="Eigen values" class="calibre15"/></span>. Here, each component <span class="strong"><em class="calibre9">Xi</em></span> is assumed to be a continuous random variable. Most often, and for practical and theoretical purposes, we assume that the random vector follows a multivariate normal distribution with mean vector <span class="strong"><img src="../images/00137.jpeg" alt="Eigen values" class="calibre15"/></span> and variance-covariance matrix <span class="strong"><img src="../images/00138.jpeg" alt="Eigen values" class="calibre15"/></span>. Since it is not feasible to go into the details of multivariate statistics here, the interested reader might simply consult Mardia, Kent, and Bibby (1979).</p><p class="calibre7">In this example, <span class="strong"><em class="calibre9">n = 88</em></span> students' scores are noted for the five subjects of mechanics, vectors, algebra, analysis, and statistics, and a further difference in the test is that the first two subjects, mechanics and vectors, were closed-book tests while algebra, analysis, and statistics were open-book exams. We will first perform the simple preliminary task here of calculating the mean vector, the variance-covariance matrix, and the correlation matrix:</p><div class="informalexample"><pre class="programlisting">&gt; OC &lt;- read.csv("../Data/OpenClose.csv")
&gt; pairs(OC)
&gt; OC_xbar &lt;- colMeans(OC)
&gt; OC_xbar
      MC       VC       LO       NO       SO 
38.95455 50.59091 50.60227 46.68182 42.30682 
&gt; OC_Cov &lt;- cov(OC)
&gt; OC_Cov
         MC        VC        LO        NO        SO
MC 305.7680 127.22257 101.57941 106.27273 117.40491
VC 127.2226 172.84222  85.15726  94.67294  99.01202
LO 101.5794  85.15726 112.88597 112.11338 121.87056
NO 106.2727  94.67294 112.11338 220.38036 155.53553
SO 117.4049  99.01202 121.87056 155.53553 297.75536
&gt; OC_Cor &lt;- cor(OC)
&gt; OC_Cor
          MC        VC        LO        NO        SO
MC 1.0000000 0.5534052 0.5467511 0.4093920 0.3890993
VC 0.5534052 1.0000000 0.6096447 0.4850813 0.4364487
LO 0.5467511 0.6096447 1.0000000 0.7108059 0.6647357
NO 0.4093920 0.4850813 0.7108059 1.0000000 0.6071743
SO 0.3890993 0.4364487 0.6647357 0.6071743 1.0000000</pre></div><p class="calibre7">Here, the data is imported from a <code class="literal">.csv</code> file, and using the <code class="literal">colMeans</code>, <code class="literal">cov</code>, and <code class="literal">cor</code> functions, we obtain the mean vector, variance-covariance matrix, and <a id="id116" class="calibre1"/>correlation matrix. Clearly, we can see from the output of the correlation matrix that a strong association exists between all variables. The visual depiction of the data is obtained by the <code class="literal">pairs</code> function, which gives us a matrix of scatter plots. This plot is as follows:</p><div class="mediaobject"><img src="../images/00139.jpeg" alt="Eigen values" class="calibre10"/><div class="caption"><p class="calibre14">Figure 4: Matrix of scatter plots for the five subjects scores</p></div></div><p class="calibre11"> </p><p class="calibre7">Dimensionality reduction is one of the goals of multivariate statistics. Given a large number of variables, the intent of dimensionality reduction is to find a set of variables that will explain most of the variability in the overall data. A method of dimensionality reduction is <span class="strong"><em class="calibre9">principal component analysis</em></span>. Here, we try to find a new random vector <span class="strong"><img src="../images/00140.jpeg" alt="Eigen values" class="calibre15"/></span>, which is a <span class="strong"><em class="calibre9">vector of principal components</em></span>. Each component of this new random vector is some linear combination of the original variables which will achieve two objectives: (a) the components <span class="strong"><img src="../images/00141.jpeg" alt="Eigen values" class="calibre15"/></span> will be <a id="id117" class="calibre1"/>ordered in the sense that the first component will have variance larger than the second, the second larger than the third, and so on, and (b) each principal component is uncorrelated with the others. The core working of the principal components is tied with the <code class="literal">eigen</code> values of the variance-covariance matrix or the correlation matrix. The <code class="literal">eigen</code> values of the variance-covariance matrix indicates the importance of the associated principal components. Consequently, if have p related random variables, and the estimated variance-covariance matrix is not singular, the normalized p <code class="literal">eigen</code> values will give us the fraction of the variation explained by the principal component. For the purpose of the data, we will explain this here:</p><div class="informalexample"><pre class="programlisting">&gt; OC_eigen &lt;- eigen(OC_Cov)
&gt; OC_eigen$values
[1] 686.98981 202.11107 103.74731  84.63044  32.15329
&gt; OC_eigen$vectors
           [,1]        [,2]       [,3]         [,4]        [,5]
[1,] -0.5054457  0.74874751 -0.2997888  0.296184264 -0.07939388
[2,] -0.3683486  0.20740314  0.4155900 -0.782888173 -0.18887639
[3,] -0.3456612 -0.07590813  0.1453182 -0.003236339  0.92392015
[4,] -0.4511226 -0.30088849  0.5966265  0.518139724 -0.28552169
[5,] -0.5346501 -0.54778205 -0.6002758 -0.175732020 -0.15123239
&gt; OC_eigen$values/sum(OC_eigen$values)
[1] 0.61911504 0.18214244 0.09349705 0.07626893 0.02897653</pre></div><p class="calibre7">The first <code class="literal">eigen</code> value is <code class="literal">686.9898</code>, the second one is <code class="literal">202.1111</code>, and so on. Now, these values divided by their cumulative sum gives the percentage of variation in the data<a id="id118" class="calibre1"/> explained by the principal component. Thus, the total variation of data explained by the first principal component is 61.91%, while 18.21% is explained by the second principal component. Here comes the important question then: how do we conduct the statistical inference related to this quantity? Naturally, we will provide the answer using the bootstrap method:</p><div class="informalexample"><pre class="programlisting">&gt; thetaB &lt;- NULL; sethetaB &lt;- NULL
&gt; B &lt;- 500
&gt; n &lt;- nrow(OC)
&gt; myseed &lt;- 54321
&gt; for(i in 1:B){
+   myseed &lt;- myseed+1
+   set.seed(myseed)
+   OCt &lt;- OC[sample(1:n,n,replace=TRUE),]
+   OCt_eigen &lt;- eigen(cov(OCt))
+   thetaB[i] &lt;- max(OCt_eigen$values)/sum(OCt_eigen$values)
+ }
&gt; for(j in 2:B){
+   thetaB[j] &lt;- mean(thetaB[1:j])
+   sethetaB[j] &lt;- sd(thetaB[1:j])
+ }
&gt; plot.ts(sethetaB,xlab="Number of Bootstrap Samples",
+         ylab="Bootstrap Standard Error for First Principal Component")</pre></div><div class="mediaobject"><img src="../images/00142.jpeg" alt="Eigen values" class="calibre10"/><div class="caption"><p class="calibre14">Figure 5: Bootstrap standard error of the variance explained by the first principal component</p></div></div><p class="calibre11"> </p><p class="calibre7">The 95% bootstrap confidence interval is <a id="id119" class="calibre1"/>obtained in the usual way:</p><div class="informalexample"><pre class="programlisting">&gt; TB &lt;- seq(50,500,50)
&gt; for(i in 1:length(TB)) print(quantile(thetaB[1:TB[i]],c(0.025,0.975)))
     2.5%     97.5% 
0.6300403 0.6478871 
     2.5%     97.5% 
0.6330791 0.6424721 
     2.5%     97.5% 
0.6342183 0.6401195 
     2.5%     97.5% 
0.6348247 0.6394432 
     2.5%     97.5% 
0.6348774 0.6392892 
     2.5%     97.5% 
0.6352836 0.6391456 
     2.5%     97.5% 
0.6357643 0.6390937 
     2.5%     97.5% 
0.6360647 0.6388585 
     2.5%     97.5% 
0.6360818 0.6387047 
     2.5%     97.5% 
0.6361244 0.6386785 </pre></div><div class="book" title="Rule of thumb"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch02lvl3sec06" class="calibre1"/>Rule of thumb</h3></div></div></div><p class="calibre7">Generally, the number of B = 25 bootstrap<a id="id120" class="calibre1"/> replications is enough and one rarely requires more than <code class="literal">200</code> replications. For more information on this, see Efron and Tibshirani (1990, p.52).</p><p class="calibre7">Thus far we have used simulation, resampling, and loops to carry out the bootstrap inference. However, earlier in the chapter we mentioned the <code class="literal">boot</code> package. In the following section, we will use the package for some samples and illustrate its use.</p></div></div></div>
<div class="book" title="The boot package" id="K0RQ1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec19" class="calibre1"/>The boot package</h1></div></div></div><p class="calibre7">The <code class="literal">boot</code> package is one of the core R<a id="id121" class="calibre1"/> packages, and it is optimized for the implementation of bootstrap methods. In the previous examples, we mostly used loops for carrying out the resampling technique. Here, we will look at how to use the <code class="literal">boot</code> R package.</p><p class="calibre7">The main structure of the boot function is as follows:</p><div class="informalexample"><pre class="programlisting">boot(data, statistic, R, sim = "ordinary", stype = c("i", "f", "w"), 
     strata = rep(1,n), L = NULL, m = 0, weights = NULL, 
     ran.gen = function(d, p) d, mle = NULL, simple = FALSE, ...,
     parallel = c("no", "multicore", "snow"),
     ncpus = getOption("boot.ncpus", 1L), cl = NULL)</pre></div><p class="calibre7">The central arguments of the function are <code class="literal">data</code>, <code class="literal">statistic</code>, <code class="literal">R</code>, and <code class="literal">stype</code>. The <code class="literal">data</code> argument is the standard one, as with most R functions. The <code class="literal">statistic</code> is the most important argument for the implementation of the <code class="literal">boot</code> function and it is this function that will be applied on the bootstrap samples obtained from the <code class="literal">data</code> frame. The argument <code class="literal">R</code> (and not the software) is used to specify the number of bootstrap samples to be drawn, and <code class="literal">stype</code> will indicate the second argument of <code class="literal">statistic</code>. For any inference to be completed using the <code class="literal">boot</code> function, the critical task is to define the function for the statistic. We will continue the illustration using earlier examples.</p><p class="calibre7">In the study of correlation between the <code class="literal">LSAT</code> and <code class="literal">GPA</code> variables, the trick is to define the function that will include the correlation coefficient function and the data with index specified in a manner that will give us the bootstrap sample. After declaring the function for the computation of the correlation coefficient for the bootstrap sample, we use the boot function, introduce the function, and specify the resampling type as well as the number of required bootstrap samples. The <code class="literal">boot</code> function will be in action now:</p><div class="informalexample"><pre class="programlisting">&gt; corx &lt;- function(data,i) cor(data[i,1],data[i,2])
&gt; corboot &lt;- boot(data=LS[,2:3],statistic=corx,R=200,stype="i")
&gt; corboot
ORDINARY NONPARAMETRIC BOOTSTRAP
Call:
boot(data = LS[, 2:3], statistic = corx, R = 200, stype = "i")
Bootstrap Statistics :
     original      bias    std. error
t1* 0.7763745 -0.01791293   0.1357282</pre></div><p class="calibre7">The correlation function is defined through <code class="literal">corx</code>, and the boot function is applied on it with the data frame <code class="literal">LS</code>. The number of bootstrap samples is <code class="literal">200</code> and the resampling will occur until the next iteration. From the preceding output we can obtain the value of the statistic as <code class="literal">0.7763745</code>, the bias as <code class="literal">-0.01791293</code>, and the <a id="id122" class="calibre1"/>bootstrap standard error as <code class="literal">0.1357282</code>. But what about bias? We have made almost no mention of bias in our discussion thus far. To understand what the bootstrap bias is, we will first look at the components of the fitted <code class="literal">corboot boot</code> object. The value of the statistic, correlation coefficient here, is stored as <code class="literal">t0</code>, the bootstrap sample estimates (<code class="literal">R</code> of them) in <code class="literal">t</code>, and using these two quantities we will find the bias:</p><div class="informalexample"><pre class="programlisting">&gt; corboot$t0
[1] 0.7763745
&gt; corboot$t
            [,1]
  [1,] 0.8094277
  [2,] 0.7251170
  [3,] 0.7867994
  [4,] 0.7253745
  [5,] 0.7891611

[196,] 0.9269368
[197,] 0.8558334
[198,] 0.4568741
[199,] 0.6756813
[200,] 0.7536155
&gt; mean(corboot$t)-corboot$t0
[1] -0.01791293</pre></div><p class="calibre7">We can see how useful the <code class="literal">boot</code> function is for the applications. The <code class="literal">confint</code> function can be slapped on the <code class="literal">corboot</code> object to obtain the bootstrap confidence interval:</p><div class="informalexample"><pre class="programlisting">&gt; confint(corboot)
Bootstrap quantiles, type =  bca 
      2.5 %    97.5 %
1 0.3294379 0.9441656</pre></div><p class="calibre7">Next, we will apply the <code class="literal">boot</code> function for the problem of obtaining the confidence interval of the variation explained by the first principal component. To that end, we first create the necessary <code class="literal">R</code> function that can be supplied to the boot function:</p><div class="informalexample"><pre class="programlisting">&gt; Eigen_fn &lt;- function(data,i)  {
+  eig &lt;- eigen(cov(data[i,]))
+  val &lt;- max(eig$values)/sum(eig$values)
+  val
+ }
&gt; eigenboot &lt;- boot(data=OC,statistic = Eigen_fn,R=200,stype = "i")
&gt; eigenboot
ORDINARY NONPARAMETRIC BOOTSTRAP
Call:
boot(data = OC, statistic = Eigen_fn, R = 200, stype = "i")
Bootstrap Statistics :
    original        bias    std. error
t1* 0.619115 -0.0002657842   0.0488226
&gt; confint(eigenboot)
Bootstrap quantiles, type =  bca 
      2.5 %    97.5 %
1 0.5242984 0.7130783</pre></div><p class="calibre7">Thus, the boot package can be effectively used without<a id="id123" class="calibre1"/> having the need to write loops. We have used the bootstrap method for the main purpose of estimating the parameters and their functions. Hypothesis testing based on bootstrap will be covered next.</p></div>
<div class="book" title="Bootstrap and testing hypotheses" id="KVCC1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec20" class="calibre1"/>Bootstrap and testing hypotheses</h1></div></div></div><p class="calibre7">We begin the bootstrap hypothesis testing<a id="id124" class="calibre1"/> problems with the t-test to compare means and the F-test to compare variances. It is understood that, since we are assuming normal distribution for the two populations under comparison, the distributional properties of the test statistics are well known. To carry out the nonparametric bootstrap for the t-statistic based on the t-test, we first define the function, and then run the bootstrap function boot on the Galton dataset. The Galton dataset is available in the <code class="literal">galton data.frame</code> from the <code class="literal">RSADBE</code> package. The <code class="literal">galton</code> dataset consists of <code class="literal">928</code> pairs of observations, with the pair consisting of the height of the parent and the height of their child. First, we define the <code class="literal">t2</code> function, load the Galton dataset, and run the boot function as the following unfolds:</p><div class="informalexample"><pre class="programlisting">&gt; t2 &lt;- function(data,i) {
+   p &lt;- t.test(data[i,1],data[i,2],var.equal=TRUE)$statistic
+   p
+ }
&gt; data(galton)
&gt; gt &lt;- boot(galton,t2,R=100)
&gt; gt
ORDINARY NONPARAMETRIC BOOTSTRAP
Call:
boot(data = galton, statistic = t2, R = 100)
Bootstrap Statistics :
     original     bias    std. error
t1* -2.167665 0.03612774   0.6558595
&gt; confint(gt)
Bootstrap quantiles, type =  percent 
      2.5 %     97.5 %
1 -3.286426 -0.5866314
Warning message:
In confint.boot(gt) :
  BCa method fails for this problem.  Using 'perc' instead
&gt; t.test(galton[,1],galton[,2],var.equal=TRUE)
	Two Sample t-test
data:  galton[, 1] and galton[, 2]
t = -2.1677, df = 1854, p-value = 0.03031
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.41851632 -0.02092334
sample estimates:
mean of x mean of y 
 68.08847  68.30819 </pre></div><p class="calibre7">The reader should compare the bootstrap <a id="id125" class="calibre1"/>confidence interval and the confidence interval given by the t-statistic.</p><p class="calibre7">Next, we will carry out the bootstrap hypothesis testing for the variances. The variance function is defined for the <code class="literal">var.test</code> function and it will then be used in the <code class="literal">boot</code> function:</p><div class="informalexample"><pre class="programlisting">&gt; v2 &lt;- function(data,i) {
+   v &lt;- var.test(data[i,1],data[i,2])$statistic
+   v
+ }
&gt; gv &lt;- boot(galton,v2,R=100)
&gt; gv
ORDINARY NONPARAMETRIC BOOTSTRAP
Call:
boot(data = galton, statistic = v2, R = 100)
Bootstrap Statistics :
    original       bias    std. error
t1* 1.984632 -0.002454309   0.1052697
&gt; confint(gv)
Bootstrap quantiles, type =  percent 
     2.5 %   97.5 %
1 1.773178 2.254586
Warning message:
In confint.boot(gv) :
  BCa method fails for this problem.  Using 'perc' instead
&gt; var.test(galton[,1],galton[,2])
	F test to compare two variances
data:  galton[, 1] and galton[, 2]
F = 1.9846, num df = 927, denom df = 927, p-value &lt; 2.2e-16
alternative hypothesis: true ratio of variances is not equal to 1
95 percent confidence interval:
 1.744743 2.257505
sample estimates:
ratio of variances 
          1.984632 </pre></div><p class="calibre7">The confidence interval and the bootstrap<a id="id126" class="calibre1"/> confidence interval can be compared by the reader. The bootstrap methods have been demonstrated for different estimation and hypothesis testing scenarios. In the remaining sections, we will consider some regression models where we have additional information on the observations in terms of the explanatory variables.</p></div>
<div class="book" title="Bootstrapping regression models"><div class="book" id="LTSU2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec21" class="calibre1"/>Bootstrapping regression models</h1></div></div></div><p class="calibre7">The <code class="literal">US Crime</code> <a id="id127" class="calibre1"/>dataset introduced in <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, is an example of why the linear regression model might be a good fit. In this example, we are interested in understanding the crime rate (R) as a function of thirteen related variables such as average age, the southern state indicator, and so on. Mathematically, the linear regression model is as follows:</p><div class="mediaobject"><img src="../images/00143.jpeg" alt="Bootstrapping regression models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Here, <span class="strong"><img src="../images/00144.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span> are the p-covariates, <span class="strong"><img src="../images/00145.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span> is the intercept term, <span class="strong"><img src="../images/00146.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span> are the regression coefficients, and <span class="strong"><img src="../images/00147.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span> is the error term assumed to follow a normal<a id="id128" class="calibre1"/> distribution <span class="strong"><img src="../images/00148.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>. The covariates can be written in a vector form and the <span class="strong"><em class="calibre9">ith</em></span> observation can be summarized as <span class="strong"><img src="../images/00149.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>, where <span class="strong"><img src="../images/00150.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>. The <span class="strong"><em class="calibre9">n</em></span> observations <span class="strong"><img src="../images/00151.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>, are assumed to be stochastically independent. The linear regression model has been detailed in many classical regression books; see Draper and Smith (1999), for instance. A recent book that details the implementation of the linear regression model in R is Ciaburro (2018). As the reader might have guessed, we will now fit a linear regression model to the US Crime dataset to kick off the discussion:</p><div class="informalexample"><pre class="programlisting">&gt; data(usc)
&gt; usc_Formula &lt;- as.formula("R~.")
&gt; usc_lm &lt;- lm(usc_Formula,usc)
&gt; summary(usc_lm)
Call:
lm(formula = usc_Formula, data = usc)
Residuals:
    Min      1Q  Median      3Q     Max 
-34.884 -11.923  -1.135  13.495  50.560 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.918e+02  1.559e+02  -4.438 9.56e-05 ***
Age          1.040e+00  4.227e-01   2.460  0.01931 *  
S           -8.308e+00  1.491e+01  -0.557  0.58117    
Ed           1.802e+00  6.496e-01   2.773  0.00906 ** 
Ex0          1.608e+00  1.059e+00   1.519  0.13836    
Ex1         -6.673e-01  1.149e+00  -0.581  0.56529    
LF          -4.103e-02  1.535e-01  -0.267  0.79087    
M            1.648e-01  2.099e-01   0.785  0.43806    
N           -4.128e-02  1.295e-01  -0.319  0.75196    
NW           7.175e-03  6.387e-02   0.112  0.91124    
U1          -6.017e-01  4.372e-01  -1.376  0.17798    
U2           1.792e+00  8.561e-01   2.093  0.04407 *  
W            1.374e-01  1.058e-01   1.298  0.20332    
X            7.929e-01  2.351e-01   3.373  0.00191 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 21.94 on 33 degrees of freedom
Multiple R-squared:  0.7692,	Adjusted R-squared:  0.6783 
F-statistic: 8.462 on 13 and 33 DF,  p-value: 3.686e-07</pre></div><p class="calibre7">It can be seen from the <code class="literal">summary</code> output that a lot<a id="id129" class="calibre1"/> of information is displayed about the fitted linear regression model. From the output, we can find the estimated regression coefficients in <code class="literal">Estimate</code>. The standard error of these estimators is in <code class="literal">Std</code>. <code class="literal">Error</code>, the corresponding value of the t-statistic in <code class="literal">t value</code>, and the p-values in <code class="literal">Pr(&gt;|t|)</code>. We can further estimate the residual standard deviation <span class="strong"><img src="../images/00152.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span> in <code class="literal">Residual standard error</code>. Similarly, we can obtain the respective multiple and adjusted R-square values in <code class="literal">Multiple R-squared</code> and <code class="literal">Adjusted R-squared</code>, the overall F-statistic in <code class="literal">F-statistic</code>, and finally, the model p-value in <code class="literal">p-value</code>. Many<a id="id130" class="calibre1"/> of these statistics/quantities/summaries have clean statistical properties and as such, exact statistical inference regarding the parameters can be carried out. However, this is not the case for a few of them. For instance, if one asks for a confidence interval of the adjusted R-square value, the author is not able to recollect the corresponding statistical distribution. Hence, using the convenience of the bootstrap technique, we can obtain the bootstrap confidence interval for Adjusted R-square. The reason the confidence interval of the Adjusted R-square might be sought is that it has a very good interpretation of explaining the variance explained in the Y's by the model. Let us look at its implementation in the R software.</p><p class="calibre7">With complex problems there will be many solutions, and none with a proven advantage over the other. Nevertheless, we have two main ways of carrying out the bootstrap for the linear regression model: (i) bootstrapping the residuals, and (ii) bootstrapping the observations. The two methods can work for any general regression scenario too. Before we describe the two methods, let <span class="strong"><img src="../images/00153.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span> denote the least squares estimated of <span class="strong"><img src="../images/00154.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>, and the fitted model will be as follows:</p><div class="mediaobject"><img src="../images/00155.jpeg" alt="Bootstrapping regression models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Consequently, we will also have an estimate of the variance term of the error distribution and will denote it by <span class="strong"><img src="../images/00156.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>. Define the vector of residual by <span class="strong"><img src="../images/00157.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>. The <span class="strong"><strong class="calibre8">residual bootstrapping</strong></span> method is then<a id="id131" class="calibre1"/> carried out in the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">
Draw a sample of size <span class="strong"><em class="calibre9">n</em></span> with replacement from <span class="strong"><img src="../images/00158.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span> and denote it by <span class="strong"><img src="../images/00159.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>.
</li><li class="listitem" value="2">
For the resampled <span class="strong"><img src="../images/00159.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>, obtain the new regressands using <span class="strong"><img src="../images/00160.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>. That is, <span class="strong"><img src="../images/00161.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span> is the (first) bootstrap sample <span class="strong"><strong class="calibre8">Y</strong></span> value.
</li><li class="listitem" value="3">
Using <span class="strong"><img src="../images/00162.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span> and the covariate matrix <span class="strong"><img src="../images/00163.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>, obtain the first bootstrap estimate of the regression coefficient vector <span class="strong"><img src="../images/00164.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span>.
</li><li class="listitem" value="4">Repeat the process a large number of times, say <span class="strong"><em class="calibre9">B</em></span>.</li></ol><div class="calibre13"/></div><p class="calibre7">Bootstrapping the observations is the usual bootstrapping<a id="id132" class="calibre1"/> method, which does not require any further explanation. However, the rank of <span class="strong"><img src="../images/00165.jpeg" alt="Bootstrapping regression models" class="calibre15"/></span> might be affected, especially if a covariate is a discrete variable and only one factor is chosen. Hence, for any regression problem, bootstrapping the residual is the best approach.</p><p class="calibre7">The regular <code class="literal">boot</code> package won't be useful, and we will instead use the <code class="literal">Boot</code> function from the <code class="literal">car</code> package to perform the bootstrap analysis on the linear regression model. The <code class="literal">Boot</code> function will also be required to be a specified function whose output will give the value of the required statistic. Consequently, we will first define a function <code class="literal">f</code>, which will return the adjusted R-square value:</p><div class="informalexample"><pre class="programlisting">&gt; f &lt;- function(obj) summary(obj)$adj.r.squared
&gt; usc_boot &lt;- Boot(usc_lm,f=f,R=100)
&gt; summary(usc_boot)
     R original bootBias   bootSE bootMed
V1 100  0.67833 0.096618 0.089858 0.79162
&gt; confint(usc_boot)
Bootstrap quantiles, type =  bca 

       2.5 %    97.5 %
V1 0.5244243 0.7639986
Warning message:
In norm.inter(t, adj.alpha) : extreme order statistics used as endpoints</pre></div><p class="calibre7">Thus, a 95% bootstrap confidence<a id="id133" class="calibre1"/> interval for the adjusted R-square is <code class="literal">(0.5244243, 0.7639986)</code>. Similarly, inference related to any other parameter of the linear regression model can be carried out using the bootstrap technique.</p></div>
<div class="book" title="Bootstrapping survival models*" id="MSDG1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec22" class="calibre1"/>Bootstrapping survival models*</h1></div></div></div><p class="calibre7">In the first section, we looked at the role of pseudovalues in carrying out inference related to survival data. The main idea behind the use of pseudovalues is to replace the incomplete observations with an appropriate (expected) value and then use the flexible framework of the generalized estimating equation. Survival analysis and the related specialized methods for it will be detailed in <a class="calibre1" title="Chapter 10. Ensembling Survival Models" href="part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee">Chapter 10</a>, <span class="strong"><em class="calibre9">Ensembling Survival Models</em></span>, of the book. We will briefly introduce the notation here as required to set up the parameters. Let <span class="strong"><em class="calibre9">T </em></span>denote the survival time, or the time to the event of interest, and we naturally have <span class="strong"><img src="../images/00166.jpeg" alt="Bootstrapping survival models*" class="calibre15"/></span>, which is a continuous random variable. Suppose that the lifetime<a id="id134" class="calibre1"/> cumulative distribution is F and the associated density function is <span class="strong"><em class="calibre9">f</em></span>. Since the lifetimes <span class="strong"><em class="calibre9">T</em></span> are incomplete for some of the observations and subject to censoring, we will not be able to properly infer about interesting parameters such as mean survival time or median survival time. Since there are additional complications because of censoring, it suffices to note here that we will be borrowing heavily from the material in <a class="calibre1" title="Chapter 10. Ensembling Survival Models" href="part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee">Chapter 10</a>, <span class="strong"><em class="calibre9">Ensembling Survival Models</em></span>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip02" class="calibre1"/>Tip</h3><p class="calibre7">* Asterisked sections can be omitted on the first reading, or you can continue if you are already familiar with the related concepts and terminologies.</p></div><p class="calibre7">The <code class="literal">censboot</code> function from the <code class="literal">boot</code> package is developed to handle survival data. In the <code class="literal">pbc</code> dataset, the time to event of interest is the variable named <code class="literal">time </code>and the completeness of the observation is indicated by <code class="literal">status==2</code>. The package <code class="literal">survival</code> is required to create the <code class="literal">Surv</code> objects tenable to handle the survival data. The <code class="literal">survfit</code> function would then give us an estimate of the survival function, which is the complement of the cumulative distribution function 1-F. It is well known that the mean of a continuous non-negative random variable is <span class="strong"><img src="../images/00167.jpeg" alt="Bootstrapping survival models*" class="calibre15"/></span>, and that the median survival time is that time point u which satisfies the condition of <span class="strong"><img src="../images/00168.jpeg" alt="Bootstrapping survival models*" class="calibre15"/></span>. Since the <code class="literal">summary</code> of the <code class="literal">survfit</code> object can be used to obtain the survival probabilities at the desired time, we will use it to find the median survival time. All these arguments are built in the <code class="literal">Med_Surv</code> function, which will return the median survival time.</p><p class="calibre7">Using the <code class="literal">Med_Surv</code> function as the formula/statistic for the <code class="literal">censboot</code> function, we will be able to obtain the bootstrap estimates of the median survival time; subsequently, using the<a id="id135" class="calibre1"/> bootstrap estimates, we obtain the confidence interval for the median survival time. The R program and the output are as follows:</p><div class="informalexample"><pre class="programlisting">&gt; Med_Surv &lt;- function(data){
+   s2 &lt;- survfit(Surv(time,status==2)~1,data=data)
+   s2s &lt;- summary(s2)
+   s2median &lt;- s2s$time[which.min(s2s$surv&gt;0.5)]
+   s2median
+ }
&gt; pbc2 &lt;- pbc[,2:3]
&gt; pbc_median_boot &lt;- censboot(data=pbc2,statistic=Med_Surv,R=100)
&gt; pbc_median_boot
CASE RESAMPLING BOOTSTRAP FOR CENSORED DATA
Call:
censboot(data = pbc2, statistic = Med_Surv, R = 100)

Bootstrap Statistics :
    original  bias    std. error
t1*     3395   21.36    198.2795
&gt; pbc_median_boot$t
       [,1]
  [1,] 3282
  [2,] 3358
  [3,] 3574
  [4,] 3358
  [5,] 3244

 [96,] 3222
 [97,] 3445
 [98,] 3222
 [99,] 3282
[100,] 3222
&gt; confint(pbc_median_boot)
Bootstrap quantiles, type =  percent 
  2.5 % 97.5 %
1  3090   3853
Warning message:
In confint.boot(pbc_median_boot) :
  BCa method fails for this problem.  Using 'perc' instead</pre></div><p class="calibre7">For the actual data, the estimated median survival time is <code class="literal">3395</code> days. The 95% bootstrap confidence interval for the median survival time is <code class="literal">(3090, 3853)</code>.</p><p class="calibre7">To carry out the inference about the<a id="id136" class="calibre1"/> mean survival time, we need to use the <code class="literal">survmean</code> function from the <code class="literal">survival</code> package and appropriately extract the estimated mean survival time. The <code class="literal">Mean_Surv</code> function delivers this task. The R program and its output are given here:</p><div class="informalexample"><pre class="programlisting">&gt; Mean_Surv &lt;- function(data,time){
+   s2 &lt;- survfit(Surv(time,status==2)~1,data=data)
+   smean &lt;- as.numeric(
+     survival:::survmean(s2,rmean=time)[[1]]["*rmean"])
+   smean
+ }
&gt; censboot(data=pbc2,time=2000,statistic=Mean_Surv,R=100)
CASE RESAMPLING BOOTSTRAP FOR CENSORED DATA
Call:
censboot(data = pbc2, statistic = Mean_Surv, R = 100, time = 2000)
Bootstrap Statistics :
    original    bias    std. error
t1* 1659.415 -3.582645    25.87415</pre></div><p class="calibre7">The reader is left with the task of obtaining the bootstrap confidence interval for the mean survival time. The following section will discuss using the bootstrap method for time series data.</p></div>
<div class="book" title="Bootstrapping time series models*" id="NQU21-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec23" class="calibre1"/>Bootstrapping time series models*</h1></div></div></div><p class="calibre7">An example of the time series data was seen in <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, in the <code class="literal">New Zealand Overseas</code> dataset. See <a class="calibre1" title="Chapter 10. Ensembling Survival Models" href="part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee">Chapter 10</a>, <span class="strong"><em class="calibre9">Ensembling Survival Models</em></span>, of Tattar et al. (2016). Time series is distinctive in that the observations are not stochastically independent of each other. For <a id="id137" class="calibre1"/>example, the maximum temperature of the day is very unlikely to be independent of the previous day's maximum temperature. However, we are likely to believe that the maximum temperature of a block of ten previous days is mostly independent of a ten-day block six months ago. Thus, the <code class="literal">bootstrap</code> method is modified to the <code class="literal">block bootstrap</code> method. The <code class="literal">tsboot</code> function from the <code class="literal">boot</code> package is useful to bootstrap time series data. The main structure of the <code class="literal">tsboot</code> function appears as follows:</p><div class="informalexample"><pre class="programlisting">tsboot(tseries, statistic, R, l = NULL, sim = "model",
       endcorr = TRUE, n.sim = NROW(tseries), orig.t = TRUE,
       ran.gen, ran.args = NULL, norm = TRUE, ...,
       parallel = c("no", "multicore", "snow"),
       ncpus = getOption("boot.ncpus", 1L), cl = NULL)</pre></div><p class="calibre7">Here, <code class="literal">statistic</code>, <code class="literal">tseries</code> is the time series data, which is the usual function of interest to us. <code class="literal">R</code> is the number of bootstrap replicates, and <code class="literal">l</code> is the length of the block, which we draw from the time series data. Now, we consider the problem of estimating the variance for an autoregressive (AR) time series model and we will consider a maximum order, <code class="literal">order.max</code>, of the AR model at 25. The <code class="literal">Var.fun</code> function will fit the best AR model and obtain the variance. This function will then be fed to the <code class="literal">tsboot</code> and, using the statistic calculated for each bootstrap sample, we will obtain the 95% bootstrap confidence interval:</p><div class="informalexample"><pre class="programlisting">&gt; Var.fun &lt;- function(ts) {
+   ar.fit &lt;- ar(ts, order.max = 25)
+   ar.fit$var
+ }
&gt; ?AirPassengers
&gt; AP_Boot &lt;- tsboot(AirPassengers,Var.fun,R=999,l=20,sim="fixed")
&gt; AP_Boot
BLOCK BOOTSTRAP FOR TIME SERIES
Fixed Block Length of 20 
Call:
tsboot(tseries = AirPassengers, statistic = Var.fun, R = 999, 
    l = 20, sim = "fixed")
Bootstrap Statistics :
    original   bias    std. error
t1* 906.1192 2080.571    1111.977
&gt; quantile(AP_Boot$t,c(0.025,0.975))
    2.5%    97.5% 
1216.130 5357.558 </pre></div><p class="calibre7">Consequently, we have been able to apply<a id="id138" class="calibre1"/> the bootstrap methods for the time series data.</p></div>
<div class="book" title="Summary" id="OPEK1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec24" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">The main purpose of dealing with the bootstrap method in detail is that it lays the foundation for the resampling methods. We began the chapter with a very early resampling method: the jackknife method. This method is illustrated for the purpose of multiple scenarios, including survival data, which is inherently complex. The bootstrap method kicked off for seemingly simpler problems, and then we immediately applied it to complex problems, such as principal components and regression data. For the regression data, we also illustrated the bootstrap method for survival data and time series data. In the next chapter, we will look at the central role the bootstrap method plays in resampling decision trees, a quintessential machine learning tool.</p></div></body></html>