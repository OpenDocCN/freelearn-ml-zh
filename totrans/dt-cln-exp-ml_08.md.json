["```py\n    import pandas as pd\n    import numpy as np\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.neighbors import KNeighborsClassifier\n    import sklearn.metrics as skmet\n    import matplotlib.pyplot as plt\n    ```", "```py\n    nls97compba = pd.read_csv(\"data/nls97compba.csv\")\n    feature_cols = ['satverbal','satmath','gpaoverall',\n      'parentincome','gender']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97compba[feature_cols],\\\n      nls97compba[['completedba']], test_size=0.3, random_state=0)\n    ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n    ohe.fit(X_train)\n    X_train_enc, X_test_enc = \\\n      ohe.transform(X_train), ohe.transform(X_test)\n    scaler = StandardScaler()\n    standcols = X_train_enc.iloc[:,:-1].columns\n    scaler.fit(X_train_enc[standcols])\n    X_train_enc = \\\n      pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Female']])\n    X_test_enc = \\\n      pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n      columns=standcols, index=X_test_enc.index).\\\n      join(X_test_enc[['gender_Female']])\n    ```", "```py\n    knn = KNeighborsClassifier(n_neighbors = 5)\n    knn.fit(X_train_enc, y_train.values.ravel())\n    pred = knn.predict(X_test_enc)\n    ```", "```py\n    cm = skmet.confusion_matrix(y_test, pred, labels=knn.classes_)\n    cmplot = skmet.ConfusionMatrixDisplay(\n      confusion_matrix=cm, \n      display_labels=['Negative', 'Positive'])\n    cmplot.plot()\n    cmplot.ax_.set(title='Confusion Matrix', \n      xlabel='Predicted Value', ylabel='Actual Value')\n    ```", "```py\n    tn, fp, fn, tp = skmet.confusion_matrix(\n      y_test.values.ravel(), pred).ravel()\n    tn, fp, fn, tp\n    (53, 63, 31, 126)\n    ```", "```py\n    accuracy = (tp + tn) / pred.shape[0]\n    accuracy\n    0.6556776556776557\n    sensitivity = tp / (tp + fn)\n    sensitivity\n    0.802547770700637\n    specificity = tn / (tn+fp)\n    specificity\n    0.45689655172413796\n    precision = tp / (tp + fp)\n    precision\n    0.6666666666666666\n    ```", "```py\n    skmet.accuracy_score(y_test.values.ravel(), pred)\n    0.6556776556776557\n    skmet.recall_score(y_test.values.ravel(), pred)\n    0.802547770700637\n    skmet.precision_score(y_test.values.ravel(), pred)\n    0.6666666666666666\n    ```", "```py\n    rfc = RandomForestClassifier(n_estimators=100, \n      max_depth=2, n_jobs=-1, random_state=0)\n    rfc.fit(X_train_enc, y_train.values.ravel())\n    pred = rfc.predict(X_test_enc)\n    tn, fp, fn, tp = skmet.confusion_matrix(\n      y_test.values.ravel(), pred).ravel()\n    tn, fp, fn, tp\n    (49, 67, 17, 140)\n    accuracy = (tp + tn) / pred.shape[0]\n    accuracy\n    0.6923076923076923\n    sensitivity = tp / (tp + fn)\n    sensitivity\n    0.89171974522293\n    specificity = tn / (tn+fp)\n    specificity\n    0.4224137931034483\n    precision = tp / (tp + fp)\n    precision\n    0.6763285024154589\n    ```", "```py\nnumobs = 6\n```", "```py\ninclasscnt = 3\n```", "```py\nplt.yticks([1,2,3])\n```", "```py\nplt.plot([0, numobs], [0, inclasscnt], c = 'b', label = 'Random Model')\n```", "```py\nplt.plot([0, inclasscnt, numobs], [0, inclasscnt, inclasscnt], c = 'grey', linewidth = 2, label = 'Perfect Model')\n```", "```py\nplt.title(\"Cumulative Accuracy Profile\")\n```", "```py\nplt.xlabel(\"Total Cards\")\n```", "```py\nplt.ylabel(\"In-class (Red) Cards\")\n```", "```py\n    import pandas as pd\n    import numpy as np\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    import sklearn.metrics as skmet\n    import matplotlib.pyplot as plt\n    import seaborn as sb\n    ```", "```py\n    nls97compba = pd.read_csv(\"data/nls97compba.csv\")\n    feature_cols = ['satverbal','satmath','gpaoverall',\n      'parentincome','gender']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97compba[feature_cols],\\\n      nls97compba[['completedba']], test_size=0.3, random_state=0)\n    ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n    ohe.fit(X_train)\n    X_train_enc, X_test_enc = \\\n      ohe.transform(X_train), ohe.transform(X_test)\n    scaler = StandardScaler()\n    standcols = X_train_enc.iloc[:,:-1].columns\n    scaler.fit(X_train_enc[standcols])\n    X_train_enc = \\\n      pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Female']])\n    X_test_enc = \\\n      pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n      columns=standcols, index=X_test_enc.index).\\\n      join(X_test_enc[['gender_Female']])\n    ```", "```py\n    knn = KNeighborsClassifier(n_neighbors = 5)\n    rfc = RandomForestClassifier(n_estimators=100, max_depth=2, \n      n_jobs=-1, random_state=0)\n    ```", "```py\nnumobs = y_test.shape[0]\ninclasscnt = y_test.iloc[:,0].sum()\nplt.plot([0, numobs], [0, inclasscnt], c = 'b', label = 'Random Model')\nplt.plot([0, inclasscnt, numobs], [0, inclasscnt, inclasscnt], c = 'grey', linewidth = 2, label = 'Perfect Model')\nplt.axvline(numobs/2, color='black', linestyle='dashed', linewidth=1)\nplt.axhline(numobs/2, color='black', linestyle='dashed', linewidth=1)\nplt.title(\"Cumulative Accuracy Profile\")\nplt.xlabel(\"Total Observations\")\nplt.ylabel(\"In-class Observations\")\nplt.legend()\n```", "```py\ndef addplot(model, X, Xtest, y, modelname, linecolor):\n  model.fit(X, y.values.ravel())\n  probs = model.predict_proba(Xtest)[:, 1]\n\n  probdf = pd.DataFrame(zip(probs, y_test.values.ravel()),\n    columns=(['prob','inclass']))\n  probdf.loc[-1] = [0,0]\n  probdf = probdf.sort_values(['prob','inclass'],\n    ascending=False).\\\n    assign(inclasscum = lambda x: x.inclass.cumsum())\n  inclassmidpoint = \\\n    probdf.iloc[int(probdf.shape[0]/2)].inclasscum\n  plt.axhline(inclassmidpoint, color=linecolor,\n    linestyle='dashed', linewidth=1)\n  plt.plot(np.arange(0, probdf.shape[0]),\n    probdf.inclasscum, c = linecolor,\n    label = modelname, linewidth = 4)\n```", "```py\n    addplot(knn, X_train_enc, X_test_enc, y_train,\n      'KNN', 'red')\n    addplot(rfc, X_train_enc, X_test_enc, y_train,\n      'Random Forest', 'green')\n    plt.legend()\n    ```", "```py\n    rfc.fit(X_train_enc, y_train.values.ravel())\n    pred = rfc.predict(X_test_enc)\n    pred_probs = rfc.predict_proba(X_test_enc)[:, 1]\n    probdf = pd.DataFrame(zip(\n      pred_probs, pred, y_test.values.ravel()),\n      columns=(['prob','pred','actual']))\n    probdf.groupby(['pred'])['prob'].agg(['min','max'])\n                    min             max\n    pred             \n    0.000           0.305           0.500\n    1.000           0.502           0.883\n    ```", "```py\n    sb.kdeplot(probdf.loc[probdf.actual==1].prob, \n      shade=True, color='red',\n      label=\"Completed BA\")\n    sb.kdeplot(probdf.loc[probdf.actual==0].prob,  \n      shade=True, color='green',\n      label=\"Did Not Complete\")\n    plt.axvline(0.5, color='black', linestyle='dashed', linewidth=1)\n    plt.axvline(0.65, color='black', linestyle='dashed', linewidth=1)\n    plt.title(\"Predicted Probability Distribution\")\n    plt.legend(loc=\"upper left\")\n    ```", "```py\nfpr, tpr, ths = skmet.roc_curve(y_test, pred_probs)\nths = ths[1:]\nfpr = fpr[1:]\ntpr = tpr[1:]\nfig, ax = plt.subplots()\nax.plot(ths, fpr, label=\"False Positive Rate\")\nax.plot(ths, tpr, label=\"Sensitivity\")\nax.set_title('False Positive Rate and Sensitivity by Threshold')\nax.set_xlabel('Threshold')\nax.set_ylabel('False Positive Rate and Sensitivity')\nax.legend()\n```", "```py\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, linewidth=4, color=\"black\")\n    ax.set_title('ROC curve')\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('Sensitivity')\n    ```", "```py\n    tholdind = np.where((ths>0.499) & (ths<0.501))[0][0]\n    tholdindlow = np.where((ths>0.397) & (ths<0.404))[0][0]\n    tholdindhigh = np.where((ths>0.599) & (ths<0.601))[0][0]\n    plt.vlines((fpr[tholdindlow],fpr[tholdind],\n      fpr[tholdindhigh]), 0, 1, linestyles =\"dashed\", \n      colors =[\"green\",\"blue\",\"purple\"])\n    plt.hlines((tpr[tholdindlow],tpr[tholdind],\n      tpr[tholdindhigh]), 0, 1, linestyles =\"dashed\", \n      colors =[\"green\",\"blue\",\"purple\"])\n    ```", "```py\n    prec, sens, ths = skmet.precision_recall_curve(y_test, pred_probs)\n    prec = prec[1:-10]\n    sens = sens[1:-10]\n    ths  = ths[:-10]\n    fig, ax = plt.subplots()\n    ax.plot(ths, prec, label='Precision')\n    ax.plot(ths, sens, label='Sensitivity')\n    ax.set_title('Precision and Sensitivity by Threshold')\n    ax.set_xlabel('Threshold')\n    ax.set_ylabel('Precision and Sensitivity')\n    ax.set_xlim(0.3,0.9)\n    ax.legend()\n    ```", "```py\n    fig, ax = plt.subplots()\n    ax.plot(sens, prec)\n    ax.set_title('Precision-Sensitivity Curve')\n    ax.set_xlabel('Sensitivity')\n    ax.set_ylabel('Precision')\n    plt.yticks(np.arange(0.2, 0.9, 0.2))\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.neighbors import KNeighborsClassifier\n    import sklearn.metrics as skmet\n    import matplotlib.pyplot as plt\n    ```", "```py\n    nls97degreelevel = pd.read_csv(\"data/nls97degreelevel.csv\")\n    feature_cols = ['satverbal','satmath','gpaoverall',\n      'parentincome','gender']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97degreelevel[feature_cols],\\\n      nls97degreelevel[['degreelevel']], test_size=0.3, random_state=0)\n    ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n    ohe.fit(X_train)\n    X_train_enc, X_test_enc = \\\n      ohe.transform(X_train), ohe.transform(X_test)\n    scaler = StandardScaler()\n    standcols = X_train_enc.iloc[:,:-1].columns\n    scaler.fit(X_train_enc[standcols])\n    X_train_enc = \\\n      pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Female']])\n    X_test_enc = \\\n      pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n      columns=standcols, index=X_test_enc.index).\\\n      join(X_test_enc[['gender_Female']])\n    ```", "```py\n    knn = KNeighborsClassifier(n_neighbors = 5)\n    knn.fit(X_train_enc, y_train.values.ravel())\n    pred = knn.predict(X_test_enc)\n    pred_probs = knn.predict_proba(X_test_enc)[:, 1]\n    ```", "```py\n    cm = skmet.confusion_matrix(y_test, pred)\n    cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['High School', 'Bachelor','Post-Graduate'])\n    cmplot.plot()\n    cmplot.ax_.set(title='Confusion Matrix', \n      xlabel='Predicted Value', ylabel='Actual Value')\n    ```", "```py\n    print(skmet.classification_report(y_test, pred,\n      target_names=['High School', 'Bachelor', 'Post-Graduate']))\n                   precision    recall  f1-score   support\n      High School       0.51      0.67      0.58        72\n         Bachelor       0.51      0.49      0.50        92\n    Post-Graduate       0.42      0.24      0.30        42\n         accuracy                           0.50       206\n        macro avg       0.48      0.46      0.46       206\n     weighted avg       0.49      0.50      0.49       206\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.linear_model import LinearRegression\n    import sklearn.metrics as skmet\n    import matplotlib.pyplot as plt\n    landtemps = pd.read_csv(\"data/landtemps2019avgs.csv\")\n    feature_cols = ['latabs','elevation']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(landtemps[feature_cols],\\\n      landtemps[['avgtemp']], test_size=0.3, random_state=0)\n    ```", "```py\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = \\\n      pd.DataFrame(scaler.transform(X_train),\n      columns=feature_cols, index=X_train.index)\n    X_test = \\\n      pd.DataFrame(scaler.transform(X_test),\n      columns=feature_cols, index=X_test.index)\n    scaler.fit(y_train)\n    y_train, y_test = \\\n      pd.DataFrame(scaler.transform(y_train),\n      columns=['avgtemp'], index=y_train.index),\\\n      pd.DataFrame(scaler.transform(y_test),\n      columns=['avgtemp'], index=y_test.index)\n    ```", "```py\n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n    np.column_stack((lr.coef_.ravel(),\n      X_test.columns.values))\n    array([[-0.8538957537748768, 'latabs'],\n           [-0.3058979822791853, 'elevation']], dtype=object)\n    ```", "```py\n    pred = lr.predict(X_test)\n    preddf = pd.DataFrame(pred, columns=['prediction'],\n      index=X_test.index).join(X_test).join(y_test)\n    preddf['resid'] = preddf.avgtemp-preddf.prediction\n    preddf.resid.agg(['mean','median','skew','kurtosis'])\n    mean             -0.021\n    median           0.032\n    skew              -0.641\n    kurtosis        6.816\n    Name: resid, dtype: float64\n    ```", "```py\n    Plt.hist(preddf.resid, color=\"blue\")\n    plt.axvline(preddf.resid.mean(), color='red', linestyle='dashed', linewidth=1)\n    plt.title(\"Histogram of Residuals for Temperature Model\")\n    plt.xlabel(\"Residuals\")\n    plt.ylabel(\"Frequency\")\n    ```", "```py\n    plt.scatter(preddf.prediction, preddf.resid, color=\"blue\")\n    plt.axhline(0, color='red', linestyle='dashed', linewidth=1)\n    plt.title(\"Scatterplot of Predictions and Residuals\")\n    plt.xlabel(\"Predicted Temperature\")\n    plt.ylabel(\"Residuals\")\n    ```", "```py\n    mse = skmet.mean_squared_error(y_test, pred)\n    mse\n    0.18906346144036693\n    rmse = skmet.mean_squared_error(y_test, pred, squared=False)\n    rmse\n    0.4348142838504353\n    mae = skmet.mean_absolute_error(y_test, pred)\n    mae\n    0.318307379728143\n    r2 = skmet.r2_score(y_test, pred)\n    r2\n    0.8162525715296725\n    ```", "```py\n    knn = KNeighborsRegressor(n_neighbors=5)\n    knn.fit(X_train, y_train)\n    pred = knn.predict(X_test)\n    mae = skmet.mean_absolute_error(y_test, pred)\n    mae\n    0.2501829988751876\n    r2 = skmet.r2_score(y_test, pred)\n    r2\n    0.8631113217183314\n    ```", "```py\n    preddf = pd.DataFrame(pred, columns=['prediction'],\n      index=X_test.index).join(X_test).join(y_test)\n    preddf['resid'] = preddf.avgtemp-preddf.prediction\n    plt.scatter(preddf.prediction, preddf.resid, color=\"blue\")\n    plt.axhline(0, color='red', linestyle='dashed', linewidth=1)\n    plt.title(\"Scatterplot of Predictions and Residuals with KNN Model\")\n    plt.xlabel(\"Predicted Temperature\")\n    plt.ylabel(\"Residuals\")\n    plt.show()\n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import LinearRegression\n    from sklearn.neighbors import KNeighborsRegressor\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.pipeline import make_pipeline\n    from sklearn.model_selection import cross_validate\n    from sklearn.model_selection import KFold\n    ```", "```py\n    landtemps = pd.read_csv(\"data/landtemps2019avgs.csv\")\n    feature_cols = ['latabs','elevation']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(landtemps[feature_cols],\\\n      landtemps[['avgtemp']],test_size=0.1,random_state=0)\n    ```", "```py\n    kf = Kfold(n_splits=5, shuffle=True, random_state=0)\n    ```", "```py\n    def getscores(model):\n      pipeline = make_pipeline(StandardScaler(), model)\n      scores = cross_validate(pipeline, X=X_train, \n        y=y_train, cv=kf, scoring=['r2'], n_jobs=1)\n      scorelist.append(dict(model=str(model),\n        fit_time=scores['fit_time'].mean(),\n        r2=scores['test_r2'].mean()))\n    ```", "```py\n    scorelist = []\n    getscores(LinearRegression())\n    getscores(RandomForestRegressor(max_depth=2))\n    getscores(KNeighborsRegressor(n_neighbors=5))\n    ```", "```py\n    scorelist\n    [{'model': 'LinearRegression()',\n      'fit_time': 0.004968833923339844,\n      'r2': 0.8181125031214872},\n     {'model': 'RandomForestRegressor(max_depth=2)',\n      'fit_time': 0.28124608993530276,\n      'r2': 0.7122492698889024},\n     {'model': 'KNeighborsRegressor()',\n      'fit_time': 0.006945991516113281,\n      'r2': 0.8686733636724104}]\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import LinearRegression\n    from sklearn.impute import SimpleImputer\n    from sklearn.pipeline import make_pipeline\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.impute import KNNImputer\n    from sklearn.model_selection import cross_validate, KFold\n    import sklearn.metrics as skmet\n    from sklearn.compose import ColumnTransformer\n    from sklearn.compose import TransformedTargetRegressor\n    ```", "```py\n    import os\n    import sys\n    sys.path.append(os.getcwd() + \"/helperfunctions\")\n    from preprocfunc import OutlierTrans\n    ```", "```py\nclass OutlierTrans(BaseEstimator,TransformerMixin):\n  def __init__(self,threshold=1.5):\n    self.threshold = threshold\n\n  def fit(self,X,y=None):\n    return self\n\n  def transform(self,X,y=None):\n    Xnew = X.copy()\n    for col in Xnew.columns:\n      thirdq, firstq = Xnew[col].quantile(0.75),\\\n        Xnew[col].quantile(0.25)\n      inlierrange = self.threshold*(thirdq-firstq)\n      outlierhigh, outlierlow = inlierrange+thirdq,\\\n        firstq-inlierrange\n      Xnew.loc[(Xnew[col]>outlierhigh) | \\\n        (Xnew[col]<outlierlow),col] = np.nan\n    return Xnew.values                                 \n```", "```py\nnls97wages = pd.read_csv(\"data/nls97wagesb.csv\")\nnls97wages.set_index(\"personid\", inplace=True)\nnls97wages.dropna(subset=['wageincome'], inplace=True)\nnls97wages.loc[nls97wages.motherhighgrade==95,\n  'motherhighgrade'] = np.nan\nnls97wages.loc[nls97wages.fatherhighgrade==95,\n  'fatherhighgrade'] = np.nan\nnum_cols = ['gpascience','gpaenglish','gpamath','gpaoverall',\n  'motherhighgrade','fatherhighgrade','parentincome']\ncat_cols = ['gender']\nbin_cols = ['completedba']\ntarget = nls97wages[['wageincome']]\nfeatures = nls97wages[num_cols + cat_cols + bin_cols]\nX_train, X_test, y_train, y_test =  \\\n  train_test_split(features,\\\n  target, test_size=0.2, random_state=0)\n```", "```py\n    nls97wages[['wageincome'] + num_cols].agg(['count','min','median','max']).T\n                     count    min      median    max\n    wageincome       5,091    0        40,000    235,884\n    gpascience       3,521    0        284       424\n    gpaenglish       3,558    0        288       418\n    gpamath          3,549    0        280       419\n    gpaoverall       3,653    42       292       411\n    motherhighgrade  4,734    1        12        20\n    fatherhighgrade  4,173    1        12        29\n    parentincome     3,803   -48,100   40,045    246,474\n    ```", "```py\nstandtrans = make_pipeline(OutlierTrans(2),\n  StandardScaler())\ncattrans = make_pipeline(SimpleImputer(strategy=\"most_frequent\"),\n  OneHotEncoder(drop_last=True))\nbintrans = make_pipeline(SimpleImputer(strategy=\"most_frequent\"))\ncoltrans = ColumnTransformer(\n  transformers=[\n    (\"stand\", standtrans, num_cols),\n    (\"cat\", cattrans, ['gender']),\n    (\"bin\", bintrans, ['completedba'])\n  ]\n)\n```", "```py\nlr = LinearRegression()\npipe1 = make_pipeline(coltrans,\n  KNNImputer(n_neighbors=5), lr)\nttr=TransformedTargetRegressor(regressor=pipe1,\n  transformer=StandardScaler())\n```", "```py\n    kf = KFold(n_splits=10, shuffle=True, random_state=0)\n    scores = cross_validate(ttr, X=X_train, y=y_train,\n      cv=kf, scoring=('r2', 'neg_mean_absolute_error'),\n      n_jobs=1)\n    print(\"Mean Absolute Error: %.2f, R-squared: %.2f\" % \n      (scores['test_neg_mean_absolute_error'].mean(),\n      scores['test_r2'].mean()))\n    Mean Absolute Error: -23781.32, R-squared: 0.20\n    ```"]