<html><head></head><body>
		<div id="_idContainer047">
			<h1 id="_idParaDest-125" class="chapter-number"><a id="_idTextAnchor130"/>5</h1>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor131"/>Federated Learning Client-Side Implementation</h1>
			<p>The client-side <a id="_idIndexMarker402"/>modules of a <strong class="bold">federated learning</strong> (<strong class="bold">FL</strong>) system can be implemented based on the system architecture, sequence, and procedure flow, as discussed in <a href="B18369_03.xhtml#_idTextAnchor058"><em class="italic">Chapter 3</em></a>, <em class="italic">Workings of the Federated Learning System</em>. FL client-side <a id="_idIndexMarker403"/>functionalities can connect distributed <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) applications that conduct local training and testing with an aggregator, through a communications module embedded in the client-side libraries.</p>
			<p>In the example of using the FL client libraries in a local ML engine, the minimal engine package example will be discussed, with dummy ML models to understand the process of integration with the FL client libraries that are designed in this chapter. By following the example code about integration, you will understand how to actually enable the whole process related to the FL client side, as discussed in <a href="B18369_03.xhtml#_idTextAnchor058"><em class="italic">Chapter 3</em></a>, <em class="italic">Workings of the Federated Learning System</em>, while an analysis on what will happen with the minimal example will be discussed in <a href="B18369_06.xhtml#_idTextAnchor156"><em class="italic">Chapter 6</em></a>, <em class="italic">Running the Federated Learning System and Analyzing the Results</em>.</p>
			<p>In this chapter, an overview of the design and implementation principle of FL client-side functionalities used in local ML engines will be discussed. By going through this chapter, you will be able to code the FL client-side modules and libraries as well as distributed <a id="_idIndexMarker404"/>local ML engines, such as image classification with <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>).</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>An overview of FL client-side components</li>
				<li>Implementing FL client-side main functionalities</li>
				<li>Designing FL client libraries</li>
				<li>Local ML engine integration into an FL system</li>
				<li>An example of integrating image classification into an FL system</li>
			</ul>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor132"/>Technical requirements</h1>
			<p>All the code files introduced in this chapter can be found on GitHub (<a href="https://github.com/tie-set/simple-fl">https://github.com/tie-set/simple-fl</a>). </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can use the code files for personal or educational purposes. Please note that we will not support deployments for commercial use and will not be responsible for any errors, issues, or damages caused by using the code.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor133"/>An overview of FL client-side components</h1>
			<p>The architecture <a id="_idIndexMarker405"/>of an FL client as an agent was introduced in <a href="B18369_03.xhtml#_idTextAnchor058"><em class="italic">Chapter 3</em></a>, <em class="italic">Workings of the Federated Learning System</em>. Here, we will introduce code that realizes the basic functionalities of an FL client. The client side of software architecture is simplified here, where only the <strong class="source-inline">client.py</strong> file can be used in this example, together with supporting functions from the <strong class="source-inline">lib/util</strong> folder, as shown in <em class="italic">Figure 5.1</em>:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B18369_05_01.jpg" alt="Figure 5.1 – Python software components for an FL client as an agent&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Python software components for an FL client as an agent</p>
			<p>The following section gives a brief description of the Python files for an agent of the FL system.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor134"/>Distributed agent-side code</h2>
			<p>For the <a id="_idIndexMarker406"/>agent side, there is <a id="_idIndexMarker407"/>one main file, <strong class="source-inline">client.py</strong>, in the <strong class="source-inline">fl_main/agent</strong> directory that deals with most of the FL client-side functionalities.</p>
			<h3>FL client code (client.py)</h3>
			<p>The <strong class="source-inline">client.py</strong> file in the <strong class="source-inline">agent</strong> folder has functions to participate in an FL cycle, an ML <a id="_idIndexMarker408"/>model exchange framework with an aggregator, and <em class="italic">push</em> and <em class="italic">polling</em> mechanisms to communicate with the aggregator. The client’s functions can also serve as interfaces between the local ML application and the FL system itself, providing FL client-side libraries to the ML engine. This is the main code that connects locally trained ML models to the FL server and aggregator. You need to prepare a local ML application by yourself, and we will help you understand how to integrate your ML engine into an FL system using the FL client libraries, which is another main topic of this chapter.</p>
			<h3>lib/util code</h3>
			<p>An explanation <a id="_idIndexMarker409"/>of the supporting Python code (<strong class="source-inline">communication_handler.py</strong>, <strong class="source-inline">data_struc.py</strong>, <strong class="source-inline">helpers.py</strong>, <strong class="source-inline">messengers.py</strong>, and <strong class="source-inline">states.py</strong>) as internal libraries will be covered in <em class="italic">Appendix, Exploring Internal Libraries</em>.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor135"/>Configuration of an agent</h2>
			<p>The following <a id="_idIndexMarker410"/>is an example of client-side configuration parameters saved as <strong class="source-inline">config_agent.json</strong> in the code we are using:</p>
			<pre class="source-code">
{
    "aggr_ip": "localhost",
    "reg_socket": "8765",
    "model_path": "./data/agents",
    "local_model_file_name": "lms.binaryfile",
    "global_model_file_name": "gms.binaryfile",
    "state_file_name": "state",
    "init_weights_flag": 1,
    "polling": 1
}</pre>
			<p>The aggregator’s IP (<strong class="source-inline">aggr_ip</strong>) and its port number (<strong class="source-inline">reg_socket</strong>) are used to get connected to the FL server, where the aggregation of the local models happens. In addition, the model path parameter, <strong class="source-inline">model_path</strong>, specifies the location of both the local model (named <strong class="source-inline">local_model_file_name</strong>) and the global model (named <strong class="source-inline">global_model_file_name</strong>). The local and global models are stored as binary files (<strong class="source-inline">lms.binaryfile</strong> and <strong class="source-inline">gms.binaryfile</strong> in this example). The state file (named <strong class="source-inline">state_file_name</strong>) writes the local state of the client that defines waiting for the global models, training the models, sending the trained models, and so on. <strong class="source-inline">init_weights_flag</strong> is used when the system operator wants to initialize the global model <a id="_idIndexMarker411"/>with certain weights. If the flag is <strong class="source-inline">1</strong>, the agent will send the pre-configured model; otherwise, the model will be filled with zeros on the aggregator side. The polling flag (<strong class="source-inline">polling</strong>) concerns whether to utilize the polling method or not for communication between agents and an aggregator.</p>
			<p>Now that we’ve discussed FL client-side modules, let’s look into the actual implementation and some code to realize the functionalities of an FL client.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor136"/>Implementing FL client-side main functionalities</h1>
			<p>In this section, we will explain how you can implement basic FL client-side code, which is <a id="_idIndexMarker412"/>described in the <strong class="source-inline">client.py</strong> file in the <strong class="source-inline">agent</strong> directory. Please refer to <a href="B18369_03.xhtml#_idTextAnchor058"><em class="italic">Chapter 3</em></a>, <em class="italic">Workings of the Federated Learning System</em>, for an explanation of FL client-side architecture, sequence, and procedure flow. By learning about this client-side code, you will understand how to implement an agent’s registration process, model exchange synchronization, and <em class="italic">push</em>/<em class="italic">polling</em> mechanisms, as well as the communication protocol <a id="_idIndexMarker413"/>between the agent and aggregator, with some functions that will be called from other ML applications as <strong class="bold">Application Programming Interfaces</strong> (<strong class="bold">APIs</strong>).</p>
			<p>Let’s first see what libraries will be imported for implementing FL client functions.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor137"/>Importing libraries for an agent</h2>
			<p>In this <strong class="source-inline">client.py</strong> file <a id="_idIndexMarker414"/>example, the agent imports general libraries such as <strong class="source-inline">asyncio</strong> and <strong class="source-inline">time</strong> (a detailed explanation of which is out of scope for this book):</p>
			<pre class="source-code">
import asyncio, time, logging, sys, os
from typing import Dict, Any
from threading import Thread
from fl_main.lib.util.communication_handler import \
     init_client_server, send, receive
from fl_main.lib.util.helpers import read_config, \
     init_loop, save_model_file, load_model_file, \
     read_state, write_state, generate_id, \
     set_config_file, get_ip, compatible_data_dict_read, \
     generate_model_id, create_data_dict_from_models, \
     create_meta_data_dict
from fl_main.lib.util.states import ClientState, \
     AggMsgType, ParticipateConfirmationMSGLocation, \
     GMDistributionMsgLocation, IDPrefix
from fl_main.lib.util.messengers import \
     generate_lmodel_update_message, \
     generate_agent_participation_message, \
     generate_polling_message</pre>
			<p>As for the <strong class="source-inline">communication_handler</strong>, <strong class="source-inline">helpers</strong>, <strong class="source-inline">states</strong>, and <strong class="source-inline">messengers</strong> libraries imported from <strong class="source-inline">fl_main.lib.util</strong> that are designed for enabling the FL general functionalities, please refer to the <em class="italic">Appendix, Exploring Internal Libraries</em>.</p>
			<p>After importing the necessary libraries, you will define the <strong class="source-inline">Client</strong> class.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor138"/>Defining the Client class</h2>
			<p>Let’s <a id="_idIndexMarker415"/>define the <strong class="source-inline">Client</strong> class that implements the core functionalities of an FL client, including the participation mechanism of the agent itself, the model exchange framework, and a communication interface between the agent and an aggregator, as well as libraries provided for use in the agent-side local ML engine:</p>
			<pre class="source-code">
class Client:
    """
    Client class instance with FL client-side functions
    and libraries used in the agent's ML engine
    """</pre>
			<p>Then, you will initialize the <strong class="source-inline">Client</strong> class under the <strong class="source-inline">__init__</strong> function, as discussed in the next section.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor139"/>Initializing the client</h2>
			<p>The <a id="_idIndexMarker416"/>following code inside the <strong class="source-inline">__init__</strong> constructor is an example of the initialization process of the client:</p>
			<pre class="source-code">
def __init__(self):
    self.agent_name = 'default_agent'
    self.id = generate_id()
    self.agent_ip = get_ip()
    self.simulation_flag = False
    if len(sys.argv) &gt; 1:
        self.simulation_flag = bool(int(sys.argv[1]))
    config_file = set_config_file("agent")
    self.config = read_config(config_file)
    self.aggr_ip = self.config['aggr_ip']
    self.reg_socket = self.config['reg_socket']
    self.msend_socket = 0
    self.exch_socket = 0
    if self.simulation_flag:
        self.exch_socket = int(sys.argv[2])
        self.agent_name = sys.argv[3]
    self.model_path = f'{self.config["model_path"]}
                                        /{self.agent_name}'
    if not os.path.exists(self.model_path):
        os.makedirs(self.model_path)
    self.lmfile = self.config['local_model_file_name']
    self.gmfile = self.config['global_model_file_name']
    self.statefile = self.config['state_file_name']
    self.round = 0
    self.init_weights_flag = \
                     bool(self.config['init_weights_flag'])
    self.is_polling = bool(self.config['polling'])</pre>
			<p>First, the client generates a unique ID for itself as an identifier that will be used in many <a id="_idIndexMarker417"/>scenarios to conduct FL.</p>
			<p>Second, the client gets its own IP address by using the <strong class="source-inline">get_ip()</strong> function.</p>
			<p>Also, simulation runs are supported in this implementation exercise, where we can run all the FL system components of a database, server, and multiple agents within one machine. If simulation needs to be done, then the <strong class="source-inline">simulation_flag</strong> parameter needs to be <strong class="source-inline">True</strong> (refer to the <strong class="source-inline">README</strong> file on GitHub for how to set up a simulation mode).</p>
			<p>Then, <strong class="source-inline">self.cofig</strong> reads and stores the information of <strong class="source-inline">config_agent.json</strong>.</p>
			<p>The client then configures the aggregator’s information to connect to its server, where <strong class="source-inline">self.aggr_ip</strong> reads the IP address of the aggregator machine or instance from the agent configuration file.</p>
			<p>After that, the <strong class="source-inline">reg_socket</strong> port will be set up, where <strong class="source-inline">reg_socket</strong> is used for registration of the agent, together with an aggregator IP address stored as <strong class="source-inline">self.aggr_ip</strong>. The <strong class="source-inline">reg_socket</strong> value in this example can be read from the agent configuration file as well.</p>
			<p><strong class="source-inline">msend_socket</strong>, which is used in the model exchange routine to send the local ML models to the aggregator, will be configured after participating in the FL process by sending a message to the FL server and receiving the response.</p>
			<p><strong class="source-inline">exch_socket</strong> is used when communication is not in <em class="italic">polling</em> mode for receiving global models sent from the aggregator, together with an agent IP address stored as <strong class="source-inline">self.agent_ip</strong>.</p>
			<p><strong class="source-inline">exch_socket</strong> in this example can either be read from the arguments from the command line or decided by the aggregator, depending on the simulation mode.</p>
			<p>In this <a id="_idIndexMarker418"/>example, when the aggregator is set to be able to push messages to the connected agents, which is not the case when polling mode is on, <strong class="source-inline">exch_socket</strong> can be dynamically configured by the aggregator.</p>
			<p><strong class="source-inline">self.model_path</strong> stores the path to the local and global models and can either be read from the agent configuration file or arguments from the command line, depending on the simulation mode as well. If there is no directory to save those model files, it makes sure to create the directory.</p>
			<p><strong class="source-inline">self.lmfile</strong>, <strong class="source-inline">self.gmfile</strong>, and <strong class="source-inline">self.statefile</strong> are the filenames for local models, global models, and the state of the client respectively, and read from the configuration file of the agent. In particular, in <strong class="source-inline">self.statefile</strong>, the value of <strong class="source-inline">ClientState</strong> is saved. <strong class="source-inline">ClientState</strong> is the enumeration value of the client itself where there is a state waiting for the global model (<strong class="source-inline">waiting_gm</strong>), a state for local training (<strong class="source-inline">training</strong>), a state for sending local models (<strong class="source-inline">sending</strong>), and a state for having the updated global models (<strong class="source-inline">gm_ready</strong>).</p>
			<p>The round information of the FL process, defined as <strong class="source-inline">self.round</strong>, is initialized as <strong class="source-inline">0</strong> and later updated as the FL round proceeds with model aggregation, where the aggregator will notify the change of the round usually.</p>
			<p><strong class="source-inline">self.init_weights_flag</strong> is the flag used when a system operator wants to initialize a global model with certain parameters, as explained in the configuration of the agent.</p>
			<p>The <strong class="source-inline">self.is_polling</strong> flag concerns whether to use the polling method in communication between the agents and aggregator or not. The polling flag must be the <a id="_idIndexMarker419"/>same as the one set up on the aggregator side.</p>
			<p>The code about the <strong class="source-inline">__init__</strong> constructor discussed here can be found in <strong class="source-inline">client.py</strong> in the <strong class="source-inline">fl_main/agent</strong> folder on GitHub (<a href="https://github.com/tie-set/simple-fl">https://github.com/tie-set/simple-fl</a>).</p>
			<p>Now that we have discussed how to initialize a client-side module, in the next section, we will look into how the participation mechanism works with some sample code.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor140"/>Agent participation in an FL cycle</h2>
			<p>This participation or registration process is needed for an agent to be able to participate <a id="_idIndexMarker420"/>in an FL process together with other agents. Therefore, the agent needs to be added to the list of authorized agents that can send locally trained ML models to an aggregator.</p>
			<p>The asynchronous <strong class="source-inline">participate</strong> function sends the first message to an aggregator to join the FL cycle and will receive state and communication information, such as socket numbers from the aggregator.</p>
			<p>An agent knows the IP address and port number to join the FL platform through the <strong class="source-inline">config_agent.json</strong> file. When joining the FL platform, an agent sends a participation message that contains the following information:</p>
			<ul>
				<li><strong class="source-inline">agent_name</strong>: A unique name of an agent itself.</li>
				<li><strong class="source-inline">id</strong>: A unique identifier of an agent itself.</li>
				<li><strong class="source-inline">model_id</strong>: A unique identifier of models to be sent to an aggregator.</li>
				<li><strong class="source-inline">models</strong>: A dictionary of models keyed by model names. The weights of models need not be trained if <strong class="source-inline">init_flag</strong> is <strong class="source-inline">False</strong>, since it is only used by an aggregator to remember the shapes of models.</li>
				<li><strong class="source-inline">init_weights_flag</strong>: A Boolean flag to indicate whether the sent model weights should be used as a base model. If it is <strong class="source-inline">True</strong> and there are no global models ready, an aggregator sets this set of local models as the first global models and sends it to all agents.</li>
				<li><strong class="source-inline">simulation_flag</strong>: This is <strong class="source-inline">True</strong> if it is a simulation run; otherwise, it is <strong class="source-inline">False</strong>.</li>
				<li><strong class="source-inline">exch_socket</strong>: The port number waiting for global models from the aggregator.</li>
				<li><strong class="source-inline">gene_time</strong>: The time that models are generated.</li>
				<li><strong class="source-inline">performance_dict</strong>: Performance data related to models in a dictionary format.</li>
				<li><strong class="source-inline">agent_ip</strong>: The IP address of an agent itself.</li>
			</ul>
			<p>With all the aforementioned participation messages defined, the agent is ready to exchange <a id="_idIndexMarker421"/>models with the aggregator, and the code to realize the participation process is as follows:</p>
			<pre class="source-code">
async def participate(self):
    data_dict, performance_dict = \
       load_model_file(self.model_path, self.lmfile)
    _, gene_time, models, model_id = \
       compatible_data_dict_read(data_dict)
    msg = generate_agent_participation_message(
         self.agent_name, self.id, model_id, models,
         self.init_weights_flag, self.simulation_flag,
         self.exch_socket, gene_time, performance_dict,
         self.agent_ip)
    resp = await send(msg, self.aggr_ip, self.reg_socket)
    self.round = resp[ \
       int(ParticipateConfirmaMSGLocation.round)]
    self.exch_socket = resp[ \
       int(ParticipateConfirmationMSGLocation.exch_socket)]
    self.msend_socket = resp[ \
       int(ParticipateConfirmationMSGLocation.recv_socket)]
    self.id = resp[ \
       int(ParticipateConfirmationMSGLocation.agent_id)]
    self.save_model_from_message(resp, \
        ParticipateConfirmationMSGLocation)</pre>
			<p>The <a id="_idIndexMarker422"/>agent reads the local models to tell the structure of the ML models to the aggregator, and the initial model does not necessarily need to be trained. <strong class="source-inline">data_dict</strong> and <strong class="source-inline">performance_dict</strong> store the models and their performance data respectively.</p>
			<p>Then, a message, <strong class="source-inline">msg</strong>, containing information such as the ML <strong class="source-inline">models</strong> and its <strong class="source-inline">model_id</strong>, is packaged using the <strong class="source-inline">generate_agent_participation_message</strong> function.</p>
			<p>When sending the message, in this example, the WebSocket is constructed using the aggregator’s IP address (<strong class="source-inline">aggr_ip</strong>) and the registration port number (<strong class="source-inline">reg_socket</strong>) to be connected to the aggregator.</p>
			<p>After sending the message to the aggregator via an asynchronous <strong class="source-inline">send</strong> function imported from <strong class="source-inline">communication_handler</strong>, the agent receives a response message, <strong class="source-inline">resp</strong>, from the aggregator. The response will include the round info, the port number to receive the global models’ <strong class="source-inline">exch_socket</strong>, the port number to send the local models to the aggregator’s <strong class="source-inline">msend_socket</strong>, and an updated agent ID.</p>
			<p>Finally, the global model within the response message is saved locally by calling the <strong class="source-inline">save_model_from_message</strong> function.</p>
			<p>The participation mechanism of an agent has been explained. In the next section, we will learn about the framework of model exchange synchronization.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor141"/>Model exchange synchronization</h2>
			<p>Model <a id="_idIndexMarker423"/>exchange synchronization, as shown in the following code, is for checking the state of the agent and calling a proper function based on the state:</p>
			<pre class="source-code">
Async def model_exchange_routine(self):
    while True:
        await asyncio.sleep(5)
        state = read_state(self.model_path, self.statefile)
        if state == ClientState.sending:
            await self.send_models()
        elif state == ClientState.waiting_gm:
            if self.is_polling == True:
               await self.process_polling()
            else: pass
        elif state == ClientState.training: pass
        elif state == ClientState.gm_ready: pass
        else: pass</pre>
			<p>Basically, this process is always running while the client is alive, whereas the <strong class="source-inline">while</strong> loop is used periodically to check the client’s <strong class="source-inline">state</strong> and proceed with the next steps if necessary.</p>
			<p>In the <strong class="source-inline">while</strong> loop, after waiting a few seconds, it first checks the client state by the <strong class="source-inline">read_state</strong> function. The parameters in the <strong class="source-inline">read_state</strong> function are to locate the <strong class="source-inline">state</strong> file stored in the local environment.</p>
			<p>As mentioned, <strong class="source-inline">ClientState</strong> has the enumeration value of the client state itself, defining a state for sending local models (<strong class="source-inline">sending</strong>), a state waiting for the global model (<strong class="source-inline">waiting_sgm</strong>), a state for local training (<strong class="source-inline">training</strong>), and a state for receiving the updated global models (<strong class="source-inline">gm_ready</strong>).</p>
			<p>If the client is in the <strong class="source-inline">sending</strong> state (<strong class="source-inline">state == ClientState.sending</strong>), it means it is ready to send the locally trained model to the aggregator. Therefore, the agent calls the <strong class="source-inline">send_models</strong> function to send the locally trained ML model to the aggregator.</p>
			<p>When the state is <strong class="source-inline">waiting_gm</strong> (<strong class="source-inline">state == ClientState.waiting_gm</strong>), it either proceeds with <strong class="source-inline">process_polling</strong> to poll from the agent to the aggregator if polling mode is on, or just does nothing if polling mode is off.</p>
			<p>If the <a id="_idIndexMarker424"/>client is in the <strong class="source-inline">training</strong> state (<strong class="source-inline">state == ClientState.training</strong>), it means that the client is training the local model now and just waits for a few seconds, printing the training status if necessary. You can also add any procedure if needed.</p>
			<p>If the client is in the <strong class="source-inline">gm_ready</strong> state (<strong class="source-inline">state == ClientState.gm_ready</strong>), it means that the client received the global model. This state will be handled by a local ML application, and it does nothing but show the readiness of the global models.</p>
			<p>In the next section, we will talk about how the <em class="italic">push</em> and <em class="italic">polling</em> mechanisms can be implemented for an FL cycle.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor142"/>Push and polling implementation</h2>
			<p>Once an <a id="_idIndexMarker425"/>agent is initialized <a id="_idIndexMarker426"/>and confirmed for participation in an FL process, it starts waiting for the global models sent from an aggregator. There are two ways <a id="_idIndexMarker427"/>to receive <a id="_idIndexMarker428"/>global models from the aggregator: the <strong class="source-inline">push</strong> method <a id="_idIndexMarker429"/>and the <strong class="source-inline">polling</strong> method. Although the <strong class="bold">Secure Sockets Layer</strong> (<strong class="bold">SSL</strong>) or <strong class="bold">Transport Layer Security</strong> (<strong class="bold">TSL</strong>) frameworks are not <a id="_idIndexMarker430"/>implemented in FL client-side code here for simplification, it is recommended to support them to secure constant communication.</p>
			<p>Let’s look into the mechanism for each communication framework.</p>
			<h3>The push method from aggregator to agent</h3>
			<p>With the <strong class="source-inline">push</strong> method, the <a id="_idIndexMarker431"/>aggregator will push the message that includes global models to all the connected agents right after the global models are generated.</p>
			<p>The following code shows the <em class="italic">push</em> mechanism accepting and saving global models from the aggregator:</p>
			<pre class="source-code">
async def wait_models(self, websocket, path):
    gm_msg = await receive(websocket)
    self.save_model_from_message( \
        gm_msg, GMDistributionMsgLocation)</pre>
			<p>The <strong class="source-inline">wait_models</strong> asynchronous function accepts <strong class="source-inline">websocket</strong> as a parameter. When <a id="_idIndexMarker432"/>the aggregator sends a message to the agent, it receives the <strong class="source-inline">gm_msg</strong> message through <strong class="source-inline">await recieve(websocket)</strong> and saves the global models locally by calling the <strong class="source-inline">save_model_from_message</strong> function, as defined in the <em class="italic">Toward designing FL client libraries</em> section.</p>
			<h3>The polling method from agent to aggregator</h3>
			<p>With the <strong class="source-inline">polling</strong> method, an agent will keep asking (polling) an aggregator to see whether <a id="_idIndexMarker433"/>global models are already formed or not. Once it has been created and is ready to be sent to the connected agents, the polled message will be returned to the agent with the updated global models in the response.</p>
			<p>The following code about the <strong class="source-inline">process_polling</strong> asynchronous function illustrates the <strong class="source-inline">polling</strong> method:</p>
			<pre class="source-code">
async def process_polling(self):
    msg = generate_polling_message(self.round, self.id)
    resp = await send(msg, self.aggr_ip, self.msend_socket)
    if resp[int(PollingMSGLocation.msg_type)] \
                                      == AggMsgType.update:
        self.save_model_from_message(resp, \
            GMDistributionMsgLocation)
    else: pass</pre>
			<p>It first generates the polling message with the <strong class="source-inline">generate_polling_message</strong> function to be sent to the aggregator. After receiving the response message, <strong class="source-inline">resp</strong>, from the aggregator, if the message type is <strong class="source-inline">AggMsgType.update</strong>, meaning the response message contains the updated global models, it calls the <strong class="source-inline">save_model_from_message</strong> function. Otherwise, it does nothing.</p>
			<p>The aforementioned functions are the basic but core features of an FL client, and those functions need to be efficiently used by a user-side ML application as libraries.</p>
			<p>Now that FL client design, including initialization, participation, and model exchanges, has been explained, we will learn about how to design FL client libraries.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor143"/>Designing FL client libraries</h1>
			<p>In this section, we will explain how to package essential functions to be provided as libraries to <a id="_idIndexMarker434"/>users. In this example, the simplest way to package them as libraries will be discussed. This will need to be expanded, depending on your needs and the design of your own FL client framework. By packaging FL client-side modules as libraries, developers will be easily able to integrate the FL client’s functions into the local ML engine.</p>
			<p>Let’s start with how to define a library to start and register an FL client.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor144"/>Starting FL client core threads</h2>
			<p>For local ML application developers to be able to integrate FL client-related functions, they sometimes need to be packaged as threading functions.</p>
			<p>The <a id="_idIndexMarker435"/>following code to register an agent in the FL system simply puts a <strong class="source-inline">participate</strong> function into the <strong class="source-inline">run_until_complete</strong> function of an <strong class="source-inline">asyncio.get_event_loop</strong> function:</p>
			<pre class="source-code">
def register_client(self):
    asyncio.get_event_loop().run_until_complete( \
        self.participate())</pre>
			<p>Also, the <strong class="source-inline">start_wait_model_server</strong> function is packaged, as shown in the following code block, where the <strong class="source-inline">Thread</strong> function takes care of the constant run. This way, you will be able to run the local ML module in parallel and receive global models in the <strong class="source-inline">wait_models</strong> thread when the FL system is in <em class="italic">push</em> communication mode:</p>
			<pre class="source-code">
def start_wait_model_server(self):
    th = Thread(target = init_client_server, \
        args=[self.wait_models, self.agent_ip, \
        self.exch_socket])
    th.start()</pre>
			<p>Similarly, the <strong class="source-inline">start_model_exhange_server</strong> function can be a thread to run a model <a id="_idIndexMarker436"/>exchange routine to synchronize the local and global models, while the local ML module is running in parallel. You can just call the following <strong class="source-inline">start_model_exchange_server</strong> function as a library to enable this functionality:</p>
			<pre class="source-code">
def start_model_exchange_server(self):
    self.agent_running = True
    th = Thread(target = init_loop, \
        args=[self.model_exchange_routine()])
    th.start()</pre>
			<p>Finally, it may be helpful to package all these three functions to execute at the same time when they are called outside the <strong class="source-inline">Client</strong> class. Therefore, we introduce the following code concerning <strong class="source-inline">start_fl_client</strong> that aggregates the functions of registering agents, waiting for global models and a model exchange routine to start the FL client core functions:</p>
			<pre class="source-code">
def start_fl_client(self):
    self.register_client()
    if self.is_polling == False:
        self.start_wait_model_server()
    self.start_model_exchange_server()</pre>
			<p>The initiation of the FL client is now packaged into <strong class="source-inline">start_fl_client</strong>. Next, we will define the libraries of saved ML models.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor145"/>Saving global models</h2>
			<p>While the <strong class="source-inline">load</strong> and <strong class="source-inline">save</strong> model functions are provided by the helper functions in <strong class="source-inline">lib/util</strong>, which will be explained later in the <em class="italic">Appendix</em>, <em class="italic">Exploring Internal Libraries</em>, it is helpful to provide an <a id="_idIndexMarker437"/>interface for ML developers to save global models from a message sent from an aggregator.</p>
			<p>The following <strong class="source-inline">save_model_from_message</strong> function is one that extracts and saves global models in an agent and also changes the client state to <strong class="source-inline">gm_ready</strong>. This function takes the message (<strong class="source-inline">msg</strong>) and message location (<strong class="source-inline">MSG_LOC</strong>) information as parameters:</p>
			<pre class="source-code">
def save_model_from_message(self, msg, MSG_LOC):
    data_dict = create_data_dict_from_models( \
        msg[int(MSG_LOC.model_id)],
        msg[int(MSG_LOC.global_models)],
        msg[int(MSG_LOC.aggregator_id)])
    self.round = msg[int(MSG_LOC.round)]
    save_model_file(data_dict, self.model_path, \
        self.gmfile)
    self.tran_state(ClientState.gm_ready)</pre>
			<p>The global models, model ID, and aggregator ID are extracted from the message and put into a dictionary using the <strong class="source-inline">create_data_dict_from_models</strong> library. The round information is also updated based on the received message.</p>
			<p>Then,  the received global models are saved to the local file using the <strong class="source-inline">save_model_file</strong> library, in which the data dictionary, model path, and global model file name are specified to save the models.</p>
			<p>After receiving the global models, it changes the client state to <strong class="source-inline">gm_ready</strong>, the state indicating that the global model is ready for the local ML to be utilized by calling the <strong class="source-inline">tran_state</strong> function, which will be explained in the next section.</p>
			<p>With the function of saving global models defined, we are ready to move on to how to manipulate the client state in the next section.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor146"/>Manipulating client state</h2>
			<p>In order to manipulate the client state so that it can logically handle local and global models, we prepare the <strong class="source-inline">read_state</strong> and <strong class="source-inline">tran_state</strong> functions, which can be accessed <a id="_idIndexMarker438"/>both from inside and outside the code.</p>
			<p>The following <strong class="source-inline">read_state</strong> function reads the value written in <strong class="source-inline">statefile</strong>, stored in the location specified by <strong class="source-inline">model_path</strong>. The enumeration value of <strong class="source-inline">ClientState</strong> is used to change the client state:</p>
			<pre class="source-code">
def read_state(self) -&gt; ClientState:
    return read_state(self.model_path, self.statefile)</pre>
			<p>The following <strong class="source-inline">tran_state</strong> function changes the state of the agent. In this code sample, the state is maintained in the local <strong class="source-inline">state</strong> file only:</p>
			<pre class="source-code">
def tran_state(self, state: ClientState):
    write_state(self.model_path, self.statefile, state)</pre>
			<p>Next, let’s define the functions that can send local models to an aggregator.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor147"/>Sending local models to aggregator</h2>
			<p>The <a id="_idIndexMarker439"/>following asynchronous <strong class="source-inline">send_models</strong> function is about sending models that have been saved locally to the aggregator:</p>
			<pre class="source-code">
async def send_models(self):
    data_dict, performance_dict = \
        load_model_file(self.model_path, self.lmfile)
    , _, models, model_id = \
        compatible_data_dict_read(data_dict)
    msg = generate_lmodel_update_message( \
        self.id, model_id, models, performance_dict)
    await send(msg, self.aggr_ip, self.msend_socket)
    self.tran_state(ClientState.waiting_gm)</pre>
			<p>It first extracts <strong class="source-inline">data_dict</strong> and <strong class="source-inline">performance_dict</strong> using the <strong class="source-inline">load_model_file</strong> helper function and then pulls out the models and their ID from <strong class="source-inline">data_dict</strong>, based on the <strong class="source-inline">compatible_data_dict_read</strong> function. Then, the message is packaged with the <strong class="source-inline">generate_lmodel_update_message</strong> library and sent to the aggregator, with the <strong class="source-inline">send</strong> function from <strong class="source-inline">communication_handler</strong>. After that, the client state is changed to <strong class="source-inline">waiting_gm</strong> by the <strong class="source-inline">tran_state</strong> function. Again, the SSL/TSL framework can be added to secure communication, which is not implemented here to keep the FL client-side coding simple.</p>
			<p>The <a id="_idIndexMarker440"/>following <strong class="source-inline">send_initial_model</strong> function is called when you want to send the initial <em class="italic">base model</em> to an aggregator of the model architecture for registration purposes. It takes initial models, the number of samples, and performance value as input and calls <strong class="source-inline">setup_sending_model</strong>, which will be explained later in this section:</p>
			<pre class="source-code">
def send_initial_model(self, initial_models, \
                             num_samples=1, perf_val=0.0):
    self.setup_sending_models( \
        initial_models, num_samples, perf_val)</pre>
			<p>The following <strong class="source-inline">send_trained_model</strong> function is called when you want to send trained local models to the aggregator during the FL cycle. It takes trained models, the number of samples, and performance value as input and only calls <strong class="source-inline">setup_sending_model</strong> if the client state is <em class="italic">not</em> <strong class="source-inline">gm_ready</strong>:</p>
			<pre class="source-code">
def send_trained_model(self, models, \
                             num_samples, perf_value):
    state = self.read_state()
    if state == ClientState.gm_ready:
        pass
    else:
        self.setup_sending_models( \
            models, num_samples, perf_value)</pre>
			<p>The following <strong class="source-inline">setup_sending_models</strong> function is designed to serve as an internal library to set up sending locally trained models to the aggregator. It takes parameters <a id="_idIndexMarker441"/>of models as <strong class="source-inline">np.array</strong>, the number of samples as an integer, and performance data as a float value:</p>
			<pre class="source-code">
def setup_sending_models(self, models, \
                               num_samples, perf_val):
    model_id = generate_model_id( \
                   IDPrefix.agent, self.id, time.time())
    data_dict = create_data_dict_from_models( \
                    model_id, models, self.id)
    meta_data_dict = create_meta_data_dict( \
                         perf_val, num_samples)
    save_model_file(data_dict, self.model_path, \
        self.lmfile, meta_data_dict)
    self.tran_state(ClientState.sending)</pre>
			<p>Basically, this function creates a unique model ID with the <strong class="source-inline">generate_model_id</strong> helper function, <strong class="source-inline">data_dict</strong> to store the local ML models data created with the <strong class="source-inline">create_data_dict_from_models</strong> helper function, and <strong class="source-inline">meta_data_dict</strong> to store the performance data created with the <strong class="source-inline">create_meta_data_dict</strong> helper function. And then, all the aforementioned data related to the models and performance is saved locally with the <strong class="source-inline">save_model_file</strong> function, in the location specified with <strong class="source-inline">self.model_path</strong>. Then, it changes the client state to <strong class="source-inline">sending</strong> so that the <strong class="source-inline">mode_exchange_routine</strong> function can note the change in the client state and start sending trained local models to the aggregator.</p>
			<p>Now that we know about the libraries to send ML models to the aggregator, let’s learn about an important function to wait for a global model on the agent side.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor148"/>Waiting for global models from an aggregator</h2>
			<p>The <a id="_idIndexMarker442"/>following <strong class="source-inline">wait_for_global_model</strong> function is very important to conduct an FL cycle consistently:</p>
			<pre class="source-code">
def wait_for_global_model(self):
    while (self.read_state() != ClientState.gm_ready):
        time.sleep(5)
    data_dict, _ = load_model_file( \
                       self.model_path, self.gmfile)
    global_models = data_dict['models']
    self.tran_state(ClientState.training)
    return global_models</pre>
			<p>The principle is that the function waits until the client state becomes <strong class="source-inline">gm_ready</strong>. The transition of the client state to <strong class="source-inline">gm_ready</strong> happens when the global model is received on the agent side. Once the client state changes to <strong class="source-inline">gm_ready</strong>, it proceeds to load global models from <strong class="source-inline">data_dict</strong>, extracted with the <strong class="source-inline">load_model_file</strong> function, changes the client state to <strong class="source-inline">training</strong>, and returns the global models to the local ML module.</p>
			<p>We have discussed how to design the libraries of FL client-side functions. In the next section, we will discuss how to integrate those libraries into a local ML process.</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor149"/>Local ML engine integration into an FL system</h1>
			<p>The <a id="_idIndexMarker443"/>successful integration of FL client libraries <a id="_idIndexMarker444"/>into a local ML engine is key to conducting FL in distributed environments later on.</p>
			<p>The <strong class="source-inline">minimal_MLEngine.py</strong> file in the <strong class="source-inline">examples/minimal</strong> directory found in the GitHub repository at <a href="https://github.com/tie-set/simple-fl">https://github.com/tie-set/simple-fl</a>, as shown in <em class="italic">Figure 5.2</em>, provides an example of integrating FL client-side libraries into a minimal ML engine package:</p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B18369_05_02.jpg" alt="Figure 5.2 – The minimal ML engine package&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – The minimal ML engine package</p>
			<p>Next, we <a id="_idIndexMarker445"/>will explain what libraries <a id="_idIndexMarker446"/>need to be imported into the local ML engine in the following section.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor150"/>Importing libraries for a local ML engine</h2>
			<p>The following <a id="_idIndexMarker447"/>code shows the importing process, where general libraries such as <strong class="source-inline">numpy</strong>, <strong class="source-inline">time</strong>, and <strong class="source-inline">Dict</strong> are imported first. The key part of this process is that <strong class="source-inline">Client</strong> is imported from the <strong class="source-inline">client.py</strong> file in the <strong class="source-inline">fl_main.agent</strong> folder. This way, a developer does not need to know too much about the code inside an FL system and just calls the important functionalities defined as libraries, as discussed in the <em class="italic">Toward designing FL client libraries</em> section.</p>
			<p>We will not cover the <strong class="source-inline">pip</strong> installation packaging here in this book, but it is possible to host the client-side code with either a private or public PyPI server:</p>
			<pre class="source-code">
import numpy as np
import time, logging, sys
from typing import Dict
from fl_main.agent.client import Client</pre>
			<p>After importing the necessary libraries, let’s look at the functions defined for local training and testing.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor151"/>Defining the ML models, training, and test functions</h2>
			<p>You first <a id="_idIndexMarker448"/>define the models, training, and testing <a id="_idIndexMarker449"/>functions to be integrated into the FL system. In <a id="_idIndexMarker450"/>this code example, we will use dummy models and training/testing functions, allowing users to be able to understand the minimal FL procedure without being bothered by specific ML complications.</p>
			<p>The following function called <strong class="source-inline">init_models</strong> returns the templates of models (in a dictionary format) to inform the ML model structure. The models do not need to be trained <a id="_idIndexMarker451"/>necessarily. In this case, the models have two <a id="_idIndexMarker452"/>layers defined by <strong class="source-inline">model1</strong> and <strong class="source-inline">model2</strong>, where some <a id="_idIndexMarker453"/>random NumPy array is assigned to each layer, as follows:</p>
			<pre class="source-code">
def init_models() -&gt; Dict[str,np.array]:
    models = dict()
    models['model1'] = np.array([[1, 2, 3], [4, 5, 6]])
    models['model2'] = np.array([[1, 2], [3, 4]])
    return models</pre>
			<p>After initializing the models, you will design the following <strong class="source-inline">training</strong> function that can be a placeholder function for each ML application:</p>
			<pre class="source-code">
def training(models: Dict[str,np.array],
           init_flag: bool = False) -&gt; Dict[str,np.array]:
    # return templates of models to tell the structure
    # This model is not necessarily actually trained
    if init_flag:
        return init_models()
    # ML Training. In this example, no actual training.
    models = dict()
    models['model1'] = np.array([[1, 2, 3], [4, 5, 6]])
    models['model2'] = np.array([[1, 2], [3, 4]])
    return models</pre>
			<p>The logic of this function should be in the order of taking models as input, training them, and returning trained local models. As input parameters, it takes models with the <strong class="source-inline">Dict[str,np.array]</strong> format and the <strong class="source-inline">init_flag</strong> Boolean value, indicating whether it is the initialization step or not.</p>
			<p><strong class="source-inline">init_flag</strong> is <strong class="source-inline">True</strong> when you want to call and return the predefined <strong class="source-inline">init_models</strong>, and it is <strong class="source-inline">False</strong> if it’s an actual training step.</p>
			<p>Eventually, this function returns the trained models that are decomposed into NumPy arrays, with a dictionary of <strong class="source-inline">Dict[str,np.array]</strong> in this example.</p>
			<p>In this <a id="_idIndexMarker454"/>dummy example, we are just giving you <a id="_idIndexMarker455"/>dummy models that skip the actual training process.</p>
			<p>Then, the <a id="_idIndexMarker456"/>following <strong class="source-inline">compute_performance</strong> function is designed to compute the performance of models given a set of models and a test dataset:</p>
			<pre class="source-code">
def compute_performance(models: Dict[str,np.array], \
                                      testdata) -&gt; float:
    # replace with actual performance computation logic
    accuracy = 0.5
    return</pre>
			<p>Again, in this example, just a dummy accuracy value is given, <strong class="source-inline">0.5</strong>, to keep things simple.</p>
			<p>Then, you may want to define the following <strong class="source-inline">judge_termination</strong> function to decide the criteria to finish the training process and exit from the FL process:</p>
			<pre class="source-code">
def judge_termination(training_count: int = 0,
              global_arrival_count: int = 0) -&gt; bool:
    # Depending on termination criteria, change the return bool value
    # Call a performance tracker to check if the current models satisfy the required performance
    return True</pre>
			<p>It is up to you how to design this termination condition. This function takes parameters such as the number of completed training processes (<strong class="source-inline">training_count</strong>), the number of times it received global models (<strong class="source-inline">global_arrival_count</strong>), and so on, returning a Boolean value where the flag is <strong class="source-inline">True</strong> if it continues the FL process and <strong class="source-inline">False</strong> if it stops. Here, it just gives a <strong class="source-inline">True</strong> Boolean value, meaning the FL process will not stop unless the agent is forced to stop outside of this function.</p>
			<p>If preparing the test data is needed, you can define a function such as <strong class="source-inline">prep_test_data</strong>:</p>
			<pre class="source-code">
def prep_test_data():
    testdata = 0
    return</pre>
			<p>In this example, it is just set as <strong class="source-inline">0</strong>.</p>
			<p>Now that <a id="_idIndexMarker457"/>the necessary functions for testing <a id="_idIndexMarker458"/>and training are defined, we will integrate <a id="_idIndexMarker459"/>client libraries into the local ML engine to run the FL agent working with the FL server-side components, such as an aggregator and a database.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor152"/>Integration of client libraries into your local ML engine</h2>
			<p>Now, everything is ready to start your very first FL process, although the models, training, and testing functions are set with dummy variables.</p>
			<p>The very <a id="_idIndexMarker460"/>first thing to do is to create a <strong class="source-inline">Client</strong> instance as follows so that you can call its libraries:</p>
			<pre class="source-code">
# Step1: Create Client instance
cl = Client()</pre>
			<p>Second, you create the <strong class="source-inline">initial_models</strong> with the training function, as follows:</p>
			<pre class="source-code">
# Step2: Create template models (to tell the shapes)
initial_models = training(dict(), init_flag=True)</pre>
			<p>After that, it sends the initial models to the FL aggregator by calling <strong class="source-inline">cl.send_initial_model</strong>, with <strong class="source-inline">initial_models</strong> as a parameter:</p>
			<pre class="source-code">
# Step3: Send initial models
cl.send_initial_model(initial_model)</pre>
			<p>Then, let’s just start the client-side FL process by calling <strong class="source-inline">cl.start_fl_client()</strong>. As explained earlier in the <em class="italic">Starting FL client core threads</em> section, this function can start three processes at the same time: registering the agent, waiting for global models, and the model exchange routine:</p>
			<pre class="source-code">
# Step4: Start the FL client
cl.start_fl_client()</pre>
			<p>Then, we <a id="_idIndexMarker461"/>design the client-side FL cycle of local training/testing and sending/receiving models by effectively integrating the several FL client libraries, as follows:</p>
			<pre class="source-code">
# Step5: Run the local FL loop
training_count, gm_arrival_count = 0, 0
while judge_termination(training_count, gm_arrival_count):
    global_models = cl.wait_for_global_model()
    gm_arrival_count += 1
    global_model_performance_data = \
       compute_performance(global_models, prep_test_data())
    models = training(global_models)
    training_count += 1
    perf_value = compute_performance( \
                     models, prep_test_data())
    cl.send_trained_model(models, 1, perf_value)</pre>
			<p>We use a <strong class="source-inline">while</strong> loop and the <strong class="source-inline">judge_termination</strong> function to check whether the system needs to leave the loop. It is up to you to use <strong class="source-inline">training_count</strong> and <strong class="source-inline">gm_arrival_count</strong> to judge the termination of the FL cycle.</p>
			<p>Then, the agent proceeds to wait for the global models with <strong class="source-inline">cl.wait_for_global_model()</strong>. Upon the arrival of the global models from the aggregator, it extracts <strong class="source-inline">global_models</strong>, increments <strong class="source-inline">gm_arrival_count</strong>, and sets the client state to the <strong class="source-inline">training</strong> state in the <strong class="source-inline">wait_for_global_model</strong> function.</p>
			<p>Next, <strong class="source-inline">global_model_performance_data</strong> is calculated with the <strong class="source-inline">compute_performance</strong> function, taking <strong class="source-inline">global_models</strong> and <strong class="source-inline">prep_test_data</strong> as input.</p>
			<p>While executing <strong class="source-inline">training(global_models)</strong> in the <strong class="source-inline">training</strong> state, the client might receive new global models from the aggregator. This scenario happens when the client’s local training was too slow, and the aggregator decided to utilize other local models to create a new set of global models. If the new global models have already <a id="_idIndexMarker462"/>arrived at the agent, the client’s state is changed to <strong class="source-inline">gm_ready</strong> and the current ML model being trained will be discarded.</p>
			<p>After the local training phase has finished with <strong class="source-inline">models</strong> generated by <strong class="source-inline">training(global_models)</strong>, an agent increments <strong class="source-inline">training_count</strong> and calculates the performance data, <strong class="source-inline">perf_value</strong>, of the current ML model with the <strong class="source-inline">compute_performance</strong> function.</p>
			<p>Then, the agent tries to upload the trained local models to the aggregator via <strong class="source-inline">cl.send_trained_model</strong>, taking the trained models and the performance value calculated previously as parameters.</p>
			<p>In the <strong class="source-inline">send_trained_model</strong> function, the client state is set to <strong class="source-inline">sending</strong>. Once the client’s <strong class="source-inline">model_exchange_routine</strong> observes the state transition to the <strong class="source-inline">sending</strong> state, it sends the trained local models (stored as a binary file) to the aggregator. After sending the models, it goes back to the <strong class="source-inline">waiting_gm</strong> state in the <strong class="source-inline">send_models</strong> function.</p>
			<p>After sending the local models, the aggregator stores the uploaded local models in its buffers and waits for another round of global model aggregation, until enough local models are uploaded by agents.</p>
			<p>In the next section, we will briefly talk about how to integrate image classification ML into the FL system we have discussed.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor153"/>An example of integrating image classification  into an FL system</h1>
			<p>We learned <a id="_idIndexMarker463"/>about how to initiate <a id="_idIndexMarker464"/>an FL process with a minimal example. In this section, we will give a brief example of FL with <strong class="bold">image classification</strong> (<strong class="bold">IC</strong>) using a CNN.</p>
			<p>First, the package that contains the image classification example code is found in the <strong class="source-inline">examples/image_classification/</strong> folder in the GitHub repository at <a href="https://github.com/tie-set/simple-fl">https://github.com/tie-set/simple-fl</a>, as shown in <em class="italic">Figure 5.3</em>:</p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B18369_05_03.jpg" alt="Figure 5.3 – The image classification package&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – The image classification package</p>
			<p>The main code in charge of integrating the IC algorithms into the FL systems is found in the <strong class="source-inline">classification_engine.py</strong> file.</p>
			<p>When importing the libraries, we use a couple of extra files that include CNN models, converter functions, and data managers related to IC algorithms. The details are provided in the GitHub code at <a href="https://github.com/tie-set/simple-fl">https://github.com/tie-set/simple-fl</a>.</p>
			<p>Next, let’s import some standard ML libraries as well as client libraries from the FL code we discussed:</p>
			<pre class="source-code">
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict
from .cnn import Net
from .conversion import Converter
from .ic_training import DataManger, execute_ic_training
from fl_main.agent.client import Client</pre>
			<p>In this case, we define <strong class="source-inline">TrainingMetaData</strong>, which just gives you the amount of training <a id="_idIndexMarker465"/>data that will be sent to <a id="_idIndexMarker466"/>the aggregator and used when conducting the <strong class="source-inline">FedAvg</strong> algorithm. The aggregation algorithm was discussed in <a href="B18369_04.xhtml#_idTextAnchor085"><em class="italic">Chapter 4</em></a>, <em class="italic">Federated Learning Server Implementation with Python</em>, as well as in <a href="B18369_07.xhtml#_idTextAnchor176"><em class="italic">Chapter 7</em></a>, <em class="italic">Model Aggregation</em>:</p>
			<pre class="source-code">
class TrainingMetaData:
    # The number of training data used for each round
    # This will be used for the weighted averaging
    # Set to a natural number &gt; 0
    num_training_data = 8000</pre>
			<p>The content of the <strong class="source-inline">init_models</strong> function is now replaced with a CNN that is converted into a NumPy array. It returns the template of the CNN in a dictionary format to inform the structure:</p>
			<pre class="source-code">
def init_models() -&gt; Dict[str,np.array]:
    net = Net()
    return Converter.cvtr().convert_nn_to_dict_nparray(net)</pre>
			<p>The training function, <strong class="source-inline">training</strong>, is now filled with actual training algorithms using the CIFAR-10 dataset. It takes the models and <strong class="source-inline">init_flag</strong> as parameters and returns the trained models as <strong class="source-inline">Dict[str,np.array]</strong>. The <strong class="source-inline">init_flag</strong> is a <strong class="source-inline">bool</strong> value, where it is <strong class="source-inline">True</strong> if it’s at the initial step and <strong class="source-inline">False</strong> if it’s an actual training step. When preparing for the training data, we use a certain threshold for training due to batch size. In this case, the threshold is <strong class="source-inline">4</strong>.</p>
			<p>Then, we create a CNN-based cluster global model with <strong class="source-inline">net = Converter.cvtr().convert_dict_nparray_to_nn(models)</strong>.</p>
			<p>We define the loss function and optimizer as the following:</p>
			<pre class="source-code">
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</pre>
			<p>Then, the actual training will be conducted with <strong class="source-inline">trained_net = execute_ic_training(DataManger.dm(), net, criterion, optimizer)</strong>, where the actual code of the IC training can be found in the <strong class="source-inline">ic_training.py</strong> file.</p>
			<p>After <a id="_idIndexMarker467"/>the training, the converted <a id="_idIndexMarker468"/>models will be returned.</p>
			<p>The algorithm is summarized as follows:</p>
			<pre class="source-code">
def training(models: Dict[str,np.array], \
             init_flag: bool=False) -&gt; Dict[str,np.array]:
    if init_flag:
        DataManger.dm( \
            int(TrainingMetaData.num_training_data / 4))
        return init_models()
    net = \
        Converter.cvtr().convert_dict_nparray_to_nn(models)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), \
                          lr=0.001, momentum=0.9)
    trained_net = execute_ic_training(DataManger.dm(), \
                              net, criterion, optimizer)
    models = Converter.cvtr(). \
                 convert_nn_to_dict_nparray(trained_net)
    return models</pre>
			<p>The following <strong class="source-inline">compute_performance</strong> function is filled with an algorithm to calculate the accuracy, which is simple enough – just divide the number of correct outcomes <a id="_idIndexMarker469"/>by the number of total <a id="_idIndexMarker470"/>labels. With a given set of models and a test dataset, it computes the performance of the models, with <strong class="source-inline">models</strong> and <strong class="source-inline">testdata</strong> as parameters:</p>
			<pre class="source-code">
def compute_performance(models: Dict[str,np.array], \
                       testdata, is_local: bool) -&gt; float:
    # Convert np arrays to a CNN
    net = \
       Converter.cvtr().convert_dict_nparray_to_nn(models)
    correct, total = 0, 0
    with torch.no_grad():
        for data in DataManger.dm().testloader:
            images, labels = data
            _, predicted = torch.max(net(images).data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    acc = float(correct) / total
    return acc</pre>
			<p>The <strong class="source-inline">judge_termination</strong> and <strong class="source-inline">prep_test_data</strong> functions are the same as the functions of the minimal examples.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor154"/>Integration of client libraries into the IC example</h2>
			<p>Now, everything <a id="_idIndexMarker471"/>is ready <a id="_idIndexMarker472"/>to start the IC algorithm, and all the code to integrate the preceding functions is the same as that used in the previous <em class="italic">Integration of client libraries into your local ML engine</em> section. Please look into the <strong class="source-inline">classification_engine.py</strong> file to make sure the code is the <a id="_idIndexMarker473"/>same, except for the <a id="_idIndexMarker474"/>part that shows the actual number of data samples that we are sending. This way, by just rewriting the preceding functions, you will be able to easily connect your own local ML application to the FL system that we have discussed here. Please refer to the <em class="italic">Running image classification and its analysis</em> section of <a href="B18369_06.xhtml#_idTextAnchor156"><em class="italic">Chapter 6</em></a>, <em class="italic">Running the Federated Learning System and Analyzing the Results</em>, to check the results of running the code discussed in this section.</p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor155"/>Summary</h1>
			<p>In this chapter, we discussed FL client-side implementation. There are three basic but important functionalities when participating in the FL process, receiving global models sent from an aggregator with a push or polling mechanism, and sending local models to an aggregator after the local training process. In order to effectively implement the client-side ML engines that cooperate with the FL aggregator, understanding the client state is important. The client states include a state waiting for the global model, a state indicating that local training is happening, a state showing the readiness to send local models, and a state for having the updated global models.</p>
			<p>We also discussed the design philosophy of FL client-side libraries, where the core functions need to be effectively packaged to provide user-friendly interfaces for ML developers and engineers.</p>
			<p>Last but not least, we learned how to actually use the FL client libraries to integrate a local ML engine into an FL system, where we used the minimal dummy example and IC example to understand the integration process itself.</p>
			<p>In the next chapter, we will actually run the code that was introduced in this and previous chapters so that we can dig into what is happening with the models, which are aggregated with a minimal example as well as an IC example.</p>
		</div>
	</body></html>