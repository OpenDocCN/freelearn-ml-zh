- en: Tour of Machine Learning Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法巡礼
- en: In this chapter, we're going to explore the different ways to categorize the
    types of tasks that **machine learning** (**ML**) can accomplish, and categorize
    the ML algorithms themselves. There are many different ways to organize the ML
    landscape; we can categorize algorithms by the type of training data we give them,
    we can categorize by the type of output we expect from the algorithms, we can
    categorize algorithms by their specific methods and tactics, we can categorize
    them by the format of the data they work with, and so on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨对**机器学习**（**ML**）能够完成的任务类型的不同分类方法，并对ML算法本身进行分类。组织ML领域的方法有很多种；我们可以根据我们提供给它们的训练数据类型来分类算法，我们可以根据我们期望从算法中获得的结果类型来分类，我们可以根据它们的特定方法和策略来分类算法，我们可以根据它们处理的数据格式来分类，等等。
- en: 'As we discuss the different types and categories of ML tasks and algorithms
    throughout this chapter, we''ll also introduce many of the algorithms that you''ll
    encounter throughout this book. Only the high-level concepts of algorithms will
    be discussed in this chapter, allowing us to go into detail in later chapters.
    The topics that we will be covering in this chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论不同类型和类别的ML任务和算法，同时也会介绍你将在本书中遇到的一些算法。本章将只讨论算法的高级概念，以便我们在后面的章节中深入探讨。本章将涵盖以下主题：
- en: Introduction to machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: Types of learning—unsupervised learning, supervised learning, and reinforcement
    learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习类型——无监督学习、监督学习和强化学习
- en: Categories of algorithms—clustering, classification, regression, dimensionality
    reduction, optimization, natural language processing, and image processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法类别——聚类、分类、回归、降维、优化、自然语言处理和图像处理
- en: At the end of this chapter, you should have an understanding of supervised learning
    versus unsupervised learning, and should understand the overall landscape of the
    algorithms that we'll apply throughout this book.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你应该对监督学习和无监督学习有一个理解，并且应该了解我们将在这本书中应用的整体算法景观。
- en: Introduction to machine learning
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: 'In general, ML is the name we give to the practice of making computers learn
    without explicitly programming insights into the algorithm. The converse practice—that
    is, programming an algorithm with a set of instructions that it can apply to datasets—is
    often called **heuristics**. This is our first classification of algorithms: machine
    learning versus heuristic algorithms. If you are managing a firewall and are manually
    maintaining a blacklist of IP address ranges to block, you can be said to have
    developed a heuristic for your firewall. On the other hand, if you develop an
    algorithm that analyzes patterns in web traffic, infers from those patterns, and
    automatically maintains your blacklist, you can be said to have developed an ML
    approach to firewalls.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，ML是我们对让计算机在没有明确编程算法洞察力的情况下学习的实践所赋予的名字。相反的实践——即用一组指令编程算法，使其能够应用于数据集——通常被称为**启发式**。这是我们算法的第一种分类：机器学习与启发式算法。如果你在管理防火墙时手动维护一个要阻止的IP地址范围的黑名单，那么可以说你已经为你的防火墙开发了一个启发式方法。另一方面，如果你开发了一个分析网络流量模式、从这些模式中推断并自动维护你的黑名单的算法，那么可以说你已经开发了一种针对防火墙的ML方法。
- en: We can, of course, further subcategorize our ML firewall approach. If your algorithm
    is designed with no *a priori* knowledge (knowledge beforehand), that is, if the
    algorithm *starts from scratch*, then it can be called an **unsupervised learning** algorithm.
    On the other hand, if you train the algorithm by showing it examples of requests
    from sources that should be blocked and expect it to learn by example, then the
    algorithm can be called a **supervised learning** algorithm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然可以进一步细分我们的ML防火墙方法。如果你的算法设计时没有**先验知识**（事先的知识），也就是说，如果算法**从头开始**，那么它可以被称为**无监督学习**算法。另一方面，如果你通过展示应该被阻止的源请求的示例来训练算法，并期望它通过示例进行学习，那么这个算法可以被称为**监督学习**算法。
- en: The specific algorithm you implement may also fall into yet another subcategory.
    Your algorithm may rely on *clustering* similar requests in order to determine
    which cluster a given request might belong to, or your algorithm may use Bayesian
    statistics to determine the probability that a request should be *classified* good
    or bad, or your algorithm may use a combination of techniques such as clustering,
    classification, and heuristics! Like many other taxonomical systems, there is
    often ambiguity in classifying special cases, but for the most part, algorithms
    can be divided into different categories.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你实施的特定算法也可能属于另一个子类别。你的算法可能依赖于*聚类*相似请求以确定给定请求可能属于哪个簇，或者你的算法可能使用贝叶斯统计来确定请求应该被**分类**为好或坏的几率，或者你的算法可能使用聚类、分类和启发式等技术的组合！像许多其他分类系统一样，在分类特殊情况时往往存在模糊性，但就大部分而言，算法可以被分为不同的类别。
- en: Types of learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习类型
- en: All ML algorithms consume data as an input and are expected to generate insights,
    predictions, classifications, or analyses as an output. Some algorithms have an
    additional *training* step, where the algorithm is trained on some data, tested
    to make sure that they have learned from the training data, and at a future date
    given a new data point or set of data for which you desire insights.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机器学习算法都消耗数据作为输入，并期望生成见解、预测、分类或分析作为输出。一些算法有一个额外的*训练*步骤，在这个步骤中，算法在某个数据上被训练，测试以确保它们已经从训练数据中学习，然后在未来的某个日期给出一个你希望获得见解的新数据点或数据集。
- en: All ML algorithms that use training data expect the data to be *labeled*, or
    somehow marked with the desired result for that data. For instance, when building
    a spam filter, you must first teach or train the algorithm on what spam looks
    like as compared to what normal messages (called **ham**) look like. You must
    first train the spam filter on a number of messages, each labeled either *spam* or *ham*,
    so that the algorithm can learn to distinguish between the two. Once the algorithm
    is trained, you can present it with a new, never-before-seen message, and expect
    it to guess whether that message is ham or spam. In this example, the set of messages
    you train the algorithm with is called the **training data** or **training set**,
    the labels in use are *spam* and *ham*, and the guesswork that the algorithm performs
    is called **inference**. This practice of training an algorithm on a set of prelabeled
    training data is called **supervised learning**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有使用训练数据的机器学习算法都期望数据是*标记的*，或者以某种方式标记出该数据的期望结果。例如，当构建垃圾邮件过滤器时，你必须首先教会或训练算法垃圾邮件与正常消息（称为**ham**）的外观区别。你必须首先在一系列消息上训练垃圾邮件过滤器，每条消息都标记为*spam*或*ham*，这样算法才能学会区分两者。一旦算法被训练，你就可以向它展示一条新的、以前从未见过的消息，并期望它能猜测该消息是ham还是spam。在这个例子中，你用来训练算法的消息集被称为**训练数据**或**训练集**，使用的标签是*spam*和*ham*，而算法进行的猜测工作被称为**推理**。这种在一系列预标记的训练数据上训练算法的实践被称为**监督学习**。
- en: Other algorithms do not require training, or can inspect a dataset without any
    labels and develop insights directly from the data. This is called **unsupervised
    learning**, and this classification is marked by the lack of labels on the data.
    If you work in a scientific laboratory and are developing an image processing
    algorithm to inspect pictures of bacterial cultures in Petri dishes, with the
    goal of the algorithm telling you how many distinct bacterial colonies are seen
    in the photograph, you have developed an unsupervised learning algorithm. In this
    case, you do not need to train the algorithm with training data that has the number
    of colonies prelabeled; the algorithm is expected to work from scratch to find
    patterns and structures in the data. The inputs and outputs are similar to the
    supervised learning example, in that you give the data to the algorithm and expect
    to receive insights as output, but these inputs and outputs are different in that
    there is no training step or *a* *priori* knowledge required by the algorithm.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其他算法不需要训练，或者可以在没有任何标签的数据集上检查数据，并直接从数据中得出见解。这被称为**无监督学习**，这种分类的特点是数据上没有标签。如果你在科学实验室工作，正在开发一个图像处理算法来检查培养皿中细菌培养物的图片，目的是让算法告诉你照片中可以看到多少不同的细菌菌落，那么你已经开发了一个无监督学习算法。在这种情况下，你不需要用带有预标记菌落数量的训练数据来训练算法；算法预计将从零开始寻找数据中的模式和结构。输入和输出与监督学习示例相似，即你将数据提供给算法，并期望得到见解作为输出，但不同之处在于没有训练步骤或算法需要的*先验*知识。
- en: There are further classifications that fall within a spectrum between supervised
    and unsupervised learning. For instance, in *semi-supervised *learning, an algorithm
    receives a prelabeled training set, but not every label is represented by the
    training data. In this case, the algorithm is expected to fit examples to the
    trained labels where applicable, but also expected to generate new labels when
    appropriate.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习和无监督学习之间，还存在进一步的分类，这些分类位于一个光谱上。例如，在*半监督学习*中，算法接收一个预标记的训练集，但并非每个标签都由训练数据表示。在这种情况下，算法预计将示例拟合到适用的已训练标签，但也预计在适当的时候生成新的标签。
- en: Another mode of learning is **reinforcement learning**. Reinforcement learning
    is similar to both supervised learning and unsupervised learning in various ways.
    In reinforcement learning, the training data does not have explicit labels, but
    the results that the algorithm generates may be associated with a certain penalty
    or reward; the goal of the algorithm is to eventually optimize its results such
    that the penalty is minimized. Reinforcement learning is often used in conjunction
    with supervised learning. An algorithm may be initially trained on some labeled
    training data, but then is expected to update its model based on feedback about
    the decisions it has made.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种学习模式是**强化学习**。强化学习在许多方面与监督学习和无监督学习相似。在强化学习中，训练数据没有明确的标签，但算法生成的结果可能与某种惩罚或奖励相关；算法的目标是最终优化其结果，以使惩罚最小化。强化学习通常与监督学习结合使用。一个算法可能最初在带有标记的训练数据上训练，但随后预计将根据其对所做决策的反馈来更新其模型。
- en: For the most part, you will find that supervised and unsupervised learning are
    the two major categories of algorithms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，你会发现监督学习和无监督学习是两种主要的算法类别。
- en: Unsupervised learning
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In unsupervised learning, the goal is to infer structure or patterns from data
    without needing any prior labeling of the data. Because the data is unlabeled,
    there is typically no way to evaluate the accuracy of the learning algorithm,
    a major distinction from supervised learning. Unsupervised learning algorithms
    typically are not given any *a* *priori* knowledge of the data, except perhaps
    indirectly by the tuning parameters given to the algorithm itself.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，目标是无需对数据进行任何先前的标记，就从数据中推断结构或模式。由于数据未标记，通常无法评估学习算法的准确性，这是与监督学习的一个主要区别。无监督学习算法通常不会获得关于数据的任何*先验*知识，除非可能是通过算法本身给出的调整参数间接获得。
- en: Unsupervised learning is commonly used for problems that might be solvable by
    eye if the data had very few dimensions, but the large dimensionality of the data
    makes this impossible or very difficult for a human to infer. Unsupervised learning
    can also be used for lower-dimension problems that may be solved intuitively by
    a human, but where there is a lot of data to be processed, it is unreasonable
    to do manually.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习通常用于可能通过肉眼解决的数据维度非常少的问题，但由于数据的维度很大，这使得人类推断变得不可能或非常困难。无监督学习也可以用于可能通过直觉解决的低维问题，但在需要处理大量数据的情况下，手动处理是不合理的。
- en: Imagine that you're writing an algorithm that looks at satellite imagery data
    and the task is to identify buildings and cluster them into geographically-separated
    neighborhoods. If you have just one image, or a handful of images, this is easy
    to accomplish by hand. A researcher would mark all the buildings on a photo and
    visually inspect the photo to determine clusters of buildings. The researcher
    then records the latitude and longitude of the neighborhood's center and puts
    the results in a spreadsheet. Great, the head scientist says, only three million
    more images to go! This is an example of a low-dimensional problem (there are
    only two dimensions, *latitude* and *longitude*, to consider) that is made implausible
    by the sheer volume of the task. Clearly a more sophisticated solution is required.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在编写一个算法，该算法查看卫星图像数据，任务是识别建筑物并将它们聚类成地理位置分离的社区。如果你只有一张图像，或者只有几张图像，手动完成这项任务很容易。研究人员会在照片上标记所有建筑物，并视觉检查照片以确定建筑物的集群。然后，研究人员记录社区中心的纬度和经度，并将结果放入电子表格中。太好了，首席科学家说，还有三百万张图像要处理！这是一个低维问题（只有两个维度，*纬度*和*经度*需要考虑）的例子，但由于任务的庞大体积而变得不切实际。显然需要一个更复杂的解决方案。
- en: 'To develop an unsupervised learning approach to this problem, a researcher
    might divide the problem into two stages: **preprocessing** and **analysis**.
    In the preprocessing step, each image should be run through an algorithm that
    detects buildings in the photograph and returns their latitude/longitude coordinates.
    This preprocessing step can be managed in several ways: one approach would be
    to send the images to a team of interns to be manually marked; another approach
    could be a non-machine learning edge detection algorithm that looks for rectangular
    shapes; and a third approach could be a **Convolutional Neural Network** (**CNN**)
    that is trained to identify images of buildings.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发一种无监督学习方法来解决此问题，研究人员可能会将问题分为两个阶段：**预处理**和**分析**。在预处理步骤中，每张图像都应该通过一个算法来检测照片中的建筑物并返回它们的纬度/经度坐标。这个预处理步骤可以通过几种方式来管理：一种方法是将图像发送给一组实习生进行手动标记；另一种方法可能是一个非机器学习的边缘检测算法，它寻找矩形形状；第三种方法可能是一个**卷积神经网络**（**CNN**），它被训练来识别建筑物的图像。
- en: Once the preprocessing has been done and a list of building coordinates is on
    hand, the coordinates can then be run through an unsupervised clustering algorithm,
    such as the k-means algorithm, which we'll explore later. The unsupervised algorithm
    does not need to know what a *building* is, it doesn't need to know about any
    existing neighborhoods or clusters of buildings, and doesn't need any other *a
    priori* knowledge of the problem. The algorithm is simply able to read a list
    of millions or billions of latitude/longitude coordinates, and group them into
    geographically-centered clusters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成预处理并手头有一份建筑物坐标列表，就可以将这些坐标通过无监督聚类算法，如我们稍后将要探讨的k-means算法，进行运行。无监督算法不需要知道*建筑物*是什么，它不需要了解任何现有的社区或建筑物集群，也不需要任何其他*先验*知识。该算法只需能够读取数百万或数十亿个纬度/经度坐标，并将它们分组成以地理位置为中心的集群。
- en: Because unsupervised algorithms cannot judge the accuracy of their results,
    there is no guarantee that this algorithm will generate neighborhoods that match
    up with census data or that the algorithm's concept of *neighborhood* will be
    semantically correct. It's possible that a single town or neighborhood may be
    considered two separate neighborhoods if, for instance, a wide highway divides
    two halves of the town. It's also possible that the algorithm may combine two
    neighborhoods that are considered by their residents to be distinct into one single
    cluster, if there is no clear separation between the two neighborhoods.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无监督算法无法判断其结果的准确性，因此无法保证该算法将生成与人口普查数据相匹配的邻域，或者该算法对“邻域”的概念在语义上是正确的。例如，如果一条宽阔的高速公路将城镇的两个部分分开，那么一个城镇或邻域可能被视为两个独立的邻域。同样，如果两个邻域之间没有明显的分隔，算法可能将两个被认为是不同的邻域合并成一个单一的群集。
- en: In many cases, this type of semantic error is acceptable; the benefit of this
    approach to the problem is that it can process millions or billions of data points
    quickly and provides at least a logical sense of clustering. The results of the
    unsupervised clustering can be further postprocessed, either by another algorithm
    or reviewed by hand, to add semantic information to the results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，这种语义错误是可以接受的；这种方法解决问题的好处是它可以快速处理数百万或数十亿个数据点，并至少提供一种逻辑上的群集感。无监督群集的结果可以通过另一种算法进一步后处理，或者手动审查，以向结果添加语义信息。
- en: Unsupervised algorithms can also find patterns in high-dimensional datasets
    that humans are unable to visualize intuitively. In the building clustering problem,
    it's easy for a researcher to visually inspect the two-dimensional map and identify
    clusters by eye. Imagine now that you have a set of data points, each existing
    in a 100-dimensional space (that is, data that has 100 distinct features). If
    the amount of data you possess is non-trivial, for example, more than 100 or 1,000
    data points, it may be nearly impossible for a human to interpret the data, because
    the relationships between the features are too difficult to visualize in a 100-dimensional
    space.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督算法也可以在人类无法直观可视化的高维数据集中找到模式。在建筑群集问题中，研究人员很容易通过视觉检查二维地图并凭肉眼识别群集。现在想象一下，你有一组数据点，每个数据点存在于一个100维的空间中（即具有100个不同特征的数据）。如果你拥有的数据量非同寻常，例如超过100或1,000个数据点，对于人类来说几乎不可能解释这些数据，因为特征之间的关系在100维空间中难以可视化。
- en: As a contrived example of the preceding problem, imagine you're a psychologist
    and your task is to interpret a thousand surveys given to participants that ask
    100 different questions, each on a 1-10 scale. Each question is designed to rate
    a different aspect of the participant's personality. Your goal in interpreting
    this data is to determine how many distinct personality types are represented
    by the respondents.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为上述问题的虚构例子，想象你是一位心理学家，你的任务是解释给参与者的一千份调查问卷，问卷中有100个不同的问题，每个问题都是1-10分的评分。每个问题都是为了评估参与者的不同性格方面。你的目标是确定有多少不同的性格类型由受访者代表。
- en: Processing only 1,000 data points by hand is certainly achievable, and common
    practice in many fields. In this case, however, the high dimensionality of the
    data makes it very difficult to discover patterns. Two respondents may have answered
    some questions very similarly, but answered other questions differently; are these
    two respondents similar enough to be considered of the same personality type?
    And how similar is that personality type to any other given personality type?
    The same algorithm we used previously to detect clusters of buildings can be applied
    to this problem in order to detect clusters of respondents and their personality
    types (my apologies to any actual psychologists reading this; I know I have grossly
    oversimplified the problem!).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过手工处理仅1,000个数据点当然是可以实现的，这在许多领域都是常见的做法。然而，在这种情况下，数据的高度维度使得发现模式变得非常困难。两位受访者可能对某些问题的回答非常相似，但对其他问题的回答却不同；这两位受访者是否足够相似，可以被认为是同一性格类型？而且这种性格类型与其他任何给定的性格类型有多相似？我们之前用来检测建筑群集的相同算法可以应用于这个问题，以便检测受访者的群集及其性格类型（对于阅读此文的任何实际心理学家表示歉意；我知道我极大地简化了这个问题！）。
- en: 'In this case, the unsupervised clustering algorithm does not have any difficultly
    with *visualizing* the 100 dimensions involved, and will perform similarly to
    the two-dimensional neighborhood clustering problem. The same caveats apply, as
    well: there is no guarantee that the clusters the algorithm detects will be psychologically
    correct, nor that the questions themselves were designed correctly to appropriately
    capture all the distinct personality types. The only promise this algorithm makes
    is that it will identify clusters of similar data points.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，无监督聚类算法在*可视化*涉及到的100个维度上没有任何困难，其表现将与二维邻域聚类问题相似。同样需要注意：不能保证算法检测到的聚类在心理上是正确的，也不能保证问题本身被设计得正确，以适当捕捉所有不同的个性类型。这个算法唯一做出的承诺是，它将识别出相似数据点的聚类。
- en: In [Chapter 2](94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml), *Data Exploration,*
    we discussed the importance of preprocessing data before giving it to a ML algorithm.
    We are now beginning to understand the importance of postprocessing and interpreting
    results, especially when looking at unsupervised algorithms. Because unsupervised
    algorithms can only judge their overall statistical distribution (that is, the
    average distance from any point to its cluster center in this case), rather than
    their semantic error (that is, how many data points are actually **correct**),
    the algorithm cannot make any claims as to its semantic correctness. Looking at
    metrics such as root-mean-squared error or standard deviation may give you a hint
    as to how the algorithm performed, but this cannot be used as a judgment of the
    algorithm's accuracy, and can only be used to describe the statistical properties
    of the dataset. Looking at these metrics won't tell you if the results are correct,
    and will only tell you how clustered or unclustered the data is (some neighborhoods
    are sparse, other neighborhoods are dense, and so on).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml)“数据探索”中，我们讨论了在将数据提供给机器学习算法之前预处理数据的重要性。我们现在开始理解后处理和解释结果的重要性，尤其是在查看无监督算法时。因为无监督算法只能判断它们的整体统计分布（即在这种情况下，任何点到其聚类中心的平均距离），而不是它们的语义错误（即有多少数据点是实际**正确**的），因此算法不能对其语义正确性做出任何断言。查看诸如均方根误差或标准差之类的指标可能会给你一些关于算法表现如何的线索，但这不能用作判断算法准确性的依据，而只能用来描述数据集的统计特性。查看这些指标不会告诉你结果是否正确，只会告诉你数据是聚集的还是分散的（某些邻域稀疏，其他邻域密集，等等）。
- en: So far we have considered unsupervised learning in the context of clustering
    algorithms, which is indeed a major family of unsupervised learning algorithms,
    but there are also many others. For instance, our discussion of outlier detection
    from [Chapter 2](94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml), *Data Exploration*,
    would fall under the category of unsupervised learning; we are looking at unlabeled
    data with no *a priori* knowledge, and attempting to glean insights from that
    data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在聚类算法的背景下考虑了无监督学习，这确实是无监督学习算法的一个主要家族，但还有许多其他算法。例如，我们在[第二章](94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml)“数据探索”中对异常值检测的讨论就属于无监督学习的范畴；我们正在查看没有*先验*知识的无标签数据，并试图从这些数据中获取洞察。
- en: Another example of a popular unsupervised learning technique is **Principal
    Component Analysis** (**PCA**), which we briefly introduced in [Chapter 2](94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml),
    *Data Exploration*. PCA is an unsupervised learning algorithm commonly used during
    preprocessing for feature detection and dimensionality reduction, and this algorithm
    fits the use case of interpreting high dimensional data. Unlike clustering algorithms,
    which aim to tell you how many logical clusters of data points exist in a dataset,
    PCA aims to tell you which features or dimensions of a dataset can be neatly combined
    into statistically significant derived features. In some sense, PCA can be thought
    of as the clustering of features or dimensions, rather than the clustering of
    data points.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习技术的另一个流行例子是**主成分分析**（PCA），我们在[第二章](94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml)“数据探索”中简要介绍了它。PCA是一种常用的无监督学习算法，通常用于预处理中的特征检测和降维，这个算法适用于解释高维数据的使用场景。与旨在告诉你在数据集中有多少逻辑数据点聚类的聚类算法不同，PCA旨在告诉你可以将数据集的哪些特征或维度整洁地组合成具有统计意义的派生特征。在某种程度上，PCA可以被视为特征或维度的聚类，而不是数据点的聚类。
- en: An algorithm such as PCA does not necessarily need to be used exclusively for
    preprocessing, and can in fact be used as the primary ML algorithm from which
    you want to gain insight.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 像PCA这样的算法并不一定需要专门用于预处理，实际上它可以作为你想要从中获得洞察的主要机器学习算法。
- en: Let's return to our psychological survey example. Rather than clustering survey
    respondents, we might want to analyze the questions themselves with a PCA. The
    results of the algorithm would tell you which survey questions are most significantly
    correlated to one another, and this insight can help you rewrite the actual survey
    questions so that they better target the personality traits that you wish to study.
    Additionally, the dimensionality reduction that PCA provides can help a researcher
    visualize the relationship between the questions, respondents, and results. The
    algorithm will convert your 100-dimensional, highly interconnected feature space
    that's impossible to visualize into distinct, lower-dimensional spaces that can
    actually be graphed and visually inspected.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的心理调查示例。与其对调查受访者进行聚类，我们可能更愿意用PCA分析问题本身。算法的结果会告诉你哪些调查问题彼此之间相关性最大，这种洞察可以帮助你重新编写实际的调查问题，以便更好地针对你想要研究的个性特征。此外，PCA提供的降维可以帮助研究人员可视化问题、受访者和结果之间的关系。该算法会将你的100维、高度互联的特征空间转换为可以实际绘制和视觉检查的独立、低维空间。
- en: As with all unsupervised learning algorithms, there is no guarantee that the
    principal component algorithm will be semantically correct, there is only a guarantee
    that the algorithm will be able to statistically determine the relationships between
    features. This means that some results may seem nonsensical or unintuitive; the
    algorithm might combine questions that don't seem to make intuitive sense when
    combined. In a situation like this, it's up to the researcher to postprocess and
    interpret the results of the analysis, potentially modifying the questions or
    changing their approach for the next round of surveys.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有无监督学习算法一样，无法保证主成分算法在语义上是正确的，只能保证算法能够从统计上确定特征之间的关系。这意味着一些结果可能看起来没有意义或不直观；算法可能会将看起来结合在一起没有直观意义的题目组合在一起。在这种情况下，研究人员需要后处理和解释分析结果，可能需要修改问题或改变他们在下一轮调查中的方法。
- en: There are many other examples of unsupervised learning algorithms, including
    the autoencoder neural network, which we will discuss in a later chapter. The
    most important feature of an unsupervised learning algorithm is the lack of labels
    in its input data, which results in the inability to determine the semantic correctness
    of its results. Do not make the mistake of dismissing unsupervised learning algorithms
    as being *lesser* than other algorithms, however, as they are very important in
    data preprocessing and many other types of data exploration tasks. Just as a wrench
    is no more and no less valuable than a screwdriver, each tool has its place and
    purpose in the world of ML.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法还有许多其他例子，包括将在后续章节中讨论的自动编码器神经网络。无监督学习算法最重要的特征是其输入数据中没有标签，这导致无法确定其结果的语义正确性。然而，不要犯将无监督学习算法视为比其他算法*低级*的错误，因为它们在数据预处理和许多其他类型的数据探索任务中非常重要。正如扳手和螺丝刀一样，每个工具在机器学习的世界中都有其位置和用途。
- en: Supervised learning
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: Like unsupervised learning, the goal of a supervised learning algorithm is to
    interpret input data and generate insights as its output. Unlike unsupervised
    learning, supervised learning algorithms are first trained on labeled training
    examples. The training examples are used by the algorithm to build a *model*,
    or an internal representation of the relationships between the data's properties
    and its label, and the model is then applied to new, unlabeled data points that
    you wish to glean insight from.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与无监督学习一样，监督学习算法的目标是解释输入数据并生成输出作为洞察。与无监督学习不同，监督学习算法首先在标记的训练示例上进行训练。训练示例被算法用来构建*模型*，即数据属性与其标签之间关系的内部表示，然后该模型被应用于你希望从中获得洞察的新、未标记的数据点。
- en: Supervised learning is often more exciting to ML students, as this category
    of algorithms aims to provide semantically correct results. When a supervised
    learning algorithm works well, the results almost seem magical! You can train
    an algorithm on 1,000 prelabeled data points and then use that model to process
    millions of future data points, with some expectation of semantic accuracy in
    the results.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通常对机器学习学生来说更有趣，因为这个类别的算法旨在提供语义正确的结果。当监督学习算法运行良好时，结果几乎看起来像是魔法！你可以在1,000个预标记的数据点上训练一个算法，然后使用该模型处理数百万未来的数据点，并对结果中的语义准确性有一定的期望。
- en: Because supervised learning algorithms aim to be semantically correct, we must
    first discuss how this correctness is measured. First, we must introduce the concepts
    of *true positives*, *false positives*, *true negatives*, and *false negatives*,
    then we will introduce the concepts of accuracy, precision, and recall.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于监督学习算法旨在提供语义正确的结果，我们首先必须讨论如何衡量这种正确性。首先，我们必须介绍*真正例*、*假正例*、*真反例*和*假反例*的概念，然后我们将介绍准确率、精确率和召回率的概念。
- en: Measuring accuracy
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率测量
- en: Imagine you are developing a spam filter for a commenting system you've developed
    for your blog. Spam filters are a type of supervised learning algorithm, as the
    algorithm must first be told what constitutes spam versus ham. You train your
    spam system on many examples of ham and spam messages, and then release it into
    production and allow it to classify all new comments, automatically blocking spam
    messages and letting genuine ham messages go through.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在为你的博客开发的一个评论系统开发垃圾邮件过滤器。垃圾邮件过滤器是一种监督学习算法，因为算法必须首先被告知什么是垃圾邮件，什么是正常邮件。你在许多垃圾邮件和正常邮件的例子上训练你的垃圾邮件系统，然后将其投入生产，并允许它对所有新的评论进行分类，自动阻止垃圾邮件，让真正的正常邮件通过。
- en: Let us consider a *positive* to be a comment that the algorithm identifies as
    spam (we're calling this *positive* because we're calling the algorithm a spam
    filter; this is only a semantic distinction, as we could call the filter a *ham
    filter* and instead use *positive* to denote suspected ham messages). Let's consider
    a *negative* to be a comment identified as a genuine (ham) comment.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把一个*正例*想象成算法识别为垃圾邮件的评论（我们称之为*正例*，因为我们把算法称为垃圾邮件过滤器；这只是一个语义上的区别，因为我们也可以把过滤器称为*正常邮件过滤器*，并用*正例*来表示可疑的正常邮件）。让我们把一个*反例*想象成被识别为真正的（正常）邮件的评论。
- en: If your algorithm categorizes a comment as spam (positive), and does so semantically
    correctly (that is, when you read the message, you also determine that it is spam),
    the algorithm has generated a *true positive*, or a positive result that is truly
    and correctly a positive result. If, on the other hand, a genuine comment incorrectly
    gets identified as spam and blocked, that is considered a *false positive*, or
    a positive result that is not actually positive. Similarly, a genuine ham message
    that is identified as ham is a *true negative*, and a spam comment that is identified
    as ham and let through is considered a *false negative*. It is unreasonable to
    expect an algorithm to provide 100% correct results, so there will always be some
    amount of false positives and false negatives in practice.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的算法将一条评论分类为垃圾邮件（正例），并且这样做是语义正确的（也就是说，当你阅读消息时，你也确定它是垃圾邮件），那么算法就生成了一个*真正例*，或者是一个真正且正确的结果。相反，如果一条真正的评论被错误地识别为垃圾邮件并被阻止，这被认为是*假正例*，或者是一个实际上不是正例的正例。同样，被识别为正常邮件的真正正常邮件是一个*真反例*，而被识别为正常邮件并通过的垃圾邮件被认为是*假反例*。期望算法提供100%正确的结果是不合理的，所以在实践中总会存在一定数量的假正例和假反例。
- en: 'If we take our four classifications of result accuracy, we can count the number
    of instances of each classification and determine a rate for each: we can easily
    calculate the false positive rate, the true positive rate, the false negative
    rate, and the true negative rate. However, these four rates may be clumsy to discuss
    if we treat them independently, so we can also combine these rates into other
    categories.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑我们的四种结果准确度分类，我们可以计算每个分类的实例数量，并为每个分类确定一个比率：我们可以轻松地计算出误报率、真正率、误判率和真正率。然而，如果我们独立地讨论这四个比率，可能会显得有些笨拙，因此我们也可以将这些比率组合成其他类别。
- en: For instance, the *recall*, or *sensitivity,* of an algorithm is its true positive
    rate, or the percentage of times that a positive classification is a true positive.
    In our spam example, the recall therefore refers to the percentage of spam messages
    correctly identified out of all actual spam messages. This can be calculated as
    either *true positives divided by actual positives*, or alternatively *true positives
    divided by true positives plus false negatives* (recall that false negatives are
    comments that are actually spam, but incorrectly identified as ham). Recall, in
    this case, refers to the algorithm's ability to correctly detect a spam comment,
    or put simply, *of all the actual spam messages there are, how many did we identify?*
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，算法的*召回率*或*灵敏度*是其真正阳性的比率，或者说是正分类为真正阳性的百分比。在我们的垃圾邮件示例中，召回率因此指的是在所有实际垃圾邮件中正确识别的垃圾邮件百分比。这可以计算为*真正阳性除以实际阳性*，或者也可以是*真正阳性除以真正阳性加上假阴性*（记住，假阴性是实际上是垃圾邮件但被错误地识别为正常邮件的评论）。在这种情况下，召回率指的是算法正确检测垃圾邮件评论的能力，或者简单地说，*在所有实际的垃圾邮件中，我们识别了多少？*
- en: Specificity is similar to recall, except that it represents the algorithm's
    true negative rate. Specificity asks the question *of all actual ham messages,
    how many did we correctly identify?*
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 特异性与召回率相似，但表示算法的真正阴性率。特异性询问的问题是：*在所有实际的正常邮件中，我们正确识别了多少？*
- en: Precision, on the other hand, is defined as the number of true positives divided
    by the sum of true positives and false positives. In terms of our spam example,
    precision answers the question *of all the messages we think are spam, how many
    guesses did we get correctly?* The distinction between the two metrics lies in
    whether we are considering all *actual* spam messages, or considering messages
    we *think* are spam.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，精确度定义为真正阳性数除以真正阳性数和假阳性数之和。在我们的垃圾邮件示例中，精确度回答了这样一个问题：*在我们认为的垃圾邮件中，我们有多少猜测是正确的？*这两个指标之间的区别在于，我们是否在考虑所有*实际*的垃圾邮件，或者考虑我们认为的垃圾邮件。
- en: Accuracy is distinct from both precision and recall, and focuses on overall
    correct results. It is defined as the rate of true positives and true negatives
    divided by the total number of trials (that is, how many guesses were correct
    overall). A common mistake that students of ML often make is to focus on accuracy
    alone, because it is intuitively easier to understand, but accuracy often is not
    sufficient when evaluating the performance of an algorithm.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度与精确度和召回率都不同，它关注整体正确结果。它被定义为真正阳性和真正阴性率除以总试验次数（即，总体上有多少猜测是正确的）。机器学习的学生常犯的一个错误是只关注准确度，因为它直观上更容易理解，但准确度在评估算法性能时通常是不够的。
- en: To demonstrate this, we must consider the impact of the performance of our spam
    filter on our real-world results. In some cases, you want a spam filter that never
    ever lets a single spam message through, even if that means incorrectly blocking
    some ham messages. In other cases, it's better to make sure that all ham messages
    are allowed, even if a few spam messages evade your filter. It's possible for
    two different spam filters to have the same exact *accuracy*,but totally different
    characteristics of precision and recall. For this reason, accuracy (while very
    useful) cannot always be the only performance metric you consider.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这一点，我们必须考虑我们的垃圾邮件过滤器对现实世界结果的影响。在某些情况下，你可能希望有一个垃圾邮件过滤器永远不会让任何一条垃圾邮件通过，即使这意味着错误地阻止了一些正常邮件。在其他情况下，确保所有正常邮件都能通过可能更好，即使这意味着一些垃圾邮件会绕过你的过滤器。两个不同的垃圾邮件过滤器可能有相同的*准确度*，但精确度和召回率的特征却完全不同。因此，尽管准确度非常有用，但它不能总是你考虑的唯一性能指标。
- en: 'Because the previous mathematical definitions may be a little difficult to
    internalize, let''s put numbers to the example. Consider 100 messages, 70 of which
    are genuinely ham and 30 of which are genuinely spam:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于之前的数学定义可能有点难以理解，让我们用数字来举例。假设有100条消息，其中70条是真正的正常邮件，30条是真正的垃圾邮件：
- en: '|  | **30** Actual Spam (Positive) | **70** Actual Ham (Negative) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | **30** 实际垃圾邮件（阳性） | **70** 实际正常邮件（阴性） |'
- en: '| **26** Guessed Spam | 22 (True Positive) | 4 (False Positive) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **26** 猜测的垃圾邮件 | 22（真正阳性） | 4（假阳性） |'
- en: '| **74** Guessed Ham | 8 (False Negative) | 66 (True Negative) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **74** 猜测的正常邮件 | 8（假阴性） | 66（真正阴性） |'
- en: 'To calculate the accuracy of the algorithm, we add up the correct guesses:
    `22` true positives and `66` true negatives, which equals 88 correct guesses in
    total. Our accuracy is therefore 88%.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算算法的准确率，我们将正确的猜测相加：`22`个真阳性（true positives）和`66`个真阴性（true negatives），总共是88个正确的猜测。因此，我们的准确率是88%。
- en: 'As an aside: 88% accuracy would be considered very good for advanced algorithms
    on difficult problems, but a little poor for a spam filter.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句：88%的准确率对于复杂问题上的高级算法来说非常好，但对于垃圾邮件过滤器来说稍微有点差。
- en: The recall or sensitivity of the algorithm is the *true positive rate*, or the
    number of times we guessed correctly when looking at examples that are *actually*
    spam. This means that we only consider the left hand column in the preceding table.
    The recall of the algorithm is the number of true positives among the actual positives,
    that is, the number of true positives divided by the true positives plus the false
    negatives. In this case, we have 22 true positives and 30 actual spam messages,
    so the recall of our algorithm is 22/30, or 73%.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的召回率或灵敏度是**真阳性率**，即我们在查看实际上是垃圾邮件的示例时正确猜测的次数。这意味着我们只考虑前面表格中的左侧列。算法的召回率是实际正性中的真阳性数，即真阳性除以真阳性和假阴性的总和。在这种情况下，我们有22个真阳性和30个实际的垃圾邮件消息，因此我们算法的召回率是22/30，或73%。
- en: The precision of the algorithm relates not to the messages that are *actually*
    spam, but instead to the messages that we *guessed* are spam. In this case, we
    only consider the top row, or the true positives divided by the sum of true positives
    and false positives; that is, the true positives divided by the guessed positives.
    In our case, there are 22 true positives and 26 total guessed positives, so our
    precision is 22/26, or 84%.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的精确度与实际是垃圾邮件的消息无关，而是与我们猜测是垃圾邮件的消息有关。在这种情况下，我们只考虑最上面一行，即真阳性除以真阳性和假阳性的总和；也就是说，真阳性除以猜测的正性。在我们的例子中，有22个真阳性和26个总猜测的正性，因此我们的精确度是22/26，或84%。
- en: Note that this algorithm is more precise than it is sensitive. This means that
    its spam guesses are 84% correct *when it guesses spam*, but the algorithm also
    has a tendency to lean towards guessing ham, and misses a good number of actual
    spam messages. Also note that the total accuracy is 88%, but both its precision
    and recall are lower than that figure.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个算法比它敏感。这意味着当它猜测垃圾邮件时，垃圾邮件的猜测是84%正确的，但该算法也有倾向于猜测正常邮件，并且会错过很多实际的垃圾邮件。此外，总准确率是88%，但它的精确度和召回率都低于这个数字。
- en: 'Another way to think about these performance metrics intuitively is as follows:
    precision is the algorithm''s ability to guess correctly when it guesses positive,
    but recall is the algorithm''s ability to remember what a spam message looks like.
    High precision and low recall would mean that an algorithm is very selective when
    guessing that a message is spam; the algorithm really needs to be convinced that
    a message is spam before identifying it as spam.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种直观地思考这些性能指标的方法如下：精确度（precision）是算法在猜测为正时正确猜测的能力，而召回率（recall）是算法记住垃圾邮件样式的记忆能力。高精确度和低召回率意味着算法在猜测邮件是垃圾邮件时非常挑剔；算法在将邮件识别为垃圾邮件之前，必须确信该邮件是垃圾邮件。
- en: The algorithm is very *precise* about saying that a message is spam.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在说一条消息是垃圾邮件时非常**精确**。
- en: It, therefore, might favor letting ham messages through at the cost of accidentally
    letting some spam through. A low-precision, high-recall algorithm, on the other
    hand, will tend to more aggressively identify messages as spam, however, it will
    also incorrectly block a number of ham messages (the algorithm better *recalls*
    what spam looks like, it is more *sensitive* to spam, therefore it thinks more
    messages are spam and will act accordingly).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它可能会牺牲一些垃圾邮件的误判，而让一些正常的邮件通过。另一方面，低精确度、高召回率的算法倾向于更积极地识别邮件为垃圾邮件，然而，它也会错误地阻止一些正常的邮件（该算法更好地“回忆”垃圾邮件的样子，对垃圾邮件更敏感，因此认为更多的邮件是垃圾邮件，并相应地采取行动）。
- en: Of course, some algorithms can have high accuracy, precision, and recall—but
    more realistically, the way you train your algorithms will involve trade-offs
    between precision and recall, and you must balance these trade-offs against the
    desired goals of your system.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一些算法可以具有高准确率、精确度和召回率——但更现实的是，你训练算法的方式将涉及精确度和召回率之间的权衡，你必须根据你系统的预期目标来平衡这些权衡。
- en: Supervised learning algorithms
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习算法
- en: 'Now that we have developed an understanding of accuracy, precision, and recall,
    we can continue with the topic at hand: supervised learning algorithms. The key
    distinction between supervised and unsupervised learning algorithms is the presence
    of prelabeled data, typically introduced during the training phase of the algorithm.
    A supervised learning algorithm should be able to learn from labeled training
    data and then analyze a new, unlabeled data point and guess that data''s label.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对准确率、精确率和召回率有了理解，我们可以继续讨论当前的主题：监督学习算法。监督学习和无监督学习算法之间的关键区别是有预标记数据的存在，通常在算法的训练阶段引入。一个监督学习算法应该能够从标记的训练数据中学习，然后分析一个新的、未标记的数据点并猜测该数据的标签。
- en: 'Supervised learning algorithms further divide into two subcategories: **classification**
    and **regression**. Classification algorithms aim to predict the label of an unseen
    data point, based on the generalized patterns that it has learned from the training
    data, as described previously. Regression algorithms aim to predict the value
    of a new point, again based on the generalized patterns that it has learned during
    training. While classification and regression feel different in practice, the
    preceding description betrays how similar the two categories actually are; the
    major distinction between the two is that regression algorithms typically work
    with continuous data, for instance, time-series or coordinate data. For the rest
    of this section, however, we will discuss only classification tasks.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法进一步分为两个子类别：**分类**和**回归**。分类算法旨在根据从训练数据中学到的泛化模式预测未见数据点的标签，如前所述。回归算法旨在预测新点的值，同样基于它在训练期间学到的泛化模式。虽然分类和回归在实践中感觉不同，但前面的描述揭示了这两个类别实际上是多么相似；两者之间的主要区别是回归算法通常处理连续数据，例如时间序列或坐标数据。然而，在本节的剩余部分，我们将仅讨论分类任务。
- en: Because the algorithm builds a model from labeled data, it is expected that
    the algorithm can generate *semantically* correct results, as opposed to the *statistically*
    correct results that unsupervised algorithms generate. A semantically correct
    result is a result that would hold up to external scrutiny, using the same techniques
    by which the training data was labeled. In a spam filter, a semantically correct
    result is a guess that the algorithm makes that a human would agree with.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因为算法是从标记数据中构建模型的，所以预期该算法可以生成*语义上*正确的结果，这与无监督算法生成的*统计上*正确的成果形成对比。一个语义上正确的成果是指能够经得起外部审查的结果，使用与训练数据标记相同的技巧。在垃圾邮件过滤器中，一个语义上正确的成果是算法做出的一个人类会同意的猜测。
- en: The ability to generate semantically correct results is enabled by the prelabeled
    training data. The training data itself represents the semantics of the problem,
    and is how the algorithm learns to generate its semantically correct results.
    Notice that this entire discussion—and the entire discussion of accuracy, precision,
    and recall—hinges on the ability to introduce externally validated information
    to the model. You can only know if an individual guess is correct if an external
    entity independently validates the result, and you can only teach an algorithm
    to make semantically correct guesses if an external entity has provided enough
    data points with their correct labels to train the algorithm on. You can think
    of the training data for a supervised learning algorithm as the source of truth
    from which all guesses originate.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 生成语义上正确结果的能力是由预标记的训练数据实现的。训练数据本身代表了问题的语义，这是算法学习生成其语义上正确结果的方式。请注意，整个讨论——以及准确率、精确率和召回率的整个讨论——都取决于向模型引入外部验证信息的能力。只有当外部实体独立验证结果时，你才能知道单个猜测是否正确，而且只有当外部实体提供了足够的数据点及其正确的标签来训练算法时，你才能教会算法做出语义上正确的猜测。你可以将监督学习算法的训练数据视为所有猜测起源的真理之源。
- en: While supervised learning algorithms may indeed seem like magic when they're
    working well, there are many potential pitfalls. Because the training data is
    of crucial importance to the algorithm, your results will only be as good as your
    training data and your training methods. Some noise in training data can often
    be tolerated, but if there is a source of systemic error in the training data,
    you will also have systemic errors in your results. These may be difficult to
    detect, since the validation of a model typically uses a subset of the training
    data that you set aside—the same data that has the systemic error is used to validate
    the model, so you will think that the model is running well!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当监督学习算法运行良好时，它们确实可能看起来像是魔法，但存在许多潜在的陷阱。因为训练数据对算法至关重要，你的结果将仅与你的训练数据和训练方法一样好。训练数据中的某些噪声通常可以容忍，但如果训练数据中存在系统性的错误来源，你的结果也将存在系统性的错误。这些可能很难检测，因为模型的验证通常使用你预留的训练数据的一个子集——包含系统错误的相同数据被用来验证模型，所以你会认为模型运行得很好！
- en: Another potential pitfall is not having enough training data. If the problem
    you're solving is highly-dimensional, you will need a correspondingly large amount
    of training data; the training data must be sufficient to actually present all
    of the various patterns to the machine learning algorithm. You wouldn't expect
    to train a spam filter on only 10 emails and also expect great results.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个潜在的陷阱是训练数据不足。如果你正在解决的问题高度多维，你需要相应的大量训练数据；训练数据必须足够，以便实际上向机器学习算法展示所有各种模式。你不应该期望只用10封邮件来训练垃圾邮件过滤器，并期望得到很好的结果。
- en: These factors often present a sort of startup cost to supervised learning. Some
    amount of investment needs to be made in procuring or generating an appropriate
    amount of and in the distribution of training examples. The training data typically,
    though not always, needs to be generated by human knowledge and evaluation. This
    can be costly, especially in the case of image processing and object detection,
    which generally need many labeled training examples. In a world where ML algorithms
    are becoming ever more accessible, the true competition lies in having the best
    data to work with.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些因素通常会给监督学习带来一种启动成本。在获取或生成适当数量的训练示例以及其分布方面，需要投入一定量的投资。通常情况下，尽管并非总是如此，训练数据需要通过人类知识和评估来生成。这可能很昂贵，尤其是在图像处理和目标检测的情况下，通常需要许多标记的训练示例。在一个机器学习算法变得越来越容易获取的世界里，真正的竞争在于拥有最好的数据来工作。
- en: In the case of our spam filter, the need for training data means that you cannot
    simply write and launch the spam filter. You'll also need to spend some time manually
    recording which emails are spam and ham (or have your users report this). Before
    deploying your spam filter, you should make sure that you have enough training
    data to both train and validate the algorithm with, and that could mean having
    to wait until you have hundreds or thousands of examples of spam messages flagged
    by a human.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们垃圾邮件过滤器的例子中，对训练数据的需求意味着你不仅需要编写和发布垃圾邮件过滤器，还需要花一些时间手动记录哪些邮件是垃圾邮件和正常邮件（或者让你的用户报告这一点）。在部署垃圾邮件过滤器之前，你应该确保你有足够的训练数据来训练和验证算法，这可能意味着你必须等到你有数百或数千个由人类标记的垃圾邮件示例。
- en: Assuming you have an appropriate amount of high-quality training data, it's
    also possible to mismanage the training process and cause bad results with good
    data. ML novices often believe that more training is categorically better, but
    this is not the case.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拥有适当数量的高质量训练数据，也可能在训练过程中管理不当，导致即使数据良好也会产生不良结果。机器学习新手常常认为更多的训练总是更好的，但这并不正确。
- en: 'There are two new concepts to introduce at this point: **bias** and **variance**.
    When training a ML model, your hope is that the model will learn the *general*
    attributes of the training data and be able to extrapolate from there. If an algorithm
    has made significant incorrect assumptions about the structure of the data, it
    can be said to be highly biased and therefore *underfitted*. On the other hand,
    a model can demonstrate high variance, or a high sensitivity to small differences
    in training data. This is called **overfitting**, and can be thought of as the
    algorithm learning to identify individual examples, or the specific noise in individual
    examples, rather than the general trend of the data.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，需要介绍两个新的概念：**偏差**和**方差**。在训练机器学习模型时，你的希望是模型能够学习训练数据的**一般**属性，并能够从中进行外推。如果一个算法对数据的结构做出了重大错误的假设，可以说它具有高度的偏差，因此**欠拟合**。另一方面，一个模型可以表现出高方差，即对训练数据中的微小差异高度敏感。这被称为**过拟合**，可以理解为算法学习识别个别示例，或者个别示例中的特定噪声，而不是数据的总体趋势。
- en: 'Overtraining models can easily lead to overfitting. Imagine that you use the
    same keyboard every day for 10 years, but the keyboard is actually a strange model
    with an odd layout and lots of quirks. It''s to be expected that you''d become
    very good at typing on such a keyboard after so much time. Then, unexpectedly,
    the keyboard breaks and you get a new standard keyboard only to find that you
    have no idea how to type on it! The muscle memory that you''ve trained over a
    decade of typing is used to the period key being *just so*, the letter *o* being
    shifted off a little further to the right, and so on. When using the new keyboard,
    you find that you can''t type a single word without a typo. A decade of overtraining
    on a bad keyboard has only taught you how to type *on that keyboard*, and you
    haven''t been able to generalize your skill to other keyboards. Overfitting a
    model is the same concept: your algorithm gets very good at identifying your training
    data *and nothing else.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 过度训练模型很容易导致过拟合。想象一下，你每天使用同一台键盘10年，但实际上这台键盘是一个布局奇特、有很多怪癖的奇怪模型。在这么长时间后，你能够非常熟练地在这样的键盘上打字是可以预料的。然后，出乎意料的是，键盘坏了，你得到了一台新的标准键盘，却发现你不知道如何在上面打字！你经过十年打字训练的肌肉记忆已经习惯了键盘上的标点符号位置恰到好处，字母“o”稍微向右偏移一点，等等。在使用新键盘时，你会发现你打不出一个没有错别字的单词。十年在糟糕键盘上的过度训练只教会了你如何在那个键盘上打字，而你并没有将你的技能推广到其他键盘上。模型过拟合的概念与此相同：你的算法非常擅长识别你的训练数据，而无法识别其他任何数据。
- en: 'For this reason, training a model is not as simple as plugging in training
    data and letting the algorithm train for an arbitrary amount of time. One crucial
    step in the process is to divide your training data into two parts: one set for
    training the algorithm, and another part used *only* to validate the results of
    your model. You should not train the algorithm on your validation data, because
    you run the risk of training the model on how to identify your validation data,
    rather than training it and then using the validation data to independently verify
    the accuracy of your algorithm. The need for a validation set increases the cost
    of generating training data. If you determine that you need 1,000 examples to
    train your algorithm on, you may actually need to generate 1,500 examples in total
    in order to have a reasonable validation set.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练一个模型并不像插入训练数据然后让算法任意时间训练那样简单。在这个过程中，一个关键步骤是将你的训练数据分成两部分：一部分用于训练算法，另一部分仅用于验证模型的结果。你不应该在验证数据上训练算法，因为这样你可能会训练模型去识别你的验证数据，而不是训练后再使用验证数据独立验证算法的准确性。需要验证集会增加生成训练数据的成本。如果你确定你需要1,000个示例来训练你的算法，你可能实际上需要生成总共1,500个示例，以便有一个合理的验证集。
- en: Validation data is not just used to test the overall accuracy of the algorithm.
    You also often use validation data to determine when to *stop* training. During
    the training process, you should periodically test the algorithm with your validation
    data. Over time you will find that the accuracy of the validation will increase,
    as expected, and then at a certain point the validation accuracy may actually
    *decrease.* This change in direction is the point at which your model has begun
    overfitting your training data. The algorithm will always continue to get more
    accurate when you present it with an example from your training set (those are
    the examples it's learning directly), but once the model begins to overfit the
    training data, it'll begin to lose the ability to generalize and therefore perform
    worse—not better—with data it has not been trained on. For this reason, maintaining
    an independent set of validation data is crucial. If you ever train an algorithm
    and it has 100% accuracy when testing its own training data, the odds are you've
    overfitted the data and it will likely perform very poorly on unseen data. The
    algorithm has gone past learning the general trends in the data and is starting
    to memorize specific examples, including the various bits of noise in the data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据不仅仅用于测试算法的整体准确性。你通常还会使用验证数据来确定何时*停止*训练。在训练过程中，你应该定期使用你的验证数据测试算法。随着时间的推移，你会发现验证的准确性会如预期地增加，然后在某个时刻，验证的准确性可能会实际上*下降*。这种方向的变化就是你的模型开始对你的训练数据过拟合的点。当你向算法展示训练集中的例子（这些是它直接学习的例子）时，算法总是会继续变得更加准确，但一旦模型开始对训练数据过拟合，它就会开始失去泛化的能力，因此在它未训练过的数据上表现会更差——而不是更好。因此，维护一个独立的验证数据集至关重要。如果你训练了一个算法，并且在测试自己的训练数据时它达到了100%的准确率，那么你很可能已经过拟合了数据，并且它在未见过的数据上可能表现非常糟糕。算法已经超越了学习数据中的普遍趋势，开始记住具体的例子，包括数据中的各种噪声。
- en: Aside from maintaining a validation set, proper preprocessing of your data will
    also combat overfitting. The various noise reduction, feature selection, feature
    extraction, and dimensionality reduction techniques we discussed in [Chapter 2](94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml)*,
    Data Exploration*, will all serve to help generalize your model and avoid overfitting.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了维护一个验证集之外，适当的数据预处理也会对抗过拟合。我们在[第2章](94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml)*数据探索*中讨论的各种噪声减少、特征选择、特征提取和降维技术都将有助于泛化你的模型并避免过拟合。
- en: Finally, because the semantic correctness of your algorithm's inferences can
    only be determined by an external source, it's often impossible to know whether
    a guess is actually correct (unless you receive user feedback on a specific guess).
    At best, you can only infer from the precision, recall, and accuracy values you've
    calculated during your training and validation stage what the overall effectiveness
    of the algorithm is. Fortunately, many supervised learning algorithms present
    their results in a probabilistic manner (for example, *I think there's a 92% chance
    this is spam*), so you can have some indication of the algorithm's confidence
    in an inference, however, when you combine this confidence level with the precision
    and recall of the model and the fact that your training data may have systemic
    errors, even the confidence level that comes with an inference is questionable.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，因为你的算法推断的语义正确性只能由外部来源确定，所以通常无法知道一个猜测是否实际上正确（除非你收到关于特定猜测的用户反馈）。最好的情况是，你只能从你在训练和验证阶段计算出的精确度、召回率和准确率值中推断出算法的整体有效性。幸运的是，许多监督学习算法以概率方式呈现他们的结果（例如，*我认为有92%的可能性这是垃圾邮件*），这样你可以对算法在推断上的信心有所了解。然而，当你将这种置信水平与模型的精确度和召回率以及你的训练数据可能存在的系统性错误结合起来时，即使是推断带来的置信水平也是值得怀疑的。
- en: Despite these potential pitfalls, supervised learning is a very powerful technique.
    The ability to extrapolate from only a few thousand training examples in a complex
    problem domain and quickly make inferences on millions of unseen data points is
    both impressive and highly valuable.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些潜在的陷阱，监督学习是一个非常强大的技术。从复杂问题域中只有几千个训练示例中推断出，并快速对数百万未见过的数据点进行推断的能力既令人印象深刻又非常有价值。
- en: As with unsupervised learning, there are many types of supervised learning algorithms,
    each with their own strengths and weaknesses. Neural networks, Bayesian classifiers,
    k-nearest neighbor, decision trees, and random forests are all examples of supervised
    learning techniques.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与无监督学习一样，监督学习算法也有很多种类型，每种都有其自身的优点和缺点。神经网络、贝叶斯分类器、k-最近邻、决策树和随机森林都是监督学习技术的例子。
- en: Reinforcement learning
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: While supervised and unsupervised learning are the two primary subclassifications
    of machine learning algorithms, they are in fact part of a spectrum and there
    are other modes of learning. The next most significant learning mode in the context
    of this book is reinforcement learning, which in some ways can be considered a
    hybrid of supervised and unsupervised learning; however, most would categorize
    reinforcement learning as an unsupervised learning algorithm. This is one of those
    cases where the taxonomy becomes a little vague!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然监督学习和无监督学习是机器学习算法的两个主要子分类，但实际上它们是光谱的一部分，还有其他的学习模式。在本书的背景下，下一个最重要的学习模式是强化学习，它在某些方面可以被认为是监督学习和无监督学习的混合体；然而，大多数人会将强化学习归类为无监督学习算法。这就是分类变得有些模糊的那些情况之一！
- en: In unsupervised learning, almost nothing is known about the data to be processed
    and the algorithm must infer patterns from a blank slate. In supervised learning,
    significant resources are dedicated to training the algorithm on known examples.
    In reinforcement learning, *something* is known (or can be known) about the data,
    but the knowledge of the data is not an explicit labeling or a categorization.
    Instead, the *something* that is known (or can be known) is the result of an action
    based on a decision made with the data. Reinforcement learning is considered by
    many to be an unsupervised learning algorithm, because the algorithm *starts from
    scratch*, however reinforcement also *closes the loop* and continually retrains
    itself based on its own actions, which has some similarities to training in supervised
    learning.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，几乎对要处理的数据一无所知，算法必须从一张白纸中推断出模式。在监督学习中，大量的资源被用于在已知示例上训练算法。在强化学习中，关于数据的信息（或可以知道的信息）是已知的（或可以知道的），但数据的知识并不是一个明确的标签或分类。相反，已知（或可以知道）的信息是基于使用数据做出的决策采取行动的结果。强化学习被许多人视为无监督学习算法，因为算法是从零开始的，然而强化学习也“闭合循环”，并基于自己的行动不断重新训练自己，这有一些类似于监督学习中的训练。
- en: 'To use an absurd and contrived example, imagine that you''re writing an algorithm
    that is supposed to replace the function of government. The algorithm will receive
    as its input the current state of affairs of the country and must, as an output,
    develop new policies and laws in order to optimize the country in many dimensions:
    citizen happiness, economic health, low crime, and so on. The reinforcement learning
    approach to this problem starts from scratch, knowing nothing about how its laws
    and policies will affect the country. The algorithm then implements a law or set
    of laws; because it has just started, the law it implements will be completely
    arbitrary. After the law has taken some time to go into effect and make its impact
    on society, the algorithm will once again read the state of affairs of the country
    and may discover that it has turned the country into a chaotic wasteland. The
    algorithm learns from this feedback, adjusts itself, and implements a new set
    of laws. Over time, and using the initial laws it implements as experiments, the
    algorithm will come to understand the cause and effect of its policies and begin
    to optimize. Given enough time, this approach may develop a near-perfect society—if
    it doesn''t accidentally destroy the society with its initial failed experiments.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举一个荒谬且牵强的例子，想象你正在编写一个算法，该算法旨在取代政府的功能。该算法将接收国家的当前状况作为输入，并且必须作为输出，制定新的政策和法律，以优化国家在多个维度上的表现：公民幸福、经济健康、低犯罪率等等。强化学习对这一问题的处理是从零开始，对它的法律和政策将如何影响国家一无所知。然后，算法实施一项或一系列法律；因为它刚刚开始，实施的法律将是完全随机的。在法律实施一段时间并对其社会产生影响后，算法将再次阅读国家的状况，可能会发现它已经将国家变成了一个混乱的荒地。算法从这种反馈中学习，调整自己，并实施一套新的法律。随着时间的推移，并使用它最初实施的法律作为实验，算法将开始理解其政策的因果关系，并开始优化。如果给足够的时间，这种方法可能会发展出一个近乎完美的社会——前提是它不会因为最初的失败实验而意外地破坏社会。
- en: Reinforcement learning techniques are distinct from supervised and unsupervised
    algorithms in that they directly interact with their environment and monitor the
    effects of their decisions in order to update their models. Rather than aiming
    to detect patterns or to classify data, most reinforcement learning aims to optimize
    some cost or reward within an environment. The environment in question can either
    be a real-world environment, as is often the case in the field of control systems,
    or it can be a virtual environment, as is the case with genetic algorithms. In
    either case, the algorithm must have some way of characterizing an overall *cost*/*penalty*
    or *reward*, and will work to optimize that value. Reinforcement learning is an
    important optimization technique, especially in highly dimensional problem spaces,
    since a brute-force trial-and-error approach is often impossible to achieve in
    a reasonable amount of time.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习技术与监督和无监督算法不同，它们直接与环境互动并监控其决策的影响，以便更新其模型。强化学习的目标不是检测模式或对数据进行分类，而是优化环境中的某些成本或奖励。相关环境可以是现实世界环境，如控制系统领域常见的情况，也可以是虚拟环境，如遗传算法的情况。在两种情况下，算法都必须有一种方法来表征整体的*成本*/*惩罚*或*奖励*，并努力优化该值。强化学习是一种重要的优化技术，特别是在高维问题空间中，因为
    brute-force trial-and-error 方法通常无法在合理的时间内实现。
- en: Examples of reinforcement learning algorithms include genetic algorithms, which
    we will discuss in depth in a later chapter, Monte Carlo methods, and gradient
    descent (which we will discuss alongside neural networks).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法的例子包括遗传算法，我们将在后面的章节中深入讨论，还有蒙特卡洛方法和梯度下降（我们将与神经网络一起讨论）。
- en: Categories of algorithms
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法分类
- en: We've categorized ML algorithms by their learning mode, but that's not the only
    way to categorize algorithms. Another approach is to categorize them by their
    task or function. In this section we will briefly present the basic functions
    of ML algorithms and name some example algorithms.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经根据学习模式对机器学习算法进行了分类，但这并不是唯一分类算法的方法。另一种方法是按任务或功能对它们进行分类。在本节中，我们将简要介绍机器学习算法的基本功能并列举一些示例算法。
- en: Clustering
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: Clustering algorithms aim to identify groups of data points that are similar
    to one another. The definition of *similar* depends on the type of data, the problem
    domain, and the algorithm used. The easiest way to intuitively understand clustering
    algorithms is to visualize points on an *x/y* grid. A clustering algorithm's aim
    is typically to draw circles around groups of similar points; each set of circled
    points is taken to be a cluster. The clusters are generally not known beforehand,
    so clustering algorithms are generally classified as unsupervised learning problems.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法旨在识别彼此相似的数据点组。*相似*的定义取决于数据的类型、问题域和使用的算法。直观理解聚类算法的最简单方法是可视化*x/y*网格上的点。聚类算法的目标通常是围绕相似点组画圆；每个画圈的点集被视为一个簇。簇通常事先未知，因此聚类算法通常被归类为无监督学习问题。
- en: 'Some examples of clustering algorithms include:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法的一些例子包括：
- en: k-means, and variants such as k-medians
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means，以及其变体如k-medians
- en: Gaussian mixture models
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: Mean-shift
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值漂移
- en: Classification
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: 'Classification is a very broad (and very popular) category of supervised learning
    algorithms, with the goal of trying to identify a data point as belonging to some
    classification (spam or ham; male or female; animal, mineral or vegetable, and
    so on). A multitude of algorithms for classification exists, including:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是监督学习算法一个非常广泛（也非常受欢迎）的类别，其目标是尝试识别一个数据点属于某个分类（垃圾邮件或正常邮件；男性或女性；动物、矿物或植物等）。存在许多用于分类的算法，包括：
- en: k-nearest neighbor
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-最近邻
- en: Logistic regression
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Naive Bayes classifier
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单贝叶斯分类器
- en: Support Vector Machines
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: (Most) neural networks
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （大多数）神经网络
- en: Decision trees
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Random forests
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Regression
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归
- en: 'Regression algorithms aim to determine and characterize the relationship between
    variables. In the most simple case of two-dimensional linear regression, the algorithm''s
    goal is to determine the line that can be drawn most closely through a set of
    points, however, higher-degree and higher-dimensional regressions can generate
    significant insights and make predictions concerning complex data. Because these
    algorithms necessarily require known data points, they are considered to be supervised
    learning algorithms. Some examples:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 回归算法旨在确定和描述变量之间的关系。在最简单的二维线性回归案例中，算法的目标是确定一条可以通过一组点的线，然而，更高阶和更高维的回归可以产生重要的见解并预测复杂数据。因为这些算法必然需要已知的数据点，所以它们被认为是监督学习算法。一些例子包括：
- en: Linear regression
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Polynomial regression
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式回归
- en: Bayesian linear regression
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯线性回归
- en: Least absolute deviation
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小绝对偏差
- en: Dimensionality reduction
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度降低
- en: Dimensionality reduction is a family of techniques whose purpose is to convert
    data with a high number of dimensions into data with a lower number of dimensions.
    Used as a general term, this can mean either discarding dimensions entirely (such
    as feature selection), or to create new individual dimensions that simultaneously
    represent multiple original dimensions, with some loss of resolution (such as feature
    extraction).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 维度降低是一系列技术，其目的是将高维数据转换为低维数据。作为一个通用术语，这可以意味着完全丢弃维度（例如特征选择），或者创建新的单个维度，同时代表多个原始维度，但会损失一些分辨率（例如特征提取）。
- en: 'Some algorithms that can be used for dimensionality reduction include:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一些可用于维度降低的算法包括：
- en: Various types of regressions
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种类型的回归
- en: PCA
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: Image transformations (for example, converting an image to grayscale)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像变换（例如，将图像转换为灰度）
- en: Stemming and lemmatization (in natural language processing)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取和词形还原（在自然语言处理中）
- en: Optimization
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化
- en: 'Optimization algorithms have the goal of selecting a set of parameters, or
    the values for a set of parameters, such that the cost or error of a system is
    minimized (alternatively, such that the reward of a system is maximized). Feature
    selection and feature extraction is actually a form of optimization; you are modifying
    parameters with the purpose of reducing dimensionality while preserving important
    data. In the most basic optimization technique, a brute-force search, you simply
    try every possible combination of parameters and select the combination with the
    best results. In practice, most problems are complex enough that a brute-force
    search may take an unreasonable amount of time (that is, millions of years on
    a modern computer). Some optimization techniques include:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 优化算法的目标是选择一组参数，或者一组参数的值，使得系统的成本或误差最小化（或者，使得系统的奖励最大化）。特征选择和特征提取实际上是一种优化形式；你是在修改参数，目的是在保留重要数据的同时降低维度。在最基本的优化技术中，穷举搜索，你只需尝试所有可能的参数组合，并选择结果最好的组合。在实践中，大多数问题都足够复杂，以至于穷举搜索可能需要不合理的时间（即在现代计算机上可能需要数百万年）。一些优化技术包括：
- en: A brute force search (also known as an *exhaustive search*)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 穷举搜索（也称为穷尽搜索）
- en: Gradient descent
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Simulated annealing
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟退火
- en: Genetic algorithms
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遗传算法
- en: Natural language processing
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: '**Natural language processing** (**NLP**) is an entire field on its own and
    contains many techniques that are not considered in machine learning. However,
    NLP is often used in concert with ML algorithms, as the two fields combined are
    necessary to achieve generalized artificial intelligence. Many ML classification
    algorithms operate on text rather than numbers (such as our spam filter), and
    in those situations, we rely on techniques from the field of NLP: stemming, in
    particular, is a quick and easy dimensionality reduction technique for text classifiers.
    Some NLP techniques relevant to ML include:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）是一个独立的领域，包含许多在机器学习中不被考虑的技术。然而，NLP通常与ML算法结合使用，因为这两个领域的结合是实现通用人工智能所必需的。许多ML分类算法在文本上操作而不是在数字上（例如我们的垃圾邮件过滤器），在这些情况下，我们依赖于NLP领域的技巧：特别是词干提取是一种快速且简单的文本分类器的维度降低技术。与ML相关的某些NLP技术包括：'
- en: Tokenization
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词
- en: String distance
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串距离
- en: Stemming or lemmatization
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取或词形还原
- en: TF-IDF
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Image processing
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像处理
- en: 'Like NLP, image processing is its own field of study that has overlapped with
    ML but is not fully encompassed by ML. As with NLP, we may often use image processing
    techniques to reduce dimensionality before applying an ML algorithm to an image.
    Some image processing techniques relevant to machine learning include:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与自然语言处理类似，图像处理是一个独立的研究领域，它与机器学习有重叠，但并不完全包含在机器学习之中。与自然语言处理一样，我们可能经常使用图像处理技术来降低图像的维度，然后再将机器学习算法应用于图像。一些与机器学习相关的图像处理技术包括：
- en: Edge detection
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边缘检测
- en: Scale invariant transformations
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尺度不变变换
- en: Color space transformations
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 颜色空间转换
- en: Object detection
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测
- en: Recurrent neural networks
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Summary
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we've discussed the various ways we can categorize machine
    learning techniques. In particular, we discussed the difference between unsupervised
    learning, supervised learning, and reinforcement learning, presenting various
    examples of each.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了我们可以如何对机器学习技术进行分类。特别是，我们讨论了无监督学习、监督学习和强化学习之间的区别，并展示了每个类别的各种示例。
- en: We also discussed different ways to judge the accuracy of machine learning algorithms,
    in particular, the concepts of accuracy, precision, and recall as applied to supervised
    learning techniques. We also discussed the importance of the training step in
    supervised learning algorithms, and illustrated the concepts of bias, variance,
    generalization, and overfitting.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了判断机器学习算法准确性的不同方法，特别是将准确率、精确率和召回率等概念应用于监督学习技术。我们还讨论了监督学习算法中训练步骤的重要性，并阐述了偏差、方差、泛化和过拟合的概念。
- en: Finally, we looked at how machine learning algorithms can be categorized not
    by learning mode but instead by task or technique, and presented a number of algorithms
    that fit into the categories of clustering, classification, regression, dimensionality
    reduction, natural language processing, and image processing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了机器学习算法可以根据任务或技术而不是学习模式进行分类，并介绍了一系列适合于聚类、分类、回归、降维、自然语言处理和图像处理类别的算法。
- en: In the next chapter, we'll get our hands dirty and take a deep dive into clustering
    algorithms.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨聚类算法。
