<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch11" class="calibre6"/>Chapter 11. Reconstructing 3D Scenes</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Calibrating a camera</li><li class="listitem">Recovering camera pose</li><li class="listitem">Reconstructing a 3D scene from calibrated cameras</li><li class="listitem">Computing depth from stereo image</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch11lvl1sec66" class="calibre6"/>Introduction</h1></div></div></div><p class="calibre8">We learned in the previous chapter how a camera captures a 3D scene by projecting light rays on a 2D sensor plane. The image produced is an accurate representation of what the scene looks like from a particular point of view, at the instant the image was captured. However, by its nature, the process of image formation eliminates all information concerning the depth of the represented scene elements. This chapter will teach how, under specific conditions, the 3D structure of the scene and the 3D pose of the cameras that captured it, can be recovered. We will see how a good understanding of projective geometry concepts allows us to devise methods that enable 3D reconstruction. We will therefore revisit the principle of image formation introduced in the previous chapter; in particular, we will now take into consideration that our image is composed of pixels.</p><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec198" class="calibre6"/>Digital image formation</h2></div></div></div><p class="calibre8">Let's now redraw a new version of the figure shown in 
Chapter 10
, <em class="calibre16">Estimating Projective Relations in Images</em>, describing the pin-hole camera model. More specifically, we want to demonstrate the relation between a point in 3D at position <code class="literal">(X,Y,Z)</code> and its image <code class="literal">(x,y)</code> on a camera specified in pixel coordinates:</p><p class="calibre8">
</p><div><img alt="Digital image formation" src="img/image_11_001.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Notice the changes that have been made to the original figure. First, we added a reference frame that we positioned at the center of the projection. Second, we have the <code class="literal">Y</code>-axis pointing downward to get a coordinate system compatible with the usual convention that places the image origin in the upper-left corner of the image. Finally, we also identified a special point on the image plane: considering the line coming from the focal point that is orthogonal to the image plane, the point <code class="literal">(u0,v0)</code> is the pixel position at which this line pierces the image plane. This point is called the <strong class="calibre15">principal point</strong>. It could be logical to assume that this principal point is at the center of the image plane, but in practice, this one might be off by a few pixels, depending on the precision with which the camera has been manufactured.</p><p class="calibre8">In the previous chapter, we learned that the essential parameters of a camera in the pin-hole model are its focal length and the size of the image plane (which defines the field of view of the camera). In addition, since we are dealing with digital images, the number of pixels on the image plane (its resolution) is another important characteristic of a camera. We also learned previously that a 3D point <code class="literal">(X,Y,Z)</code> will be projected onto the image plane at <code class="literal">(fX/Z,fY/Z)</code>.</p><p class="calibre8">Now, if we want to translate this coordinate into pixels, we need to divide the 2D image position by the pixel width (<code class="literal">px</code>) and height (<code class="literal">py</code>), respectively. We notice that by dividing the focal length given in world units (generally given in millimeters) by <code class="literal">px,</code> we obtain the focal length expressed in (horizontal) pixels. Let's define this term, then, as <code class="literal">fx</code>. Similarly, <code class="literal">fy =f/py</code> is defined as the focal length expressed in vertical pixel units. The complete projective equation is therefore as follows:</p><p class="calibre8">
</p><div><img alt="Digital image formation" src="img/B05388_11_15.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Recall that <code class="literal">(u0,v0)</code> is the principal point that is added to the result in order to move the origin to the upper-left corner of the image. Note also that the physical size of a pixel can be obtained by dividing the size of the image sensor (generally in millimeters) by the number of pixels (horizontally or vertically). In modern sensors, pixels are generally square, that is, they have the same horizontal and vertical size.</p><p class="calibre8">The preceding equations can be rewritten in matrix form as we did in 
Chapter 10
, <em class="calibre16">Estimating Projective Relations in Images</em>. Here is the complete projective equation in its most general form:</p><p class="calibre8">
</p><div><img alt="Digital image formation" src="img/B05388_11_16.jpg" class="calibre17"/></div><p class="calibre8">
</p></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch11lvl1sec67" class="calibre6"/>Calibrating a camera</h1></div></div></div><p class="calibre8">Camera calibration is the process by which the different camera parameters (that is, the ones appearing in the projective equation) are obtained. One can obviously use the specifications provided by the camera manufacturer, but for some tasks, such as 3D reconstruction, these specifications are not accurate enough. By undertaking an appropriate camera calibration step, accurate calibration information can be obtained.</p><p class="calibre8">An active camera calibration procedure will proceed by showing known patterns to the camera and analyzing the obtained images. An optimization process will then determine the optimal parameter values that explain the observations. This is a complex process that has been made easy by the availability of OpenCV calibration functions.</p><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec199" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">To calibrate a camera, the idea is to show it a set of scene points for which their 3D positions are known. Then, you need to observe where these points project on the image. With the knowledge of a sufficient number of 3D points and associated 2D image points, the exact camera parameters can be inferred from the projective equation. Obviously, for accurate results, we need to observe as many points as possible. One way to achieve this would be to take one picture of a scene with many known 3D points, but in practice, this is rarely feasible. A more convenient way is to take several images of a set of some 3D points from different viewpoints. This approach is simpler but requires you to compute the position of each camera view in addition to the computation of the internal camera parameters, which, fortunately, is feasible.</p><p class="calibre8">OpenCV proposes that you use a chessboard pattern to generate the set of 3D scene points required for calibration. This pattern creates points at the corners of each square, and since this pattern is flat, we can freely assume that the board is located at <code class="literal">Z=0</code>, with the <code class="literal">X</code> and <code class="literal">Y</code> axes well-aligned with the grid.</p><p class="calibre8">In this case, the calibration process simply consists of showing the chessboard pattern to the camera from different viewpoints. Here is one example of a calibration pattern image made of <code class="literal">7x5</code> inner corners as captured during the calibration step:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/B05388_11_02.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The good thing is that OpenCV has a function that automatically detects the corners of this chessboard pattern. You simply provide an image and the size of the chessboard used (the number of horizontal and vertical inner corner points). The function will return the position of these chessboard corners on the image. If the function fails to find the pattern, then it simply returns <code class="literal">false</code>:</p><pre class="programlisting">    // output vectors of image points 
    std::vector&lt;cv::Point2f&gt; imageCorners; 
    // number of inner corners on the chessboard 
    cv::Size boardSize(7,5); 
    // Get the chessboard corners 
    bool found = cv::findChessboardCorners( 
                         image,         // image of chessboard pattern 
                         boardSize,     // size of pattern 
                         imageCorners); // list of detected corners 
</pre><p class="calibre8">The output parameter, <code class="literal">imageCorners</code>, will simply contain the pixel coordinates of the detected inner corners of the shown pattern. Note that this function accepts additional parameters if you need to tune the algorithm, which is not discussed here. There is also a special function that draws the detected corners on the chessboard image, with lines connecting them in a sequence:</p><pre class="programlisting">    // Draw the corners 
    cv::drawChessboardCorners(image, boardSize,  
                      imageCorners, found); // corners have been found 
</pre><p class="calibre8">The following image is obtained:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/B05388_11_03.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The lines that connect the points show the order in which the points are listed in the vector of detected image points. To perform a calibration, we now need to specify the corresponding 3D points. You can specify these points in the units of your choice (for example, in centimeters or in inches); however, the simplest thing to do is to assume that each square represents one unit. In that case, the coordinates of the first point would be <code class="literal">(0,0,0)</code> (assuming that the board is located at a depth of <code class="literal">Z=0</code>), the coordinates of the second point would be <code class="literal">(1,0,0),</code> and so on, the last point being located at <code class="literal">(6,4,0)</code>. There is a total of <code class="literal">35</code> points in this pattern, which is too small to obtain an accurate calibration. To get more points, you need to show more images of the same calibration pattern from various points of view. To do so, you can either move the pattern in front of the camera or move the camera around the board; from a mathematical point of view, this is completely equivalent. The OpenCV calibration function assumes that the reference frame is fixed on the calibration pattern and will calculate the rotation and translation of the camera with respect to the reference frame.</p><p class="calibre8">Let's now encapsulate the calibration process in a <code class="literal">CameraCalibrator</code> class. The attributes of this class are as follows:</p><pre class="programlisting">    class CameraCalibrator { 
 
      // input points: 
      // the points in world coordinates 
      // (each square is one unit) 
      std::vector&lt;std::vector&lt;cv::Point3f&gt;&gt; objectPoints; 
      // the image point positions in pixels 
      std::vector&lt;std::vector&lt;cv::Point2f&gt;&gt; imagePoints; 
      // output Matrices 
      cv::Mat cameraMatrix; 
      cv::Mat distCoeffs; 
      // flag to specify how calibration is done 
      int flag; 
</pre><p class="calibre8">Note that the input vectors of the scene and image points are in fact made of <code class="literal">std::vector</code> of point instances; each vector element is a vector of the points from one view. Here, we decided to add the calibration points by specifying a vector of the chessboard image filename as input; the method will take care of extracting the point coordinates from these images:</p><pre class="programlisting">    // Open chessboard images and extract corner points 
    int CameraCalibrator::addChessboardPoints(      
        const std::vector&lt;std::string&gt; &amp; filelist, // list of filenames 
        cv::Size &amp; boardSize) {   // calibration board size 
 
      // the points on the chessboard 
      std::vector&lt;cv::Point2f&gt; imageCorners; 
      std::vector&lt;cv::Point3f&gt; objectCorners; 
 
      // 3D Scene Points: 
      // Initialize the chessboard corners  
      // in the chessboard reference frame 
      // The corners are at 3D location (X,Y,Z)= (i,j,0) 
      for (int i=0; i&lt;boardSize.height; i++) { 
        for (int j=0; j&lt;boardSize.width; j++) { 
          objectCorners.push_back(cv::Point3f(i, j, 0.0f)); 
        } 
      } 
 
      // 2D Image points: 
      cv::Mat image; //to contain chessboard image 
      int successes = 0; 
      // for all viewpoints 
      for (int i=0; i&lt;filelist.size(); i++) { 
 
        // Open the image 
        image = cv::imread(filelist[i],0); 
 
        // Get the chessboard corners 
        bool found = cv::findChessboardCorners( 
                         image,         // image of chessboard pattern  
                         boardSize,     // size of pattern 
                         imageCorners); // list of detected corners 
 
        // Get subpixel accuracy on the corners 
        if (found) { 
          cv::cornerSubPix(image, imageCorners,  
               cv::Size(5, 5), // half size of serach window 
               cv::Size(-1, -1),  
               cv::TermCriteria( cv::TermCriteria::MAX_ITER +   
                   cv::TermCriteria::EPS, 30, // max number of iterations 
                   0.1));                     // min accuracy 
 
          // If we have a good board, add it to our data 
          if (imageCorners.size() == boardSize.area()) { 
            //Add image and scene points from one view 
            addPoints(imageCorners, objectCorners); 
            successes++; 
          } 
        } 
 
        // If we have a good board, add it to our data 
        if (imageCorners.size() == boardSize.area()) { 
          //Add image and scene points from one view 
          addPoints(imageCorners, objectCorners); 
          successes++; 
        } 
      } 
      return successes; 
    } 
</pre><p class="calibre8">The first loop inputs the 3D coordinates of the chessboard, and the corresponding image points are the ones provided by the <code class="literal">cv::findChessboardCorners</code> function. This is done for all the available viewpoints. Moreover, in order to obtain a more accurate image point location, the <code class="literal">cv::cornerSubPix</code> function can be used; and as the name suggests, the image points will then be localized with subpixel accuracy. The termination criterion that is specified by the <code class="literal">cv::TermCriteria</code> object defines the maximum number of iterations and the minimum accuracy in subpixel coordinates. The first of these two conditions that is reached will stop the corner refinement process.</p><p class="calibre8">When a set of chessboard corners have been successfully detected, these points are added to our vectors of image and scene points using our <code class="literal">addPoints</code> method. Once a sufficient number of chessboard images have been processed (and consequently, a large number of 3D scene point/2D image point correspondences are available), we can initiate the computation of the calibration parameters as follows:</p><pre class="programlisting">    // Calibrate the camera 
    // returns the re-projection error 
    double CameraCalibrator::calibrate(cv::Size &amp;imageSize) { 
      // Output rotations and translations 
      std::vector&lt;cv::Mat&gt; rvecs, tvecs; 
 
      // start calibration 
      return 
        calibrateCamera(objectPoints,  // the 3D points 
                        imagePoints,   // the image points 
                        imageSize,     // image size 
                        cameraMatrix,  // output camera matrix 
                        distCoeffs,    // output distortion matrix 
                        rvecs, tvecs,  // Rs, Ts  
                        flag);         // set options 
    } 
</pre><p class="calibre8">In practice, <code class="literal">10</code> to <code class="literal">20</code> chessboard images are sufficient, but these must be taken from different viewpoints at different depths. The two important outputs of this function are the camera matrix and the distortion parameters. These will be described in the next section.</p></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec200" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">In order to explain the result of the calibration, we need to go back to the projective equation presented in the introduction of this chapter. This equation describes the transformation of a 3D point into a 2D point through the successive application of two matrices. The first matrix includes all of the camera parameters, which are called the intrinsic parameters of the camera. This <code class="literal">3x3</code> matrix is one of the output matrices returned by the <code class="literal">cv::calibrateCamera</code> function. There is also a function called <code class="literal">cv::calibrationMatrixValues</code> that explicitly returns the value of the intrinsic parameters given by a calibration matrix.</p><p class="calibre8">The second matrix is there to have the input points expressed into camera-centric coordinates. It is composed of a rotation vector (a <code class="literal">3x3</code> matrix) and a translation vector (a <code class="literal">3x1</code> matrix). Remember that in our calibration example, the reference frame was placed on the chessboard. Therefore, there is a rigid transformation (made of a rotation component represented by the matrix entries <code class="literal">r1</code> to <code class="literal">r9</code> and a translation represented by <code class="literal">t1</code>, <code class="literal">t2</code>, and <code class="literal">t3</code>) that must be computed for each view. These are in the output parameter list of the <code class="literal">cv::calibrateCamera</code> function. The rotation and translation components are often called the <strong class="calibre15">extrinsic parameters</strong> of the calibration, and they are different for each view. The intrinsic parameters remain constant for a given camera/lens system.</p><p class="calibre8">The calibration results provided by the <code class="literal">cv::calibrateCamera</code> are obtained through an optimization process. This process aims to find the intrinsic and extrinsic parameters that minimize the difference between the predicted image point position, as computed from the projection of the 3D scene points, and the actual image point position, as observed on the image. The sum of this difference for all the points specified during the calibration is called the <strong class="calibre15">re-projection error</strong>.</p><p class="calibre8">The intrinsic parameters of our test camera obtained from a calibration based on <code class="literal">27</code> chessboard images are <code class="literal">fx=409</code> pixels, <code class="literal">fy=408</code> pixels, <code class="literal">u0=237</code> pixels, and <code class="literal">v0=171</code>pixels. Our calibration images have a size of <code class="literal">536x356</code> pixels. From the calibration results, you can see that, as expected, the principal point is close to the center of the image, but yet off by few pixels. The calibration images were taken using a Nikon D500 camera with a <code class="literal">18mm </code>lens. Looking at the manufacturer specifications, we find that the sensor size of this camera is <code class="literal">23.5mm x 15.7mm</code>, which gives us a pixel size of <code class="literal">0.0438mm</code>. The estimated focal length is expressed in pixels, so multiplying the result by the pixel size gives us an estimated focal length of <code class="literal">17.8mm</code>, which is consistent with the actual lens we used.</p><p class="calibre8">Let's now turn our attention to the distortion parameters. So far, we have mentioned that under the pin-hole camera model, we can neglect the effect of the lens. However, this is only possible if the lens that is used to capture an image does not introduce important optical distortions. Unfortunately, this is not the case with lower quality lenses or with lenses that have a very short focal length. Even the lens we used in this experiment introduced some distortion: the edges of the rectangular board are curved in the image. Note that this distortion becomes more important as we move away from the center of the image. This is a typical distortion observed with a fish-eye lens, and it is called <strong class="calibre15">radial distortion</strong>.</p><p class="calibre8">It is possible to compensate for these deformations by introducing an appropriate distortion model. The idea is to represent the distortions induced by a lens by a set of mathematical equations. Once established, these equations can then be reverted in order to undo the distortions visible on the image. Fortunately, the exact parameters of the transformation that will correct the distortions can be obtained together with the other camera parameters during the calibration phase. Once this is done, any image from the newly calibrated camera will be undistorted. Therefore, we have added an additional method to our calibration class:</p><pre class="programlisting">    // remove distortion in an image (after calibration) 
    cv::Mat CameraCalibrator::remap(const cv::Mat &amp;image) { 
 
      cv::Mat undistorted; 
 
      if (mustInitUndistort) { // called once per calibration 
 
        cv::initUndistortRectifyMap(   
                     cameraMatrix, // computed camera matrix 
                     distCoeffs,   // computed distortion matrix 
                     cv::Mat(),    // optional rectification (none)  
                     cv::Mat(),    // camera matrix to generate undistorted 
                     image.size(), // size of undistorted 
                     CV_32FC1,     // type of output map 
                     map1, map2);  // the x and y mapping functions 
 
        mustInitUndistort= false; 
      } 
 
      // Apply mapping functions 
      cv::remap(image, undistorted, map1, map2,        
                cv::INTER_LINEAR);     // interpolation type 
 
      return undistorted; 
    } 
</pre><p class="calibre8">Running this code on one of our calibration image results in the following undistorted image:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_11_006.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">To correct the distortion, OpenCV uses a polynomial function that is applied to the image points in order to move them to their undistorted position. By default, five coefficients are used; a model made of eight coefficients is also available. Once these coefficients are obtained, it is possible to compute two <code class="literal">cv::Mat</code> mapping functions (one for the <code class="literal">x</code> coordinate and one for the <code class="literal">y</code> coordinate) that will give the new undistorted position of an image point on a distorted image. This is computed by the <code class="literal">cv::initUndistortRectifyMap</code> function, and the <code class="literal">cv::remap</code> function remaps all the points of an input image to a new image. Note that because of the nonlinear transformation, some pixels of the input image now fall outside the boundary of the output image. You can expand the size of the output image to compensate for this loss of pixels, but you now obtain output pixels that have no values in the input image (they will then be displayed as black pixels).</p></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec201" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">More options are available when it comes to camera calibration.</p><div><div><div><div><h3 class="title3"><a id="ch11lvl3sec44" class="calibre6"/>Calibration with known intrinsic parameters</h3></div></div></div><p class="calibre8">When a good estimate of the camera's intrinsic parameters is known, it could be advantageous to input them in the <code class="literal">cv::calibrateCamera</code> function. They will then be used as initial values in the optimization process. To do so, you just need to add the <code class="literal">cv::CALIB_USE_INTRINSIC_GUESS</code> flag and input these values in the calibration matrix parameter. It is also possible to impose a fixed value for the principal point (<code class="literal">cv::CALIB_FIX_PRINCIPAL_POINT</code>), which can often be assumed to be the central pixel. You can also impose a fixed ratio for the focal lengths <code class="literal">fx</code> and <code class="literal">fy</code> (<code class="literal">cv::CALIB_FIX_RATIO</code>), in which case, you assume that the pixels are square.</p></div><div><div><div><div><h3 class="title3"><a id="ch11lvl3sec45" class="calibre6"/>Using a grid of circles for calibration</h3></div></div></div><p class="calibre8">Instead of the usual chessboard pattern, OpenCV also offers the possibility to calibrate a camera by using a grid of circles. In this case, the centers of the circles are used as calibration points. The corresponding function is very similar to the function we used to locate the chessboard corners, for example:</p><pre class="programlisting">    cv::Size boardSize(7,7); 
    std::vector&lt;cv::Point2f&gt; centers; 
    bool found = cv:: findCirclesGrid(image, boardSize, centers); 
</pre></div></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec202" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre16">A flexible new technique for camera calibration</em> article by <em class="calibre16">Z. Zhang</em> in <em class="calibre16">IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22</em>, no 11, 2000, is a classic paper on the problem of camera calibration</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch11lvl1sec68" class="calibre6"/>Recovering camera pose</h1></div></div></div><p class="calibre8">When a camera is calibrated, it becomes possible to relate the captured images with the outside world. We previously explained that if the 3D structure of an object is known, then one can predict how the object will be imaged on the sensor of the camera. The process of image formation is indeed completely described by the projective equation that was presented at the beginning of this chapter. When most of the terms of this equation are known, then it becomes possible to infer the value of the other elements (2D or 3D) through the observation of some images. In this recipe, we will look at the camera pose recovery problem when a known 3D structure is observed.</p><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec203" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Let's consider a simple object, a bench in a park. We took an image of this one using the camera/lens system calibrated in the previous recipe. We also have manually identified eight distinct image points on the bench that we will use for our camera pose estimation:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/B05388_11_05.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Having access to this object, it is possible to make some physical measurements. The bench is composed of a seat that is <code class="literal">242.5cm x 53.5cm x 9cm</code> and a back that is <code class="literal">242.5cm x 24cm x 9cm</code> fixed <code class="literal">12cm</code> over the seat. Using this information, we can then easily derive the 3D coordinates of the eight identified points in some object-centric reference frame (here, we fixed the origin at the left extremity of the intersection between the two planes). We can then create a <code class="literal">cv::Point3f</code> vector containing these coordinates:</p><pre class="programlisting">    // Input object points 
    std::vector&lt;cv::Point3f&gt; objectPoints; 
    objectPoints.push_back(cv::Point3f(0, 45, 0)); 
    objectPoints.push_back(cv::Point3f(242.5, 45, 0)); 
    objectPoints.push_back(cv::Point3f(242.5, 21, 0)); 
    objectPoints.push_back(cv::Point3f(0, 21, 0)); 
    objectPoints.push_back(cv::Point3f(0, 9, -9)); 
    objectPoints.push_back(cv::Point3f(242.5, 9, -9)); 
    objectPoints.push_back(cv::Point3f(242.5, 9, 44.5)); 
    objectPoints.push_back(cv::Point3f(0, 9, 44.5)); 
</pre><p class="calibre8">The question now is where the camera was with respect to these points when the shown picture was taken. Since the coordinates of the image of these known points on the 2D image plane are also known, then it becomes easy to answer this question by using the <code class="literal">cv::solvePnP</code> function. Here, the correspondence between the 3D and the 2D points has been established manually, but one should be able to come up with some methods that would allow you to obtain this information automatically:</p><pre class="programlisting">    // Input image points 
    std::vector&lt;cv::Point2f&gt; imagePoints; 
    imagePoints.push_back(cv::Point2f(136, 113)); 
    imagePoints.push_back(cv::Point2f(379, 114)); 
    imagePoints.push_back(cv::Point2f(379, 150)); 
    imagePoints.push_back(cv::Point2f(138, 135)); 
    imagePoints.push_back(cv::Point2f(143, 146)); 
    imagePoints.push_back(cv::Point2f(381, 166)); 
    imagePoints.push_back(cv::Point2f(345, 194)); 
    imagePoints.push_back(cv::Point2f(103, 161)); 
 
    // Get the camera pose from 3D/2D points 
    cv::Mat rvec, tvec; 
    cv::solvePnP( 
                 objectPoints, imagePoints,      // corresponding 3D/2D pts  
                 cameraMatrix, cameraDistCoeffs, // calibration  
                 rvec, tvec);                    // output pose 
 
    //Convert to 3D rotation matrix 
    cv::Mat rotation; 
    cv::Rodrigues(rvec, rotation); 
</pre><p class="calibre8">This function in fact, computes the rigid transformation (rotation and translation) that brings the object coordinates in the camera-centric reference frame (that is, the one that has its origin at the focal point). It is also important to note that the rotation computed by this function is given in the form of a 3D vector. This is a compact representation in which the rotation to apply is described by a unit vector (an axis of rotation) around which the object is rotated by a certain angle. This axis-angle representation is also called the <strong class="calibre15">Rodrigues' rotation formula</strong>. In OpenCV, the angle of the rotation corresponds to the norm of the output rotation vector, the latter being aligned with the axis of rotation. This is why the <code class="literal">cv::Rodrigues</code> function is used to obtain the 3D matrix of rotation that appears in our projective equation.</p><p class="calibre8">The pose recovery procedure described here is simple, but how do we know we obtained the right camera/object pose information? We can visually assess the quality of the results by using the <code class="literal">cv::viz</code> module that gives us the ability to visualize 3D information. The use of this module is explained in the last section of this recipe, but let's display a simple 3D representation of our object and the camera that captured it:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_11_008.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">It might be difficult to judge of the quality of the pose recovery just by looking at this image but if you test the example of this recipe on your computer, you will have the possibility to move this representation in 3D using your mouse, which should give you a better sense of the solution obtained.</p></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec204" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">In this recipe, we assumed that the 3D structure of the object was known, as well as the correspondence between sets of object points and image points. The camera's intrinsic parameters were also known through calibration. If you look at our projective equation presented at the end of the <em class="calibre16">Digital image formation</em> section of the introduction of this chapter, this means that we have points for which coordinates <code class="literal">(X,Y,Z)</code> and <code class="literal">(x,y)</code> are known. We also have the elements of the first matrix known (the intrinsic parameters). Only the second matrix is unknown; this is the one that contains the extrinsic parameters of the camera that is the camera/object pose information. Our objective is then to recover these unknown parameters from the observation of 3D scene points. This problem is known as the <strong class="calibre15">Perspective-n-Point</strong> (<strong class="calibre15">PnP</strong>) problem.</p><p class="calibre8">Rotation has three degrees of freedom (for example, angle of rotation around the three axes) and translation also has three degrees of freedom. We therefore have a total of six unknowns. For each object point/image point correspondence, the projective equation gives us three algebraic equations, but since the projective equation is up to a scale factor, we only have two independent equations. A minimum of three points is therefore required to solve this system of equations. Obviously, more points provide a more reliable estimate.</p><p class="calibre8">In practice, many different algorithms have been proposed to solve this problem and OpenCV proposes a number of different implementation in its <code class="literal">cv::solvePnP</code> function. The default method consists in optimizing what is called the reprojection error. Minimizing this type of error is considered to be the best strategy to get accurate 3D information from camera images. In our problem, it corresponds to finding the optimal camera position that minimizes the 2D distance between the projected 3D points (as obtained by applying the projective equation) and the observed image points given as input.</p><p class="calibre8">Note that OpenCV also has a <code class="literal">cv::solvePnPRansac</code> function. As the name suggests, this function uses the <strong class="calibre15">RANSAC</strong> algorithm in order to solve the PnP problem. This means that some of the object point/image point correspondences may be wrong and the function will return the ones that have been identified as outliers. This is very useful when these correspondences have been obtained through an automatic process that can fail for some points.</p></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec205" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">When working with 3D information, it often difficult to validate the solutions obtained. To this end, OpenCV offers a simple yet powerful visualization module that facilitates the development and debugging of 3D vision algorithms. It allows inserting points, lines, cameras and other objects in a virtual 3D environment that you can interactively visualize from various points of views.</p><div><div><div><div><h3 class="title3"><a id="ch11lvl3sec46" class="calibre6"/>cv::Viz, a 3D Visualizer module</h3></div></div></div><p class="calibre8">
<code class="literal">cv::Viz</code> is an extra module of the OpenCV library that is built on top of the <strong class="calibre15">Visualization Toolkit</strong> (<strong class="calibre15">VTK</strong>) open source library. This is a powerful framework used for 3D computer graphics. With <code class="literal">cv::viz</code>, you create a 3D virtual environment to which you can add a variety of objects. A visualization window is created that displays the environment from a given point of view. You saw in this recipe an example of what can be displayed in a <code class="literal">cv::viz</code> window. This window responds to mouse events that are used to navigate inside the environment (through rotations and translations). This section describes the basic use of the <code class="literal">cv::viz</code> module.</p><p class="calibre8">The first thing to do is to create the visualization window. Here, we use a white background:</p><pre class="programlisting">    // Create a viz window 
    cv::viz::Viz3d visualizer("Viz window"); 
    visualizer.setBackgroundColor(cv::viz::Color::white()); 
</pre><p class="calibre8">Next, you create your virtual objects and insert them into the scene. There is a variety of predefined objects. One of them is particularly useful for us; it is the one that creates a virtual pin-hole camera:</p><pre class="programlisting">    // Create a virtual camera 
    cv::viz::WCameraPosition cam( 
                    cMatrix,     // matrix of intrinsics 
                    image,       // image displayed on the plane 
                    30.0,        // scale factor 
                    cv::viz::Color::black()); 
    // Add the virtual camera to the environment 
    visualizer.showWidget("Camera", cam); 
</pre><p class="calibre8">The <code class="literal">cMatrix</code> variable is a <code class="literal">cv::Matx33d</code> (that is, a <code class="literal">cv::Matx&lt;double,3,3&gt;</code>) instance containing the intrinsic camera parameters as obtained from calibration. By default, this camera is inserted at the origin of the coordinate system. To represent the bench, we used two rectangular cuboid objects:</p><pre class="programlisting">    // Create a virtual bench from cuboids 
    cv::viz::WCube plane1(cv::Point3f(0.0, 45.0, 0.0),             
                          cv::Point3f(242.5, 21.0, -9.0),   
                          true,     // show wire frame  
                          cv::viz::Color::blue()); 
    plane1.setRenderingProperty(cv::viz::LINE_WIDTH, 4.0); 
    cv::viz::WCube plane2(cv::Point3f(0.0, 9.0, -9.0), 
                          cv::Point3f(242.5, 0.0, 44.5),                
                          true,    // show wire frame  
                          cv::viz::Color::blue()); 
    plane2.setRenderingProperty(cv::viz::LINE_WIDTH, 4.0); 
    // Add the virtual objects to the environment 
    visualizer.showWidget("top", plane1); 
    visualizer.showWidget("bottom", plane2); 
</pre><p class="calibre8">This virtual bench is also added at the origin; it then needs to be moved at its camera-centric position as found from our <code class="literal">cv::solvePnP</code> function. It is the responsibility of the <code class="literal">setWidgetPose</code> method to perform this operation. This one simply applies the rotation and translation components of the estimated motion:</p><pre class="programlisting">    cv::Mat rotation; 
    // convert vector-3 rotation 
    // to a 3x3 rotation matrix 
    cv::Rodrigues(rvec, rotation); 
 
    // Move the bench  
    cv::Affine3d pose(rotation, tvec); 
    visualizer.setWidgetPose("top", pose); 
    visualizer.setWidgetPose("bottom", pose); 
</pre><p class="calibre8">The final step is to create a loop that keeps displaying the visualization window. The <code class="literal">1ms</code> pause is there to listen to mouse events:</p><pre class="programlisting">    // visualization loop 
    while(cv::waitKey(100)==-1 &amp;&amp; !visualizer.wasStopped()) { 
 
      visualizer.spinOnce(1,      // pause 1ms  
                          true);  // redraw 
    } 
</pre><p class="calibre8">This loop will stop when the visualization window is closed or when a key is pressed over an OpenCV image window. Try to apply inside this loop some motion on an object (using <code class="literal">setWidgetPose</code>); this is how animation can be created.</p></div></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec206" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre16">Model-based object pose in 25 lines of code</em> by <em class="calibre16">D. DeMenthon</em> and <em class="calibre16">L. S. Davis</em>, in the <em class="calibre16">European Conference on Computer Vision</em>, 1992, pp.335-343 is a famous method for recovering camera pose from scene points</li><li class="listitem">The <em class="calibre16">Matching images using random sample consensus</em> recipe in 
Chapter 10
, <em class="calibre16">Estimating Projective Relations in Images</em> describes the RANSAC algorithm</li><li class="listitem">The <em class="calibre16">Installing the OpenCV library</em> recipe in <a href="ch01.html" title="Chapter 1. Playing with Images">
Chapter 1
</a>, <em class="calibre16">Playing with Images</em> explains how to install the RANSAC <code class="literal">cv::viz</code> extra module</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch11lvl1sec69" class="calibre6"/>Reconstructing a 3D scene from calibrated cameras</h1></div></div></div><p class="calibre8">We saw in the previous recipe that it is possible to recover the position of a camera observing a 3D scene, when this one is calibrated. The approach described took advantage of the fact that, sometimes, the coordinates of some 3D points visible in the scene might be known. We will now learn that if a scene is observed from more than one point of view, 3D pose and structure can be reconstructed even if no information about the 3D scene is available. This time, we will use correspondences between image points in the different views in order to infer 3D information. We will introduce a new mathematical entity encompassing the relation between two views of a calibrated camera, and we will discuss the principle of triangulation in order to reconstruct 3D points from 2D images.</p><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec207" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Let's again use the camera we calibrated in the first recipe of this chapter and take two pictures of some scene. We can match feature points between these two views using, for example, the SIFT detector and descriptor presented in <a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <em class="calibre16">Detecting Interest Points</em> and <a href="ch09.html" title="Chapter 9. Describing and Matching Interest Points">
Chapter 9
</a>, <em class="calibre16">Describing and Matching interest points</em>.</p><p class="calibre8">The fact that the calibration parameters of the camera are available, allows us to work in world coordinates; and therefore establish a physical constraint between the camera poses and the position of the corresponding points. Basically, we introduce a new mathematical entity called the <strong class="calibre15">Essential matrix</strong>, which is the calibrated version of the fundamental matrix introduced in the previous chapter. Therefore, there is a <code class="literal">cv::findEssentialMat</code> function that's identical to the <code class="literal">cv::findFundametalMat</code> that was used in the <em class="calibre16">Computing the fundamental matrix of an image pair</em> recipe in 
Chapter 10
, <em class="calibre16">Estimating Projective Relations in Images</em>. We can call this function with the established point correspondences and through a RANSAC scheme, filter out the outlier points to retain only the matches that comply with the found geometry:</p><pre class="programlisting">    // vector of keypoints and descriptors 
    std::vector&lt;cv::KeyPoint&gt; keypoints1; 
    std::vector&lt;cv::KeyPoint&gt; keypoints2; 
    cv::Mat descriptors1, descriptors2; 
 
    // Construction of the SIFT feature detector  
    cv::Ptr&lt;cv::Feature2D&gt; ptrFeature2D =   
                           cv::xfeatures2d::SIFT::create(500); 
 
    // Detection of the SIFT features and associated descriptors 
    ptrFeature2D-&gt;detectAndCompute(image1, cv::noArray(),  
                                   keypoints1, descriptors1); 
    ptrFeature2D-&gt;detectAndCompute(image2, cv::noArray(),  
                                   keypoints2, descriptors2); 
 
    // Match the two image descriptors 
    // Construction of the matcher with crosscheck  
    cv::BFMatcher matcher(cv::NORM_L2, true); 
    std::vector&lt;cv::DMatch&gt; matches; 
    matcher.match(descriptors1, descriptors2, matches); 
 
    // Convert keypoints into Point2f 
    std::vector&lt;cv::Point2f&gt; points1, points2; 
    for (std::vector&lt;cv::DMatch&gt;::const_iterator it =  
           matches.begin(); it != matches.end(); ++it) { 
 
      // Get the position of left keypoints 
      float x = keypoints1[it-&gt;queryIdx].pt.x; 
      float y = keypoints1[it-&gt;queryIdx].pt.y; 
      points1.push_back(cv::Point2f(x, y)); 
      // Get the position of right keypoints 
      x = keypoints2[it-&gt;trainIdx].pt.x; 
      y = keypoints2[it-&gt;trainIdx].pt.y; 
      points2.push_back(cv::Point2f(x, y)); 
    } 
 
    // Find the essential between image 1 and image 2 
    cv::Mat inliers; 
    cv::Mat essential = cv::findEssentialMat(points1, points2,            
                                Matrix,         // intrinsic parameters 
                                cv::RANSAC,
                                0.9, 1.0,       // RANSAC method 
                                inliers);       // extracted inliers 
</pre><p class="calibre8">The resulting set of inliers matches is then as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_11_009.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">As it will be explained in the next section, the essential matrix encapsulates the rotation and translation components that separate the two views. It is therefore possible to recover the relative pose between our two views directly from this matrix. OpenCV has a function that performs this operation, it is the <code class="literal">cv::recoverPose</code> function. This one is used as follows:</p><pre class="programlisting">    // recover relative camera pose from essential matrix 
    cv::Mat rotation, translation; 
    cv::recoverPose(essential,             // the essential matrix 
                    points1, points2,      // the matched keypoints 
                    cameraMatrix,          // matrix of intrinsics 
                    rotation, translation, // estimated motion 
                    inliers);              // inliers matches 
</pre><p class="calibre8">Now that we have the relative pose between the two cameras, it becomes possible to estimate the location of points for which we have established correspondence between the two views. The following screenshot illustrates how this is possible. It shows the two cameras at their estimated position (the left one is placed at the origin). We also have selected a pair of corresponding points and, for these image points, we traced a ray that, according to the projective geometry model, corresponds to all possible locations of the associated 3D point:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_11_010.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Clearly, since these two image points have been generated by the same 3D point, the two rays must intersect at one location, the location of the 3D point. The method that consists of intersecting the lines of projection of two corresponding image points, when the relative position of two cameras is known, is called <strong class="calibre15">triangulation</strong>. This process first requires the two projection matrices and can be repeated for all matches. Remember, however, that these ones must be expressed in world coordinates; this is done here by using the <code class="literal">cv::undistortPoints</code> function.</p><p class="calibre8">Finally, we call our triangulate function, which computes the position of the triangulated point, and that will be described in the next section:</p><pre class="programlisting">    // compose projection matrix from R,T 
    cv::Mat projection2(3, 4, CV_64F); // the 3x4 projection matrix 
    rotation.copyTo(projection2(cv::Rect(0, 0, 3, 3))); 
    translation.copyTo(projection2.colRange(3, 4)); 
  
    // compose generic projection matrix  
    cv::Mat projection1(3, 4, CV_64F, 0.); // the 3x4 projection matrix 
    cv::Mat diag(cv::Mat::eye(3, 3, CV_64F)); 
    diag.copyTo(projection1(cv::Rect(0, 0, 3, 3))); 
 
    // to contain the inliers 
    std::vector&lt;cv::Vec2d&gt; inlierPts1; 
    std::vector&lt;cv::Vec2d&gt; inlierPts2; 
 
    // create inliers input point vector for triangulation 
    int j(0); 
    for (int i = 0; i &lt; inliers.rows; i++) { 
      if (inliers.at&lt;uchar&gt;(i)) { 
        inlierPts1.push_back(cv::Vec2d(points1[i].x, points1[i].y)); 
        inlierPts2.push_back(cv::Vec2d(points2[i].x, points2[i].y)); 
      } 
    } 
 
    // undistort and normalize the image points 
    std::vector&lt;cv::Vec2d&gt; points1u; 
    cv::undistortPoints(inlierPts1, points1u,  
                        cameraMatrix, cameraDistCoeffs); 
    std::vector&lt;cv::Vec2d&gt; points2u; 
    cv::undistortPoints(inlierPts2, points2u,  
                        cameraMatrix, cameraDistCoeffs); 
 
    // triangulation 
    std::vector&lt;cv::Vec3d&gt; points3D; 
    triangulate(projection1, projection2,  
                points1u, points2u, points3D); 
</pre><p class="calibre8">A cloud of 3D points located on the surface of the scene elements is thus found:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_11_011.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Note that from this new point of view, we can see that the two rays we drew do not intersect as they were supposed to. This fact will be discussed in the next section.</p></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec208" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">The calibration matrix is the entity allowing us to transform pixel coordinates into world coordinates. We can then more easily relate image points to the 3D points that have produced them. This is demonstrated in the following figure, which we will now use to demonstrate a simple relationship between a world point and its images:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_11_012.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The figure shows two cameras separated by a rotation <code class="literal">R</code> and a translation <code class="literal">T</code>. It is interesting to note that the translation vector <code class="literal">T</code> joins the centers of projection of the two cameras. We also have a vector <code class="literal">x</code> joining the first camera center to an image point and a vector <code class="literal">x'</code> joining the second camera center to the corresponding image point. Since we have the relative motion between the two cameras, we can express the orientation of <code class="literal">x</code> in terms of the second camera reference as <code class="literal">Rx</code>. Now, if you carefully observe the geometry of the image points shown, you will observe that vectors <code class="literal">T</code>, <code class="literal">Rx</code>, and <code class="literal">x'</code> are all coplanar. This fact can be expressed by the following mathematical relation:</p><p class="calibre8">

</p><div><img alt="How it works..." src="img/B05388_11_17.jpg" class="calibre17"/></div><p class="calibre8">

</p><p class="calibre8">It was possible to reduce the first relation to a single <code class="literal">3x3</code> matrix <code class="literal">E</code> because a cross-product can also be expressed by a matrix operation. This matrix <code class="literal">E</code> is called the essential matrix and the associated equation is the calibrated equivalent of the epipolar constraint presented in the <em class="calibre16">Computing the fundamental matrix of an image pair</em> recipe in 
Chapter 10
, <em class="calibre16">Estimating Projective Relations in Images</em>. We can then estimate this one from image correspondences, as we did for the fundamental matrix, but this time expressing these ones in world coordinates. Also, as demonstrated, the essential matrix is built from the rotation and translation components of the motion between the two cameras. This means that once this one has been estimated, it can be decomposed to obtain the relative pose between the cameras. This is what we did by using the <code class="literal">cv::recoverPose</code> function. This function calls the <code class="literal">cv::decomposeEssentialMat</code> function, which produces four possible solutions for the relative pose. The right one is identified by looking at the set of provided matches to determine the solution that is physically possible.</p><p class="calibre8">Once the relative pose between the cameras has been obtained, the position of any point corresponding to a match pair is recovered through triangulation. Different methods have been proposed to solve the triangulation problem. Probably the simplest solution consists of considering the two projection matrices, <code class="literal">P</code> and <code class="literal">P'</code>. The seek 3D point in homogenous coordinates can be expressed as <code class="literal">X=[X,Y,Z,1]<sup class="calibre20">T</sup></code>, and we know that <code class="literal">x=PX</code> and <code class="literal">x'=P'X</code>. Each of these two homogenous equations brings two independent equations, which is sufficient to solve the three unknowns of the 3D point position. This over determined system of equation can be solved using a least-square approach, which can be accomplished using a convenient OpenCV utility function called <code class="literal">cv::solve</code>. The complete function is as follows:</p><pre class="programlisting">    // triangulate using Linear LS-Method 
    cv::Vec3d triangulate(const cv::Mat &amp;p1,  
                          const cv::Mat &amp;p2,                 
                          const cv::Vec2d &amp;u1,  
                          const cv::Vec2d &amp;u2) { 
 
    // system of equations assuming image=[u,v] and X=[x,y,z,1] 
    // from u(p3.X)= p1.X and v(p3.X)=p2.X 
    cv::Matx43d A(u1(0)*p1.at&lt;double&gt;(2, 0) - p1.at&lt;double&gt;(0, 0),  
                  u1(0)*p1.at&lt;double&gt;(2, 1) - p1.at&lt;double&gt;(0, 1),                      
                  u1(0)*p1.at&lt;double&gt;(2, 2) - p1.at&lt;double&gt;(0, 2),   
                  u1(1)*p1.at&lt;double&gt;(2, 0) - p1.at&lt;double&gt;(1, 0),                 
                  u1(1)*p1.at&lt;double&gt;(2, 1) - p1.at&lt;double&gt;(1, 1),   
                  u1(1)*p1.at&lt;double&gt;(2, 2) - p1.at&lt;double&gt;(1, 2),  
                  u2(0)*p2.at&lt;double&gt;(2, 0) - p2.at&lt;double&gt;(0, 0),  
                  u2(0)*p2.at&lt;double&gt;(2, 1) - p2.at&lt;double&gt;(0, 1),  
                  u2(0)*p2.at&lt;double&gt;(2, 2) - p2.at&lt;double&gt;(0, 2),  
                  u2(1)*p2.at&lt;double&gt;(2, 0) - p2.at&lt;double&gt;(1, 0),          
                  u2(1)*p2.at&lt;double&gt;(2, 1) - p2.at&lt;double&gt;(1, 1),    
                  u2(1)*p2.at&lt;double&gt;(2, 2) - p2.at&lt;double&gt;(1, 2)); 
 
    cv::Matx41d B(p1.at&lt;double&gt;(0, 3) - u1(0)*p1.at&lt;double&gt;(2, 3), 
                  p1.at&lt;double&gt;(1, 3) - u1(1)*p1.at&lt;double&gt;(2, 3), 
                  p2.at&lt;double&gt;(0, 3) - u2(0)*p2.at&lt;double&gt;(2, 3),  
                  p2.at&lt;double&gt;(1, 3) - u2(1)*p2.at&lt;double&gt;(2, 3)); 
 
    // X contains the 3D coordinate of the reconstructed point 
    cv::Vec3d X; 
    // solve AX=B 
    cv::solve(A, B, X, cv::DECOMP_SVD); 
    return X; 
  } 
</pre><p class="calibre8">We have noted in the previous section that very often, because of noise and digitization, the projection lines that should normally intersect do not intersect in practice. The least-square solution will therefore find a solution somewhere around the point of intersection. Also, this method will not work if you try to reconstruct a point at infinity. This is because, for such a point, the fourth element of the homogenous coordinates should be at <code class="literal">0</code> not at <code class="literal">1</code> as assumed.</p><p class="calibre8">Finally, it is important to understand that this 3D reconstruction is done up to a scale factor only. If you need to make real measurements, you need to know at least one physical distance, for example, the real distance between the two cameras or the height of one of the visible objects.</p></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec209" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">The 3D reconstruction is a rich field of research in computer vision, and there is much more to explore in the OpenCV library on the subject.</p><div><div><div><div><h3 class="title3"><a id="ch11lvl3sec47" class="calibre6"/>Decomposing a homography</h3></div></div></div><p class="calibre8">We learned in this recipe that an essential matrix can be decomposed in order to recover the rotation and translation between two cameras. We also learned in the previous chapter that a homography exists between two views of a plane. In this case, this homography contains also the rotational and translational components. In addition, it contains information about the plane, namely its normal with respect to each camera. The function <code class="literal">cv::decomposeHomographyMat</code> can be used to decompose this matrix; the condition, however, is to have a calibrated camera.</p></div><div><div><div><div><h3 class="title3"><a id="ch11lvl3sec48" class="calibre6"/>Bundle adjustment</h3></div></div></div><p class="calibre8">In this recipe, we first estimate the camera position from matches and then reconstruct the associated 3D points through triangulation. It is possible to generalize this process by using any number of views. For each of these views, feature points are detected and are matched with the other views. Using this information, it is possible to write equations that relate the rotations and translations between the views, the set of 3D points and the calibration information. All these unknowns can be optimized together through a large optimization process that aims at minimizing the reprojection errors of all points in each view where they are visible. This combined optimization procedure is called <strong class="calibre15">bundle adjustment</strong>. Have a look at the <code class="literal">cv::detail::BundleAdjusterReproj</code> class, which implements a camera parameters refinement algorithm that minimizes the sum of the reprojection error squares.</p></div></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec210" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre16">Triangulation</em> by <em class="calibre16">R. Hartley</em> and <em class="calibre16">P. Sturm</em> in <em class="calibre16">Computer Vision and Image Understanding vol. 68</em>, no. 2, 1997 presents a formal analysis of different triangulation methods</li><li class="listitem"><em class="calibre16">Modeling the World from Internet Photo Collections</em> by <em class="calibre16">N. Snavely</em>, <em class="calibre16">S.M. Seitz</em>, and <em class="calibre16">R. Szeliski</em> in <em class="calibre16">International Journal of </em><em class="calibre16">Computer Vision, </em>vol. 80, no 2, 2008 describes a large-scale application of 3D reconstruction through bundle adjustment</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch11lvl1sec70" class="calibre6"/>Computing depth from stereo image</h1></div></div></div><p class="calibre8">Humans view the world in three dimensions using their two eyes. Robots can do the same when they are equipped with two cameras. This is called <strong class="calibre15">stereovision</strong>. A stereo rig is a pair of cameras mounted on a device, looking at the same scene and separated by a fixed baseline (distance between the two cameras). This recipe will show you how a depth map can be computed from two stereo images by computing dense correspondence between the two views.</p><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec211" class="calibre6"/>Getting ready</h2></div></div></div><p class="calibre8">A stereovision system is generally made of two side-by-side cameras looking at the same direction. The following figure illustrates such a stereo system in a perfectly aligned configuration:</p><p class="calibre8">
</p><div><img alt="Getting ready" src="img/image_11_014.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Under this ideal configuration the cameras are only separated by a horizontal translation and therefore all epipolar lines are horizontal. This means that corresponding points have the same <code class="literal">y</code> coordinates, which reduces the search for matches to a 1D line. The difference in their <code class="literal">x</code> coordinates depends on the depth of the points. Points at infinity have image points at the same <code class="literal">(x,y)</code> coordinates and the closer the points are to the stereo rig the greater will be the difference of their <code class="literal">x</code> coordinates. This fact can be demonstrated formally by looking at the projective equation. When cameras are separated by a pure horizontal translation, then the projective equation of the second camera (the one on the right) becomes this:</p><p class="calibre8">

</p><div><img alt="Getting ready" src="img/B05388_11_18.jpg" class="calibre17"/></div><p class="calibre8">

</p><p class="calibre8">Here, for simplicity, we assume square pixels and same calibration parameters for both cameras. Now if you compute the difference of <code class="literal">x-x'</code> (do not forget to divide by <code class="literal">s</code> to normalize the homogenous coordinates) and isolate the <code class="literal">z</code> coordinate, you obtain the following:</p><p class="calibre8">

</p><div><img alt="Getting ready" src="img/B05388_11_19.jpg" class="calibre17"/></div><p class="calibre8">

</p><p class="calibre8">The term <code class="literal">(x-x')</code> is called the <strong class="calibre15">disparity</strong>. To compute the depth map of a stereovision system, the disparity of each pixel must be estimated. This recipe will show you how to do it.</p></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec212" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The ideal configuration shown in the previous section is, in practice, very difficult to realize. Even if they are accurately positioned, the cameras of the stereo rig will unavoidably include some extra translational and rotational components. But, fortunately, the images can be rectified such to produce horizontal epilines. This can be achieved by computing the fundamental matrix of the stereo system using, for example, the robust matching algorithm of the previous chapter. This is what we did for the following stereo pair (with some epipolar lines drawn on it):</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_11_017.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">OpenCV offers a rectifying function that uses a homographic transformation to project the image plane of each camera onto perfectly aligned virtual plane. This transformation is computed from a set of matched points and a fundamental matrix. Once computed, these homographies are then used to wrap the images:</p><pre class="programlisting">    // Compute homographic rectification 
    cv::Mat h1, h2; 
    cv::stereoRectifyUncalibrated(points1, points2,  
                                  fundamental,  
                                  image1.size(), h1, h2); 
 
    // Rectify the images through warping 
    cv::Mat rectified1; 
    cv::warpPerspective(image1, rectified1, h1, image1.size()); 
    cv::Mat rectified2; 
    cv::warpPerspective(image2, rectified2, h2, image1.size()); 
</pre><p class="calibre8">For our example, the rectified image pair is as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_11_018.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Computing the disparity map can then be accomplished using methods that assume parallelism of the cameras (and consequently horizontal epipolar lines):</p><pre class="programlisting">    // Compute disparity 
    cv::Mat disparity; 
    cv::Ptr&lt;cv::StereoMatcher&gt; pStereo =  
         cv::StereoSGBM::create(0,   // minimum disparity 
                                32,  // maximum disparity 
                                5);  // block size 
    pStereo-&gt;compute(rectified1, rectified2, disparity); 
</pre><p class="calibre8">The obtained disparity map can then be displayed as an image. Bright values correspond to high disparities and, from what we learned earlier in this recipe, those high disparity values correspond to proximal objects:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_11_019.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The quality of the computed disparity mainly depends on the appearance of the different objects that compose the scene. Highly-textured regions tend to produce more accurate disparity estimates since they can be non-ambiguously matched. Also, a larger baseline increases the range of detectable depth values. However, enlarging the baseline also makes disparity computation more complex and less reliable.</p></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec213" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">Computing disparities is a pixel matching exercise. We already mentioned that when the images are properly rectified, the search space is conveniently aligned with the image rows. The difficulty, however, is that, in stereovision, we are generally seeking a dense disparity map, that is, we want to match every pixel of one image with the pixels of the other image.</p><p class="calibre8">This can be more challenging than selecting a few distinctive points in an image and finding their corresponding points in the other image. Disparity computation is therefore a complex process that is generally composed of four steps:</p><div><ol class="orderedlist"><li class="listitem1">Matching cost calculation.</li><li class="listitem1">Cost aggregation.</li><li class="listitem1">Disparity computation and optimization.</li><li class="listitem1">Disparity refinement.</li></ol></div><p class="calibre8">These steps are detailed in the next paragraph.</p><p class="calibre8">Assigning a disparity to one pixel, is putting a pair of points in correspondence in a stereo set. Finding the best disparity map is often posed as an optimization problem. With this perspective, matching two points has a cost that must be computed following a defined metric. This can be, for example, a simple absolute or squared difference of intensities, colors or gradients. In the search for an optimal solution, the matching cost is generally aggregated over a region in order to cope with noise local ambiguity. The global disparity map can then be estimated by evaluating an energy function that includes terms to smooth the disparity map, take into account any possible occlusion, and enforce a uniqueness constraint. Finally, a post-processing step is often applied in order to refine the disparity estimates during which, for example, planar regions are detected or depth discontinuities are detected.</p><p class="calibre8">OpenCV implements a number of disparity computation methods. Here, we used the <code class="literal">cv::StereoSGBM</code> approach. The simplest method is <code class="literal">cv::StereoBM</code> , which is based on block matching.</p><p class="calibre8">Finally, it should be noted that a more accurate rectification can be performed if you are ready to undergo a full calibration process. The <code class="literal">cv::stereoCalibrate</code> and <code class="literal">cv::stereoRectify</code> functions are in this case used in conjunction with a calibration pattern. The rectification mapping then computes new projection matrices for the cameras instead of simple homographies.</p></div><div><div><div><div><h2 class="title2"><a id="ch11lvl2sec214" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The article <em class="calibre16">A Taxonomy and Evaluation of Dense two-Frame Stereo Correspondence Algorithms</em> by <em class="calibre16">D. Scharstein</em> and <em class="calibre16">R. Szeliski</em> in <em class="calibre16">International </em><em class="calibre16">Journal of Computer Vision, </em>vol. 47, 2002 is a classic reference on disparity computation methods</li><li class="listitem">The article <em class="calibre16">Stereo processing by semiglobal matching and mutual information</em> by <em class="calibre16">H. Hirschmuller</em> in <em class="calibre16">IEEE </em><em class="calibre16">Transactions on Pattern Analysis and Machine Intelligence, </em>vol. 30, no 2, pp. 328-341, 2008 describes the approach used for computing the disparity in this recipe</li></ul></div></div></div></body></html>