<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer030">
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/><em class="italic">Chapter 1</em>: OpenCV Basics and Camera Calibration</h1>
			<p>This chapter is an introduction to OpenCV and how to use it in the initial phases of a self-driving car pipeline, to ingest a video stream, and prepare it for the next phases. We will discuss the characteristics of a camera from the point of view of a self-driving car and how to improve the quality of what we get out of it. We will also study how to manipulate the videos and we will try one of the most famous features of OpenCV, object detection, which we will use to detect pedestrians.</p>
			<p>With this chapter, you will build a solid foundation on how to use OpenCV and NumPy, which will be very useful later.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>OpenCV and NumPy basics</li>
				<li>Reading, manipulating, and saving images</li>
				<li>Reading, manipulating, and saving videos</li>
				<li>Manipulating images</li>
				<li>How to detect pedestrians with HOG</li>
				<li>Characteristics of a camera</li>
				<li>How to perform the camera calibration</li>
			</ul>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>Technical requirements</h1>
			<p>For the instructions and code in this chapter, you need the following:</p>
			<ul>
				<li>Python 3.7</li>
				<li>The opencv-Python module</li>
				<li>The NumPy module</li>
			</ul>
			<p>The code for the chapter can be found here:</p>
			<p><a href="https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter1">https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter1</a></p>
			<p>The Code in Action videos for this chapter can be found here:</p>
			<p><a href="https://bit.ly/2TdfsL7">https://bit.ly/2TdfsL7</a></p>
			<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/>Introduction to OpenCV and NumPy</h1>
			<p>OpenCV is a <a id="_idIndexMarker000"/>computer vision and machine learning library that has been developed <a id="_idIndexMarker001"/>for more than 20 years and provides an impressive number of functionalities. Despite some inconsistencies in the API, its simplicity and the remarkable number of algorithms implemented make it an extremely popular library and an excellent choice for many situations.</p>
			<p>OpenCV is written in C++, but there are bindings for Python, Java, and Android.</p>
			<p>In this book, we will focus on OpenCV for Python, with all the code tested using OpenCV 4.2.</p>
			<p>OpenCV in Python is provided by <strong class="source-inline">opencv-python</strong>, which can be installed using the following command:</p>
			<p class="source-code">pip install opencv-python</p>
			<p>OpenCV can take advantage of hardware acceleration, but to get the best performance, you might need to build it from the source code, with different flags than the default, to optimize it for your target hardware.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>OpenCV and NumPy</h2>
			<p>The Python <a id="_idIndexMarker002"/>bindings use NumPy, which increases the flexibility and makes it compatible <a id="_idIndexMarker003"/>with many other libraries. As an OpenCV image is a NumPy array, you can use normal NumPy operations to get information about the image. A good understanding of NumPy can improve the performance and reduce the length of your code.</p>
			<p>Let's dive right in with some quick examples of what you can do with NumPy in OpenCV.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>Image size</h2>
			<p>The size <a id="_idIndexMarker004"/>of the image can be retrieved using the <strong class="source-inline">shape</strong> attribute:</p>
			<p class="source-code">print("Image size: ", image.shape)</p>
			<p>For a grayscale image of 50x50, <strong class="source-inline">image.shape()</strong> would return the tuple (50, 50), while for an <a id="_idIndexMarker005"/>RGB image, the result would be (50, 50, 3).</p>
			<p class="callout-heading">False friends</p>
			<p class="callout">In NumPy, the attribute size is the size in bytes of the array; for a 50x50 gray image, it would be 2,500, while for the same image in RGB, it would be 7,500. It's the <strong class="source-inline">shape</strong> attribute that contains the size of the image – (50, 50) and (50, 50, 3), respectively.</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Grayscale images</h2>
			<p>Grayscale images <a id="_idIndexMarker006"/>are represented by a two-dimensional NumPy array. The first index affects the rows (<em class="italic">y</em> coordinate) and the second index the columns (<em class="italic">x</em> coordinate). The <em class="italic">y</em> coordinates have their origin in the top corner of the image and <em class="italic">x</em> coordinates have their origin in the left corner of the image.</p>
			<p>It is possible to create a black image using <strong class="source-inline">np.zeros()</strong>, which initializes all the pixels to 0:</p>
			<p class="source-code">black = np.zeros([100,100],dtype=np.uint8)  # Creates a black image</p>
			<p>The previous code creates a grayscale image with size (100, 100), composed of 10,000 unsigned bytes (<strong class="source-inline">dtype=np.uint8</strong>).</p>
			<p>To create an image with pixels with a different value than 0, you can use the <strong class="source-inline">full()</strong> method:</p>
			<p class="source-code">white = np.full([50, 50], 255, dtype=np.uint8)</p>
			<p>To change the color of all the pixels at once, it's possible to use the <strong class="source-inline">[:]</strong> notation:</p>
			<p class="source-code">img[:] = 64        # Change the pixels color to dark gray</p>
			<p>To affect only some rows, it is enough to provide a range of rows in the first index:</p>
			<p class="source-code">img[10:20] = 192   # Paints 10 rows with light gray</p>
			<p>The previous code changes the color of rows 10-20, including row 10, but excluding row 20.</p>
			<p>The same mechanism works for columns; you just need to specify the range in the second index. To instruct NumPy to include a full index, we use the <strong class="source-inline">[:]</strong> notation that we already encountered:</p>
			<p class="source-code">img[:, 10:20] = 64 # Paints 10 columns with dark gray</p>
			<p>You can <a id="_idIndexMarker007"/>also combine operations on rows and columns, selecting a rectangular area:</p>
			<p class="source-code">img[90:100, 90:100] = 0  # Paints a 10x10 area with black</p>
			<p>It is, of course, possible to operate on a single pixel, as you would do on a normal array:</p>
			<p class="source-code">img[50, 50] = 0  # Paints one pixel with black</p>
			<p>It is possible <a id="_idIndexMarker008"/>to use NumPy to select a part of an image, also called the <strong class="bold">Region Of Interest</strong> (<strong class="bold">ROI</strong>). For example, the following code copies a 10x10 <strong class="bold">ROI</strong> from the position (90, 90) to the position (80, 80):</p>
			<p class="source-code">roi = img[90:100, 90:100]</p>
			<p class="source-code">img[80:90, 80:90] = roi </p>
			<p>The following <a id="_idIndexMarker009"/>is the result of the previous operations:</p>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="Images/Figure_1.1_B16322.jpg" alt="Figure 1.1 – Some manipulation of images using NumPy slicing" width="968" height="100"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1 – Some manipulation of images using NumPy slicing</p>
			<p>To make a copy of an image, you can simply use the <strong class="source-inline">copy()</strong> method:</p>
			<p class="source-code">image2 = image.copy()</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>RGB images</h2>
			<p>RGB images <a id="_idIndexMarker010"/>differ from grayscale because they are three-dimensional, with the third index representing the three channels. Please note that OpenCV stores the images in BGR format, not RGB, so channel 0 is blue, channel 1 is green, and channel 2 is red.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">OpenCV stores the images as BGR, not RGB. In the rest of the book, when talking about RGB images, it will only mean that it is a 24-bit color image, but the internal representation will usually be BGR.</p>
			<p>To create an RGB image, we need to provide three sizes:</p>
			<p class="source-code">rgb = np.zeros([100, 100, 3],dtype=np.uint8)  </p>
			<p>If you were going to run the same code previously used on the grayscale image with the new RGB image (skipping the third index), you would get the same result. This is because NumPy would apply the same color to all the three channels, which results in a shade of gray.</p>
			<p>To select a color, it is enough to provide the third index:</p>
			<p class="source-code">rgb[:, :, 2] = 255       # Makes the image red</p>
			<p>In NumPy, it is <a id="_idIndexMarker011"/>also possible to select rows, columns, or channels that are not contiguous. You can do this by simply providing a tuple with the required indexes. To make the image magenta, you need to set the blue and red channels to <strong class="source-inline">255</strong>, which can be achieved with the following code:</p>
			<p class="source-code">rgb[:, :, (0, 2)] = 255  # Makes the image magenta</p>
			<p>You can convert an RGB image into grayscale using <strong class="source-inline">cvtColor()</strong>:</p>
			<p class="source-code">gray = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)</p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor024"/>Working with image files</h1>
			<p>OpenCV <a id="_idIndexMarker012"/>provides a very simple way to load images, using <strong class="source-inline">imread()</strong>:</p>
			<p class="source-code">import cv2</p>
			<p class="source-code">image = cv2.imread('test.jpg')</p>
			<p>To show the image, you can use <strong class="source-inline">imshow()</strong>, which accepts two parameters:</p>
			<ul>
				<li>The name to write on the caption of the window that will show the image</li>
				<li>The image to be shown</li>
			</ul>
			<p>Unfortunately, its behavior is counterintuitive, as it will not show an image unless it is followed by a call to <strong class="source-inline">waitKey()</strong>:</p>
			<p class="source-code">cv2.imshow("Image", image)cv2.waitKey(0)</p>
			<p>The call to <strong class="source-inline">waitKey()</strong> after <strong class="source-inline">imshow()</strong> will have two effects:</p>
			<ul>
				<li>It will actually allow OpenCV to show the image provided to <strong class="source-inline">imshow()</strong>.</li>
				<li>It will wait for the specified amount of milliseconds, or until a key is pressed if the amount of milliseconds passed is <strong class="source-inline">&lt;=0</strong>. It will wait indefinitely.</li>
			</ul>
			<p>An image <a id="_idIndexMarker013"/>can be saved on disk using the <strong class="source-inline">imwrite()</strong> method, which accepts three parameters:</p>
			<ul>
				<li>The name of the file</li>
				<li>The image</li>
				<li>An optional format-dependent parameter:</li>
			</ul>
			<p class="source-code">cv2.imwrite("out.jpg", image)</p>
			<p>Sometimes, it can be very useful to combine multiple pictures by putting them next to each other. Some examples in this book will use this feature extensively to compare images.</p>
			<p>OpenCV provides two methods for this purpose: <strong class="source-inline">hconcat()</strong> to concatenate the pictures horizontally and <strong class="source-inline">vconcat()</strong> to concatenate them vertically, both accepting as a parameter a list of images. Take the following example:</p>
			<p class="source-code">black = np.zeros([50, 50], dtype=np.uint8)white = np.full([50, 50], 255, dtype=np.uint8)cv2.imwrite("horizontal.jpg", cv2.hconcat([white, black]))cv2.imwrite("vertical.jpg", cv2.vconcat([white, black]))</p>
			<p>Here's the result:</p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="Images/Figure_1.2_B16322.jpg" alt="Figure 1.2 – Horizontal concatenation with hconcat() and vertical concatenation with vconcat()" width="1553" height="131"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2 – Horizontal concatenation with hconcat() and vertical concatenation with vconcat()</p>
			<p>We could use these two methods to create a chequerboard pattern:</p>
			<p class="source-code">row1 = cv2.hconcat([white, black])row2 = cv2.hconcat([black, white])cv2.imwrite("chess.jpg", cv2.vconcat([row1, row2]))</p>
			<p>You <a id="_idIndexMarker014"/>will see the following chequerboard:</p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="Images/Figure_1.3_B16322.jpg" alt="Figure 1.3 – A chequerboard pattern created using hconcat() in combination with vconcat()" width="1095" height="100"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3 – A chequerboard pattern created using hconcat() in combination with vconcat()</p>
			<p>After having worked with images, it's time we work with videos.</p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor025"/>Working with video files</h1>
			<p>Using videos <a id="_idIndexMarker015"/>in OpenCV is very simple; in fact, every frame is an image and can be manipulated with the methods that we have already analyzed.</p>
			<p>To open a video in OpenCV, you need to call the <strong class="source-inline">VideoCapture()</strong> method:</p>
			<p class="source-code">cap = cv2.VideoCapture("video.mp4")</p>
			<p>After that, you can call <strong class="source-inline">read()</strong>, typically in a loop, to retrieve a single frame. The method returns a tuple with two values:</p>
			<ul>
				<li>A Boolean value that is false when the video is finished</li>
				<li>The next frame:</li>
			</ul>
			<p class="source-code">ret, frame = cap.read()</p>
			<p>To save a video, there is the <strong class="source-inline">VideoWriter</strong> object; its constructor accepts four parameters:</p>
			<ul>
				<li>The filename</li>
				<li>A FOURCC (four-character code) of the video code</li>
				<li>The number of frames per second</li>
				<li>The resolution</li>
			</ul>
			<p>Take the following example:</p>
			<p class="source-code">mp4 = cv2.VideoWriter_fourcc(*'MP4V')writer = cv2.VideoWriter('video-out.mp4', mp4, 15, (640, 480))</p>
			<p>Once <strong class="source-inline">VideoWriter</strong> has been created, the <strong class="source-inline">write()</strong> method can be used to add a frame to the video file:</p>
			<p class="source-code">writer.write(image)</p>
			<p>When you <a id="_idIndexMarker016"/>have finished using the <strong class="source-inline">VideoCapture</strong> and <strong class="source-inline">VideoWriter</strong> objects, you should call their release method:</p>
			<p class="source-code">cap.release()</p>
			<p class="source-code">writer.release()</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Working with webcams</h2>
			<p>Webcams <a id="_idIndexMarker017"/>are handled similarly to a video in OpenCV; you just need to provide a different parameter to <strong class="source-inline">VideoCapture</strong>, which is the 0-based index identifying the webcam:</p>
			<p class="source-code">cap = cv2.VideoCapture(0)</p>
			<p>The previous code opens the first webcam; if you need to use a different one, you can specify a different index.</p>
			<p>Now, let's try manipulating some images.</p>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/>Manipulating images</h1>
			<p>As part of a <a id="_idIndexMarker018"/>computer vision pipeline for a self-driving car, with or without deep learning, you might need to process the video stream to make other algorithms work better as part of a preprocessing step.</p>
			<p>This section will provide you with a solid foundation to preprocess any video stream.</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Flipping an image</h2>
			<p>OpenCV <a id="_idIndexMarker019"/>provides the <strong class="source-inline">flip()</strong> method to flip an image, and it accepts two parameters:</p>
			<ul>
				<li>The image</li>
				<li>A number that can be 1 (horizontal flip), 0 (vertical flip), or -1 (both horizontal and vertical flip)</li>
			</ul>
			<p>Let's see a sample code:</p>
			<p class="source-code">flipH = cv2.flip(img, 1)flipV = cv2.flip(img, 0)flip = cv2.flip(img, -1)</p>
			<p>This will produce the following result:</p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="Images/Figure_1.4_B16322.jpg" alt="Figure 1.4 – Original image, horizontally flipped, vertically flipped, and both" width="1512" height="413"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.4 – Original image, horizontally flipped, vertically flipped, and both</p>
			<p>As you <a id="_idIndexMarker020"/>can see, the first image is our original image, which was flipped horizontally and vertically, and then both, horizontally and vertically together.</p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Blurring an image</h2>
			<p>Sometimes, an image <a id="_idIndexMarker021"/>can be too noisy, possibly because of some processing steps that you have done. OpenCV provides several methods to blur an image, which can help in these situations. Most likely, you will have to take into consideration not only the quality of the blur but also the speed of execution.</p>
			<p>The simplest method is <strong class="source-inline">blur()</strong>, which applies a low-pass filter to the image and requires at least two parameters:</p>
			<ul>
				<li>The image</li>
				<li>The kernel size (a bigger kernel means more blur):</li>
			</ul>
			<p class="source-code">blurred = cv2.blur(image, (15, 15))</p>
			<p>Another option is to use <strong class="source-inline">GaussianBlur()</strong>, which offers more control and requires at least three parameters:</p>
			<ul>
				<li>The image</li>
				<li>The kernel size</li>
				<li><strong class="source-inline">sigmaX</strong>, which is the standard deviation on X</li>
			</ul>
			<p>It is recommended to specify both <strong class="source-inline">sigmaX</strong> and <strong class="source-inline">sigmaY</strong> (standard deviation on Y, the forth parameter):</p>
			<p class="source-code">gaussian = cv2.GaussianBlur(image, (15, 15), sigmaX=15, sigmaY=15)</p>
			<p>An interesting blurring method is <strong class="source-inline">medianBlur()</strong>, which computes the median and therefore has the characteristic of emitting only pixels with colors present in the image (which does not necessarily happen with the previous method). It is effective at reducing "salt and pepper" noise and has two mandatory parameters:</p>
			<ul>
				<li>The image</li>
				<li>The kernel size (an odd integer greater than 1):</li>
			</ul>
			<p class="source-code">median = cv2.medianBlur(image, 15)</p>
			<p>There is also a more complex filter, <strong class="source-inline">bilateralFilter()</strong>, which is effective at removing noise while keeping the edge sharp. It is the slowest of the filters, and it requires at least four parameters:</p>
			<ul>
				<li>The image</li>
				<li>The diameter of each pixel neighborhood</li>
				<li><strong class="source-inline">sigmaColor</strong>: Filters sigma in the color space, affecting how much the different colors are mixed together, inside the pixel neighborhood</li>
				<li><strong class="source-inline">sigmaSpace</strong>: Filters sigma in the coordinate space, affecting how distant pixels affect each other, if their colors are closer than <strong class="source-inline">sigmaColor</strong>:</li>
			</ul>
			<p class="source-code">bilateral = cv2.bilateralFilter(image, 15, 50, 50)</p>
			<p>Choosing <a id="_idIndexMarker022"/>the best filter will probably require some experiments. You might also need to consider the speed. To give you some ballpark estimations based on my tests, and considering that the performance is dependent on the parameters supplied, note the following:</p>
			<ul>
				<li><strong class="source-inline">blur()</strong> is the fastest.</li>
				<li><strong class="source-inline">GaussianBlur()</strong> is similar, but it can be 2x slower than blur().</li>
				<li><strong class="source-inline">medianBlur()</strong> can easily be 20x slower than <strong class="source-inline">blur()</strong>.</li>
				<li><strong class="source-inline">BilateralFilter()</strong> is the slowest and can be 45x slower than <strong class="source-inline">blur()</strong>.</li>
			</ul>
			<p>Here are the resultant images:</p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="Images/Figure_1.5_B16322.jpg" alt="Figure 1.5 – Original, blur(), GaussianBlur(), medianBlur(), and BilateralFilter(), with the parameters used in the code samples" width="1375" height="413"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.5 – Original, blur(), GaussianBlur(), medianBlur(), and BilateralFilter(), with the parameters used in the code samples</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Changing contrast, brightness, and gamma</h2>
			<p>A very <a id="_idIndexMarker023"/>useful function is <strong class="source-inline">convertScaleAbs()</strong>, which executes several <a id="_idIndexMarker024"/>operations on all the values of the array:</p>
			<ul>
				<li>It multiplies them by the scaling parameter, <strong class="source-inline">alpha</strong>.</li>
				<li>It adds to them the delta parameter, <strong class="source-inline">beta</strong>.</li>
				<li>If the result is above 255, it is set to 255.</li>
				<li>The result is converted into an unsigned 8-bit int.</li>
			</ul>
			<p>The <a id="_idIndexMarker025"/>function accepts four parameters:</p>
			<ul>
				<li>The source image</li>
				<li>The destination (optional)</li>
				<li>The <strong class="source-inline">alpha</strong> parameter used for the scaling</li>
				<li>The <strong class="source-inline">beta</strong> delta parameter</li>
			</ul>
			<p><strong class="source-inline">convertScaleAbs()</strong> can be used to affect the contrast, as an <strong class="source-inline">alpha</strong> scaling factor above 1 increases the contrast (amplifying the color difference between pixels), while a scaling factor below one reduces it (decreasing the color difference between pixels):</p>
			<p class="source-code">cv2.convertScaleAbs(image, more_contrast, 2, 0)cv2.convertScaleAbs(image, less_contrast, 0.5, 0)</p>
			<p>It can also be used to affect the brightness, as the <strong class="source-inline">beta</strong> delta factor can be used to increase the value of all the pixels (increasing the brightness) or to reduce them (decreasing the brightness):</p>
			<p class="source-code">cv2.convertScaleAbs(image, more_brightness, 1, 64)</p>
			<p class="source-code">cv2.convertScaleAbs(image, less_brightness, 1, -64)</p>
			<p>Let's see the resulting images:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="Images/Figure_1.6_B16322.jpg" alt="Figure 1.6 – Original, more contrast (2x), less contrast (0.5x), more brightness (+64), and less brightness (-64)" width="1344" height="330"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6 – Original, more contrast (2x), less contrast (0.5x), more brightness (+64), and less brightness (-64)</p>
			<p>A more sophisticated method to change the brightness is to apply gamma correction. This can be done with a simple calculation using NumPy. A gamma value above 1 will increase the brightness, and a gamma value below 1 will reduce it:</p>
			<p class="source-code">Gamma = 1.5</p>
			<p class="source-code">g_1_5 = np.array(255 * (image / 255) ** (1 / Gamma), dtype='uint8')</p>
			<p class="source-code">Gamma = 0.7</p>
			<p class="source-code">g_0_7 = np.array(255 * (image / 255) ** (1 / Gamma), dtype='uint8')</p>
			<p>The <a id="_idIndexMarker026"/>following <a id="_idIndexMarker027"/>images will be produced:</p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="Images/Figure_1.7_B16322.jpg" alt="Figure 1.7 – Original, higher gamma (1.5), and lower gamma (0.7)" width="1650" height="357"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7 – Original, higher gamma (1.5), and lower gamma (0.7)</p>
			<p>You can <a id="_idIndexMarker028"/>see the effect of different gamma values in the middle and right images.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Drawing rectangles and text</h2>
			<p>When working <a id="_idIndexMarker029"/>on object detection tasks, it is a common need to highlight an area to see what has been detected. OpenCV provides the <strong class="source-inline">rectangle()</strong> function, accepting at least the following parameters:</p>
			<ul>
				<li>The image</li>
				<li>The upper-left corner of the rectangle</li>
				<li>The lower-right corner of the rectangle</li>
				<li>The color to use</li>
				<li>(Optional) The thickness:</li>
			</ul>
			<p class="source-code">cv2.rectangle(image, (x, y), (x + w, y + h), (255, 255, 255), 2)</p>
			<p>To write some text in the image, you can use the <strong class="source-inline">putText()</strong> method, accepting at least six parameters:</p>
			<ul>
				<li>The image</li>
				<li>The text to print</li>
				<li>The coordinates of the bottom-left corner</li>
				<li>The font face</li>
				<li>The scale factor, to change the size</li>
				<li>The color:</li>
			</ul>
			<p class="source-code">cv2.putText(image, 'Text', (x, y), cv2.FONT_HERSHEY_PLAIN, 2, clr)</p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>Pedestrian detection using HOG</h1>
			<p>The <strong class="bold">Histogram of Oriented Gradients</strong> (<strong class="bold">HOG</strong>) is <a id="_idIndexMarker030"/>an object detection technique implemented by OpenCV. In simple cases, it can be used to see whether <a id="_idIndexMarker031"/>there is a certain <a id="_idIndexMarker032"/>object present in the image, where it is, and how big it is.</p>
			<p>OpenCV includes a detector trained for pedestrians, and you are going to use it. It might not be enough for a real-life situation, but it is useful to learn how to use it. You could also train another one with more images to see whether it performs better. Later in the book, you will see how to use deep learning to detect not only pedestrians but also cars and traffic lights.</p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Sliding window</h2>
			<p>The HOG <a id="_idIndexMarker033"/>pedestrian detector in OpenCV is trained with a model that is 48x96 pixels, and therefore it is not able to detect objects smaller than that (or, better, it could, but the box will be 48x96).</p>
			<p>At the core of the HOG detector, there is a mechanism able to tell whether a given 48x96 image is a pedestrian. As this is not terribly useful, OpenCV implements a sliding window mechanism, where the detector is applied many times, on slightly different positions; the "image window" under consideration slides a bit. Once it has analyzed the whole image, the image window is increased in size (scaled) and the detector is applied again, to be able to detect bigger objects. Therefore, the detector is applied hundreds or even thousands of times for each image, which can be slow.</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Using HOG with OpenCV</h2>
			<p>First, you <a id="_idIndexMarker034"/>need to initialize <a id="_idIndexMarker035"/>the detector and specify that you want to use the detector for pedestrians:</p>
			<p class="source-code">hog = cv2.HOGDescriptor()det = cv2.HOGDescriptor_getDefaultPeopleDetector()</p>
			<p class="source-code">hog.setSVMDetector(det)</p>
			<p>Then, it is just a matter of calling <strong class="source-inline">detectMultiScale()</strong>:</p>
			<p class="source-code">(boxes, weights) = hog.detectMultiScale(image, winStride=(1, 1), padding=(0, 0), scale=1.05)</p>
			<p>The parameters that we used require some explanation, and they are as follows:</p>
			<ul>
				<li>The image</li>
				<li><strong class="source-inline">winStride</strong>, the window stride, which specifies how much the sliding window moves every time</li>
				<li>Padding, which can add some padding pixels at the border of the image (useful to detect pedestrians close to the border)</li>
				<li>Scale, which specifies how much to increase the window image every time</li>
			</ul>
			<p>You should consider that <strong class="bold">decreasing</strong> <strong class="source-inline">winSize</strong> can improve the accuracy (as more positions are considered), but it has a big impact on performance. For example, a stride of (4, 4) can be up to 16 times faster than a stride of (1, 1), though in practice, the performance <a id="_idIndexMarker036"/>difference is a bit less, maybe 10 times.</p>
			<p>In general, <strong class="bold">decreasing the scale</strong> also improves the precision and decreases the performance, though the impact is not dramatic.</p>
			<p>Improving <a id="_idIndexMarker037"/>the precision means detecting more pedestrians, but this can also increase the false positives. <strong class="source-inline">detectMultiScale()</strong> has a couple of advanced parameters that can be used for this:</p>
			<ul>
				<li><strong class="source-inline">hitThreshold</strong>, which <a id="_idIndexMarker038"/>changes the distance required from the <strong class="bold">Support Vector Machine</strong> (<strong class="bold">SVM</strong>) plane. A higher threshold means than the detector is more confident with the result.</li>
				<li><strong class="source-inline">finalThreshold</strong>, which is related to the number of detections in the same area.</li>
			</ul>
			<p>Tuning these parameters requires some experiments, but in general, a higher <strong class="source-inline">hitThreshold</strong> value (typically in the range 0–1.0) should reduce the false positives.</p>
			<p>A higher <strong class="source-inline">finalThreshold</strong> value (such as 10) will also reduce the false positives.</p>
			<p>We will use <strong class="source-inline">detectMultiScale()</strong> on an image with pedestrians generated by Carla:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="Images/Figure_1.8_B16322.jpg" alt="Figure 1.8 – HOG detection, winStride=(1, 2), scale=1.05, padding=(0, 0) Left: hitThreshold = 0, finalThreshold = 1; Center: hitThreshold = 0, inalThreshold = 3; Right: hitThreshold = 0.2, finalThreshold = 1" width="1100" height="184"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.8 – HOG detection, winStride=(1, 2), scale=1.05, padding=(0, 0)Left: hitThreshold = 0, finalThreshold = 1; Center: hitThreshold = 0, finalThreshold = 3;Right: hitThreshold = 0.2, finalThreshold = 1</p>
			<p>As you can see, we have pedestrians being detected in the image. Using a low hit threshold <a id="_idIndexMarker039"/>and a low final <a id="_idIndexMarker040"/>threshold can result in false positives, as in the left image. Your goal is to find the right balance, detecting the pedestrians but without having too many false positives.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Introduction to the camera</h2>
			<p>The camera <a id="_idIndexMarker041"/>is probabl<a id="_idTextAnchor036"/>y one of the most ubiquitous <a id="_idIndexMarker042"/>sensors in our modern world. They are used in everyday life in our mobile phones, laptops, surveillance systems, and of course, photography. They provide rich, high-resolution imagery containing extensive information about the environment, including spatial, color, and temporal information.</p>
			<p>It is no surprise that they are heavily used in self-driving technologies. One reason why the camera is so popular is that it mirrors the functionality of the human eye. For this reason, we are very comfortable using them as we connect on a deep level with their functionality, limitations, and strengths.</p>
			<p>In this section, you will learn about the following:</p>
			<ul>
				<li>Camera terminology</li>
				<li>The components of a camera</li>
				<li>Strengths and weaknesses</li>
				<li>Choosing the right camera for self-driving</li>
			</ul>
			<p>Let's discuss each in detail.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor037"/>Camera terminology</h2>
			<p>Before you <a id="_idIndexMarker043"/>learn about the components <a id="_idIndexMarker044"/>of a camera and its strengths and weaknesses, you need to know some basic terminology. These terms will be important when evaluating and ultimately choosing your camera for your self-driving application.</p>
			<h3>Field of View (FoV)</h3>
			<p>This is the <a id="_idIndexMarker045"/>vertical and horizontal angular portion of the environment (scene) that is visible to the sensor. In self-driving cars, you typically want to balance the FoV <a id="_idIndexMarker046"/>with the resolution of the sensor to ensure we see as much of the environment as possible with the least number of cameras. There is a trade space related to FoV. Larger FoV usually means more lens distortion, which you will need to compensate for in your camera calibration (see the section on camera calibration):</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="Images/Figure_1.9_B16322.jpg" alt="Figure 1.9 – Field of View, credit: https://www.researchgate.net/figure/Illustration-of-camera-lenss-field-of-view-FOV_fig4_335011596" width="1207" height="463"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.9 – Field of View, credit: <a href="https://www.researchgate.net/figure/Illustration-of-camera-lenss-field-of-view-FOV_fig4_335011596">https://www.researchgate.net/figure/Illustration-of-camera-lenss-field-of-view-FOV_fig4_335011596</a></p>
			<h3>Resolution</h3>
			<p>This is the total number of pixels in the horizontal and vertical directions on the sensor. This parameter <a id="_idIndexMarker047"/>is often discussed using the term <strong class="bold">megapixels</strong> (<strong class="bold">MP</strong>). For <a id="_idIndexMarker048"/>example, a 5 MP camera, such as the FLIR Blackfly, has a <a id="_idIndexMarker049"/>sensor with 2448 × 2048 pixels, which equates to 5,013,504 pixels.</p>
			<p>Higher resolutions allow you to use a lens with a wider FoV but still provide the detail needed for running your computer vision algorithms. This means you can use fewer cameras to cover the environment and thereby lower the cost.</p>
			<p>The Blackfly, in all <a id="_idIndexMarker050"/>its different flavors, is a common <a id="_idIndexMarker051"/>camera used in self-driving vehicles thanks to its cost, small form, reliability, robustness, and ease of integration:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="Images/Figure_1.10_New.jpg" alt="Figure 1.10 – Pixel resolution" width="1650" height="572"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.10 – Pixel resolution</p>
			<h3>Focal length</h3>
			<p>This is the <a id="_idIndexMarker052"/>length from the lens optical center to the sensor. The focal <a id="_idIndexMarker053"/>length is best thought of as the zoom of the camera. A longer focal length means you will be zoomed in closer to objects in the environment. In your self-driving car, you may choose different focal lengths based on what you need to see in the environment. For example, you might choose a relatively long focal length of 100 mm to ensure enough resolution for your classifier algorithm to detect a traffic signal at a distance far enough to allow the car to react with smooth and safe stopping:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="Images/Figure_1.11_B16322.jpg" alt="Figure 1.11 – Focal length, credit: https://photographylife.com/what-is-focal-length-in-photography" width="1099" height="335"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.11 – Focal length, credit: <a href="https://photographylife.com/what-is-focal-length-in-photography">https://photographylife.com/what-is-focal-length-in-photography</a></p>
			<h3>Aperture and f-stop</h3>
			<p>This is the <a id="_idIndexMarker054"/>opening through which light passes to shine on the sensor. The unit <a id="_idIndexMarker055"/>that is commonly used to describe the size of the opening <a id="_idIndexMarker056"/>is the f-stop, which refers to the ratio of the focal length over the aperture size. For example, a lens <a id="_idIndexMarker057"/>with a 50 mm focal length and an aperture diameter of 35 mm will equate to an f-stop of f/1.4. The following figure illustrates different aperture diameters and their f-stop values on a 50 mm focal length lens. Aperture size is very important in your self-driving <a id="_idIndexMarker058"/>car as it is directly correlated with the <strong class="bold">Depth of Field</strong> (<strong class="bold">DoF</strong>). Large apertures also allow the camera to be tolerant of obscurants (for example, bugs) that may be on the lens. Larger apertures allow light to pass around the bug and still make it to the sensor:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="Images/Figure_1.12_B16322.jpg" alt="Figure 1.12 – Aperture, credit: https://en.wikipedia.org/wiki/Aperture#/media/File:Lenses_with_different_apertures.jpg" width="1650" height="519"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.12 – Aperture, credit: <a href="https://en.wikipedia.org/wiki/Aperture#/media/File:Lenses_with_different_apertures.jpg">https://en.wikipedia.org/wiki/Aperture#/media/File:Lenses_with_different_apertures.jpg</a></p>
			<h3>Depth of field (DoF)</h3>
			<p>This is the <a id="_idIndexMarker059"/>distance range in the environment that will be in focus. This is directly correlated to the size of the aperture. Generally, in self-driving cars, you will <a id="_idIndexMarker060"/>want a deep DoF so that everything in the FoV is in focus for your computer vision algorithms. The problem is that deep DoF is achieved with a small aperture, which means less light impacting the sensor. So, you will need to balance DoF with dynamic range and ISO to ensure you see everything you need to in your environment.</p>
			<p>The following figure depicts the relationship between DoF and aperture:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="Images/Figure_1.13_B16322.jpg" alt="Figure 1.13 – DoF versus aperture, credit: https://thumbs.dreamstime.com/z/aperture-infographic-explaining-depth-field-corresponding-values-their-effect-blur-light-75823732.jpg" width="741" height="216"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.13 – DoF versus aperture, credit: <a href="https://thumbs.dreamstime.com/z/aperture-infographic-explaining-depth-field-corresponding-values-their-effect-blur-light-75823732.jpg">https://thumbs.dreamstime.com/z/aperture-infographic-explaining-depth-field-corresponding-values-their-effect-blur-light-75823732.jpg</a></p>
			<h3>Dynamic range</h3>
			<p>This is a <a id="_idIndexMarker061"/>property of the sensor that indicates its contrast ratio or the ratio of the brightest over the darkest subjects that it can resolve. This may be referred to using <a id="_idIndexMarker062"/>the unit dB (for example, 78 dB) or contrast ratio (for example, 2,000,000/1). </p>
			<p>Self-driving cars need to operate both during the day and at night. This means that the sensor needs to be sensitive enough to provide useful detail in dark conditions while not oversaturating when driving in <a id="_idIndexMarker063"/>bright sunlight. Another reason for <strong class="bold">High Dynamic Range</strong> (<strong class="bold">HDR</strong>) is the example of driving when the sun is low on the horizon. I am sure you have experienced this while driving yourself to work in the morning and the sun is right in your face and you can barely see the environment in front of you because it is saturating your eyes. HDR means that the sensor will be able to see the environment even in the face of direct sunlight. The following figure illustrates these conditions:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="Images/Figure_1.14_New.jpg" alt="Figure 1.14 – Example HDR, credit: https://petapixel.com/2011/05/02/use-iso-numbers-that-are-multiples-of-160-when-shooting-dslr-video/" width="569" height="240"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.14 – Example HDR, credit: <a href="https://petapixel.com/2011/05/02/use-iso-numbers-that-are-multiples-of-160-when-shooting-dslr-video/">https://petapixel.com/2011/05/02/use-iso-numbers-that-are-multiples-of-160-when-shooting-dslr-video/</a></p>
			<p class="callout-heading">Your dream dynamic range</p>
			<p class="callout">If you <a id="_idIndexMarker064"/>could make a wish and have whatever dynamic range <a id="_idIndexMarker065"/>you wanted in your sensor, what would it be?</p>
			<h3>International Organization for Standardization (ISO) sensitivity</h3>
			<p>This is <a id="_idIndexMarker066"/>the sensitivity of the pixels to incoming photons.</p>
			<p><em class="italic">Wait a </em><em class="italic"><a id="_idIndexMarker067"/></em><em class="italic">minute</em>, you say, <em class="italic">do you have your acronym mixed up</em>? It looks like it, but the International Organization for Standardization decided to standardize even their acronym since it would be different in every language otherwise. Thanks, ISO!</p>
			<p>The standardized ISO values can range from 100 to upward of 10,000. Lower ISO values correspond to a lower sensitivity of the sensor. Now you may ask, "why wouldn't I want the highest sensitivity?" Well, sensitivity comes at a cost...NOISE. The higher the ISO, the more noise you will see in your images. This added noise may cause trouble for your computer vision algorithms when trying to classify objects. In the following figure, you can see the effect of higher ISO values on noise in an image. These images are all taken with the lens cap on (fully dark). As you increase the ISO value, random noise starts to creep in:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="Images/Figure_1.15_New.jpg" alt="Figure 1.15 – Example ISO values and noise in a dark room" width="620" height="349"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.15 – Example ISO values and noise in a dark room</p>
			<h3>Frame rate (FPS)</h3>
			<p>This is <a id="_idIndexMarker068"/>the rate at which the sensor can obtain consecutive images, usually expressed in Hz or <strong class="bold">Frames Per Second</strong> (<strong class="bold">FPS</strong>). Generally speaking, you want <a id="_idIndexMarker069"/>to have the fastest frame rate so that fast-moving objects are not blurry in your scene. The main trade-off here is latency: the time from a real event happening until your computer vision algorithm detects it. The higher the frame rate that must be processed, the higher the latency. In the following figure, you can see the effect of frame rate on motion blur.</p>
			<p>Blur is <a id="_idIndexMarker070"/>not the only reason for choosing a higher frame rate. Depending on the speed of your vehicle, you will need a frame rate that will allow the vehicle to react if an object suddenly appears in its FoV. If your frame rate is too slow, by the time the vehicle sees something, it may be too late to react:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="Images/Figure_1.16_B16322.jpg" alt="Figure 1.16 – 120 Hz versus 60 Hz frame rate, credit: https://gadgetstouse.com/blog/2020/03/18/difference-between-60hz-90hz-120hz-displays/" width="1650" height="316"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.16 – 120 Hz versus 60 Hz frame rate, credit: <a href="https://gadgetstouse.com/blog/2020/03/18/difference-between-60hz-90hz-120hz-displays/">https://gadgetstouse.com/blog/2020/03/18/difference-between-60hz-90hz-120hz-displays/</a></p>
			<h3>Lens flare</h3>
			<p>These are <a id="_idIndexMarker071"/>the artifacts of light from an object that impact pixels on <a id="_idIndexMarker072"/>the sensor that do not correlate with the position of the object in the environment. You have likely experienced this driving at night when you see oncoming headlights. That starry effect is due to light scattered in the lens of your eye (or camera), due to imperfections, leading some of the photons to impact "pixels" that do not correlate with where the photons came from – that is, the headlights. The following figure shows what that effect looks like. You can see that the starburst makes it very difficult to see the actual object, the car!</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="Images/Figure_1.17_B16322.jpg" alt="Figure 1.17 – Lens flare from oncoming headlights, credit: https://s.blogcdn.com/cars.aol.co.uk/media/2011/02/headlights-450-a-g.jpg" width="1333" height="315"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.17 – Lens flare from oncoming headlights, credit: <a href="https://s.blogcdn.com/cars.aol.co.uk/media/2011/02/headlights-450-a-g.jpg">https://s.blogcdn.com/cars.aol.co.uk/media/2011/02/headlights-450-a-g.jpg</a></p>
			<h3>Lens distortion</h3>
			<p>This is the <a id="_idIndexMarker073"/>difference between the rectilinear or real scene to <a id="_idIndexMarker074"/>what your camera image sees. If you have ever seen action camera footage, you probably recognized the "fish-eye" lens effect. The following figure shows an extreme example of the distortion from a wide-angle lens. You will learn to correct this distortion with OpenCV:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="Images/Figure_1.18_B16322.jpg" alt="Figure 1.18 – Lens distortion, credit: https://www.slacker.xyz/post/what-lens-should-i-get" width="1486" height="734"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.18 – Lens distortion, credit: <a href="https://www.slacker.xyz/post/what-lens-should-i-get">https://www.slacker.xyz/post/what-lens-should-i-get</a></p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor038"/>The components of a camera</h2>
			<p>Like the <a id="_idIndexMarker075"/>eye, a camera is made up of <a id="_idIndexMarker076"/>a light-sensitive array, an aperture, and a lens.</p>
			<h3>Light sensitive array – CMOS sensor (the camera's retina)</h3>
			<p>The light-sensitive <a id="_idIndexMarker077"/>array, in most consumer cameras, is <a id="_idIndexMarker078"/>called a CMOS active-pixel sensor (or just a sensor). Its basic function is to convert incident <a id="_idIndexMarker079"/>photons into an electrical current that can be digitized based on the color wavelength of the photon.</p>
			<h3>The aperture (the camera's iris)</h3>
			<p>The aperture <a id="_idIndexMarker080"/>or iris of a camera is the opening through which light can pass on its way to the sensor. This can be variable or fixed depending on the type of camera you are using. The aperture is used to control parameters <a id="_idIndexMarker081"/>such as depth of field and the amount of light hitting the sensor.</p>
			<h3>The lens (the camera's lens)</h3>
			<p>The lens <a id="_idIndexMarker082"/>or optics are the components of the camera that focus the light from the environment onto the sensor. The <a id="_idIndexMarker083"/>lens primarily determines the <strong class="bold">FoV</strong> of the camera through its focal length. In self-driving applications, the FoV is very important since it determines how much of the environment the car can see with a single camera. The optics of a camera are often some of the most expensive parts and have a large impact on image quality and lens flare.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor039"/>Considerations for choosing a camera</h2>
			<p>Now that <a id="_idIndexMarker084"/>you have learned all the basics <a id="_idIndexMarker085"/>of what a camera is and the relevant terminology, it is time to learn how to choose a camera for your self-driving application. The following is a <a id="_idIndexMarker086"/>list of the primary factors that you will need to balance when choosing a camera:</p>
			<ul>
				<li>Resolution</li>
				<li>FoV</li>
				<li>Dynamic range</li>
				<li>Cost</li>
				<li>Size</li>
				<li>Ingress protection (IP rating)<p class="callout-heading">The perfect camera</p><p class="callout">If you could design the ideal camera, what would it be? </p></li>
			</ul>
			<p>My perfect self-driving camera would be able to see in all directions (spherical FoV, 360º HFoV x 360º VFoV). It would have infinite resolution and dynamic range, so you could digitally resolve objects at any distance in any lighting condition. It would be the size of a grain of rice, completely water- and dustproof, and would cost $5! Obviously, this is not possible. So, we must make some careful trade-offs for what we need.</p>
			<p>The best <a id="_idIndexMarker087"/>place to start is with your budget for cameras. This will give <a id="_idIndexMarker088"/>you an idea of what models and specifications to look for. </p>
			<p>Next, consider what you need to see for your application:</p>
			<ul>
				<li>Do you need to be able to see a child from 200 m away while traveling at 100 km/h? </li>
				<li>What coverage around the vehicle do you need, and can you tolerate any blind spots on the side of the vehicle?</li>
				<li>Do you need to see at night and during the day?</li>
			</ul>
			<p>Lastly, consider how much room you have to integrate these cameras. You probably don't want your vehicle to look like this:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="Images/Figure_1.19_B16322.jpg" alt="Figure 1.19 – Camera art, credit: https://www.flickr.com/photos/laughingsquid/1645856255/" width="1650" height="720"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.19 – Camera art, credit: <a href="https://www.flickr.com/photos/laughingsquid/1645856255/">https://www.flickr.com/photos/laughingsquid/1645856255/</a></p>
			<p>This may be very overwhelming, but it is important when thinking about how to design your computer vision system. A good camera to start with that is very popular is the FLIR Blackfly <a id="_idIndexMarker089"/>S series. They strike an excellent balance <a id="_idIndexMarker090"/>of resolution, FPS, and cost. Next, pair it with a lens that meets your FoV needs. There <a id="_idIndexMarker091"/>are some helpful FoV calculators available on the internet, such as the one from <a href="http://www.bobatkins.com/photography/technical/field_of_view.html">http://www.bobatkins.com/photography/technical/field_of_view.html</a>.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/>Strengths and weaknesses of cameras</h2>
			<p>Now, no <a id="_idIndexMarker092"/>sensor is perfect, and even your <a id="_idIndexMarker093"/>beloved camera will have its pros and cons. Let's go over some of them now.</p>
			<p>Let's look <a id="_idIndexMarker094"/>at the strengths first:</p>
			<ul>
				<li><strong class="bold">High-resolution</strong>: Relative to other sensor types, such as radar, lidar, and sonar, cameras have an excellent resolution for picking out objects in your scene. You can easily find cameras with 5 MP resolution quite cheaply.</li>
				<li><strong class="bold">Texture, color, and contrast information</strong>: Cameras provide very rich information about the environment that other sensor types just can't. This is because of a variety of wavelengths that cameras sense.</li>
				<li><strong class="bold">Cost</strong>: Cameras are one of the cheapest sensors you can find, especially for the quality of data they provide.</li>
				<li><strong class="bold">Size</strong>: CMOS technology and modern ASICs have made cameras incredibly small, many less than 30 mm cubed.</li>
				<li><strong class="bold">Range</strong>: This is really thanks to the high resolution and passive nature of the sensor.</li>
			</ul>
			<p>Next, here <a id="_idIndexMarker095"/>are the weaknesses:</p>
			<ul>
				<li><strong class="bold">A large amount of data to process for object detection</strong>: With high resolution comes a lot of data. Such is the price we pay for such accurate and detailed imagery.</li>
				<li><strong class="bold">Passive</strong>: A camera requires an external illumination source, such as the sun, headlights, and so on.</li>
				<li><strong class="bold">Obscurants (such as bugs, raindrops, heavy fog, dust, or snow)</strong>: A camera is not particularly good at seeing through heavy rain, fog, dust, or snow. Radars are typically better suited for this.</li>
				<li><strong class="bold">Lack native depth/velocity information</strong>: A camera image alone doesn't give you <a id="_idIndexMarker096"/>any information on an object's speed or distance.<p>Photogrammetry is helping to bolster this weakness but costs valuable processing resources (GPU, CPU, latency, and so on.) It is also less accurate than a radar or lidar sensor, which produce this information natively.</p></li>
			</ul>
			<p>Now <a id="_idIndexMarker097"/>that you have a good <a id="_idIndexMarker098"/>understanding of how a camera works, as well as its basic parts and terminology, it's time to get your hands dirty and start calibrating a camera with OpenCV.</p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor041"/>Camera calibration with OpenCV</h1>
			<p>In this <a id="_idIndexMarker099"/>section, you will learn how to take objects with a known <a id="_idIndexMarker100"/>pattern and use them to correct lens distortion using OpenCV.</p>
			<p>Remember the lens distortion we talked about in the previous section? You need to correct this to ensure you accurately locate where objects are relative to your vehicle. It does you no good to see an object if you don't know whether it is in front of you or next to you. Even good lenses can distort the image, and this is particularly true for wide-angle lenses. Luckily, OpenCV provides a mechanism to detect this distortion and correct it! </p>
			<p>The idea is to take pictures of a chessboard, so OpenCV can use this high-contrast pattern to detect the position of the points and compute the distortion based on the difference between the expected image and the recorded one.</p>
			<p>You need to provide several pictures at different orientations. It might take some experiments to find a good set of pictures, but 10 to 20 images should be enough. If you use a printed chessboard, take care to have the paper as flat as possible so as to not compromise <a id="_idIndexMarker101"/>the measurements:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="Images/Figure_1.20_B16322.jpg" alt="Figure 1.20 – Some examples of pictures that can be used for calibration" width="774" height="148"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.20 – Some examples of pictures that can be used for calibration</p>
			<p>As you <a id="_idIndexMarker102"/>can see, the central image clearly shows some barrel distortion.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>Distortion detection</h2>
			<p>OpenCV tries <a id="_idIndexMarker103"/>to map a series of three-dimensional points to the two-dimensional <a id="_idIndexMarker104"/>coordinates of the camera. OpenCV will then use this information to correct the distortion.</p>
			<p>The first thing to do is to initialize some structures:</p>
			<p class="source-code">image_points = []   # 2D points object_points = []  # 3D points coords = np.zeros((1, nX * nY, 3), np.float32)coords[0,:,:2] = np.mgrid[0:nY, 0:nX].T.reshape(-1, 2)</p>
			<p>Please note <strong class="source-inline">nX</strong> and <strong class="source-inline">nY</strong>, which are the number of points to find in the chessboard on the <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> axes,, respectively. In practice, this is the number of squares minus 1.</p>
			<p>Then, we need to call <strong class="source-inline">findChessboardCorners()</strong>:</p>
			<p class="source-code">found, corners = cv2.findChessboardCorners(image, (nY, nX), None)</p>
			<p><strong class="source-inline">found</strong> is true if OpenCV found the points, and <strong class="source-inline">corners</strong> will contain the points found.</p>
			<p>In our code, we will assume that the image has been converted into grayscale, but you can calibrate using an RGB picture as well.</p>
			<p>OpenCV provides a nice image depicting the corners found, ensuring that the algorithm is working properly:</p>
			<p class="source-code">out = cv2.drawChessboardCorners(image, (nY, nX), corners, True)object_points.append(coords)   # Save 3d points image_points.append(corners)   # Save corresponding 2d points</p>
			<p>Let's <a id="_idIndexMarker105"/>see the <a id="_idIndexMarker106"/>resulting image:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="Images/Figure_1.21_B16322.jpg" alt="Figure 1.21 – Corners of the calibration image found by OpenCV" width="1650" height="290"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.21 – Corners of the calibration image found by OpenCV</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor043"/>Calibration</h2>
			<p>After finding <a id="_idIndexMarker107"/>the corners in several images, we are finally <a id="_idIndexMarker108"/>ready to generate the calibration data using <strong class="source-inline">calibrateCamera()</strong>:</p>
			<p class="source-code">ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(object_points, image_points, shape[::-1], None, None)</p>
			<p>Now, we are ready to correct our images, using <strong class="source-inline">undistort()</strong>:</p>
			<p class="source-code">dst = cv2.undistort(image, mtx, dist, None, mtx)</p>
			<p>Let's see the result:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="Images/Figure_1.22_B16322.jpg" alt="Figure 1.22 – Original image and calibrated image" width="763" height="221"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.22 – Original image and calibrated image</p>
			<p>We can see that the second image has less barrel distortion, but it is not great. We probably need more and better calibration samples.</p>
			<p>But we can <a id="_idIndexMarker109"/>also try to get more precision from the same calibration images by looking for <strong class="bold">sub-pixel precision</strong> when looking for the corners. This <a id="_idIndexMarker110"/>can be done by calling <strong class="source-inline">cornerSubPix()</strong> after <strong class="source-inline">findChessboardCorners()</strong>:</p>
			<p class="source-code">corners = cv2.cornerSubPix(image, corners, (11, 11), (-1, -1), (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001))</p>
			<p>The <a id="_idIndexMarker111"/>following is the resulting image:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="Images/Figure_1.23_B16322.jpg" alt="Figure 1.23 – Image calibrated with sub-pixel precision" width="1232" height="433"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.23 – Image calibrated with sub-pixel precision</p>
			<p>As the <a id="_idIndexMarker112"/>complete code is a bit long, I recommend checking out <a id="_idIndexMarker113"/>the full source code on GitHub.</p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor044"/>Summary</h1>
			<p>Well, you have had a great start to your computer vision journey toward making a real self-driving car.</p>
			<p>You learned about a very useful toolset called OpenCV with bindings for Python and NumPy. With these tools, you are now able to create and import images using methods such as <strong class="source-inline">imread()</strong>, <strong class="source-inline">imshow()</strong>, <strong class="source-inline">hconcat()</strong>, and <strong class="source-inline">vconcat()</strong>. You learned how to import and create video files, as well as capturing video from a webcam with methods such as <strong class="source-inline">VideoCapture()</strong> and <strong class="source-inline">VideoWriter()</strong>. Watch out Spielberg, there is a new movie-maker in town!</p>
			<p>It was wonderful to be able to import images, but how do you start manipulating them to help your computer vision algorithms learn what features matter? You learned how to do this through methods such as <strong class="source-inline">flip()</strong>, <strong class="source-inline">blur()</strong>, <strong class="source-inline">GaussianBlur()</strong>, <strong class="source-inline">medianBlur()</strong>, <strong class="source-inline">bilateralFilter()</strong>, and <strong class="source-inline">convertScaleAbs()</strong>. Then, you learned how to annotate images for human consumption with methods such as <strong class="source-inline">rectangle()</strong> and <strong class="source-inline">putText()</strong>.</p>
			<p>Then came the real magic, where you learned how to take the images and do your first piece of real computer vision using HOG to detect pedestrians. You learned how to apply a sliding window to scan the detector over an image in various sized windows using the <strong class="source-inline">detectMultiScale()</strong> method, with parameters such as <strong class="source-inline">winStride</strong>, <strong class="source-inline">padding</strong>, <strong class="source-inline">scale</strong>, <strong class="source-inline">hitThreshold</strong>, and <strong class="source-inline">finalThreshold</strong>.</p>
			<p>You had a lot of fun with all the new tools you learned for working with images. But there was something missing. How do I get these images on my self-driving car? To answer this, you learned about the camera and its basic terminology, such as <strong class="bold">resolution</strong>, <strong class="bold">FoV</strong>, <strong class="bold">focal length</strong>, <strong class="bold">aperture</strong>, <strong class="bold">DoF</strong>, <strong class="bold">dynamic range</strong>, <strong class="bold">ISO</strong>, <strong class="bold">frame rate</strong>, <strong class="bold">lens flare</strong>, and finally, <strong class="bold">lens distortion</strong>. Then, you learned the basic components that comprise a camera, namely the lens, aperture, and light-sensitive arrays. With these basics, you moved on to some considerations for choosing a camera for your application by learning about the strengths and weaknesses of a camera.</p>
			<p>Armed with this knowledge, you boldly began to remove one of these weaknesses, lens distortion, with the tools you learned in OpenCV for distortion correction. You used methods such as <strong class="source-inline">findChessboardCorners()</strong>, <strong class="source-inline">calibrateCamera()</strong>, <strong class="source-inline">undistort()</strong>, and <strong class="source-inline">cornerSubPix()</strong> for this.</p>
			<p>Wow, you are really on your way to being able to perceive the world in your self-driving application. You should take a moment and be proud of what you have learned so far. Maybe you can celebrate with a selfie and apply some of what you learned!</p>
			<p>In the next chapter, you are going to learn about some of the basic signal types and protocols you are likely to encounter when trying to integrate sensors in your self-driving application.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor045"/>Questions</h1>
			<ol>
				<li value="1">Can OpenCV take advantage of hardware acceleration?</li>
				<li>What's the best blurring method if CPU power is not a problem?</li>
				<li>Which detector can be used to find pedestrians in an image?</li>
				<li>How can you read the video stream from a webcam?</li>
				<li>What is the trade-off between aperture and depth of field?</li>
				<li>When do you need a high ISO?</li>
				<li>Is it worth computing sub-pixel precision for camera calibration?</li>
			</ol>
		</div>
	</div>



  </body></html>