- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The work that researchers do to prepare data for analysis – extraction, transformation,
    cleaning, and exploration – has not changed fundamentally with the increased popularity
    of machine learning tools. When we prepared data for multivariate analyses 30
    years ago, we were every bit as concerned with missing values, outliers, the shape
    of the distribution of our variables, and how variables correlate, as we are when
    we use machine learning algorithms now. Although it is true that widespread use
    of the same libraries for machine learning (scikit-learn, TensorFlow, PyTorch,
    and others) does encourage greater uniformity in approach, good data cleaning
    and exploration practices are largely unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: How we talk about machine learning is still very much algorithm-focused; just
    choose the right model and organization-changing insights will follow. But we
    have to make room for the same kind of learning from data that we have been engaged
    in over the last few decades, where the predictions we make from data, our modeling
    of relationships in the data, and our cleaning and exploration of that data are
    very much part of the conversation. Getting our models right has as much to do
    with gleaning as much information as we can from a histogram or a confusion matrix
    as from carefully tuning hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the work that data analysts and scientists do does not progress neatly
    from cleaning, to exploration, to preprocessing, to modeling, to evaluation. We
    have potential models in mind at each step of the process, regularly updating
    our previous models. For example, we may initially think that we will be using
    logistic regression to model a particular binary target but then recognize when
    we see the distribution of features that we might need to at least try using random
    forest classification. We will discuss implications for modeling throughout this
    text, even when explaining relatively routine data cleaning tasks. We will also
    explore the use of machine learning tools early in the process to help us identify
    anomalies, impute values, and select features.
  prefs: []
  type: TYPE_NORMAL
- en: This points to another change in the workflow of data analysts and scientists
    over the last decade – less emphasis on *the one model* and greater acceptance
    of model building as an iterative process. A project might require multiple machine
    learning algorithms – for example, principal component analysis to reduce dimensions
    (the number of features) and then logistic regression for classification.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, there is one key difference in our approach to data cleaning,
    exploration, and modeling as machine learning tools guide more of our work – an
    increased emphasis on prediction over an understanding of the underlying data.
    We are more concerned with how well our features (also known as independent variables,
    inputs, or predictors) predict our targets (dependent variables, outputs, responses)
    than with the relationships between features and the underlying structure of our
    data. I point out throughout the first two sections of this book how that alters
    our focus somewhat, even when we are cleaning and exploring our data.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I had multiple audiences in mind as I wrote this book, but I most consistently
    thought about a dear friend of mine who bought a Transact-SQL book 30 years ago
    and instantly developed great confidence in her database work, ultimately building
    a career around those skills. I would love it if someone just starting their career
    as a data scientist or analyst worked through this book and had a similar experience
    as my friend. More than anything else, I want you to feel good and excited about
    what you can do as a result of reading this book.
  prefs: []
  type: TYPE_NORMAL
- en: I also hope this book will be a useful reference for folks who have been doing
    this kind of work for a while. Here, I imagine someone opening the book and wondering
    to themselves, *what are good values to use in my grid search for my logistic
    regression model?*
  prefs: []
  type: TYPE_NORMAL
- en: In keeping with the hands-on nature of this text, every bit of output is reproducible
    with code in this book. I also stuck to a rule throughout, even when it was challenging.
    Every section, except for the conceptual sections, starts with raw data largely
    unchanged from the original downloaded file. You go from data file to model in
    each section. If you have forgotten how a particular object was created, all you
    will ever need to do is turn back a page or two to see.
  prefs: []
  type: TYPE_NORMAL
- en: Readers who have some knowledge of pandas and NumPy will have an easier time
    with some code blocks, as will folks with some knowledge of Python and scikit-learn.
    None of that is essential though. There are just some sections you might want
    to pause over longer. If you need additional instruction on doing data work with
    Python, my *Python Data Cleaning Cookbook* is a good companion book I think.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 1*](B17978_01_ePub.xhtml#_idTextAnchor014), *Examining the Distribution
    of Features and Targets*, explores using common NumPy and pandas techniques to
    get a better sense of the attributes of our data. We will generate summary statistics,
    such as `mean`, `min`, and `max`, and standard deviation, and count the number
    of missings. We will also create visualizations of key features, including histograms
    and boxplots, to give us a better sense of the distribution of each feature than
    we can get by just looking at summary statistics. We will hint at the implications
    of feature distribution for data transformation, encoding and scaling, and the
    modeling that we will be doing in subsequent chapters with the same data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B17978_02_ePub.xhtml#_idTextAnchor025), *Examining Bivariate
    and Multivariate Relationships between Features and Targets*, focuses on the correlation
    between possible features and target variables. We will use pandas methods for
    bivariate analysis, and Matplotlib for visualizations. We will discuss the implications
    of what we find for feature engineering and modeling. We also use multivariate
    techniques in this chapter to understand the relationship between features.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B17978_03_ePub.xhtml#_idTextAnchor034), *Identifying and Fixing
    Missing Values*, goes over techniques for identifying missing values for each
    feature or target, and for identifying observations where values for a large number
    of the features are absent. We will explore strategies for imputing values, such
    as setting values to the overall mean, to the mean for a given category, or forward
    filling. We will also examine multivariate techniques for imputing values for
    missings and discuss when they are appropriate.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B17978_04_ePub.xhtml#_idTextAnchor043), *Encoding, Transforming,
    and Scaling Features*, covers a range of feature engineering techniques. We will
    use tools to drop redundant or highly correlated features. We will explore the
    most common kinds of encoding – one-hot, ordinal, and hashing encoding. We will
    also use transformations to improve the distribution of our features. Finally,
    we will use common binning and scaling approaches to address skew, kurtosis, and
    outliers, and to adjust for features with widely different ranges.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058), *Feature Selection* will
    go over a number of feature selection methods, from filter, to wrapper, to embedded
    methods. We will explore how they work with categorical and continuous targets.
    For wrapper and embedded methods, we consider how well they work with different
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing for Model
    Evaluation*, will see us build our first full-fledged pipeline, separating our
    data into testing and training datasets, and learning how to do preprocessing
    without data leakage. We will implement cross-validation with k-fold and look
    more closely into assessing the performance of our models.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B17978_07_ePub.xhtml#_idTextAnchor091), *Linear Regression Models*,
    is the first of several chapters on building regression models with an old favorite
    of many data scientists, linear regression. We will run a classical linear model
    while also examining the qualities of a feature space that make it a good candidate
    for a linear model. We will explore how to improve linear models, when necessary,
    with regularization and transformations. We will look into stochastic gradient
    descent as an alternative to **ordinary least square** (**OLS**) optimization.
    We will also learn how to do hyperparameter tuning with grid searches.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B17978_08_ePub.xhtml#_idTextAnchor106), *Support Vector Regression*,
    discusses key support vector machine concepts and how they can be applied to regression
    problems. In particular, we will examine how concepts such as epsilon-insensitive
    tubes and soft margins can give us the flexibility to get the best fit possible,
    given our data and domain-related challenges. We will also explore, for the first
    time but definitely not the last, the very handy kernel trick, which allows us
    to model nonlinear relationships without transformations or increasing the number
    of features.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17978_09_ePub.xhtml#_idTextAnchor113), *K-Nearest Neighbors,
    Decision Tree, Random Forest, and Gradient Boosted Regression*, explores some
    of the most popular non-parametric regression algorithms. We will discuss the
    advantages of each algorithm, when you might want to choose one over the other,
    and possible modeling challenges. These challenges include how to avoid underfitting
    and overfitting with careful adjusting of hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126), *Logistic Regression*,
    is the first of several chapters on building classification models with logistic
    regression, an efficient algorithm with low bias. We will carefully examine the
    assumptions of logistic regression and discuss the attributes of a dataset and
    a modeling problem that make logistic regression a good choice. We will use regularization
    to address high variance or when we have a number of highly correlated predictors.
    We will extend the algorithm to multiclass problems with multinomial logistic
    regression. We will also discuss how to handle class imbalance for the first,
    but not the last, time.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B17978_11_ePub.xhtml#_idTextAnchor135), *Decision Trees and
    Random Forest Classification*, returns to the decision tree and random forest
    algorithms that were introduced in [*Chapter 9*](B17978_09_ePub.xhtml#_idTextAnchor113),
    *K-Nearest Neighbors, Decision Tree, Random Forest, and Gradient Boosted Regression*,
    this time dealing with classification problems. This gives us another opportunity
    to learn how to construct and interpret decision trees. We will adjust key hyperparameters,
    including the depth of trees, to avoid overfitting. We will then explore random
    forest and gradient boosted decision trees as good, lower variance alternatives
    to decision trees.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B17978_12_ePub.xhtml#_idTextAnchor144), *K-Nearest Neighbors
    for Classification*, returns to **k-nearest neighbors** (**KNNs**) to handle both
    binary and multiclass modeling problems. We will discuss and demonstrate the advantages
    of KNN – how easy it is to build a no-frills model and the limited number of hyperparameters
    to adjust. By the end of the chapter, we will know both – how to do KNN and when
    we should consider it for our modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B17978_13_ePub.xhtml#_idTextAnchor152), *Support Vector Machine
    Classification*, explores different strategies for implementing **support vector
    classification** (**SVC**). We will use linear SVC, which can perform very well
    when our classes are linearly separable. We will then examine how to use the kernel
    trick to extend SVC to cases where the classes are not linearly separable. Finally,
    we will use one-versus-one and one-versus-rest classification to handle targets
    with more than two values.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B17978_14_ePub.xhtml#_idTextAnchor162), *Naïve Bayes Classification*,
    discusses the fundamental assumptions of naïve Bayes in this chapter and how the
    algorithm is used to tackle some of the modeling challenges we have already explored,
    as well as some new ones, such as text classification. We will consider when naïve
    Bayes is a good option and when it is not. We will also examine the interpretation
    of naïve Bayes models.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B17978_15_ePub.xhtml#_idTextAnchor170), *Principal Component
    Analysis*, examines **principal component analysis** (**PCA**), including how
    it works and when we might want to use it. We will learn how to interpret the
    components created from PCA, including how each feature contributes to each component
    and how much of the variance is explained. We will learn how to visualize components
    and how to use components in subsequent analyses. We will also examine how to
    use kernels for PCA and when that might give us better results.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B17978_16_ePub.xhtml#_idTextAnchor177), *K-Means and DBSCAN
    Clustering*, explores two popular clustering techniques, k-means and **Density-based
    spatial clustering of applications with noise** (**DBSCAN**). We will discuss
    the strengths of each approach and develop a sense of when to choose one clustering
    algorithm over the other. We will also learn how to evaluate our clusters and
    how to change hyperparameters to improve our model.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To run the code in this book, you will need to have installed a scientific distribution
    of Python, such as Anaconda. All code was tested with scikit-learn versions 0.24.2
    and 1.0.2.
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning).
    If there’s an update to the code, it will be updated in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that has color images of the screenshots and diagrams
    used in this book. You can download it here: [https://packt.link/aLE6J](https://packt.link/aLE6J).'
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: “For learning purposes, we have provided two example
    `mlruns` artifacts and the `huggingface` cache folder in the GitHub repository
    under the `chapter08` folder.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold**. Here is an example:
    “To execute the code in this cell, you can just click on **Run Cell** in the top-right
    drop-down menu.”'
  prefs: []
  type: TYPE_NORMAL
- en: Tips or Important Notes
  prefs: []
  type: TYPE_NORMAL
- en: Appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '`customercare@packtpub.com` and mention the book title in the subject of your
    message.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    and fill in the form.'
  prefs: []
  type: TYPE_NORMAL
- en: '`copyright@packt.com` with a link to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Share Your Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you’ve read *Data Cleaning and Exploration with Machine Learning*, we’d
    love to hear your thoughts! Please click here to go straight to the Amazon review
    page for this book and share your feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL
