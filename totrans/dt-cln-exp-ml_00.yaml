- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: The work that researchers do to prepare data for analysis – extraction, transformation,
    cleaning, and exploration – has not changed fundamentally with the increased popularity
    of machine learning tools. When we prepared data for multivariate analyses 30
    years ago, we were every bit as concerned with missing values, outliers, the shape
    of the distribution of our variables, and how variables correlate, as we are when
    we use machine learning algorithms now. Although it is true that widespread use
    of the same libraries for machine learning (scikit-learn, TensorFlow, PyTorch,
    and others) does encourage greater uniformity in approach, good data cleaning
    and exploration practices are largely unchanged.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员为数据分析准备数据的工作——包括提取、转换、清洗和探索——在机器学习工具日益普及的情况下，其本质并没有发生根本变化。30年前，当我们为多元分析准备数据时，我们同样关注缺失值、异常值、变量分布的形状以及变量之间的相关性，就像我们现在使用机器学习算法时一样。虽然确实，广泛使用相同的机器学习库（如scikit-learn、TensorFlow、PyTorch等）确实鼓励了方法上的更大一致性，但良好的数据清洗和探索实践在很大程度上并未改变。
- en: How we talk about machine learning is still very much algorithm-focused; just
    choose the right model and organization-changing insights will follow. But we
    have to make room for the same kind of learning from data that we have been engaged
    in over the last few decades, where the predictions we make from data, our modeling
    of relationships in the data, and our cleaning and exploration of that data are
    very much part of the conversation. Getting our models right has as much to do
    with gleaning as much information as we can from a histogram or a confusion matrix
    as from carefully tuning hyperparameters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们谈论机器学习的方式仍然非常侧重于算法；只需选择正确的模型，随之而来的就是组织变革的洞见。但我们必须为过去几十年中我们一直在进行的数据学习留出空间，其中我们从数据中做出的预测、在数据中建模关系以及我们对数据的清洗和探索都是对话的一部分。确保我们的模型正确与从直方图或混淆矩阵中获取尽可能多的信息一样重要，需要仔细调整超参数。
- en: Similarly, the work that data analysts and scientists do does not progress neatly
    from cleaning, to exploration, to preprocessing, to modeling, to evaluation. We
    have potential models in mind at each step of the process, regularly updating
    our previous models. For example, we may initially think that we will be using
    logistic regression to model a particular binary target but then recognize when
    we see the distribution of features that we might need to at least try using random
    forest classification. We will discuss implications for modeling throughout this
    text, even when explaining relatively routine data cleaning tasks. We will also
    explore the use of machine learning tools early in the process to help us identify
    anomalies, impute values, and select features.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，数据分析师和科学家的工作并不从清洗、探索、预处理、建模到评估这一过程有序推进。我们在过程的每一步都怀有潜在模型的想法，并定期更新我们之前的模型。例如，我们最初可能认为我们将使用逻辑回归来建模特定的二元目标，但当我们看到特征的分布时，我们可能至少需要尝试使用随机森林分类。我们将在整篇文章中讨论建模的影响，即使在解释相对常规的数据清洗任务时也是如此。我们还将探讨在早期过程中使用机器学习工具，以帮助我们识别异常、插补值和选择特征。
- en: This points to another change in the workflow of data analysts and scientists
    over the last decade – less emphasis on *the one model* and greater acceptance
    of model building as an iterative process. A project might require multiple machine
    learning algorithms – for example, principal component analysis to reduce dimensions
    (the number of features) and then logistic regression for classification.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这指向了数据分析师和科学家在过去十年中工作流程的另一个变化——对“单一模型”的重视减少，以及对模型构建作为迭代过程的接受度提高。一个项目可能需要多个机器学习算法——例如，主成分分析来降低维度（特征的数量），然后是逻辑回归进行分类。
- en: That being said, there is one key difference in our approach to data cleaning,
    exploration, and modeling as machine learning tools guide more of our work – an
    increased emphasis on prediction over an understanding of the underlying data.
    We are more concerned with how well our features (also known as independent variables,
    inputs, or predictors) predict our targets (dependent variables, outputs, responses)
    than with the relationships between features and the underlying structure of our
    data. I point out throughout the first two sections of this book how that alters
    our focus somewhat, even when we are cleaning and exploring our data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们在数据清洗、探索和建模方面的方法有一个关键的区别——随着机器学习工具在我们的工作中扮演越来越重要的角色，我们对预测的重视超过了对底层数据的理解。我们更关心我们的特征（也称为自变量、输入或预测因子）如何预测我们的目标（因变量、输出、响应），而不是特征之间的关系以及我们数据的底层结构。我在本书的前两章中指出了这一点如何改变我们的关注点，即使我们在清洗和探索数据时也是如此。
- en: Who this book is for
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书面向的对象
- en: I had multiple audiences in mind as I wrote this book, but I most consistently
    thought about a dear friend of mine who bought a Transact-SQL book 30 years ago
    and instantly developed great confidence in her database work, ultimately building
    a career around those skills. I would love it if someone just starting their career
    as a data scientist or analyst worked through this book and had a similar experience
    as my friend. More than anything else, I want you to feel good and excited about
    what you can do as a result of reading this book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写这本书时，我考虑了多个受众，但我最经常想到的是我的一个好朋友，30年前她买了一本 Transact-SQL 书，立刻对自己的数据库工作有了极大的信心，最终围绕这些技能建立起了自己的职业生涯。如果有人刚开始作为数据科学家或分析师的职业生涯，通过这本书并获得了与我朋友相似的经历，我将感到非常高兴。最重要的是，我希望你通过阅读这本书后感到满意和兴奋，对你可以做到的事情感到自豪。
- en: I also hope this book will be a useful reference for folks who have been doing
    this kind of work for a while. Here, I imagine someone opening the book and wondering
    to themselves, *what are good values to use in my grid search for my logistic
    regression model?*
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我还希望这本书对那些已经从事这类工作一段时间的人来说是一本有用的参考书。在这里，我想象有人打开这本书，自己问自己，*在我的逻辑回归模型网格搜索中，应该使用哪些好的值？*
- en: In keeping with the hands-on nature of this text, every bit of output is reproducible
    with code in this book. I also stuck to a rule throughout, even when it was challenging.
    Every section, except for the conceptual sections, starts with raw data largely
    unchanged from the original downloaded file. You go from data file to model in
    each section. If you have forgotten how a particular object was created, all you
    will ever need to do is turn back a page or two to see.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持本书的实践性质，书中的每一部分输出都可以通过本书中的代码进行重现。我始终坚持一个规则，即使有时会遇到挑战。除了概念性章节外，每个章节都是从原始下载文件中基本未变的数据开始的。你将在每个章节中从数据文件到模型进行转换。如果你忘记了某个特定对象是如何创建的，你只需要翻回一页或两页就能看到。
- en: Readers who have some knowledge of pandas and NumPy will have an easier time
    with some code blocks, as will folks with some knowledge of Python and scikit-learn.
    None of that is essential though. There are just some sections you might want
    to pause over longer. If you need additional instruction on doing data work with
    Python, my *Python Data Cleaning Cookbook* is a good companion book I think.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对 pandas 和 NumPy 有一定了解的读者来说，在处理一些代码块时会更加得心应手，同样，对 Python 和 scikit-learn
    有一定了解的人也是如此。尽管如此，这些都不是必需的。有些部分你可能需要花更多的时间去仔细阅读。如果你需要关于使用 Python 进行数据工作的额外指导，我认为我的
    *Python 数据清洗食谱* 是一本很好的配套书籍。
- en: What this book covers
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书涵盖的内容
- en: '[*Chapter 1*](B17978_01_ePub.xhtml#_idTextAnchor014), *Examining the Distribution
    of Features and Targets*, explores using common NumPy and pandas techniques to
    get a better sense of the attributes of our data. We will generate summary statistics,
    such as `mean`, `min`, and `max`, and standard deviation, and count the number
    of missings. We will also create visualizations of key features, including histograms
    and boxplots, to give us a better sense of the distribution of each feature than
    we can get by just looking at summary statistics. We will hint at the implications
    of feature distribution for data transformation, encoding and scaling, and the
    modeling that we will be doing in subsequent chapters with the same data.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第一章*](B17978_01_ePub.xhtml#_idTextAnchor014)，*检查特征和目标的分布*，探讨了使用常见的NumPy和pandas技术来更好地了解数据的属性。我们将生成汇总统计量，如`均值`、`最小值`和`最大值`，以及标准差，并计算缺失值的数量。我们还将创建关键特征的可视化，包括直方图和箱线图，以比仅查看汇总统计量更好地了解每个特征的分布。我们将暗示特征分布对数据转换、编码和缩放以及我们在后续章节中用相同数据进行建模的影响。'
- en: '[*Chapter 2*](B17978_02_ePub.xhtml#_idTextAnchor025), *Examining Bivariate
    and Multivariate Relationships between Features and Targets*, focuses on the correlation
    between possible features and target variables. We will use pandas methods for
    bivariate analysis, and Matplotlib for visualizations. We will discuss the implications
    of what we find for feature engineering and modeling. We also use multivariate
    techniques in this chapter to understand the relationship between features.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第二章*](B17978_02_ePub.xhtml#_idTextAnchor025)，*检查特征和目标之间的双变量和多变量关系*，专注于可能特征和目标变量之间的相关性。我们将使用pandas方法进行双变量分析，并使用Matplotlib进行可视化。我们将讨论我们发现的特征工程和建模的影响。我们还在本章中使用多元技术来理解特征之间的关系。'
- en: '[*Chapter 3*](B17978_03_ePub.xhtml#_idTextAnchor034), *Identifying and Fixing
    Missing Values*, goes over techniques for identifying missing values for each
    feature or target, and for identifying observations where values for a large number
    of the features are absent. We will explore strategies for imputing values, such
    as setting values to the overall mean, to the mean for a given category, or forward
    filling. We will also examine multivariate techniques for imputing values for
    missings and discuss when they are appropriate.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第三章*](B17978_03_ePub.xhtml#_idTextAnchor034)，*识别和修复缺失值*，将介绍识别每个特征或目标缺失值的技术，以及识别大量特征值缺失的观测值。我们将探讨填充值的方法，例如将值设置为整体均值、给定类别的均值或前向填充。我们还将检查用于填充缺失值的多元技术，并讨论它们何时适用。'
- en: '[*Chapter 4*](B17978_04_ePub.xhtml#_idTextAnchor043), *Encoding, Transforming,
    and Scaling Features*, covers a range of feature engineering techniques. We will
    use tools to drop redundant or highly correlated features. We will explore the
    most common kinds of encoding – one-hot, ordinal, and hashing encoding. We will
    also use transformations to improve the distribution of our features. Finally,
    we will use common binning and scaling approaches to address skew, kurtosis, and
    outliers, and to adjust for features with widely different ranges.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第四章*](B17978_04_ePub.xhtml#_idTextAnchor043)，*编码、转换和缩放特征*，涵盖了各种特征工程技术。我们将使用工具删除冗余或高度相关的特征。我们将探索最常见的编码类型——独热编码、有序编码和哈希编码。我们还将使用转换来改善特征的分布。最后，我们将使用常见的分箱和缩放方法来解决偏斜、峰度和异常值，以及调整范围差异较大的特征。'
- en: '[*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058), *Feature Selection* will
    go over a number of feature selection methods, from filter, to wrapper, to embedded
    methods. We will explore how they work with categorical and continuous targets.
    For wrapper and embedded methods, we consider how well they work with different
    algorithms.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第五章*](B17978_05_ePub.xhtml#_idTextAnchor058)，*特征选择*将介绍多种特征选择方法，从过滤器到包装器，再到嵌入式方法。我们将探讨它们如何与分类和连续目标一起工作。对于包装器和嵌入式方法，我们将考虑它们与不同算法结合时的效果。'
- en: '[*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing for Model
    Evaluation*, will see us build our first full-fledged pipeline, separating our
    data into testing and training datasets, and learning how to do preprocessing
    without data leakage. We will implement cross-validation with k-fold and look
    more closely into assessing the performance of our models.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第六章*](B17978_06_ePub.xhtml#_idTextAnchor078)，*为模型评估做准备*，将展示我们构建第一个完整的流水线，将数据分为测试集和训练集，并学习如何在没有数据泄露的情况下进行预处理。我们将实现k折交叉验证，并更深入地研究评估模型性能的方法。'
- en: '[*Chapter 7*](B17978_07_ePub.xhtml#_idTextAnchor091), *Linear Regression Models*,
    is the first of several chapters on building regression models with an old favorite
    of many data scientists, linear regression. We will run a classical linear model
    while also examining the qualities of a feature space that make it a good candidate
    for a linear model. We will explore how to improve linear models, when necessary,
    with regularization and transformations. We will look into stochastic gradient
    descent as an alternative to **ordinary least square** (**OLS**) optimization.
    We will also learn how to do hyperparameter tuning with grid searches.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第7章*](B17978_07_ePub.xhtml#_idTextAnchor091)，*线性回归模型*，是关于使用许多数据科学家喜爱的老方法——线性回归来构建回归模型的几个章节中的第一个。我们将运行一个经典的线性模型，同时考察使特征空间成为线性模型良好候选者的特性。我们将探讨在必要时如何通过正则化和变换来改进线性模型。我们将研究随机梯度下降作为**普通最小二乘法**（**OLS**）优化的替代方案。我们还将学习如何使用网格搜索进行超参数调整。'
- en: '[*Chapter 8*](B17978_08_ePub.xhtml#_idTextAnchor106), *Support Vector Regression*,
    discusses key support vector machine concepts and how they can be applied to regression
    problems. In particular, we will examine how concepts such as epsilon-insensitive
    tubes and soft margins can give us the flexibility to get the best fit possible,
    given our data and domain-related challenges. We will also explore, for the first
    time but definitely not the last, the very handy kernel trick, which allows us
    to model nonlinear relationships without transformations or increasing the number
    of features.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第8章*](B17978_08_ePub.xhtml#_idTextAnchor106)，*支持向量回归*，讨论了关键的支持向量机概念以及它们如何应用于回归问题。特别是，我们将考察诸如epsilon敏感管和软边界等概念如何为我们提供灵活性，以在数据和领域相关挑战下获得最佳拟合。我们还将首次但肯定不是最后一次探索非常实用的核技巧，它允许我们在不进行变换或增加特征数量的情况下建模非线性关系。'
- en: '[*Chapter 9*](B17978_09_ePub.xhtml#_idTextAnchor113), *K-Nearest Neighbors,
    Decision Tree, Random Forest, and Gradient Boosted Regression*, explores some
    of the most popular non-parametric regression algorithms. We will discuss the
    advantages of each algorithm, when you might want to choose one over the other,
    and possible modeling challenges. These challenges include how to avoid underfitting
    and overfitting with careful adjusting of hyperparameters.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第9章*](B17978_09_ePub.xhtml#_idTextAnchor113)，*K近邻、决策树、随机森林和梯度提升回归*，探讨了最流行的非参数回归算法中的一些。我们将讨论每个算法的优点，何时可能想要选择一个而不是另一个，以及可能的建模挑战。这些挑战包括如何通过仔细调整超参数来避免欠拟合和过拟合。'
- en: '[*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126), *Logistic Regression*,
    is the first of several chapters on building classification models with logistic
    regression, an efficient algorithm with low bias. We will carefully examine the
    assumptions of logistic regression and discuss the attributes of a dataset and
    a modeling problem that make logistic regression a good choice. We will use regularization
    to address high variance or when we have a number of highly correlated predictors.
    We will extend the algorithm to multiclass problems with multinomial logistic
    regression. We will also discuss how to handle class imbalance for the first,
    but not the last, time.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第10章*](B17978_10_ePub.xhtml#_idTextAnchor126)，*逻辑回归*，是关于使用逻辑回归构建分类模型的几个章节中的第一个，逻辑回归是一种高效且偏差低的算法。我们将仔细检查逻辑回归的假设，并讨论数据集和建模问题中使逻辑回归成为良好选择的属性。我们将使用正则化来解决高方差问题或当我们有许多高度相关的预测因子时。我们将通过多项式逻辑回归将算法扩展到多类问题。我们还将讨论如何首次但肯定不是最后一次处理类别不平衡问题。'
- en: '[*Chapter 11*](B17978_11_ePub.xhtml#_idTextAnchor135), *Decision Trees and
    Random Forest Classification*, returns to the decision tree and random forest
    algorithms that were introduced in [*Chapter 9*](B17978_09_ePub.xhtml#_idTextAnchor113),
    *K-Nearest Neighbors, Decision Tree, Random Forest, and Gradient Boosted Regression*,
    this time dealing with classification problems. This gives us another opportunity
    to learn how to construct and interpret decision trees. We will adjust key hyperparameters,
    including the depth of trees, to avoid overfitting. We will then explore random
    forest and gradient boosted decision trees as good, lower variance alternatives
    to decision trees.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第11章*](B17978_11_ePub.xhtml#_idTextAnchor135), *决策树和随机森林分类*，回到在第9章中介绍的决策树和随机森林算法，这次处理分类问题。这为我们提供了另一个学习如何构建和解释决策树的机会。我们将调整包括树深度在内的关键超参数，以避免过拟合。然后我们将探索随机森林和梯度提升决策树作为决策树的优秀、低方差替代方案。'
- en: '[*Chapter 12*](B17978_12_ePub.xhtml#_idTextAnchor144), *K-Nearest Neighbors
    for Classification*, returns to **k-nearest neighbors** (**KNNs**) to handle both
    binary and multiclass modeling problems. We will discuss and demonstrate the advantages
    of KNN – how easy it is to build a no-frills model and the limited number of hyperparameters
    to adjust. By the end of the chapter, we will know both – how to do KNN and when
    we should consider it for our modeling.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第12章*](B17978_12_ePub.xhtml#_idTextAnchor144), *用于分类的K近邻*，回到**k近邻**（**KNNs**）来处理二元和多类建模问题。我们将讨论并展示KNN的优势——构建无装饰模型的简便性以及需要调整的超参数数量有限。到本章结束时，我们将了解两个问题——如何进行KNN以及何时应该考虑它来进行我们的建模。'
- en: '[*Chapter 13*](B17978_13_ePub.xhtml#_idTextAnchor152), *Support Vector Machine
    Classification*, explores different strategies for implementing **support vector
    classification** (**SVC**). We will use linear SVC, which can perform very well
    when our classes are linearly separable. We will then examine how to use the kernel
    trick to extend SVC to cases where the classes are not linearly separable. Finally,
    we will use one-versus-one and one-versus-rest classification to handle targets
    with more than two values.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第13章*](B17978_13_ePub.xhtml#_idTextAnchor152), *支持向量机分类*，探讨了实现**支持向量分类**（**SVC**）的不同策略。我们将使用线性SVC，当我们的类别是线性可分时，它可以表现得非常好。然后我们将考察如何使用核技巧将SVC扩展到类别不是线性可分的情况。最后，我们将使用一对一和一对多分类来处理具有两个以上值的标签。'
- en: '[*Chapter 14*](B17978_14_ePub.xhtml#_idTextAnchor162), *Naïve Bayes Classification*,
    discusses the fundamental assumptions of naïve Bayes in this chapter and how the
    algorithm is used to tackle some of the modeling challenges we have already explored,
    as well as some new ones, such as text classification. We will consider when naïve
    Bayes is a good option and when it is not. We will also examine the interpretation
    of naïve Bayes models.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第14章*](B17978_14_ePub.xhtml#_idTextAnchor162), *朴素贝叶斯分类*，本章讨论了朴素贝叶斯的基本假设以及该算法如何被用来解决我们已探讨的一些建模挑战，以及一些新的挑战，例如文本分类。我们将考虑何时朴素贝叶斯是一个好的选择，何时则不是。我们还将探讨朴素贝叶斯模型的解释。'
- en: '[*Chapter 15*](B17978_15_ePub.xhtml#_idTextAnchor170), *Principal Component
    Analysis*, examines **principal component analysis** (**PCA**), including how
    it works and when we might want to use it. We will learn how to interpret the
    components created from PCA, including how each feature contributes to each component
    and how much of the variance is explained. We will learn how to visualize components
    and how to use components in subsequent analyses. We will also examine how to
    use kernels for PCA and when that might give us better results.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第15章*](B17978_15_ePub.xhtml#_idTextAnchor170), *主成分分析*，考察**主成分分析**（**PCA**），包括其工作原理以及我们可能想要使用它的时机。我们将学习如何解释PCA创建的成分，包括每个特征如何贡献到每个成分以及解释了多少方差。我们将学习如何可视化成分以及如何在后续分析中使用成分。我们还将考察如何使用核函数进行PCA以及何时这可能给我们带来更好的结果。'
- en: '[*Chapter 16*](B17978_16_ePub.xhtml#_idTextAnchor177), *K-Means and DBSCAN
    Clustering*, explores two popular clustering techniques, k-means and **Density-based
    spatial clustering of applications with noise** (**DBSCAN**). We will discuss
    the strengths of each approach and develop a sense of when to choose one clustering
    algorithm over the other. We will also learn how to evaluate our clusters and
    how to change hyperparameters to improve our model.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第16章*](B17978_16_ePub.xhtml#_idTextAnchor177)，*K-Means和DBSCAN聚类*，探讨了两种流行的聚类技术，k-means和**基于密度的空间聚类应用噪声**（**DBSCAN**）。我们将讨论每种方法的优点，并培养在何时选择一种聚类算法而不是另一种算法的感觉。我们还将学习如何评估我们的聚类以及如何更改超参数以改进我们的模型。'
- en: To get the most out of this book
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要充分利用本书
- en: To run the code in this book, you will need to have installed a scientific distribution
    of Python, such as Anaconda. All code was tested with scikit-learn versions 0.24.2
    and 1.0.2.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本书中的代码，您需要安装一个科学版的Python，例如Anaconda。所有代码都使用scikit-learn版本0.24.2和1.0.2进行了测试。
- en: Download the example code files
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning).
    If there’s an update to the code, it will be updated in the GitHub repository.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从GitHub（[https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning)）下载本书的示例代码文件。如果代码有更新，它将在GitHub仓库中更新。
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有其他来自我们丰富的图书和视频目录的代码包，可在[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)找到。查看它们吧！
- en: Download the color images
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载彩色图像
- en: 'We also provide a PDF file that has color images of the screenshots and diagrams
    used in this book. You can download it here: [https://packt.link/aLE6J](https://packt.link/aLE6J).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一份包含本书中使用的截图和图表的彩色图像的PDF文件。您可以从这里下载：[https://packt.link/aLE6J](https://packt.link/aLE6J)。
- en: Conventions used
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用了多种文本约定。
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: “For learning purposes, we have provided two example
    `mlruns` artifacts and the `huggingface` cache folder in the GitHub repository
    under the `chapter08` folder.”'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`文本中的代码`：表示文本中的代码词汇、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟URL、用户输入和Twitter昵称。以下是一个示例：“为了学习目的，我们在GitHub的`chapter08`文件夹下提供了两个示例`mlruns`工件和`huggingface`缓存文件夹。”'
- en: 'A block of code is set as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块设置如下：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望将您的注意力引到代码块的一个特定部分时，相关的行或项目将以粗体显示：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出都如下所示：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold**. Here is an example:
    “To execute the code in this cell, you can just click on **Run Cell** in the top-right
    drop-down menu.”'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示新术语、重要词汇或屏幕上看到的词汇。例如，菜单或对话框中的单词以**粗体**显示。以下是一个示例：“要执行此单元格中的代码，您只需在右上角的下拉菜单中点击**运行单元格**。”'
- en: Tips or Important Notes
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士或重要提示
- en: Appear like this.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来是这样的。
- en: Get in touch
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎读者反馈。
- en: '`customercare@packtpub.com` and mention the book title in the subject of your
    message.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`customercare@packtpub.com`并在邮件主题中提及书名。'
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    and fill in the form.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误表**：尽管我们已经尽最大努力确保内容的准确性，但错误仍然可能发生。如果您在这本书中发现了错误，如果您能向我们报告，我们将不胜感激。请访问[www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)并填写表格。'
- en: '`copyright@packt.com` with a link to the material.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`copyright@packt.com`并附上材料链接。'
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您有兴趣成为作者**：如果您在某个领域有专业知识，并且有兴趣撰写或为书籍做出贡献，请访问[authors.packtpub.com](http://authors.packtpub.com)。'
- en: Share Your Thoughts
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享您的想法
- en: Once you’ve read *Data Cleaning and Exploration with Machine Learning*, we’d
    love to hear your thoughts! Please click here to go straight to the Amazon review
    page for this book and share your feedback.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您阅读了《使用机器学习进行数据清洗和探索》，我们非常想听听您的想法！请点击此处直接进入此书的亚马逊评论页面并分享您的反馈。
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您的评论对我们和科技社区都非常重要，并将帮助我们确保我们提供高质量的内容。
