- en: 'Chapter 4: Adding Feature Store to ML Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：将特征存储添加到机器学习模型
- en: In the last chapter, we discussed **Feast** installation in your local system,
    common terminology in Feast, what the project structure looks like, API usage
    with an example, and a brief overview of the Feast architecture.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了在本地系统中安装 **Feast**，Feast 中的常见术语，项目结构的样子，以及使用示例的 API 使用方法，并对 Feast
    架构进行了简要概述。
- en: So far in the book, we have been talking about issues with feature management
    and how a feature store can benefit data scientists and data engineers. It is
    time for us to get our hands dirty with an ML model and add Feast to the ML pipeline.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书一直在讨论特征管理的问题以及特征存储如何使数据科学家和数据工程师受益。现在是时候让我们亲自动手，将 Feast 添加到机器学习管道中。
- en: In this chapter, we will revisit the **Customer Lifetime Value** (**LTV/CLTV**)
    ML model built in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview
    of the Machine Learning Life Cycle*. We will use AWS cloud services instead of
    the local system to run the examples in this chapter. As mentioned in [*Chapter
    3*](B18024_03_ePub.xhtml#_idTextAnchor050), *Feature Store Fundamentals, Terminology,
    and Usage*, installation for AWS is different from that of a local system, so
    we will have to create a few resources. I will be using some Free Tier services
    and some that are featured services (free for the first 2 months of use with limits).
    Also, the terms and API usage examples we looked at in [*Chapter 3*](B18024_03_ePub.xhtml#_idTextAnchor050),
    *Feature Store Fundamentals, Terminology, and Usage*, will be very useful as we
    try to include Feast in the ML pipeline.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾在 [*第一章*](B18024_01_ePub.xhtml#_idTextAnchor014)，《机器学习生命周期概述》中构建的
    **客户终身价值**（**LTV/CLTV**）机器学习模型。我们将使用 AWS 云服务而不是本地系统来运行本章的示例。正如在 [*第三章*](B18024_03_ePub.xhtml#_idTextAnchor050)，《特征存储基础、术语和用法》中提到的，AWS
    的安装与本地系统不同，因此我们需要创建一些资源。我将使用一些免费层服务和一些特色服务（前两个月使用免费，但有限制）。此外，我们在 [*第三章*](B18024_03_ePub.xhtml#_idTextAnchor050)，《特征存储基础、术语和用法》中查看的术语和
    API 使用示例，在我们尝试将 Feast 包含到机器学习管道中时将非常有用。
- en: The goal of this chapter is to learn what it takes to include a feature store
    in a project and how it differs from the traditional ML model building that we
    did in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview of the
    Machine Learning Life Cycle*. We will learn about Feast installation, how to build
    a feature engineering pipeline for the LTV model, how to define feature definitions,
    and we will also look at feature ingestion in Feast.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是了解将特征存储添加到项目中的所需条件以及它与我们在 [*第一章*](B18024_01_ePub.xhtml#_idTextAnchor014)，《机器学习生命周期概述》中进行的传统机器学习模型构建有何不同。我们将学习
    Feast 的安装，如何为 LTV 模型构建特征工程管道，如何定义特征定义，我们还将查看 Feast 中的特征摄取。
- en: 'We will discuss the following topics in order:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按以下顺序讨论以下主题：
- en: Creating Feast resources in AWS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS 中创建 Feast 资源
- en: Feast initialization for AWS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feast 初始化针对 AWS
- en: Exploring the ML life cycle with Feast
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Feast 探索机器学习生命周期
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To follow the code examples in the chapter, all you need is familiarity with
    Python and any notebook environment, which could be a local setup such as Jupyter
    or an online notebook environment such as Google Collab, Kaggle, or SageMaker.
    You will also need an AWS account with full access to resources such as Redshift,
    S3, Glue, DynamoDB, the IAM console, and more. You can create a new account and
    use all the services for free during the trial period. You can find the code examples
    for the book at the following GitHub link:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随本章中的代码示例，您只需要熟悉 Python 和任何笔记本环境，这可以是本地设置，如 Jupyter，或者在线笔记本环境，如 Google Collab、Kaggle
    或 SageMaker。您还需要一个 AWS 账户，并有权访问 Redshift、S3、Glue、DynamoDB、IAM 控制台等资源。您可以在试用期间创建新账户并免费使用所有服务。您可以在以下
    GitHub 链接找到本书的代码示例：
- en: https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter04
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter04
- en: 'The following GitHub link points to the feature repository:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 GitHub 链接指向特征存储库：
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation)'
- en: Creating Feast resources in AWS
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 AWS 中创建 Feast 资源
- en: As discussed in the previous chapter, Feast aims to provide a quick setup for
    beginners to try it out; however, for team collaboration and to run a model in
    production, it requires a better setup. In this section, we will set up a Feast
    environment in the AWS cloud and use it in model development. In the previous
    chapter, we also discussed that Feast provides multiple choices when picking an
    online and offline store. For this exercise, Amazon S3 with Redshift will be used
    as an offline/historical store and DynamoDB will be used as an online store. So,
    we need a few resources on AWS before we can start using the feature store in
    our project. Let's create the resources one after another.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Amazon S3 for storing data
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the AWS documentation, **Amazon Simple Storage Service** (**Amazon
    S3**) *is an object storage service offering industry-leading scalability, data
    availability, security, and performance*. Feast provides the capability to use
    S3 to store and retrieve all data and metadata. You could also use version control
    systems such as GitHub or GitLab to collaborate on the metadata and sync to S3
    during deployment. To create an S3 bucket in AWS, log in to your AWS account,
    navigate to the S3 service using the search box, or visit [https://s3.console.aws.amazon.com/s3/home?region=us-east-1](https://s3.console.aws.amazon.com/s3/home?region=us-east-1).
    A web page will be displayed, as shown in *Figure 4.1*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – AWS S3 home page'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – AWS S3 home page
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: If you already have the buckets, you will see them on the page. I am using a
    new account, hence I don't see any buckets yet. To create a new bucket, click
    on `feast-demo-mar-2022`. One thing to keep in mind is that S3 bucket names are
    unique across accounts. If bucket creation fails with an error, **Bucket with
    the same name already exists**, try adding a few random characters to the end.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – S3 Create bucket'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – S3 Create bucket
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: After successful bucket creation, you will see a screen similar to *Figure 4.3*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – After S3 bucket creation'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – After S3 bucket creation
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: AWS Redshift for an offline store
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the AWS documentation, *Amazon Redshift uses SQL to analyze
    structured and semi-structured data across data warehouses, operational databases,
    and data lakes, using AWS-designed hardware and machine learning to deliver the
    best price performance at any scale*. As mentioned earlier, we will use a Redshift
    cluster for querying historical data. We need to create a cluster since we don't
    have one already. Before we create a cluster, let's create an **Identity and Access
    Management** (**IAM**) role. This is a role that Redshift will assume on our behalf
    to query the historical data in S3.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating an IAM role:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: To create an IAM role, navigate to the AWS IAM console using the search or visit
    the URL https://us-east-1.console.aws.amazon.com/iamv2/home?region=us-east-1#/roles.
    A web page similar to the one in *Figure 4.4* will be displayed.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建 IAM 角色，请使用搜索导航到 AWS IAM 控制台或访问 URL https://us-east-1.console.aws.amazon.com/iamv2/home?region=us-east-1#/roles。将显示一个类似于
    *图 4.4* 的网页。
- en: '![Figure 4.4 – IAM home page'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4 – IAM 主页'
- en: '](img/B18024_04_04.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B18024_04_04.jpg]'
- en: Figure 4.4 – IAM home page
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – IAM 主页
- en: To create a new role, click on the **Create role** button in the top-right corner.
    The following page will be displayed.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建新角色，请点击右上角的 **创建角色** 按钮。将显示以下页面。
- en: '![Figure 4.5 – IAM Create role'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5 – IAM 创建角色'
- en: '](img/B18024_04_05.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B18024_04_05.jpg]'
- en: Figure 4.5 – IAM Create role
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – IAM 创建角色
- en: 'From the options available on the page, select **Custom trust policy**, copy
    the following code block, and replace the policy in the JSON in the textbox:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在页面上的可用选项中，选择 **自定义信任策略**，复制以下代码块，并用文本框中的 JSON 中的策略替换它：
- en: '[PRE0]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Scroll all the way to the bottom and click on **Next**. On the next page, you
    will see a list of IAM policies that can be attached to the role, as shown in
    *Figure 4.6*.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将页面滚动到最底部并点击 **下一步**。在下一页，您将看到一个可以附加到角色的 IAM 策略列表，如图 *图 4.6* 所示。
- en: '![Figure 4.6 – IAM permissions for the role'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6 – 角色的 IAM 权限'
- en: '](img/B18024_04_06.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B18024_04_06.jpg]'
- en: Figure 4.6 – IAM permissions for the role
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 角色的 IAM 权限
- en: We need **S3** access, since the data will be stored in S3 as Parquet files,
    and **AWS Glue** access. The data stored in S3 will be loaded as an external schema
    into Redshift using AWS Glue Data Catalog/Lake Formation. Follow along here and
    you will understand what it means to load data as an external schema. For S3 access,
    search for **AmazonS3FullAccess** and select the corresponding checkbox, then
    search for **AWSGlueConsoleFullAccess** and select the corresponding checkbox.
    Scroll all the way down and click on **Next**.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要 **S3** 访问权限，因为数据将以 Parquet 文件的形式存储在 S3 中，以及 **AWS Glue** 访问权限。存储在 S3 中的数据将通过
    AWS Glue 数据目录/Lake Formation 作为外部模式加载到 Redshift 中。请跟随这里，您将了解将数据作为外部模式加载的含义。对于
    S3 访问，搜索 **AmazonS3FullAccess** 并选择相应的复选框，然后搜索 **AWSGlueConsoleFullAccess** 并选择相应的复选框。将页面滚动到最底部并点击
    **下一步**。
- en: Important Note
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: We are providing full access to S3 and Glue on all the resources here, but it
    is recommended to restrict access to specific resources. I will leave that as
    an exercise since it is out of scope for this chapter.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在这里为所有资源提供对 S3 和 Glue 的完全访问权限，但建议限制对特定资源的访问。我将将其留作练习，因为这不属于本章的范围。
- en: The following page will be displayed after you click on **Next**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在点击 **下一步** 后，将显示以下页面。
- en: '![Figure 4.7 – IAM review'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – IAM 审查'
- en: '](img/B18024_04_07.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B18024_04_07.jpg]'
- en: Figure 4.7 – IAM review
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – IAM 审查
- en: On this page, provide a name for the role. I have named the role `feast-demo-mar-2022-spectrum-role`.
    Review the details of the role and click on **Create role**. On successful creation,
    you will find the role on the IAM console page.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此页面上，为角色提供一个名称。我已将角色命名为 `feast-demo-mar-2022-spectrum-role`。审查角色的详细信息并点击 **创建角色**。在成功创建后，您将在
    IAM 控制台页面上找到该角色。
- en: Now that we have the IAM role ready, the next step is to create a **Redshift**
    cluster and assign the created IAM role to it. To create the Redshift cluster,
    navigate to the Redshift home page using the search bar or visit the link https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#clusters.
    The following page will be displayed.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了 IAM 角色，下一步是创建一个 **Redshift** 集群并将创建的 IAM 角色分配给它。要创建 Redshift 集群，请使用搜索栏导航到
    Redshift 主页或访问链接 https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#clusters。将显示以下页面。
- en: '![Figure 4.8 – Redshift home page'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8 – Redshift 主页'
- en: '](img/B18024_04_08.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B18024_04_08.jpg]'
- en: Figure 4.8 – Redshift home page
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – Redshift 主页
- en: On the page in *Figure 4.8*, click on **Create cluster**. The following page
    will be displayed.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *图 4.8* 中的页面上，点击 **创建集群**。将显示以下页面。
- en: '![Figure 4.9 – Create a Redshift cluster'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.9 – 创建 Redshift 集群'
- en: '](img/B18024_04_09.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B18024_04_09.jpg]'
- en: Figure 4.9 – Create a Redshift cluster
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 创建 Redshift 集群
- en: From the web page displayed in *Figure 4.9*, I am picking **Free trial** for
    the demo, but this can be configured based on the dataset size and load. After
    picking **Free trial**, scroll all the way down and pick a password. The following
    figure shows the lower half of the window when you scroll down.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从显示在 *图 4.9* 中的网页，我选择了用于演示的 **免费试用**，但可以根据数据集大小和负载进行配置。选择 **免费试用** 后，滚动到页面底部并选择一个密码。以下图显示了向下滚动后的窗口下半部分。
- en: '![Figure 4.10 – Create cluster lower half'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.10 – 创建集群下半部分'
- en: '](img/B18024_04_10.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_04_10.jpg)'
- en: Figure 4.10 – Create cluster lower half
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 创建集群下半部分
- en: Once you've picked a password, click on **Create cluster** at the bottom. Cluster
    creation takes a few minutes. Once the cluster creation is complete, you should
    see the newly created cluster in the AWS Redshift console. One last thing that
    is pending is associating the IAM role that we created earlier with the Redshift
    cluster. Let's do that now. Navigate to the newly created cluster. You will see
    a web page similar to the one in *Figure 4.11*.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择密码后，点击底部的 **创建集群**。集群创建需要几分钟。一旦集群创建完成，你应该在 AWS Redshift 控制台中看到新创建的集群。最后一件待办事项是将我们之前创建的
    IAM 角色与 Redshift 集群关联起来。现在让我们来做这件事。导航到新创建的集群。你会看到一个类似于 *图 4.11* 中的网页。
- en: '![Figure 4.11 – Redshift cluster page'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.11 – Redshift 集群页面'
- en: '](img/B18024_04_11.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_04_11.jpg)'
- en: Figure 4.11 – Redshift cluster page
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – Redshift 集群页面
- en: On the cluster home page, select the **Properties** tab and scroll down to **Associated
    IAM roles**. You will see the options displayed in *Figure 4.12*.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群主页上，选择 **属性** 选项卡并滚动到 **关联 IAM 角色**。你将看到 *图 4.12* 中显示的选项。
- en: '![Figure 4.12 – Redshift Properties tab'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.12 – Redshift 属性选项卡'
- en: '](img/B18024_04_12.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_04_12.jpg)'
- en: Figure 4.12 – Redshift Properties tab
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – Redshift 属性选项卡
- en: From the web page, click on the `feast-demo-mar-2022-spectrum-role`, hence I
    am associating that role. Once you click on the button, the cluster will be updated
    with the new role. It may take a few minutes again. Once the cluster is ready,
    we are done with the required infrastructure for now. We will add the external
    data catalog when the features are ready to be ingested.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从网页上，点击 `feast-demo-mar-2022-spectrum-role`，因此我将关联该角色。点击按钮后，集群将更新为新角色。这又可能需要几分钟。一旦集群准备就绪，我们现在就完成了所需的必要基础设施。当功能准备就绪以进行摄取时，我们将添加外部数据目录。
- en: '![Figure 4.13 – Redshift Associate IAM roles'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.13 – Redshift 关联 IAM 角色'
- en: '](img/B18024_04_13.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_04_13.jpg)'
- en: Figure 4.13 – Redshift Associate IAM roles
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – Redshift 关联 IAM 角色
- en: We need an IAM user to access these resources and perform operations on them.
    Let's create that next.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个 IAM 用户来访问这些资源并对它们进行操作。让我们接下来创建它。
- en: Creating an IAM user to access the resources
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 IAM 用户以访问资源
- en: 'There are different ways to provide access to the users for the resources.
    If you are part of the organization, then the IAM roles can be integrated with
    Auth0 and active directories. Since that is out of scope here, I will be creating
    an IAM user and will give the required permissions for the user to access the
    resources created earlier:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方式为用户提供对资源的访问权限。如果你是组织的一部分，那么 IAM 角色可以与 Auth0 和活动目录集成。由于这超出了本节范围，我将创建一个
    IAM 用户，并将为用户分配必要的权限以访问之前创建的资源：
- en: Let's create the IAM user from the AWS console. The IAM console can be accessed
    using the search or visiting https://console.aws.amazon.com/iamv2/home#/users.
    The IAM console looks as shown in *Figure 4.14*.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从 AWS 控制台创建 IAM 用户。可以通过搜索或访问 https://console.aws.amazon.com/iamv2/home#/users
    访问 IAM 控制台。IAM 控制台的外观如 *图 4.14* 所示。
- en: '![Figure 4.14 – IAM user page'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.14 – IAM 用户页面'
- en: '](img/B18024_04_14.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_04_14.jpg)'
- en: Figure 4.14 – IAM user page
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – IAM 用户页面
- en: On the IAM user page, click on the **Add users** button in the top right. The
    following web page will be displayed.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 IAM 用户页面上，点击右上角的 **添加用户** 按钮。将显示以下网页。
- en: '![Figure 4.15 – IAM Add user'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.15 – IAM 添加用户'
- en: '](img/B18024_04_15.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_04_15.jpg)'
- en: Figure 4.15 – IAM Add user
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 – IAM 添加用户
- en: 'On the web page, provide a user name and select **Access key - Programmatic
    access**, then click on **Next: Permissions** at the bottom. The following web
    page will be displayed.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网页上，提供一个用户名并选择 **访问密钥 - 程序化访问**，然后点击底部的 **下一步：权限**。将显示以下网页。
- en: '![Figure 4.16 – IAM permissions'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.16 – IAM 权限'
- en: '](img/B18024_04_16.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_04_16.jpg)'
- en: Figure 4.16 – IAM permissions
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16 – IAM 权限
- en: 'On the displayed web page, click on **Attach existing policies directly** and
    from the list of available policies, search for and attach the following policies:
    **AmazonRedshiftFullAccess**, **AmazonS3FullAccess**, and **AmazonDynamoDBFullAccess**.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在显示的网页上，点击**直接附加现有策略**，然后从可用策略列表中搜索并附加以下策略：**AmazonRedshiftFullAccess**、**AmazonS3FullAccess**和**AmazonDynamoDBFullAccess**。
- en: Important Note
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: We are attaching full access here without restricting the user to specific resources.
    It is always a good practice to restrict access based on the resources and only
    provide required permissions.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在这里提供了完整的访问权限，而不限制用户访问特定的资源。根据资源限制访问并仅提供所需的权限总是一个好的做法。
- en: 'Click **Next: Tags** and feel free to add tags and again click on **Next: Review**.
    The review page looks like the following:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步：标签**，您可以自由添加标签，然后再次点击**下一步：审查**。审查页面看起来如下：
- en: '![Figure 4.17 – IAM user review'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.17 – IAM 用户审查'
- en: '](img/B18024_04_17.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_04_17.jpg)'
- en: Figure 4.17 – IAM user review
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.17 – IAM 用户审查
- en: From the review page, click on the **Create user** button. The web page in *Figure
    4.18* will be displayed.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从审查页面，点击**创建用户**按钮。*图 4.18*中的网页将会显示。
- en: '![Figure 4.18 – IAM user credentials'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.18 – IAM 用户凭据'
- en: '](img/B18024_04_18.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_04_18.jpg)'
- en: Figure 4.18 – IAM user credentials
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.18 – IAM 用户凭据
- en: On the web page, click on the **Download.csv** button and save the file in a
    secure location. It contains the **Access key ID** and **Secret access key** for
    the user we just created. The secret will be lost if you don't download and save
    it from this page. However, you can go into the user from the IAM user page and
    manage the secret (delete the existing credentials and create new credentials).
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网页上点击**Download.csv**按钮，并将文件保存在安全的位置。它包含我们刚刚创建的用户**访问密钥 ID**和**秘密访问密钥**。如果您不从这个页面下载并保存它，秘密将会丢失。然而，您可以从
    IAM 用户页面进入用户并管理秘密（删除现有的凭据并创建新的凭据）。
- en: Now that the infrastructure is ready, let's initialize the Feast project.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在基础设施已经就绪，让我们初始化 Feast 项目。
- en: Feast initialization for AWS
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Feast AWS 初始化
- en: 'We have the infrastructure required for running Feast now. However, we need
    to initialize a Feast project before we can start using it. To initialize a Feast
    project, we need to install the Feast library as we did in [*Chapter 3*](B18024_03_ePub.xhtml#_idTextAnchor050),
    *Feature Store Fundamentals, Terminology, and Usage*. However, this time, we also
    need to install the AWS dependencies. Here is the link to the notebook: [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_Feast_aws_initialization.ipynb.](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_Feast_aws_initialization.ipynb
    )'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有运行 Feast 所需的基础设施。然而，在我们开始使用它之前，我们需要初始化一个 Feast 项目。要初始化 Feast 项目，我们需要像在[*第
    3 章*](B18024_03_ePub.xhtml#_idTextAnchor050)中那样安装 Feast 库，即*特征存储基础、术语和用法*。但是，这次，我们还需要安装
    AWS 依赖项。以下是笔记本的链接：[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_Feast_aws_initialization.ipynb.](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_Feast_aws_initialization.ipynb
    )
- en: 'The following command installs Feast with the required AWS dependencies:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令安装 Feast 并带有所需的 AWS 依赖项：
- en: '[PRE1]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once the dependencies are installed, we need to initialize the Feast project.
    Unlike the initialization we did in the last chapter, here, Feast initialization
    needs additional inputs such as Redshift ARN, database name, S3 path, and so on.
    Let''s look at how the initialization differs here. Before we initialize the project,
    we need the following details:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖项安装完成后，我们需要初始化 Feast 项目。与上一章中我们进行的初始化不同，这里的 Feast 初始化需要额外的输入，例如 Redshift ARN、数据库名称、S3
    路径等。让我们看看初始化在这里是如何不同的。在我们初始化项目之前，我们需要以下详细信息：
- en: '**AWS Region**: The Region where your infrastructure is running. I have created
    all the resources in **us-east-1**. If you have created them in a different Region,
    use that.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS 区域**：您的基础设施运行的区域。我已在**us-east-1**创建了所有资源。如果您在另一个区域创建了它们，请使用该区域。'
- en: '**Redshift Cluster ID**: The cluster identifier of the Redshift cluster that
    was created earlier. It can be found on the home page.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Redshift 集群 ID**：之前创建的 Redshift 集群的标识符。它可以在主页上找到。'
- en: '`dev`.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dev`。'
- en: '`awsuser`. If you gave a different user name during cluster creation, use that
    here.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`awsuser`。如果您在集群创建时提供了不同的用户名，请在这里使用。'
- en: '`s3://feast-demo-mar-2022/staging`. Also create the staging folder in the bucket.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s3://feast-demo-mar-2022/staging`。同时也在存储桶中创建 staging 文件夹。'
- en: '`arn:aws:iam::<account_number>:role/feast-demo-mar-2022-spectrum-role`.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`arn:aws:iam::<account_number>:role/feast-demo-mar-2022-spectrum-role`。'
- en: 'Once you have the values for the mentioned parameters, the new project can
    be initialized in two ways. One is using the following command:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了提到的参数值，新的项目可以通过以下两种方式之一初始化。一种是使用以下命令：
- en: '[PRE2]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding command initializes the Feast project. During initialization,
    the command will ask you for the mentioned arguments.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令初始化 Feast 项目。在初始化过程中，命令将要求你提供提到的参数。
- en: 'The second way is to edit the `feature_store.yaml` file:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是编辑 `feature_store.yaml` 文件：
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Whichever method you choose for initializing the project, make sure that you
    provide the appropriate values for the parameters. I have highlighted the parameter
    that may need to be replaced for the Feast functionalities to work without issues.
    If you are using the first method, the `init` command will give the option to
    choose whether to load example data or not. Choose `no` to upload the example
    data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择哪种方法来初始化项目，确保你提供了适当的参数值。我已经突出显示了可能需要替换的参数，以便 Feast 功能能够无问题地工作。如果你使用第一种方法，`init`
    命令将提供选择是否加载示例数据的选项。选择 `no` 以不上传示例数据。
- en: 'Now that we have our feature repository initialized for the project, let''s
    apply our initial feature set, which is basically empty. The following code block
    removes the unwanted files that get created if you use `feast init` for the initialization
    of the project:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为项目初始化了特征存储库，让我们应用我们的初始特征集，它基本上是空的。以下代码块移除了如果你使用 `feast init` 初始化项目时创建的不需要的文件：
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If you don't run the preceding commands, it will create the feature definitions
    for the entity and feature views in the `driver_repo.py` file.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有运行前面的命令，它将在 `driver_repo.py` 文件中创建实体和特征视图的特征定义。
- en: 'The following code block creates feature and entity definitions defined in
    the project. In this project, there are none so far:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块创建了项目中定义的特征和实体定义。在这个项目中，目前还没有：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: When the preceding command is run, it displays the message **No changes to registry**,
    which is correct since we don't have any feature definitions yet.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行前面的命令时，它将显示消息 **No changes to registry**，这是正确的，因为我们还没有任何特征定义。
- en: The folder structure of `customer_segmentation` should look like *Figure 4.19*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`customer_segmentation` 文件夹的结构应该看起来像 *图4.19*。'
- en: '![Figure 4.19 – Project folder structure'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.19 – 项目文件夹结构'
- en: '](img/B18024_04_19.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_04_19.jpg)'
- en: Figure 4.19 – Project folder structure
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19 – 项目文件夹结构
- en: The feature repository is ready for use now. This can be checked into *GitHub*
    or *GitLab* for versioning and collaboration.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储库现在已准备好使用。这可以提交到 *GitHub* 或 *GitLab* 以进行版本控制和协作。
- en: Important Note
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Also note that all the preceding steps can be automated using infrastructure
    as code frameworks such as Terraform, the AWS CDK, Cloud Formation, or others.
    Depending on the team structure followed in the organization, it will be the responsibility
    of the data engineer or platform/infrastructure team to create the required resources
    and share the repository details that can be used by data scientists or engineers.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，所有前面的步骤都可以使用基础设施即代码框架（如 Terraform、AWS CDK、Cloud Formation 等）自动化。根据组织遵循的团队结构，数据工程师或平台/基础设施团队将负责创建所需资源并共享数据科学家或工程师可以使用的存储库详细信息。
- en: In the next section, let's look at how the ML life cycle changes with the feature
    store.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们看看机器学习生命周期如何随着特征存储而变化。
- en: Exploring the ML life cycle with Feast
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Feast 探索机器学习生命周期
- en: In this section, let's discuss what ML model development looks like when you
    are using a feature store. We went through the ML life cycle in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014),
    *An Overview of the Machine Learning Life Cycle*. This makes it easy to understand
    how it changes with a feature store and enables us to skip through a few steps
    that will be redundant.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们讨论一下当你使用特征存储时，机器学习模型开发看起来是什么样子。我们在 [*第1章*](B18024_01_ePub.xhtml#_idTextAnchor014)
    中回顾了机器学习生命周期，*机器学习生命周期概述*。这使得理解它如何随着特征存储而变化变得容易，并使我们能够跳过一些将变得冗余的步骤。
- en: '![Figure 4.20 – ML life cycle'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.20 – 机器学习生命周期'
- en: '](img/B18024_04_20.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_04_20.jpg)'
- en: Figure 4.20 – ML life cycle
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.20 – 机器学习生命周期
- en: Problem statement (plan and create)
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述（计划和创建）
- en: The problem statement remains the same as it was in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014),
    *An Overview of the Machine Learning Life Cycle*. Let's assume that you own a
    retail business and would like to improve the customer experience. First and foremost,
    you want to find your customer segments and customer lifetime value.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 问题陈述与[*第一章*](B18024_01_ePub.xhtml#_idTextAnchor014)，*机器学习生命周期概述*中的一致。假设你拥有一家零售业务，并希望提高客户体验。首先，你想要找到你的客户细分和客户终身价值。
- en: Data (preparation and cleaning)
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据（准备和清理）
- en: 'Unlike in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview
    of the Machine Learning Life Cycle*, before exploring the data and figuring out
    the access and more, here the starting point for model building is the feature
    store. Here is the link to the notebook:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与[*第一章*](B18024_01_ePub.xhtml#_idTextAnchor014)，*机器学习生命周期概述*不同，在探索数据并确定访问权限等之前，这里的模型构建起点是特征存储。以下是笔记本的链接：
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb
    )'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb)'
- en: '[Let''s start with the Feature Store:](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb
    )'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[让我们从特征存储开始：](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb)'
- en: 'So, let''s open up a notebook and install Feast with AWS dependencies:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，让我们打开一个笔记本并使用AWS依赖项安装Feast：
- en: '[PRE19]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If the feature repository created in the last section was pushed into source
    control such as GitHub or GitLab, let''s clone the repository. The following code
    clones the repository:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在上一节中创建的特征仓库已推送到源代码控制，如GitHub或GitLab，请克隆该仓库。以下代码克隆了仓库：
- en: '[PRE20]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now that we have the feature repository, let''s connect to Feast/the feature
    store and check what''s available:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了特征仓库，让我们连接到Feast/特征存储并检查可用性：
- en: '[PRE21]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The preceding code block connects to the Feast feature repository. The `repo_path="."`
    parameter indicates that `feature_store.yaml` is in the current working directory.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块连接到Feast特征仓库。`repo_path="."`参数表示`feature_store.yaml`位于当前工作目录。
- en: 'Let''s check whether the feature store contains any **entities** or **feature
    views** that can be used in the model instead of exploring the data and regenerating
    the features that already exist:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查特征存储是否包含任何可用于模型的**实体**或**特征视图**，而不是探索数据并重新生成已存在的特征：
- en: '[PRE22]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code block lists the **entities** and **feature views** that
    exist in the current feature repository we are connected to. The code block outputs
    two empty lists as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码块列出了我们连接到的当前特征仓库中存在的**实体**和**特征视图**。代码块输出如下两个空列表：
- en: '[PRE23]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Important Note
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You may be wondering *What about the features created by other teams? How can
    I get access to them and check what's available?* There are ways to manage that.
    We will get to that a little later.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，*其他团队创建的特征怎么办？我如何获取访问权限并检查可用性？* 有方法可以管理这些。我们稍后会提到。
- en: Since the entities and feature views are empty, there is nothing that can be
    used. The next step is to perform data exploration and feature engineering.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实体和特征视图为空，没有可用的内容。下一步是进行数据探索和特征工程。
- en: We will be skipping over the data exploration stage as we have already done
    it in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview of the
    Machine Learning Life Cycle*. Also, the steps for generating the features would
    be the same. Hence I will not be expanding on feature engineering. Instead, I
    will use the same code and briefly mention what the code does. Refer to [*Chapter
    1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview of the Machine Learning
    Life Cycle*, for a detailed description of how features are generated.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将跳过数据探索阶段，因为我们已经在[*第一章*](B18024_01_ePub.xhtml#_idTextAnchor014)，*机器学习生命周期概述*中完成了它。因此，生成特征的步骤也将相同。因此，我将不会详细说明特征工程。相反，我将使用相同的代码，并简要说明代码的功能。有关特征生成详细描述，请参阅[*第一章*](B18024_01_ePub.xhtml#_idTextAnchor014)，*机器学习生命周期概述*。
- en: Model (feature engineering)
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型（特征工程）
- en: 'In this section, we will generate the features required for the model. Just
    the way we did in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview
    of the Machine Learning Life Cycle*, we will use 3 months of data to generate
    RFM features and 6 months of data to generate the labels for the dataset. We will
    go through the steps in the same order as we did in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014),
    *An Overview of the Machine Learning Life Cycle*. Here is the link to the feature
    engineering notebook:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将生成模型所需的特征。就像我们在[*第1章*](B18024_01_ePub.xhtml#_idTextAnchor014)，“机器学习生命周期概述”中所做的那样，我们将使用3个月的数据来生成RFM特征，并使用6个月的数据来生成数据集的标签。我们将按照与[*第1章*](B18024_01_ePub.xhtml#_idTextAnchor014)，“机器学习生命周期概述”中相同的顺序进行操作。以下是特征工程笔记本的链接：
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb)。'
- en: 'Let''s start with feature engineering:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从特征工程开始：
- en: 'The following code block reads the dataset and filters out the data that doesn''t
    belong to `United Kingdom`:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块读取数据集并过滤掉不属于`United Kingdom`的数据：
- en: '[PRE24]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Once we have the filtered data, the next step is to create two DataFrames, one
    for 3 months and one for 6 months.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了过滤后的数据，下一步是创建两个DataFrame，一个用于3个月，一个用于6个月。
- en: 'The following code block creates two different DataFrames, one for the data
    between `2011-03-01 00:00:00.054000` and `2011-06-01 00:00:00.054000`, the second
    one for the data between `2011-06-01 00:00:00.054000` and `2011-12-01 00:00:00.054000`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块创建了两个不同的DataFrame，一个用于`2011-03-01 00:00:00.054000`和`2011-06-01 00:00:00.054000`之间的数据，第二个用于`2011-06-01
    00:00:00.054000`和`2011-12-01 00:00:00.054000`之间的数据：
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The next step is to generate RFM features from the 3 months DataFrame. The
    following code block generates RFM values for all customers:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是从3个月DataFrame中生成RFM特征。以下代码块为所有客户生成RFM值：
- en: '[PRE26]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now that we have generated RFM values for all the customers, the next step is
    to generate an R group, an F group, and an M group for each of the customers ranging
    from 0 to 3\. Once we have the RFM groups for the customers, they will be used
    to calculate the RFM score by summing the individual group values for the customer.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为所有客户生成了RFM值，下一步是为每个客户生成一个R组、一个F组和三个M组，范围从0到3。一旦我们有了客户的RFM组，它们将被用来通过累加客户各个组的单个值来计算RFM分数。
- en: 'The following code block generates RFM groups for the customers and calculates
    the RFM score:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块为客户生成RFM组并计算RFM分数：
- en: '[PRE27]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: With the RFM score calculated, it is time to group customers into low-, mid-,
    and high-value customers.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RFM分数计算完成后，是时候将客户分为低价值、中价值和高价值客户了。
- en: 'The following code block groups customers into these groups:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块将这些客户分组到这些组中：
- en: '[PRE28]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now we have the RFM features ready. Let's keep those aside and calculate the
    revenue using the 6-month DataFrame that was created in an earlier step.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了RFM特征，让我们先把这些放一边，并使用之前步骤中创建的6个月DataFrame来计算收入。
- en: 'The following code block calculates the revenue from every customer in the
    6-months dataset:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块计算6个月数据集中每个客户的收入：
- en: '[PRE29]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The next step is to merge the 6-months dataset with revenue into the RFM features
    DataFrame. The following code block merges both the DataFrames in the `CustomerId`
    column:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是将6个月数据集与收入合并到RFM特征DataFrame中。以下代码块在`CustomerId`列中合并了两个DataFrame：
- en: '[PRE30]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Since we are treating the problem as a classification problem, let's generate
    the customer LTV labels to use the **k-means** clustering algorithm. Here, we
    will be using the 6-months revenue to generate the labels. Customers will be grouped
    into three groups, namely **LowLTV**, **MidLTV**, and **HighLTV**.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们将问题视为一个分类问题，让我们生成客户LTV标签以使用**k-means**聚类算法。在这里，我们将使用6个月的收入来生成标签。客户将被分为三个组，即**LowLTV**、**MidLTV**和**HighLTV**。
- en: 'The following code block generates the LTV groups for the customers:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块为客户生成LTV组：
- en: '[PRE31]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now we have the final dataset, let''s look at what the feature set that we
    have generated looks like. The following code block converts categorical values
    into integer values:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了最终的数据库，让我们看看我们生成的特征集是什么样的。以下代码块将分类值转换为整数值：
- en: '[PRE32]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding code block produces the following feature set:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18024_04_21.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – Final feature set for the model
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview of the
    Machine Learning Life Cycle*, the next step that was performed was model training
    and scoring. This is where we'll diverge from that. I am assuming this will be
    our final feature set. However, during the model development, the feature set
    evolves over time. We will discuss how to handle these changes in later chapters.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a feature set, the next thing is to create entities and feature
    views in Feast.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Creating entities and feature views
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous chapter, [*Chapter 3*](B18024_03_ePub.xhtml#_idTextAnchor050),
    *Feature Store Fundamentals, Terminology, and Usage*, we defined **entities**
    and **feature views**. An entity is defined as a collection of semantically related
    features. Entities are domain objects to which features can be mapped. A feature
    view is defined as feature view is like a database table. It represents the structure
    of the feature data at its source. A feature view consists of entities, one or
    more features, and a data source. A feature view is generally modeled around a
    domain object similar to database objects. Since creating and applying a feature
    definition is a one-time activity, it is better to keep it in a separate notebook
    or Python file. Here is a link to the notebook:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb
    )'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s open a notebook, install the libraries, and clone the feature repository
    as mentioned before:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now that we have cloned the feature repository, let''s create the entity and
    feature views. Going by the definition of entity and feature views, the job is
    to identify the entities, features, and feature views in the feature set in *Figure
    4.21*. Let''s start with the entities. The only domain object that can be found
    in *Figure 4.21* is `customerid`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by defining the customer entity. The following code block defines
    the customer entity for Feast:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The preceding entity definition has a few required attributes, such as `name`,
    `value_type`, and `join_key`, and others are optional. There are additional attributes
    that can be added if the users want to provide more information. The most important
    attribute is `join_key`. The value of this attribute should match the column name
    in the feature DataFrame.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: We have figured out the entity in the feature set. The next job is to define
    the feature views. Before we define feature views, a thing to keep in mind is
    to define the feature views as if you are a consumer who didn't generate the feature
    set. What I mean by that is do not name the feature views `customer_segmentation_features`
    or `LTV_features` and push all of them to a single table. Always try to break
    them into logical groups that are meaningful when other data scientists browse
    through them.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, let's look at the feature set and decide how many logical
    groups can be formed here and what features go into what groups. From *Figure
    4.21*, it can be grouped into either one or two groups. The two groups I see are
    RFM features for the customers and revenue features. Since RFM also has revenue
    details, I would rather group them into one group instead of two as there are
    no clear subgroups here. I will call it `customer_rfm_features`.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code block defines the feature view:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The preceding code block has two definitions. The first one is the batch source
    definition. Depending on the offline store that is being used, the definition
    of the batch source differs. In the previous chapter, we used `FileSource` in
    the example. Since we are using Redshift to query the offline store, `RedshiftSource`
    has been defined. The input to the object is query, which is a simple `SELECT`
    statement. The source can be configured to have complex SQL queries with joins,
    aggregation, and more. However, the output should match the column names defined
    in `FeatureView`. The other input to the source is `created_timestamp_column`
    and `event_timestamp_column`. These columns are missing in *Figure 4.21*. The
    columns represent what their headings state, the time when the event occurred,
    and when the event was created. These columns need to be added to the data before
    we ingest it.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '`FeatureView` represents the table structure of the data at the source. As
    we looked at it in the last chapter, it has `entities`, `features`, and the `batch_source`.
    In *Figure 4.21*, the entity is `customer`, that was defined earlier. The rest
    of the columns are the features and the batch source, which is the `RedshiftSource`
    object. The feature name should match the column name and `dtype` should match
    the value type of the columns.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the feature definition for our feature set, we must register
    the new definitions to be able to use them. To register the definitions, let's
    copy the entity and feature definitions into a Python file and add this file to
    our feature repository folder. I will be naming the file `rfm_features.py`. After
    adding the file to the repository, the folder structure looks like the following
    figure.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Project with feature definitions file'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_22.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.22 – Project with feature definitions file
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Before registering the definition using the `apply` command, let's map the external
    schema on Redshift.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Creating an external catalog
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you recall correctly, during the Redshift resource creation, I mentioned
    that the data in Amazon S3 will be added as an external mapping using Glue/Lake
    Formation. What that means is data will not be ingested into Redshift directly;
    instead, the dataset will be in S3\. The structure of the dataset will be defined
    in the Lake Formation catalog, which you will see in a moment. Then, the database
    will be mapped as an external schema on Redshift. Hence, the ingestion will push
    data into S3 directly and the query will be executed using the Redshift cluster.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the workings of ingestion and querying, let''s create
    the database and catalog for our feature set in Lake Formation:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a database, visit the AWS Lake Formation page via a search or using
    this URL: https://console.aws.amazon.com/lakeformation/home?region=us-east-1#databases.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.23 – Databases in AWS Lake Formation'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_23.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.23 – Databases in AWS Lake Formation
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.23* displays the list of databases in AWS Lake Formation.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: On the web page, click on **Create database**. The following web page will appear.
    If you see any popups in the transition, asking you to get started with Lake Formation,
    it can either be canceled or accepted.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Lake formation Create database'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_24.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.24 – Lake formation Create database
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: From the web page displayed above, give the database a name. I am calling it
    `dev`. Leave everything else as the default and click on **Create database**.
    The database will be created, and it will redirect to the database details page.
    As databases are groupings of tables together, you can think of this database
    as a grouping for all the feature views in the project. Once you have the database,
    the next step is to create the table. As you might have already realized, the
    table we create here corresponds to the feature view. In the current exercise,
    there is just one feature view. Hence, a corresponding table needs to be created.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As and when you add a new feature view, a corresponding table needs to be added
    to the database in Lake Formation.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To create a table in the database, click on **Tables** from the page in *Figure
    4.23* or visit this URL: [https://console.aws.amazon.com/lakeformation/home?region=us-east-1#tables.](https://console.aws.amazon.com/lakeformation/home?region=us-east-1#tables
    )'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.25 – Lake Formation tables'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_25.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.25 – Lake Formation tables
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'From the web page in *Figure 4.25*, click on the **Create table** button at
    the top right. The following web page will be displayed:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.26 – Lake Formation Create table 1'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_26.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.26 – Lake Formation Create table 1
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: For the `customer_rfm_features` and I have selected the database that was created
    earlier (`dev`). A description is optional. Once these details are filled in,
    scroll down. The following options will be seen in the next part of the **Create
    table** page.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.27 – Lake Formation Create table 2'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_27.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.27 – Lake Formation Create table 2
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The data store is one of the important properties here. It stands for the location
    of data in S3\. So far, we haven't pushed any data to S3 yet. We will be doing
    that soon. Let's define where data for this table will be pushed to. I am going
    to use the S3 bucket we created earlier, hence the location will be `s3://feast-demo-mar-2022/customer-rfm-features/`.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create the `customer-rfm-features` folder in the S3 path.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting the S3 path, scroll down to the last part of the page – the
    following options will be displayed.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.28 – Lake Formation Create table 3'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_28.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.28 – Lake Formation Create table 3
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.28* shows the last part of the table creation. The **Data format**
    section is asking for the file format of the data. We will be selecting **PARQUET**
    for this exercise. Feel free to experiment with others. Whatever format is selected
    here, all the ingested data files should be of the same format, else it might
    not work as expected.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'The last section is the **Schema** definition of the dataset. You can either
    click on the **Add column** button and add the columns individually or can click
    on the **Upload Schema** button to upload a JSON defining all the columns at once.
    Let''s use the **Add column** button and add all the columns in order. Once all
    the columns are added along with the data types, the columns should look like
    the following:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.29 – Column list in Create table'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_29.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.29 – Column list in Create table
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen from *Figure 4.29*, all the columns have been added, along with
    the entity `customerid` and the two timestamp columns: `event_timestamp` and `created_timestamp`.
    Once the columns are added, click on the **Submit** button at the bottom.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Now, the only thing that is pending is to map this table in the Redshift cluster
    that has been created. Let's do that next. To create the mapping of the external
    schema, visit the Redshift cluster page and select the cluster that was created
    earlier. A web page similar to the one in *Figure 4.30* will be displayed.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.30 – Redshift cluster details page'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_30.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.30 – Redshift cluster details page
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'From the web page displayed in *Figure 4.30*, click on **Query data** in the
    top right of the page. Among the options in the dropdown, pick **Query in query
    editor v2**. It will open up a query editor as shown in the following figure:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.31 – Redshift query editor v2'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_31.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.31 – Redshift query editor v2
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the cluster from the left panel and also the database if not selected
    by default. In the query editor shown in *Figure 4.31*, run the following query
    to map the external database into a schema called `spectrum`:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In the preceding code block, replace `<redshift_role_arn>` with the **ARN**
    of the role that was created and associated with Redshift. The ARN can be found
    in the IAM console on the role details page, similar to the one in *Figure 4.32*.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.32 – IAM role details page'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_32.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.32 – IAM role details page
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: On successful execution of the query, you should be able to see the output `spectrum`
    schema under the database after refreshing the page as shown in *Figure 4.33*.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.33 – Redshift spectrum schema'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_33.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.33 – Redshift spectrum schema
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also verify the mapping by executing the following SQL `SELECT` query:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The preceding SQL query will return an empty table in the result as the data
    is not ingested yet.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: We have completed the mapping of the external table now. All we are left with
    is to apply the feature set and ingest the data. Let's do that next.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: It might seem like a lot of work to add a feature store in an ML pipeline, however,
    that is not true. Since we are doing it for the first time, it just seems like
    that. Also, all the steps from resource creation to mapping the external table
    can be automated using infrastructure as code. Here is a link to an example that
    automates infrastructure creation ([https://github.com/feast-dev/feast-aws-credit-scoring-tutorial](https://github.com/feast-dev/feast-aws-credit-scoring-tutorial)).
    Apart from that, if you use managed feature stores such as Tecton, SageMaker,
    or Databricks, the infrastructure is managed and all you will have to do is to
    create features, ingest them, and use them without worrying about the infrastructure.
    We will do a comparison of Feast with other feature stores in [*Chapter 7*](B18024_07_ePub.xhtml#_idTextAnchor113),
    *Feast Alternatives and ML Best Practices*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Applying definitions and ingesting data
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have performed data cleaning, feature engineering, defined the entities
    and feature definitions, and also created and mapped the external table to Redshift.
    Now, let's apply the feature definitions and ingest the data. Continue in the
    same notebook that we created in the *Creating entities and feature views* section
    ([https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb)).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply a feature set, we need the IAM user credentials that was created earlier.
    Recall that, during the creation of the IAM user, the credential files were available
    for download. The file contains `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
    Once you have it handy, replace `<aws_key_id>` and `<aws_secret>` in the following
    code block:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Important Note
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: It is never a good idea to set the credentials in the notebook as a raw string.
    Depending on the tools that are available to the user, it is a good practice to
    use a secret manager to store secrets.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting the environment variable, all you have to do is to run the following
    code block to apply the defined feature set:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The preceding code block registers the new feature definitions and also creates
    the AWS DynamoDB tables for all the feature views in the definition. The output
    of the preceding code block is displayed in *Figure 4.34*.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.34 – Feast apply output'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_34.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.34 – Feast apply output
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: To verify that DynamoDB tables are created for the feature views, navigate to
    the DynamoDB console, using the search or visit https://console.aws.amazon.com/dynamodbv2/home?region=us-east-1#tables.
    You should see the `customer_rfm_features` table as shown in *Figure 4.35*.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.35 – DynamoDB tables'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_35.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.35 – DynamoDB tables
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Now that feature definitions have been applied, to ingest the feature data,
    let's pick up the feature engineering notebook created in the *Model (feature
    engineering)* section ([https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb))
    and continue in that (the last command of feature engineering produced *Figure
    4.21*). To ingest the data, the only thing we have to do is write the features
    DataFrame to the S3 location that is mapped in *Figure 4.28*. I mapped the data
    store location as `s3://feast-demo-mar-2022/customer-rfm-features/`. Let's write
    the DataFrame to the location as Parquet.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block ingests the data in the S3 location:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The preceding code block sets the AWS credentials of the IAM user, adds the
    missing columns, `event_timestamp` and `created_timestamp`, and finally writes
    the Parquet file to the S3 location. To verify that the file is written successfully,
    navigate to the S3 location and verify that the file exists. To make sure that
    the file is in the correct format, let''s navigate to the Redshift query editor
    in *Figure 4.32* and run the following query:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The preceding command should result in success, with the output as shown in
    *Figure 4.36*.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.36 – Redshift query after ingesting the data'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_36.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.36 – Redshift query after ingesting the data
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to the next stage of ML, let's just run a couple of APIs,
    look at what our feature repository looks like, and verify that the query to the
    historical store works okay. For the following code, let's use the notebook we
    used to create and apply feature definitions ([https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb)).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code connects to the feature store and lists the available entities
    and feature views:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The preceding code block prints the `customer` entity and `customer_rfm_features`
    feature view. Let's query the offline store for a few entities and see if it works
    as expected.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'To query offline data, we need entity IDs and timestamp columns. The entity
    ID column is a list of customer IDs and the timestamp column is used for performing
    point-in-time join queries on the dataset. The following code creates an entity
    DataFrame for the query:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The preceding code block produces an entity DataFrame like the one in *Figure
    4.37*.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.37 – Entity DataFrame'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_37.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.37 – Entity DataFrame
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'With the sample entity DataFrame, let''s query the historical data. The following
    code fetches a subset of features from the historical store:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The following code block may take a couple of minutes to run but finally outputs
    the following results:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.38 – Historical retrieval job output'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_38.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.38 – Historical retrieval job output
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Now we can say that our feature engineering pipeline is ready. The next steps
    that are required are to train the model, perform validation, and, if happy with
    the performance of the model, deploy the pipeline into production. We will look
    at training, validation, deployment, and model scoring in the next chapter. Let's
    briefly summarize what we have learned next.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with the goal of adding the Feast feature store
    to our ML model development. We accomplished that by creating the required resources
    on AWS, adding an IAM user to access those resources. After creating the resources,
    we went through the steps of the ML life cycle again from the problem statement
    to feature engineering and feature ingestion. We also verified that created feature
    definitions and ingested data could be queried through the API.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have set the stage for the next steps of the ML life cycle – model
    training, validation, deployment, and scoring, in the next chapter, we will learn
    how the addition of the feature store right from the beginning makes the model
    production-ready when the development is complete.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Feast documentation: [https://docs.feast.dev/](https://docs.feast.dev/)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Credit scoring with Feast on AWS: [https://github.com/feast-dev/feast-aws-credit-scoring-tutorial](https://github.com/feast-dev/feast-aws-credit-scoring-tutorial)'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
