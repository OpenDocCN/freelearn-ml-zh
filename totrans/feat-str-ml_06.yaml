- en: 'Chapter 4: Adding Feature Store to ML Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we discussed **Feast** installation in your local system,
    common terminology in Feast, what the project structure looks like, API usage
    with an example, and a brief overview of the Feast architecture.
  prefs: []
  type: TYPE_NORMAL
- en: So far in the book, we have been talking about issues with feature management
    and how a feature store can benefit data scientists and data engineers. It is
    time for us to get our hands dirty with an ML model and add Feast to the ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will revisit the **Customer Lifetime Value** (**LTV/CLTV**)
    ML model built in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview
    of the Machine Learning Life Cycle*. We will use AWS cloud services instead of
    the local system to run the examples in this chapter. As mentioned in [*Chapter
    3*](B18024_03_ePub.xhtml#_idTextAnchor050), *Feature Store Fundamentals, Terminology,
    and Usage*, installation for AWS is different from that of a local system, so
    we will have to create a few resources. I will be using some Free Tier services
    and some that are featured services (free for the first 2 months of use with limits).
    Also, the terms and API usage examples we looked at in [*Chapter 3*](B18024_03_ePub.xhtml#_idTextAnchor050),
    *Feature Store Fundamentals, Terminology, and Usage*, will be very useful as we
    try to include Feast in the ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this chapter is to learn what it takes to include a feature store
    in a project and how it differs from the traditional ML model building that we
    did in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview of the
    Machine Learning Life Cycle*. We will learn about Feast installation, how to build
    a feature engineering pipeline for the LTV model, how to define feature definitions,
    and we will also look at feature ingestion in Feast.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will discuss the following topics in order:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating Feast resources in AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feast initialization for AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the ML life cycle with Feast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow the code examples in the chapter, all you need is familiarity with
    Python and any notebook environment, which could be a local setup such as Jupyter
    or an online notebook environment such as Google Collab, Kaggle, or SageMaker.
    You will also need an AWS account with full access to resources such as Redshift,
    S3, Glue, DynamoDB, the IAM console, and more. You can create a new account and
    use all the services for free during the trial period. You can find the code examples
    for the book at the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter04
  prefs: []
  type: TYPE_NORMAL
- en: 'The following GitHub link points to the feature repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating Feast resources in AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, Feast aims to provide a quick setup for
    beginners to try it out; however, for team collaboration and to run a model in
    production, it requires a better setup. In this section, we will set up a Feast
    environment in the AWS cloud and use it in model development. In the previous
    chapter, we also discussed that Feast provides multiple choices when picking an
    online and offline store. For this exercise, Amazon S3 with Redshift will be used
    as an offline/historical store and DynamoDB will be used as an online store. So,
    we need a few resources on AWS before we can start using the feature store in
    our project. Let's create the resources one after another.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon S3 for storing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the AWS documentation, **Amazon Simple Storage Service** (**Amazon
    S3**) *is an object storage service offering industry-leading scalability, data
    availability, security, and performance*. Feast provides the capability to use
    S3 to store and retrieve all data and metadata. You could also use version control
    systems such as GitHub or GitLab to collaborate on the metadata and sync to S3
    during deployment. To create an S3 bucket in AWS, log in to your AWS account,
    navigate to the S3 service using the search box, or visit [https://s3.console.aws.amazon.com/s3/home?region=us-east-1](https://s3.console.aws.amazon.com/s3/home?region=us-east-1).
    A web page will be displayed, as shown in *Figure 4.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – AWS S3 home page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – AWS S3 home page
  prefs: []
  type: TYPE_NORMAL
- en: If you already have the buckets, you will see them on the page. I am using a
    new account, hence I don't see any buckets yet. To create a new bucket, click
    on `feast-demo-mar-2022`. One thing to keep in mind is that S3 bucket names are
    unique across accounts. If bucket creation fails with an error, **Bucket with
    the same name already exists**, try adding a few random characters to the end.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – S3 Create bucket'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – S3 Create bucket
  prefs: []
  type: TYPE_NORMAL
- en: After successful bucket creation, you will see a screen similar to *Figure 4.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – After S3 bucket creation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – After S3 bucket creation
  prefs: []
  type: TYPE_NORMAL
- en: AWS Redshift for an offline store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the AWS documentation, *Amazon Redshift uses SQL to analyze
    structured and semi-structured data across data warehouses, operational databases,
    and data lakes, using AWS-designed hardware and machine learning to deliver the
    best price performance at any scale*. As mentioned earlier, we will use a Redshift
    cluster for querying historical data. We need to create a cluster since we don't
    have one already. Before we create a cluster, let's create an **Identity and Access
    Management** (**IAM**) role. This is a role that Redshift will assume on our behalf
    to query the historical data in S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating an IAM role:'
  prefs: []
  type: TYPE_NORMAL
- en: To create an IAM role, navigate to the AWS IAM console using the search or visit
    the URL https://us-east-1.console.aws.amazon.com/iamv2/home?region=us-east-1#/roles.
    A web page similar to the one in *Figure 4.4* will be displayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.4 – IAM home page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – IAM home page
  prefs: []
  type: TYPE_NORMAL
- en: To create a new role, click on the **Create role** button in the top-right corner.
    The following page will be displayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.5 – IAM Create role'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – IAM Create role
  prefs: []
  type: TYPE_NORMAL
- en: 'From the options available on the page, select **Custom trust policy**, copy
    the following code block, and replace the policy in the JSON in the textbox:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Scroll all the way to the bottom and click on **Next**. On the next page, you
    will see a list of IAM policies that can be attached to the role, as shown in
    *Figure 4.6*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.6 – IAM permissions for the role'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – IAM permissions for the role
  prefs: []
  type: TYPE_NORMAL
- en: We need **S3** access, since the data will be stored in S3 as Parquet files,
    and **AWS Glue** access. The data stored in S3 will be loaded as an external schema
    into Redshift using AWS Glue Data Catalog/Lake Formation. Follow along here and
    you will understand what it means to load data as an external schema. For S3 access,
    search for **AmazonS3FullAccess** and select the corresponding checkbox, then
    search for **AWSGlueConsoleFullAccess** and select the corresponding checkbox.
    Scroll all the way down and click on **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are providing full access to S3 and Glue on all the resources here, but it
    is recommended to restrict access to specific resources. I will leave that as
    an exercise since it is out of scope for this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The following page will be displayed after you click on **Next**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – IAM review'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – IAM review
  prefs: []
  type: TYPE_NORMAL
- en: On this page, provide a name for the role. I have named the role `feast-demo-mar-2022-spectrum-role`.
    Review the details of the role and click on **Create role**. On successful creation,
    you will find the role on the IAM console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have the IAM role ready, the next step is to create a **Redshift**
    cluster and assign the created IAM role to it. To create the Redshift cluster,
    navigate to the Redshift home page using the search bar or visit the link https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#clusters.
    The following page will be displayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Redshift home page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – Redshift home page
  prefs: []
  type: TYPE_NORMAL
- en: On the page in *Figure 4.8*, click on **Create cluster**. The following page
    will be displayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Create a Redshift cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – Create a Redshift cluster
  prefs: []
  type: TYPE_NORMAL
- en: From the web page displayed in *Figure 4.9*, I am picking **Free trial** for
    the demo, but this can be configured based on the dataset size and load. After
    picking **Free trial**, scroll all the way down and pick a password. The following
    figure shows the lower half of the window when you scroll down.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Create cluster lower half'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10 – Create cluster lower half
  prefs: []
  type: TYPE_NORMAL
- en: Once you've picked a password, click on **Create cluster** at the bottom. Cluster
    creation takes a few minutes. Once the cluster creation is complete, you should
    see the newly created cluster in the AWS Redshift console. One last thing that
    is pending is associating the IAM role that we created earlier with the Redshift
    cluster. Let's do that now. Navigate to the newly created cluster. You will see
    a web page similar to the one in *Figure 4.11*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Redshift cluster page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Redshift cluster page
  prefs: []
  type: TYPE_NORMAL
- en: On the cluster home page, select the **Properties** tab and scroll down to **Associated
    IAM roles**. You will see the options displayed in *Figure 4.12*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Redshift Properties tab'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.12 – Redshift Properties tab
  prefs: []
  type: TYPE_NORMAL
- en: From the web page, click on the `feast-demo-mar-2022-spectrum-role`, hence I
    am associating that role. Once you click on the button, the cluster will be updated
    with the new role. It may take a few minutes again. Once the cluster is ready,
    we are done with the required infrastructure for now. We will add the external
    data catalog when the features are ready to be ingested.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Redshift Associate IAM roles'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.13 – Redshift Associate IAM roles
  prefs: []
  type: TYPE_NORMAL
- en: We need an IAM user to access these resources and perform operations on them.
    Let's create that next.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an IAM user to access the resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are different ways to provide access to the users for the resources.
    If you are part of the organization, then the IAM roles can be integrated with
    Auth0 and active directories. Since that is out of scope here, I will be creating
    an IAM user and will give the required permissions for the user to access the
    resources created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's create the IAM user from the AWS console. The IAM console can be accessed
    using the search or visiting https://console.aws.amazon.com/iamv2/home#/users.
    The IAM console looks as shown in *Figure 4.14*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.14 – IAM user page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.14 – IAM user page
  prefs: []
  type: TYPE_NORMAL
- en: On the IAM user page, click on the **Add users** button in the top right. The
    following web page will be displayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.15 – IAM Add user'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.15 – IAM Add user
  prefs: []
  type: TYPE_NORMAL
- en: 'On the web page, provide a user name and select **Access key - Programmatic
    access**, then click on **Next: Permissions** at the bottom. The following web
    page will be displayed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.16 – IAM permissions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.16 – IAM permissions
  prefs: []
  type: TYPE_NORMAL
- en: 'On the displayed web page, click on **Attach existing policies directly** and
    from the list of available policies, search for and attach the following policies:
    **AmazonRedshiftFullAccess**, **AmazonS3FullAccess**, and **AmazonDynamoDBFullAccess**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are attaching full access here without restricting the user to specific resources.
    It is always a good practice to restrict access based on the resources and only
    provide required permissions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click **Next: Tags** and feel free to add tags and again click on **Next: Review**.
    The review page looks like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.17 – IAM user review'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.17 – IAM user review
  prefs: []
  type: TYPE_NORMAL
- en: From the review page, click on the **Create user** button. The web page in *Figure
    4.18* will be displayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.18 – IAM user credentials'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.18 – IAM user credentials
  prefs: []
  type: TYPE_NORMAL
- en: On the web page, click on the **Download.csv** button and save the file in a
    secure location. It contains the **Access key ID** and **Secret access key** for
    the user we just created. The secret will be lost if you don't download and save
    it from this page. However, you can go into the user from the IAM user page and
    manage the secret (delete the existing credentials and create new credentials).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the infrastructure is ready, let's initialize the Feast project.
  prefs: []
  type: TYPE_NORMAL
- en: Feast initialization for AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have the infrastructure required for running Feast now. However, we need
    to initialize a Feast project before we can start using it. To initialize a Feast
    project, we need to install the Feast library as we did in [*Chapter 3*](B18024_03_ePub.xhtml#_idTextAnchor050),
    *Feature Store Fundamentals, Terminology, and Usage*. However, this time, we also
    need to install the AWS dependencies. Here is the link to the notebook: [https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_Feast_aws_initialization.ipynb.](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_Feast_aws_initialization.ipynb
    )'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command installs Feast with the required AWS dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the dependencies are installed, we need to initialize the Feast project.
    Unlike the initialization we did in the last chapter, here, Feast initialization
    needs additional inputs such as Redshift ARN, database name, S3 path, and so on.
    Let''s look at how the initialization differs here. Before we initialize the project,
    we need the following details:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Region**: The Region where your infrastructure is running. I have created
    all the resources in **us-east-1**. If you have created them in a different Region,
    use that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Redshift Cluster ID**: The cluster identifier of the Redshift cluster that
    was created earlier. It can be found on the home page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dev`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`awsuser`. If you gave a different user name during cluster creation, use that
    here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3://feast-demo-mar-2022/staging`. Also create the staging folder in the bucket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`arn:aws:iam::<account_number>:role/feast-demo-mar-2022-spectrum-role`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you have the values for the mentioned parameters, the new project can
    be initialized in two ways. One is using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command initializes the Feast project. During initialization,
    the command will ask you for the mentioned arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second way is to edit the `feature_store.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Whichever method you choose for initializing the project, make sure that you
    provide the appropriate values for the parameters. I have highlighted the parameter
    that may need to be replaced for the Feast functionalities to work without issues.
    If you are using the first method, the `init` command will give the option to
    choose whether to load example data or not. Choose `no` to upload the example
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our feature repository initialized for the project, let''s
    apply our initial feature set, which is basically empty. The following code block
    removes the unwanted files that get created if you use `feast init` for the initialization
    of the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If you don't run the preceding commands, it will create the feature definitions
    for the entity and feature views in the `driver_repo.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block creates feature and entity definitions defined in
    the project. In this project, there are none so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: When the preceding command is run, it displays the message **No changes to registry**,
    which is correct since we don't have any feature definitions yet.
  prefs: []
  type: TYPE_NORMAL
- en: The folder structure of `customer_segmentation` should look like *Figure 4.19*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – Project folder structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.19 – Project folder structure
  prefs: []
  type: TYPE_NORMAL
- en: The feature repository is ready for use now. This can be checked into *GitHub*
    or *GitLab* for versioning and collaboration.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Also note that all the preceding steps can be automated using infrastructure
    as code frameworks such as Terraform, the AWS CDK, Cloud Formation, or others.
    Depending on the team structure followed in the organization, it will be the responsibility
    of the data engineer or platform/infrastructure team to create the required resources
    and share the repository details that can be used by data scientists or engineers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let's look at how the ML life cycle changes with the feature
    store.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the ML life cycle with Feast
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let's discuss what ML model development looks like when you
    are using a feature store. We went through the ML life cycle in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014),
    *An Overview of the Machine Learning Life Cycle*. This makes it easy to understand
    how it changes with a feature store and enables us to skip through a few steps
    that will be redundant.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – ML life cycle'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.20 – ML life cycle
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement (plan and create)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem statement remains the same as it was in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014),
    *An Overview of the Machine Learning Life Cycle*. Let's assume that you own a
    retail business and would like to improve the customer experience. First and foremost,
    you want to find your customer segments and customer lifetime value.
  prefs: []
  type: TYPE_NORMAL
- en: Data (preparation and cleaning)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview
    of the Machine Learning Life Cycle*, before exploring the data and figuring out
    the access and more, here the starting point for model building is the feature
    store. Here is the link to the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb
    )'
  prefs: []
  type: TYPE_NORMAL
- en: '[Let''s start with the Feature Store:](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb
    )'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s open up a notebook and install Feast with AWS dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the feature repository created in the last section was pushed into source
    control such as GitHub or GitLab, let''s clone the repository. The following code
    clones the repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have the feature repository, let''s connect to Feast/the feature
    store and check what''s available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code block connects to the Feast feature repository. The `repo_path="."`
    parameter indicates that `feature_store.yaml` is in the current working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check whether the feature store contains any **entities** or **feature
    views** that can be used in the model instead of exploring the data and regenerating
    the features that already exist:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code block lists the **entities** and **feature views** that
    exist in the current feature repository we are connected to. The code block outputs
    two empty lists as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering *What about the features created by other teams? How can
    I get access to them and check what's available?* There are ways to manage that.
    We will get to that a little later.
  prefs: []
  type: TYPE_NORMAL
- en: Since the entities and feature views are empty, there is nothing that can be
    used. The next step is to perform data exploration and feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: We will be skipping over the data exploration stage as we have already done
    it in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview of the
    Machine Learning Life Cycle*. Also, the steps for generating the features would
    be the same. Hence I will not be expanding on feature engineering. Instead, I
    will use the same code and briefly mention what the code does. Refer to [*Chapter
    1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview of the Machine Learning
    Life Cycle*, for a detailed description of how features are generated.
  prefs: []
  type: TYPE_NORMAL
- en: Model (feature engineering)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will generate the features required for the model. Just
    the way we did in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview
    of the Machine Learning Life Cycle*, we will use 3 months of data to generate
    RFM features and 6 months of data to generate the labels for the dataset. We will
    go through the steps in the same order as we did in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014),
    *An Overview of the Machine Learning Life Cycle*. Here is the link to the feature
    engineering notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with feature engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block reads the dataset and filters out the data that doesn''t
    belong to `United Kingdom`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once we have the filtered data, the next step is to create two DataFrames, one
    for 3 months and one for 6 months.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code block creates two different DataFrames, one for the data
    between `2011-03-01 00:00:00.054000` and `2011-06-01 00:00:00.054000`, the second
    one for the data between `2011-06-01 00:00:00.054000` and `2011-12-01 00:00:00.054000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to generate RFM features from the 3 months DataFrame. The
    following code block generates RFM values for all customers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have generated RFM values for all the customers, the next step is
    to generate an R group, an F group, and an M group for each of the customers ranging
    from 0 to 3\. Once we have the RFM groups for the customers, they will be used
    to calculate the RFM score by summing the individual group values for the customer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block generates RFM groups for the customers and calculates
    the RFM score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the RFM score calculated, it is time to group customers into low-, mid-,
    and high-value customers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code block groups customers into these groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now we have the RFM features ready. Let's keep those aside and calculate the
    revenue using the 6-month DataFrame that was created in an earlier step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code block calculates the revenue from every customer in the
    6-months dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to merge the 6-months dataset with revenue into the RFM features
    DataFrame. The following code block merges both the DataFrames in the `CustomerId`
    column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since we are treating the problem as a classification problem, let's generate
    the customer LTV labels to use the **k-means** clustering algorithm. Here, we
    will be using the 6-months revenue to generate the labels. Customers will be grouped
    into three groups, namely **LowLTV**, **MidLTV**, and **HighLTV**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code block generates the LTV groups for the customers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have the final dataset, let''s look at what the feature set that we
    have generated looks like. The following code block converts categorical values
    into integer values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code block produces the following feature set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18024_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – Final feature set for the model
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014), *An Overview of the
    Machine Learning Life Cycle*, the next step that was performed was model training
    and scoring. This is where we'll diverge from that. I am assuming this will be
    our final feature set. However, during the model development, the feature set
    evolves over time. We will discuss how to handle these changes in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a feature set, the next thing is to create entities and feature
    views in Feast.
  prefs: []
  type: TYPE_NORMAL
- en: Creating entities and feature views
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous chapter, [*Chapter 3*](B18024_03_ePub.xhtml#_idTextAnchor050),
    *Feature Store Fundamentals, Terminology, and Usage*, we defined **entities**
    and **feature views**. An entity is defined as a collection of semantically related
    features. Entities are domain objects to which features can be mapped. A feature
    view is defined as feature view is like a database table. It represents the structure
    of the feature data at its source. A feature view consists of entities, one or
    more features, and a data source. A feature view is generally modeled around a
    domain object similar to database objects. Since creating and applying a feature
    definition is a one-time activity, it is better to keep it in a separate notebook
    or Python file. Here is a link to the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb
    )'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s open a notebook, install the libraries, and clone the feature repository
    as mentioned before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have cloned the feature repository, let''s create the entity and
    feature views. Going by the definition of entity and feature views, the job is
    to identify the entities, features, and feature views in the feature set in *Figure
    4.21*. Let''s start with the entities. The only domain object that can be found
    in *Figure 4.21* is `customerid`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by defining the customer entity. The following code block defines
    the customer entity for Feast:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding entity definition has a few required attributes, such as `name`,
    `value_type`, and `join_key`, and others are optional. There are additional attributes
    that can be added if the users want to provide more information. The most important
    attribute is `join_key`. The value of this attribute should match the column name
    in the feature DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: We have figured out the entity in the feature set. The next job is to define
    the feature views. Before we define feature views, a thing to keep in mind is
    to define the feature views as if you are a consumer who didn't generate the feature
    set. What I mean by that is do not name the feature views `customer_segmentation_features`
    or `LTV_features` and push all of them to a single table. Always try to break
    them into logical groups that are meaningful when other data scientists browse
    through them.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, let's look at the feature set and decide how many logical
    groups can be formed here and what features go into what groups. From *Figure
    4.21*, it can be grouped into either one or two groups. The two groups I see are
    RFM features for the customers and revenue features. Since RFM also has revenue
    details, I would rather group them into one group instead of two as there are
    no clear subgroups here. I will call it `customer_rfm_features`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code block defines the feature view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block has two definitions. The first one is the batch source
    definition. Depending on the offline store that is being used, the definition
    of the batch source differs. In the previous chapter, we used `FileSource` in
    the example. Since we are using Redshift to query the offline store, `RedshiftSource`
    has been defined. The input to the object is query, which is a simple `SELECT`
    statement. The source can be configured to have complex SQL queries with joins,
    aggregation, and more. However, the output should match the column names defined
    in `FeatureView`. The other input to the source is `created_timestamp_column`
    and `event_timestamp_column`. These columns are missing in *Figure 4.21*. The
    columns represent what their headings state, the time when the event occurred,
    and when the event was created. These columns need to be added to the data before
    we ingest it.
  prefs: []
  type: TYPE_NORMAL
- en: '`FeatureView` represents the table structure of the data at the source. As
    we looked at it in the last chapter, it has `entities`, `features`, and the `batch_source`.
    In *Figure 4.21*, the entity is `customer`, that was defined earlier. The rest
    of the columns are the features and the batch source, which is the `RedshiftSource`
    object. The feature name should match the column name and `dtype` should match
    the value type of the columns.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the feature definition for our feature set, we must register
    the new definitions to be able to use them. To register the definitions, let's
    copy the entity and feature definitions into a Python file and add this file to
    our feature repository folder. I will be naming the file `rfm_features.py`. After
    adding the file to the repository, the folder structure looks like the following
    figure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Project with feature definitions file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.22 – Project with feature definitions file
  prefs: []
  type: TYPE_NORMAL
- en: Before registering the definition using the `apply` command, let's map the external
    schema on Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an external catalog
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you recall correctly, during the Redshift resource creation, I mentioned
    that the data in Amazon S3 will be added as an external mapping using Glue/Lake
    Formation. What that means is data will not be ingested into Redshift directly;
    instead, the dataset will be in S3\. The structure of the dataset will be defined
    in the Lake Formation catalog, which you will see in a moment. Then, the database
    will be mapped as an external schema on Redshift. Hence, the ingestion will push
    data into S3 directly and the query will be executed using the Redshift cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the workings of ingestion and querying, let''s create
    the database and catalog for our feature set in Lake Formation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a database, visit the AWS Lake Formation page via a search or using
    this URL: https://console.aws.amazon.com/lakeformation/home?region=us-east-1#databases.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.23 – Databases in AWS Lake Formation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.23 – Databases in AWS Lake Formation
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.23* displays the list of databases in AWS Lake Formation.'
  prefs: []
  type: TYPE_NORMAL
- en: On the web page, click on **Create database**. The following web page will appear.
    If you see any popups in the transition, asking you to get started with Lake Formation,
    it can either be canceled or accepted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Lake formation Create database'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.24 – Lake formation Create database
  prefs: []
  type: TYPE_NORMAL
- en: From the web page displayed above, give the database a name. I am calling it
    `dev`. Leave everything else as the default and click on **Create database**.
    The database will be created, and it will redirect to the database details page.
    As databases are groupings of tables together, you can think of this database
    as a grouping for all the feature views in the project. Once you have the database,
    the next step is to create the table. As you might have already realized, the
    table we create here corresponds to the feature view. In the current exercise,
    there is just one feature view. Hence, a corresponding table needs to be created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As and when you add a new feature view, a corresponding table needs to be added
    to the database in Lake Formation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To create a table in the database, click on **Tables** from the page in *Figure
    4.23* or visit this URL: [https://console.aws.amazon.com/lakeformation/home?region=us-east-1#tables.](https://console.aws.amazon.com/lakeformation/home?region=us-east-1#tables
    )'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.25 – Lake Formation tables'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.25 – Lake Formation tables
  prefs: []
  type: TYPE_NORMAL
- en: 'From the web page in *Figure 4.25*, click on the **Create table** button at
    the top right. The following web page will be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.26 – Lake Formation Create table 1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.26 – Lake Formation Create table 1
  prefs: []
  type: TYPE_NORMAL
- en: For the `customer_rfm_features` and I have selected the database that was created
    earlier (`dev`). A description is optional. Once these details are filled in,
    scroll down. The following options will be seen in the next part of the **Create
    table** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.27 – Lake Formation Create table 2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.27 – Lake Formation Create table 2
  prefs: []
  type: TYPE_NORMAL
- en: The data store is one of the important properties here. It stands for the location
    of data in S3\. So far, we haven't pushed any data to S3 yet. We will be doing
    that soon. Let's define where data for this table will be pushed to. I am going
    to use the S3 bucket we created earlier, hence the location will be `s3://feast-demo-mar-2022/customer-rfm-features/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create the `customer-rfm-features` folder in the S3 path.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting the S3 path, scroll down to the last part of the page – the
    following options will be displayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.28 – Lake Formation Create table 3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.28 – Lake Formation Create table 3
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.28* shows the last part of the table creation. The **Data format**
    section is asking for the file format of the data. We will be selecting **PARQUET**
    for this exercise. Feel free to experiment with others. Whatever format is selected
    here, all the ingested data files should be of the same format, else it might
    not work as expected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last section is the **Schema** definition of the dataset. You can either
    click on the **Add column** button and add the columns individually or can click
    on the **Upload Schema** button to upload a JSON defining all the columns at once.
    Let''s use the **Add column** button and add all the columns in order. Once all
    the columns are added along with the data types, the columns should look like
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.29 – Column list in Create table'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.29 – Column list in Create table
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen from *Figure 4.29*, all the columns have been added, along with
    the entity `customerid` and the two timestamp columns: `event_timestamp` and `created_timestamp`.
    Once the columns are added, click on the **Submit** button at the bottom.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, the only thing that is pending is to map this table in the Redshift cluster
    that has been created. Let's do that next. To create the mapping of the external
    schema, visit the Redshift cluster page and select the cluster that was created
    earlier. A web page similar to the one in *Figure 4.30* will be displayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.30 – Redshift cluster details page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.30 – Redshift cluster details page
  prefs: []
  type: TYPE_NORMAL
- en: 'From the web page displayed in *Figure 4.30*, click on **Query data** in the
    top right of the page. Among the options in the dropdown, pick **Query in query
    editor v2**. It will open up a query editor as shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.31 – Redshift query editor v2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_31.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.31 – Redshift query editor v2
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the cluster from the left panel and also the database if not selected
    by default. In the query editor shown in *Figure 4.31*, run the following query
    to map the external database into a schema called `spectrum`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code block, replace `<redshift_role_arn>` with the **ARN**
    of the role that was created and associated with Redshift. The ARN can be found
    in the IAM console on the role details page, similar to the one in *Figure 4.32*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.32 – IAM role details page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_32.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.32 – IAM role details page
  prefs: []
  type: TYPE_NORMAL
- en: On successful execution of the query, you should be able to see the output `spectrum`
    schema under the database after refreshing the page as shown in *Figure 4.33*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.33 – Redshift spectrum schema'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.33 – Redshift spectrum schema
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also verify the mapping by executing the following SQL `SELECT` query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding SQL query will return an empty table in the result as the data
    is not ingested yet.
  prefs: []
  type: TYPE_NORMAL
- en: We have completed the mapping of the external table now. All we are left with
    is to apply the feature set and ingest the data. Let's do that next.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: It might seem like a lot of work to add a feature store in an ML pipeline, however,
    that is not true. Since we are doing it for the first time, it just seems like
    that. Also, all the steps from resource creation to mapping the external table
    can be automated using infrastructure as code. Here is a link to an example that
    automates infrastructure creation ([https://github.com/feast-dev/feast-aws-credit-scoring-tutorial](https://github.com/feast-dev/feast-aws-credit-scoring-tutorial)).
    Apart from that, if you use managed feature stores such as Tecton, SageMaker,
    or Databricks, the infrastructure is managed and all you will have to do is to
    create features, ingest them, and use them without worrying about the infrastructure.
    We will do a comparison of Feast with other feature stores in [*Chapter 7*](B18024_07_ePub.xhtml#_idTextAnchor113),
    *Feast Alternatives and ML Best Practices*.
  prefs: []
  type: TYPE_NORMAL
- en: Applying definitions and ingesting data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have performed data cleaning, feature engineering, defined the entities
    and feature definitions, and also created and mapped the external table to Redshift.
    Now, let's apply the feature definitions and ingest the data. Continue in the
    same notebook that we created in the *Creating entities and feature views* section
    ([https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply a feature set, we need the IAM user credentials that was created earlier.
    Recall that, during the creation of the IAM user, the credential files were available
    for download. The file contains `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
    Once you have it handy, replace `<aws_key_id>` and `<aws_secret>` in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: It is never a good idea to set the credentials in the notebook as a raw string.
    Depending on the tools that are available to the user, it is a good practice to
    use a secret manager to store secrets.
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting the environment variable, all you have to do is to run the following
    code block to apply the defined feature set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block registers the new feature definitions and also creates
    the AWS DynamoDB tables for all the feature views in the definition. The output
    of the preceding code block is displayed in *Figure 4.34*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.34 – Feast apply output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_34.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.34 – Feast apply output
  prefs: []
  type: TYPE_NORMAL
- en: To verify that DynamoDB tables are created for the feature views, navigate to
    the DynamoDB console, using the search or visit https://console.aws.amazon.com/dynamodbv2/home?region=us-east-1#tables.
    You should see the `customer_rfm_features` table as shown in *Figure 4.35*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.35 – DynamoDB tables'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_35.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.35 – DynamoDB tables
  prefs: []
  type: TYPE_NORMAL
- en: Now that feature definitions have been applied, to ingest the feature data,
    let's pick up the feature engineering notebook created in the *Model (feature
    engineering)* section ([https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb))
    and continue in that (the last command of feature engineering produced *Figure
    4.21*). To ingest the data, the only thing we have to do is write the features
    DataFrame to the S3 location that is mapped in *Figure 4.28*. I mapped the data
    store location as `s3://feast-demo-mar-2022/customer-rfm-features/`. Let's write
    the DataFrame to the location as Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block ingests the data in the S3 location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block sets the AWS credentials of the IAM user, adds the
    missing columns, `event_timestamp` and `created_timestamp`, and finally writes
    the Parquet file to the S3 location. To verify that the file is written successfully,
    navigate to the S3 location and verify that the file exists. To make sure that
    the file is in the correct format, let''s navigate to the Redshift query editor
    in *Figure 4.32* and run the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command should result in success, with the output as shown in
    *Figure 4.36*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.36 – Redshift query after ingesting the data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_36.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.36 – Redshift query after ingesting the data
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to the next stage of ML, let's just run a couple of APIs,
    look at what our feature repository looks like, and verify that the query to the
    historical store works okay. For the following code, let's use the notebook we
    used to create and apply feature definitions ([https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb](https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code connects to the feature store and lists the available entities
    and feature views:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block prints the `customer` entity and `customer_rfm_features`
    feature view. Let's query the offline store for a few entities and see if it works
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'To query offline data, we need entity IDs and timestamp columns. The entity
    ID column is a list of customer IDs and the timestamp column is used for performing
    point-in-time join queries on the dataset. The following code creates an entity
    DataFrame for the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block produces an entity DataFrame like the one in *Figure
    4.37*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.37 – Entity DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_37.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.37 – Entity DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'With the sample entity DataFrame, let''s query the historical data. The following
    code fetches a subset of features from the historical store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code block may take a couple of minutes to run but finally outputs
    the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.38 – Historical retrieval job output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18024_04_38.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.38 – Historical retrieval job output
  prefs: []
  type: TYPE_NORMAL
- en: Now we can say that our feature engineering pipeline is ready. The next steps
    that are required are to train the model, perform validation, and, if happy with
    the performance of the model, deploy the pipeline into production. We will look
    at training, validation, deployment, and model scoring in the next chapter. Let's
    briefly summarize what we have learned next.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with the goal of adding the Feast feature store
    to our ML model development. We accomplished that by creating the required resources
    on AWS, adding an IAM user to access those resources. After creating the resources,
    we went through the steps of the ML life cycle again from the problem statement
    to feature engineering and feature ingestion. We also verified that created feature
    definitions and ingested data could be queried through the API.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have set the stage for the next steps of the ML life cycle – model
    training, validation, deployment, and scoring, in the next chapter, we will learn
    how the addition of the feature store right from the beginning makes the model
    production-ready when the development is complete.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Feast documentation: [https://docs.feast.dev/](https://docs.feast.dev/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Credit scoring with Feast on AWS: [https://github.com/feast-dev/feast-aws-credit-scoring-tutorial](https://github.com/feast-dev/feast-aws-credit-scoring-tutorial)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
