<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Creating a Machine Learning Architecture</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we're going to summarize many of the concepts discussed in the book with the purpose of defining a complete machine learning architecture that is able to preprocess the input data, decompose/augment it, classify/cluster it, and eventually, show the results using graphical tools. We're also going to show how scikit-learn manages complex pipelines and how it's possible to fit them, and search for the optimal parameters in the global context of a complete architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning architectures</h1>
                </header>
            
            <article>
                
<p>Until now we have discussed single methods that could be employed to solve specific problems. However, in real contexts, it's very unlikely to have well-defined datasets that can be immediately fed into a standard classifier or clustering algorithm. A machine learning engineer often has to design a full architecture that a non-expert could consider like a black-box where the raw data enters and the outcomes are automatically produced. All the steps necessary to achieve the final goal must be correctly organized and seamlessly joined together in a processing chain similar to a computational graph (indeed, it's very often a direct acyclic graph). Unfortunately, this is a non-standard process, as every real-life problem has its own peculiarities. However, there are some common steps which are normally included in almost any ML pipeline.</p>
<p>In the following picture, there's a schematic representation of this process:</p>
<div class="CDPAlignCenter CDPAlign"><img height="444" width="559" class="image-border" src="assets/988b11a0-9eff-4559-9150-14856f4a30c5.png"/></div>
<p>Now we will briefly explain the details of each phase with some possible solutions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data collection</h1>
                </header>
            
            <article>
                
<p>The first step is always the most generic because it depends on each single context. However, before working with any data, it's necessary to collect it from all the sources where it's stored. The ideal situation is to have a <strong>comma separated values</strong> (<strong>CSV</strong>) (or another suitable format) dump that can be immediately loaded, but more often, the engineer has to look for all the database tables, define the right SQL query to collect all the pieces of information, and manage data type conversion and encoding. We're not going to discuss this topic, but it's important not to under-evaluate this stage because it can be much more difficult than expected. I suggest, whenever possible, to extract flattened tables, where all the fields are placed on the same row, because it's easier to manipulate a large amount of data using a DBMS or a big data tool, but it can be very time and memory consuming if done on a normal PC directly with Python tools. Moreover, it's important to use a standard character encoding for all text fields. The most common choice is UTF-8, but it's also possible to find DB tables encoded with other charsets and normally it's a good practice to convert all the documents before starting with the other operations. A very famous and powerful Python library for data manipulation is pandas (part of SciPy). It's based on the concept of DataFrame (an abstraction of SQL table) and implements many methods that allow the selection, joining, grouping, and statistical processing of datasets that can fit in memory. In <em>Heydt M., Learning pandas - Python Data Discovery and Analysis Made Easy</em>, Packt, the reader can find all the information needed to use this library to solve many real-life problems. A common problem that must be managed during this phase, is imputing the missing features. In <a href="7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml" target="_blank">Chapter 3</a>, <em>Feature Selection and Feature Engineering</em>, we discussed some practical methods that can be employed automatically before starting with the following steps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalization</h1>
                </header>
            
            <article>
                
<p>Normalizing a numeric dataset is one of the most important steps, particularly when different features have different scales. In <a href="7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml" target="_blank">Chapter 3</a>, <em>Feature Selection and Feature Engineering</em>,<em> </em>we discussed several methods that can be employed to solve this problem. Very often, it's enough to use a <kbd>StandardScaler</kbd> to whiten the data, but sometimes it's better to consider the impact of noisy features on the global trend and use a <kbd>RobustScaler</kbd> to filter them out without the risk of conditioning the remaining features. The reader can easily verify the different performances of the same classifier (in particular, SVMs and neural networks) when working with normalized and unnormalized datasets. As we're going to see in the next section, it's possible to include the normalization step in the processing pipeline as one of the first actions and include the <kbd>C</kbd> parameter in grid search in order to impose an <em>L1</em>/<em>L2</em> weight normalization during the training phase (see the importance of regularization in <a href="3905e421-e623-457f-bd52-13d69cf88467.xhtml" target="_blank">Chapter 4</a>, <em>Linear Regression</em>, when discussing about Ridge, Lasso and ElasticNet).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dimensionality reduction</h1>
                </header>
            
            <article>
                
<p>This step is not always mandatory, but, in many cases, it can be a good solution to memory leaks or long computational times. When the dataset has many features, the probability of some hidden correlation is relatively high. For example, the final price of a product is directly influenced by the price of all materials and, if we remove one secondary element, the value changes slightly (more generally speaking, we can say that the total variance is almost preserved). If you remember how PCA works, you know that this process decorrelates the input data too. Therefore, it's useful to check whether a PCA or a Kernel PCA (for non-linear datasets) can remove some components while keeping the explained variance close to 100 percent (this is equivalent to compressing the data with minimum information loss). There are also other methods discussed in <a href="7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml">Chapter 3</a>, <em>Feature Selection and Feature Engineering</em> (like <kbd>NMF</kbd> or <kbd>SelectKBest</kbd>), that can be useful for selecting only the best features according to various criteria (like ANOVA or chi-squared). Testing the impact of each factor during the initial phases of the project can save time that can be useful when it's necessary to evaluate slower and more complex algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data augmentation</h1>
                </header>
            
            <article>
                
<p>Sometimes the original dataset has only a few non-linear features and it's quite difficult for a standard classifier to capture the dynamics. Moreover, forcing an algorithm on a complex dataset can result in overfitting the model because all the capacity is exhausted in trying to minimize the error considering only the training set, and without taking into account the generalization ability. For this reason, it's sometimes useful to enrich the dataset with derived features that are obtained through functions of the existing ones. <kbd>PolynomialFeatures</kbd> is an example of data augmentation that can really improve the performances of standard algorithms and avoid overfitting. In other cases, it can be useful to introduce trigonometric functions (like <em>sin(x)</em> or <em>cos(x)</em>) or correlating features (like <em>x<sub>1</sub>x<sub>2</sub></em>). The former allows a simpler management of radial datasets, while the latter can provide the classifier with information about the cross-correlation between two features. In general, data augmentation can be employed before trying a more complex algorithm; for example, a logistic regression (that is a linear method) can be successfully applied to augmented non-linear datasets (we saw a similar situation in <a href="3905e421-e623-457f-bd52-13d69cf88467.xhtml" target="_blank">Chapter 4</a>, <em>Linear Regression</em>, when we had discussed the polynomial regression). The choice to employ a more complex (with higher capacity) model or to try to augment the dataset is up to the engineer and must be considered carefully, taking into account both the pros and the cons. In many cases, for example, it's preferable not to modify the original dataset (which could be quite large), but to create a scikit-learn interface to augment the data in real time. In other cases, a neural model can provide faster and more accurate results without the need for data augmentation. Together with parameter selection, this is more of an art than a real science, and the experiments are the only way to gather useful knowledge.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"> Data conversion</h1>
                </header>
            
            <article>
                
<p>This step is probably the simplest and, at the same time, the most important when handling categorical data. We have discussed several methods to encode labels using numerical vectors and it's not necessary to repeat the concepts already explained. A general rule concerns the usage of integer or binary values (one-hot encoding). The latter is probably the best choice when the output of the classifier is the value itself, because, as discussed in <a href="7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml" target="_blank">Chapter 3</a>, <em>Feature Selection and Feature Engineering</em>, it's much more robust to noise and prediction errors. On the other hand, one-hot encoding is quite memory-consuming. Therefore, whenever it's necessary to work with probability distributions (like in NLP), an integer label (representing a dictionary entry or a frequency/count value) can be much more efficient.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modeling/Grid search/Cross-validation</h1>
                </header>
            
            <article>
                
<p>Modeling implies the choice of the classification/clustering algorithm that best suits every specific task. We have discussed different methods and the reader should be able to understand when a set of algorithms is a reasonable candidate, and when it's better to look for another strategy. However, the success of a machine learning technique often depends on the right choice of each parameter involved in the model as well. As already discussed, when talking about data augmentation, it's very difficult to find a precise method to determine the optimal values to assign, and the best approach is always based on a grid search. scikit-learn provides a very flexible mechanism to investigate the performance of a model with different parameter combinations, together with cross-validation (that allows a robust validation without reducing the number of training samples), and this is indeed a more reasonable approach, even for experts engineers. Moreover, when performing different transformations, the effect of a choice can impact the whole pipeline, and, therefore, (we're going to see a few examples in the next section) I always suggest for application of the grid search to all components at the same time, to be able to evaluate the cross-influence of each possible choice. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualization</h1>
                </header>
            
            <article>
                
<p>Sometimes, it's useful/necessary to visualize the results of intermediate and final steps. In this book, we have always shown plots and diagrams using matplotlib, which is part of SciPy and provides a flexible and powerful graphics infrastructure. Even if it's not part of the book, the reader can easily modify the code in order to get different results; for a deeper understanding, refer to Mcgreggor D., <em>Mastering matplotlib</em>, Packt. As this is an evolving sector, many new projects are being developed, offering new and more stylish plotting functions. One of them is Bokeh (<a href="http://bokeh.pydata.org">http://bokeh.pydata.org</a>), that works using some JavaScript code to create interactive graphs that can be embedded into web pages too.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">scikit-learn tools for machine learning architectures</h1>
                </header>
            
            <article>
                
<p>Now we're going to present two very important scikit-learn classes that can help the machine learning engineer to create complex processing structures including all the steps needed to generate the desired outcomes from the raw datasets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pipelines</h1>
                </header>
            
            <article>
                
<p>scikit-learn provides a flexible mechanism for creating pipelines made up of subsequent processing steps. This is possible thanks to a standard interface implemented by the majority of classes therefore most of the components (both data processors/transformers and classifiers/clustering tools) can be exchanged seamlessly. The class <kbd>Pipeline</kbd> accepts a single parameter <kbd>steps</kbd>, which is a list of tuples in the form (name of the component—instance), and creates a complex object with the standard fit/transform interface. For example, if we need to apply a PCA, a standard scaling, and then we want to classify using a SVM, we could create a pipeline in the following way:</p>
<pre><strong>from sklearn.decomposition import PCA</strong><br/><strong>from sklearn.pipeline import Pipeline</strong><br/><strong>from sklearn.preprocessing import StandardScaler</strong><br/><strong>from sklearn.svm import SVC</strong><br/><br/><strong>&gt;&gt;&gt; pca = PCA(n_components=10)</strong><br/><strong>&gt;&gt;&gt; scaler = StandardScaler()</strong><br/><strong>&gt;&gt;&gt; svc = SVC(kernel='poly', gamma=3)</strong><br/><br/><strong>&gt;&gt;&gt; steps = [</strong><br/><strong>&gt;&gt;&gt;    ('pca', pca),</strong><br/><strong>&gt;&gt;&gt;    ('scaler', scaler),</strong><br/><strong>&gt;&gt;&gt;    ('classifier', svc)</strong><br/><strong>&gt;&gt;&gt; ]</strong><br/><br/><strong>&gt;&gt;&gt; pipeline = Pipeline(steps)</strong></pre>
<p>At this point, the pipeline can be fitted like a single classifier (using the standard methods <kbd>fit()</kbd> and <kbd>fit_transform()</kbd>), even if the the input samples are first passed to the <kbd>PCA</kbd> instance, the reduced dataset is normalized by the <kbd>StandardScaler</kbd> instance, and finally, the resulting samples are passed to the classifier.</p>
<p>A pipeline is also very useful together with <kbd>GridSearchCV</kbd>, to evaluate different combinations of parameters, not limited to a single step but considering the whole process. Considering the previous example, we can create a dummy dataset and try to find the optimal parameters:</p>
<pre><strong>from sklearn.datasets import make_classification</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 500</strong><br/><strong>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_informative=15, n_redundant=5, n_classes=2)</strong></pre>
<p>The dataset is quite redundant. Therefore, we need to find the optimal number of components for PCA and the best kernel for the SVM. When working with a pipeline, the name of the parameter must be specified using the component ID followed by a double underscore and then the actual name, for example, <kbd>classifier__kernel</kbd> (if you want to check all the acceptable parameters with the right name, it's enough to execute: <kbd>print(pipeline.get_params().keys())</kbd>). Therefore, we can perform a grid search with the following parameter dictionary:</p>
<pre><strong>from sklearn.model_selection import GridSearchCV</strong><br/><br/><strong>&gt;&gt;&gt; param_grid = {</strong><br/><strong>&gt;&gt;&gt;    'pca__n_components': [5, 10, 12, 15, 18, 20],</strong><br/><strong>&gt;&gt;&gt;    'classifier__kernel': ['rbf', 'poly'],</strong><br/><strong>&gt;&gt;&gt;    'classifier__gamma': [0.05, 0.1, 0.2, 0.5],</strong><br/><strong>&gt;&gt;&gt;    'classifier__degree': [2, 3, 5]</strong><br/><strong>&gt;&gt;&gt; }</strong><br/><br/><strong>&gt;&gt;&gt; gs = GridSearchCV(pipeline, param_grid)</strong><br/><strong>&gt;&gt;&gt; gs.fit(X, Y)</strong></pre>
<p>As expected, the best estimator (which is a complete pipeline) has 15 principal components (that means they are uncorrelated) and a radial-basis function SVM with a relatively high <kbd>gamma</kbd> value (0.2):</p>
<pre><strong>&gt;&gt;&gt; print(gs.best_estimator_)<br/>Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=15, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('classifier', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=2, gamma=0.2, kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False))])</strong></pre>
<p>The corresponding score is:</p>
<pre><strong>&gt;&gt;&gt; print(gs.best_score_)</strong><br/><strong>0.96</strong></pre>
<p>It's also possible to use a <kbd>Pipeline</kbd> together with <kbd>GridSearchCV</kbd> to evaluate different combinations. For example, it can be useful to compare some decomposition methods, mixed with various classifiers:</p>
<pre><strong>from sklearn.datasets import load_digits</strong><br/><strong>from sklearn.decomposition import NMF</strong><br/><strong>from sklearn.feature_selection import SelectKBest, f_classif</strong><br/><strong>from sklearn.linear_model import LogisticRegression</strong><br/><br/><strong>&gt;&gt;&gt; digits = load_digits()</strong><br/><br/><strong>&gt;&gt;&gt; pca = PCA()</strong><br/><strong>&gt;&gt;&gt; nmf = NMF()</strong><br/><strong>&gt;&gt;&gt; kbest = SelectKBest(f_classif)</strong><br/><strong>&gt;&gt;&gt; lr = LogisticRegression()</strong><br/><br/><strong>&gt;&gt;&gt; pipeline_steps = [</strong><br/><strong>&gt;&gt;&gt;    ('dimensionality_reduction', pca),</strong><br/><strong>&gt;&gt;&gt;    ('normalization', scaler),</strong><br/><strong>&gt;&gt;&gt;    ('classification', lr)</strong><br/><strong>&gt;&gt;&gt; ]</strong><br/><br/><strong>&gt;&gt;&gt; pipeline = Pipeline(pipeline_steps)</strong></pre>
<p>We want to compare <strong>principal component analysis</strong> (<strong>PCA</strong>), <strong>non-negative matrix factorization</strong> (<strong>NMF</strong>), and k-best feature selection based on the ANOVA criterion, together with logistic regression and kernelized SVM:</p>
<pre><strong>&gt;&gt;&gt; pca_nmf_components = [10, 20, 30]<br/><br/>&gt;&gt;&gt; param_grid = [</strong><br/><strong>&gt;&gt;&gt;    {</strong><br/><strong>&gt;&gt;&gt;        'dimensionality_reduction': [pca],</strong><br/><strong>&gt;&gt;&gt;        'dimensionality_reduction__n_components': pca_nmf_components,</strong><br/><strong>&gt;&gt;&gt;        'classification': [lr],</strong><br/><strong>&gt;&gt;&gt;        'classification__C': [1, 5, 10, 20]</strong><br/><strong>&gt;&gt;&gt;    },</strong><br/><strong>&gt;&gt;&gt;    {</strong><br/><strong>&gt;&gt;&gt;        'dimensionality_reduction': [pca],</strong><br/><strong>&gt;&gt;&gt;        'dimensionality_reduction__n_components': pca_nmf_components,</strong><br/><strong>&gt;&gt;&gt;        'classification': [svc],</strong><br/><strong>&gt;&gt;&gt;        'classification__kernel': ['rbf', 'poly'],</strong><br/><strong>&gt;&gt;&gt;        'classification__gamma': [0.05, 0.1, 0.2, 0.5, 1.0],</strong><br/><strong>&gt;&gt;&gt;        'classification__degree': [2, 3, 5],<br/>&gt;&gt;&gt;</strong>        <strong>'classification__C': [1, 5, 10, 20]</strong><br/><strong>&gt;&gt;&gt;    },</strong><br/><strong>&gt;&gt;&gt;    {</strong><br/><strong>&gt;&gt;&gt;        'dimensionality_reduction': [nmf],</strong><br/><strong>&gt;&gt;&gt;        'dimensionality_reduction__n_components': pca_nmf_components,</strong><br/><strong>&gt;&gt;&gt;        'classification': [lr],</strong><br/><strong>&gt;&gt;&gt;        'classification__C': [1, 5, 10, 20]</strong><br/><strong>&gt;&gt;&gt;    },</strong><br/><strong>&gt;&gt;&gt;    {</strong><br/><strong>&gt;&gt;&gt;        'dimensionality_reduction': [nmf],</strong><br/><strong>&gt;&gt;&gt;        'dimensionality_reduction__n_components': pca_nmf_components,</strong><br/><strong>&gt;&gt;&gt;        'classification': [svc],</strong><br/><strong>&gt;&gt;&gt;        'classification__kernel': ['rbf', 'poly'],</strong><br/><strong>&gt;&gt;&gt;        'classification__gamma': [0.05, 0.1, 0.2, 0.5, 1.0],</strong><br/><strong>&gt;&gt;&gt;        'classification__degree': [2, 3, 5],<br/>&gt;&gt;&gt;        'classification__C': [1, 5, 10, 20]</strong><br/><strong>&gt;&gt;&gt;    },</strong><br/><strong>&gt;&gt;&gt;    {</strong><br/><strong>&gt;&gt;&gt;        'dimensionality_reduction': [kbest],</strong><br/><strong>&gt;&gt;&gt;        'classification': [svc],</strong><br/><strong>&gt;&gt;&gt;        'classification__kernel': ['rbf', 'poly'],</strong><br/><strong>&gt;&gt;&gt;        'classification__gamma': [0.05, 0.1, 0.2, 0.5, 1.0],</strong><br/><strong>&gt;&gt;&gt;        'classification__degree': [2, 3, 5],<br/>&gt;&gt;&gt;        'classification__C': [1, 5, 10, 20]</strong><br/><strong>&gt;&gt;&gt;    },</strong><br/><strong>&gt;&gt;&gt; ]</strong><br/><br/><strong>&gt;&gt;&gt; gs = GridSearchCV(pipeline, param_grid)</strong><br/><strong>&gt;&gt;&gt; gs.fit(digits.data, digits.target)</strong></pre>
<p>Performing a grid search, we get the pipeline made up of PCA with 20 components (the original dataset 64 features) and an RBF SVM with a very small <kbd>gamma</kbd> value (0.05) and  a medium (5.0) <em>L2</em> penalty parameter <kbd>C</kbd> :</p>
<pre><strong>&gt;&gt;&gt; print(gs.best_estimator_)</strong><br/><strong>Pipeline(steps=[('dimensionality_reduction', PCA(copy=True, iterated_power='auto', n_components=20, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('normalization', StandardScaler(copy=True, with_mean=True, with_std=True)), ('classification', SVC(C=5.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=2, gamma=0.05, kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False))])</strong></pre>
<p>Considering the need to capture small details in the digit representations, these values are an optimal choice. The score for this pipeline is indeed very high:</p>
<pre><strong>&gt;&gt;&gt; print(gs.best_score_)</strong><br/><strong>0.968836950473</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature unions</h1>
                </header>
            
            <article>
                
<p>Another interesting class provided by scikit-learn is <kbd>FeatureUnion</kbd>, which allows concatenating different feature transformations into a single output matrix. The main difference with a pipeline (which can also include a feature union) is that the pipeline selects from alternative scenarios, while a feature union creates a unified dataset where different preprocessing outcomes are joined together. For example, considering the previous results, we could try to optimize our dataset by performing a PCA with 10 components joined with the selection of the best 5 features chosen according to the ANOVA metric. In this way, the dimensionality is reduced to 15 instead of 20:</p>
<pre><strong>from sklearn.pipeline import FeatureUnion</strong><br/><br/><strong>&gt;&gt;&gt; steps_fu = [</strong><br/><strong>&gt;&gt;&gt;    ('pca', PCA(n_components=10)),</strong><br/><strong>&gt;&gt;&gt;    ('kbest', SelectKBest(f_classif, k=5)),</strong><br/><strong>&gt;&gt;&gt; ]</strong><br/><br/><strong>&gt;&gt;&gt; fu = FeatureUnion(steps_fu)</strong><br/><br/><strong>&gt;&gt;&gt; svc = SVC(kernel='rbf', C=5.0, gamma=0.05)</strong><br/><br/><strong>&gt;&gt;&gt; pipeline_steps = [</strong><br/><strong>&gt;&gt;&gt;    ('fu', fu),</strong><br/><strong>&gt;&gt;&gt;    ('scaler', scaler),</strong><br/><strong>&gt;&gt;&gt;    ('classifier', svc)</strong><br/><strong>&gt;&gt;&gt; ]</strong><br/><br/><strong>&gt;&gt;&gt; pipeline = Pipeline(pipeline_steps)</strong></pre>
<p>We already know that a RBF SVM is a good choice, and, therefore, we keep the remaining part of the architecture without modifications. Performing a cross-validation, we get:</p>
<pre><strong>from sklearn.model_selection import cross_val_score</strong><br/><strong><br/>&gt;&gt;&gt; print(cross_val_score(pipeline, digits.data, digits.target, cv=10).mean())</strong><br/><strong>0.965464333604</strong></pre>
<p>The score is slightly lower than before (&lt; 0.002) but the number of features has been considerably reduced and therefore also the computational time. Joining the outputs of different data preprocessors is a form of data augmentation and it must always be taken into account when the original number of features is too high or redundant/noisy and a single decomposition method doesn't succeed in capturing all the dynamics. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Mcgreggor D., <em>Mastering matplotlib</em>, Packt</li>
<li>Heydt M., <em>Learning pandas - Python Data Discovery and Analysis Made Easy</em>, Packt</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this final chapter, we discussed the main elements of machine learning architecture, considering some common scenarios and the procedures that are normally employed to prevent issues and improve the global performance. None of these steps should be discarded without a careful evaluation because the success of a model is determined by the joint action of many parameter, and hyperparameters, and finding the optimal final configuration starts with considering all possible preprocessing steps.</p>
<p>We saw that a grid search is a powerful investigation tool and that it's often a good idea to use it together with a complete set of alternative pipelines (with or without feature unions), so as to find the best solution in the context of a global scenario. Modern personal computers are fast enough to test hundreds of combinations in a few hours, and when the datasets are too large, it's possible to provision a cloud server using one of the existing providers.</p>
<p>Finally, I'd like to repeat that till now (also considering the research in the deep learning field), creating an up-and-running machine learning architecture needs a continuous analysis of alternative solutions and configurations, and there's no silver bullet for any but the simplest cases. This is a science that still keeps an artistic heart!</p>


            </article>

            
        </section>
    </body></html>