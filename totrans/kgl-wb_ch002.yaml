- en: '01 The most renowned tabular competition: Porto Seguro’s Safe Driver Prediction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Join our book community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/file0.png)'
  prefs: []
  type: TYPE_IMG
- en: Learning how to perform top on the leaderboard on any Kaggle competition requires
    patience, diligence and many attempts in order to learn the best way to compete
    and achieve top results. For this reason, we have thought of a workbook that can
    help you build faster those skills by leading you to try some Kaggle competitions
    of the past and to learn how to make top competitions for them by reading discussions,
    reusing notebooks, engineering features and training various models.
  prefs: []
  type: TYPE_NORMAL
- en: We start in the book from one of the most renowned tabular competitions, Porto
    Seguro’s Safe Driver Prediction. In this competition, you are asked to solve a
    common problem in insurance, to figure out who is going to have an auto insurance
    claim in the next year. Such information is useful in order to increase the insurance
    fee to drivers more likely to have a claim and to lower it to those less likely
    to.
  prefs: []
  type: TYPE_NORMAL
- en: In illustrating the key insight and technicalities necessary for cracking this
    competition we will show you the necessary code and ask you to study and answer
    about topics to be found on the Kaggle book itself. Therefore, without much more
    ado, let’s start immediately this new learning path of yours.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: How to tune and train a LightGBM model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a denoising auto-encoder and how to use it to feed a neural network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to effectively blend models that are quite different from each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the competition and the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Porto Seguro is the third largest insurance company in Brazil (it operates in
    Brazil and Uruguay), offering car insurance coverage as well as many other insurance
    products. Having used analytical methods and machine learning for the past 20
    years in order to tailor their prices and make auto insurance coverage more accessible
    to more drivers. In order to explore new ways to achieve their task, they sponsored
    a competition ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction)),
    expecting Kagglers to come up with new and better methods of solving some of their
    core analytical problems.
  prefs: []
  type: TYPE_NORMAL
- en: The competition aims at having Kagglers build a model that predicts the probability
    that a driver will initiate an auto insurance claim in the next year, which is
    a quite common kind of task (the sponsor mentions it as a “classical challenge
    for insurance”). For doing so, the sponsor provided a train and test sets and
    the competition appears ideal for anyone since the dataset is not very large and
    it seems very well prepared.
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated in the page of the competition devoted to presenting the data ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data)):'
  prefs: []
  type: TYPE_NORMAL
- en: '*“features that belong to similar groupings are tagged as such in the feature
    names (e.g., ind, reg, car, calc). In addition, feature names include the postfix
    bin to indicate binary features and cat to indicate categorical features. Features
    without these designations are either continuous or ordinal. Values of -1 indicate
    that the feature was missing from the observation. The target column signifies
    whether or not a claim was filed for that policy holder”.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The data preparation for the competition had been quite carefully not to leak
    any information and, though, maintaining secrecy about the meaning of the features,
    it is quite clear to refer the different used tags to specific kind of features
    commonly used insurance modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: ind refers to “individual characteristics”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: car refer to “cars characteristics”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calc to “calculated features”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reg to “regional/geographic features”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As for the individual features, there has been much speculation during the competition.
    See for instance [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41489](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41489)
    or [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41488](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41488)
    both by Raddar or again the attempts to attribute the feature to Porto Seguro’s
    online quote form [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41057](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41057).
    In spite of all such efforts, in the end the meaning of the features has remained
    a mystery up until now.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interesting fact about this competition is that:'
  prefs: []
  type: TYPE_NORMAL
- en: the data is real world, though the features are anonymous
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the data is really very well prepared, without leakages of any sort (no magic
    features here)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the test set not only holds the same categorical levels of the train test, and
    it also seems to be from the same distribution, though Yuya Yamamoto argues that
    pre-processing the data with t-SNE leads to a failing adversarial validation test
    ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44784](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44784))
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a first exercise, referring to the contents and the code on the Kaggle Book
    related to adversarial validation (starting from page 179), prove that train and
    test data most probably originated from the same data distribution.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'An interesting post by Tilii (Mensur Dlakic, associate Professor at Montana
    State University: [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42197](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42197))
    demonstrates using tSNE that “there are many people who are very similar in terms
    of their insurance parameters, yet some of them will file a claim and others will
    not”. What Tilii mentions is quite typical of what happens in insurance where
    to certain priors (insurance parameters) there is sticked the same probability
    of something happening but that event will happen or not based on how long we
    observe the situation.'
  prefs: []
  type: TYPE_NORMAL
- en: Take for instance IoT and telematic data in insurance. It is quite common to
    analyze a driver's behavior in order to predict if she or he will file a claim
    in the future. If your observation period is too short (for instance one year
    as in the case of this competition), it may happen that even very bad drivers
    won’t have a claim because it is a matter of not too high probability of the outcome
    that can become a reality only after a certain amount of time. Similar ideas are
    discussed by Andy Harless ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42735](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42735))
    which argues instead that the real task of the competition is to guess *“the value
    of a latent continuous variable that determines which drivers are more likely
    to have accidents”* because actually *“making a claim is not a characteristic
    of a driver; it's a result of chance”*.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Evaluation Metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The metric used in the competition is the "normalized Gini coefficient" (named
    for the similar Gini coefficient/index used in Economics), which has been previously
    used in another competition, the Allstate Claim Prediction Challenge ([https://www.kaggle.com/competitions/ClaimPredictionChallenge](https://www.kaggle.com/competitions/ClaimPredictionChallenge)).
    From that competition, we can get a very clear explanation about what this metric
    is about:'
  prefs: []
  type: TYPE_NORMAL
- en: '*When you submit an entry, the observations are sorted from "largest prediction"
    to "smallest prediction". This is the only step where your predictions come into
    play, so only the order determined by your predictions matters. Visualize the
    observations arranged from left to right, with the largest predictions on the
    left. We then move from left to right, asking "In the leftmost x% of the data,
    how much of the actual observed loss have you accumulated?" With no model, you
    can expect to accumulate 10% of the loss in 10% of the predictions, so no model
    (or a "null" model) achieves a straight line. We call the area between your curve
    and this straight line the Gini coefficient.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*There is a maximum achievable area for a "perfect" model. We will use the
    normalized Gini coefficient by dividing the Gini coefficient of your model by
    the Gini coefficient of the perfect model.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another good explanation is provided in the competition by the notebook by
    Kilian Batzner: [https://www.kaggle.com/code/batzner/gini-coefficient-an-intuitive-explanation](https://www.kaggle.com/code/batzner/gini-coefficient-an-intuitive-explanation)
    that, though means of plots and toy examples tries to give a sense to a not so
    common metric but in the actuarial departments of insurance companies.'
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 5 of the Kaggle Book (pages 95 onward), we explained how to deal
    with competition metrics, especially if they are new and generally unknown. As
    an exercise, can you find out how many competitions on Kaggle have used the normalized
    Gini coefficient as an evaluation metric?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The metric can be approximated by the Mann–Whitney U non-parametric statistical
    test and by the ROC-AUC score because it approximately corresponds to 2 * ROC-AUC
    - 1\. Hence, maximizing the ROC-AUC is the same as maximizing the Normalized Gini
    coefficient (for a reference see the “Relation to other statistical measures”
    in the Wikipedia entry: [https://en.wikipedia.org/wiki/Gini_coefficient](https://en.wikipedia.org/wiki/Gini_coefficient)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The metric can also be approximately expressed as the covariance of scaled
    prediction rank and scaled target value, resulting in a more understandable rank
    association measure (see Dmitriy Guller: [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/40576](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/40576))'
  prefs: []
  type: TYPE_NORMAL
- en: From the point of view of the objective function, you can optimize for the binary
    logloss (as you would do in a classification problem). Nor ROC-AUC nor the normalized
    Gini coefficient are differentiable and they may be used only for metric evaluation
    on the validation set (for instance for early stopping or for reducing the learning
    rate in a neural network). However, optimizing for the logloss doesn’t always
    improve the ROC-AUC and the normalized Gini coefficients. There is actually a
    differentiable ROC-AUC approximation (Calders, Toon, and Szymon Jaroszewicz. "Efficient
    AUC optimization for classification." European conference on principles of data
    mining and knowledge discovery. Springer, Berlin, Heidelberg, 2007 [https://link.springer.com/content/pdf/10.1007/978-3-540-74976-9_8.pdf](https://link.springer.com/content/pdf/10.1007/978-3-540-74976-9_8.pdf)).
    However, it seems that it is not necessary to use anything different from logloss
    as objective function and ROC-AUC or normalized Gini coefficient as evaluation
    metric in the competition.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are actually a few Python implementations among the notebooks. We have
    used here and we suggest the work by CPMP ([https://www.kaggle.com/code/cpmpml/extremely-fast-gini-computation/notebook](https://www.kaggle.com/code/cpmpml/extremely-fast-gini-computation/notebook))
    that uses Numba for speeding up computations: it is both exact and fast.'
  prefs: []
  type: TYPE_NORMAL
- en: Examining the top solution’s ideas from Michael Jahrer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Michael Jahrer ([https://www.kaggle.com/mjahrer](https://www.kaggle.com/mjahrer),
    competitions grandmaster and one of the winners of the Netflix Prize in the team
    "BellKor's Pragmatic Chaos"), has led for long time and by a fair margin the public
    leaderboard during the competition and, when finally the private one has been
    disclosed, has been declared the winner.
  prefs: []
  type: TYPE_NORMAL
- en: Shortly after, in the discussion forum, he published a short summary of his
    solution that has become a reference for many Kagglers because of his smart usage
    of denoising autoencoders and neural networks ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629)).
    Although Michael hasn’t accompanied his post by any Python code regarding his
    solution (he quoted it as an “old school” and “low level” one, being directly
    written in C++/CUDA with no Python), his writing is quite rich in references to
    what models he has used as well to theirs hyper-parameters and architectures.
  prefs: []
  type: TYPE_NORMAL
- en: First, Michael explains that his solution is composed of a blend of six models
    (one LightGBM model and five neural networks). Moreover, since no advantage could
    be gained by weighting the contributions of each model to the blend (as well as
    doing linear and non-linear stacking) likely because of overfitting, he states
    that he resorted to just a plain blend of models (an arithmetic mean) that have
    been built from different seeds.
  prefs: []
  type: TYPE_NORMAL
- en: Such insight makes the task much easier for us in order to replicate his approach,
    also because he also mentions that just having blend together the LightGBM’s results
    with one from the neural networks he built would have been enough to guarantee
    the first place in the competition. That will limit our exercise work to two good
    single models instead of a host of them. In addition, he mentioned having done
    little data processing, but dropping some columns and one-hot encoding categorical
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Building a LightGBM submission
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our exercise starts by working out a solution based on LightGBM. You can find
    the code already set for execution at the Kaggle Notebook at this address: [https://www.kaggle.com/code/lucamassaron/workbook-lgb](https://www.kaggle.com/code/lucamassaron/workbook-lgb).
    Although we made the code readily available, we instead suggest you to type or
    copy the code directly from the book and execute it cell by cell, understanding
    what each line of code does and you could furthermore personalise the solution
    and have it perform even better.'
  prefs: []
  type: TYPE_NORMAL
- en: We start by importing key packages (Numpy, Pandas, Optuna for hyper-parameter
    optimization, LightGBM and some utility functions). We also define a configuration
    class and instantiate it. We will discuss the parameters defined into the configuration
    class during the exploration of the code as we progress. What is important to
    remark here is that by using a class containing all your parameters it will be
    easier for you to modify them in a consistent way along the code. In the heat
    of a competition, it is easy to forget to update a parameter that it is referred
    to in multiple places in the code and it is always difficult to set the parameters
    when they are dispersed among cells and functions. A configuration class can save
    you a lot of effort and spare you mistakes along the way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The next step requires importing train, test and the sample submission datasets.
    By doing so by pandas csv reading function, we also set the index of the uploaded
    dataframes to the identifier (the ‘id’ column) of each data example.
  prefs: []
  type: TYPE_NORMAL
- en: Since features that belong to similar groupings are tagged (using ind, reg,
    car, calc tags in their labels) and also binary and categorical features are easy
    to locate (they use the bin and cat, respectively, tags in their labels), we can
    enumerate them and record them into lists.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then, we just extract the target (a binary target of 0s and 1s) and remove it
    from the train dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: At this point, as pointed out by Michael Jahrer, we can drop the calc features.
    This idea has recurred a lot during the competition ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41970](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41970)),
    especially in notebooks, both because it could be empirically verified that dropping
    them improved the public leaderboard score, both because they performed poorly
    in gradient boosting models (their importance is always below the average). We
    can argue that, since they are engineered features, they do not contain new information
    in respect of their origin features but they just add noise to any model trained
    comprising them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'During the competition, Tilii has tested feature elimination using Boruta ([https://github.com/scikit-learn-contrib/boruta_py](https://github.com/scikit-learn-contrib/boruta_py)).
    You can find his kernel here: [https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook](https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook).
    As you can check, there is no calc_feature considered as a confirmed feature by
    Boruta.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Exercise: based on the suggestions provided in the Kaggle Book at page 220
    (“Using feature importance to evaluate your work”), as an exercise, code your
    own feature selection notebook for this competition and check what features should
    be kept and what should be discarded.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Categorical features are instead one-hot encoded. Since we want to retrain their
    labels, and since the same levels are present both in train and test (the result
    of a careful train/test split between the two arranged by the Porto Seguro team),
    instead of the usual Scikit-Learn OneHotEncoder ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html))
    we use the pandas get_dummies function ([https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)).
    Since the pandas function may produce different encodings if the features and
    their levels differ from train to test set, we assert a check on the one hot encoding
    resulting in the same for both.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After one hot encoding the categorical features we have completed doing all
    the data processing. We proceed to define our evaluation metric, the Normalized
    Gini Coefficient, as previously discussed. Since we are going to use a LightGBM
    model, we have to add a suitable wrapper (`gini_lgb`) in order to return to the
    GBM algorithm the evaluation of the training and the validation sets in a form
    that could works with it (see: [https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html?highlight=higher_better#lightgbm.Booster.eval](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html?highlight=higher_better#lightgbm.Booster.eval)
    - “Each evaluation function should accept two parameters: preds, eval_data, and
    return `(eval_name`, `eval_result`, `is_higher_better`) or list of such tuples”).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As for the training parameters, we found that the parameters suggested by Michael
    Jahrer in his post ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629))
    work perfectly. You may also try to come up with the same parameters or with similarly
    performing ones by performing a search by optuna ([https://optuna.org/](https://optuna.org/))
    if you set the `optuna_lgb` flag to True in the config class. Here the optimization
    tries to find the best values for key parameters such as the learning rate and
    the regularization parameters, based on a 5-fold cross-validation test on training
    data. In order to speed up things, early stopping on the validation itself is
    taken into account (which, we are aware, could actually advantage some values
    that can overfit better the validation fold - a good alternative could be to remove
    the early stopping callback and keep a fixed number of rounds for the training).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'During the competition, Tilii has tested feature elimination using Boruta ([https://github.com/scikit-learn-contrib/boruta_py](https://github.com/scikit-learn-contrib/boruta_py)).
    You can find his kernel here: [https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook](https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook).
    As you can check, there is no calc_feature considered as a confirmed feature by
    Boruta.'
  prefs: []
  type: TYPE_NORMAL
- en: In the Kaggle Book, we explain about both hyper-parameter optimization (pages
    241 onward) and provide some key hyper-parameters for the LightGBM model. As an
    exercise, try to improve the hyper-parameter search by reducing or increasing
    the explored parameters and by trying alternative methods such as the random search
    or the halving search from Scikit-Learn (pages 245-246).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Once we have our best parameters (or we simply try Jahrer’s ones), we are ready
    to train and predict. Our strategy, as suggested by the best solution, is to train
    a model on each cross-validation folds and use that fold to contribute to an average
    of test predictions. The snippet of code will produce both the test predictions
    and the out of fold predictions on the train set, later useful for figuring out
    how to ensemble the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The model training shouldn’t take too long. In the end you can get reported
    the Normalized Gini Coefficient obtained during the cross-validation procedure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The results are quite encouraging because the average score is 0.289 and the
    standard deviation of the values is quite small.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: All that is left is to save the out-of-fold and the test predictions as a submission
    and to verify the results on the public and private leaderboards.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The obtained public score should be around 0.28442\. The associated private
    score is about 0.29121, placing you in the 30^(th) position in the final leaderboard.
    A quite good result, but we still have to blend it with a different model, a neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging the training set (i.e. taking multiple bootstraps of the training data
    and training multiple models based on the bootstraps) should increase the performance,
    though, as Michael Jahrer himself noted in his post, not all that much.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Denoising Auto-encoder and a DNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is not to set up a denoising auto-encoder (DAE) and a neural
    network that can learn and predict from it. You can find the running code at this
    notebook: [https://www.kaggle.com/code/lucamassaron/workbook-dae](https://www.kaggle.com/code/lucamassaron/workbook-dae)
    . The notebook can be run in GPU mode (speedier), but it can also run in CPU one
    with some slight modifications.'
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about denoising auto-encoders as being used in Kaggle competitions
    in the Kaggle Book, at pages 226 and following.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Actually there are no examples around reproducing Michael Jahrer’s approach
    in the competition using DAEs, but we tooked example from a working TensorFlow
    implementation in another competition by OsciiArt ([https://www.kaggle.com/code/osciiart/denoising-autoencoder](https://www.kaggle.com/code/osciiart/denoising-autoencoder)).
  prefs: []
  type: TYPE_NORMAL
- en: Here we start by importing all the necessary packages, especially TensorFlow
    and Keras. Since we are going to create multiple neural networks, we point out
    to TensorFlow not to use all the GPU memory available by using the experimental
    `set_memory_growth` command. Such will avoid having memory overflow problems along
    the way. We also record the Leaky Relu activation as a custom one, so we can just
    mention it as an activation by a string in Keras layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Related to our intention of creating multiple neural networks without running
    out of memory, we also define a simple function for cleaning the memory in GPU
    and removing models that are no longer needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We also reconfigure the Config class in order to take into account multiple
    parameters related to the denoising auto-encoder and the neural network. As previously
    stated about the LightGBM, having all the parameters in a single place renders
    simpler modifying them in a consistent way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As previously, we load the datasets and proceed processing the features by removing
    the calc features and one hot encoding the categorical ones. We leave missing
    cases valued at -1, as Michael Jahrer pointed out in his solution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: However, differently from our previous approach, we have to rescale all the
    features that are not binary or one hot-encoded categorical. Rescaling will allow
    the optimization algorithm of both the auto-encoder and the neural network to
    converge to a good solution faster because it will have to work with values set
    into a comparable and predefined range. Instead of using statistical normalisation,
    GaussRank is a procedure that also allows the modification of the distribution
    of transformed variables into a Gaussian one.
  prefs: []
  type: TYPE_NORMAL
- en: 'As also stated in some papers, such as in the Batch Normalization paper: [https://arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf),
    neural networks perform even better if you provide them a Gaussian input. Accordingly
    to this NVidia blog post, [https://developer.nvidia.com/blog/gauss-rank-transformation-is-100x-faster-with-rapids-and-cupy/](https://developer.nvidia.com/blog/gauss-rank-transformation-is-100x-faster-with-rapids-and-cupy/),
    GaussRank works most of the times but when features are already normally distributed
    or are extremely asymmetrical (in such cases applying the transformation may lead
    to worsened performances).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can apply the GaussRank transformation separately on the train and test
    features on all the numeric features of our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: When normalising the features, we simply turn our data into a NumPy array of
    float32 values, the ideal input for a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Next, we just prepare some useful functions such as the evaluation function,
    the normalized Gini coefficient and a plotting function helpful representing a
    Keras model history of fitting on both training and validation sets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The next functions are actually a bit more complex and more related to the functioning
    of both the denoising auto-encoder and the supervised neural network. The `batch_generator`
    is a function that will create a generator providing shuffled chunks of the data
    based on a batch size. It isn’t actually used as a stand-alone generator but as
    part of a more complex batch generator that we are soon going to describe, the
    `mixup_generator`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `mixup_generator`, another generator, returns batches of data whose values
    have been partially swapped in order to create some noise and augment the data
    in order for the denoising auto-encoder (DAE) not to overfit the train set. It
    works based on a swap rate, fixed at 15% of features as suggested by Michael Jahrer.
  prefs: []
  type: TYPE_NORMAL
- en: The function generates two distinct batches of data, one to be released to the
    model and another to be used as a source for the value to be swapped in the batch
    to be released. Based on a random choice, whose base probability is the swap rate,
    at each batch a certain number of features is decided to be swapped between the
    two batches.
  prefs: []
  type: TYPE_NORMAL
- en: Such assures that the DAE cannot always rely on the same features (since they
    can be randomly swapped from time to time) but it has to concentrate on the whole
    of the features (something similar to dropout in a certain sense). in order to
    find relations between them and reconstruct correctly the data at the end of the
    process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `get_DAE` is the function that builds the denoising auto-encoder. It accepts
    a parameter for defining the architecture which in our case has been set to three
    layers of 1500 nodes each (as suggested from Michael Jahrer’s solution). The first
    layer should act as an encoder, the second is a bottleneck layer ideally containing
    the latent features capable of expressing the information in the data, the last
    layer is a decoding layer, capable of reconstructing the initial input data. The
    three layers have a relu activation function, no bias and each one is followed
    by a batch normalization layer. The final output with the reconstructed input
    data has a linear activation. The training is provided using an adam optimizer
    with standard settings (the optimised cost function is the mean squared error
    - mse).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `extract_dae_features` function is reported here only for educational purposes.
    The function helps in the extraction of the values of specific layers of the trained
    denoising auto-encoder. The extraction works by building a new model combining
    the DAE input layer and the desired output one. A simple predict will then extract
    the values we need (the predict also allows fixing the preferred batch size in
    order to fit any memory requirement).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the competition, given the number of observations and the number
    of the features to be taken out from the auto-encoder, if we were to use this
    function, the resulting dense matrix would be too large to be handled by the memory
    of a Kaggle notebook. For this reason our strategy won’t be to transform the original
    data into the auto-encoder node values of the bottleneck layer but to fuse the
    auto-encoder with its frozen layers up to the bottleneck with the supervised neural
    network, as we will be discussing soon.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: To complete the work with the DAE, we have a final function wrapping all the
    previous ones into an unsupervised training procedure (at least partially unsupervised
    since there is an early stop monitor set on a validation set). The function sets
    up the mix-up generator, creates the denoising auto-encoder architecture and then
    trains it, monitoring its fit on a validation set for an early stop if there are
    signs of overfitting. Finally, before returning the trained DAE, it plots a graph
    of the training and validation fit and it stores the model on disk.
  prefs: []
  type: TYPE_NORMAL
- en: Even if we try to fix a seed on this model, contrary to the LightGBM model,
    the results are extremely variable and they may influence the final ensemble results.
    Though the result will be a high scoring one, it may land higher or lower on the
    private leaderboard, though the results obtained on the public one are very correlated
    to the public leaderboard and it will be easy for you to always pick up the best
    final submission based on its public results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Having dealt with the DAE, we take the chance also to define the supervised
    neural model down the line that should predict our claim expectations. As a first
    step, we define a function to define a single layer of the work:'
  prefs: []
  type: TYPE_NORMAL
- en: Random normal initialization, since empirically it has been found to converge
    to better results in this problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dense layer with L2 regularization and parametrable activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An excludable and tunable dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for creating the dense blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As you may have already noticed, the function defining the single layer is quite
    customizable. The same goes for the wrapper architecture function, taking inputs
    for the number of layers and units in them, dropout probabilities, regularization
    and activation type. The idea is to be able to run a Neural Architecture Search
    (NAS) and figure out what configuration should perform better in our problem.
  prefs: []
  type: TYPE_NORMAL
- en: As a final note on the function, among the inputs it is required to provide
    the trained DAE because its inputs are used as the neural network model inputs
    while its first layers are connected to the DAE’s outputs. In such a way we are
    de facto concatenating the two models into one (the DAE weights are frozen anyway
    and not trainable, though). This solution has been devised in order to avoid having
    to transform all your training data but only the single batches that the neural
    network is processing, thus saving memory in the system.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We conclude with a wrapper for the training process, including all the steps
    in order to train the entire pipeline on a cross-validation fold.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Since our DAE implementation is surely different from Jahrer’s, although the
    idea behind is the same, we cannot rely completely on his indications on the architecture
    of the supervised neural network and we have to look for the ideal ones as we
    have been looking for the best hyper-parameters in the LightGBM model. Using Optuna
    and leveraging the multiple parameters that we set open for configuring the network’s
    architecture, we can run this code snippet for some hours and get an idea about
    what could work better.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our experiments we that:'
  prefs: []
  type: TYPE_NORMAL
- en: we should use a two layer network with few nodes, 64 and 32 respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: input dropout, dropout between layers and some L2 regularization do help.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is better to use the SELU activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code snippet for running the entire optimization experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: If you are looking for more information about more on Neural Architecture Search
    (NAS), you can have a look at the Kaggle Book, at pages 276 onward. In the case
    of the DAE and the supervised neural network, it is critical to look for the best
    architecture since we are implementing something surely different from Michael
    Jahrer solution.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As an exercise, try to improve the hyper-parameter search by using KerasTuner
    (to be found on pages 285 onward in the Kaggle Book), a fast solution for optimizing
    neural networks that has seen the important contribution of François Chollet,
    the creator of Keras.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Having finally set everything ready, we are set to start the training. In about
    one hour, on a Kaggle notebook with GPU, you can obtain complete test and out-of-fold
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As done with the LighGBM model, we can get an idea of the results by looking
    at the average fold normalized Gini coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The results won’t be quite in line with what previously obtained using the LightGBM.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Producing the submission and submitting it will result in a public score of
    about 0.27737 and a private score of about 0.28471 (results may vary wildly as
    we previously mentioned), not quite a high score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The scarce results from the neural network seem to go by the adage that neural
    networks underperform in tabular problems. As Kagglers, anyway, we know that all
    models are useful for a successful placing on the leaderboard, we just need to
    figure out how to best use them. Surely, a neural network feed with an auto-encoder
    has worked out a solution less affected by noise in data and elaborated the information
    in a different way than a GBM.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembling the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, having two models, what’s left is to mix them together and see if we can
    improve the results. As suggested by Jahrer we go straight for a blend of them,
    but we do not limit ourselves to producing just an average of the two (since our
    approach in the end has slightly differed from Jahrer’s one) but we will also
    try to get optimal weights for the blend. We start importing the out-of-fold predictions
    and having our evaluation function ready.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Once done, we turn the out-of-fold predictions of the LightGBM and the neural
    network into ranks since the normalized Gini coefficient is sensible to rankings
    (as would be a ROC-AUC evaluation).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now we just test if, by combining the two models using different weights we
    can get a better evaluation on the out-of-fold data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'When ready, by running the snippet we can get interesting results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: It seems that blending using a strong weight (0.8) on the LightGBM model and
    a weaker one (0.2) on the neural network may lead to an over performing model.
    We immediately try this hypothesis by setting a blend with the same weights for
    the models and the ideal weights that we have found out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'First we try the equal weights solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'It leads to a public score of 0.28393 and a private score of 0.29093, which
    is around 50^(th) position in the final leaderboard, a bit far from our expectations.
    Now let’s try using the weights the out-of-fold predictions helped us to find:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Here the results lead to a public score of 0.28502 and a private score of 0.29192
    that turns out to be around the seventh position in the final leaderboard. A much
    better result indeed, because the LightGBM is a good one but it is probably missing
    some nuances in the data that can be provided as a favorable correction by adding
    some information from the neural network trained on the denoised data.
  prefs: []
  type: TYPE_NORMAL
- en: As pointed out by CPMP in his solution ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44614](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44614)),
    depending on how to build your cross-validation, you can experience a “huge variation
    of Gini scores among folds”. For this reason, CPMP suggests to decrease the variance
    of the estimates by using many different seeds for multiple cross-validations
    and averaging the results.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Exercise: as an exercise, try to modify the code we used in order to create
    more stable predictions, especially for the denoising auto-encoder.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this first chapter, you have dealt with a classical tabular competition.
    By reading the notebooks and discussions of the competition, we have come up with
    a simple solution involving just two models to be easily blended. In particular,
    we have offered an example on how to use a denoising auto-encoder in order to
    produce an alternative data processing particularly useful when operating with
    neural networks for tabular data. By understanding and replicating solutions on
    past competitions, you can quickly build up your core competencies on Kaggle competitions
    and be quickly able to perform consistently and highly in more recent competitions
    and challenges.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore another tabular competition from Kaggle,
    this time revolving about a complex prediction problem with time series.
  prefs: []
  type: TYPE_NORMAL
