- en: '01 The most renowned tabular competition: Porto Seguro’s Safe Driver Prediction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 01 最著名的表格竞赛：Porto Seguro的Safe Driver Prediction
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的Discord书社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![](img/file0.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file0.png)'
- en: Learning how to perform top on the leaderboard on any Kaggle competition requires
    patience, diligence and many attempts in order to learn the best way to compete
    and achieve top results. For this reason, we have thought of a workbook that can
    help you build faster those skills by leading you to try some Kaggle competitions
    of the past and to learn how to make top competitions for them by reading discussions,
    reusing notebooks, engineering features and training various models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何在任何Kaggle竞赛的排行榜上名列前茅需要耐心、勤奋和多次尝试，以便学会最佳的竞赛方式并取得优异成绩。因此，我们想到了一个工作簿，可以通过引导你尝试一些过去的Kaggle竞赛，并通过阅读讨论、重用笔记本、特征工程和训练各种模型来帮助你更快地掌握这些技能。
- en: We start in the book from one of the most renowned tabular competitions, Porto
    Seguro’s Safe Driver Prediction. In this competition, you are asked to solve a
    common problem in insurance, to figure out who is going to have an auto insurance
    claim in the next year. Such information is useful in order to increase the insurance
    fee to drivers more likely to have a claim and to lower it to those less likely
    to.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在书中从最著名的表格竞赛之一，Porto Seguro的Safe Driver Prediction开始。在这个竞赛中，你需要解决保险中的一个常见问题，即确定下一年谁会提出汽车保险索赔。这样的信息对于提高那些更有可能提出索赔的司机的保险费，以及降低那些不太可能提出索赔的司机的保险费是有用的。
- en: In illustrating the key insight and technicalities necessary for cracking this
    competition we will show you the necessary code and ask you to study and answer
    about topics to be found on the Kaggle book itself. Therefore, without much more
    ado, let’s start immediately this new learning path of yours.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在阐述破解这个竞赛所需的关键洞察和技术细节时，我们将向您展示必要的代码，并要求您研究并回答在Kaggle书籍本身中可以找到的主题。因此，无需多言，让我们立即开始您的新学习之旅。
- en: 'In this chapter, you will learn:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习：
- en: How to tune and train a LightGBM model.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何调整和训练LightGBM模型。
- en: How to build a denoising auto-encoder and how to use it to feed a neural network.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建去噪自编码器以及如何使用它来为神经网络提供数据。
- en: How to effectively blend models that are quite different from each other.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何有效地融合彼此差异很大的模型。
- en: Understanding the competition and the data
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解竞赛和数据
- en: Porto Seguro is the third largest insurance company in Brazil (it operates in
    Brazil and Uruguay), offering car insurance coverage as well as many other insurance
    products. Having used analytical methods and machine learning for the past 20
    years in order to tailor their prices and make auto insurance coverage more accessible
    to more drivers. In order to explore new ways to achieve their task, they sponsored
    a competition ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction)),
    expecting Kagglers to come up with new and better methods of solving some of their
    core analytical problems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Porto Seguro是巴西第三大保险公司（它在巴西和乌拉圭运营），提供汽车保险以及其他许多保险产品。在过去20年中，他们使用分析方法和机器学习来调整他们的价格，使汽车保险覆盖面更易于更多司机获得。为了探索实现他们任务的新方法，他们赞助了一个竞赛([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction))，期望Kagglers能够提出解决他们核心分析问题的新方法。
- en: The competition aims at having Kagglers build a model that predicts the probability
    that a driver will initiate an auto insurance claim in the next year, which is
    a quite common kind of task (the sponsor mentions it as a “classical challenge
    for insurance”). For doing so, the sponsor provided a train and test sets and
    the competition appears ideal for anyone since the dataset is not very large and
    it seems very well prepared.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该竞赛的目标是让Kagglers构建一个模型，预测司机在接下来一年内提出汽车保险索赔的概率，这是一个相当常见的任务（赞助商将其称为“保险的古典挑战”）。为此，赞助商提供了训练集和测试集，并且由于数据集不是很大，看起来准备得非常好，因此对任何人来说都是一个理想的竞赛。
- en: 'As stated in the page of the competition devoted to presenting the data ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data)):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在竞赛数据展示页面所述([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data))：
- en: '*“features that belong to similar groupings are tagged as such in the feature
    names (e.g., ind, reg, car, calc). In addition, feature names include the postfix
    bin to indicate binary features and cat to indicate categorical features. Features
    without these designations are either continuous or ordinal. Values of -1 indicate
    that the feature was missing from the observation. The target column signifies
    whether or not a claim was filed for that policy holder”.*'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“属于相似分组特征在特征名称中标记为如此（例如，ind，reg，car，calc）。此外，特征名称包括后缀bin来指示二元特征，cat来指示分类特征。没有这些指定的特征要么是连续的，要么是序数的。-1的值表示该特征在观测中缺失。目标列表示该保单持有人是否提交了索赔”*。'
- en: 'The data preparation for the competition had been quite carefully not to leak
    any information and, though, maintaining secrecy about the meaning of the features,
    it is quite clear to refer the different used tags to specific kind of features
    commonly used insurance modeling:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛的数据准备非常仔细，以确保不泄露任何信息，尽管如此，关于特征的含义仍然保持保密，但很明显，可以将不同使用的标签指代到保险建模中常用的特定类型的特征：
- en: ind refers to “individual characteristics”
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ind指代“个人特征”
- en: car refer to “cars characteristics”
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “car”指的是“汽车特征”。
- en: calc to “calculated features”
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: calc指代“计算特征”
- en: reg to “regional/geographic features”
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: reg指代“区域/地理特征”
- en: As for the individual features, there has been much speculation during the competition.
    See for instance [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41489](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41489)
    or [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41488](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41488)
    both by Raddar or again the attempts to attribute the feature to Porto Seguro’s
    online quote form [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41057](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41057).
    In spite of all such efforts, in the end the meaning of the features has remained
    a mystery up until now.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 至于个别特征，在比赛中已经有很多猜测。例如，参见[https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41489](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41489)或[https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41488](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41488)，这两篇文章都由Raddar撰写，或者再次尝试将特征归因于Porto
    Seguro的在线报价表[https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41057](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41057)。尽管做出了所有这些努力，但直到现在，特征的含义仍然是个谜。
- en: 'The interesting fact about this competition is that:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个竞赛的有趣事实是：
- en: the data is real world, though the features are anonymous
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据是真实的，尽管特征是匿名的
- en: the data is really very well prepared, without leakages of any sort (no magic
    features here)
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备得非常好，没有任何泄露（这里没有魔法特征）
- en: the test set not only holds the same categorical levels of the train test, and
    it also seems to be from the same distribution, though Yuya Yamamoto argues that
    pre-processing the data with t-SNE leads to a failing adversarial validation test
    ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44784](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44784))
    .
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试集不仅与训练测试集具有相同的分类级别，而且似乎来自相同的分布，尽管山本优雅认为使用t-SNE对数据进行预处理会导致对抗验证测试失败（[https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44784](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44784)）。
- en: As a first exercise, referring to the contents and the code on the Kaggle Book
    related to adversarial validation (starting from page 179), prove that train and
    test data most probably originated from the same data distribution.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作为第一次练习，参考Kaggle Book中关于对抗验证的内容和代码（从第179页开始），证明训练数据和测试数据很可能来自同一数据分布。
- en: 'An interesting post by Tilii (Mensur Dlakic, associate Professor at Montana
    State University: [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42197](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42197))
    demonstrates using tSNE that “there are many people who are very similar in terms
    of their insurance parameters, yet some of them will file a claim and others will
    not”. What Tilii mentions is quite typical of what happens in insurance where
    to certain priors (insurance parameters) there is sticked the same probability
    of something happening but that event will happen or not based on how long we
    observe the situation.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Tilii（蒙大拿州立大学的副教授Mensur Dlakic）的一篇有趣的文章（[https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42197](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42197)）展示了使用tSNE技术，指出“在保险参数方面，许多人非常相似，但其中一些人会提出索赔，而另一些人则不会”。Tilii提到的情况在保险行业中很典型，即对于某些先验（保险参数）来说，发生某事的概率是相同的，但该事件是否发生则取决于我们观察情况的时间长短。
- en: Take for instance IoT and telematic data in insurance. It is quite common to
    analyze a driver's behavior in order to predict if she or he will file a claim
    in the future. If your observation period is too short (for instance one year
    as in the case of this competition), it may happen that even very bad drivers
    won’t have a claim because it is a matter of not too high probability of the outcome
    that can become a reality only after a certain amount of time. Similar ideas are
    discussed by Andy Harless ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42735](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42735))
    which argues instead that the real task of the competition is to guess *“the value
    of a latent continuous variable that determines which drivers are more likely
    to have accidents”* because actually *“making a claim is not a characteristic
    of a driver; it's a result of chance”*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以保险行业中的物联网和遥测数据为例。分析驾驶员的行为以预测他们未来是否会提出索赔是很常见的。如果你的观察期太短（例如，像这次比赛一样，只有一年），那么即使是非常糟糕的驾驶员也可能不会提出索赔，因为这只是一个不太可能成为现实的低概率事件，而这一事件只有在一定时间后才会发生。类似的观点在Andy
    Harless的讨论中也有提及（[https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42735](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/42735)），他反而认为比赛的真正任务是猜测*“一个潜在连续变量的值，该变量决定了哪些驾驶员更有可能发生事故”*，因为实际上*“提出索赔并不是驾驶员的特征；它是偶然的结果”*。
- en: Understanding the Evaluation Metric
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解评估指标
- en: 'The metric used in the competition is the "normalized Gini coefficient" (named
    for the similar Gini coefficient/index used in Economics), which has been previously
    used in another competition, the Allstate Claim Prediction Challenge ([https://www.kaggle.com/competitions/ClaimPredictionChallenge](https://www.kaggle.com/competitions/ClaimPredictionChallenge)).
    From that competition, we can get a very clear explanation about what this metric
    is about:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛中使用的指标是“标准化基尼系数”（以经济学中使用的类似基尼系数/指数命名），它之前在另一个比赛中也被使用过，即全美保险公司索赔预测挑战赛（[https://www.kaggle.com/competitions/ClaimPredictionChallenge](https://www.kaggle.com/competitions/ClaimPredictionChallenge)）。从那次比赛，我们可以清楚地了解这个指标的含义：
- en: '*When you submit an entry, the observations are sorted from "largest prediction"
    to "smallest prediction". This is the only step where your predictions come into
    play, so only the order determined by your predictions matters. Visualize the
    observations arranged from left to right, with the largest predictions on the
    left. We then move from left to right, asking "In the leftmost x% of the data,
    how much of the actual observed loss have you accumulated?" With no model, you
    can expect to accumulate 10% of the loss in 10% of the predictions, so no model
    (or a "null" model) achieves a straight line. We call the area between your curve
    and this straight line the Gini coefficient.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*当你提交一个条目时，观察结果是从“预测值最大”到“预测值最小”排序的。这是唯一一个你的预测发挥作用的地方，所以只有由你的预测决定的顺序才是重要的。将观察结果从左到右可视化，预测值最大的在左边。然后我们从左到右移动，问“在数据的最左侧x%中，你积累了多少实际观察到的损失？”如果没有模型，你可以预期在10%的预测中积累10%的损失，所以没有任何模型（或“零”模型）会得到一条直线。我们将你的曲线和这条直线之间的区域称为基尼系数*。'
- en: '*There is a maximum achievable area for a "perfect" model. We will use the
    normalized Gini coefficient by dividing the Gini coefficient of your model by
    the Gini coefficient of the perfect model.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*“一个‘完美’模型有一个可达到的最大面积。我们将通过将你的模型的基尼系数除以完美模型的基尼系数来使用归一化基尼系数。”*'
- en: 'Another good explanation is provided in the competition by the notebook by
    Kilian Batzner: [https://www.kaggle.com/code/batzner/gini-coefficient-an-intuitive-explanation](https://www.kaggle.com/code/batzner/gini-coefficient-an-intuitive-explanation)
    that, though means of plots and toy examples tries to give a sense to a not so
    common metric but in the actuarial departments of insurance companies.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛中Kilian Batzner的笔记本提供了另一个很好的解释：[https://www.kaggle.com/code/batzner/gini-coefficient-an-intuitive-explanation](https://www.kaggle.com/code/batzner/gini-coefficient-an-intuitive-explanation)，尽管通过图表和玩具示例试图给一个不太常见的度量一个感觉，但在保险公司的精算部门。
- en: In chapter 5 of the Kaggle Book (pages 95 onward), we explained how to deal
    with competition metrics, especially if they are new and generally unknown. As
    an exercise, can you find out how many competitions on Kaggle have used the normalized
    Gini coefficient as an evaluation metric?
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在Kaggle书籍的第5章（第95页及以后），我们解释了如何处理比赛度量，特别是如果它们是新的且普遍未知的情况。作为一个练习，你能找出在Kaggle上有多少比赛使用了归一化基尼系数作为评估指标吗？
- en: 'The metric can be approximated by the Mann–Whitney U non-parametric statistical
    test and by the ROC-AUC score because it approximately corresponds to 2 * ROC-AUC
    - 1\. Hence, maximizing the ROC-AUC is the same as maximizing the Normalized Gini
    coefficient (for a reference see the “Relation to other statistical measures”
    in the Wikipedia entry: [https://en.wikipedia.org/wiki/Gini_coefficient](https://en.wikipedia.org/wiki/Gini_coefficient)).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该度量可以通过Mann–Whitney U非参数统计检验和ROC-AUC分数来近似，因为它大约对应于2 * ROC-AUC - 1。因此，最大化ROC-AUC等同于最大化归一化基尼系数（参见维基百科条目中的“与其他统计度量之间的关系”：[https://en.wikipedia.org/wiki/Gini_coefficient](https://en.wikipedia.org/wiki/Gini_coefficient)）。
- en: 'The metric can also be approximately expressed as the covariance of scaled
    prediction rank and scaled target value, resulting in a more understandable rank
    association measure (see Dmitriy Guller: [https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/40576](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/40576))'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该度量也可以近似表示为缩放预测排名与缩放目标值的协方差，从而得到一个更易于理解的排名关联度量（参见Dmitriy Guller：[https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/40576](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/40576)）
- en: From the point of view of the objective function, you can optimize for the binary
    logloss (as you would do in a classification problem). Nor ROC-AUC nor the normalized
    Gini coefficient are differentiable and they may be used only for metric evaluation
    on the validation set (for instance for early stopping or for reducing the learning
    rate in a neural network). However, optimizing for the logloss doesn’t always
    improve the ROC-AUC and the normalized Gini coefficients. There is actually a
    differentiable ROC-AUC approximation (Calders, Toon, and Szymon Jaroszewicz. "Efficient
    AUC optimization for classification." European conference on principles of data
    mining and knowledge discovery. Springer, Berlin, Heidelberg, 2007 [https://link.springer.com/content/pdf/10.1007/978-3-540-74976-9_8.pdf](https://link.springer.com/content/pdf/10.1007/978-3-540-74976-9_8.pdf)).
    However, it seems that it is not necessary to use anything different from logloss
    as objective function and ROC-AUC or normalized Gini coefficient as evaluation
    metric in the competition.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从目标函数的角度来看，你可以优化二进制对数损失（就像你在分类问题中做的那样）。ROC-AUC和归一化基尼系数都不是可微的，它们只能用于验证集上的度量评估（例如，用于早期停止或降低神经网络中的学习率）。然而，优化对数损失并不总是能提高ROC-AUC和归一化基尼系数。实际上，存在一个可微的ROC-AUC近似（Calders,
    Toon, 和 Szymon Jaroszewicz. "Efficient AUC optimization for classification." European
    conference on principles of data mining and knowledge discovery. Springer, Berlin,
    Heidelberg, 2007 [https://link.springer.com/content/pdf/10.1007/978-3-540-74976-9_8.pdf](https://link.springer.com/content/pdf/10.1007/978-3-540-74976-9_8.pdf)）。然而，似乎在比赛中没有必要使用与对数损失不同的目标函数，以及将ROC-AUC或归一化基尼系数作为评估指标。
- en: 'There are actually a few Python implementations among the notebooks. We have
    used here and we suggest the work by CPMP ([https://www.kaggle.com/code/cpmpml/extremely-fast-gini-computation/notebook](https://www.kaggle.com/code/cpmpml/extremely-fast-gini-computation/notebook))
    that uses Numba for speeding up computations: it is both exact and fast.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些笔记本中实际上有几个Python实现。我们在这里使用了CPMP的工作（[https://www.kaggle.com/code/cpmpml/extremely-fast-gini-computation/notebook](https://www.kaggle.com/code/cpmpml/extremely-fast-gini-computation/notebook)），该工作使用Numba来加速计算：它既精确又快速。
- en: Examining the top solution’s ideas from Michael Jahrer
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查Michael Jahrer的顶级解决方案的想法
- en: Michael Jahrer ([https://www.kaggle.com/mjahrer](https://www.kaggle.com/mjahrer),
    competitions grandmaster and one of the winners of the Netflix Prize in the team
    "BellKor's Pragmatic Chaos"), has led for long time and by a fair margin the public
    leaderboard during the competition and, when finally the private one has been
    disclosed, has been declared the winner.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Michael Jahrer ([https://www.kaggle.com/mjahrer](https://www.kaggle.com/mjahrer)，竞赛大师级选手，同时也是“BellKor's
    Pragmatic Chaos”团队在Netflix Prize竞赛中的获奖者之一)，在竞赛期间长时间以较大优势领先公开排行榜，并在最终私人排行榜公布后，被宣布为获胜者。
- en: Shortly after, in the discussion forum, he published a short summary of his
    solution that has become a reference for many Kagglers because of his smart usage
    of denoising autoencoders and neural networks ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629)).
    Although Michael hasn’t accompanied his post by any Python code regarding his
    solution (he quoted it as an “old school” and “low level” one, being directly
    written in C++/CUDA with no Python), his writing is quite rich in references to
    what models he has used as well to theirs hyper-parameters and architectures.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，在讨论论坛中，他发布了他解决方案的简要总结，由于他对去噪自编码器和神经网络的巧妙使用，这个总结已成为许多Kagglers的参考（[https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629)）。尽管Michael没有附上关于他解决方案的任何Python代码（他将其称为“老式”和“低级”的，是直接用C++/CUDA编写的，没有使用Python），但他的写作中充满了对所使用模型的引用，以及它们的超参数和架构。
- en: First, Michael explains that his solution is composed of a blend of six models
    (one LightGBM model and five neural networks). Moreover, since no advantage could
    be gained by weighting the contributions of each model to the blend (as well as
    doing linear and non-linear stacking) likely because of overfitting, he states
    that he resorted to just a plain blend of models (an arithmetic mean) that have
    been built from different seeds.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Michael解释说，他的解决方案由六个模型（一个LightGBM模型和五个神经网络）的组合构成。此外，由于加权每个模型对组合的贡献（以及进行线性和非线性堆叠）可能无法获得优势，这可能是由于过拟合，他声称他转而使用了一个简单的模型组合（算术平均值），这些模型是从不同的种子构建的。
- en: Such insight makes the task much easier for us in order to replicate his approach,
    also because he also mentions that just having blend together the LightGBM’s results
    with one from the neural networks he built would have been enough to guarantee
    the first place in the competition. That will limit our exercise work to two good
    single models instead of a host of them. In addition, he mentioned having done
    little data processing, but dropping some columns and one-hot encoding categorical
    features.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的洞察使我们的任务更容易复制他的方法，也因为他提到，仅仅将LightGBM的结果与他自己构建的神经网络的其中一个结果混合在一起，就足以保证在竞赛中获得第一名。这将限制我们的练习工作只关注两个优秀的单一模型，而不是一大堆模型。此外，他还提到，他做了很少的数据处理，但删除了一些列并对分类特征进行了独热编码。
- en: Building a LightGBM submission
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建LightGBM提交
- en: 'Our exercise starts by working out a solution based on LightGBM. You can find
    the code already set for execution at the Kaggle Notebook at this address: [https://www.kaggle.com/code/lucamassaron/workbook-lgb](https://www.kaggle.com/code/lucamassaron/workbook-lgb).
    Although we made the code readily available, we instead suggest you to type or
    copy the code directly from the book and execute it cell by cell, understanding
    what each line of code does and you could furthermore personalise the solution
    and have it perform even better.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的练习从基于LightGBM制定解决方案开始。您可以在以下地址找到已设置好的用于执行的代码：[https://www.kaggle.com/code/lucamassaron/workbook-lgb](https://www.kaggle.com/code/lucamassaron/workbook-lgb)。尽管我们已经提供了代码，但我们建议您直接从书中键入或复制代码并逐个执行代码块，理解每行代码的作用，并且您还可以进一步个性化解决方案，使其表现更佳。
- en: We start by importing key packages (Numpy, Pandas, Optuna for hyper-parameter
    optimization, LightGBM and some utility functions). We also define a configuration
    class and instantiate it. We will discuss the parameters defined into the configuration
    class during the exploration of the code as we progress. What is important to
    remark here is that by using a class containing all your parameters it will be
    easier for you to modify them in a consistent way along the code. In the heat
    of a competition, it is easy to forget to update a parameter that it is referred
    to in multiple places in the code and it is always difficult to set the parameters
    when they are dispersed among cells and functions. A configuration class can save
    you a lot of effort and spare you mistakes along the way.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入关键包（Numpy、Pandas、Optuna 用于超参数优化、LightGBM 和一些实用函数）。我们还定义了一个配置类并实例化了它。随着我们对代码的探索，我们将讨论配置类中定义的参数。在此需要强调的是，通过使用包含所有参数的类，您将更容易在代码中一致地修改它们。在比赛的紧张时刻，很容易忘记更新在代码中多处引用的参数，并且当参数分散在单元格和函数中时，设置参数总是很困难。配置类可以节省您大量的精力并避免在过程中犯错误。
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The next step requires importing train, test and the sample submission datasets.
    By doing so by pandas csv reading function, we also set the index of the uploaded
    dataframes to the identifier (the ‘id’ column) of each data example.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步需要导入训练集、测试集和样本提交数据集。通过使用 pandas csv 读取函数，我们还设置了上传数据框的索引为每个数据示例的标识符（即‘id’列）。
- en: Since features that belong to similar groupings are tagged (using ind, reg,
    car, calc tags in their labels) and also binary and categorical features are easy
    to locate (they use the bin and cat, respectively, tags in their labels), we can
    enumerate them and record them into lists.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于属于相似分组特征被标记（使用 ind、reg、car、calc 标签在其标签中）以及二元和分类特征易于定位（它们分别使用 bin 和 cat 标签在其标签中），我们可以枚举它们并将它们记录到列表中。
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, we just extract the target (a binary target of 0s and 1s) and remove it
    from the train dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们只提取目标（一个由 0 和 1 组成的二元目标）并将其从训练数据集中删除。
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At this point, as pointed out by Michael Jahrer, we can drop the calc features.
    This idea has recurred a lot during the competition ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41970](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41970)),
    especially in notebooks, both because it could be empirically verified that dropping
    them improved the public leaderboard score, both because they performed poorly
    in gradient boosting models (their importance is always below the average). We
    can argue that, since they are engineered features, they do not contain new information
    in respect of their origin features but they just add noise to any model trained
    comprising them.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，正如 Michael Jahrer 所指出的，我们可以删除 calc 特征。这个想法在比赛中反复出现 ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41970](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/41970))，尤其是在笔记本中，一方面是因为经验上可以验证删除它们可以提高公共排行榜的分数，另一方面是因为它们在梯度提升模型中的表现不佳（它们的重要性总是低于平均水平）。我们可以争论，由于它们是工程特征，它们在其原始特征方面不包含新信息，但它们只是向包含它们的任何训练模型添加噪声。
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'During the competition, Tilii has tested feature elimination using Boruta ([https://github.com/scikit-learn-contrib/boruta_py](https://github.com/scikit-learn-contrib/boruta_py)).
    You can find his kernel here: [https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook](https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook).
    As you can check, there is no calc_feature considered as a confirmed feature by
    Boruta.'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在比赛中，Tilii 使用了 Boruta 进行特征消除测试 ([https://github.com/scikit-learn-contrib/boruta_py](https://github.com/scikit-learn-contrib/boruta_py))。您可以在以下链接找到他的内核：[https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook](https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook)。如您所查，Boruta
    没有将 calc_feature 作为确认的特征。
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Exercise: based on the suggestions provided in the Kaggle Book at page 220
    (“Using feature importance to evaluate your work”), as an exercise, code your
    own feature selection notebook for this competition and check what features should
    be kept and what should be discarded.'
  id: totrans-57
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 练习：根据 Kaggle 书籍第 220 页提供的建议（“使用特征重要性评估你的工作”），作为一个练习，为这次比赛编写自己的特征选择笔记本，并检查应该保留哪些特征以及应该丢弃哪些特征。
- en: Categorical features are instead one-hot encoded. Since we want to retrain their
    labels, and since the same levels are present both in train and test (the result
    of a careful train/test split between the two arranged by the Porto Seguro team),
    instead of the usual Scikit-Learn OneHotEncoder ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html))
    we use the pandas get_dummies function ([https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)).
    Since the pandas function may produce different encodings if the features and
    their levels differ from train to test set, we assert a check on the one hot encoding
    resulting in the same for both.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 分类特征则采用独热编码。由于我们想要重新训练它们的标签，并且由于相同的级别在训练和测试集中都存在（这是Porto Seguro团队在两个数据集之间进行仔细的训练/测试分割的结果），我们不是使用常规的Scikit-Learn
    OneHotEncoder（[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)），而是使用pandas的get_dummies函数（[https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)）。由于pandas函数可能会根据特征及其级别在训练集和测试集中不同而产生不同的编码，我们确保独热编码的结果对于两者都是相同的。
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After one hot encoding the categorical features we have completed doing all
    the data processing. We proceed to define our evaluation metric, the Normalized
    Gini Coefficient, as previously discussed. Since we are going to use a LightGBM
    model, we have to add a suitable wrapper (`gini_lgb`) in order to return to the
    GBM algorithm the evaluation of the training and the validation sets in a form
    that could works with it (see: [https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html?highlight=higher_better#lightgbm.Booster.eval](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html?highlight=higher_better#lightgbm.Booster.eval)
    - “Each evaluation function should accept two parameters: preds, eval_data, and
    return `(eval_name`, `eval_result`, `is_higher_better`) or list of such tuples”).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在对分类特征进行独热编码后，我们已经完成了所有数据处理。我们继续定义我们的评估指标，即正常化基尼系数，正如之前讨论的那样。由于我们打算使用LightGBM模型，我们必须添加一个合适的包装器（`gini_lgb`），以便将训练集和验证集的评估以LightGBM算法可以处理的形式返回（见：[https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html?highlight=higher_better#lightgbm.Booster.eval](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html?highlight=higher_better#lightgbm.Booster.eval)
    - “每个评估函数应接受两个参数：preds, eval_data，并返回`(eval_name`, `eval_result`, `is_higher_better`)或此类元组的列表”）。
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As for the training parameters, we found that the parameters suggested by Michael
    Jahrer in his post ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629))
    work perfectly. You may also try to come up with the same parameters or with similarly
    performing ones by performing a search by optuna ([https://optuna.org/](https://optuna.org/))
    if you set the `optuna_lgb` flag to True in the config class. Here the optimization
    tries to find the best values for key parameters such as the learning rate and
    the regularization parameters, based on a 5-fold cross-validation test on training
    data. In order to speed up things, early stopping on the validation itself is
    taken into account (which, we are aware, could actually advantage some values
    that can overfit better the validation fold - a good alternative could be to remove
    the early stopping callback and keep a fixed number of rounds for the training).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 关于训练参数，我们发现迈克尔·雅赫尔在其帖子中建议的参数（[https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629)）效果极佳。如果您在配置类中将`optuna_lgb`标志设置为True，您也可以尝试通过optuna（[https://optuna.org/](https://optuna.org/））进行搜索来找到相同的参数或类似性能的参数。在这里，优化尝试根据训练数据的5折交叉验证测试来找到关键参数（如学习率和正则化参数）的最佳值。为了加快速度，我们考虑了在验证过程中的早期停止（我们知道，这实际上可能有利于更好地拟合验证折的一些值
    - 一个好的替代方案可能是移除早期停止回调并保持固定的训练轮数）。
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'During the competition, Tilii has tested feature elimination using Boruta ([https://github.com/scikit-learn-contrib/boruta_py](https://github.com/scikit-learn-contrib/boruta_py)).
    You can find his kernel here: [https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook](https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook).
    As you can check, there is no calc_feature considered as a confirmed feature by
    Boruta.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在比赛中，Tilii测试了使用Boruta（[https://github.com/scikit-learn-contrib/boruta_py](https://github.com/scikit-learn-contrib/boruta_py)）进行特征消除。你可以在他的核函数中找到：[https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook](https://www.kaggle.com/code/tilii7/boruta-feature-elimination/notebook)。正如你可以检查的那样，没有calc_feature被Boruta视为已确认的特征。
- en: In the Kaggle Book, we explain about both hyper-parameter optimization (pages
    241 onward) and provide some key hyper-parameters for the LightGBM model. As an
    exercise, try to improve the hyper-parameter search by reducing or increasing
    the explored parameters and by trying alternative methods such as the random search
    or the halving search from Scikit-Learn (pages 245-246).
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在Kaggle书中，我们解释了超参数优化（从第241页开始）并提供了LightGBM模型的一些关键超参数。作为一个练习，尝试通过减少或增加探索的参数以及尝试Scikit-Learn中的随机搜索或减半搜索等替代方法来改进超参数搜索（第245-246页）。
- en: Once we have our best parameters (or we simply try Jahrer’s ones), we are ready
    to train and predict. Our strategy, as suggested by the best solution, is to train
    a model on each cross-validation folds and use that fold to contribute to an average
    of test predictions. The snippet of code will produce both the test predictions
    and the out of fold predictions on the train set, later useful for figuring out
    how to ensemble the results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了最佳参数（或者我们简单地尝试Jahrer的参数），我们就可以开始训练和预测。根据最佳解决方案的建议，我们的策略是在每个交叉验证折上训练一个模型，并使用该折来对测试预测的平均值做出贡献。代码片段将生成测试预测和训练集上的出卷预测，这后来对于确定如何集成结果非常有用。
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The model training shouldn’t take too long. In the end you can get reported
    the Normalized Gini Coefficient obtained during the cross-validation procedure.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练不应该花费太多时间。最终，你可以在交叉验证过程中报告获得的归一化基尼系数。
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The results are quite encouraging because the average score is 0.289 and the
    standard deviation of the values is quite small.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 结果相当鼓舞人心，因为平均分数是0.289，而值的方差相当小。
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: All that is left is to save the out-of-fold and the test predictions as a submission
    and to verify the results on the public and private leaderboards.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的工作是将出卷和测试预测保存为提交，并在公共和私人排行榜上验证结果。
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The obtained public score should be around 0.28442\. The associated private
    score is about 0.29121, placing you in the 30^(th) position in the final leaderboard.
    A quite good result, but we still have to blend it with a different model, a neural
    network.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的公共分数应该在0.28442左右。相关的私人分数约为0.29121，将你在最终排行榜上的位置排在第30位。这是一个相当好的结果，但我们仍然需要将其与不同的模型，一个神经网络，进行混合。
- en: Bagging the training set (i.e. taking multiple bootstraps of the training data
    and training multiple models based on the bootstraps) should increase the performance,
    though, as Michael Jahrer himself noted in his post, not all that much.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Michael Jahrer在他的帖子中提到，对训练集进行Bagging（即对训练数据进行多次自助抽样并基于这些自助样本来训练多个模型）应该会增加性能，但增加的幅度并不大。
- en: Setting up a Denoising Auto-encoder and a DNN
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置去噪自动编码器和深度神经网络
- en: 'The next step is not to set up a denoising auto-encoder (DAE) and a neural
    network that can learn and predict from it. You can find the running code at this
    notebook: [https://www.kaggle.com/code/lucamassaron/workbook-dae](https://www.kaggle.com/code/lucamassaron/workbook-dae)
    . The notebook can be run in GPU mode (speedier), but it can also run in CPU one
    with some slight modifications.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步不是设置一个去噪自动编码器（DAE）和一个可以从中学习和预测的神经网络。你可以在以下笔记本中找到运行代码：[https://www.kaggle.com/code/lucamassaron/workbook-dae](https://www.kaggle.com/code/lucamassaron/workbook-dae)。笔记本可以在GPU模式下运行（更快），但也可以通过一些轻微的修改在CPU上运行。
- en: You can read more about denoising auto-encoders as being used in Kaggle competitions
    in the Kaggle Book, at pages 226 and following.
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在Kaggle书中了解更多关于去噪自动编码器在Kaggle比赛中应用的信息，在第226页及以后。
- en: Actually there are no examples around reproducing Michael Jahrer’s approach
    in the competition using DAEs, but we tooked example from a working TensorFlow
    implementation in another competition by OsciiArt ([https://www.kaggle.com/code/osciiart/denoising-autoencoder](https://www.kaggle.com/code/osciiart/denoising-autoencoder)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在比赛中使用DAEs重现Michael Jahrer方法的例子并不多，但我们从OsciiArt在另一个比赛中使用的工作TensorFlow实现中汲取了经验[https://www.kaggle.com/code/osciiart/denoising-autoencoder](https://www.kaggle.com/code/osciiart/denoising-autoencoder)。
- en: Here we start by importing all the necessary packages, especially TensorFlow
    and Keras. Since we are going to create multiple neural networks, we point out
    to TensorFlow not to use all the GPU memory available by using the experimental
    `set_memory_growth` command. Such will avoid having memory overflow problems along
    the way. We also record the Leaky Relu activation as a custom one, so we can just
    mention it as an activation by a string in Keras layers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先导入所有必要的包，特别是TensorFlow和Keras。由于我们将创建多个神经网络，我们指出TensorFlow不要使用所有可用的GPU内存，通过使用实验性的`set_memory_growth`命令。这将避免在过程中出现内存溢出问题。我们还记录了Leaky
    Relu激活作为自定义激活，这样我们就可以在Keras层中通过字符串来提及它。
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Related to our intention of creating multiple neural networks without running
    out of memory, we also define a simple function for cleaning the memory in GPU
    and removing models that are no longer needed.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们创建多个神经网络而不耗尽内存的意图相关，我们还定义了一个简单的函数来清理GPU中的内存并移除不再需要的模型。
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We also reconfigure the Config class in order to take into account multiple
    parameters related to the denoising auto-encoder and the neural network. As previously
    stated about the LightGBM, having all the parameters in a single place renders
    simpler modifying them in a consistent way.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还重新配置了Config类，以便考虑与去噪自动编码器和神经网络相关的多个参数。正如之前关于LightGBM所述，将所有参数放在一个地方可以简化以一致方式修改它们。
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As previously, we load the datasets and proceed processing the features by removing
    the calc features and one hot encoding the categorical ones. We leave missing
    cases valued at -1, as Michael Jahrer pointed out in his solution.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们加载数据集并继续处理特征，通过移除计算特征和将分类特征进行one-hot编码。我们保留缺失的案例值为-1，正如Michael Jahrer在他的解决方案中指出的。
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: However, differently from our previous approach, we have to rescale all the
    features that are not binary or one hot-encoded categorical. Rescaling will allow
    the optimization algorithm of both the auto-encoder and the neural network to
    converge to a good solution faster because it will have to work with values set
    into a comparable and predefined range. Instead of using statistical normalisation,
    GaussRank is a procedure that also allows the modification of the distribution
    of transformed variables into a Gaussian one.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与我们的先前方法不同，我们必须重新缩放所有非二进制或非one-hot编码的分类特征。重新缩放将允许自动编码器和神经网络优化算法更快地收敛到良好的解决方案，因为它将不得不在一个可比较和预定义的范围内处理值。与使用统计归一化不同，GaussRank是一种允许将转换变量的分布修改为高斯分布的程序。
- en: 'As also stated in some papers, such as in the Batch Normalization paper: [https://arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf),
    neural networks perform even better if you provide them a Gaussian input. Accordingly
    to this NVidia blog post, [https://developer.nvidia.com/blog/gauss-rank-transformation-is-100x-faster-with-rapids-and-cupy/](https://developer.nvidia.com/blog/gauss-rank-transformation-is-100x-faster-with-rapids-and-cupy/),
    GaussRank works most of the times but when features are already normally distributed
    or are extremely asymmetrical (in such cases applying the transformation may lead
    to worsened performances).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如某些论文所述，例如在批量归一化论文[https://arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf)中，如果提供高斯输入，神经网络的表现会更好。根据这篇NVIDIA博客文章[https://developer.nvidia.com/blog/gauss-rank-transformation-is-100x-faster-with-rapids-and-cupy/](https://developer.nvidia.com/blog/gauss-rank-transformation-is-100x-faster-with-rapids-and-cupy/)，GaussRank在大多数情况下都有效，但当特征已经呈正态分布或极端不对称时（在这种情况下应用转换可能会降低性能）。
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can apply the GaussRank transformation separately on the train and test
    features on all the numeric features of our dataset:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在数据集的所有数值特征上分别应用GaussRank转换到训练和测试特征：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: When normalising the features, we simply turn our data into a NumPy array of
    float32 values, the ideal input for a GPU.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在归一化特征时，我们只需将我们的数据转换为float32类型的NumPy数组，这是GPU的理想输入。
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, we just prepare some useful functions such as the evaluation function,
    the normalized Gini coefficient and a plotting function helpful representing a
    Keras model history of fitting on both training and validation sets.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们只准备一些有用的函数，如评估函数、归一化基尼系数和有助于表示Keras模型在训练集和验证集上拟合历史的绘图函数。
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The next functions are actually a bit more complex and more related to the functioning
    of both the denoising auto-encoder and the supervised neural network. The `batch_generator`
    is a function that will create a generator providing shuffled chunks of the data
    based on a batch size. It isn’t actually used as a stand-alone generator but as
    part of a more complex batch generator that we are soon going to describe, the
    `mixup_generator`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的函数实际上要复杂一些，并且与去噪自动编码器和监督神经网络的功能更相关。`batch_generator`是一个函数，它将创建一个生成器，提供基于批次大小的数据块。它实际上不是一个独立的生成器，而是作为我们将要描述的更复杂的批次生成器的一部分，即`mixup_generator`。
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `mixup_generator`, another generator, returns batches of data whose values
    have been partially swapped in order to create some noise and augment the data
    in order for the denoising auto-encoder (DAE) not to overfit the train set. It
    works based on a swap rate, fixed at 15% of features as suggested by Michael Jahrer.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`mixup_generator`是另一个生成器，它返回部分交换值的批次数据，以创建一些噪声并增强数据，以便去噪自动编码器（DAE）不会过度拟合训练集。它基于一个交换率，固定为Michael
    Jahrer建议的15%的特征。'
- en: The function generates two distinct batches of data, one to be released to the
    model and another to be used as a source for the value to be swapped in the batch
    to be released. Based on a random choice, whose base probability is the swap rate,
    at each batch a certain number of features is decided to be swapped between the
    two batches.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数生成两组不同的数据批次，一组用于释放给模型，另一组则用作在释放批次中交换值的来源。基于随机选择，其基本概率为交换率，在每个批次中，决定交换两个批次之间的一定数量的特征。
- en: Such assures that the DAE cannot always rely on the same features (since they
    can be randomly swapped from time to time) but it has to concentrate on the whole
    of the features (something similar to dropout in a certain sense). in order to
    find relations between them and reconstruct correctly the data at the end of the
    process.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以确保去噪自动编码器不能总是依赖于相同的特征（因为它们可能随时被随机交换），但它必须关注所有特征（在某种程度上类似于dropout）。以便在它们之间找到关系，并在过程结束时正确地重建数据。
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `get_DAE` is the function that builds the denoising auto-encoder. It accepts
    a parameter for defining the architecture which in our case has been set to three
    layers of 1500 nodes each (as suggested from Michael Jahrer’s solution). The first
    layer should act as an encoder, the second is a bottleneck layer ideally containing
    the latent features capable of expressing the information in the data, the last
    layer is a decoding layer, capable of reconstructing the initial input data. The
    three layers have a relu activation function, no bias and each one is followed
    by a batch normalization layer. The final output with the reconstructed input
    data has a linear activation. The training is provided using an adam optimizer
    with standard settings (the optimised cost function is the mean squared error
    - mse).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_DAE`函数用于构建去噪自动编码器。它接受一个参数来定义架构，在我们的情况下，已经设置为三个各含1500个节点的层（如迈克尔·耶雷尔的建议）。第一层应作为编码器，第二层是瓶颈层，理想情况下包含能够表达数据信息的潜在特征，最后一层是解码层，能够重建初始输入数据。这三层具有relu激活函数，没有偏差，每一层后面都跟着一个批量归一化层。最终输出与重建的输入数据具有线性激活。训练使用具有标准设置的adam优化器（优化的成本函数是均方误差
    - mse）。'
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `extract_dae_features` function is reported here only for educational purposes.
    The function helps in the extraction of the values of specific layers of the trained
    denoising auto-encoder. The extraction works by building a new model combining
    the DAE input layer and the desired output one. A simple predict will then extract
    the values we need (the predict also allows fixing the preferred batch size in
    order to fit any memory requirement).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里仅报告`extract_dae_features`函数，仅用于教育目的。该函数有助于提取训练去噪自动编码器特定层的值。提取是通过构建一个新的模型来完成的，该模型结合了DAE输入层和所需的输出层。然后，简单的预测将提取我们需要的值（预测还允许固定首选的批次大小，以便满足任何内存需求）。
- en: In the case of the competition, given the number of observations and the number
    of the features to be taken out from the auto-encoder, if we were to use this
    function, the resulting dense matrix would be too large to be handled by the memory
    of a Kaggle notebook. For this reason our strategy won’t be to transform the original
    data into the auto-encoder node values of the bottleneck layer but to fuse the
    auto-encoder with its frozen layers up to the bottleneck with the supervised neural
    network, as we will be discussing soon.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在竞赛的情况下，考虑到观察的数量和从自编码器中提取的特征数量，如果我们使用这个函数，得到的密集矩阵将太大，无法由Kaggle笔记本的内存处理。因此，我们的策略不会是将原始数据转换成瓶颈层的自编码器节点值，而是将自编码器与其冻结的层（直到瓶颈层）与监督神经网络融合，正如我们很快将要讨论的。
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: To complete the work with the DAE, we have a final function wrapping all the
    previous ones into an unsupervised training procedure (at least partially unsupervised
    since there is an early stop monitor set on a validation set). The function sets
    up the mix-up generator, creates the denoising auto-encoder architecture and then
    trains it, monitoring its fit on a validation set for an early stop if there are
    signs of overfitting. Finally, before returning the trained DAE, it plots a graph
    of the training and validation fit and it stores the model on disk.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成与DAE的工作，我们有一个最终函数，将所有之前的函数包装成一个无监督的训练过程（至少部分是无监督的，因为有一个早期停止监控器设置在验证集上）。该函数设置混合增强生成器，创建去噪自编码器架构，然后对其进行训练，监控其在验证集上的拟合度以实现早期停止，如果有过度拟合的迹象。最后，在返回训练好的DAE之前，它绘制了训练和验证拟合的图表，并将模型存储在磁盘上。
- en: Even if we try to fix a seed on this model, contrary to the LightGBM model,
    the results are extremely variable and they may influence the final ensemble results.
    Though the result will be a high scoring one, it may land higher or lower on the
    private leaderboard, though the results obtained on the public one are very correlated
    to the public leaderboard and it will be easy for you to always pick up the best
    final submission based on its public results.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们在该模型上尝试固定一个种子，与LightGBM模型相反，结果极其不稳定，它们可能会影响最终的集成结果。虽然结果可能是一个高分，但它可能会在私有排行榜上得分更高或更低，尽管在公共排行榜上获得的结果与公共排行榜非常相关，这将使你能够根据其公共结果始终选择最佳的最终提交。
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Having dealt with the DAE, we take the chance also to define the supervised
    neural model down the line that should predict our claim expectations. As a first
    step, we define a function to define a single layer of the work:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理了DAE之后，我们也有机会定义一个监督神经网络模型，该模型应该预测我们的索赔预期。作为第一步，我们定义了一个函数来定义工作中的一个单层：
- en: Random normal initialization, since empirically it has been found to converge
    to better results in this problem
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机正态初始化，因为经验上已经发现这种方法在这个问题中能收敛到更好的结果
- en: A dense layer with L2 regularization and parametrable activation function
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有L2正则化和可参数化激活函数的密集层
- en: An excludable and tunable dropout
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可排除和可调整的dropout
- en: 'Here is the code for creating the dense blocks:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建密集块的代码：
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you may have already noticed, the function defining the single layer is quite
    customizable. The same goes for the wrapper architecture function, taking inputs
    for the number of layers and units in them, dropout probabilities, regularization
    and activation type. The idea is to be able to run a Neural Architecture Search
    (NAS) and figure out what configuration should perform better in our problem.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能已经注意到的，定义单层的函数相当可定制。同样，对于包装架构函数也是如此，它接受层的数量和其中的单元数、dropout概率、正则化和激活类型作为输入。我们的想法是能够运行神经架构搜索（NAS），并找出在我们的问题中应该表现更好的配置。
- en: As a final note on the function, among the inputs it is required to provide
    the trained DAE because its inputs are used as the neural network model inputs
    while its first layers are connected to the DAE’s outputs. In such a way we are
    de facto concatenating the two models into one (the DAE weights are frozen anyway
    and not trainable, though). This solution has been devised in order to avoid having
    to transform all your training data but only the single batches that the neural
    network is processing, thus saving memory in the system.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 关于该函数的最后一句话，在输入中需要提供训练好的DAE，因为它的输入被用作神经网络模型的输入，而它的第一层与DAE的输出相连。这样，我们实际上是将两个模型合并为一个（DAE的权重已经冻结，不可训练）。这个解决方案是为了避免需要转换所有训练数据，而只需要神经网络处理的单个批次，从而在系统中节省内存。
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We conclude with a wrapper for the training process, including all the steps
    in order to train the entire pipeline on a cross-validation fold.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以一个训练过程的包装来总结，包括训练整个管道所需的步骤，以交叉验证折进行训练。
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Since our DAE implementation is surely different from Jahrer’s, although the
    idea behind is the same, we cannot rely completely on his indications on the architecture
    of the supervised neural network and we have to look for the ideal ones as we
    have been looking for the best hyper-parameters in the LightGBM model. Using Optuna
    and leveraging the multiple parameters that we set open for configuring the network’s
    architecture, we can run this code snippet for some hours and get an idea about
    what could work better.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的DAE实现与Jahrer的不同，尽管背后的想法相同，我们不能完全依赖他对监督神经网络架构的指示，我们必须寻找理想的架构，就像我们在LightGBM模型中寻找最佳超参数一样。使用Optuna并利用我们为配置网络架构而设置的多个参数，我们可以运行这个代码片段几个小时，并了解什么可能工作得更好。
- en: 'In our experiments we that:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们注意到：
- en: we should use a two layer network with few nodes, 64 and 32 respectively.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该使用一个具有少量节点的两层网络，分别是64和32。
- en: input dropout, dropout between layers and some L2 regularization do help.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入dropout、层间dropout以及一些L2正则化确实有所帮助。
- en: It is better to use the SELU activation function.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SELU激活函数会更好。
- en: 'Here is the code snippet for running the entire optimization experiments:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是运行整个优化实验的代码片段：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If you are looking for more information about more on Neural Architecture Search
    (NAS), you can have a look at the Kaggle Book, at pages 276 onward. In the case
    of the DAE and the supervised neural network, it is critical to look for the best
    architecture since we are implementing something surely different from Michael
    Jahrer solution.
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你想要了解更多关于神经网络架构搜索（NAS）的信息，你可以查看Kaggle书，从第276页开始。在DAE和监督神经网络的情况下，寻找最佳架构至关重要，因为我们正在实施与Michael
    Jahrer解决方案肯定不同的东西。
- en: ''
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As an exercise, try to improve the hyper-parameter search by using KerasTuner
    (to be found on pages 285 onward in the Kaggle Book), a fast solution for optimizing
    neural networks that has seen the important contribution of François Chollet,
    the creator of Keras.
  id: totrans-131
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作为一项练习，尝试通过使用KerasTuner（可在Kaggle书中第285页及以后找到）来改进超参数搜索，这是一种优化神经网络的快速解决方案，它得到了Keras的创造者François
    Chollet的重要贡献。
- en: Having finally set everything ready, we are set to start the training. In about
    one hour, on a Kaggle notebook with GPU, you can obtain complete test and out-of-fold
    predictions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在一切准备就绪之后，我们就可以开始训练了。大约一个小时后，在带有GPU的Kaggle笔记本上，你可以获得完整的测试和跨折预测。
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As done with the LighGBM model, we can get an idea of the results by looking
    at the average fold normalized Gini coefficient.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们对LighGBM模型所做的那样，我们可以通过查看平均折归一化基尼系数来了解结果。
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The results won’t be quite in line with what previously obtained using the LightGBM.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 结果不会与之前使用LightGBM获得的结果完全一致。
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Producing the submission and submitting it will result in a public score of
    about 0.27737 and a private score of about 0.28471 (results may vary wildly as
    we previously mentioned), not quite a high score.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 制作提交并提交将导致公开分数大约为0.27737，私人分数大约为0.28471（结果可能与我们之前提到的有很大差异），并不是一个很高的分数。
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The scarce results from the neural network seem to go by the adage that neural
    networks underperform in tabular problems. As Kagglers, anyway, we know that all
    models are useful for a successful placing on the leaderboard, we just need to
    figure out how to best use them. Surely, a neural network feed with an auto-encoder
    has worked out a solution less affected by noise in data and elaborated the information
    in a different way than a GBM.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的少量结果似乎遵循了这样一个谚语：神经网络在表格问题上的表现不佳。无论如何，作为Kagglers，我们知道所有模型都对在排行榜上取得成功有用，我们只需要找出如何最好地使用它们。当然，一个使用自动编码器的神经网络已经提出了一种受数据噪声影响较小、以不同方式阐述信息的解决方案，这比GBM要好。
- en: Ensembling the results
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果集成
- en: Now, having two models, what’s left is to mix them together and see if we can
    improve the results. As suggested by Jahrer we go straight for a blend of them,
    but we do not limit ourselves to producing just an average of the two (since our
    approach in the end has slightly differed from Jahrer’s one) but we will also
    try to get optimal weights for the blend. We start importing the out-of-fold predictions
    and having our evaluation function ready.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，拥有两个模型后，剩下的就是将它们混合在一起，看看我们是否能提高结果。正如Jahrer所建议的，我们直接尝试将它们混合，但我们并不局限于仅仅产生两个模型的平均值（因为我们的方法最终与Jahrer的方法略有不同），我们还将尝试为混合找到最佳权重。我们开始导入折叠外的预测，并准备好我们的评估函数。
- en: '[PRE32]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Once done, we turn the out-of-fold predictions of the LightGBM and the neural
    network into ranks since the normalized Gini coefficient is sensible to rankings
    (as would be a ROC-AUC evaluation).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，我们将LightGBM和神经网络的折叠外预测转换为排名，因为归一化的基尼系数对排名敏感（就像ROC-AUC评估一样）。
- en: '[PRE33]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now we just test if, by combining the two models using different weights we
    can get a better evaluation on the out-of-fold data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只是测试，通过使用不同的权重结合两个模型，我们是否能得到更好的折叠外数据评估。
- en: '[PRE34]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'When ready, by running the snippet we can get interesting results:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 准备就绪后，通过运行代码片段，我们可以得到有趣的结果：
- en: '[PRE35]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: It seems that blending using a strong weight (0.8) on the LightGBM model and
    a weaker one (0.2) on the neural network may lead to an over performing model.
    We immediately try this hypothesis by setting a blend with the same weights for
    the models and the ideal weights that we have found out.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，使用强权重（0.8）在LightGBM模型上和弱权重（0.2）在神经网络上混合可能会产生一个表现优异的模型。我们立即通过为模型设置相同的权重和我们已经找到的理想权重来测试这个假设。
- en: '[PRE36]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'First we try the equal weights solution:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们尝试等权重解决方案：
- en: '[PRE37]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'It leads to a public score of 0.28393 and a private score of 0.29093, which
    is around 50^(th) position in the final leaderboard, a bit far from our expectations.
    Now let’s try using the weights the out-of-fold predictions helped us to find:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了公共得分为0.28393，私人得分为0.29093，大约位于最终排行榜的第50位，离我们的预期有点远。现在让我们尝试使用折叠外预测帮助我们找到的权重：
- en: '[PRE38]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Here the results lead to a public score of 0.28502 and a private score of 0.29192
    that turns out to be around the seventh position in the final leaderboard. A much
    better result indeed, because the LightGBM is a good one but it is probably missing
    some nuances in the data that can be provided as a favorable correction by adding
    some information from the neural network trained on the denoised data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，结果导致公共得分为0.28502，私人得分为0.29192，最终在最终排行榜上大约位于第七位。确实是一个更好的结果，因为LightGBM是一个很好的模型，但它可能缺少一些数据中的细微差别，这些差别可以通过添加来自在去噪数据上训练的神经网络的某些信息来作为有利的纠正。
- en: As pointed out by CPMP in his solution ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44614](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44614)),
    depending on how to build your cross-validation, you can experience a “huge variation
    of Gini scores among folds”. For this reason, CPMP suggests to decrease the variance
    of the estimates by using many different seeds for multiple cross-validations
    and averaging the results.
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如CPMP在他的解决方案中指出的（[https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44614](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44614)），根据如何构建你的交叉验证，你可能会经历“折叠间基尼分数的巨大变化”。因此，CPMP建议通过使用多个交叉验证的许多不同种子来减少估计的方差，并平均结果。
- en: ''
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Exercise: as an exercise, try to modify the code we used in order to create
    more stable predictions, especially for the denoising auto-encoder.'
  id: totrans-159
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 练习：作为一个练习，尝试修改我们使用的代码，以便创建更稳定的预测，特别是对于去噪自编码器。
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this first chapter, you have dealt with a classical tabular competition.
    By reading the notebooks and discussions of the competition, we have come up with
    a simple solution involving just two models to be easily blended. In particular,
    we have offered an example on how to use a denoising auto-encoder in order to
    produce an alternative data processing particularly useful when operating with
    neural networks for tabular data. By understanding and replicating solutions on
    past competitions, you can quickly build up your core competencies on Kaggle competitions
    and be quickly able to perform consistently and highly in more recent competitions
    and challenges.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经处理了一个经典的表格竞赛。通过阅读竞赛的笔记本和讨论，我们提出了一种简单的解决方案，仅涉及两个易于混合的模型。特别是，我们提供了一个示例，说明如何使用去噪自编码器来生成一种特别适用于处理表格数据的替代数据处理方法。通过理解和复制过去竞赛中的解决方案，你可以在Kaggle竞赛中快速建立你的核心能力，并迅速在最近的竞赛和挑战中表现出色和稳定。
- en: In the next chapter, we will explore another tabular competition from Kaggle,
    this time revolving about a complex prediction problem with time series.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索Kaggle的另一个表格竞赛，这次是关于一个复杂的时间序列预测问题。
